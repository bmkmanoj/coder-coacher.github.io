<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSRNE 5th Anniversary Symposium - Mathematics and Theoretical Computer Science | Coder Coacher - Coaching Coders</title><meta content="MSRNE 5th Anniversary Symposium - Mathematics and Theoretical Computer Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSRNE 5th Anniversary Symposium - Mathematics and Theoretical Computer Science</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c1g8i-KtT8M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
this morning is on mathematics and
theoretical computer science and our
first speaker will be nadia heninger
she's an assistant professor of computer
science at the University of
Pennsylvania and she's a really
important part of the Microsoft Research
New England community she was an intern
here twice in graduate school and then
she came and did a postdoc year before
starving at you pan anyway I don't want
to use up her time but please welcome
while he's doing that I want to say that
I'm really excited to be here because ms
our new england has been sort of my home
away from whatever institution i've been
associated with for you know the last
couple of years of my PhD and during the
time that i was a postdoc and so it's
really great that there's such a
welcoming environment for all sorts of
strange children who do strange work
okay so the so the title of my talk is
factoring made easy and I'm going to
talk about sort of a thread of work that
has sort of continued through I guess
the end of my PhD in my postdoc and I'm
I guess still continuing it as I just
start my professorship does this work
yes okay so this is a big number it has
4096 bits which is about 1200 decimal
digits and this is actually part of an
RSA public key RSA is the most commonly
used public key cryptosystem on the
internet and the hardness of RSA we hope
that RSA is hard for someone to break
the hardness depends entirely on numbers
like this being difficult to factor into
their prime factors and this key I think
is particularly important for Microsoft
because this is the public key for the
Microsoft Internet authority which has
signed a bunch of certificate
authorities which have signed things
like microsoft com so if somebody could
factor this number they would be able to
impersonate Microsoft and do all sorts
of terrible things on the internet and
it would totally break the sort of trust
relationship that Microsoft has with
people on the internet so that would be
terrible we don't want anybody to be
able to back to this number so how hard
is factoring well sort of the state of
the art is that factoring seems hard not
hard in the sense of end being np-hard
it's probably not NP hard but it's it's
hard in that we don't have anything
better than slightly sub exponential
algorithms to solve it so the current
classical record for factoring is a 768
bit RSA modulus this is a paper from
2010 actually Peter Montgomery who's at
Microsoft Research in redmond because a
co-author on this paper so Emma sarah
has its talons and all sorts of
different
interesting things some people may be
aware that factoring is easy for quantum
computers if someone could build a
quantum computer so the current quantum
factoring record as far as I can tell is
21 that's not 21 digits that's the
number 21 this is from last year so okay
factoring seems hard so like any good
computer scientist we take a hard
problem and we start making it easier
and I'm going to tell you how to cheat
at factoring so here's a here's an
interesting fact having to do with the
GCD GC d stands for greatest common
divisor so any two integers have a
unique greatest common divisor and just
by the construction of RSA modulator the
product of two large primes of equal
size if you have to RSA moduli here's n1
and n2 and they share exactly one large
prime factor in common say P then the
GCD of the two numbers is P and if you
could compute that then you could just
divide out by P and get the prime
factorizations of both RSA moduli this
seems kind of fat and actually the GCD
algorithm is widely considered to be the
first actual algorithm ever published by
euclid and 300 BC this is the foundation
of if you take a really broad view of
computer science this is the felt you
know sort of the foundation of computer
science 2,000 years ago and the GCD
algorithm is extremely efficient on my
home computer it takes about 15 micro
seconds to compute the GCD up to 1024
bit RSA moduli in contrast even for say
the NSA it probably takes them as far as
we can tell with a educated guess still
a fair amount of effort to factor 1024
bit RSA modular and we haven't done it
in public so this is quite a difference
so you might be wondering is this like
is this fatal for for the RSA public key
cryptosystem the most commonly used
public key cryptosystem on the internet
the thing on which billions of dollars
of people's faith in the security of
their online transactions rely
so you can imagine sort of an experiment
you collect a whole bunch of RSA public
keys off the internet and you start
computing G CDs and what should happen
nothing so for sort of the
mathematically inclined people in the
audience we know how to do this sort of
analysis just back at the envelope the
prime number theorem tells us how many
primes there are of a particular size so
if we have 1024 bit RSA moduli which is
sort of the smallest size that seems
relevant we have 512 bit prime so we
know there's about 10 to the 150 512-bit
primes that's a lot and then we're sort
of asking the question if you take a
whole bunch of numbers that are the
product of 512 bit primes we throw them
into a box we ask what is the
probability of collisions this is a
balls into a bins problem or you can
analyze it using sort of the birthday
paradox analysis basically we can
calculate the probability that there's a
collision if you choose these at random
and we know that essentially nothing
interesting happens until you get about
square root of the number of total
primes there are two primes per module
modulus so this is the probability of a
non-trivial GCD and I have graphed this
function for you with some helpful y
points along the way essentially this
function is about 0 until you get pretty
close to the number of atoms in the
universe this is like astronomically
small probability so this says that if
we somehow wanted to make an internet of
things that had a public key for every
atom in the universe we may not want to
use 1024 bit RSA
but until then we have really nothing to
worry about everything is fine your keys
are random you're safe you're good okay
okay so life is good what else could
happen so I want to show you a little
magic trick this involves computer code
hopefully we're comfortable with this
this is actually code from sage which is
some mathematics software so here I'm
generating a random RSA modulus and
there's nothing under my sleeve here
this you know two random primes 512 fits
i multiply them together i get my RSA
modulus n and then i create this value a
which is the sort of i'm handing myself
the most significant bits of one of the
prime factors so I've deleted the least
significant 86 bits of this prime and so
I don't know first we're going to
consider the case where an attacker gets
knowledge of a but not the recipe and we
should be good because eighty six bits
it will even though it's sort of a
relatively small fraction of the bits of
P it's still way more than we know that
anybody can brute force you can brute
force I don't know up to like 50 bits
realistically okay so here's the here's
the magic trick part i create a matrix
consisting of values depending only on a
86 and my public modulus n this is the
part where I sort of have my magicians
hat and I throw everything into the Hat
and I I mix it up then I call this LOL
command which this is the Leinster
leinster lovas lattice basis reduction
algorithm you don't need to know that
it's just we're using it as a black box
but it just takes a matrix and it
transforms it into another matrix then i
take this transformed matrix i take sum
of the coefficients of my transformed
matrix and i create a polynomial and
then i look for
the roots of this polynomial and one of
the roots is this value to 7753 blah
blah blah and if I take a plus this
value that's actually p so somehow doing
this arbitrary looking magical
computation I have reconstructed in
seconds on my laptop a value that not
even the NSA should be able to
brute-force so magic trick what's
actually going on here this is actually
a simplified version of a theorem of
coppersmith this is from 1996 which says
that given half of the bits of a prime
factor of an RSA modulus we can factor
the modulus in polynomial time where
polynomial time means efficient for
computer science purposes and this is
this is actually efficient like in the
real world and i think this theorem is
really cool it's really sort of
counterintuitive factoring is hard but
somehow if you get half of the bits of
one of the factors you shouldn't be able
to brute-force the other half but that
gives you enough information to
reconstruct the whole thing that's
that's totally magical so actually the
first time that I was an intern here in
2009 we do at the weekly meetings on
Thursday we there's always a 15-minute
presentation of research you get up and
talk about whatever you want on the
whiteboard and I got up and presented a
version of this theorem because I
thought it was so cool and actually
after I did that presentation Madhu got
up and said like hey there's like
business connection with coding theory
that's like it kind of works like this
but and so we sort of started talking
about codes and that actually turned
into a series of papers that i published
with henry who was my mentor where the
super hand-wavy version of the theorem
was that there's sort of deep
connections between these sort of crypt
analysis techniques about RSA moduli and
decoding of reed-solomon codes and this
the sort of main technique here or the
main idea here is that there's a
analogy between integers and polynomials
if you're a mathematician these are both
algebraic rings and this sort of this
one insight gives us the ability to sort
of have a framework that lets us
translate results back and forth between
cryptography and coding theory and we
can sort of do arbitrage and this is
constructive it's not just you know it's
not just a fact it actually gives us new
algorithms in both areas so I think this
is really cool but we're not doing just
math here so I want to sort of finish up
with some concrete results here so
despite the fact that these things these
two mathematical facts that I just
explained should never ever happen in
the real world I just explained I mean
who is going to give you all but eighty
six bits of an RSA factor or who is who
is going to be stupid enough to create
RSA moduli that happen to share common
factors that nevertheless over the past
couple of years I somehow ended up with
a series of papers with this general
research outline which is collect a
whole bunch of public cryptographic keys
and then look for stuff that should
never ever ever happen that's the
outline so here is the results of one
paper we scanned all of the publicly
visible ipv4 space and collected all of
the HTTPS certificates so the things
that protect you when you are surfing
the web and we collected all of the ssh
host keys so the things that tell you
that you're actually connecting to the
SS a toast that you hope you're
connecting to and then we ran the
pairwise GCD algorithm on all of the RSA
keys that we got and this gave us the
factorization for 64,000 HTTPS hosts and
2500 SSH hosts which as I showed earlier
should never ever ever happen and it
turns out that what we found after doing
sort of more systems level analysis is
that there were widespread random number
generation flaw is mostly combined to
confined to
certain kinds of embedded devices things
like your little home Wi-Fi router is
actually not very good at generating
cryptographic keys and in particular we
discovered a flaw in the Linux random
number generator that was responsible
for a lot of this there's also a lot of
people just have software bugs that keep
them from generating secure Keys as
follow-up work this is actually going to
be presented in a couple months at AJ
crypt you might say okay one of the what
can you do about this let's use Hardware
random number generators that's much
better than software right so in this
work we actually looked at a smart card
deployment in Taiwan so Taiwan has these
smart cards you can use them to
authenticate yourself online to pay your
income taxes or change your car
registration like they have a
certificate which you can use to provide
you know legally binding digital
signatures and so basically and and
these these smart cards they generate
the keys on the card they were certified
by all of the buzz word agencies that do
cryptographic certifications so in this
research we collected three million
certificates corresponding to real
Taiwanese people that were publicly
available because they are public
certificates and computed the GCD s and
a hundred and three of them were
factored due to non trivial g cds but
after staring at the factors that we
found the most common factor was the
next prime after 2 to the 511 plus 2 to
the 510 this is really not random and so
the fact that these some of the factors
had these like special structures let us
predict large numbers of bits that might
appear in subsequent problematically
generated keys and so we were actually
able to use the coppersmith method that
i described to factor another 80 keys
and it's pretty clear that you know the
more computational effort we put into
this the more keys will be able to
factor and essentially the the reason
for this is that the
there was both a flawed hardware random
number generator and a failure to
post-process the the keys that were
generated due to sort of human error so
the lessons of this well cryptographic
failures can be both subtle and
mathematically beautiful and turns out
that random number generation is really
difficult to get right into practice
thank you so okay how the question is
how can there be a flawed Hardware
random number generator one common way
apparently to do harder random number
generation is that you have sort of
these oscillating circuits that should
have some kind of chaotic interaction
between them but if they happen to get
stuck in a particular kind of
oscillation then they will outputs a
periodic sequences of bits which is what
we saw
so the questions yes so the question is
were there any Microsoft keys among the
flawed HTTPS and ssh keys and nothing
that was explicitly generated by
Microsoft products there are a few
things that were running on like flawed
implementations of things running on
like Microsoft is servers but actually
yeah after after we went sort of public
with the results some of our friends at
Microsoft emailed in a panic saying did
you find any Microsoft keys and the
answer is no yeah did we make any
attempts to tell people whose faulty
keys we discovered the answer is yes it
was actually an extremely painful
process I I personally emailed about 60
companies fewer than half of them ever
got back to us yes people changed what
they did
so it has has the Linux bug been fixed
the answer is yes actually we contacted
the Linux kernel security team last
summer about this and lena's himself
wrote back within about half an hour and
there were code patches flying back and
forth by the end of the day so it was it
was a very fast change can I say more
about the human error so these cards
were allowed to operate in FIPS mode
where they were sort of like certified
to the federal information processing
standard or they could operate in non
FIPS mode and they were being operated
in non FIPS mode I mean it's probably
much more efficient if you're not
actually doing all this post processing
yeah
well so the question is what were the
different failure modes so there were
two projects there's the Taiwanese Keys
we've had the three million overall keys
and about 200 that were that we've
factored and then there was the Linux
flaw which was out of 10 million keys
64,000 almost all of well no not almost
all more than fifty percent of the
flawed HTTPS certificates were due to a
particular router product made by a
Juniper they just happen to have a lot
of them that were very poorly generated
they were not using linux on that
machine I think Linux I probably was
probably a significant fraction of the
sort of remaining forty percent of the
flawed HTTPS certificates the Linux had
nothing to do with the hardware random
number generation flaws in the Taiwanese
keys so our next speaker this morning is
Madhu Sudan he is a researcher here at
Microsoft Research in New England and a
great theoretical computer scientist
thank you all thank you all for coming I
would like to thank microsoft research
writing me here I was it's been four
years since they did took me two years
to figure out really whether I wanted to
be here and the last four years have
been just wonderful great colleagues
fantastic environment wonderful visitors
and I've been enjoying every minute I'll
tell you a little bit about the kind of
work that we've been doing in the last
this is probably been going on for the
last eight years and spit up some new
steam in the last four and i'll be
focusing mostly on that so let's see if
I know how to do
I want to talk about the problem of
reliable communication but I want to
talk about reliability in the new sense
but let me talk about it first in the
old sense this is a question dating back
to the 1940s and there are two parties
Alice and Bob trying to communicate over
a channel now communication channels are
always noisy there's always going to be
some error when you're talking over some
channel and unfortunately when you are
talking about digital information noise
can have sort of catastrophic effects so
this is not the kind of errors that you
want to see it's okay if you're
listening to some audio and the music is
slightly warped but when you're talking
about digital information and using
communication media to communicate that
you don't want to see a few errors this
task of trying to figure out what are
errors and how to fix them was
undertaken by Shannon and he came up
with a beautiful mathematical theory of
communication now i want to emphasize
the part that it was mathematical
because this was a setting where you
didn't quite know how to even model the
notion of an error leave alone figure
out how can you fix them and work in the
limits be and Shannon undertook this
task in a significant piece of work in
1948 which was published in 1948 after
about four to five years of research on
his own the kind of questions that he
considered over here and I want to tell
you about these so that we can talk
about variations on these for today is
how to take a large amount of
information that you may have and
transmit it over expensive but reliable
channel to someone else who also knows
quite a bit of this large amount of
information that you have so you should
be able to compress the information the
question is how do you do it amongst
other things he also asked the question
how do you detect errors when they're
happening and how do you correct them
and this led to some beautiful
mathematical concepts notions such as he
invented the notion of a bit others such
as cheering had encountered the notion
but I think Shannon gave it the name the
notion of entropy as a mathematical
concept not related to heat and disorder
and physical systems the concept of
mutual information which really codifies
what
information means for us today concept
such as coding decoding etc etc and this
kind of research was just phenomenal it
drove the technology as well as the
theory for 60 years 70 years since then
ever since then we've been living under
the shadow of this particular theory
today I want to talk about a new class
of uncertainties now uncertainty is
something that we were always
encountering and trying to cope up with
cope with and uncertainty was usually
defined as what did the channel due to
this communication there was never an
issue the uncertainty was introduced by
the channel but now I think we are in a
stage where we are worried less about
the uncertainty introduced by the
channel mainly because of the success of
the Shannon theory but we are now
starting to focus a lot about the
uncertainty of the communicating parties
about each other we are living in a
world where communication devices are no
longer pure communication devices they
are all capable of computing they are
all capable of being programmed and when
you can program one device but not the
other into interacting systems they can
get quickly out of sync in terms of what
they mean what they know what they do so
these are sort of I'll give you a couple
of examples of questions that would fall
under this umbrella which were not
captured you know you go into a new
place you want to print on to a printer
it seems to take a lot of time for the
computer to figure out what it is that
the printer you know how it would accept
this format why should it be so hard the
computer out there is a pretty
intelligent device has as much thinking
powers as maybe my brain does there's
this printer which also has the same
amount of capabilities and resources and
so far what we've been trying to tell
these two devices is to turn this brains
off and try to communicate the way the
third party told them to do it why can't
they just sort of learn to interact and
figure out a way around this setting a
different a non-interactive setting in
which we want to worry about which we I
am worried about right now I mean I have
all these photographs which are all
digitized in various formats over the
course of the last 15 years and I don't
know what is the one for my
in which I should save them so that in
another 10 years I'll be able to read
them and scan them and what is the
uncertainty here we are uncertain about
what the future holds for us what are
the devices and formats that will be
understandable in the future I am trying
to decide today how to encode
information so that it will be readable
20 years from now but I don't know what
the future holds the sender and receiver
are not really understanding each other
completely they don't even the receiver
is not yet even defined it's going to
happen in 20 years from now so I mean if
I give you massive amounts of data would
you just take all of this in a beautiful
compression algorithm would you take
that compression algorithm and apply it
I suggest not okay so there's a new
class of questions over here and some
new solutions are needed but before we
go into this particular thing and one
thing that I want to say is a lot of
design communication mechanisms the
kinds of things that we've been using so
far to let computers talk to each other
are very different from the other
communication model that we've seen
which is communication amongst humans
they are they satisfy very different
behavioral characteristics and we would
like to expand this mathematical theory
of communication also to expand cover
those other parts so that we understand
what's going on there and maybe find
some new methods that can be used in our
setting so but before we get two answers
to the new set of questions we should
propose what is the new model how does
it differ from the old model of
communication and to be honest this took
us you know I mean the next slide will
explain what it is it won't be a major
revelation but it took us about five
years to understand this is what we were
doing okay these things take some time
and much of this work is joined for
Brendan Juba I was sitting over here so
in the classical model the shaman model
there was two players this was not
called Alice and Bob by Shannon but by
RSA whom we heard about from Nadia's
wonderful talk earlier I RSA coined this
term Alice and Bob these are going to be
the two interacting players in our talk
also and they're talking across the
channel but in the new model I don't
want to talk about an Alice
and a bob I want to talk instead of some
uncertainty about Alice's and
uncertainty about Bob's so I'm going to
replace them by a cloud of possibilities
there's some this cloud consists of many
possible incarnations of Alice many
possible incarnations of Bob these
incarnations may differ in what their
knowledge and beliefs are about the
world they may also differ in how they
act and behave and under interpret
messages that they are sending to each
other so for instance these different
Alice's maybe ones that use a particular
you know they may differ in the kind of
operating systems that they use they may
differ in the kind of software they have
loaded on them and so on and Alice one
is perfectly compatible with Bob number
one this is what they were meant to be
talking to each other Alice too it's
perfectly compatible with Bob too
however what's going to happen in this
model is that an adversary possibly
comes in and says look Alice number k
you're supposed to talk to Bob number
two ok and sorry he goes and tells Alice
number K that she's supposed to be the
one speaking on this channel and she
tells Bob number two that he's the one
who's supposed to be speaking on the
sound but they don't know each other's
identity they don't quite part of the
challenge now is to either learn each
other's identity or to overcome the fact
that they don't quite know precisely who
the other player is so this leads to a
new class of questions new problems I'll
give you a couple of examples of
problems that we've looked at in the
setting one in a non-interactive case
and other in a more interactive setting
just to give you a collection and a
flavor of the kinds of questions that
come up the kind of answers that are
coming and often will be trying to talk
about you know and how did this differ
from standard classical communication so
the first of these examples that I want
to talk about is when you are
communicating when you are not certain
of the priors of information but the
other people have in order to explain
this so this is a setting where the
sender and receiver new starter
with a common background they agreed how
they should be talking to each other but
then they went out into the world lots
learn lots of new things and they
thought they think that you learnt about
the same kinds of things as the other
person but you're never in perfect
synchronization this is a very very very
common phenomenon in natural
communication I mean when I stand up
here to give this talk I don't assume
that you know English in exactly the
same way as I do I don't expect you will
know the same kind of mathematics in
exactly the same way and yet I don't
want to start off with a course in
English and of course in mathematics
before I launch into my lecture so in
some senses I have to abstract the
knowledge that you have and fit all the
things that i want to say into a
20-minute talk so this is an example of
a communication with some uncertainty
what would I like to do I'd want to take
all the information that I want to
communicate compress it given an
approximate understanding of what you
and I can agree on so i have a few
slides which a few more sentences which
say the same thing but the important
thing over here is we are talking of a
game where we are compressing
information compression always is a
function of what is the probability with
which you think which with i think a
particular message is going to be said
ok and the precise mathematical way in
which we make sure that you and i don't
agree is that we don't assume that the
probability distributions are identical
we can allow them to be close but not
identical and you might ask so how does
this influence our classical solutions
and the classical compression schemes
those proposed by Shannon by Huffman etc
completely break down if the senders and
priors distribution is not exactly equal
to the receivers okay so this is a
pretty fragile model of compression this
is something that should be pretty
dangerous on the other hand when you
look at humans that are communicating
they do it pretty reasonably so in this
work
we try to address this question so let
me first sort of there's a little bit of
a mathematical model that I'll present
you can sort of to not for a couple of
minutes while this happens if you prefer
so there's a space of msgs n possible
messages that I want to communicate
there's a distribution p on this
messages this is the probability with
which p of i is the problem p of M
little m would be the probability with
which the message m is chosen to be
transmitted and this is known to me the
sender and I'm going to have to encode
this message m using knowledge of this
prior p this is what standard
compression is decoding and this is
where the uncertainty comes up the
decoder know some probability measure on
the same space but it is not the same
probability and now the question is well
given that these two are not identical
can you somehow manage to compress the
information so you minimize the expected
number of bits that i transmit to you
while at the same time the decoding
algorithm uses a different prior than
the encoding algorithm and yet you
expect the message to be true correctly
recovered so this is a very succinct
precise mathematical description of the
problem at hand this is the question
that we considered in the work with
Brendan Juba who is here Adam kalai
sitting somewhere in the back waving his
hands sanjeev khanna who's not with us
today which has visited to a lab during
these things and we showed that if the
sender and receiver have some amount of
randomness in common and this randomness
unfortunately has to be in perfect
agreement then we can actually get nice
decoding schemes I will not have time to
talk about the actual scheme and so on
but in case you do happen to go around
looking at slides of presentations the
next slight l's you roughly how this
happened later work with allaha ramati
who was a intern in our lab and frequent
visitor we were able to convert many of
these mechanisms to ones which did not
require the sender and receiver to have
perfect random strings in common led to
many new challenges and many significant
open questions for those
to you who are interested in
mathematical questions in this arena I
would strongly recommend reading this
particular paper gives you many nice
challenges which we had not been able to
resolve so here was the flavor of the
theorem but I won't get into it let me
just skip ahead to example number two
and this is a somewhat harder task to
consider and of course Brendan and I've
who were sort of stupid eight years back
took this as a first challenge we said
well you know uncertainty the main
challenge over here is that you may not
understand what I'm saying and we should
try to somehow figure out how to detect
and correct misunderstandings what I
want to say in this particular talk is
yes you know this is a very very
challenging question it's not impossible
and it can be abstracted mathematically
and it can be there is progress one can
make on this mathematically so the main
thing that we all understand is you know
in you know the point and me
communicating to you and you
communicating to me is not just to
exchange you know maximize the number of
bits we can exchange I mean that's you
know you can just toss random coins and
collect a lot of bits I mean the whole
point is probably to create some some
action for me the bits that i send you
lead to some action in your minds and
the bits that you send me back lead to
some action in my minds and we want to
make sure that we are taking the right
actions that were intended by the sender
and receiver so how can we ensure the
sender and receiver understand the
instructions and act accordingly ok so
this is sort of the big challenge over
here unfortunately if you're just not
following the instructions I am sending
you that is not a proof that you're not
understanding me ok that simply means
that it might not have I mean there are
two possibilities you may not understand
me or you might understand me and
realize that I'm giving you instructions
which are clearly detrimental to you so
they are incentives are very important
what are your objectives why are you
trying to do this so it some amount of
game theory has to be brought into the
play
over here to make sure that it is in
your interest to follow the instructions
that I am sending you and if we once
you've sort of hypothesized this it may
be possible for us to start testing and
correcting the misunderstanding of bits
so this was sort of the hope and belief
and we started and work with Juba with
Oded go rake in Juba and later we sort
of created a very simplified example of
this with Jacob national who is a
postdoc in our lap and let me just tell
you a little bit about the first pieces
of work over here in this work with gold
rack and Juma we started saying positing
the fact that communication ought to
have a goal and this goal should be
something that we should explicitly
formalize explain why the sender is in
this act of communicating with the
receiver explain why the receiver is in
the act of communicating with the sender
define what it means to be compatible
and try to explain what are the certain
settings in which they can overcome
misunderstanding and arrive at you know
reach success and the main thing that we
were able to formalize over here is the
concept that understanding is not a
measurable entity or a detectable entity
on its own however you have a very good
proxy for it functionally in saying that
you achieved your goal if communication
was essential to achieving the goal and
you are able to achieve the girl then
you must be understanding each of them
so this was the kind of a message that
we wanted to give unfortunately the
generality of the setting was a little
too complicated and so we tried to
simplify this a little bit in this work
with Jacob leshner where we decided to
say you know let's throw away the
incentives and the incompatibility
potential incompatibility by arranging a
very simple game in which players are
clearly interested in coordinating with
each other this is a coordination game
will have a series of games between two
players repeated games so every time we
play a game we get to choose one of two
binary actions if the two of us agree on
the action we get some reward if we
disagree
do not and we were a little bit more
careful with that in fact there were two
kinds of actions we would try to emulate
Bob the other party is Alice Alice's
playoff might be coming from anywhere
and we don't quite know and we are sort
of oblivious to it the only thing we
know is that one she is decided on an
action she is not going to suffer
because she coordinates with us so
because Bob tries to coordinate with it
Bob on the other hand clearly is going
to have payoff if he agrees with Alice
and this no payoff if he doesn't and
what Bob would like to do is saying well
if I don't know alice is strategy
perfectly can I somehow learn it so we
have to formalize the notion of what is
it that you what does it mean to say we
don't know what alice is alice's
strategy is and we do that by this cloud
once again alice is one of this large
set of possibilities this cloud may be
infinitely large but she is playing a
strategy from this set and that's all
that Bob knows this is the set of all
possible reasonable things that someone
else might be trying to do and I would
like to know how can I learn about it so
what we were able to show in this work
was that if Bob's try something tries to
figure out what which one of these
strategies Alice may be playing he can
use lack of coordination as a signal to
detect misunderstanding and eventually
find the right strategy which will allow
him to coordinate but unfortunately
during the same process Alice may be
trying to coordinate with him as well
and so under deterministic options the
two of them could be stuck in a rut of
not being able to coordinate with each
other but fortunately there's way out to
be a randomness and you can actually
manage to solve all of these problems
with randomness nicely so I'm out of
time so I'll just skip ahead to my
conclusions and say that look
communication when mixed with
computation is leading to a new class of
mathematical problems these are problems
which are interesting i believe and
relevant and we should pay more
attention to them and there are many
many such challenges appearing today and
so i think a good theoretical
understanding will give us very sound
foundations to work
within the future and lots of work to be
done okay thank you very much so let's
skip questions but you can talk with
Madhu during the break between sessions
this morning is Peter Winkler who's the
William moral professor of mathematics
computer science at Dartmouth he is a
frequent visitor at Microsoft Research
new england and we always look forward
to his visits so please welcome Peter
everyone who believes in telekinesis
please raise my hand okay good okay I'd
like to tell you about some exciting
work that microsoft research has been in
the middle of not just for the past five
years but really for the past 20 even 25
years and especially Jennifer chaise and
Christian Borges and I think this is
really exciting stuff and it's it's
gotten lately to an even more exciting
point so how does this go okay let's
start with a graph and two problems on
the graph okay first problem we would
like to find a path in the graph that
traverses every edge exactly once and
some of you knows are called an oil
Aryan tour and to the time it take to
solve this for out of nodes really weigh
less than a second very similar seeming
problem I want to have a tour around the
graph that hits every vertex exactly
once some type of cold Hamilton path
problem and how long does that take this
all for 100 nodes more than a million
years even by the best known randomized
algorithm ok this is this is kind of a
stunning dichotomy Annie this is this
kind of thing is something we see often
in the theory of computing where there's
a huge gap between problems that we can
solve efficiently and problems that we
haven't got such a method for and what
we're trying to do of course is to try
to figure what's going on here I mean
why is this problem so easy and this
problem so hard ok well let me tell you
about a little experiment that Dominic
will SH Oxford University performed many
years ago and something that got me
interested but similar experiments
around the world got other people
interest
he took three sets of nodes and created
a random graph by putting edges between
the parts and then he wanted to three
color of the resulting graph so three
coloring and graph means give every
vertex of the graph one of three colors
in such a way that no two adjacent
vertices have the same color so we
discovered that if you if you introduce
not too many edges and you do some naive
randomized algorithm to try to three
color it it's easy you also discovered
that if you introduce edges of high
density use the same algorithm it's
again easy you do it this way you covers
the unique color of that coloring that
you sort of set up in advance here but
if you introduce edges at it just
carefully chosen medium density the
problem is hard okay whoops compactor
the problem is hard so what's going on
here so now instead of oil arian tour
versus Hamilton circuit we have a
problem which seems to depend on a
parameter and when the parameter changes
the problem suddenly changes in this
case from easy too hard and then back to
easy again right and if we can
understand exactly what's going on with
this parameter changes maybe we can help
get some insight as to why some things
are easy and some things are hard in
computer science now this business of
thresholds where a parameter changes and
then all of a sudden the world changes
in some qualitative way well this is the
province of an area known as statistical
physics okay it's exactly what
statistical physicists do why'd you take
some water and you lower the temperature
and suddenly it turns into ice what's
going on here okay so maybe the
physicist achill physicists would have
some insight as to what's going on in
these
in these computational problems could we
find some statistical physicists who
could be induced to be interested in
computer science we chose Jennifer and
Christian for this task a few other
people came on board as well and it
started to get quite interesting let me
show you a statistical physics problem
put in simple combinatorial terms okay
take a checkerboard and put checkers on
it in such a way that no two of the
checkers are orthogonal adjacent and do
this any uniformly random voice so every
possible way of doing this has equal
probability this is one way of doing it
okay the red square is an odd squares
the blue squares or even squares of
course you have some checkers on even in
some checkers on odd okay and here what
I've done is I've just put in the colors
of the squares that the checkers happen
to fall on and there's some you see red
squares and you see blue squares there's
there's a tendency for these things to
cluster right because you can't have a
blue square and a red square that are
closer than a knight's move apart now
this actually is one corner of this
picture generated by me and peter shor
many years ago and although for some
reason my colors have flipped to cyan
and magenta maybe Microsoft can explain
that okay now now let's do the following
suppose we want more checkers on the
board we want a bigger independent set
in this graph we can do this in a in a
way which statistical physicists have
taught us about which is we just create
extra incentive for the set to be big by
rewarding it by rewarding it by giving
it higher probability of it as more
points and we increase the probability
by a factor of lambda whatever that is
for every additional point the set has
okay now if we take lambda up
all the way up to 3.7 87 this is what
our random set looks like now we have a
really serious tendency to cluster okay
take it up just a little bit more 3.79
to and all of a sudden one color takes
over okay and this is the threshold
something has really happened here
something kind of exciting it could have
been the other color that took over but
some color will take over the world
somewhere between three point seven
eight seven and 3.79 to this this
actually is a model known to visit this
as the hardcore model and and it's a
well-studied model it's so well studied
that you can perform this experiment you
can actually type the letter the words
hardcore model into your browser with
children around okay so what's going on
here we get we get what the physics is
called ordered phase we get long-range
correlation to get slow whatever some of
you know what these terms are very we
get lots of interesting properties occur
when we cross this threshold could it be
that one of these properties has
something to do with computability okay
could it be that there really is a
connection between the statistical
physicists and the computer scientists
well we tried a kinetic connection
emerged through rapid mixing of markov
chains okay again some of you know what
this is the connection looks something
like this when you have the disordered
regime this is this is where we started
with mixed up pictures with cyan and
magenta you get rapid mixing Markov
chains and you can use that to do
efficient sampling and you can use
efficient sampling to get polynomial
time approximation for important
problems in computer science so seeing
that there was definitely a there's a
real connection mr.
erection from a statistical physics
model to computability the other
direction you have the order of regime
then you get slow mixing and then so
what we don't really know slow mixes so
that means we can't solve the problem
that way but maybe we can solve the
problem some other way and and then
maybe we get hardness of approximation
but this was kind of a out in the open
so for a long time you know we kind of
had this belief that there's this big
connection between these two things but
no real verification here's a similar
picture corresponds to else's problem
sometimes for a decision problem you
have one regime where you have rapid
mixing and searching and you get the
answer yes in polynomial time for your
decision problem and another regime
where you get the answer know quite
quickly for your decision problem and
then there's a critical point in the
middle boom hard to decide okay but
again lots of us have been working on
these things for 20 years and most of
the time it just seems to work this way
we can't prove it okay so we need
rigorous connections okay here's a
little bit of the history here in the
Ising model which is the most famous
physical physics model of all time it
turns out that we actually can show a
trichotomy for mixing above at and below
threshold but we also know that we can
compute things at the other side of the
threshold for the Ising model this goes
back to German Sinclair so this seemed
to be put some doubt into our minds
about whether these things are really
connected maybe really just a one-way
street and we can't nail the difference
between computability and non
computability using statistical physics
let's take a quick look at a classical
problem 3 set the mother of all np-hard
problems trying to figure out whether we
can we can find a satisfying assignment
to a formula like this we can turn that
it something that looks like a
statistical physics model and it turns
out that the techniques used by
statistical physics there are in fact
very helpful in solving random three-set
okay now too sad which doesn't present
the computability dichotomy was studied
very successfully by a few familiar
names here and we understand what the
transition looks like they're the
transition for three set we don't we're
learning something every day we prove
there's a sharp threshold but we don't
know if it occurs exactly place where a
fixed there's a fixed ratio between
number of clauses and number of
variables lots of cool stuff going on
here's a situation there seems you have
random 3sat you have a fixed ratio
between number of clauses and number of
variables that that determines whether
it's going to be solvable below the
threshold solvable easily high above the
threshold you can prove it's not
solvable easily at the threshold it
seems to be around four and a quarter
it's hard we think okay and lots of work
on this stuff lots of results we're
closing in but it's not there yet okay
now yeah here's an example of graph
theory of we're studying this problem if
you can to see that when you're looking
for solutions for a decision problem
they begin getting rare and they begin
getting disconnected from one another so
we given one we can't find another
here's an example of where we know that
this happens in graph theory okay so
what's going on
let's look at a particular problem how
can we count the number of independent
sets in the graphs an independent set of
a graph is just a bunch of vertices no
two of which are adjacent that's
basically what we were looking at before
on the checkerboard and we know the
answer that it was that that of a
certain Markov chain mixes rapidly we
can do it ok well there's an old trick a
favorite of buying and lots of other
people here in the audience which is the
study of the problem on trees you know
it's like looking for your keys under
the lamp right and look my Landon
actually got lower in this picture it's
quite interesting this is ok yeah I said
we raise the land after the line ok now
it turns out that yeah counting sparse
independent sets of a tree seem to be
easy counting dessylyn seems to be heard
and it turns out that for this case the
threshold for lambda is known exactly
it's this expression the maximum degree
Delta minus 1 to the power delta minus 1
over Delta minus 2 to the Delta and
rather interestingly this marvelous
touring was discovered actually not by a
statistical physicist but by a computer
scientist studying models will
communication well mathematician Frank a
probable list Frank ok yes it does at
least come from so that's good enough
right and ok there's still a lot of
confusion between Microsoft Research
Cambridge of course in Microsoft
Research New England my solution to that
was to rename the other one Microsoft
Research Old England right I don't know
why they haven't taken me up on this but
there you have it this is ok Alan sly
has proved that the computational
threshold
for approximately counting independent
sets on a graph actually corresponds to
exactly the statistical threshold the
statistical physics threshold for this
model okay and this is a this is just
absolutely marvelous so we here we have
a complete connection between
computability and this threshold and
statistical physics and one of the
consequences is that we can efficiently
estimate the number of independent sets
and a graph of maximum degree five and
we cannot efficiently estimate the
number of independent sets in a graph of
a maximum degree six kind of interesting
I think this is marvelous so here's what
Alan has to say about it that this is
the first rigorous correspondence
between the harder to approximate
counting a sampling with statistical
physics phase transition and I agree
completely and I expect that this is the
beginning of a completion of a marvelous
program putting our statistical
physicists and computer scientists into
this wonderful boat together so thank
you very much for listening
well let me state it a little bit more
precisely so I can answer that question
what we're trying to say is that is that
in the whole class of graphs of a
maximum degree 6 we can't expect to find
an algorithm to do this efficiently
whereas in the whole class of grass of a
maximum degree 5 we have such an
algorithm okay that doesn't tell you
that there's a fixed graph which you can
add one more edge and you can no longer
do it well maybe you could do something
like that somehow yeah I mean maybe you
can get something like that and
Boulevard some wonderful results were
you know you you know exactly that the
the change from one thing to another
happens at the same time and change from
another thing happen and there might be
something like that here that would be
kind of neat
for that I don't think we know the
answer questions oh you know okay I take
it back it's possible that my computer
has gotten to know me so I
hey this is or maybe I just don't look
past the first first page the first page
really okay I stand corrected please pay
attention to that yes Nicole
no it doesn't fact yes so that the you
know these message yeah let me repeat
the question is this just a coincidence
a coincidence or does Alan's proof
actually show where the statistical
physics is connected to the
computability and I think I think the
latter is more the case okay so you put
some of you have heard of algorithms
which are based on something called
message passing maybe this goes back to
to Medusa talk maybe even a little bit
to Nadia stoic as well and these
algorithms based on message passing
turns out can be really studied and it
turns out that that that if you're as
smart as Alan has for example you can
you can you can figure out when they can
be done when they cannot be done and use
that to get to get the to get the
hardness resolve
I wouldn't even want to speculate I
think on that note we give all of you 15
minutes time to for a little break and
let sank all the speakers of the morning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>