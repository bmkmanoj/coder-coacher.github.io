<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microsoft Translator Hub - Discussion of BLEU Score | Coder Coacher - Coaching Coders</title><meta content="Microsoft Translator Hub - Discussion of BLEU Score - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microsoft Translator Hub - Discussion of BLEU Score</b></h2><h5 class="post__date">2014-09-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-UqDljMymMg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Oh
hello my name is Chris rent from
Microsoft research we have this product
called Microsoft translator with an
extension called Microsoft translator
hub and our users ask as many questions
about the right way to evaluate the
quality of machine translation and the
quality of their own customized machine
translation doctor will lose from the
Microsoft translator team is here with
us to shed some light right to the first
question what is the purpose of
evaluating the quality of automatic
translation and what are our users able
to do with us you know that's a that's a
very good question so to answer that
question I mean as you know automatic
translation is used in a variety of
scenarios for translating documents from
one language into another language and
the reason for translating documents
automatically could be that you want to
translate them for direct human human
consumption you might want to translate
them in a post editing scenario where
the output is actually then edited by a
human for subsequent consumption or
maybe to generate output that other
machines might use for data mining or
extraction of content or what have you
so is the basis of an evaluation always
a previously done human translation do I
always need to have that in order to
make an evaluation yes generally we want
to have a human translation to compare
against so automatic metrics typically
use as a reference human translation and
you compare the quality of the output
from the machine translation engine
against that human reference and we can
measure factors such as just a you know
some very generic difference between the
reference and the machine translation
output or we can look at other factors
such as linguistic quality word order
and other things that actually
we can measure but since the point of
machine translation is to generate
output that is understandable and
readable by humans we use as our
reference point human translations well
can you walk us through the different
methods of automatically scoring the
quality of translation I understand
there is automatic scoring as well as
other of methods let's look at the
automatic scoring method so can you give
us an overview of that yeah so there are
multiple methods for automatic scoring
the basic notion of automatic scoring is
to have a reference that you actually
compare against and then you generate
what we call a score so there's a
there's an actual number assigned to the
difference between the machine
translation output and the human
translation reference and there are
multiple methods that are actually used
for automatic evaluation the point of
automatic evaluation is that it's fast
and cheap if if I want to look at
multiple languages say in comparison are
these automatic scoring methods giving
me any indication that allows me to
compare between languages so it is a
score comparable between language a and
language be the automatic scoring method
that's typically used or there and that
we use is blue spelled BL EU and the
blue score is basically an automatic
method that looks at the machine
translation output and human reference
it's what about comparing the quality of
a rule-based machine translation engine
and a statistical machine translation
engine does the blue score or any
automatic score help me there the kind
of fundamental comparison point for blue
is the word and so and it's kind of a
statistical it's biased toward
statistical engine so phrase based
engines tend to fare better and blue
then than rule-based engines so if you
have a score of a certain number on a
rule-based engine and comparing it
against a phrase based engine isn't
exactly fair because the rule-based
engine will be penalized which scoring
method do you use
at Microsoft points to improve Bing
Translator the method that we use in
Bing Translator is actually a blue score
as I mentioned earlier spelled BL EU and
in blue score is a measure of the
difference between the human translation
and the machine translation output and
it looks at the presence or absence of
particular words as well as their
ordering and the degree of distortion
how much they actually are separated in
the in the output the blue score is
typically measured on a zero to one
scale so it's a it's a effectively a
probability that's assigned to that
output and we oftentimes I'll multiply
that number by a hundred and give it as
a percent so we'll talk about a blue
score say thirty five or blue score of
42 and that more or less means that
we've taken the the actual output and
multiplied it by a hundred so a hundred
would mean a perfect match yes that's
zero would mean absolutely so the blue
score measures the difference between
the input and output if there's no words
in common between the human and the
Machine translated output and the human
translated reference then it would be
assigned a score of zero if they're
exactly the same including the order of
the translation then it gets a score of
1 or 100 can you explain the difference
between ultimate differences between
automatic evaluation and and human
scoring yes absolutely
so a Blue has been shown to be highly
correlated with human evaluations and
that's one of the reasons we're able to
use it because we can use it as a
surrogate for an actual human evaluation
but some of the problems with blue is
it's very sensitive to the words that
are present in the input or excuse me in
the output of the machine translation
engine so if we break the words
differently
using what's called a word breaker from
you know a machine translation output it
could actually give us a different blue
score word breaking sounds like it would
be something that's fairly trivial and
actually it isn't and it's especially
not trivial with languages such as
Chinese or Thai where the notion of word
is flexible or where the notion of word
is not completely clear blue it's
important that you basically take into
account what word breakers being used
and between the comparisons that you're
doing I have the different machine
translation vendors quote different low
scores to me what else do I need to
watch out for when when somebody quotes
a blue score to me I mean it gets back
at first of all since we talked about
word breakers already the word breaker
obviously has an impact on the quality
blue score that's actually quoted so
that needs to be taken into account
typically if you have multiple
references the blue score tends to be
higher so if you hear a very large blue
score someone gives you a value that
seems very high you can ask them if
there are multiple references that are
being used because then that is the
reason that the score is actually higher
so what is the test that's that's being
used so a blue score by itself really is
not as mean is not meaningful in
isolation you need to really have to
know what what's being tested as well
what is the test set so if you're
comparing blue scores between two divs
different systems and there are two
different test sets that are being used
then they're not they're not called
comparable scores you need to be
measuring against a standard test set or
be using the same test set in both
scenarios
probably makes sense to use my own test
yet that comes out of my own application
scenario absolutely if you have your own
test set and have the ability to measure
it against another engine for instance
that is going to be the most meaningful
result especially if that test set is
representative of the translations that
you want to produce from your particular
machine translation engine in Microsoft
translator hub I am getting a blue score
after every training that that I'm doing
what is it telling me so the blue score
that you receive after training gives
you
a sense of the quality of that
particular machine translation he's
built so each training will produce a
different blue score so the blue score
then gives you a sense of the
improvement how you're actually how the
engine is actually improving and gives
you a sense of what of the direction
that you actually want to go in so
higher blue scores are better I always
see two numbers oh I see the I see the
blue score and a a a negative or a
positive number behind that what does
that tell me
okay so the blue score in the hub
basically is measured not only against
your engine it's also measuring against
our generic purpose engine basically one
that's not trained on your particular
data so the difference you you see there
is a difference between your engine and
the generic engine it also you can
compare different iterations of your
engine as well over time so can you can
compare your engine now against the
training that you did let's say a week
ago we talked about the the test set the
blue score being relative to the test
set when I design my test set what do I
need to pay attention to so that the hub
allows me to to either have the hub
automatically select a test set or give
me a chance to define my own right let's
say I define my own what would how would
I do that and what's the purpose of that
okay
the hub automatically as you meant as
you indicated automatically you can
actually tell it to automatically
generate your own test set by doing that
it basically samples sentences from your
training data to produce a test set that
you can contain you can continue to use
now one of the things it does when it
actually samples that test set is to
ensure that that the test data that's in
that test set does not overlap with the
data that's in the training data and
that's actually very important so if
you're producing your own test set you
need to ensure that the test data in
your test set do not overlap with the
training data that your training on that
actually invalidates the results if they
actually do overlap you also want to
have a test set that's representative of
the
kinds of translations that you want to
do so what what is it that you want to
output what is the kind of documents
that you're going to be passing through
your engine you want your test set to be
as representative as possible of the
kind of content that you're actually
going to be translating using your
engine that's good it's an it is an
interesting aspect I need to make a good
guess of what I'm going to translate and
in the future and choose what looks
right well you want to measure the
quality improvements or that the quality
on content that matters to you not some
other content that doesn't isn't
relevant yeah well in addition to the
testator the hub also asked me to define
tuning data and I can either have that
selector automatically or I can submit
my own tuning set
what is this tuning data are used for
that's a very good question
tuning data is is basically another kind
of training data it's used to set the
large number of parameters that are used
by the machine translation to generate
its output so it's another kind of
training data
now typically tuning data is drawn from
the same set of material or the same
content as your test data is you want
them to kind of match because basically
the tuning data is used to set the
parameters to make the machine
translation engine do better on data
that looks like the tuning data so if
you're tested is like you're tuning data
that you're going to do better on your
test data because of that it's also
important that you're tuning data not
overlap with your training data so that
the content is distinct between the two
and that you're tuning data does not
overlap with your test data either it
can be the same kind of content but it
can't be the exact same content that
makes sense but why can't I just used a
tuning data to also give me the give me
the score because tuning data is
training data you don't want to use the
tuning data as your test data so you
need to have an independent source of
testing data yeah so you get you get
really useful scores only if you keep it
clean so just that's that you don't
contaminate your your test with what you
use previously
in building 2 engine I mean the the
engine when it's actually training does
use does calculate blue scores for
instance against that tuning data that's
part of the mechanism that it uses to
basically maximize the value on that
time that tuning data the market of
translator hub gives me the option to
train on a particular project repeatedly
presumably with with the intent of me
changing something in between so what
are the things I can change what are the
things I should change in order to
optimize the system for my for my
purpose you often will make changes
between each training you'll make
changes to the data so what you want to
do is to have the best selection of data
to give you the the best blue score the
best output for your tests I'm adding in
documents or removing documents maybe
that aren't helping and then retraining
can help you to basically improve your
blue score over time it might be that
you've decided that you want to increase
the size of your tuning set or you have
some additional content you want to add
to that tuning set what you typically
don't want to do is change your test
data because if you change your test
data then you invalidate the previous
results you're not going to be able to
directly compare the results from your
current test set against an older test
set now you may want to change the test
set at some point but you you don't want
to do that with each training you want
to keep the test set constant and then
change the other parameters that you can
change which are your training data and
you're tuning data good so I did that a
couple of times on my english-french
project I got a score of thirty two
point six what does it mean thirty two
point six is that good or bad I can't
say whether thirty two point six is a
good blue score or a bad blue score it
kind of depends on the scenario and it
depends on comparisons against what you
trained before we talked about the
automatic evaluation now but you also
mentioned the option of human evaluation
what is that good for the ultimate test
of machine translation is engine is
human eval we basically want the the
want to have a sense of how good that
engine is or how the engine is perceived
by humans how good the output is for
human consumption
now at Microsoft we basically use a one
to four scale for measuring the quality
of the machine translation output using
human evaluators one means that it's not
good at all it's a really bad
translation 4 means it's perfect means
it's really good and we typically have a
set of 4 evaluators and we we have them
evaluate the output they and then they
each assign scores to the output for a
set of sentences within a test set and
then we average across those sets of
evaluations so well now we talked about
the the absolute scoring that gives me
just one one one score on the test set
what else can one do or what else do you
do with the human evaluators yeah well
often we actually don't just look at
absolute scores on a particular engine
we'll look at differences so relative
scores between like two iterations of
the same engine or comparative scores
from this engine to say some competitors
engine and that actually is really
useful will actually have the human
evaluators look at the the output from
two different engines and then assign
ascorbate a relative to each of them so
you'll get a one to four number of some
type for each engine and that actually
proves to be quite useful in doing a
vowels in these kinds of scenarios yeah
so that the judge then looks at a human
reference and the output from engine one
and it's absolutely and that's
absolutely true we can also if we have
bilingual evaluate bilingual evaluators
we could have them actually look at the
source two and then make a judgment call
as to what the quality of the
translation output is and then we're
asking them to assign scores based on
their judgments about the quality so
it's not necessary to hire experts in
this field to basically do the human
event one way so in a way you don't want
them to be experts you want them to be
regular users or potential users of your
service thank you well that makes
perfect sense to me and it answered
answered my questions that I had for you
today and I hope it answered to your
questions as well thank you very much
for watching this coming
thank you each year Microsoft Research
helps hundreds of influential speakers
from around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>