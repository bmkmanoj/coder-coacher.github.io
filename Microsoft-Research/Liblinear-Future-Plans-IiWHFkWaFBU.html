<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Liblinear Future Plans | Coder Coacher - Coaching Coders</title><meta content="Liblinear Future Plans - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Liblinear Future Plans</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IiWHFkWaFBU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so so now it's time to talk about a
future plan well i don't think i have a
very good plan yeah so one reason is led
so you know pastry several years uh I
think we have put a good enough models
for the linear classification on one
machine so in other words i think if you
consider like algorithms on one computer
then linear classification in some sense
is a soft problem that because now you
can efficiently chan large data on a
single computer yeah so so one one
direction we are thinking about is a
more automatic setting for machine
learning so starting with least linear
classification so so in my morning talk
so in I have a discussion session I
mentioned a few issues such as a
parameter selection and the data scaling
so all those things are related to to an
automatic procedure for users so we are
hoping that so to have a automatic or
semi-automatic procedure so users lay so
to help users to do things like
parameter selection and to do data
coding yeah so this kind of automatic
machine learning i believe is a good
direction by even though the hosting may
be beyond the scope of the current live
linear project okay but the machine
learning me automatic machine learning
means the the procedure okay the
procedure running a classifier but
beyond the lad is for the whole machine
learning process okays from your
application to the final results how can
we do a more automatic process that's
even more difficult another important
direction I am trying to work on is the
distributed a linear classification so
so currently we have a distributed
version of lib linear it's implemented
in MPI as well as spark so what are
challenges for this direction first it's
not very clear like how many for how
many applications you need to do a
distributed training because you can
always subset a set and then doula
training by using one machine yeah but
of course doing so may interrupt the
whole pipeline of your big data
application yeah so it's not very clear
to me yeah like for how many percent of
applications of big data applications
that you really need to do training in a
distributed environment yeah by the same
token if you have more examples
available you can always generate more
features so that you do need more retire
to actually Oh having more parameters
usually building yeah yeah yeah yeah
well that's the standard the argument of
big data right right you using a limited
amount of data and using more features
then you are supposed to get better
results but that may not be always the
case and another question that a lot of
people are asking is now for a given
application then can we know how many
data or how many features do we need in
order to get a satisfactory result that
is not very clear yet actually this part
of this automatic machine learning is
like so you have different
dimensionality like training time the
feature size but data size yeah can you
decide like
for how many Alan how many other nerve
for you in this is none not that easy by
talking about distributed is saying so
we have seen a bunch of our successful
applications such as a computational
advertising the click-through rate
prediction is a very successful example
that people do distributed linear
classification yeah but but we it's not
clear how many applications are like lat
yeah okay so this is certainly one
challenge but anyway so therefore we we
decided to release that distributed
extension of lib linear then probably
from our users we can learn more like
for all kind of applications we need to
do distributed trending so one effort we
are working on is to add this
distributed lip linear to sparks a
machine running library called ml leap
yeah so we are working with lows
sparkpeople to to have this included
English library so from more users and
we can see what the direction should be
yeah so those are problem basically what
I I have yeah so any comments
yes I mean you said that big data is
always going to improve the classifier
but it some there's me some problems
where you know I'm gonna give me more
accurate and estimating my betas by
having a billion data points but
classifications not going to get any
better right i mean because i just don't
have enough features in this problem too
exactly classify anything there's just
an upper limit of classification oh you
guys are generating more features that's
okay you're somebody made a telescope
you might not have the ability to add
more you know pixels to your camera yeah
but but if say you're a number of
features is about the same but now you
have more and more data then it is true
that probably your classifier will hit
the limit yeah so that's why I mentioned
a question can we tell users like how
many data do you need that's actually
important yeah billon even though users
just always think that they need as many
data as possible then they need to buy
storage and a lot of things those can be
expensive yeah what do you use as a
basis for for computing or for
distributing oh yeah my schedule or some
some interface to some scheduling system
work I think maybe all the different
projects have the same issues here
we need to interface with oh yeah yeah
well well okay but your eyes king of the
equation is about how to learn the
technical side of distributing the
classification well but today we don't
we don't have time to talk about lat but
I do have a whole set of slice actually
early this week in at a microsoft in
redmond i gave a talk about technical
details of our distributed romania so
insured we ok so so so now now people
talk about a distributed classification
then you have to waste to to develop
your algorithms one is that you try to
parallel lies existing algorithms and
one machine okay and another is that
well because of this distributed said
you try to design something completely
new and of course there is also
something in between and therefore for
my example here I basically paralyzed
the Newton method in bulimia but for
Newton method in live linear it involves
a sequence of a matrix vector product so
basically I do payroll a matrix vector
product oh yeah okay okay so now we we
know we want to do distributed matrix
vector product then of course you want
to do in memory stuff rather than
MapReduce Sing prison for may be reduced
then every iteration you need to do
heavy disk i/o so but in in one Newton
iteration 1 Newton method that you need
to do many matrix vector not many but
quite a few matrix vector products then
you cannot afford a lot all right so we
in our implementation we have mpi we
also have spark so labels to in-memory
communication so that's faster I think
yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>