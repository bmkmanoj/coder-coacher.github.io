<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Invited Talks: Computational Principles for Deep Neuronal Architectures | Coder Coacher - Coaching Coders</title><meta content="Invited Talks: Computational Principles for Deep Neuronal Architectures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Invited Talks: Computational Principles for Deep Neuronal Architectures</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BkVyE7SgxiA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
so I want to welcome you to invited
lecture by haim sample in ski hime is a
computational neuroscientist for all
seasons you've probably heard about the
brain initiative this was announced from
the White House on April second 2013 so
it's been going for more than two years
the NIH wanted to set priorities for the
next ten years and for a five billion
dollar project so you know these grand
challenges come along once every two
decades or so the last one was a the
Human Genome Project which has
completely transformed biology and the
brain initiative we think will have a
similar impact on systems neuroscience
understanding the very complex patterns
of activity and brains that give rise to
complex behaviors so I served on the NIH
committee that wrote that report for
priorities you can't fund everybody for
everything in fact NIH already is
spending five billion a year on human
disorders like schizophrenia depression
bipolar autism spectrum disease these
are extremely complex and devastating
illnesses and just simply throwing pills
at them and chemicals it hasn't really
been very productive there aren't really
any new drugs that have been introduced
which have any of real curative power
that basically band-aids so the brain
initiative is really an attempt to bring
engineers and physicists and
mathematicians into neuroscience because
we desperately need the those tools to
be able to help us make better
measurements to be able to to be able to
record for more neurons to be able to
link them to behavior and in what was I
was very pleased that admission to all
the
the neuroscientific areas the brain
theory was considered one of the most
important areas and that includes
statistics computation modeling and
theory and and haim has contributed to
all for those areas interestingly we
also identified machine learning as
being incredibly important for being
able to analyze cell identity from
genomic sequences for transcription
factors for being able to connect omics
and for being able to interpret decode
large patterns of activity in
populations of neurons Haim was one of
the early members of the group of
physicists who were attracted to
neuroscience in the early days of
hopfield net works his background is in
condensed matter theory he was trained
at bar-ilan University near Tel Aviv and
and was very very hadn't had the tools
he had the chops now with not only that
but after the night he was here at nips
in the early 90s of his one of his
papers is in our collection our archive
and the ax dinner last night we had a
wonderful discussion discovered that
quite a few of the speakers that we have
with us today were actually present or
had come to early nips meetings when
they were in Denver so this is in a
sense a second coming from for time and
several of the others including rob the
gave this wonderful talk this morning
tips are running high has won the
Schwarz prize for computational and
theoretical neuroscience he's really one
of the Giants in the field and I don't
know of anybody smarter than he is in
our business hi
I would like to thank the organizers for
inviting me and to thank Terry for his
kind introduction when the organizers
asked me for a title for my talk as it
was many months ago then I said well
it's too early I have to I need more
time to think about what I want to talk
about here and then Terry wrote to me
said don't worry do like what I do when
I have this problem I just give the
title the computational brain and it
always works so this is an alternate
title the computational brain but that
reminds me to tell you about a little
bit about my perspective of nips
actually according to google my first
contribution to nips is 1993 paper with
iris Ginsburg on correlation function
for stochastic or large stochastic
neural networks actually it was a nice
paper and I since then I had been a
regular contributor to this conference
however I and I think many in the
computational science community felt
over the years that this conference grew
out of its nips roots and and became
Apes diverging somehow from the neural
of nibs so I think this is an
opportunity to encourage this community
to forge back strong relations and
stronger relations to computational
neuroscience so let me go through some
of the biological motivation of the
things that I'm going to discuss with
you here today a transformations of
sense of representations its ubiquitous
feature of sensory systems in the brain
that one representation one set of data
is transformed in the next stage into
another representation one of the early
system that have been studied is the
cerebellum where the inputs
the most fiber inputs they project into
the granule cell layer and then back
converge onto a single purkinje cell as
they as a computational unit and perhaps
the first computation no sign theories
pioneering work by David Mann followed
by Jim elbows and others already he's
not know yeah you can see it on the
right drew may be the first diagram
abstract diagram of a neural network of
a real biological structure and they
realize that there is enormous expansion
from the mossy fiber layer about a
thousand of them in this and in one unit
expanding in by a factor of 100 to 200
to 200,000 granule cells which converge
back to an output unit which is the
purkinje cell a similar similar pattern
you see in hippocampus well internal
cortex expand into into granule cell
layer in hippocampus and then converge
into the c3 and then it's another system
the in sexual function and similarly in
the manual function you see a projection
of sensory sensory information into
cortex or cortical like structures and
then converging to various output units
so this and different systems motivate
several several questions first of all
what is the what is the computational
logic of expanding some number of
neurons in projection layer over the
number of rumors an input layer is the
expansion ratio and typical numbers is
between ten and hundred another feature
is the sparseness or sparsity i denote
by f which is the number of neurons that
are active for any given relevant
stimulus in a layer so the fraction of
neurons
active is between point one and point 01
the sparseness level of the
representation and often a dense sensory
representation is projected into a
sparse and expanded the presentation and
then other questions have to do with the
statistical nature of the synaptic
weights that mediate this projection in
some brain areas the statistics appealed
to be random motivating the question
whether indeed random projections are
computationally reasonable ways of
transforming sensory information okay so
this is about 11 layer 1 transformation
but in many sensory systems we know that
the hierarch is the visual hierarchy is
very famous one the van essen diagram
but others like the auditory pathway are
also consist or consist of several
stages and information propagate from
one state to another computational
science largely focused in last 20 years
about understanding local circuits in
many system including cortical circuits
and in recent review paper one opinion
paper in canto p neurobiology you can
read my my idea that it kind has come
for computational sign to devote more
intellectual resources into
understanding the computational
principles underlying larger structures
like hierarchical sensory structures and
end and others so this is the motivation
behind what I'm going to tell you today
so the outline of my talk is going to be
first of all discussing sparseness and
expansion from one layer to a next
intermediate layer then expanding the
study too deep architectures multi-layer
sensory systems then I'm going to talk
about two elements additional elements
one is in addition to propagating of
signals from bottom-up sensory signals
across deep architectures how one
integrate top down or protection inputs
into it and file and finally I'll
discuss a recent work on perceptual
manifolds and how it relates to all
these questions now what I'm going to
talk to you about today both in terms of
the themes and in terms of the
techniques theoretical and computational
techniques are not nua actually roots
are in the old 80's days of
computational science and the list is
very long the issues of sparseness and
different representations most of them
are in the context of learning and
associative memory but the angle that
i'm going to give today is focusing on
sensory processing rather than on the
learning aspect on the memory aspect
okay so let me start with facets and
expansion this is work which has been
already published so you can see the
details for yourself but let me give you
the highlight of the work so we are
discussing now a very simple system
there is an input layer call it stimulus
or sensory layer and with NS neurons
projecting into an expanded layer which
usually call it cortical layer
projection they're typically has more
neurons so it's expanded and sponsored
presentation and then finally converging
into for a simplicity one readout unit
so how do you analyze the computational
logic or the principles underlying
distance formation now we have to make
assumptions and here is the hypothesis
first of all the philosophy is that
information is of course in general is
not created by transforming into another
layer and in expansion it's also not
lost usually so the question is what is
there the computation advantage of
reformatting the same data and one
apophysis is that reformatting the same
data is done in order to allow for
easier readout of information for
downstream systems so specifically I'm
going to assume that the problem at hand
is that sensory perceptual objects are
how to discriminate you to process or to
make decisions upon because of the
enormous variability of the underlying
physical features which I'll call noise
so changes in representations allow for
simple readout and combating this aspect
of this variance on noise in in order to
be able to to a decode or classify
recognized object of perception and what
I mean by simple readout i'll take the
working apophysis will be single layer
Network perceptron linear plot threshold
which is a very plausible biologically
and that will be the working model to
implement this hypothesis and there's a
long list of authors that elaborated on
these hypotheses you can read some of it
in the literature so how do we how are
we going to test these hypotheses and we
have to make assumptions and in the in
the tradition of theoretical physics and
its contribution to computational
science we need to abstract the problem
we need to construct a minimal model
which is the minimal model which is
simple but interesting enough so that
these features of propagating of signals
can be studied by analytical and by
numerical simulations in a systematic
way so here are the assumptions that I'm
going to to make I'm going to assume
that in state space of the neural of the
neural state space object are presented
by Gaussian mixture so by clusters they
have sent each each object at the center
and has a has a cloud of points which
are associated or to be associated with
the same object however because of the
violence in various features they form
these clouds so this is the way the
input is represented the output is
binary classification with random
labeling of those clouds of those
clusters so you concede in the down
down in the bottom in the bottom diagram
we are going to assume that the output
has to make a linear classification of
those of those of those inputs okay so
already now there are many parameters
there are number of clusters speed the
size of the cluster as it shows in the
diagram Delta s the level of noise
normalized between 0 and 1 the size of
the stimulus layer and as decided
projection layer and see and this passes
in cortical area f but nevertheless
these systems are large can be hundreds
and thousands or millions and the
characterization is only by several five
to ten parameters so that that's good
but there is a there is a fundamental
question and this is how to actually
design or choose or learn the various
weights in this system so the read-out
weights w is actually easy because we
want to apply and win you a readout we
use some supervised learning perceptron
heavy on pseudo inverse support vector
machine you choose your favorite linear
classifier learning algorithm for them
but how to choose the projection wait
what is the reasonable strategy to
choose J the projection ways and Jay
does not know about the particular
classification that the system has to
perform downstream so it's more like
unsupervised synaptic wait so as I said
before the simple assumption is that
this is simply random projections soda
noting the first layer by guess the
second day by seeing you just choose
random matrix between them J Gaussian
for instance you then each neuron in the
projection they and the cortical layer
linearly sounds the activity of the pair
of the first layer then you enforce the
sparseness by some threshold and you
generate and a non linear expansion into
a sparse representation simply by random
projections is this a good strategy or
not now it turns out that the study then
analysis of this system showed that it's
actually
a bad strategy and what you see here is
the readout error as a function of the
sparseness f of this of the cortical
layer and the different different curves
correspond to different levels of noise
so 40 noise actually the error is 0 the
expansion is strong enough to actually
be able to implement those binary
classifications with no error that's
fine but as long as you inject noise
into the sensory a layer the error
becomes poor and know that it doesn't
really improve by making the
representation sparse enough actually
there is an optimal sparseness in this
case and what is the reason for that and
the bottom diagram shows the actual
reason for that if you look at the size
of the cloud as in the projected layer
you find that actually it's expanded by
this transformation so random projection
actually amplifies noise and you can see
here in the y-axis the the size of the
clusters in the protocol layer where
should the side of the clusters in there
is an input layer and you see that there
is a super threshold so to speak
expansion of the noise of the side of
the cluster so that doesn't seem to be a
good strategy that shouldn't surprise us
compressed sensing theory tells us that
random projections in some sense may be
a very good strategy to compress signals
but you were not compressing signal
actually expanding it another
interesting theoretical result coming
out of the analysis of this simple
network is that actually there is a
limit to how much you gain by expanding
the projection there and you can see in
this graph that that this that the
reduction in error actually saturate and
doesn't keep improving as the size
increases that's that's a remarkable and
surprising is that this is nothing to do
with the issue of overfitting because 3
is not a learning problem and I'm
assuming we have all the data
that that we need but it's more an issue
of representation and actually the
reason underlying it is that by
projecting signals from the low
dimensional to the high dimensional even
with random projections those signals
become correlated so I do not buy I
won't define it to you they know the
correlation but pure just say that in
the input layer I'm assuming that the
center's are random so this would be Q
equal to 0 uncorrelated but as soon as
you project them to the coating layer
those signals are correlated and and
generate nonzero pew which then can be
shown to underlie the fact that even
though you keep expand even if you keep
expanding the projection there you don't
actually improve the performance of the
system so there are two important issues
one issue is that the amplification of
noise or the suppression of noise and
another issue is the generation of
correlations so what are the
alternatives alternatives is to get rid
of render idea of random projection and
look for some some other alternative
ideas and I'm going to suggest the heavy
on synapse as an alternative now we
usually think about having synapse in
the in the context of recurrent networks
and then it's naturally the hebden
synapse are encoding patrons attractors
that has some cognitive will define
cognitive cognitive interpretation but
here it's different because those
intermediate representation that don't
have an obvious a cognitive represent a
interpretation so what we suggest here
is that are to stay there is the
allocation state and associations thing
in the allocation state we simply choose
pure random spots representations to
represent or to be the core of the new
representation of the sensory signals
and then we use hebbian rules to
associate between the original sensory
representations and and the newly chosen
or allocated smarts of presentations so
that's a very simple rule and the
performance is enormously better so you
can see here the error is
is especially for highly sparse regime
the arrow is is almost zero compared to
the random case and you can again Tracy
to to the enormous suppression or
exponential suppression of noise when
the signal propagates from the from the
center layer to the to the cortical
layer here at the bottom and analysis
shows that that this heavy on
suppression of noise in this expanded
sparseness is works well in the regime
in the high space regime defined by F
times the the load on the input layer
the number of clusters divided by the
number of neurons in the input layer so
as long as this as this product the
product of these two turned load time
sparseness is small enough and then you
have a very good performance and
suppression of noise in this in this
pond says ok so what what if we are not
in this regime what if the size of the
cortical area is not so loud that we
cannot afford such a sparse
representation or the load is high so
here we go too deep architectures in the
recent work with a battered body and
your Franklin ajantan cadman the last
two are students at the hip University
and here the structure does the strategy
is going to be the following we're going
to be to iterative processes one
iterative process is how we construct
layers as I'll show you you just have
iterative process going from one layer
to building the another layer and then
there is an iterative dynamics one we
constructed the deep network the
publication of the single themselves
define dynamics which can study by
statistical mechanics or dynamical
systems concept so first of all can
redirect the random weight hypothesis by
going into deep network so very simply
assume that you have many stages of
random weights followed by threshold
non-linearity and then sparsity well the
answer is no
if original error happens to be going
down for the first layer it will
eventually go up to chance level and the
noise will go up to the maximum level of
one and you can actually analyze why
this is so by again using the iterative
map tools from dynamical system by
thinking about how the noise propagate
from one layer to another in this large
or long deep networks and what happens
is that the two fixed points 10 fixed
point with a zero noise fixed point one
maximum noise one and it so happened
that the zero fixed point is unstable
all the way so no matter where you start
the system eventually the system will go
to a fixed point at the deep layers with
with a maximum spread of noise and
complete destruction of the of this
clustering in the in the sensory input
so that's that's a bad strategy okay
what about destruction heavy on a heavy
n wait so what we do here is simply
iterate the process at each stage we
take the previous representations of the
clusters we generate random candidates
in the next layer and associate them by
heavy on weight and this works
remarkably well it was a matter blue
well even in the high load and not so
sparse regime where the error even when
you go down to to the first layer is
really not impressive go down from point
32.2 but as you further propagated
system the noise become actually
crunched out until you get actually zero
error and that has to become you can be
compared to an infinitely shallow or an
infinitely wide single layer and as I
said before in this case the arrow
doesn't really go down to zero even for
infinitely wide and let us also the
actual sequence of expected of
nonlinearities and associations with the
headline associations is
crucial for the success of the system it
actually interesting to to explore the
the propagation of the signals this is
high dimensional signals the networks
are large many layers but you can
actually understand what's going on by
reducing this the problem into the
dynamics in two dimensional phase space
one is the level of noise Delta in the
x-axis and the other one is the level of
correlations that are at each stage each
layer q and as you can see here the
signals the initial values all initial
conditions are all start with the y axis
be at zero because the inputs are always
random centers so the uncorrelated
however they each line start from a
different level of noise and you can see
for this particular values that it's not
so sparse and high load then actually as
you go from diff is input layer sorry
the input layer to the next day actually
there is expansion of the noise in these
parameters so the performance get worse
however after one more layer the system
dynamics go down both correlations and
noise converge to 0 which is a almost a
perfect representation or invariant
representation of the clusters and in
the end here in this regime when you
start from zero correlation and high
values of noise you actually converge to
a fixed point at one both noise and
correlations are the maximal confusing
values and in between there is a regime
of initial values will actually converge
to a line of intermediate fixed points
which are shown here this ends of those
trajectories so that's kind of general
structure there is an enormous range of
initial values that even though for a
one stage is actually not enough to do
the job by iterating a few more steps
the network is actually converge to 0 to
very small values of noise and
from other remarkably well okay let me
now switch to a top-down question okay
so it's another work with the idea
Franklin ajantan cut bone it's related
to other rocks in the literature
addressing the problem of how in how to
incorporate button-up propagation of
sensory signals with top down
information top down information can be
attention can be curious about the task
or can be information top down
information about from long-term memory
from other sources about the category of
concept and so on so forth okay so
here's the way are we incorporated into
our framework so we have this object
representations as before but now
imagine that they are organized into
categories and we are not going to
assume any physical feature similarity
between members or tokens of the same
category we're going to assume that it's
random labeling combining several
objects into one category that to
simplify the analysis and to focus on
the questions how to now integrate
information from category to help us
process the sensory signal coming
sensory signals so that's very simple
question imagine that I am given
information that the current signal is
part is a member of a particular
category so this seems that it should be
easy to suppress all other possibilities
and to remain with only the members of
that category makes making the problem
of processing the sensory information
and enormously simple so why this is a
challenging and I believe this is a
challenger has not met enough attention
from our community but the reason is
that most problem most models of gating
and attention and contextual integration
are doing dealing with low-level
representations so if you want to attend
to a particular part of the visual fee
what you will do you suppress neurons
that represent that have receptive
fields and other parts so basically the
suppression of the Selective
amplification walks on the physical
locations or of obtained of neurons part
of neurons in the systems are suppressed
and others are enhanced but the problem
is when you talk about more complex
representations like objects and concept
we are talking about distributed
representation where the same groups of
group of neurons are representing all
objects including the one that we want
to suppress and the one that we want to
select so how do we amplify or suppress
selectively not in physical space parts
of the networks but in state space we
want to suppress some of the states of
the network or selectively amplifying
others that turns out not to be simple
there's the recent work published in
Nature Bible Newsome and and colleagues
where they show a model of a recurrent
network that perform such suppression or
gating between two attractors but that's
still far cry from building a model that
is computationally feasible that can do
it in numbers that are interesting
computationally so we are going to to
propose again within our feed forward
Network the following model imagine that
we have a noisy sensory signal coming
into the system but it's the noisy level
of noise is such that by itself it won't
be able to be recognized properly
downstream but now we have some cues
about category and so on that invokes a
category presentation which then is
going to be integrated in intermediate
layer and the key is that we have to now
in our model generate or create an
intermediate layer on mixed
representation where both the sensory
inputs and the category inputs are in a
nonlinear highly nonlinear way are
integrated into a mixed representation
actually in in the quadratic or
multiplicative representation then we
can actually propagate this further
downstream so we have our multiple
multiplicative representation mixed
representation from that we create a
cleaned of sparse representation of the
input and then we keep iterating this
again we put the category input creating
in New multiplicative representation and
again a new sense of representation and
so on it turns out that it's essential
to actually integrate top-down
information within our model into each
stage of this deep network it's not
enough to do it neither at the beginning
or the end in order to be effective and
once we do it the results are really
remarkable so this is an example of a
readout error as a function of the
layers at these stages that i described
now when you have let's say nine hundred
objects' organized into three categories
but in the regime where without
information from top down basically the
read-out will be a chance level this is
what you see here and as you see by
integrating top down in a nonlinear
fashion at each stage of the cascade we
get essentially zero error so so this is
an example of how in a nonlinear way
multiplicative and nonlinear integration
of top-down information can be
integrated in in order to help assist in
in propagating sensory input now of
course if you are basing on this is a
very simple task to do you just set the
prior to be on a one for the cat one
category and zero for the others but in
a way this is the game going back to
this localized representation will each
objects or each category are separated
of different parts of the network and in
a way this model that suggests here is a
model where quadratic or other
nonlinearities are essentially doing
bayesian like computation but in
neural network distributed
representation finally I want to talk
about a little bit about an exciting new
work with our one of our chairs then Lee
and send ensuing Zhang student from
Harvard but perceptual manifolds so so
far I I spoke about the violence and the
physical features of a of object as
noise that has to be suppressed out by
propagating in in a deep network now the
question is perhaps a Dan alternative
way to think about it perhaps you have
to actually think about many folds as
the relevant entities for processing
rather than points with noise so
previously i discussed work where the
object representation where gaussian
nice round gaussian mixtures or or
clusters but in in fact the way many
thought perceptual manifolds will come
about will have in general more
complicated geometry they may have
ellipsoid they may have actually be
lines closed strings or open lines they
may have rectangular shapes or rather
other complicated shapes so imagine in
our vision current vision is that we
have to stop thinking about processing
or classifying points of vectors that's
what you usually think about there are
points in attractive space in a
recurrent network or in the feed-forward
network we classify points and and we
compute the number of points and the
capacities and so on and the VC
dimension perhaps you should think about
more relevant to buy to biology and also
to machine learning in many in many ways
is to think about naught point naught
vectors but about manifolds to think
about classifying manifolds the capacity
how many many folds can be classified in
a given architecture what would be the
VC dimension
of them what would be the structure of
Max margin solutions to a linear
classification of this manifold what
would be the analog of support vectors
maybe support manifolds and Colonel
manifolds how you learn and generalize
in manifold space how the capabilities
of such a system that classifies many
Falls depend on the dimensionality of
the manifold the size the geometry and
so on and finally how do many fold
themselves propagate not simply noisy
noisy clouds but dramatic manifolds how
they propagate across the architecture
what happened to the size what happened
to the dimensionality what happened to
the geometric shapes and how these
changes improve their subsequent
processing in downstream systems so let
me just give you a few examples of of
thinking about manifold and transforming
or extending classical theory about
linear classification of perception
classification of points and 2 / septum
classifications of manifolds so let's
let me remind you about dinner
classifications of points so imagine you
have points in n dimension and some of
them have to be classified as minus
others as plus denoted by the colors and
a perceptron has a vector of weight and
has a decision hyperplane so this is a
linear classification on the point and
if you are looking at the max margin
solution then we will choose a solution
such that there will be this hyper
planes a distance Kappa the margin from
the decision plane so that all points
lie on one side of the margin either
above or below the margin and we know
tremendous amount of information about
the system from random at least for the
case of random labeling of points
there's a classical work of Elizabeth
Ghana from the late
ATS that develop the statistical
mechanics of perception and an example
of the result is shown here we're on the
y axis we have the capacity or the
maximum number of points per dimension
or per weight p over n alpha that can be
classified as a function of the margin
that you demand okay so either the
margin over the lord or the load the
maximum load the virtue of the margin
for cup i equal to 0 when you don't
demand and finite margin the capacity is
too and that is well known cover theorem
but as you see here that there is a
well-defined theoretical prediction
about how the capacity goes down if you
if you increase your margin and the
simulations are you perfectly well with
this classical theory but the theory
tells you more than just the capacity
tells you the structure of the max
margin solution and you can actually
compute how many points are going to be
like on the plane themselves on the
margin plane as drawn here and how many
points are interior so the point is
applying on the margin as as you may
know the support vectors or potentially
the support vectors of the solution they
are the critical examples that define
the the solution with the interior
points are points which if you can move
if you move them around nothing happens
and the theory tells you the fraction of
the support vectors that as a function
of the margin again a classical paper by
Abbott and Kepler from the late 80s and
as you can see here the more the higher
the margin is more and more point are
going to be on the margin which makes
sense because you push the margin high
then it you not have enough room for
examples to be in the interior space
copy equal to zero when you don't demand
imagine there's a classical poll results
then basically you have half of the
points of support vectors of potential
support vectors if half the points are
they entail so this is
one of the ingredients of Max margin
solutions of the theory of the
perceptron so now let's move to 22
manifold so the simplest manifold will
be line segments and may imagine you
have line segments and for simplicity
imagine that you have there the same
rate or the same length are but they
have random orientations and position in
space and you now randomly label them x
plus and minus ones okay binary
classification so again you can you can
look for a decision hyperplane and you
can look for a max margin solution by
distance kappa from the decision hyper
planes and you can generalizing the
statistical mechanics theory of
classification of points you can
actually compute the capacity as a
function not only of the margin but in
this case of the radios so if they if
they arrange means they extend the
length of the line so if the length is
zero you are back into point
classification if the length is infinite
your infinite line so the weight vector
has simply to be orthogonal to all the
lines there is no other way of
classifying them and in between you have
a case where interesting case where the
weight vector is not entirely orthogonal
to all the lines not only to a fraction
of them so what you see here in the
right diagram is again the configuration
as a function of the length of the of
this manifold and now you see that there
are two types of support elements the
lines that only touch them the margin
these are against support vectors
because they i have only points on the
margin but there are also lines or
segments that are fully embedded in the
margin and they are the support segments
in this case and as you can see here the
larger r is the more support segments
are being embedded into into the into
the margin planes which actually makes
sense because the larger there are is
the more organizing the weight vector
has to be so the theory is now capturing
not only numbers of elements but also
their in geometry in this case their
extent now you can look at more
complicated structures like squares
two-dimensional squares randomly
oriented and positions and you want to
classify them by a linear by a linear
classifier or by perception and now you
have more complicated structure for the
max margin solution you have a fraction
of points which are on the margin is a
function of their of this egg of their
size but you have also a fraction of
many folds squared which are fully
embedded on the manifolds but you have
also partially embedded manifolds where
the sides of the squares are embedded in
the manifold but not describe themselves
now in both cases in both case the lines
and the squirrels the problem actually
is strongly related to points because
the convex hull has these points either
the two endpoints of the lines or the
four corners of the of the squares so
you actually can ask instead of
classifying the full manifold
classifying only those two endpoints in
the line case or four points in the in
the square case now this of course are
not necessarily the same calculation as
the points case because the labeling now
is that random because the labeling is
taking the entire points that define
them the manifold and give them the same
label but you can also look at cases but
the convex hull of the manifold is
smooth so you cannot simply take two
points or four points or finite number
of points simply and turned it into into
a calculation of the of the of the
classifier nevertheless the theory can
be extended also to accommodate
as feels in any D dimension which have a
smooth convex hull and as you can see in
the in the right plot it gives us useful
information not only about the capacity
limitation is a function of the size of
those manifold but also about their
dimensionality and actually it is
interesting scaling that the size are
the radius of those manifold had to
scale like 1 over square root of the
dimensionality of those manifold in
order food had to have finite capacity
so let me then summarize what I've been
discussing today so first of all the the
first part was an attempt to provide a
framework where one can study
systematically how central
representations are changed in the
statistical character when signals
propagate from one layer to another in
particular when they are expanded into
space representations and interesting is
out which is somewhat intriguing from
the biology from the phone from from the
consequence to biology is that random
projections seem to be amplifying noise
especially in space representations and
don't seem to be a useful candidate for
for such a for such a an operation on
the other hand simple-minded
unsupervised structure like heavy on a
location and Association is that the sky
as i described can do a fantastic job in
suppressing noise and improve lead out
in particular when you do it not in one
stage but use as iterate this into deep
architectures then from a broad range of
initial noisy conditions the system
actually converges to a fixed point of
zero noise so what we have learned along
the way is that even though the problem
is still high dimensional you can get
inside and also quantitative predictions
by focusing on the red
a few relevant statistical features like
in our case the size of the noise is it
projected propagate along the network
and the correlations between those
clouds so for instance in the in
particular deep network that I that I
described I showed you how the
phenomenology of two-dimensional
iterative maps can give us insight about
what happens for give initial condition
as the system propagates along the along
the network now this is a very
simplified framework where I made
simplifying assumptions about the about
the statistical structure of the input
and simplifying assumptions about how
the network is created nevertheless the
hope is and it so happened in the past I
believe this also be in the future that
inside of this nature coming from theory
of simplified abstract models can be
useful also to understand more realistic
systems both in the brain and also in
machine learning applications and
surprising is that was that expanding
the architecture in the width has a
limited benefit on the other hand it's
much more beneficial to stage them into
a deep deep architecture finally I told
you about two additional variants one is
top-down information how the challenge
to find good models that take up down
information and integrate them into into
the deep network bottom-up information
flow and finally that if you if we take
hopefully those elements and combine
them with the last element that I talked
about which is not thinking about noisy
points propagating along those networks
but thinking about many folds of
different geometry and different sizes
and different dimensionality and how
they are propagating in on the network
eventually being read out by downstream
systems as a framework hope for
for object recognition and object
classification in neuronal networks
thank you so thank you hi while you're
clapping there is an ad about
opportunities for aspiring students and
postdocs who want to join us in the
journey of conquering the brain
understanding the computational brain
both at the Hebrew University at Harvard
the opportunities please write to me
yeah the time is delocalized having us
two groups of students one in Harvard
and one in Hebrew University in
Jerusalem so that is like a balancing
act all right a balancing act where you
have two balls in the air at the same
time what's quantum computing
delocalized so we have time for just one
or two questions one in the back usha so
I'd like to understand better the for
the deep architectures lines that you
presented the shallow infinitely wide
layer how come it's not having zero
error even though there it is possible
to have zero error because you said you
have as much data as you want you have
an infinite number of units so this
should be a consistent estimator it
should converge to the base error what
am I missing well our understanding of
that is that we have basically a finite
dimensional inherent structure due to
the correlations which don't disappear
when you when you increase the
dimensionality so that's kind of limit
the effectiveness of increasing the
dimensionality namely the the system the
signals are not really in low
dimensional manifold because of the
non-linearity but nevertheless they are
still incorporating sufficiently strong
sufficiently strong correlations not to
be not to be amenable to 22
improving the performance further
further indefinitely actually we have an
estimate of the size of the saturation a
layer which is dependent on both
inversely with the correlation and also
with the with the with the sparseness so
this part of the system is the more you
gain by going to why the system the
lower the correlations are the more you
gain by going to buy the system but for
any finite correlations and sparsity
level you eventually hit the wall so
even the sparseness is going down as you
increase the size of the layer or this
then the higher highest positive means f
small so the most part the
representation is the more you can gain
by expanding but up to a limit okay we
have one last question hi thank thank
you again for the beautiful talk the it
looks like the capacity estimations for
this for these manifolds are based in
some sort of distributional assumptions
for for whatever for the distribution
from which you generate these manifolds
or points some lettuce could you please
elaborate which distributions you're
using and whatever yeah and if whether
you think that these sort of whatever
distributions you are using for
theoretical analysis will be
representative of of natural
distributions over manifolds or points
or something well so the natural
application would be the following
suppose you have aighty cortex in
biology a population of neurons that
respond to objects or a face area that's
12 faces you know the number of neurons
you know more less the the response
properties so you can compute that
population vector or points in this
taste step space states-based of the
network that correspond to how the
neurons is a population represent a
single instant age instantiation of the
object now suppose you change the
physical features of the object in a
continuous
manually so it turns out that although
there is some degree of robustness in
those in those represent representations
neurons even in IT contexts are
sensitive to those physical variations
in orientation impose in scale in
lighting and so on so you can imagine in
staking set of points representing this
particular object a whole manifold of
New Orleans entation representing this
object this manifold is an invariant
manifold in the sense that eventually
perception or the cognitive system has
to intensify this manifold as the object
okay so you can imagine that now we take
multiple objects each one of them will
be represented by a manifold of normal
state and now the challenge is to
compute how many of such objects can be
represented by a particular part of IT
cortex now the theory properly extended
can be actually applied to get answers
to these questions oh that will be a
thing a project for next nips being
manifold learning in populations of
neurons thank you
can I ask my question well maybe while
you're switching computers is it a quick
question has to be very very quick I've
been coming to nips for 28 years of only
miss two I don't have to say your talk
today is one of the most profound
lectures I have had the pleasure of
listening to I have a very simple
question when you look at the brain on
the one side we have perception on the
other side we are the executive from an
engineering perspective I know they are
the do love each other my question is
having described perceptual manifolds
how do you see the executive manifold
how do I motor system does your
framework also you know apply to the
output motor manifold it's a great
question raised an okay we are meeting a
breakfast tomorrow morning moto manifold
it's a great question this was a joke
but I think it's a great question
because many of the theories about
coding in relation to natural statistics
are strictly applied to the perceptual
sensory systems and it's I think the
challenge how to apply similar ideas to
the motor system I think it's a great
question
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>