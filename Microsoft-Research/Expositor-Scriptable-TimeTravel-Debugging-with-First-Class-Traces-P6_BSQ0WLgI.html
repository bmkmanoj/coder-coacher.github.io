<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Expositor: Scriptable Time-Travel Debugging with First Class Traces | Coder Coacher - Coaching Coders</title><meta content="Expositor: Scriptable Time-Travel Debugging with First Class Traces - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Expositor: Scriptable Time-Travel Debugging with First Class Traces</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/P6_BSQ0WLgI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's my great pleasure to have
Michael Hicks visit us again at from the
University of Maryland Mike was my
advisor so it's a particularly great to
have him here and he's going to tell us
about some new work on time travel to
value thanks Nick okay so this is a
joint work with my student yet yet Fang
qu and also my colleague Jeff foster
okay so debugging this work was inspired
by a talk from Rob o'callaghan at PLD I
a workshop co-located there in 2010
where he said you guys are great at
finding bugs but we want you to stop we
have enough bugs already we want you to
help us fix the bugs so we thought well
there is a lot of work on finding bugs
but there's not a lot of work in the pl
community on helping people fix bugs so
what might that work look like so we
started thinking about the debugging
process and we started reading about it
and as I'm sure you've thought about
before debugging is the scientific
method in action your program doesn't
work the way you expect and so you
develop a hypothesis for why that is
based on your hypothesis you design a
test and you predict what the outcome
will be if your hypothesis is correct
you run the test if you're the outcome
is as you expect then great you've
figured out what the bug is so go fix it
otherwise it's not exactly what you
expect until you refine your hypothesis
and continue and this process for trying
to figure out why is the program not
behaving the way I expect is according
to ondrea's eller who wrote wrote the
book literally on debugging the hardest
part or the most time-consuming part of
the debugging process and it's why we
have lots of tools and sort of our own
pet techniques for gathering the
information to help us formulate and
test the hypotheses so some people will
use an interactive debugger some will
use print statements some will use
profilers and visualizers or maybe
special static analysis tools or dynamic
analysis tools and record replay
execution is sort of a recent tool in
the tool chain to help you to do this
sort of thing
another thing that you can do when
you're debugging is write scripts so you
can think in some sense whenever you
stick an assertion or a print statement
or something like that in your program
to gather some function of the execution
state of the program oh look at this
heat value compare it to this other heat
value look at it over time see whether
an invariant is held you're modifying
the program but you could just as well
have written a script that somehow looks
at the program execution in some way and
tries to establish the information that
you're interested in and the benefit of
doing that would be to avoid the sort of
tedious represent repetition of coming
up with these scripts over and over
again and hopefully also allow you to
then reuse the script in a future
context given some set of scripts that
you might have designed for a particular
program you'd like to be able to compose
the scripts into bigger scripts and you
might be able to link them up with
sophisticated tools for doing
visualization in short why not make the
debugging process like the programming
process in the first place rather than
have it be an ad hoc do it once throw it
away kind of event what we're going to
spend as a programmer you're going to
spend lots of your time debugging
programs so why not in the same way that
we automate things like that we write
programs for in the first place help
automate the process of doing the
debugging to to help the programmer okay
so I don't know how many of you write
scripts for debugging how many people do
scriptable debugging okay an
overwhelming number zero so why is that
well one reason might be that scripts at
the moment are fairly ad-hoc they're
brittle and they don't actually meet any
of these criterias that I just meant
mentioned up here there they tend to be
hard to reuse hard to compose hard to
even get right so to illustrate let me
show you what it would take you to write
a script to count calls to the function
foo in some program execution using G
db's scripting interface to Python I bet
you didn't even know you could do that
so Python there's a GDB extension that
allows you to script gdb commands step
set breakpoints and so on and I'm going
to step through that interface here in a
minute and you can write those scripts
run them on top of gdb okay so the first
line of this rip says I want to set a
breakpoint at foo so you call the
breakpoint command to set the breakpoint
and i'm going to set two additional
variables count and more explain those
in a minute I'm going to define two
handlers the first is the stop handler
which will be called every time the
function foo is called in the program
execution and a little bit of logic
there make sure that the breakpoint is
due to foo if it is it's going to
increase this count variable by one the
exit handler is going to be called when
the program terminates and we're going
to use that to set the more flag to
false which tells us to stop running our
Python script the Kinect functions are
going to set the assign the break point
to a particular handler to these two
handlers the break point and the exited
the the exit status and then finally
we're going to start the program and
we're going to start the script and the
script is going to basically keep
running continue keep running the
program until it's finished at which
point the count field the count variable
in Python is going to contain all the
calls the total number of calls to foo
and then we can disconnect these and
delete the breakpoint okay Wow so uh
what's what's wrong with this idea well
basically this is classic call back
style programming and as as you might be
familiar with this has several
disadvantages first of all the control
flow of the program is not straight line
code anymore it's now smeared across the
program text so if we look here we can't
just read the code straight down to see
exactly what it's doing instead the stop
handler is going to be called indirectly
via these calls two more so this is
going to be running for a while and then
it's going to jump up here change this
state jump back run for a while then
maybe jump here and then jump back and
jump here and jump back it's not code
that you can read straight down it's
code that the control flow of the
program execution is is nonlinear okay
so that's that's frustrating more
importantly it's it's also brittle so if
the handlers can have side effects for
example of a handler could set in
remove breakpoints then it may compose
poorly with other scripts that you've
written to also have these handlers
right so for example let's suppose I was
writing a program and I had a script
that counted all calls in my program
execution and I also wanted to run it
alongside this script account just the
food calls right I should be able to do
that I should be able to take n scripts
and run them on my program execution and
not worry about them interfering except
that will clearly interfere in this case
because it'll destroy the all call
script and of course there could be
inadvertent clashes on global variable
names and things like this right so this
is going to hurt our ability to reuse
scripts and of course imagine how you
how you might compose this script with
another script that say only counts the
food call such that the argument X is
greater than one right I'm not going to
be able to take this and just add on a
little bit that says limit what you can
do you're going to have to come in and
modify this script so it's no longer
composable anymore you have to change
the original script this this should
still work so gdb will still be oh I see
you mean what if that stop handler and
so on could be called multiple times I
don't that's really depends on the
semantics of gdb so i'm not i'm not sure
what you TP does okay so to address
these problems we developed a scripting
language for debugging and the system
that it's implemented in is called
expositor an expository execution trace
is a first class object so instead of
sort of being in the middle of the
execution and stopping and starting in
and reading and writing we pretend that
the execution has already completed and
we have it in our hands and now we can
compute over it we view the execution as
a trace a list of snapshots and now we
can perform common list processing
operations from functional programming
to filter a trace to map a trace to
certain events in the trace to fold over
it to count up the number of values and
so on treating the trace as a as
something to be computed over in a
functional way to implement this idea we
build on top of time travel debugging
and we use laziness
so time travel debugging gives you a way
of treating an execution trace as a
single object right I can go forward
backward basically it's like a time
indexed series of events and the time
travel mechanism it is an efficient way
of going to this point or this point or
this point or this point in the
execution without having to capture the
entire execution and throw it to disk
which would be extremely expensive and
very slow so expositor is implemented on
top of gdb and undo DB undo DB as a
Pratt is a commercial tool that extends
gdb with record replay time travel style
debugging and we basically just build on
top of that by providing this functional
programming interface okay so for the
rest of the the talk I'm going to
describe to you what the expositor
interfaces what is this programming
language that we're providing this
abstraction of traces what does it look
like I'm going to tell you about how we
implement how we use laziness and
expositor to implement these scripts
efficiently and in particular I'm going
to tell you about some data structures
we had to invent to take advantage of
laziness in the right way and then I'm
going to show you some performance
experiments that explain why this
laziness turns out to be very important
for efficiency and why these data
structures are also important and then
finally I'm going to tell you about how
we used expositor to debug a what turn
well a memory leak and Firefox which
turned out to be a data race the
interesting thing is the official fix in
Firefox for this bug didn't actually fix
the problem they ended up adding some
other trigger for the garbage collector
when in fact they didn't need to do that
and they should have fixed this data
race instead we started to debug it and
a fix came in but it was a non fix and
then once we had debugged it we realized
the fix was the wrong fix okay so how do
we count foo calls in expositor um it's
really easy it's two lines of code so
basically we treat the execution is a
first class object so we have in our
hands this variable V execution it
implements the execution class that
expositor provides and it has several
methods which i'll explain in a minute
one of them is the breakpoints method
which is you can think of a way of
filtering the execution of all possible
events in the program to only those
events only those snapshots those
program states in which foo is being
called okay so of course under the
covers this is going to be implemented
you can think of as setting a breakpoint
running the program having the
breakpoint hit every time and then
remembering those states in the program
and then making those available in this
resulting trace okay so once we have
this resulting trace of all of the foo
of ents we've now filtered the execution
down to those events we can just compute
the length of the trace and we're done
we now have all of the the flu events
okay so two advantages that the other
script didn't have first of all the
control flow is straight line it's very
easy to see what this program is doing
and second of all it's very easy to
reuse and compose with other scripts
this is a purely functional the
execution and foo are immutable objects
so no matter what other computation you
do they're not going to be influenced by
it foo is always going to be only the
food calls and makes no difference what
other traces you might derive and as a
result you can do something like filter
all the food calls to only those
snapshots in which the variable X is
equal to 0 and now i can count those
instead okay so let me tell you more
about the the execution classes methods
so they're all listed on this slide
we've just seen the breakpoints call
there is also a call for system calls
for checking read events or write read
calls or right calls or saka calls or
things like that the first line is the
get at line and it says that you can the
get at call you can go to any particular
time in the middle of a trace and get
the snapshot at that particular time
there's also the watch points call that
gives you all of the events wear a watch
point would be flagged so you can set
the watch point on a variable and you
can say whether you're interested in
reads or writes to that variable and
again that's effectively going to filter
the execution down to only those
snapshots where that read or write is
happening and then finally you can also
ask for an execution for all of the
function calls or all of the returns now
the bottom two lines are four
interactive control so the way that you
use expositor is you'd fire up gdb and
you would say
I want to start running and the program
is going to run for a while now the
thing is the program might be
interactive and so we need it's going to
wait for the user to enter a bunch of
events so for our Firefox bug for
example the Firefox bug requires you to
scroll the screen on a particular test
page and then wait a little while and
scroll the screen again and then you can
see the memory leak and then you can see
it disappear and so in order to
reproduce that bug and debug it you're
going to have to run the program for a
while and manually do the scroll bar and
so on and then once you've gotten to the
point where you're you're done you can
hit control C and at that point you now
have an execution all the way up till
that moment and the execution variable
will represent the execution up to the
point that you hit control C if you want
to keep running because you discover oh
I need to do more work well then you can
call continue or you can call get time
to run it run it further along until you
want to stop and look at it again okay
sure so so record replay is different
than stashing the entire state in
performing snapshots and logging the
results of non deterministic events so
the idea is that let's say every ten
events it will snap will make it more
than that maybe every 1000 tix it will
record the entire snapshot so you can
snapshot snapshot snapshot snapshot so
in order to make sure you have a
deterministic replay all that's required
is you record the results of system
calls at every moment in between each
snapshot okay so every time you do a
read every time you take a signal every
time you do something that might affect
a non-deterministic choice undo TV will
remember the result of that event and so
when you call get at it the equivalent
of it in on duty be to go to a
particular time it will basically go to
the furthest back snapshot preceding
that time and then replay the execution
forward filling in its logged set of
system call events
break points of food you don't actually
eat early computer all the places where
being involved right that would be very
expensive so undo DB will do that if you
set a breakpoint and you run the program
on dudaev will stop every time and
capture all the traces the the
breakpoints but our implementation is
lazy and we will not do that and I'll
show you exactly how okay so these
things will give you a trace and a trace
as i said is basically a list of snap
you can view it as a list of snapshots
where a snapshot is just in a program
state that you can use standard sort of
debugging commands to look at to look at
its variable values and chase pointers
and everything else ok so now given a
trace you can perform more computations
on the trace so for example I can ask
for the length of a trace and that will
compute the total number of items I can
go to a particular point in the trace by
calling get at get after or get before
get after saying because the trace may
be very sparse now if you give it a time
it may be that there's no event at that
time in the tray so get after will give
you the first one after that time and
get before will get give you the first
one before that time and then the next
three are basically functional
programming thing so filter is going to
filter the trace events according to
this predicate and map is going to map
each event according to the function f
to this new value so for example you can
imagine having our trace of all foo
calls and we could call map on it to
make a trace of say all of the values of
the first argument to foo okay you could
call the get their thing on the snapshot
and then create a new trace of just
those argument values yeah before with a
pretty good time index and they may not
be a team at that time indexes yes food
why would they not be so imagine I did
let's let's say I'm doing the Firefox
example again so what will happen is we
ran the Firefox example we do the first
scroll event which is going to cause the
memory leak to happen and then I hit
control C and then what I do is I say
what's the current time so there was
this execution get time here ok now I
have a value that's at that current time
okay then i'm going to say continue and
I'm going to keep running it I'm going
to do the other
scroll event I'm going to hit control C
and then you could get time again and me
the programmer i'm going to squirrel
away those values of those times it's a
hundred 110 whatever that is okay so now
what I might do is I might want to
filter the entire execution to be only
calls to the garbage collector okay so
now my trace only consists of those
calls to in this case jsg see now what
I'll do is I'll go to one of those times
whose values I got and I'll say give me
the most recent call prior to this time
where I call the garbage collector
that's right okay and then this is just
emphasize this last line is emphasizing
what I said before is that traces are
immutable that they're defined yeah
they're immutable so there's no way to
add or remove elements to a trace
they're just defined as relative to an
execution okay so how do we implement
this we implement it lazily and the
trace data structure is such that when
you call something like the execution
breakpoints foo nothing is going to
happen it's basically going to make a
thunk that when the funk is queried will
actually perform some of the computation
so the way the trace data structure
represents this idea is we're going to
have this dotted box and at the
beginning of the box is the beginning
time that the box represents in the
trace and the right-hand side is the end
time so when we start off and we create
this thing the box represents the entire
execution and we know nothing about it
we haven't forced it yet now unlike
normal lazy implementation of lazy stuff
where you just have a thunk and you push
it you say you run the thunk to get the
value our trace thunks are going to take
as an argument a request to either do a
get before again after or a get ad okay
so we're going to pass a time and we're
going to pass whether we're going
forward or backward in time because it's
the gap before and get after that's
effectively going to force the trace so
let's say that we do that we want to
find out well at time 100 what's the
most recent value of this trace for all
of these execution break points at foo
so way that will work is this way what
we'll do is we'll we'll pass in get
before in 100 to our funk and it will go
to time 100 it will tell
DB go to time 100 and then it will say
start running backward in this it will
set the breakpoint sorry because it will
have remembered that when i created the
thunk in the first place and will start
running backwards they basically keep on
running until you hit the first call to
foo running backwards starting at time
100 so what will happen in this case is
let's say that foo is at time 50 it will
go to time 100 run back to time 50 find
the call to foo so now it knows two
things it knows that there are no calls
to foo between 50.1 and 100 so we'll
make a node remembering that inside of
the trace data structure there are no
events here and it will also know hey
there is a call to foo at this time and
it will cash that call that snapshot in
store it here okay it knows nothing
about what happens after time 100 or
what happens before time 50 and so
that's represented in this data
structure these things are unknown so if
in the future I then wanted to call say
get before 25 well it's going to on this
data structure now this somewhat
expanded version of of the food trace
and it will go come over here and
discover oh there's nothing here and it
will do the same trick it'll go to time
25 and push back if instead we said Oh
get before 75 after this well it's just
going to go straight to here and then go
straight to food knowing that I've
already computed this and then I'm done
yeah so filter will basically layer on
top of these things so you can imagine
what will happen is I'll do this and
then I'll do filter and then I'll call
it get it before or get after so if I
did it get before on a filter that's
basically going to push down to finding
to the base trace and it's going to
start working backwards to find events
which will then test against the filter
and keep going until it find one it
finds one that's acceptable and then
remember that event right so basically
filters and maps and so on are just
going to compose on top of these raw
traces at the bottom and when we try to
force a filter we're going to end up
forcing parts of the underlying trace as
well as parts of the filter above the
trace and the same will go with map and
slice and everything else
okay so that's the that's the basically
the simple idea so there's some there
are more complex operations that we
perform where we're for example going to
merge traces together so might why might
we do that well imagine we have a trace
that we have all the calls to foo and we
also care about all the calls to bar
okay we have to set those as separate
break points and the way we'll do that
as will create the break points for food
the break points for bar and then we'll
merge the traces together and merging
happens according to time ordering
there's also this trailing merge call
that allows us to combine together
elements of two traces scan which was
basically like a fold except it's it
stores its accumulator results at each
moment in the trace and then t scan is
sort of a version of fold that takes us
antigen AZ associative operator so
here's some here's how those things are
implemented so if we call tr0 merge of F
on TR 1 I want to merge trace 0 and
trace one if the true traces events are
distinct so with the calls to human
calls to bar okay no problem then the
resulting trace which is shown on the
bottom line is just going to have all
the events from both traces now the only
interesting case then is when you have
two events that are at the same time in
that case you're going to compute the
result of the third trace by taking the
arguments of the first two traces so for
example if we were merging let's say we
were merging traces of all of the food
calls where we looked at all of the
values that were greater than zero and
all of the values that were greater than
one a whole bunch of those things are
going to match up and f is going to
disambiguate those and the case where
it's only greater than zero but not
greater than one is just going to float
directly down trailing merge is going to
take two traces and it's going to
compute the result in the third trace by
looking at an event in the first trace
an immediately preceding event in the
second trace and computing the result in
the third race and then likewise right
all the way across and trailing merges
reb trailing merge is just going to do
the opposite and i'm going to show you
in a minute an example of
this is used so you can see why you
would care about this I just want to
emphasize remember that traces are not
only lists of snapshots their lists of
any values right like the example that I
gave you with map you might take a
snapshot as an argument executions will
always produce traces of snapshots but
from that moment on you can produce
traces of anything else you could map
the traces of snapshots to a trace of
integers by just pulling off the first
argument of every call to food for
example so this function f might be
combining things right if I had I don't
know I had a trace of all of the first
arguments of tofu and a trace of all the
second arguments to foo well all of
those trays values are going to be at
the same moment in time and the function
f could just add the arguments together
for example okay so the other
computations are this scan and reb scan
so this is like fold okay so that's what
it's showing here on the bottom you're
going to take an accumulator and the
original event compute some function of
those two that's going to go here and
then this accumulator will feed into the
call to F for the next element and store
the result there and so on all the way
down the trace so normally a fold and
functional programming only gives you
the end result and this fold is going to
store each intermediate result in its
own trace at the same time that the
original trace elements were stored okay
so scan that's actually a standard
Python function that we we bring to this
context as well and refs can just goes
the opposite direction TR scan or t scan
is stands for tree scan and basically
you can think of that is just an
associative an infix operator that's
associative so the resulting value at a
particular n is just going to be F
applied to all of the values before it
where you could compute them in any
order right so if I had four values I
could have computed after the 1st 2 and
F of the second two and then F of those
or I could have done it the way that
it's shown there and the idea there is
it gives you more flexibility in
incrementally learning materializing the
con the contents of a trace by youth
taking advantage of an associative
operator
okay so those are the basic operations
and now I'm going to tell you a little
bit more about how laziness interacts
with those operations and how we try to
take the most advantage of laziness in
the interface okay so let's say for
example we're going to call count the
number of calls to foo X from the end of
the trace all the way back until I see a
call of food 0 okay so I'm basically you
can think of the trace is a whole bunch
of calls to foo at some point there's
going to be a call to foo 0 and I want
to know well how many calls to foo
happen after that point okay so what
this is going to do is it's going to
first of all create the execution break
points for foo its then i'm going to
call scan and call this function with a
nonzero foo and the accumulator is going
to be 0 so it's doing the fold okay so
inside of the fold it's going to read
the variable for the snapshot x and if
it's not 0 then it's going to increment
the accumulator by one which is right
here otherwise if it is zero then I need
to reset the accumulator because now
I've seen a few 0 so I only want to
count the things that happen after the
last food 0 so by the time I get to the
end I reset the accumulator the last
time and I've only ended up counting up
here the calls to foo after that last
reset okay so notice that the
accumulator here is lazy so i said i
called this lazy accumulator and the
programmer explicitly calls force here
to force the lazy computation and why is
that well the reason is that by
definition of the scan i'm going to call
F on the previous accumulator and the
current input value and if the
accumulator were not lazy i would be
effectively forced to go all the way
back to the beginning and compute F for
the entire trace even if I could
conceivably compute the result of this
call without having to do that so in
particular one way that I would like to
be able to compute the result is to go
to the end of the trace and go backwards
in time only and basically stopping at
the first moment here that the
accumulator is 0 and I don't need to
compute anything about the prefix of the
trace anymore and by making the
accumulator lazy that's that's not
possible
okay so now here's our first big example
of how we can use expositor to write a
generic script and this is a script
that's going to find stack corruption in
any program basically the idea is if
your stack has been corrupted then what
you the return address that you wrote
when you enter the function call might
be different than the return address
that's there now when you're about to
leave it and so we'd like to tell when
that happens and the way that we're
going to do that is we're going to write
a script that maintains a shadow stack
so that it will mirror the execution
stack and we will make sure that the
shadow stack and the real stack are
always matching up during the execution
the first time they don't match up well
know what the address was that was
overwritten okay so the first call here
says give me all the calls in all the
returns and then merge them together
okay so now i have a trace of all calls
and all returns to every function in my
program and the shadow stack is just
going to create for each one of these
events the stack at that location so ret
adders just lists all of the return
addresses of the stack in that snapshot
okay so now this shadow stacks trace has
the stack at every call on every return
in the program every function call on
every return okay great so now I want to
I'm down here with corrupted finding the
corrupted address is what I'm going to
do is I'm going to call trailing merge
on this function up here using the
shadow stacks as an argument and what's
that going to do okay well this is the
snapshot at a particular moment in the
in the program execution in the call
rats and this is the shadow stack at
that moment what we're going to do
assuming that there is a shadow stack
this is only going to happen this not
none it's only going to be none in the
very first function call that you you
haven't had a shadow stack what we're
going to do is we're going to compare
sorry this is a trailing merge so
trailing merge is going to take the
current call in the program and look at
the immediately previous immediately
preceding call in the in the other in
this list here so what it's going to do
is at each call it's going to look to
the immediately preceding call or return
in the shadow stack or each return it's
going to look at immediately preceding
thing in the shadow tech stack which is
the call and then it's going to compare
the return addresses for
snapshot with the return addresses that
are shown in the shadow stack zip is
only going to look at the up to the
prefix right so one of these is going to
be one shorter than the other if the
immediately preceding thing was a return
then the call is going to have one more
return address than the return did if
the immediately preceding thing was a
return then it's going to have one less
so it's just going to look at the
prefixes and make sure that those are
the same and if they're not the same
then there's a potential problem in that
case it's going to remember the the
stack that failed otherwise it's going
to return none okay so let's see how we
could use this script we're going to
start off with running a program tiny
httpd that we developed this is an
existing open source program and for our
security class we modified this program
so it had a stack smashing boner it had
a buffer overrun in it that you could
hit with a stack smashing attack so we
wrote a stack smashing attack for it we
start running this program so we call
the execution continue and it's now
running on this port in a separate
window we call our exploit function
which connects to the program and does
the stack smashing attack which now it
pones my computer isn't the debugging
window I hit control C and now I call
the stack corruption function which is
the code that I just showed you ok so
now i have a trace where all of the
elements in the trace if there are any
have the list the address of the stack
that there sorry has the snapshot of the
stack that was corrupted ok so what I'm
going to do then is I'm going to get the
current time for the execution and i'm
going to call get before the current
time to find the most recently corrupted
stack address then what i'll do is i
will set a watch point on the last
corrupted value right so i'll look at
the last corrupted stack address and
i'll look to find out where that value
was written and then i will call get
before the time when the corruption took
place to find where the value was
written and now I found where the my
buffer overrun vulnerability is
okay so the key to pump to see here is
first of all this wasn't very much code
to write this out we're taking advantage
of all these nice facilities of
programming with lists and so on it's
completely reusable you can use the
script to debug stack smashing any sort
of stack corruption on any program it's
not specific to this tiny HTTP to PD
program and the other thing that's
really nice is that because I'm running
what if is effectively a dynamic
analysis as the script inside of an
interactive debugger I get to interact
with the results right so I run I don't
just run this the stack corruption and
then get some address and say now go
figure it out I now have my hands on the
execution in which it took place and I
can start querying the execution and
running additional script so here I set
a watch point and I go back and I find
out where the corruption actually took
place if there was a bug and I want to
figure out why there's a bug i can
continue to interact with the execution
trace using these scripts to help me
learn more and more information as
another example of a reusable script for
the firefox case we developed a simple
happens before data race detector that
to help us discover this problem so
that's again a reusable dynamic analysis
but thing is great about is when it
finds a bug you immediately have all of
the power of an interactive debugger to
start figuring out why did it happen
whereas if you write a one-off dynamic
analysis tool it's going to say oh you
have a data race online 27 involving
variable X and these two other threads
go figure it out here you can figure it
out immediately by starting to interact
with the debugger to help you understand
why it might have happened ok so so that
sounds good now thinking ahead to other
sorts of scripts you know what we might
want to write we're going to want data
structures yes
cska suppose one of the problems that
people often complain about in saying I
mean bigger SD textured is that that the
overhead is so high and it messes up all
the scheduling and you go yeah that's
the problem it's still a problem in fact
it's worse because because the debugger
is much slower but I think that this is
what you want so i would say that our
goal here ought to be not to give up on
it but to figure out how to make
debuggers better I mean in my experience
I mean I don't know about the windows
debugger I hope it's better than gdb I
mean gtb is kind of a crap debugger to
be honest and i'll i'll talk more about
ways in which i would hope we can get
closer to making this vision a reality
by driving the performance down but it's
not going to eliminate the problem that
you're talking about okay so another
sort of analysis that you might like to
perform when you're writing a script
involves data structures so for example
if you wanted to debug a memory leak you
might want to keep track of all of the
things that are current you might want
to have a sort of a shadow data
structure that represents all of them
Alex data so every time you Malik
something you add it to your set every
time you free it you remove it from your
set and at some point in the program
where you expect or you know something
about elite value you can look in your
set and then you know that's the address
and then start doing the same sort of
stuff I just show you so therefore we
need a data structure we need sets maps
hash tables multisets things like that
in order to program with okay but the
problem is if we just use pythons data
structures we're going to end up
compromising laziness in the same in a
similar way that we might have
compromised it if we hadn't made scan
and fold and so on lazy so let me show
you why that is so here we're going to
write a program that's going to collect
all of the arguments to all the calls of
foo in a Python set and what's going to
happen here is I'm going to compute all
the breakpoints I'm going to scan so
that's like a fold by collecting all
these arguments in the empty set and
then at the end once I've gotten all the
way to the end I'll look in the very
last or look at the most recent to t
value and ask whether 42 is in the set
okay so what does collect foo args going
to do well it's going to force
the accumulator so that's the set starts
out as empty and then it's going to
union it with whatever the current value
of x is okay so why is that a problem
well the problem is that each time I go
to perform this union I'm going to then
first of all I'm forcing the trace again
so I'm going to have to go all the way
back to the beginning and second of all
union is going to deep copy each each
the set each time I add an element to it
in order well in order to make it purely
functional okay so you're going to get
killed on performance both because of
the deep copy and because you have to go
all the way back to the beginning even
if it turns out that the last thing
maybe at time T just before that you
stuck 42 in the set and so therefore we
don't care anything about what happened
before it and yet you're still going to
be forced to go all the way back to the
beginning to compute the entire set okay
so so we invented a data structure
called an edit hash array mapped try or
edit Hampton immutable set map or multi
set multi map each update is going to
share structure with the prior edit hams
and only when you do lookups are we
going to force the necessary computation
so it's lazy and the implementation of
it is going to involve an edit list and
a lazy variant of an of a hash array map
to try so hash array maps tries exists
those are not a new idea and it lists
exists those are not a new idea but
combining these two things together
that's the new idea and applying it to
this debugging context ok so now I'm
going to force you to think again and
while I explain to you how these data
structures work so if you don't want to
think you're gonna have to zone out for
10 minutes but otherwise I'm going to do
my best to explain to you how it works
ok so what's an edit list an edit list
is just instead of maintaining the the
actual values inside of a set it's going
to maintain all of the operations on the
set so every time I add an element to a
set instead of actually representing the
set is that value and then removing it
representing it as an empty list and
then adding this representing it as a
list I'm going to represent the entire
thing as this list of operations I added
this I remove this I added this when I
want to look up whether something is in
the
I'm going to go to the most recent thing
and I'm going to keep going back until I
either find a find or remove the first
one of those things i find will tell me
whether or not it's in the set so if i
find a remove it's not in there if i
find an ad it is okay so why is this a
good a good thing well in our case we
actually care about each of these we're
separate Lee maintaining it each element
in our trace is going to be the complete
set so we really need at time t0 this
and at time t1 this and at time t2 this
right we need a cheap way of
representing all of the possible
incarnations of the set throughout the
entire trace and an edit list is a cheap
way to do that now the drawback of an
edit list is that look up sorrow of one
anymore there now you have to move your
way back inside of the trace and I'm
going to show you how we overcome that
problem in a sec oh and this these
dotted lines are saying that it's lazy
that the previous things are lazy and
I'll show you in a minute how that comes
into play okay so now what's a Hamptons
a hashtable tree hybrid you can think of
a if you think of a chained hash table
as an array right you hash into all of
the elements of the array and then you
have a list of buckets for dealing with
collisions well in a hand the array is
going to be an amped and array map to
try so i'm good i'm going to get rid of
the actual array and i'm going to
replace it with this array map to try
instead and then instead of maintaining
a list of buckets for resolution of the
hash collisions i'm going to have i'm
going to do double hashing i'm going to
basically have another HAMP debt
resolves cases where i have a collision
so if i hash two values and they end up
going to the same spot in the amp'd then
what I will do is I will have a new hash
function that then store that
distinguishes those two values in a nest
in hamp debt is to resolve chaining to
resolve the collisions okay
right and you just keep doing this all
the way down you just keep doing the
nesting yeah you end up having a whole
bunch of hash functions so it turns out
we're not in our edit HAMP's we're not
going to end up doing this nested HAMP
don't get too fixated on it but a
regular HAMP twitch is not our idea
that's the way that they would propose
to resolve collisions we're actually
going to resolve collisions with edit
lists in our final version okay so
what's an amped and amped is just a
representation of an array as a tree and
it's a map from integer keys to values
so the internal nodes are sparse arrays
basically what you're going to do is
you're going to break up the key space
into n bit values and you're going to
have n level sorry you're going to have
whatever the total key length is divided
by the number of size of the bits that's
how many levels you're going to have in
your tree and the leaf nodes then
contain the bindings so as an example
let's say I want to do find seven inside
of this particular array well seven in
binary is are these three pieces these
things are two bits each right so they
represent 00 01 10 11 and what we're
going to do is we're going to iterate
through the key space moving through
this array if we find a pointer in the
array for this particular key value then
we follow the pointer if we don't find
it then we know that the value is not in
the list it's not in the array and then
we're finished okay so in this case we
have 00 so that's here so we're going to
follow the array the next thing is 0 1
so then up we see this so we're going to
follow it and then finally we see 11
which is this and we see indeed 7 is
mapped to value be inside the array now
if you were going to 50 here we would
end up breaking up the key space and we
would have again 000 I guess in this
case we would have 11 is the first of
the key for 50 plus it would have other
elements of the key but in this case
those other elements are not actually in
the tree so looking up 50 immediately
leaves us to this new leaf node and we
can look up 50 and say yeah it's in the
tree and then we're done so in the worst
case you're going to end up going to the
completely bottom part of the tree but
the tree is a fixed height so
essentially you still have of one
lookups right it's based on the length
of the key ok so a lazy amped is a
lazy variant of this where the internal
nodes are lazy sparse arrays and again
the idea here is that these arrays are
going to be computed as they're needed
where the laziness is essentially
inherited from the laziness in the trace
data structure and the nodes are going
to be lazily created during look up so a
way to think about this is when we go to
insert something into the tree we're
going to create a thunk that will work
its way down the existing tree to find
out whether that thing is already in
there and then you know resolve the hash
collision and everything else or if it
isn't in there it's going to then cause
this tree to then be materialized and
it's going to share internal nodes so in
this particular case if we inserted 57
we would say oh well I'm going to go to
this first element of the tree I'm going
to need to go to the right here which
means i need to inherit this version so
this is lazy but it's not actually going
to it may be that this node was not even
materialized in the original tree so
this is going to be sort of a lazy
pointer to that note if it was
eventually forced by say looking up that
note inside the tree etc working its way
down here and eventually sticking 50-70
in along the way we just discovered that
here's where 50 would go and so we
insert that in the tree okay so the
point is rather than eagerly computing
this tree when you do the ad you're
going to just take a thunk that will
compute this tree whenever you force it
through a lookup ok so an edit hand is
an edit list plus a hash plus a lazy
amped ok so the chained hash table is
the Edit hamp the hash key look up is
the lazy amped and the hash collision
resolution instead of being nest in
hamps is going to be edit lists and it
turns out the Edit lists are also going
to be useful for a remove operation ok
so here's an example where we start with
an expository and let's say for example
at each element in this trace we're
going to add these elements a 0 a 1 and
so on to to our resulting trace ok so
the way that that's going to work is in
each moment here we're going to create
the thunk that represents the the
process of adding or removing or adding
an element to the trace in this
particular case we added a zero
and this is where a zero would go here
we remove a zero and again that removal
is then just going to chain with the ad
that was there before because these guys
hash to the same location in this case
we're adding a one and so it's going to
go a different way and it has nothing
that's that's previous to it all of
these arrows and so on are all lazy so
you're not going to see them until you
actually start forcing things to happen
in the computation so you can see here
how edit lists are dealing with removals
so this is hashing to the same value and
it's augmenting what was there before
but it's also going to deal with it's
also going to deal with collisions so
let's say for example that a 5 happens
to collide with the same place as a 0
then all that's going to happen is here
is going to be add a five and then in
order to look up a 5 i'm just going to
have to reverse my way down the Edit
list until I find a 5 or I hit none just
like you would have is with a bucket
array in a regular hash table ok so I'm
just going to skip this
okay so i hope you get at least the
basic idea so let's see now how
important this is for performance we
made a micro benchmark that looks to see
how effective this is in the way the
micro benchmark works is we're going to
run a program to create a trace will
then call get after get before to
enumerate all of the events from the
trace so the trace that we're going to
look at in this case I'll show you in a
moment it's a bunch of function calls
will create the trace and we'll call get
before to find all the events or get
after repeatedly to find all of the
events to force each one one at a time
therefore incrementally lazily expanding
the lazy at the lazy trace after we do
that we're going to repeat for trace to
where trays to is derived from the same
input as trace one so it should be able
to take advantage of the materialized
laziness and things should be computed
faster because it doesn't have to force
all of these thunks anymore okay so
here's the test of debugging target
program that we're going to use inside
of this program it's a couple of nested
loops there's a funk there's calls to
the function foo here and calls to the
function bar here food takes two
arguments x and y bar takes one argument
z generally we're interested in these
calls to foo so we're going to get food
traces but we may also be in some cases
interested in calls to borrow though
mostly they're just there for filler
okay so here's our first benchmark we're
going to capture all the breakpoints to
foo and then we're going to filter based
on all of those things that have even
first arguments versus all those things
I'd have odd first arguments okay so we
make two traces that are derived from
the first execution all right and here
is the here's the the performance
results okay so let me explain how all
this looks first of all times zero here
is where we create trace one then this
sequence is imagine that I have a loop
that's between trace one and trays too
that's continuously calling get after
until we're finished to get all of the
events that's this side or is
continually calling get before by
starting at the end of the trace on all
the events that's this side okay so this
is the first call to get after the
second call the third call and so on all
the way up until we've
every call to foo and we do that first
for trace one and then we rewind the
program and we do it again for trace too
so trace one is going to force
everything to happen for the first time
and then then we're going to create
trace two and then we're going to query
trace to again okay and it's the same on
both sides the y-axis is the cumulative
time to perform all of these events okay
so what are the different lines so the
baseline is just the time to simply
start up running GD be okay so that
takes almost a second just to start up
GT be so we don't want to count that
time the the lazy trace is regular
expositor and the way I've described it
strict race is expositor with all the
laziness disabled so all the problems of
not making things lazy that I used to
motivate why we use laziness are going
to be evident in this trace and then
finally gdb is just doing it with gdb
Python scripts doing it in this sort of
non-reusable non composable way and then
undo DB is using the same thing as jdb
but instead of restarting the program
for the second trace is what you have to
do with gdb once you've run the whole
program you have to restart the program
to run with the second trace with undue
DB you can rewind the program and rerun
it which is it turns out will improve
time a little bit okay so lazy trace has
a low trace creation cost basically it
is costing you essentially zero here
notice that the time of creating it is
basically nothing has happened now on
the first query all of a sudden we're
going to pay this cost to start up gdb
and run it and we're going to eventually
work our way over here materializing all
of the events ok so the places where
there's a steep curve here or all of the
calls to bar that have nothing to do
with any call to foo okay so it's going
to take some amount of time but no
events of interest have taken place
inside of that period right so in the
case that all the foo a vent sorry vynn
i'm going to capture all the even food
calls and then there's a bunch of odd
food calls that i'm going to need to
skip over and that's what's happening in
these steeper curves because there's no
event of interest
is there but you can notice that
basically the amount of time that's
taken is the same looking up on the on
the y-axis in both of those cases
because I'm pushing through the same
number of events the lazy trace works
faster than just vanilla undo TB or gdb
for the first third of the queries so
that's good the slope of the trace in
the second query is much shallower
because I've already computed I've
already materialized all the lazy events
and so I end up just getting to reuse
all of this stuff I save and notice that
the undo DB curve is below the gdb curve
so it's faster to rewind and keep and go
again than it is to restart finally
notice that the strict expositor trace
up here is always worse than the lazy
trace so forcing all the computation is
is a worse thing to do and then finally
reverse execution is slower than forward
execution and so that's why this graph
looks basically the same but it's just
stretched upwards because running
backwards is slower than running
forwards but the same trends apply the
laziness with some runtime support
yourself or is it all just implanted
inside by them it's all implemented in
Python so you're really implementing
lazy thoughts in the sense and once you
claim that you use stash the result in
use it again yes
okay so the second benchmark illustrates
the use of edit HAMP's to improve the
situation and what we're going to do is
we're going to create a trace of edit
hams mapping z two snapshots of calls to
bar Z okay so every time bar is called
with 1 i'm going to have a map for key
one in the snap in the Edit HAMP to all
of the snapshots in the trace that have
that have that is the one is the
argument devour okay now that i have
that trace of edit HAMP's i'm now going
to create a trace of events when foo XY
is called such that the snapshots of
calls to bar Z have y equal to Z okay so
what I'm going to do is I'm going to
look up in the Edit hand the the
previous thing and I'm going to compare
it to the current call of a foo so if my
edit hamp contains all of these wealth
this is not showing the Edit ham but if
it contained this was my trace then when
I sifu 12 here I'm going to pull out
this call to bar too when i see barr 3
here i'm going to see these things are
the same and create this trace here okay
so i'm going to be computing all of
these things in my edit hams and then
i'm going to go through and post process
them to find these events that I care
about okay so what this shows is that
I'm going to use lazy trace again which
uses a regular edit Hampton and then I'm
going to have this strict map where I
use the Python map function instead
which is going to force me to
materialise all the trees and what we
can see is that when we force to
materialise all the trace the
performance is crappy it's basically
going to cause us to go every time I
force the trace at a particular moment
I'm going to have to recompute the whole
thing from scratch at that moment I go
to the next moment I'm going to have to
recompute the whole thing from scratch
again and so on which is couldn't cause
the time to be really awful here in the
case of expositor we're going to get
this nice slope here from laziness that
won't require us to have to do that and
in fact you know we're faster than just
gdb with no expositor overhead for the
first half of the queries and once again
the get before is a is a little bit
slower you can see this line ends up up
here for the the lazy map were yeah
it's much worse okay so here's the
here's my concluding technical
information slide which is the how we
used expositor to debug this memory leak
in Firefox so basically the problem with
the memory leak was that there was a
trigger to run the garbage collector to
give back memory to the operating system
and the trigger flag is set at various
moments and it can be set from multiple
threads so there's a lock that's
guarding this flag and one particular
place in the program that had to do with
an event handler set the flag without
acquiring and releasing the lock first
and caused the flag to get corrupted so
that the garbage collector was not
calling until some later time and the
way that we figured that out is we sort
of systematically started testing
hypotheses so we figured okay a memory
leak probably has something to do with
the GC so we first of all made a trace
of the garbage collector behavior we got
all of the calls and all of the returns
to the garbage collector so all the
calls are the breakpoints to Jay SGC and
all the returns are jsg see where the
index is minus one that's saying give me
the return snapshot and then we did
emerge to merge these two snapshots
together so we get the calls returned
call return call return and so on now in
the order of events when I run the
program I do the the first scroll and
that starts the process of allocating a
bunch of data and then I do the second
scroll after which eventually the data
is removed so those are shown at these
two times in the program and we did the
control sees like I said to give us
times in between those two times ok so
when looking at these things we really
we also realized that the problem was
that memory was not going back to the
operating system so we also wanted to
track the calls to em map &amp;amp; M unmap
which is the way firefox brings memory
into the process and takes it out again
so here we see there are a whole bunch
of em map calls to allocate a bunch of
memory and here the garbage collector is
called but notice that none of the
memory is getting freed so then the
garbage collector is getting called
again it's getting called again and then
eventually we see the unmap calls here
that makes the memory go
so the question is why didn't the memory
go away sooner but it took several of
these calls returns before it eventually
went away so after looking at the way
things we're working we hypothesized
that there what we noticed that the
garbage collector was being set to run
by a trigger and so what we do is we
looked at these timer events that
Firefox sets to set a time or wait some
amount of time and when the timer goes
to run the garbage collector at that
time in the future so here's where the
timer is set and here's where it
eventually runs and we used an edit hamp
there's a shadow data structure to map
calls for when the trigger was set into
when the garbage collector actually runs
and so you can see here here's the
trigger set here's the garbage collector
running that's where it is here after we
call the GC it sets this trigger again
which then it triggers the next one and
then it triggers this which triggers the
next one and so on and then eventually
the this free stuff happens okay so we
also looked at we discovered that there
was this this variable that says whether
there are chunks that are waiting to
expire or not and a bunch of these
events were happening in these locations
and we want we looked at the code and we
saw that when those events are happening
it ought to be the case that the map em
on maps are called and we've tried to
figure out why that was and then we saw
that there was this variable that was
set for multiple threads so we
hypothesized maybe there's a data race
on this variable so then we wrote a
simple happens before data race detector
that we call one lock so we know it was
protected by one lock them you know it
was one bet variable so we wrote a data
race detector that just was on the one
variable for the one lock and we
discovered sure enough that there was a
data race there and then that revealed
what the bug was
ok
okay so there's a bunch of related work
our design was inspired by functional
reactive programming so the idea of
functional reactive programming is that
instead of using callbacks you can have
time varying values so you can think of
our traces in fact other people this
tool mistake uses functional reactive
programming for the purpose of debugging
where basically you're going to have
your scripts and you're going to call
your script every time an event happens
and it's going to process it through the
entire execution of the program in a
similar way to our filters or maps or
folds are working on side of expositor
traces the main difference is that with
functional reactive programming time
always marches forward so there's no way
to go to particular moments in time to
learn what times are to go backwards in
time and all of these things are useful
during the debugging process and you
don't have them available to you in the
frp style you'd also don't have laziness
and so you're going to end up
potentially computing lots of stuff that
you really don't care about time travel
de buggers we build on top of these
we're agnostic as to what kind of time
travel debugger you provide although it
seems like replay debuggers are more
efficient than logging every instruction
you can view our traces as a way of
querying a stream right you're filtering
it you're mapping in and so on it's a
string query so you can look at all of
the research literature on just regular
querying of streaming data sources and
databases in the case of programming
languages there are two projects p TQ l
and p ql that look at writing queries
over program executions which they
implement via compilation they do it
with dynamic instrumentation so i think
that that that's great i would think you
could compile these languages down to
expositor i would say the main
disadvantage of doing that is expositor
is going to be slower than dynamic
instrumentation i would say in the main
disadvantage of dynamic instrumentation
is that many programs are non-trivial to
build and so if you have to compile by
dan instrumentation you have to rebuild
your program every time and that could
be very slow how many people how long do
you think it takes to build Firefox just
a guess it takes an hour to build
Firefox so every time I'm interacting
with the debugger if I want a new trace
I have to wait an hour to recompile the
program ok so I probably don't want to
do that
they entered this on the other hand
again you're going to get better
performance so it may be that there are
some usage models when one possibility
is you can imagine using this log every
instruction idea to log everything and
then once you have it all computing
locally within a log trace is going to
be much faster I think then using gdb
and undo DB so there's probably some
interesting trade-off there but I would
say that the main limitation of
expositor right now is that the the
speed with which it can process events
is limited by the speed with which gdb
and undo DB can provide them which is
very slow like it you know it can
process thousands of break points per
second okay well thousands of
breakpoints sucks if you want to
implement a race detector you care about
every read and write to a particular
variable it's very slow like I think
just to find this one data race where we
knew roughly where it happened and we
went just backwards from that moment in
time and it didn't it didn't go very far
back in time it took like 30 seconds to
find this this bug whereas it would have
been milliseconds or less if you use an
instrumentation based tool so really we
need a way of implementing faster watch
points or break points to do these more
fine-grained analyses and that's
basically going to be part of our our
future work okay so thank you for your
attention for listening to our approach
to debugging doing scriptable debugging
on execution traces the advantages of
this approach it's purely functional and
it gives you a way to build reusable
modular and compositional scripts these
the two scripts that I there are two
scripts that I talked about this stack
corruption one and the data race run you
can reuse for any program and you can
imagine lots of these scripts that
explore problems with I don't know based
on function calls or look for memory
leaks and so on you could code up as
expositor scripts and reuse and we use
laziness for efficiency and there you go
take off then should feel free but Mike
should be around the questions that you
are with time travel debugging he's got
a lot yes well it's true you're right I
was I was speeding through so but that
that is the most important aspect of
your work right because for example
consider the problem of a touristy but
it's very hard to get a repo every time
you want to find some other think it
happened in the past you have to means
to it then recompile react to get what
are the chances of the same show yes
that's right so I would say that we have
the same problem in getting the bug to
happen for the first time and I would
say you would want to compose with
something like chess or some other
active testing tool to help you get the
the non-deterministic bug to happen
while you're running expositor and then
once you find it yes then you can
interact with it and so on and so that
is definitely an advantage but we still
have the problem that you mentioned in
the beginning of well you have to get
the bug to happen and we might confound
that because of the slowness of running
with expositor yes then the chances of I
mean they will they do have a more
abstract notion of
and they're not just getting triggered
by all these random dining sequences
that's right the chances of getting
something that happens even in non
instrumented executions would be would
be higher yep I think another advantage
that we're hoping to exploit in future
work I'm I should have mentioned it
which is is that you this this approach
I think gives you a way of potentially
integrating with other sources of
information for example doing a static
analysis on your program our vision for
all of this is that static analysis even
dynamic analysis and debugging or
basically treated as separate things you
do this one or you do this one or you do
this one and there may be some
translation between these that happens
in the programmers head but there's no
translation between them in terms of the
tool suite so what I think would be
great is if you had some static analysis
results that said things about the
invariance between variables or things
like that you could use them you could
even use dynamic instrumentation to
generate results that you could attach
to these traces right so that this
information was available to you in an
end-to-end sort of debugging platform
that takes advantage of all sources of
information you can provide so I haven't
thought carefully about how we would
integrate all of those things but I feel
like once you get to the point now where
you can interact with a trace in this
functional reusable kind of way it gives
you this extra opportunity
look at the requirements for using
expositor with the debugger so for
instance you use it with GV to try it
with VMware's retrace and would work
with windows debugger I think it should
work with those all we really care about
is that the underlying tool gives us a
time indexed representation of the
execution so those tools will give you
that I mean you can just use the API
that's provided by them to essentially
the way we did with expositors we built
a python class that gives you this time
indexed trace and then it calls the be
calls and so on to give you that
abstraction I gotta believe you can
implement that abstraction for these
other tools so you could I think so yes
absolutely I was thinking you had this
thing where if I if I didn t exact the
execution and I said give me the
breakpoints but flew in the human break
points per bar you're going to each time
you're going to go back and read other
thing to build the traces balloon
football right do any of these debuggers
or is something that you're working on
maybe going to try to make the second
execution posture and it will be faster
it will be faster so what under DB does
is it will as I mentioned it will cash
so it remembers the whole execution so
it doesn't actually rerun it it will
replay all of the system calls and the
replay of the system calls is much much
faster than actually running the program
from the first time just because the
system yes yes just because you don't
have to take a do a trap to go into the
kernel and come back again yeah so under
DB is cashing it there they're
remembering a snapshot and they're
remembering all of the results of system
calls so that stuff is all cached and
when you go to rerun it it's it's going
to save all this information for you and
so it will make the rerunning faster so
one thing I didn't show you is you know
what's the space overhead of using this
so using a record replay debugger has
potentially significant space overhead
and if you wanted to dial down the space
overhead then you're basically it's a
direct space-time trade-off of how much
stuff you're going to have to re-execute
the same goes with the laziness so these
performance experiments that i showed
you are like five days old
and so we're still in the process of
figuring out what the space overhead is
but you know I expect that it's not good
and you know that's going to be part of
the trade-off but will optimize it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>