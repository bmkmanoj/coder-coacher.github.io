<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intersection Workshop - TBC | Coder Coacher - Coaching Coders</title><meta content="Intersection Workshop - TBC - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intersection Workshop - TBC</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Z3U6shQcz9M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
this is basically the artistic summary
of the whole talk now so if you already
get it you can already leave that's fine
i think i will be talking about what is
performance analysis and why ground
truth is important and how we can
generate rock and 0 first of all I will
put credits from a students down here i
will mention them every now and then so
you see who was involved in which
project there are many things like six
most important things to define what is
actually performance analysis and I
think everything starts with a very
accurate problem definition and I argue
that and then try to be provocative that
most techniques like object of lower
segmentation are not really defined well
and this is like more question to you so
please don't hesitate to challenge this
hypothesis but for example for optical
flow I'm very sure there's particle
image velocimetry there's medical image
registrations all the same but it's also
everything very different we have
different models and so on so this is
the first step then I'm pretty sure we
need to have the application in mind and
it depends on how we really define the
application one definition might be let
me just say we want to solve the problem
of constant albedo moving objects that
would be fine it could be an application
but another application could be a
driver system system and there are these
assumptions to not hold so we have to be
very clear about various assumptions and
we have to be which is also encoding
here but we also have to be very clear
about our requirements and I would say
that jumping down here the single or
scalar performance measure will not help
to evaluate all the requirements you
might have a set of ten or twenty
requirements for a optical flow
algorithm and some of these requirements
might be met and some of them not and
we're meeting
which of these algorithms can choose and
then there's three points so let's go
here so i will leave these out for today
that's just a few points i will quickly
talk about the model i will quickly
about talk about why we need reference
implementations and why they are
fundamental and why are they made why
they are research result we actually
need to publish along with paper and
finally there's ground truth and this is
the main part of the talk there is a
paper discussing all of these points and
I wrote this paper recently of s after
that Stewart from workshop and all these
guys have been part of working group in
this workshop and we said together
actually so I wrote most of the paper
and then we said together and refine the
paper and all of these authors basically
say we like what Daniel wrote here and
I'm like to strongly encourage you to
also read this because for interesting
is not strictly on an optical flow it's
basically on performance analysis in
general with application to optical flow
so why I'm talking about optical flow
all the time looking at how many
publications we have in the past years
everything started around 1918 and every
ten years the number of publications
doubled exponential growth I've said
I've been searching only for optical
flow no patterns only in optical
engineering computer science mathematics
so it's a google scholar search and you
will see right now we are at about three
thousand applications questions how many
of these publications meta how many of
them are useful or meaningful for us so
also had a look at each of these
publications individually and i found
out there are at least 1,400 methods so
new models how to measure optical flow
and i came up with this formula and my
thesis i call it the condiment formula
if you like it but it's summarizing all
of these optical flow techniques in one
formula so it's basically you just take
one vacation of
model here gun model here plug it
together and you have a new method this
is not perfectly true but I I really try
to capture all models I found local
models global models sometimes you have
neighborhoods sometimes you have motion
models relating to a parameter vector
which is love the flow but a fine model
for example you can have a sum of
different regular Rises with different
penalty functions and usually you either
have a sum year of several data terms or
you solve this integral as I'm
practicing my friend of here but
basically the same so there are many
methods and we need to choose the right
one and I use this formula to motivate
and the number of complications to show
that there's a big need to really see
which method applies to my specific
problem the next problem is
implementations during my thesis this is
also part of my thesis but it has been
reimplemented by my students in a better
way much better way we try to implement
many algorithms in this modular fashion
so we have this formula and we saw that
we can really black models together and
here for example this is the method by
clicking so it's the I think it's the
learning objective flow method and here
you see all the side effects you have
for example here we have the solver down
there's over which is just a linear
systems or by first creating a system of
equations and then we have some
iterations over these being a systems of
equations because you have to reel in
your eyes then there is a data term by
Connie Chung with a brightness model and
motion model then we have pure spatial
regularizer by be consumed and the data
term and so on and this is all one model
and in case we now want to change one
aspect of the full algorithm we remove
just a part of the model and replaced
with something else so everything else
remains constant the code is exactly the
same and this is a big problem that we
want to compare algorithms because if I
take men of implementation a and c++
implementation be they provide hugely so
we try to do this and I encourage you to
have a look at our open source there's
also a food through you
instruction pipeline in the code which
is very similar to visual as if n which
has recently been published it's open
source as well and there are many many
students these are three of my PhD
students working on this topic but there
are some outs of insurance and under
graduates working on this as well so
these were the first products and now
I'm going over to ground troops
generation which is my major concern at
the moment this is a scene i will show
you it's probably kind of copyright
digital rights management protected i
don't really know so i put the sauce its
movie cloverfield and i hope the fair
use whatever rules in Great Britain are
more legs than in Germany the movie
basically shows something where you want
to estimate different problems we see
infrared light sources we see artifacts
we see motion blur there's all these
problems and if somebody asks can you
can regenerate ground truth you always
have to ask ground truth for what and
that's not only for the application it's
all in for the type of data you have to
have to deal good and I I'd say for this
one we don't need ground truth because
we can compute anything at the scene so
let's have a look at the related work
there's the Yosemite sequence from 1994
here you see the seven frames of the
sequence and we all know this has been
sold that's what problem here just for
fun I googled and I'm you to bite on
this is Yosemite sequence so this is a
real Yosemite sequence it just shows
different conceptual difference we have
light blooming youth all this shaking
camera been getting and stuff like that
and this might also be a crucial part so
this ground truth is obviously not hard
for any longer it was very helpful for a
while but this real scene is very
so we want to go there next point is
then we had two thousand and I think
2007 or 2006 was the first publication
on the Middlebury benchmark so it's not
the john up version which your head and
I think my 2010 or something so here you
see one of those tests since the gross
the grove seam and this is a real scene
I found on YouTube again and here you
find a scene from the game crisis and
this game was published in the same year
and here we could theoretically create
ground truth and large amounts just by
taking the real time rendered images and
the complexity of this scene is much
higher than here but we still have to
ask the question this our application
domain imaging in the woods is it make
doesn't make sense or do we have flying
cameras do we usually fly into the
camera are these motions realistic for
our application to go that's very
interesting and I don't think I have an
answer for this problem and I think we
can now try to approach the problem of
generating this ground truth with
different ways so there might be
reference data without prompt truth and
it might be useful and it will come to
that in a second then there might be
weak one ground truth which is of
unknown accuracy for example we measured
something and we don't know what the
variance of these measurements was this
would be reground fruit and finally
there would be real ground truth so we
actually know the accuracy is like one
magnitude larger than the occurs if you
can measure that our algorithm this can
be achieved with graphics and this was
very impressive what I saw what Michael
was talking about on Monday so this is a
huge amount of work already it sounds
simple on first sight just create
graphics but create the content of the
graphics create the renderers or the
shaders for creating the ground truth
how to deal with anti-aliasing and stuff
like that there's lots of things in
there and then you still have to ask
about the realism of these seeds today
do they make sense for on occasion or do
they not and talk in a well get cut then
there's the idea of measurement
so if i have a measurement device which
is much more accurate than the data i
have for example i estimate optical flow
on a scene taking with an RGB camera i
might have a measurement device behind
it which gives me online through your
reconstructions and i can use these to
get ground truth and if i know the
security then m then I'm almost done wit
ground truth can be created with more
data so we can for example is in the
middlebury data set we created these not
we those guys at military created these
huge images using UV paint so they
sprays and painstaking procedure this
spray is UV paint onto the scenes then
record it the scene once with a 10 meter
with our normal life with regular light
and then with UV lights so we have more
data and we also have destruction there
then you have larger resolution can
downsample these images if your
algorithm is not biased if it were bias
your hand down simple for more you see
and that's that's another option to
create a ground truth and its really
very interesting and finally there's
annotations so it's difficult to create
annotations for ticket flows for each
pixel you want to have the sub-pixel
accurate location where it moved this
might be difficult but I'm actually
trying to find out whether it works
whether we can do it if you don't have
ground truth we can use experts some
people just look at the flow I'd say it
doesn't work that's wrong it cannot be
and then if you ask them by they will
very close to look at it and show the
boundaries of the of the depth
discontinuities and so on and they will
see that whether there is a reflection
or if there are multiple motions which
motion is estimated or is it just an
average of the motions which have been
estimated in this location so you can I
think you can rely on experts if you
have some type of application domain
where people are really experienced so
we created such a ground truth dataset
and I just quickly skim over it it's
really very related to Raquel's work she
also did an impressive job
I'm creating a huge data set and they
even created ground truth we did not
create well we just recorded data but it
was in a very specific way so we have 15
data sets and each data set create
exists of a number of locations these
relations are for example city town
Autobahn country and so on they are
triggered via GPS they are triggered
every time so we have these 15 data sets
recorded during four different during
all seasons monthly for one year and
then we recorded for each data set we
have this GPS tracker location so it's
always the same or approximately the
same location at at each location we
have 1600 frames and we recorded this
rhythm 112 so this high-speed cameras
which is quite large resolutions
approximately the same as regular
printed it and with 12 bits accuracy is
a high it's an industrial great camera
machine vision camera the data was index
and annotated we rectified the cameras
radiometric Lee and geometrically and
there are two datasets who are related
records data set and miscellaneous and
Peter dataset which is also very
interesting next problem is how do we
ask people to work with these datasets
most people say what's so much data I'm
not interested or the data is too
difficult or whatever so our first to
try now this so first try is this CCTV
workshop and I would like to invite all
of you to take part in this workshop
this year's ECC be and we also have a
robust vision challenge and this robust
wisdom challenge is about these
sequences so you see here mysterious
sequences we have downsampled them we
have complex scenes of different types
for example for this we had to cut out
this scene for privacy but for example
here you see these snow everywhere in
the car of throwing up the snow then we
have these night scenes
and winter and you have lots of
artifacts and actually what we want to
do is we want to encourage researchers
to look at stereo and optical flow from
different perspective this is about
detecting when these models cannot work
at all so in these sequences there are
situations where you will never estimate
a good flow or if you estimate a good
flow then you have very high knowledge
about the scene at the world and this
would be interesting perhaps for
confidence measures estimate when you
cannot estimate flow or for measured
methods which you just estimate the flow
but use high level knowledge photography
know this is a car these reflections lon
meta but these reflections on this power
will be very problematic if we want to
do a photometric comparison of pixels so
you will find our more details on this
URL or you can just drop me an email and
I also send this to image world and i
really be happy if you try to work on
this data again it's not about getting
very good results in terms of nice optic
throw reserves it's more about trying
out your method and try to find out how
your method can be improved to do what
should you take in such situations where
the model actually fails this big red
box says recalling a lot of data is not
selecting representative data so first
of all we have a condom problem we can
record all of the data in the world and
we don't know which of this data is
meaningful for our application or
evaluation the first problem but then we
still need ground truth and for this
sequence we don't have gone through so
what can we do one thing and this is a
really bold endeavor and I'm not sure
whether it will work out well but we are
in the fourth or 745 fifth iteration now
we take the data and there's a tool by a
silly you he was creating a
CBPR 2008 and she created a human
annotated ground truth which was
difficult and it was controversial and
it's still interesting I think because
he found that for example at each pixel
look at each object you segment the
objects first and then you drag the
objects after you track them inside each
object you try to estimate the flow and
these motion boundaries are the major
problem I'd say in optic flow estimation
so we created this mechanical turk
interface so you can take some use
software edit the first frame drag the
scene send it to mechanical turk then
it's improved or poisoned we don't know
and this is the interface here on the
web and then you can edit this and this
is this is the part this is an old video
from last year we have some newer videos
where we also do the feature matching so
we have interfaces for feature matching
and it's a major challenge as if we just
heard here to get this human into the
loop of this kind of image processing so
another option is to create ground truth
based on existing assets and that's what
we heard from from Michael Blagg what we
did we did a slightly different approach
this is Crysis 2 is one of the most
acclaimed games for its computer
graphics and degree of realism this is
real-time graphics we thought and you
see all these problems here is Sun and
so on so we thought what about the death
channel it just switched on the death
channel we ran the left and ran the
right views there are lots of problems
in this approach but we do have this
death ground truth and we can now let
playoffs again and play this game for
hours and hours and hours and they will
every time they will create new content
we can even design your levels put them
on a multiplayer server people go and
play on this multiplayer server and we
will get as much content as we need and
I think that's a very interesting
approach as well so the main problem is
creating content so
assets which really are meaningful and
in these games you can say about the sci
fi game so there are some assets which
doesn't make sense like UFOs or
something but I think it's also one
approach which might complement the
other approaches let me ask okay now if
you have these synthetic data sets and
real datasets without ground truth what
can we do do we really know that these
synthetic data sets are comparable to
two real datasets and this is very
preliminary work and we noticed there
are lots and lots of problems we publish
it in a small conference and we ran out
a scene and we recall it the scene and
we compared the two or it was sequences
so this is the real image and these are
different renderings with different
parameter sets it's all blender and very
simple rendering so we are no computer
graphic guys we just try to figure out
how how we can get with that one and we
found out that if we increase the
realism in the renderings and if you
have very good and geometry we will get
almost the same results from optical
flow compared to the real sequence where
we have optical flow but it's only on
average and if you really look at the
details there are systematic biases so
if we take a real scene we will have
other for example here this is
supporting Buddhist very different
material it's like Edwards so there were
the Lakers we will get reflections and
in other regions we won't get
reflections innocent uses motions if you
simulate this with computer graphics on
this primitive simple manner as we do
this will induce of motion artifacts so
I still think this is a very interesting
approach but it's a very long-term
project I'd say and I'm inviting
everybody doing computer graphics to
assist us in this thing we can do hiya
curiously 3d scans and we have lots of
devices from structured light to lie
down to whatever you need but we don't
know much about this material capturing
or you
materials and so on so first problem was
we need more geometry this was
handcrafted geometry what can we do now
there are two small projects we can Oh
first of all we can create create ground
truth by manual measurements and this is
from my point of view the best way
because it's the most accurate way if
you do many of measurements for example
take this box we call it the heidelberg
box you see all of these discontinuities
here at the boundaries and no 3d scanner
can really capture these discontinuities
if the scale is too large you can use
structured lights candles and then gets
very nice founders but a lighter won't
work and many other metrics won't work
as well so there's high end structured
light scanner which is like 50 microns
accurate could get it but it will take
hours and hours or days and days to
really call it the size is about 1 1
metre fifty times 10 to 15 or something
like that so we fitted this model
manually we took this primitive shapes
to facilitate the process and then we
create its grant through the depth maps
for example so this is one one set of
grant food so then the problem was ok we
use if you slide our men your
measurements or introduce UV paint as
middlebury that takes a lot of time
lidar especially we scan to rule with
Vega and it took the whole day just 45
46 views and he still had only a point
cloud and then we intend to reduce the
reviews party manually and that's a lot
of work it's and it's not fun at all and
we want to create a lot of grant rules
not just a skin of a single room so we
thought about connect fusion and cosmos
at our lab and I talked to him about
that stuff and suddenly we had a project
so what did we want to find out we
wanted to find out how to connect
correct actuators it how how does it
compare to a lidar scanner or something
and we took three reference objects one
is 30 centimeter large we scanned this
with a structure light scanner which was
like 50 microns accurate I hope this is
15 microns I always miss this zeros then
we have this box I already showed it to
you but finally we have the slider scan
of our office this is one of our offices
here you see this is a volume of about
seven meters by certain leaders by
something and let me use connect fusion
to actually scan these rooms and we
compare pictures and here you already
see ok for this such small scales it
doesn't really work well so we had a
look at the arrows do you see the
scaling so up here blue is a small error
read as a high error and then you have
these eight millimeters 15 millimeters
so different stadiums here you see there
are small errors actually but the detail
isn't captured so this is the lower
limit of what you can get that the
current is makers the current connect
has a minimum distance of point eight
meters and this is one limiting factor
and it might also be other factors but
in this box we saw ok the arrows are
about 50 millimeters we looked at the
statistics using the phone statistics
here we for this rumor we only had the
point cloud we couldn't compute the
distance to the mesh so we had to
compute the distance to the nearest
vertex and you about that but you see
that the mean here here the mean is
about five millimeters no
centimeters here and here 10 and here we
have nine billion be directors let me
see the problem of the slightest can the
lightest canis only five to 10 million
accurate so we are you can be order of
the same magnitude so we can this is
just a sanity check whether we are in
the right location oh no here I was just
wondering this is why zeros I know this
meters here have to look it up again I'm
not sure about this I think we get here
we have this 0 to 250 millimeter sarah
said you see all these errors of the
boundaries of the object where we don't
have actually data actual data and this
is kind of problem but with this box and
with this feasibility or our sanity
check we can see we can get around 10
millimeters accuracy I would say that's
very interesting and this was one
important result I think and the next
question was okay can we improve the
quality so we still are in the range of
10 millimeter sometimes more as a
statistic where you have a median or
average but that's not all we need so
the limits of connects out there at
least so we thought okay let's take turn
off lights turn off light has to hunt
this one has 200 by 200 pixels size this
is compared to full HD roughly full HD
so we try to observe all this and found
that doesn't really work that well so we
had to do a sense of you so what we took
the original image just scaling it up
looks like that next version is using
something like sgm this is SG emissary
global matching is just using the stereo
information
lots of systemic problems for example in
this region here now we take the time of
flight data and use this as input for a
stereo plug match only this just try to
be as simple the algorithms as possible
and as complex with a hardware as we
needed to so that's what you see and if
you then regularize this with a with a
second order TV you will get something
like this so this is our next approach
or perhaps I have been abated here
compare this with ground truth before
out we do get some some relatedly good
results but the errors in the order
centimeters now for a single view if you
take neck fusion put this into the
pipeline we hope we get small arrows
until the sexy of two seconds per frame
and the global method 90 seconds per
frame virus team still needs materials
we need light sources we need Center
simulations or render us and this is
something we cannot do so we do have
this light-filled project going on by
other guys at the HCI with developer sin
banar McCarthy era they are currently
trying to play around with please
stanford's like gantries we acquire to
Rachel explain of the cameras we built
this later this light don't go near
reflectometer which is you can change
the light sauce you can also so you can
basically change the spectrum it's
relatedly fast and you have only one
camera and different rotation angles
problems calibration we will see how far
we get with this and let me try to build
specialized hardware like for example
the step edge camera which is based on
the work of mosca in sikkre of 2004 but
we use a color camera and multiple light
sources and we get these controls on an
object and from these cast shadows we
can find the discontinuities of the
depth which is also very interesting
because discontinuity is in motion very
often correlates with the
discontinuities depth as well and we can
try to
this information to create better ground
truth one point I left out completely
thus my conclusion one point I left out
completely is this interactivity stuff I
dropped it for today because I can show
many results on that but we also taught
okay if optical flow computation or a
segmentation or any other correspondents
estimation would be really interactive
so we have the user in the loop and we
can afford that we can get much better
results so to conclude my talk grant
Ruth generation is the loss are a lot of
trade-offs and there are many options
and there are different kinds of ground
truths and we have to decide what we
need we have quality versus quantity how
much ground truth do we need how good
does it have to be we have geometry
versus audiometry do we really need to
take into account this information the
light fields or whatever or if hardware
was software we can build hardware
systems which record the ground truth
which are very specialized and extremely
expensive and then we can try to see
whether we can get this with a small
camera speed was a security our
algorithms for ground with generation
kind of slow and you can have a human in
the loop measurement accuracy is
important for real data and predictive
rendering like having physically correct
models is important finding synthetic
data or is it we don't know and then
finally generalization versus
specialization I think this is a very
important point if the estimate optical
flow for a given application we can
really tune this model we want to fit to
the data for this application if you
don't know any application if you want
to have a generalist model it will not
perform well and all it will not perform
well on all these specialized cases will
always be some average quality algorithm
so that's it thank you very much for
your attention and I hope there are any
questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>