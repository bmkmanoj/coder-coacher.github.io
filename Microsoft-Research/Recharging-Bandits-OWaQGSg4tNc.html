<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Recharging Bandits | Coder Coacher - Coaching Coders</title><meta content="Recharging Bandits - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Recharging Bandits</b></h2><h5 class="post__date">2018-04-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OWaQGSg4tNc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
okay welcome back everyone for the
second section of this serie day so we
are very happy to have a nickel in
Malika from Microsoft Research telling
us about recharging bandits thank you
this is joint work of Pavia klineberg
from Cornell okay so I think we all here
know what the multi-armed bandit setup
is you have a bunch of arms each arm has
a reward that you get when you choose to
pull it and the reward is drawn from
some unknown distribution and your goal
is to play around with the arms a bit
until you you know get a lot of rewards
out of the arms and the thing about the
standard setup here is that the
distribution of the reward of an arm is
fixed over time but in many settings of
interest the distribution is not fixed
but rather is a function of the idle
time since you last pulled the arm so
you might think for example about music
maybe you really love nirvana and you're
very excited to hear it and if you
haven't heard Nirvana song for the past
week you're getting anxious and you're
really missing it and so you really want
to hear it Nirvana song and so this is
going to be the extent of my motivation
following you Vols recommendation that
there not be any motivation so in the
recharging bandit problem the
formalization that we have in this paper
is that the reward that you get from
pulling an arm is a function of the
delay since you last pulled it so
there's some function H sub I which is
the the expectation of the reward that
you get if you pulled it D days ago
let's pull the arm T days ago and we're
going to assume that this function is
increasing so rewards accumulate over
time and that it's concave which can be
thought of as meaning that pulling an
arm in a time step when you anyway
weren't doing anything is better for
your payoff than not pulling it and
again your goal is to maximize your
total expected payoff so if I have a
delay of like 10 days between two poles
of the arm I could insert a pole of that
arm in between the two poles and now
I'll have to five-day delays and that's
better than having a 10-day delay so I'm
gonna discuss the problem you know
finding the optimal schedule of when to
pull arms when I assume that these
payoffs H sub I are known this turns out
to be a highly non-trivial problem
unlike in multi-armed bandit swear if
you knew all of the rewards you would
just greedily pull the arm that gives
you the best reward and every time step
and so after like solving this problem
is going to be related to a nice problem
in combinatorics which I'll have a short
interlude about it's got a fun open
problem that I like to think about which
you can you can talk about in your
cancelled open problem session and then
we'll return to solving the known payoff
setting and then at the end of the talk
I'll discuss how to transform that into
a learning algorithm for the case when
these H functions are unknown and must
be learned over time all right so the
sort of first thing you might try is a
greedy algorithm at every time step I
can look at the reward I would get from
so I'm considering where I know that H I
so in every time step I can just look at
the delay since I last pulled each arm I
I understand which one has the largest
current reward for me just pull that arm
and this is a policy greedy and I guess
maybe it's not too surprising I'm going
to skip the proof but it shouldn't be
too surprising that this is not going to
be optimal there's a lower bound that we
have in the paper showing that it can be
arbitrarily close to 1/2
yeah everyday just give it to you you
don't know what the new fans this
function H it's fixed over time yeah
what's changing like you know you the
amount you get in a particular poll
changes because there's a different
delay since you last pulled it yes so
I'm studying this is a known problem
this is a commission yeah it's all
completely a problem yeah I will later
make it into this bandit but so then
there's the screen algorithm which just
says for each arm what is the reward I
would get if I pulled it today and this
is a function of when I last pulled it
so and I know that so I can follow the
greedy policy and greedy is going to be
could be as bad as half opt and the
positive results is that this is in fact
tight so the ratio of greedy to opt is
never less than 1/2 so greedy is a to
approximation and I'll prove that for
you do you guys want me to go over the
negative result to make sure you
understand the problem ok let's go back
a slide then
so the negative result is saying that I
it could be as bad as a factor 2 so
greedy is going to say always pull the
always play the song for which I have
the best payoff today so you could
imagine that the payoff function of the
Beatles is if you haven't heard the
Beatles in T days your reward for
hearing the Beatles is T whereas for
Nirvana if you no matter when you last
heard it your reward is some fixed one
minus Epsilon now the greedy policy is
each time you pull it this is what you
get but but each day is just T since the
last time it's not like the sound of it
previous day's you're not adding some
money to the pile every day yes since
last book yeah yes the marginal will be
little each so the greedy policy will
pull the Beatles all the time and ops
will play Nirvana 40 time steps allowing
the Beatles rewards to accumulate and
then pull the Beatles arm and get a
total payoff of 2t approximately so this
is a half approximation great so the
first thing I want to prove to you is
that this is Tigers yes I want to prove
to you that this is tight and in order
to do that I am going to define a new
policy called enhanced greedy so the
greedy policy has some schedule maybe it
plays the Beatles all the time opt has
potentially some other schedule it plays
Beatles Monday Tuesday Wednesday and
then it plays Nirvana and the enhanced
greedy I'm going to allow it to pull two
arms in one time steps so this is a you
know hypothetical policy that I'm
imagining I can't actually do this but
let's suppose that I allow this enhanced
greedy to pull two arms in one time step
and what it's going to do is it's going
to whenever opt and greedy agree it's
gonna play the arm that they agree on
and when they differ it will pull both
the greedy arm and the optimist wished
it's an inner liner and then I don't
know if it's consistent with
you're saying that I I claimed that one
yeah I see yeah well this is now like
proof of the positive result so let's
assume that they had totally different
payoff functions and this happened to be
out you can you can also yeah I mean
this could be a Nirvana cover you know
John Lennon you're not buying it
and we have a pee test and we do not
know if it's np-hard I so ok so this is
the enhanced greedy it's going to pull
the arm that they if they agree it pulls
the arm that they agree on and when they
disagree it pulls both arms and it gets
credit for both arms and now to prove
the two approximation notice because
greedy is pulling the best arm for that
time step the enhanced twice greedy is
at least enhanced greedy because greedy
only gets to pull two arms and each time
step and enhanced greedy pulls two arms
in each time step and greediest pulling
the best of fine and then by concavity
so this is where I'm using concavity I
you know enhanced greedy might be
pulling arms that opted and expect to
get pulled but that doesn't matter
because of concavity inserting polls in
between arms is only helpful so enhanced
greedy is better than opt and that gives
us the two approximation for the greedy
scheduling algorithm okay so we want to
improve this and the first idea that we
have is to define what we call this rate
of return function and so the rate of
return is going to be the maximum
long-run payoff that you can get from
arm I if you're allowed to
an X fraction of time steps so let's
just focus on one our my and suppose you
can schedule it for only an X fraction
of time steps then which time steps
would you choose for that arm in order
to maximize the value that you get from
that arm that's this function art and
one thing that we can observe about R is
that it's concave and piecewise linear
so why is that well the optimal sequence
has two distinct gap sizes in order to
prove that let's suppose there's an
optimal sequence that pulls nirvana for
out of what is this eight so half of the
time steps and it doesn't have equal gap
sizes then I can because of concavity
improve this schedule by equalizing the
gap sizes so I've lost going to here to
here
I lost a gap size of zero and a gap size
of two but I gained two gap sizes of one
and by concavity this is better so this
is intuition and you know you can prove
it from here that the RI function is
concave and piecewise linear because I'm
asking you to do it up to some bounty so
you might not be able to pack yeah yeah
yeah you have to you have to mix between
these two to get to your gap to get your
capital T so thanks device and would it
just be one care yeah
capital I think of TS my time horizon
yes and as the number of arms okay so
then that gives me the following concave
relaxation I want to maximize my total
reward which is the sum over each of the
arms of the reward of each arm subject
to
being able to schedule them and one the
relaxation now is saying well certainly
I can't ask for the frequencies to be
larger than one like the sum of the
frequencies to be larger than one that
would certainly be not schedulable so
this is my concave relaxation I have my
piecewise linear our functions and it's
easy to find the solution to this
concave relaxation it gives me the
frequencies with which I want to pull
each arm so in this example it's telling
me that I want to pull arm one one-sixth
of the time arm to one-third of the time
and arm three one half of the time okay
that's great I have now the optimal
frequencies and I now need to round this
concave program so I want to get a
schedule that hits these target
frequencies and pulls each RMI at least
one over every one over X I time steps
okay now this is where we get into this
cool combinatorial problem which I you
can think of as scheduling songs with
particular frequencies this is known as
the pinwheel scheduling problem so
here's a puzzle for you let's suppose my
concave program said I should schedule
Nirvana
at least every two songs ie its
frequency was one-half scheduled Beatles
every three songs and schedule Pink
Floyd every five songs can I do that
okay your deadline first so you're gonna
do you're gonna do Nirvana and then
beetles and then Nirvana and then Pink
Floyd world war beetles so what exactly
by MV in every interval of length - you
have to have at least one Nirvana and
every interval of link 3 you have to
have at least one beetles they sum to
less than 1 that is definitely a
constraint but you can't do it because
in the first 30 songs you're gonna need
15 Nirvana's 10 beetles and 6 Pink
Floyd's and that's 31 songs which is
greater than 30 and if you start trying
to write it out say by this you know
closest deadline first you'll run into a
problem okay so oh I'm sorry
I guess those ones to sum up - great
yours yes
so that is definitely a constraint they
have to sum up to less than one here's a
bunch that sum up to less than one okay
so here you can do it and this is a
schedule I'll make you guys come up with
it here is a bunch that sum up to less
than one and you can't do it so I want
to schedule Nirvana every two songs so
every other time step must be Nirvana
and then I have to schedule beetles
every three songs so I have to have
beetles in between all the Nirvana's
which leaves no time for Pink Floyd
I thought about changing out the songs
to make them more palatable to you but I
thought no one else would recognize it
yeah so practiced you like how binding
is this grounding oh yeah that is
definitely this is an interlude because
I don't need to do this exactly and I
will show you how to do it approximately
most applications like you're not
playing bought out the two songs you're
playing at every hundred songs asks the
conjecture is that you can always do it
if the sum of frequencies if and only if
the sum of frequencies is less than five
six people think about it I don't know
yes this is the classic conjecture yes
this is not our conjecture this is well
established in the literature is the
classic different what yes
he's possibility at all of you do these
bases or something yes like what happens
obviously one half is obvious because
you can do like put it all into a binary
tree and round everything up and then
like one when you Brown one half up it's
still less than one and so now you can
do it I think five sixes it's easy too
okay sorry five six plus epsilon is I
showed you on the last slide that's not
possible and less than five like getting
it to show that those you can do it with
five six is the classic conjecture and
one half is easy and I think people have
done something between one half and five
six like 3/4 I think is known yeah so
only it you know it like what happened
quarter one quarter
oh yes yeah yeah okay so please which
constants ensure that any oh yeah yeah
it's true that if all your frequencies
are powers of two you can do it and they
sum to less than one okay
so cute problem but not super relevant
because what I really need to do is
around my concave program and I don't
need to solve this problem exactly yeah
you have a question Pink Floyd songs are
too long anyway back to our scheduling
problem I need to find a rounding for my
concave relaxation I have my target
frequencies and here's a way I could do
this I could use independent rounding at
each time step I'm gonna pick an arm
independently with probability X I okay
so this is a very natural rounding
scheme and what's it gonna do for me
well the delay of an arm let's call that
tau sub I when I pull it is geometric we
distributed and it has expectation one
over X I and so my rounding is going to
get X I fraction of the time it's
playing arm I and when it's playing RMI
the distribution is identically
distributed across time it's tau I so
it's getting expectation of my H
function of tau I every time I pull it
the relaxation is in fact getting X I
times H I of so I sort of went over this
fast but I had told you that the RI
function is equal to X I of H I of 1
over X I and so my relaxation is getting
this so what did i do I switched the
expectation and the H let's is concave
and so I'm getting a 1 minus 1 over e
fact
and through this independent rounding
scheme and intuitively like if I want to
improve this what's going wrong is that
this random variable tau I is to diffuse
I need to that variance is too high I
need to cut down on the variance to get
a stronger approximation here and so we
do this through a technique that we call
interleaved rounding and an interleaved
rounding I am going to schedule arms in
continuous time at a regular rate so I'm
going to in continuous time I'm going to
pull arm I exactly everyone over X I
steps so that's the target delay I'm
looking for and I'm going to push it by
a random offset so that arms don't
collide so this is a random offset say
between 0 and 1 and so that's that's how
I'm scheduling arm I so for example I
have this purple arm maybe 1 over X I
was whatever the distances between these
purple dots I schedule it every 1 over X
I steps I do this again for the blue arm
and so on for all the arms and I get
some schedule of the arms in continuous
time they don't overlap because of this
random offset now how do I get from this
continuous time world to my discrete
time world I'm going to very simply map
the just can you attend you as time to
discrete time by just preserving the
order so that gives me some schedule and
I claim that this is a lower variance
rounding scheme and that will allow me
to improve my approximation so why is it
a lower variance rounding scheme well
I'm now I need to calculate the delay of
our my when I pull it the delay of our
my is a function of the number of arms
that in continuous time end up between
two poles of our M I so for each J let's
let ZJ
be the number of times that arm J ended
up between two poles of our my in
continuous time well ZJ its expectation
is the length of time between two poles
of RM I is 1 over X
and CJ is getting J is getting scheduled
an XJ fraction of times the expectation
of ZJ is this and you can also not
without too much trouble argue that it's
supported in fact on two points and this
is the same argument as before and so
that's great that's now what I can do is
use this cool definition of an order of
random variables that we found in a
paper of costume
maxime survey denko which is called the
convex stochastic order and so you say
that X is less than Y in the convex
stochastic order if for every convex
function the expectation of that
function of X is less than the
expectation of that function of Y and it
turns out that if X is a sum of
independent Bernoulli's which art a
light consists of and Y is plus on then
X is less than Y in the convex
stochastic order and so I have this
lemma which is great because now I can
look at the contribution of our my in
this concave relaxation I can bound it
using this convex stochastic ordering
property my the thing that the algorithm
gets as before is the expectation of H I
of the random variable which is its
delay and by the convex stochastic order
remember this is a concave function so
by the convex stochastic order this is
only larger than this thing and now I
can use a fact about concave functions
of Poisson random variables to get a
proof of a 1 minus 1 over 2 e
approximation which is great but that's
not a great improvement over 1 minus 1
over e why do I care why I care is
because the same technique can be used
on small arms only to get me a 1 minus
epsilon factor approximation so here a
small R
an arm that's pulled only an epsilon
fraction of epsilon squared a fraction
of time steps and this is very useful
because I can now to use some sort of
brute force search over large arms
combined with this concave relaxation
plus rounding scheme on the small arms
to get a PETA's for my problem so what
do I do
I guess which arms are big and there's
at most one over epsilon squared of them
so I'm thinking of epsilon is a constant
here and I then I I also decide how many
gaps I want between big arms in which
I'm going to stuff the small arms and
then I so I search over all such
schedules by brute force and then I use
this relaxation that I told you about to
stuff the small arms into the gaps
between the big arms so this is a very
technical result I told you the one key
piece of it which is the the concave
relaxation that we use for scheduling
the small arms but there's many other
things we need to deal with here like
the gaps might not be evenly spaced
let me stuff the small arms in there so
the gaps might not be evenly spaced and
we have to deal with that and we have to
make sure that there is a regular
scheduling of the so we only search over
periodic schedules of the big arms we
have to make sure that there's a
periodic schedule of the big arms that
gives us enough payoff so there's a lot
of details you can look at the paper
which does not exist yet to learn about
something pinwheel conjecture does it
implies that there is an energy gap
between the relaxation and yeah so okay
yet you can still get to 1 minus 1 over
2 e just by rounding the relaxation the
gap is not the 5/6 from that but we can
use pinwheel scheduling to find a gap
and the gap that we find is 16 over 17
think with like that's the best thing we
were able to construct so there is a
variant of penis difficult you know to
teach them on schedule so you've given
these numbers si you can schedule so
that the interval P the number of poles
of our my is within one of P times X I
so that's actually non-trivial factor
theorem from nineteen eighty so that's
almost the pinwheel but not it doesn't
satisfy condition that's possible just
provided the sum of the X is less than
one so there's no gap for that and
that's this deadline nearest deadline
first
video 30 years and one so small pot
money card earliest earliest earliest
deadline first day so earliest deadline
first actually a special case that was
discovered in the seventies which is
only for X I which I want to integers
and the general cases for 1980 but
so I wonder if you plug that into your
own volume so that condition looks like
me as good at the thing I mean I need to
found I need this delay to be if I ever
have an unbounded delay I need to I
might lose a lot X the number of
products is different but no so the
number of peppers by it both the number
of Chris's Bieber's by the most one from
you know from from what you what you
want so like just in the example of the
laptop you might have to wait I wanna
you know it's just a Korean very well in
that thing and you just rotate once at
the last time so the time you're waiting
depends on use the time horizon I think
we should take this offline because I
think I'm out of time
but I had a whole third section with the
talk on unknown payoffs and let me see
what I want to tell you about it if the
payoffs are not known then you know I
don't have I don't know how to solve
this problem optimally so I'm gonna lose
some factor in my regret bound so I'm
gonna get something like alpha where
alpha is the approximation ratio of
whatever algorithm I'm using to solve
offline problem times opt minus some
regret bound that's gonna look something
like this and in order to do this I mean
one problem is that these scheduling
algorithms are counting on the future
when they decide what's a schedule and
so a standard trick to deal with that
would be to divide time into planning
epochs and in each epoch you commit to
some scheduling policy and then you
repeat and okay so we can do that too
but bigger challenge is that our
approximation algorithms are assuming
this H function is concave but if you
want to do something like use upper
confidence bounds on what you think
these HS are they're no longer concave
these upper confidence bound phone
and so we need to deal with that in
order to do that we have a black box
sort of semi black box reduction so we
define an augmented reality we say if
the algorithm works well in this
augmented reality then it will also be
work in a regret in a multi-armed bandit
world and I think I will not tell you
what the augmented reality is because so
the h's of function is the expectation
of the reward of that arm and and we get
some augmented reality properties if
your algorithm satisfies those augmented
reality properties then there's a
reduction that you can plug it into to
get a low regret algorithm and as a
result our P tasks can be plugged in to
get lowered right algorithm so in
summary I absence makes the heart grow
fonder and so you need to deal with that
when you're scheduling your bandits the
recharging bandit problem the payout of
an arm is an increasing function of the
delay when it was last pulled we have
fast approximations that are constant we
also have this P test which has a
terrible running time because we have to
do this brute-force search over the
large arms and then you can plug any of
these things into our blackbox learning
which uses upper confidence bounds to
reduce to an own pay off case and that's
it
in question now is it easy to explain
what of all convex functions of
Poisson random variable to yield yeah I
used exactly
oh you mean to prove that bound I used
this fact yeah sorry yeah I proved this
from the paper
wait this is a lemma we proven the paper
moves I don't know can you into concave
age yes okay yeah can you say like a
sentence about what your ironing is
gonna correspond to if it's not it's
actually for the pts so the pts is
working with this art thing and so it is
going to instead of feeding in this this
should be a bar over this thing so
instead of feeding in this thing it's
going to assume the irons function here
and then the pts is going to round based
on this ironed function and it will just
never schedule things that have this
interior gap so it's going to push all
of the if they wanted to schedule
something here the rounding is going to
push it off to the endpoints and you
have to show you don't lose much by that
which is super technical and is the
reason the papers not on archive yet</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>