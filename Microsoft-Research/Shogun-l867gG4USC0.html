<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Shogun | Coder Coacher - Coaching Coders</title><meta content="Shogun - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Shogun</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l867gG4USC0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi my name is psycho I just had a PhD at
the Gatsby unit in London do research in
neuroscience and machine learning and
going to talk to you about an open
source machine learning through box
called the sugar and machine learning
two books so here's some small bits
smoked some small bits about children so
we on open source to works it got
initiated by in 1999 by gonna retch is
here and the student of his all kind of
of his back then and the library first
made public in 2004 so it's quite a spin
around for a while but these days it's
more it's more of an open source
community project so there's no big
group of the company behind it currently
but we're still with like eight people
eight core developers that are kind of
constantly you know active every day and
about 20 people that are active on a
weekly basis or something like this and
i want to emphasize it because most of
these projects have kind of these super
super high profile and supporters we
don't have those which is an open source
project what mainly drives us where man
power comes from is from google Summer
of Code where we have quite some impact
we've done 29 project so far this year
some of co just ended on Monday I'm
going to talk a bit more about this in a
bit um I don't know if you know the
website hola that's like a little data
mining website that crawls github and
produce statistics so we live 126 people
have contributed code we business lines
of code without comments with comments
we just hit the million we've written in
C++ or like like this comment here I'm
going to talk a bit I'm going to talk
more about the the how you interface
with a library in a bit but I like these
things that kind of the estimate how how
are the kind of the development
intensity how stable it is over time so
we have with active development of a
long time now not so this he has a very
nice number they fit some models and
produce the estimated effort
okay here's here's our code base
developed listen I joined and this is
when people decided that half my coat
wasn't so good but yeah we gotta we
gotta quite nice increase in trend and
yeah I mentioned we are mostly written
in C++ but we got some nice interfacing
going on and talk about it so the
library itself is mostly just like
things like scikit-learn it's mostly a
collection of black box algorithms that
you can apply to you to your data
problems so you know you open your
textbook and you want to learn about
machine learning you first see okay
supervised learning we get some pears x
and y we want to predict some labels for
something we've seen and we want to
train our classification methods on this
so all these methods all these textbook
methods are implemented in sugar and all
of them are implemented under kind of
nice unified interface so that it's easy
to use them easy to compare them easy to
learn about them same goes for a
regression so um originally back when
when the library was initiated so
support vector machines was what's the
hype today it's something else and so a
bit of bit of focus traditionally in the
library was on the support vector
machine so we got some super
cutting-edge implementation in there and
kind of hold the torch for like the most
most largest collection of these methods
but now we also got GPS and go get
cigarette and you know easier things and
same goes for regression and then goes
for all these methods for unsupervised
learning that you want to try out when
you first deal with your data science
problem like all you know machine
learning so don't really need to talk
about this um but an interesting ones
maybe this one here so one of one of our
developers is this guy who lives in the
NOAA in Russia he really liked the
matlab toolbox for damage dimensionality
reduction so we just cloned it we got
all these methods also in children yeah
lots of things like I see a and you know
this like oh this kind of it's almost
like a pre-processing steps if you do
data science it's quite useful to have
them and this is all under unified
interface so apart from these textbook
methods also we got some researchers
putting that
odisha goon for example I do i do
research in kernel based hypothesis
testing and all this like proper
large-scale hypothesis testing all this
goes in children we had people like the
sky from Germany Maris cloth he did lots
of research multiple colonel learning
all their stuff is in shegaon recently
we integrated some more sophisticated
methods on structured output a metric
learning through some of code currently
I'm working a lot with a guy called mts
con whoo-hoo every year releases three
new variational methods for latent
Gaussian models and we we got them in we
got them in children we got a deep
learning project recently where you can
we can stack up layers of neurons
connect them and press the button and
you get a new network and you can train
Egypt gradient descent so that's quite
nice and another thing we have because
we're we've written and see so we have
the ability to interface with other
projects so for example in 2011 I think
we had a son of code project where a guy
wrote an interface to VW it's a bit
older these days but I think this is a
great opportunity to maybe collaborate
also we have an interface to lib linea
so it's very easy within chigan to run a
classifier from VW and lip linea cross
validate them and compare them it's just
a couple of lines of code because all
this is embedded into the interface um
we got recently started writing some
super nice tutorials using ipython
notebooks they all on our websites we'll
just have a look there's about 20 of
these notebooks we can see like some of
the cool stuff we can do and we require
Google some of code students to write
these kind of reports ok so um she only
had like the focus on large-scale
learning but back 10 years ago
large-scale meant something different
than today back then it meant that you
can deal with lots of data on one
computer which means you want to do
proper programming you want to do proper
data structures and you want to be as
fast as possible on one computer so some
of the examples that we have insurance
for example splice I prediction example
where you know we do classification on
50 million examples of 200 million
dimensions because we do things properly
and we have the ability to do this
because we've written in C++ we also can
do like super nice face recognition
tasks we currently also working on
extending or working we are more
thinking about how we can go distributed
computing in a nice way but that's all
projects for the future I'm going to
mention this later bit so something
that's really important if you if you
have all these black box black box
methods is that you somehow have a nice
modular framework to connect them and to
be still get like some reasonable
flexibility because well you have black
box methods so but one thing we can for
example do unlike for example other tool
boxes is we have a very nice way to
represent data so you can we just have
this data object and you can you can you
kind of matrices you canna sparse
matrices you can have collections of
strings you can define streams like for
example the VW framework works and on
their own the stream framework and in
theory most of the algorithms that we
implement we kind of make sure that you
know this is modular that you can that
you can plug things together in this way
you can also change data types with this
sometimes important especially if you
don't have lots of computer memory you
can just say okay yeah well if you if
you train if you train a GP regression
you have to invert your kernel matrix so
you know if you like for this you need
to hold it a memory there's no way
around it but further things like for
example if you want to do your
stochastic gradient descent SVM and you
can you can either you can give it a
matrix or you can give it a stream
handle and the stream handle can stream
in vectors or can stream in strings
letter than pastor Colonel like that ok
it's also super easy to change the the
data types that's useful I think and we
also got a bunch of pre-processing tools
like all this dimensionality reduction
that I mentioned this part of the
pre-processing framework who's got other
things like you know this like d
correlation removing the mean filling
missing data these kind of things yeah
you want sometimes when I evaluate
methods so
things for for doing cross validation
computing area under the curve other
things in there and this is again this
is this is general so as I said earlier
I can super easy cross-validated GP
against linear classifier without any
hassle which is nice and also select
parameters based on on route force or
doing some kind of gradient optimization
and we also support various file formats
from the low level which is kind of nice
because um it's unified under all the
interfaces which I'll talk about now one
a bit first first some some technical
more technical details is so we've
written in C++ and be written in what I
calls proper C++ which means the entire
framework is written in this it's not
just that we the we that we take like
very isolated methods say matrix
multiplication and do it in C++ like you
can do it in Python we built the whole
framework in this and this means that we
can be a bit more efficient in the sense
how we how we set things up we can have
a high throughput of objects without
having problems about this well the
downside is we have to deal with memory
by hand and we got some tools for this
but this this comes this comes for the
with the advantage that we can build the
whole framework in this in this in this
see language which makes it quite slim
to use and also we can have this unified
interface under all the languages yeah
and another thing I mean people have
been talking about this a lot but we
also you know we use I ghen lapack
mainly for linear algebra and also MP
openmp and threading but what we're kind
of trying to do with be trying to
isolate the back end for computation
from the algorithms so we've like a like
a little you know set of operations
linear algebra save matrix
factorizations and then you can you can
decide when you install shubham whether
you want to do this on your GPU or
whether you want it on your cpu or
whether you want some hybrid or maybe
somebody comes up with a more clever
implementation then we can do this also
without having to change the algorithms
so in this way we try to be a bit
sustainable if you want to get an idea
of kind of
of the C code have a list have a look at
this link sorry I may big so I can they
can be an eigen already is negative zone
so the question is what's special about
I ghen well it's just a way to you can
in see you can write a matrix algebra
code that almost looks like my lap so
it's it's a it's a header only library I
think probably the stand guys we'll talk
about this a bit more it's yeah you
basically I mean if you have ever used
this library the horrible pain you know
if you just want to do a matrix
multiplication you have to pass like
seven arguments to a function and then
you know it's hard to debug and all this
an eigen just makes it a bit nicer okay
so now this is the main selling point of
the toolbox is so we don't think it's a
good idea to tell people which
programming language to use the tool box
from so we just support all of them and
this sounds ridiculous and unfeasible
but we have we come up with an automatic
way to do this so here's how it works we
use this super cool open source project
for this what if i write a new algorithm
what i do is i I've ride the C++ class
to do it and you know it's a class I can
instantiate it has a bunch of methods
some interfaces the sugar people were
really nice and rode like all these type
maps so we have type maps that directly
map the memory from an umpire array to
say C array without copying things
around so this is all works directly on
the memory and then I just add my class
to the list of classes that I want to
expose and then I press a button and
then what i get is wrapper classes in
interface files so that i can use my
class from all the from all the
languages that we support this is super
cool because whenever somebody asks
something new to shogun we get this
available from all the classes whenever
we bind to another external open source
project this is all available from all
these classes so through sugar on it to
be easy to use lip linea from
in any of these languages under the same
interface and I think that's very useful
okay let me give you an example so you
some C code that's like a skeleton how
children code looks like and see so you
get some includes you get the namespace
you initialize you exit ok that's the
skeleton is pretty standard so yeah I do
a super old-school example dousing
Colonel lesbian i define some training
data here i would put in some it has a
type that's like a data type it's it's a
class i done ciated here would plug in
some some matrix but i don't have so
much space here i define some tests
features i define some labels i define
the colonel i define an SVM object that
I give the objects that have defined
before I call a method I give the method
something I called another method to
apply to test data I collect the results
use a strongly typed language you have
to cast I print the prediction I free
the memory ok fine so now let's look at
Python so it's quite similar the
structure is quite similar so I'm port i
define the objects i call methods on the
objects i print so the modular
interfaces can't be templated because
languages like Python don't support
templates but that's fine so here i plug
in a an umpire right now rather than a
CRA and in piping you instantiate things
like this and in Python you call methods
with the dot but the method is really
the same and I print so Python start
indexing vectors at 0 so this is how
print now go to octave and all that
changes really this goes one but I I
still I just call methods on the objects
and now I pass in just octave matrices
but I want emphasize none of this is
copied around this is all we do mapping
of the memory this one is horrible
yeah so Java doesn't doesn't natively
support matrices linear algebra object
so we have to import another library and
it's also strongly typed and but it's
really the same story okay okay so now
here's some of the stuff that we have
been doing in the last couple of years
to kind of make the toolbox a bit more
visible so we really like ipython
notebooks and I already mentioned the
collection of notebooks we have on our
website earlier is kind of a super nice
where you have you have you can write a
little tutorial you can have latex when
you kind of math and you kind of code
any kind of plots so we thought I would
be really cool if people would be able
to use these notebooks and then kind of
interactively use them so what we did is
we set up an iPad notebook server on our
web server so when you when you're on
our website you can press and you click
on a notebook you can press trying cloud
and then you get an interactive session
where you can play with the toolbox
without ever having to install it I've
to apologize it's currently broken but
the wheel will fix it very very soon so
have a look on this it's super cool
sorry your general method for
translation between languages you just
press the bar yeah exactly it's all
about the button that's how I like to
present it is a bit more tricky practice
of the efficient well they this this web
server is quite nice because you don't
need to install the tool box you can
just play around and kind of have this
is really nice if you just want to
really kind of you have this tutorial
and you wanna you want to modify it I
mean all the piping people kind of you
know they know this they can do it also
but we still have all this code that is
written in and see underneath so this is
on top of this yeah then we also got
something nice we set up a little
javascript-based interactive web demo
framework so this is an example of OCR
detection that you can do is we can draw
digits can classify them it's also on
our website you can you can click points
together trained in SVM you can
do a GP regression you can do a sparse
GP regression where you set you inducing
points and you see how the particular
variants changes stuff like this okay I
mentioned earlier that chewiness unlike
most of these other projects it's just a
it's just really a community project so
here's here's some of the Mendes members
of the project it's quite nice because
it's very it's very diverse so you know
like the people who are active they live
in Siberia in India in Singapore in the
u.s. Germany London and so on and it's
also quite diverse in terms of kind of
where these people are so I just started
my PhD we got some undergraduate
programming for us this guy's professor
in gutting in 44 she learning and data
security finger and it's a really cool
team and so the project is a nice
opportunity to bring lots of people
together and learn from each other over
over borders which is one of my main
motivations to work in it um most of the
other people said this but we also got a
super active mailing list so come tell
us your questions tell us what you like
what you don't like we have an IRC where
developers hang around so yeah just come
say hello and this is what really pushes
the the project and the community is
this group some of course anyone who
doesn't know this this program ok all of
you know it so we this year we got eight
students they work for three month on
projects that we thought about they got
so nice it's depend for this especially
the guys in India they are super happy
about this money i mean if you live in
New York or London this is nothing but
if you live in you know somewhere in
South India this is a lot of money um
this is super cool and we're in
particular we always looking for to push
this further so for example this year we
have this we had this deep learning
project and even though we can't really
compete you know with with you know with
like deep learning labs that are pushing
their libraries outside we can still
have people learn about the stuff so
students implement this
learn about it but also this will be
useful further people to play with it
because all this code is being embedded
in this very general framework so it
will be very easy to say compare your
your favorite deep learning algorithm
against your favorite non deep learning
algorithm for classification right um so
one of the things i'll be talking about
bit later with the future we were kind
of we're going to try to push the that
this toolbox is used in education
because it's kind of very general and
brings things together and for this we
we always look for people who are
willing to mentor open-source students
to implement stuff was also if you have
a super sophisticated algorithm just
tell us we will have it so yeah we need
we need help we are horribly undermanned
powered so we really appreciate feedback
we really appreciate if people report
box or fix bug so if you like if you
know how to do proper C++ software
engineering we also appreciate this
because we have lots of problems we if
you if you if you like to write
documentation about machine learning
algorithms come to us write a note book
nice it's really it's really fun
actually to play with this stuff and it
all goes to the website if you have a
website developers nice if you if you
have like a nice algorithm come tell us
if you want to join as a mentor in this
case I guess come join us and we also we
also realtor need money for
infrastructure workshops hacks so we
just we just had a workshop just in
Berlin a couple of weeks ago we had our
second workshop that we organized a work
of people there we did some hacking it
was very nice we recorded it it's on
YouTube I'll be planning a to do like a
hex print we bring together lots of
people interest in this project hack a
bit and we are really interested also in
collaborating with other projects but
more on this may be in private
discussions thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>