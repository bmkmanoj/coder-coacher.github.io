<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Correctness Checking Concepts and Tools for HPC: Call for Action | Coder Coacher - Coaching Coders</title><meta content="Correctness Checking Concepts and Tools for HPC: Call for Action - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Correctness Checking Concepts and Tools for HPC: Call for Action</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oed7nrrza-4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's my great pleasure to welcome
Ganesh Gopalakrishnan from the
University of Utah to Microsoft Research
Ganesh has visited MSR many times before
he works in the area of high performance
computing formal verification and other
related topics today is going to tell us
about some work that at the intersection
of these these two topics yeah thank you
very much for the kind introduction and
good to see all of you for my talk it's
a great pleasure for me to be able to
present some of the work that I've been
doing in the area of trying to apply
high performance in the area of applying
formal techniques systematic techniques
for high-performance computing so I gave
the initial title to be correctness
checking concepts and tools for
high-performance computing and if I
really want to address the upcoming road
map of exascale computing I would
probably offer a more PT title such as
this is really the black ice that is
waiting for people who aspire to attain
exascale rates of computing and let me
see a say why and about how we might be
able to help some of those ok so today's
high performance computing research is
driven by the mantra of wanting to
attain the maximum amount of compute /
rat whatever compute means and we have
enjoyed growth thanks to two governing
laws in this area Moore's law is
familiar which is roughly the doubling
of the number of transistors and Dennard
slow has been at work also because as
and when we scale transistors they
remained rather efficient they operated
in such a way that you could pack twice
the number of transistors and not exceed
the thermal limits this was thanks to
the voltage scaling that was possible as
in when you scale down to the
lithography but right now you cannot
scale down voltage anymore it's almost
at one volt and
hence the BAC v square f hits you and
we're power capped so Dennard's law is
over and this is from bob called l and
there are a number of worries that this
induces because if you have to enjoy the
same apparently limited way of evolving
in terms of computing advanced we have
to be hiding several agencies in gaining
performance where we were initially not
able to I so never have any excess
design or slack so this is how we had to
be proceeding yeah but what is not being
mentioned in hpc image tall this is that
HPC is also going to address very
serious computational concerns and the
challenges going forward so one is
familiar with the various kinds of
industrial simulations or weather
modeling but even for surgical planning
or studying the effect of introducing
extent people are able to use fluid
mechanics based simulations to predict
the wall shear so you can do at least
various experimental procedures of line
pedestrian accident avoiding systems are
going to be deployed with the assistance
of GPUs which are extremely
energy-efficient and mobile so
correctness is a concern and hpc is no
longer the province of big scale hpc
centers but it is coming everywhere yeah
so there are even verified high performs
potato chips this is just for laughs but
Pringles apparently uses aerodynamic
simulation in flying Pringles so they
shouldn't take off so you want planes to
take off thanks to hpc you don't want
Pringles to take off again thanks to hpc
all right all right so let us see how
the correctness field is in this domain
I hope previously worked on cash
currency for a college a verification
things like DP 0 R and preemption about
variations of bounded search and around
2003 or 2004 I started collaborating
with researchers in high problems okay
high performance computing just because
that area was largely virgin and yet
they were writing concurrency codes
pretty much
on all the time so real pressure or the
governing force for them to operate in a
certain way is kept the maximum amount
of science per dollar machines are
bought and commissioned there the
commissioning and bring up takes several
months and then machine is useful for
six years or so and these are expensive
machines so they really have to push on
that front and many times HPC is used to
explore unknown aspects of science you
are trying to predict what happens when
you travel closer to relativistic speeds
or what happens inside a black hole
things like that so you don't know what
the right answer is so you are trying to
find out the right answer ie cannot
write assertions easily in many cases I
will give the approximations are made
the physics can never be modeled as such
so that is again another variability
with all the hardware performance
concerns and their knots lobbying dead
we need to we are facing the prospect of
dark silicon which means you cannot
really operate all the transistors so we
really have to eke out every bit of
performance which means you need to have
different kinds of cpu and GPU elements
and memory subsystems it almost
heterogeneity different core types are
used when you look at number
representations you are thinking in real
number space but you are implementing
floating point and the floating point is
like a noisy signal that rights over
your ideal real number curve you don't
know where that noise is or where it
builds up and where it affects
computation so and precision allocation
is an important consideration because if
you can compute it lower precision you
are better off doing so there are even
attempts to actually do half precision
in terms of storage and bring it to the
chip and expand it to full precision lot
of games available in this area bit
flips are aware because bits may may
actually may not be reliable and the
scientists themselves in this area are
busy doing science and also catering to
hpc so Atlas Oscar who visited our
department calls and PI men they need to
have two different strengths and it's
hard to find people who can do the both
well as opposed to t-man
alright so this is an interesting team n
which is a deep singles skill here you
have to draw a deep into areas all right
said las casas a picture so all right
all right so here's an example of what
heterogeneity might cause and this is an
actual bug story illustrates group at
Utah has been pushing on there hpc
simulations and they recently made their
code run in a symmetric mode on Zeon's
and xeon fees so the code was working
perfectly correctly in this yarn regime
but when some of the processes server
computing certain ratio in the sea on
fee in this case trying to send the
number of messages being 162 the other
side calculated the same ratio again
because of floating-point roundoff
issues which are different and this
actually cost a deadlock so they were
wondering why the code was deadlocking
finally to a self-loading fine precision
issue so there are ramifications of this
kind where I continuous quantities may
affect discrete decisions if it happens
in a sequence you may not be preserving
invariance that you're had in mind
because the decisions can be
inconsistent and so we don't really know
the answer we they just went ahead with
double precision and it seemed to work
and when I read it further and my
colleagues on Amir question me hi sing
why is this really happening I initially
thought it was a 80 bit precision
available here 64 available here but
apparently 80 bit position is available
here but there are subtle differences in
terms of around round off and there is a
safe flag that you can apply to a
compiler so these are yet to be actually
tried in this area but given the busy
nature we have to be the cs people had
to be the ones coming and answering
these questions let me talk a little bit
about resilience and finally my talk is
going to drive towards correctness
issues of the familiar kind but let me
at least address some of the looming
issues if you look at some of the
advanced GPUs there are 7 billion
transistors and Kepler and if you do
some math there must be easily turn
roughly 18 transistors on a large system
and now they are all throbbing at
gigahertz over a week and end at the end
of that two-week you
an answer now what's the probability
that every such transmission went as
planned you can do the math and find the
probability of the failure it has to be
so so so so low that you get a libel
answer so moral is that failures are
occurring all the time in a system of
this scale but with HP see the Dean
seems to be that number the computations
are additive in the sense you're trying
to compute a single number as opposed to
a cloud transactional system where
things might be happening in a disjoint
way so the window of vulnerability of
each active computation is lower that
can have an effect and people are
actually trying to discover whether this
is actually happening on some of the
largest machines so I had the luxury of
seeing the Sequoia machine i lovingly
touched it and so on at Livermore one of
the larger which was top one recently
they are trying to run identical
computations on two of the subsystems
and trying to vote and see whether the
bits are actually occurring so verdict
is are yet to be fully announced but
they think it is happening bigflip is a
loaded term it is a multiplicity of
issues I mean the fabricated chips don't
have a single operating rate they can be
off by thirty percent so which frequency
do you pick to operate the chips on you
have to be guessing that coupled with
that hot spots so LSB is getting bashed
exponentially more than the MSB okay so
thermal hot spots you can imagine and
transistors are physical objects they
were out and thermal stresses and cracks
they have actually pictures of how to
prune so I'm just saying that with all
this particle strikes can make this
situation worse so it's all an additive
effect so with the scaling and the large
number of parts bit flips are a reality
which yesterday we heard Michael carbons
talk was I trying to address that
innocence and energy is the currency I
just was at Pacific National Labs before
coming here and there is this very
interesting project where they were
trying to see which messages are
arriving at a barrier sufficiently early
and if it happens consistently and they
can learn that quickly they just do a
dynamic voltage frequency scaling which
gives them 18k loads of saving which is
18 toasters and these are highly I mean
exascale attained at 20 25 mega mega
mega watts is the goal right now it is a
gigawatt or so so every kilowatt say oh
it is important so when you do these
voltage and frequency games it adds to
the noise the systems are going to be
unreliable as a result anyway so a few
more slides and then we'll get into the
beat of the talk power 6 they actually
tried to measure bit rates one of my
former students was in the study this is
a highly heavily engineered machine
which has all kinds of error correction
and they actually took it an e-beam
facility and short electrons on the chip
whatever higher energy particles and
then they had a very interesting
triaging of where the errors are
filtered so a lot of don't pairs at the
circuit level at the software level at
the application level and finally how
many errors are marching without hanging
the machine or crashing the software
that's the silent data corruption rate
and there was apparently point oh three
of the latch flips are turning into some
data corruption which is a very good
number very reassuring many others are
not getting hit okay but this is a
heavily over engineered machine and this
group subsequently wrote another paper
no reflecting and power 7 and they make
the remark that this Hardware protecting
you is not going to go on you really
need to have software level differences
asserts basically and you can evaluate
their search a triple model or a
tendency or other ways and somehow make
this whole system bulletproof
self-checking I think so this is a
reality now all right so despite all
this what are the most worrisome HPC
bugs since many of these things are
controllable and many things you don't
have control over I would really say
that concurrency books are still a place
where we can exert a lot of control and
those are going to be noticed and some
of the engineers at Nvidia who talked to
me recently or a last year said that if
you really have any race condition and
so on it tends to show up at larger
scale it is simply the probability
builds up so you do run into them you
cannot silently mask the movie all right
so that with that I will tell you a
little bit about
what we are trying to do in terms of our
broad scope of research and then try to
drill into some of the specifics so one
of the most exciting projects we are
involved in is called the pruner project
which is in collaboration with Livermore
and they are basically interested in
trying to identify when systems become
non reproducible you cannot replay a
behavior and why is that which subsystem
is acting up that way so please help
build a harness to say deterministically
executes up to a point and then schedule
around so similar ideas as being talked
about here and at least talk today
address some of the controlled methods
except we had to now do it at a larger
scale in the presence of multiple api's
and so the data arrays detection
challenges in MPI or open MP MP has
message racing has supposed to date
erasing some of the failures they
noticed in some of some of the failures
at Livermore was a failures at delicate
ranges of sizes and it know if one
turned out to be an highly optimized MPI
library causing it now the user course
so again there is no single place so we
will slowly work towards this and give
them a test harness we have a replay
facility for message passing programs
based on a distributed execution of MPI
with the piggybacked Lamport clocks we
have a special version of Lamport clocks
that detects a few more happens
concurrently that we have engineered and
they are trying to use that as a
framework to then drive other scheduling
so we will be looking at OpenMP
scheduling very so on and then we also
have to isolate whether it is floating
point determinism about non determinism
so we may have to determine eyes the
reduction operation to avoid that issue
flaring up and then see what else is
causing it all right here it was an
overly optimized MPI collective which
was preventing MPA point2 points from
finishing so within the NPA ecosystem
some message types were not allowed to
finish just because the other message
type because an MPI is a runtime you can
think of all the processes impinging
calls so some calls which were
had to be scheduled in a fair way were
not a scheduled and so the system
deadlocked and this happened only for
specific sizes and it turned out to be a
bug which I the vendor characters and
all that we are looking into
floating-point error estimation which is
another very nice computer science style
problem despite the fact that I trip Lee
has standardized floating point it
doesn't mean that there are there is a
good understanding of when roundoff
happens so if you write and a floating
point an expression you involving
floating point David Bailey and others
have shown that if you selectively
assign higher precision to some of the
operands and keep all the others at
lower precision it's two computers fine
so a lot of analysis questions in this
area and again the floating point error
that i mentioned earlier so people tend
to slap double precision but double
precision power co double the bandwidth
at least 4gb use so you can't really
afford to be indiscriminate in terms of
precision allocation so we have a search
based lower bound determining so we run
two modules so one being the native
precision one being and excise rated
precision try to walk through the input
space in a by in a basically Delta
debugging or genetic search way seeing
where the outputs deviate is that you
see a good estimate of lower bound that
we had a peep of paper this year on that
and for the upper bounding which alex is
working on we have several abstraction
techniques in trouble are for arithmetic
a fine arithmetic they're all valuable
methods but sometimes they don't track
dependencies of variables so we are
trying to come up with much tighter
bound so on we can then see whether the
bounds I agree in which case we are
getting a reasonable estimate system
resilience wise we are beginning to work
on that we have developed an LLB amor
level fault injector and there are some
collaboration with xuan do lahiri going
on here with Zhou Tamir okay so let me
now turn the attention to hpc and
concurrency itself and the real problem
with concurrency now I'm sitting aside
all the other issues and looking at the
concurrency bugs is
the explosion of the number of AP ice
this is not all of them needs to be
presented as given setting but many of
them are and each API sort of things
that it owns the machine which is a
resource issue I'm not getting into it
but it's looming in the horizon we don't
know how they're on x interact but the
real body which I will illustrate with
some examples is that everyday
programmers are exposed to lower level
concurrency in some cases and this is
not the desired path in the concurrency
space because we thought that the job
program or didn't need to know about
loyal concurrency but apparently that's
not to do too true yeah so then I will
talk a little bit about that and then go
on to other aspects of concurrency ok so
I thought I will you also give some
sense of reality to the torque realism
to the talks i will quickly demonstrate
to tools even though it's coming out of
context because I don't want to
interrupt the talk later so I'll give
you a feel for all said and done what do
our current solutions look like and then
we'll come to the more slide oriented so
we will i'll be talking about calculus
checking of GPU programs and we have a
tool called giclee i'll run that for a
few minutes and then i will also we have
a dynamic checker for mpi programs so so
i'll be talking about giclee which is a
which has been itchy balled out of CLE
we just engineered the CLE to understand
the different memory organization which
is unlike GPUs so if I were to present
as in the benefit of the people who are
watching fibre to present symbolic
execution I could even start with binary
search and then it puts the assumes and
searches for a symbolic item and if the
item is found it prints a star if it is
to the low sided Prince and heir Prince
a term for an edge so i can now compile
it with llvm and then that's different
so it's a running symbolic execution and
it found the eleven ways to execute the
program it has generated 11 inputs so
you can sort of look at this and so HHS
not found means it went through the
hedge three times the high path L not
phone died it found that path it found
that LHH so you get the idea so this is
what symbolic execution does it does
input generation entrance the program
this is the dart and smart kind of
technology engineer and well implement
in giclee so the point I'm making is
sequential good sequential symbolic
analysis is going to be central to us
and the in fact we will turn the GPU
verification problem into a sequential
sequential Isis the examination of the
schedule so that's how the technology is
going to play out so let's run some GPU
programs just since we are here so if I
were to run this GPU kernel and the
syntax is kind of not entirely CUDA in
this version but we have caught up with
cuda so here this main program will call
a single kernel called race and this
kernel will be run by the given number
of threads which I am not mentioning
here but it's easy to see that if two
threads with 0 and 1 are run thread 0
will be writing location 0 a reading
location 1 and thread yeah one will be
writing location one reading location
too so there's a race so we are going to
schedule in a manner that I'll explain
soon but the bottom line is that it
actually finds a ways and you can drill
into the llvm and that's where the races
and all that and to give you a feel for
how well this might work on a CUDA
kernel which is a radix sort
implementation again I wouldn't explain
what it does or how the problem might be
but if I at least it'll give you a sense
of time it takes for a tool like this
this is the first implementation of
giclee and i'll be talking about a few
others so it founds a raise and you can
drill
and identify that it is a race between
thread 0 and thread for and then you let
you scratch your head and look at the
input and find out where it raised and
how okay so that's that's enough for
this demo what else is possible in the
space of message passing programs and
this is not symbolic analysis this is
dynamic analysis so let me write again
run a quick demo of this illustration
now here the students put a lot more
effort and we have engineered an eclipse
integration we hope to have that
available but again to give you a feel
for what what we might do here's message
passing program written in the MPI style
and we want to find out if there are
issues here so here's rank 0 Rank 1 and
ranked to code so rank 0 means process 0
which is going to send a non-blocking
send a message to process 1 medtronic
one process one is going to have a
wild-card receive any source means
anywhere then a specific receive and
then rank 2 is going to do a normal send
okay so let's see how we can test it we
can do MPIC see this is how you compile
it and then the standard their new MP
iron no problem this code seems to test
fine okay sometimes you get lucky okay
didn't get lucky whereas our reply
debugger applies a different compilation
is PCC which is a little bit different
way to compile and if I run it now under
ISP it's a it actually finds out if
there are dependencies and then you go
so it actually replayed the code code
twice and then in the second run it
found a problem and then I can spawn a
user interface and then it tells you
there are two ways of running the
program we're in process 0 this was a
non-blocking send which was picked up by
this
non-blocking receive there is this
barrier and then somebody put this end
after the barriers thinking that this
end is forcing to force to be matched
here but if you now go to the second
interleaving that's not the case there
was a racing communication because once
this non blocking calls reach the
barrier they can cross it and that
enables this other send also to pop up
its head and it is an eligible match or
so so this is a we actually schedule
based on an underlying happens before
graph and so we have a happens before a
theory of mpi we know how to schedule it
yeah cannot have a receiving advocacies
for his barrier well in this case this
era CEO is just a specific receive got
orphan it wants another message from
source to but there is no more message
to offer yeah so it's really the sense
one you should have been received by the
receiver tube that's right okay yeah so
this this was intended match yeah so so
at least this gives us a feel for the
kind of tools that we have built but
let's now go on with the main line of
the talk and feel free to ask questions
at any point so so I have all right so
now let us dig in and try to revisit the
GPU space and let's start from the
beginning and look at how a user
experiences concurrency and what errors
they might be subject to just because
they don't understand or the
documentation is not clear alright so to
remind you of GPU programming GP use our
model of computation where you have a
memory hierarchy of threads accessing
shared memory and then global memory
barriers can be used within this and so
if you want to do a summation of an
array with AI plus B you deploy multiple
threads the details are not important
it's a sim t-model sim single
instruction multiple thread and
typically threads are patched and
scheduled ok so here's one a piece of
code that is contrived but it
illustrates a certain point because many
people tend to assume that since there
is this work business meaning a bunch of
threads in this case 32 threads are
scheduled as a badge this single
instruction going is going to finish
before any thread engages in this
execution and hence there is nothing no
need to put anything here okay and so
when you run this compile and run this
the expected answer show sir because
each location is initialized to I so it
would have been 0 2 4 6 8 when you do
this addition except to this thread this
instruction came in and the ayah thread
wrote in the I plus first location a
flag value so it killed those values so
that's the right answer fine okay so as
soon as you apply an optimization you're
getting a different answer okay so these
are issues that are widely discussed in
CUDA forum so I'm just saying things
that really turn to trouble users are at
this level well before we come into
races and all that okay so what's going
on here we had to ask the nvidia
compiler expert and he said that the
warp programming style is discouraged
and compilers really understand the war
to be of size one so really you must
think as if this is a synchronously
interleaving with the threads being able
to interleave in anyways there's no
guarantee that this will be entirely
done so that's this and we had dug into
the code to see why this is happening
and all that so so no guarantees
provided okay so real reason why this
happened was this was while he was held
in a register and this value is computed
but in the register updated so it's bad
cold there's no so the recommended
practice is to slap a volatile so this
is another thing that sort of
illustrates how this area works in terms
of published documentation and what the
users are so so what r c volatile so
this is going back to the sea volatiles
which is a technology device for device
programming those days when she had to
respond to commands in the manner
described what does it have to do with
concurrency there is no clear answer
see semantics of C&amp;amp;C Levin memory model
has no mention of C volatiles it is
going the c c11 Atomics and all that so
their side effect that you're getting is
the values are no longer held in a
register but that's giving you this
benefit and hence everybody's happy and
going away so there has to be some user
education and retraining and the worst
thing is that if you really track the
documentation chain that is going on in
this area the volatile practice was
recommended best practice in an earlier
version of coda the volatile practice
has not been mentioned in this version
of the code of manual and it's not clear
what is going on so the point here is
there are errors of a low i would say
great but of a user education
documentation nature that really are
important this area this is a
fast-moving area with many things
happening and the users cannot be
expected to keep up so it's so
unsatisfactory because in all the
versions of the document yeah it's
explicitly encouraged this work started
programming saying yes if you want to
get high performance yes do no need to
insert barriers and give an example yes
and then yes I told me it was a phase
where they said boom the compiler might
do something to change unless you slap a
volatile yes I know they've just removed
any mentions they having it this is now
decide it's just not so right it's not
like now it says is deprecated it's just
disappear on I'm not going yellow so
yeah I'm not entirely relinked I mean
this is a computer science has seen this
before I'm if you go back to the 70s or
early 80s MC 60,000 came in the picture
and it couldn't do page fault handling
at the last location of a segment and mm
use came because they were new you know
when how new things happen in hardware
people really have struggled with a lot
of these breakages so I really would
attribute that to growth pain and just
thinks being happening so fast yeah and
they're open to engage with us so Ali
and I have some engagements with the
opencl nvidia teams and we need to work
on formalizing these but if you look at
the threads these are all issues latakia
threads talking about this issue that i
mentioned and here is a user's a lament
I wrote this piece of code supposed to
work under the warp semantics but
doesn't work unless a hyper synchron's
in this case a big hammer okay which is
a barrier so that's safe though same way
so I won't go on this tangent for a long
except to say that we have been also
some of our students have been
diligently looking at things out there
there is an actual text book with a
published example of reduction and it
doesn't work and we have actually
demonstrated it doesn't work we have
talked to nbt expert so I can get into
the details later so the basic idea is
that in different thread blocks thread 0
was responsible for accumulating doing
the reduction in unto itself which is
fine now having acquired values in
thread 0 they were all engaging in an
atomic add into a global there they
assumed memory visibility so that the
Atomics have a fence semantics and it
doesn't so you can actually be picking
up a stale value so this is a so these
are the levels of trouble there are
rights I mean this is in a book so why
would people not use it there is a bird
stealing queue in GPU gems which we have
a broken and we are documenting the
error they have in contact with authors
but again this is a store store issue
where the two stores are assumed to
happen in that order we need a fence
between them so there aren't that many
of these but these need to be captured
and thoroughly documented and people are
relying on this happening reliably
especially in a very ambitious GPU codes
where those zones are used to multiple
overlapping so ghost cells and then they
compute locally and but they really need
a fence semantics there yeah so CS has a
role to play this is just a little
interlude we need to build some poor
show pieces so we built a cool show
piece and we're planning to teach a
concurrency course based on it so this
is our little raspberry pi cluster we
actually since we are getting up to
teaching SPG and getting hands-on we ask
the students so my two good students
gave my four hundred dollar blank check
basically they bought the Raspberry Pi
it run Cynthia is
some okay it's far we can run the I can
show you offline I can do the core
temperature and then running a matrix
multiply true looks like a 90s late 90s
linux machine each core and then look at
the quarter pressure again for the fan
and how's that yeah alright so I have
looked at the basics of GPUs and now let
me get into more into their more serious
aspects of GPO debugging so having
gotten past the first order errors where
do we go next so this is a project where
alli and I and showers have inspired
other and published in as many papers
with a similar thrust and the tools that
we have is called giclee like I said
before so let me get a little bit into
its operation so we can take advantage
of the fact that for much of what the
programmers write GPU program is largely
a secret barrier where a threads are all
aligned they all compute in their own
private spaces or private portions of
the global memory and then synchronize
again so the general idea is that a CUDA
program can be thought of in this
abstraction as sink run some read rights
control flow whatever but shown as
linear sequences so this is a very
embarrassing leap our situation so you
don't need to run all possible
interleaving here ie all these
executions are if they're so see if this
is the only conflicting pair let us say
this is a read this right for the same
address and let us say every other
reason right are falling into different
locations then you don't need to run all
these schedules to discover this race
it's a DP or to the extreme you can
actually run a single canonical schedule
this is also going back to Sarah
pathways days of 1990 where you can sort
of run this thread sequentially and then
record the every access in that
sequential run and then at the end of
the run you call the SMT solver to see
if there is any overlapping access that
you can solve if those if so there's
arrays so this is this is the
style of face checking that we
implemented initially and of course let
us says let's also see I mean this of
course didn't scale that well so we are
coming to better techniques the reason
this doesn't scale is obvious because
you are given a 200,000 CUDA program the
execution length is pretty long those
were some symmetry there why are we
doing all that so we will come to that
but let's even look at an actual CUDA
program to see what races might there be
in the scooter program so obviously this
is a waiting race when threads 0 and 1
or 1 and 2 or colliding but then they
barrier and then there is a for all the
even threads do a read and for all the
odd threads to our right involving that
okay well this is a conditional so look
this happens all that happens is that
true well that's where we need to be a
bit careful about what conditional it is
it is a conditional on the threads
divergence so either all the odd threads
execute first then the even threads or
vice versa and the order is unspecified
so there is a race there so you do have
a classical race and then there is a
unordered situation of whether you do a
read over right along divergent warp so
this is one more this is assuming one
warp in this case yeah yeah and this is
not mentioned in here and put a
recommendation so you gave it a name
called porting race so if you put it for
different platform you might find a
different behavior kind of thing so we
are looking for both races kinds of
races in a sense yeah but you see works
as one oh yeah yeah yeah then yeah this
isn't go full stop that's right that's
right yeah okay so I guess this is still
equal kind of exit yeah but still the
fact that it is getting sequentialized
in a certain way may give somebody the
comfort that results are looking
deterministic even though your have a
conflicting access across and then and
al so the caution there is if you were
to run into a different platform where
the right happens before we'd is also
change and thing yeah so this is like a
different arguments of a c function
called being unordered there is a
possible race same thing
so we actually do a canonical schedule
in this way of this kind so just to give
you a point so we are going to run two
threads and for as many threads as there
are and then we notice the accessing
pairs across them that's the that's
that's what is it we will solve okay
okay so thus that was the original tool
in pea pod 12 and we built an ll bean
paste flow but and then the as i said
the concrete witnesses are a nice handy
feature of these tools they tell you
where the error is with threads and all
that okay so the next improvement was to
try and avoid having to examine all the
threads completely how about if we look
at how threads are diverging based on
their IDs yes that's the next nugget of
an idea so imagine a contrived program
where you have a decoding based on block
idea of threads and then thread ID and
then block ID so what we have is a
situation where all the threads come in
to the point where some of them flow
this way others flow this way and then
they are splitting lanes so we have
symmetry in each flow group that's the
observation so within each flow group
like here the threads are executing
essentially identical code so what we do
in an actual implementation is to model
too well basically we run one symbolic
thread and then make a clone of it and
give it another t ID which is different
from the original guy 30 ad so we are
running to symbolic threads per flow
group and that is possible in a symbolic
executor you don't need to get concrete
IDs you can pretty much run these flows
right absolutely I don't get this
business of whether you have 10 chamber
flu which is it is it it is in actual
implementation he may have my
student may have taken a choice which I
don't understand but I let us say that
we are in principle let us say that we
have an assume tid a different from tid
prime
and then we are executing each flow yeah
to arbitrary obviously try to this
condition yes and you want you master
constraint solver yeah is it is there
only one solution I'm not gonna both go
the same way if it's possible for them
to go different ways you yeah and with
four threads right that's right so each
time you then assume of that condition
assume of that condition okay so you see
these forces all have distinct IDs yes
and in this flow yeah my knees are even
exists whether he's a hard that's right
okay then yes and that would be part if
they assume assume that on that
execution path so very clean model City
is modeling llvm level constraints so
for each llvm load and store it is an SS
a kind of constraint so it's equality
between new and old and for each
conditional it is a path constraint so
once you once you have one of these
diverging conditionals yes split your
initial work is like no return from
splitting initial with it with this
initial work yeah it's like once you
split into two flows yeah that's it no
way back there's no way back here
streets that's right yeah so this for
this example itself will be generating
several race checking conditions so we
don't do safety assertions here because
we don't have full assertions people
don't we are assuming that a world where
we are not given an input condition or
intermediate assertions and we are
boiling so far we have focused on
database checking for this tool so if i
were to lift the skeleton of the flow
what we will be generating would be
verification conditions for race
checking which amount to recording all
the accesses in this flow and under that
condition which is the flow condition
and the tid is being different is it
possible to solve their being an overlap
between conflicting pairs in that flow
and we do that for every intra flow and
then we do interflow also just because
like i said in the divergent warp you
don't know the scheduling order between
the odds and evens so we do checking
across also some kind of in
engines of addresses on data or
something like that you want to look
that race by looking at some finite
number of threads yes so I'm assuming
that I read at is not dependent on some
value i read that we had written by some
other thread that we're not I i we
haven't written a theorem to be honest
so I think that is possible to write a
proof that if you have an actual
assertion for the starting state of the
system solid let me make an example a
reduction colonel sort of works like a
logarithmic tree so each thread adds to
its neighbor and then it adds two away
and then far away and so on so so after
you do that the behavior the value of
thread 0 is going to dependent on the
all the locations of the array and
things like that given threads reading
or writing that's right so so javanese
the addresses determine the race that's
what truth next register if you did a
prefix um yeah then it's quite common to
your prefix some when I'm right to an
index which is like a or b of i but i
use the prefix of that's right yeah so
long a bit of flows into a sensitive
spot which is a in this case and address
access that becomes germain the value
that you compute which turns into a
dress becomes chairman yes right so then
let's say so basically if I if I
understand yes if you have here you know
some sort of concrete threads yes a
bunch in restaurant threats so the
computers are paralyzed by their further
ID'd the concrete starts basically gives
you a range for the whole thread
population and what we execute yeah yes
how hmm I read some data yes I'm ready
and I and use that as an index right you
know our isn't address yes right then
that value may have been written by some
other I'm sort of a novel here so not
yeah Mac is doing progress or not
reporters in this case there is a very
interesting thing going on you cannot be
reading and then using that value and
you are going we are going to do this
check burberry of in trouble so within
each better in trouble you basically
cannot have any communication between
threads yeah so we are going to be just
picking up the value
and then incorporating in whatever value
we have with the final state but when
you want to build information across
threads you had to sort of be crossing
bad ears yeah so then Barry yes you you
may need some kind that's right yeah
well in this case since we are computing
this is a computational framework the
invariant is basically the strongest
invariant we basically are computing
forward I don't know whether you need a
more relaxed invariant that invit have
you compared means you have to be able
to compute strongest post for the
contrary number of threads right for
parameter the system right see you there
why why we're dream everybody happens
machinist suppose you have a simple
example mmm a thread does a matted yeah
becomes equal to tit okay and then
there's a barrier okay so this would not
reduce a new flow mmm finds to correctly
if t equal to did no control over here
just a single statement arrows you could
take I yeah that's an interesting point
ah depending on yeah I think you have to
basically fork I had to yeah I don't
know whether it is happening in the
current system but that assertion can be
satisfied or not by different instances
of dead right well if for a given
starting the content of their way i mean
i right am I you were right you teach
your kid okay then you synchronize okay
okay then you say hmm be of now a of 10
plus 10 k becomes equal to 42 I say pay
now this may be read day to raise free
yeah by construction because some a is
populated in a disorder all the little
birthday or effort that's so very soon
so late pre-k so GPU verified would not
be able to prove this cone correct use
what we got a barrier in there it okay
make this work okay maybe when I was we
didn't your paper is done hmm whether
we're the GTP understand ji dam would
have no problem being a compute it so
what why would I wanted
p all right do you have this
approximation of the shared state I have
as I told you we were yet to write up
full formal account of this work but I
believe that if you have a full
assertion covering the entire state a
set of state variables what you are
doing is computing for to proxy threads
but if you were to expand these two
proxy threads for all the tea IDs
present in every floor you have the
power to determine the full extent of
the states that you get at the barrier
and then you will be solving a specific
starting initial state you are not
covering for the full generality of the
code but a given an initial stage you
are computing essentially all the
symbolic next state serve for every
location and then the usage of the array
cannot well let's not get into too many
details here but I do understand the
concerns here but the usage of the
calculated value as you expressed in the
example happens in a subsequent face I
guess because we are doing if you have a
database in this interim we would have
reported and found it but the usual race
for usages of calculated values happen
after the barrier so let me think about
that and we need to drill into a little
bit more and i'ma say yeah I compact
yeah from over the next yes then if you
wanted precise make you have to do quite
an expensive assumption across all right
compositions about conversations yeah I
am yet to write a proof but I don't
haven't really found yeah I guess the
work is yet to be taken to that level
okay yeah yeah so what we are doing
actually the completeness theorem would
be interesting to write saying that if
there is a raised under what conditions
are this guarantees that's something
that the student who is finishing up
soon has to be right up but i don't i
don't see why this should be an issue
because here again starting from an
assertion and i'll leave it at that okay
the i will come to a related story which
is
under construction again this is
assuming that there isn't a let us say
usage that breaks the address use break
symmetry for many examples we were able
to find all the races that we had found
before but there is another issue
lurking here which is what we addressed
initially so you might have this flawed
you shouldn't happen in a loop so we are
back to exponential ality again because
if one flow has to build up enough
knowledge for the next flow so we have
experimented with doing llvm static
analysis to see how much information
flows from one better control to the
next and the flow is being calculated
for the sensitive spots which are the
precisely the address spots so we are
able to take the flows in one barrier in
trouble and we do a forward analysis to
see whether some of the values computer
under the different flow conditions
flows into the address or if the
conditionals and if that doesn't happen
we are doing a big or of them so for any
flow that doesn't feed forward we are
basically building an or constrained and
going with a single flow so this
technique is valuable that's the most
definitely result barring a since we
don't have a theorem at hand is that
this is tremendously valuable and we
have covered several of the benchmarks
that you couldn't handle before and we
have gone after several of the larger
benchmarks the lonestar benchmark from
UT which is a pretty serious set of GPU
benchmarks a power boil UIC and yeah
this explosion goes away for several
classes of examples so if you look at by
tonic sort this was one example where it
exploded but if you look at this
structure of the Bionic sort the values
that you compute because each thread
decides where the location would be at
the end of one better in trouble where
the data and then so these actions that
certain threads will have to take in the
next barrier in trouble depend on other
threads actions in the previous
interval but there is no flow into the
address position itself so we were able
to collapse the flow so there seems to
be the basis to formalize that all right
so I would say this is the story for GPU
verification as far as the tool goes the
tools exist in the student is wrapping
up and we'll discuss the later details
when we come to his dissertation the
nice thing with an execution based
framework is also that we are able to
detect unexpected traces which is a
valuable thing so you have several
programs we're out of bounds access has
happen they are able to thanks to a
symbolic execution find those cases
which it occurs all right what I'm
planning to focus on now in the
remaining part of the talk is how we are
able to take at some of the formal ideas
and maybe try and apply to larger scale
so this is just one project which has
gone this way but what it has allowed us
to is to situate ourselves in the
present in the context of a large-scale
project going on at Utah so the project
itself is has been going on for a decade
now and they're building a multiphysics
simulation framework and it's a very
structured computational framework with
a million lines of code in it and pretty
serious effort over several PhD students
and they have achieved the scalability
of this code based on national
leadership machines we really so let me
tell you a little bit about how the
system is architected and tell you the
one technique we have tried which has
gained some traction and see how we
might take that forward so the system
itself is designed around a nice
partitioning of concerns where the
designer writes the computational intent
in C++ as a task graph of sequential
activities so they largely right
sequential functions and the express
dependencies and this might be various
force calculations and other temperature
whatever pressure calculations and how
they are to be orchestrated and then the
infrastructure takes on this task graph
and then execute sit in a maximally
parallel way it has a scheduler that
understands how to schedule these and
arranges for all the message passing
mess MPI messages and all that and the
system architecture just to give you a
glimpse of what the scheduler contains
it's a fairly interesting system we rely
on the designers of the system to give
us a handle on understanding it so the
main part of this was the experience of
seeing something in design to a large
extent they have data warehouse which is
a distributed hash table that part's the
data they have various task graph nodes
being evaluated in an award there will
be a replicas of this evaluation engine
elsewhere split system and then when
there are data dependencies there are
messages that are being waited on the
overview what I would like to convey is
that this is a system that they have
engineered to have GPUs and xeon fees
very smoothly extended the system what
can we do and how complex are these
these are all the examples of
fast-moving HPC frameworks that groups
develop to get some signs done and we
had this to your involvement with the
postdoc so a lot of what we tried to do
was to understand the system then we
said okay this these they have
documented failures so how about are
trying to find a cheap way to diff two
versions and that may be a good bug root
causing facility that we can give them
so by two versions I mean if they have a
schedule or in some cases they had an
older scheduler when they migrated to a
different schedule or something broke so
we they want we want to identify where
the fault was an isolate help my salad
it in some cases two different inputs
may have caused the system to behave
differently or in some cases there might
have been a latent non determinism where
they observe something so when I said if
it would be difficult so the idea is
kind of very similar and in fact
tomball and Jim lares I worked on
similar graph structures they call it
some call path graphs and things like
that long ago a lot of these call graphs
methods that are around and we devised a
particular version of it and the name
that we give it is a coalesced stack
stack trace graphs or coalesced path
graphs what it is is a control flow
summary of the system so how did the
control to reach a particular point and
the exam the usage modalities that the
user identifies where the system broke
and picks a certain function call as
having as being the vantage point of
interest in this case it's a directory
where has put so then the system
recording is turned on for say one
scheduler and then that records all the
calls that start from the system
initialization and goes through
different function calls and each node
here represents a call from a certain
calling context of the apparent and then
we develop these big fat lines that
tells you how many of this function call
reach this function and things like that
so this is a summary of how many calls
went through various functions starting
from the top and reached a particular
point and what we basically do is record
it over two versions and then find a
diff and all we can say at this point is
it informs it has helped inform the
design event that we have four or five
compiled examples of how this brought us
very close to being able to debug
further so this is a facility where the
user interacts and the might use printf
smite use a debugger but this brings you
closer with a less effort so the
formalization of this I don't know what
to say it is just a quick way to x-ray
it and we have seen that this is a
something that you can turn on without
too much overhead so what we do is we
don't deploy it a thousand million
course or anything in fact in a real
instance we take the full code base and
populate this do this instrumentation
for 34 processes this code is available
also you can actually slap collect calls
in any sequin
to a private program it records the
things and we even found an
uninitialized variable case this way by
recording over two different frames of
time there was a one instance where we
found a extra call going through one way
in we had to then use a debugger and
then versus surprising value that sort
of indicates some some garbage sitting
there and so this is all right so it
really is an interesting eye-opener for
us because we want to help these kinds
of users and we thought we thought what
is the next step so the real eye-opener
is that these systems so this is a big
picture graph and I am almost done this
is trying to in research how we might
make some of these large-scale systems
absorb so not only do we want these
systems and the subsystems to be
verified and simple together and put a
push a button but we really need a
hierarchy of observers in any such
system we can't really run these systems
open loop so ie Automator there seems to
be ways to infer the protocols existing
at various interfaces and it's also
clear that these automatons cannot be at
one grain of time because and and we
cannot use some of these automate at the
lowest level if you put an automated to
observe the actions of a non-blocking
data structure well you're going to lose
all over performance so we had to sort
of scituate these automated a medium
even sighs and then because actions here
span several orders of magnitude in
times from nanoseconds two milliseconds
so we may have to define it however keep
this observable data center and then the
way to we have tried to use automated
learning for selected modules so we did
the passive learning on the data
warehouse we have some examples of
automatons being able to be inferred so
the positive signatures are the observed
behaviors the negatives are both user
has to write assertion saying these
causalities are there and we try to use
some sad pace to automate on learning
methods so I think this is a really a
good way to imagine building a large
scale system where
you have the ability to actively test
and observe and then if a fault occurs
there must be a quick way to summarize
the fault maybe using a call a stack
trace crowd or something like that so
hopefully if this idea flies I'll get
some money in a few months otherwise
this is NSF proposal submitted again all
right this is proposal where okay yeah
with that I think I'm done so
correctness in hpc has a different ring
to it and it has been an interesting
experience of you're really going
forward in some specific areas I think
their main pet projects are floating
point correctness that's a real
attention gripper for us because we
think we can get somewhere with that
system resilience there's some effort
and GPU verification we really want to
push forward and get some tighter
guarantees and also release the tools
and finally the need for use of trading
and the community involvement is also
clear so with that I'll attend the I'll
wind down and takes you take you take
any questions yeah good so there's been
you know people thinking about the brand
up problem in hardware for example vs
something goes wrong in the lab and
you're trying to reconstruct some trace
of what happened in the harbor even
though you can serve it and you have
among other things this problem or what
do you want to record is arguing that
you have a limited ability to reorder
trace runtime yeah so I would suppose
that the same kind of problem occurs in
in HB senior amputation or younger you
can't trace everything yeah yeah so are
there ways that you can get at this and
try to infer you know what is the what
do you know what you need to know what
Jim to reconstruct you know what happens
if you have a look like a useful the
trace all right yeah well lipitor
ambitions lectures always says and I'm
going to sort of attack smaller problems
to get more control so just even
imagining a textbook or IT and pa
programs for
the user does all the bank division and
lets the processes do something yeah we
could record some of the salient events
where dependences happens or while
caught receives non-deterministic probes
rest also rest will all play out
deterministic Lee so there is a minimal
recording possible micro checkpoints are
possible depending on a PA system but as
soon as you cross now you have two
things in play and open okay yeah things
start getting ugly probably is because
in hardware yeah in bring up you have an
inner ability problem yes but usually if
something's going wrong it's going right
after a fairly small amount of time you
know right so you know something maybe
some billions of cycles no it's pretty
fast yep and so you can do a lot of
replay trying to hit desperate mr. Allen
who's the backspace for days right as I
say yeah and if we're not really but you
can ya get some gigantic computation
yeah you know ran over night or
something yeah yeah it seems like you
have a in a way oh my god much larger
problem it is it is I think of you hope
is ID says this system you entice a very
very beautifully architected and there
are other examples of that the charm
plus plus system and so on they have an
architecture which reduces these task
graphs according to co salty so there
may be some events that you can record
at this level so it gives you at least
an initial control skeleton of where
things happened they are not and this
can be recorded distributed in a
distributed way really early architects
interested in
determinism and replay and so forth so
that on the river yeah this is a this is
an interesting new reality check
question because yeah they also have the
same problem as four methods researchers
they had two graduate students and then
look for the next which makes to written
to continue the reserves and and they
have there are they had to meet certain
other objectives like they had to solve
the next physics problem of the next new
machines I think this area has this
feeding frenzy if there's a new machine
makes all the sense in the world to jump
onto the new machine just because you
could run five times faster with less
energy and then that's where things
break also so but all said and done yeah
we have we are working in this control
way of writing NSF proposals and the if
this gets funded we will have that per
person yes yeah and the other thing is
this task or a framework is so nice that
you can meet make some of these tasks
checker tasks and the nice thing is then
it can use the load balancer it can run
on an idle core automatically so the
framework is well engineered to stick in
some of these checker tasks along with
the regular so I really am very
enthusiastic as honest yeah wrote this
valve yeah had this experience and wrote
this proposal with you think that this
submission is not bad and this is the
right because there are so many ways you
can get queues task graph reduction you
see a partial order this as a well orkut
x architected scheduler flow the
scheduler queues you know so the design
intent is clear from many many facets so
we can somehow intersect them and put
the right minimal observers that might
be a good way to replay up to that point
and then this this is systems also check
point after every Millie say again
though seconds takes a system check
point although that's not going to be
affordable in the long run yeah yeah
parently I when I went to Sequoia
machine load more apparently they have
this maintenance days if I day six days
a week the computer and what one day
whatever their AC has to be fixed is a
huge facility so all jobs get a check
pointed but a course frequency but this
needs more than that yeah
well thanks for all the questions and
again thanks for the opportunity yes and
I'll follow</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>