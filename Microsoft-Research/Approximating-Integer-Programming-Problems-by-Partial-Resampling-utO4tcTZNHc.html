<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Approximating Integer Programming Problems by Partial Resampling | Coder Coacher - Coaching Coders</title><meta content="Approximating Integer Programming Problems by Partial Resampling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Approximating Integer Programming Problems by Partial Resampling</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/utO4tcTZNHc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
welcome everyone it's a pleasure to have
David Harris from Marilyn today is going
to tell us about a new randomized
rounding algorithm based on lavas local
Emma hi this is a work with amine Arvind
stuyvesant about a pressure over
sampling so we're going to consider two
types of integer programs of there is
talk so the simplest one to describe and
I'll spend most of the time onyx is just
a little bit easier to describe it is
the cover image you're covering problem
so you have n integer variables and you
have some linear constraints on them so
some cover and constraints so all the
coefficients are positive and you have a
positive right hand side and you can
just scale all these constraints of the
coefficients are all in the range 0 to 1
and you want to solve these constraints
and you want to minimize some linear
objective function CX so that you can
kind of think of this as a weighted a
generalization of set cover and so in a
sub cover instance you're given a
collection of sets and you want to find
a subset of them that that covers the
entire space so you can kind of think of
this as each element and Nero grounds
that gives you a covering constraint but
the coefficients are all just 0 or 1 and
in the set cover instance you want to
find the smallest set cover so your
projector pumpkin is just the sum with
coefficient 1 so the energy recovering
problem you can have other objective
functions of other weights in your
abductor functions and other weights in
your constraints yeah oh ok I just did
sure so another integer programming
problem I want to consider is the
assignment packing problem so you have
variables x1 through xn but these are
just kind of categorical variables so
they can take on some values in some
set gay one through JN or just just a
set of integers for example and and you
have the constraints that are they're
all there are linear constraints but
they have the coefficient a ki jai but
they also have a term which is a an
indicator variable for whether variable
variable i takes on value J so this is
just the Iverson notation it's one if
variable I takes on value J and 0
otherwise so these are all the concerns
that are all packing constraints you
also the additional constraint that ever
variable has to take at least it has to
take one value out of some range and you
want to find just some values for the
variables would satisfy all the
constraints there's not necessarily an
objective function here you just want to
find a feasible solution so it's a
packing your packing constraints but you
also have assignment constraints because
you need to assign every variable one
value from a set so you can't solve
these problems exactly you can you can
approximate them and there are a lot of
different ways you can talk about
approximating these types of integer
programs and we're talking about mostly
about the integer covering problem
because it's a lot simpler to describe
so one scheme is you won't need to
satisfy all the constraints exactly but
you want to minimize the objective
function you want to get the objective
function as close to the optimal one is
possible you know you there are other
types of approximations where you might
only you know approx satisfy the
covering constraints and so on but we'll
just talk about this variant where you
satisfy the cover and concerns exactly
and you satisfy the objective function
approximately so one type of
approximation algorithm is is based on
the LP relaxation followed by randomized
rounding paradigm so the first step is
you replace every constraint that the
variables have to be integers with the
constraint that they have to be real
numbers and if you do that then
you get a linear program which you can
solve exactly and you can get a
fractional solution and the fractional
solution has value less than the optimal
one so the next step is you want to find
an integer value for the variables
that's cool that makes it close to the
fractional value for the objective
function and we're actually do something
a little bit more general we're going to
create a random process with the
property that for any individual
variable the expected value of x I the
integer value X I is that most some
parameter beta times the fractional
value X hat I and this will be true for
each variable individually in this case
it's automatically true that the
expected objective function expected
value of the objective function is the
most beta times the optimal one so you
you automatically get a beta
approximation algorithm but in fact it's
kind of an oblivious approximation
algorithm because when you're running
this algorithm you don't actually need
to know what the objective function is
reading this yes yes so the the x i
satisfy the feasible bility constraints
with probability 1 and they they have
this expected value property which kind
of automatically gives you an oblivious
approximation algorithm at least an
expectation you by just repeating this
algorithm multiple times you can get
very close in actuality to the expected
value so I won't talk about that issue
it just getting a good approximation
ratio and expectation will be good
enough for us so the simplest brand is
rounding scheme as you just draw the
variables to be Bernoulli independently
with a probability which is slightly
bigger than X hat I and I'm going to
assume here that all the values of x had
I are very small so that you can you can
multiply them by constant factor by you
know small constant factors and you
don't have
worried about them becoming bigger than
one and and that being probabilities
anymore it turns out that that's the
hardest case to deal with and in
reducing the general case to that is
kind of cumbersome so I won't really get
into that now I'll just assume that X
hat I is small so if you do this then if
you look at any individual covering
constraint well the expected value of
the variables X I is at least equal to
alpha times AK because the fractional
solution is equal to AK so you have a
sum of independent 01 random variables
with mean alpha AK and you want to know
what's the probability that it is
actually at least equal to AK in in a
quality not just an expectation greater
than equal to a que ya and you can use
the standard Chernoff bound and if you
do this and you set the parameter alpha
to be about 1 plus log M over the
minimum value of a of the right hand
side plus this square root term you have
to remember that either a min or m could
be big so it's possible that a min is
going to infinity so in that case in the
case that a min is very big and m is
small then the square root term becomes
a dominant one and you get close to
approximation factor 1 so if you set
alpha did this value then all the
constraints are satisfied with high
probability and you control the expected
value of the excise given that the
constraints are satisfied is still close
to alpha so you get this same kind of
approximation ratio 1 plus this term
which is like log of M over a min plus
it's square root of that same value so
it's the standard Chernoff bound
standard random is running so one
problem with this type of approximation
algorithm is that the dependence of the
approximation ratio depends on the
overall number of constraints which is
kind of
I can go to an fid knee as the problem
size becomes big so we you often like as
a scale-free approximation ratio one
which does not depend on the overall
size of the system but kind of only on
its structural property and one very
common way of getting this in this
context is in terms of the system is
column sparse that is every variable
appears in relatively few constraints so
there are two ways you could measure how
sparse how column sparse the system is
two common ways are in terms of the l0
or l1 norms of the columns so the l0
norm is just the number of nonzero
entries in every column and the l1 norm
is just the sum of the coefficients in a
column and remember we've scaled all the
entries to the coefficients are in the
range 0 to 1 so the l1 norm is always
smaller than the l0 norm and it's
possible that you could have systems in
which the l1 norm is much smaller and
these are both much smaller than em so
can you get an approximation ratio which
is a function of these columns varsity
measures not the overall system size so
it was previous work by sreenivasan
which gave an approximation algorithm
and it was based on a random process was
analyzed using the fkg inequality I
won't get into a lot of detail with it
here but it gives you an approximation
ratio that has this form sorry there's a
error on the slide there should be an
extra term here there should be an extra
term but when it for the square root
trim their should be an extra term log
of a min over a minute I left that off
the slide you get in this approximation
ratio and the the work of sort of
Awesome was not based on the lavash
local em up but the lovas local emma is
another a very standard technique for
getting these kind of scale-free
approximation ratios and you could use
that tool to get a similar approximation
ratio although that was not the approach
taken basement of acid
so let's review the basic form of the
low-pass local Emma and how it would
apply to this problem first so in the
low-pass local Emma you have bad events
in some probability space and in our
contacts a bad event would be that one
of our covering constraints is violated
and these these bad events involve a
subset of the variables so in this case
the variables are the integer variables
you're drawing which are Bernoulli pie
and you have this a separate bad event
for a recovering the straight namely the
sum of the variables is less than a sub
K which it's supposed to be and the the
key property and understanding the local
M is is to decide whether bad events
affect each other and in the case of the
local lemma this would be bad events
affect each other if they overlap on a
variable if there's a common variable
that affects both of them so one thing
you have to be careful of in the local
llama context is that there's a very
binary classification of whether a
variable affects a bad event or not that
is if the bad event is a function of
that variable then that variable affects
that bad event even if the effect the if
it's hardly ever affects it even if the
amount of the effect is very small the
local lemma just says does this variable
effect that bad event and you could
imagine a system where all the
coefficients are nonzero but are all
tiny and in that case every variable is
affecting every constraint and so
everything overlaps with everything else
so this is why you if you use the local
lemma you'll always get an approximation
ratio this way it's phrased in terms of
the l0 norm of the of a column the
number of nonzero entries because an
entry which is very small but nonzero is
the point of view of the local lemma
affects a constraint just as much as if
the coefficient were big even though if
the coefficient is very small you might
think that heuristic alee it shouldn't
really matter for that constraint so
you'll get that's why you get these
Delta 0 terms
in the approximation ratio if you use
the local Emma all right so the local
Emma by itself is not a constructive it
only shows a very small probability that
you satisfy all the constraints so
that's not an algorithm outright you can
turn into an algorithm using the
framework of Mozart our dish which turns
almost all the applications of the local
Emma into constructive algorithms and
you could use it for this this problem
just like you could use it for
everything else it's basically work like
this you you begin by drawing all your
variables from their original
distribution bernoulli p and if you find
some cover and constraint is violated
that is the the sum the sum of the
variables is less than the right-hand
side a sub k then for every nonzero
coefficient a sub ki you draw X I from
its original distribution again evasive
ki is 0 then you don't kink its value
you just leave it alone and if you set
alpha to be that same value for the
approximation ratio of this algorithm
converges so I just want to talk just
heuristic alee why this algorithm even
though it is kind of the generic way of
transforming the local Emma it doesn't
really make sense for this problem so
suppose you come to a violated
constraint some some covering constraint
k is violated well if x sub i is equal
to 1 then the algorithm says you should
still you still might need to resemble
that variable if the coefficient is
nonzero but why I mean if X sub I is 1
then then that variable is kind of
helping that constraint be satisfied so
you're kind of make going in the
opposite direction of the progress if
you're resampling it and maybe setting
it to 0 your kind of messing that
variable up that variable is helping you
you shouldn't change it and if x 0 x is
0 then probably XII is not really at
fault for violating that constraint it
probably didn't have a very big effect
on that constraint I mean most of the
variables would
may be expected to be equal to 0 anyway
so that variable is probably not causing
that constraint to be violated you could
think that the guilty variables the ones
that are causing the constraint are the
difference between the actual number of
0 variables and the expected number
that's why the constraint is violated is
you had fewer variables being one than
you expected so only about square root
of them are kind of the difference
between what you what the mean is and
what you expect to happen in a in a
deviation so you should really be really
resampling about maybe square root ASAP
a of the variables not all a sub K of
them so the most erotic algorithm is
really resampling wait too many
variables for constraint so instead of
resampling all the variables will use
partial resembling and this is actually
a very general framework which extends
the local Emma in in a very general way
you can apply to many problems involving
Latin transversals packet routing etc I
just want to describe how this applies
to the integer covering problems integer
programming problems where I don't
really need to get into the full
generality of the framework so for this
particular application here is how you
would apply partial or sampling so again
you draw x1 through xn through from the
original distribution and now if you
come to some constraint that K that's
that's violated you do this if X I is
equal to 1 then you just leave it alone
it's helping you so don't mess with it
if X I is equal to 0 then you resample
it but you don't draw it with the
original probability P I you draw it
with a smaller probability this
probability is a depends linearly on the
coefficient a sub K I and it also is x
another scaling parameter Sigma so you
can see that um if the coefficient is 0
you never resample it just like in the
local Emma but this is kind of smoothly
interpolating between a 0 coefficient
and a coefficient of 1 and also you can
see that the values of x I are always
increasing over time
so you you never change a 120 you only
change 0 to 1 so this algorithm
obviously terminates so the only
question is what's the expected value of
x I at the end of this process because
it because it certainly will satisfy all
the coupling constraints and we will
show that this satisfies this type of
approximation expect a proximate in
ratio with this expected value or the
expected the probability that variable
is equal to 1 is a small multiple of the
fractional value which will
automatically give us our good
approximation ratio so we're going to
analyze this algorithm in a kind of
strange way so if we come to a
constraint k which is violated the
algorithm says you resample exile with
probability Sigma times a sub ki times P
I so instead of thinking of it as
drawing X I as a Bernoulli random
variable with this probability you think
of it as a two-step process you have a
set of variables Y and each variable I
goes into the set Y with probability
Sigma times a sub ki and then you look
at all the variables and why and you
draw them as new grenouille variables
with probability P sub I so this is
obviously equivalent this two-step
process but this kind of to step way of
thinking of things ought to be very
important to analyzing the algorithm
even though it's kind of weird that
you're breaking apart for no no really
good reason so our goal is to bound get
an upper bound on the probability the x
I is equal to 1 and in order to do
though so we're going to construct a
kind of a witness which explains why you
set x 2 x equal to 1 this witness is
going to be a just a structure which is
kind of a the explanation for that
variable then you're going to take a
Union bound over all possible witnesses
and then the expected value of x I is it
most the sum over all these witnesses of
the probability of seeing that
particular witness so this is the same
proof strategy for the original Moser
and tardive algorithm but our witnesses
will be much simpler than theirs
before I talk about the witnesses for
this algorithm I'm going to try to
motivate this approach by talking about
how you just do witnesses for a standard
Chernoff bound not any kind of algorithm
resampling algorithm just turn off
bounds for the lower tail so suppose you
have n independent Bernoulli P variables
and mu is they're mean and you want to
bound the lower tail to probably the sum
of these is less than T or T is
something that's smaller than the mean
so consider the following process if the
sum of the variables Z is less than T
you mark a subset of the variables you
mark them how if zi is equal to 0 then
it gets marked independently with
probability Sigma otherwise you do not
mark it well since you only have any
mark variables at all if the sum of the
variables is less than equal to t you
have this obvious equality the
probability that's less than equal to t
is the sum over all possible subsets of
2n that v is the marked set of variables
and v you can think of it as a witness
the sum was too small now suppose you
consider some fixed v subset of
unendurable the following are necessary
conditions for v to be the mark set well
first any variable inside v had to have
z is i equal to 0 and that has probably
one minus P to the size of V any I and V
had to be marked well that has
probability Sigma to the v the third
condition is that any integer which is
not marked but still had zi is equal to
0 must have any variable which is not
envy but it was equal to 0 must have
been unmarked otherwise you would have
put into V the last term well you take
the product of 1 minus Sigma over all
variables which we r equal to 0 but not
envy but the key point here is there
have to be at least n minus t minus the
size of v of them in order to mark
anything otherwise you would not have
had the sum of Zi less than equal to T
so this last term is it most equal to 1
minus Sigma
to the power of n minus t minus the size
of V so the overall probability that V
was the dumb mark set is it most the
product of those three terms and if you
sum over all V you get the probability
that the sum of Z is less than T is it
most that expression there so that's
true that that bound is a valid bound
for any Sigma in the range 0 to 1 so we
can optimize it and when we do and do
some further calculus we get the
following expression which is the
classical turn off bound so you've kind
of given a witness based proof for the
standard turn off lowered hellbound so
if you really keep this example in mind
as we talk about other witnesses which
are more complex so we need to give
about a kind of a witness bound not just
for the event that the initial values
for these variables failed the to
satisfy some constraint but the values
of the some variables after multiple
rounds of riesling failed to satisfy
some covering those constraints there's
going to be a more complicated witness
the same intuition will apply so for any
constraint you can list all the result
sets for those constraints for that
constraint remember we have this
two-step process first we choose a set Y
and then for every variable and Y we
draw the variables from with probability
P sub I so y sub K 1 Y sub K 2 are the
rasam hold sets the the first set you
draw in this two-step process so there
are two ways you could have had some
variable X i equal to 1 well first of
all in the very first step of the
algorithm you could have drawn x sub i
is equal to one in that case your
witness structure will just be the empty
empty list will just be the null
structure the second way you could have
it is during the L for a sampling of
some constraint k you set X I is equal
to 1 for the first time and in that case
the witness will be the list of sets y
sub K 1 through y sub K L and you
necessarily have to have variable I
inside wise
KL because otherwise the variable X I
won't change during that row sampling so
the witness will be the that list those
list of sets um yes sorry here's the
constraint what is the 120 these are the
coop might need to do multiple rounds of
resampling in order to fix a constraint
yes then so for simplicity in order to
explain this this argument let's just
say L is equal to 1 so we're just
dealing with the simplest type of
structure which is a single set for some
constraint k so you have a potential
witness which is just the set out you so
you're talking about so so fix them set
Z sub K 1 what is the probability that
the actual witness that you generate for
this variable is equal to this fixed
value Z sub K 1 so y sub K 1 is a random
variable Z sub K 1 is just some fixed
value which is just a set a subset of
the variables so the following events
are necessary in order to have Y sub k 1
equal this fixed set Z after the first /
sampling of that constraint k you must
set x of i equal to 1 that's what just
what we said that on that that was the
definition of the the length of the list
of the witness is the time when variable
X I've got equal to 1 for any variable
in that set you must have set X sub J
equal to 0 initially during the initial
ring of the variables y if x sub k is
equal to 1 initially that will never be
resampled in particular it cannot be
resampled during the first four sampling
of constraint k third that set Z was
chosen as the first resembled set for
that constraint k those are all
necessary conditions in order to have
this particular witness structure so
just writing those conditions again the
first event has probability P sub I
because
at every state every time you fix a
resembled set the variables are drawn
independently from their distribution
pi/4 any variable inside Z the second
event has probability 1 minus p j and
those are all independent because
they're all based on the initial
sampling and for the third event well if
you consider the current value of the
variables at the time you were sample
that constraint so not their initial
value but whatever value they had just
guessed at the time you were about to
resemble them again if a variable had
value 0 then it can't then it goes into
the set Z with probability Sigma a sub
KJ if VJ is equal to 1 then it cannot go
and does that into Z again letting v1
through V and denote the current value
of the variables just at the time you
were a sample k so the probability that
you set y 1 equal to Z is this product
where it's in terms of the current value
of the variables that for any variable
which is equal to 0 um it goes into Z
with probability sigma x akj and if it's
equal to 1 you don't it definitely does
not go into Z and again you make the key
observation that the constraint k is not
currently being satisfied in order to
have it being more sampled so the sum of
AK JV j is less than a k at the time it
was resembled and if you plug that into
this expression you see that this term
there can be upper bounded by 1 minus
Sigma to the minus AK so if you put this
all together you see that the total
probability even have been countering
this witness structure is is it most
this probability which you see and if
you some now overall value so this is
just for one particular type of witness
structure the simplest type that has
just a single set in it so now if you
sum over all possible witness structures
including allowing k and now
two very you can get this expression
here and you see that you have the sum
of a sub ki so you have the bound you
can put in is the l1 norm of that column
and the last line you just have to
choose Sigma and alpha carefully in
order to you know you basically optimize
them in order to minimize that
expression which is kind of involved but
it's nothing to too interesting so you
get this dependence on Sigma 1 instead
of Sigma 0 because you actually have the
sum of the Aki and in fact the
coefficient there is one so it's not
constant it's not one plus constant
times that term it's actually one plus
that term that constant for that that
first term is actually equal to one so
you get this this bound on the expected
value of any variable and that
automatically gives you that same
approximation ratio so we've talked
about before so let's see if we can get
any lower bounds on this approximation
ratio how close is this approximation
ratio to optimal well well for one thing
when when the minimum value a min goes
to infinity the nur approximation ratio
basically is something of the order of 1
plus 0 square root of the natural log of
Delta 1 plus 1 over a min so how close
is that it is that top them well that's
one half of the asymptotics how the
other half is what happens when Delta is
very large in that case you get an
approximation ratio which is basically 1
minus some little o of one term times
the natural log of Delta 1 plus 1 over
amen so that actually is um is optimal
if you reduce to set cover so the
hardness of approximating set cover
shows you that that at least the first
order term the first order term is
optimal including the optimal constant
but that kind of hardness ratio is
really vacuous when a man goes to
infinity so the hardness of set cover
gives you nothing when a men is large
because that's you're saying the
hardness is some value less than one
which is stupid i mean the approximation
ratio can't be better than one so
previously there were no nontrivial
bounds that we're actually known in that
regime when a man is much larger than
delta yes yes so here's actually
construction of an integral T gap for
the case when a man is as large so you
can kind of you can consider the
following in it you're covering system
so for all you think of I and K that I
is the set of variables in case a set of
constraints as vectors over GF 2 to the
N and you can consider a system which is
defined by the sum over all I us which
are perpendicular to K that should be
eyes that such that ki is equal to 0 I
left that off my slide of those X I is
at least equal to a so you have 2 to the
n constraints into to the end variables
and the objective function is just the
symmetric one where you just summing
over all X I so this is a very simple
fractional solution where you just set X
i equal to a over 2 to the N minus 1 and
there's a previous analysis by vazirani
which which was really only targeted for
the case when a is equal to 1 just the
the simplest case which showed that any
integral solution has to satisfy that
sum of X sub I has to be at least equal
to N and this kind of gives you an inner
gravity gap on these types of energy
recovering systems but this analysis I
don't know sorry
but again this this analysis is not
helpful when a is large because the
integrality gap that claims is not even
bigger than one so one result we have is
that any integral solution actually has
to satisfy a stronger condition which is
it has to be at least equal to 2a plus
Omega of n and with this basically
amounts to showing is that any sparse
boolean function has to have a large
Fourier coefficient for our coefficient
meaning / GF 2 to the N and this shows
you that this covering system actually
has an integral T gap of 1 plus
something on the order of log of Delta 1
plus 1 over amen so our approximation
algorithm kind of is almost optimal it
has a square root instead of a linear
dependence on that term so it's it's
kind of close but there's kind of an
interesting polynomial gap there this is
but this is the first non-trivial bound
at all for what happens in the regime
when a min becomes large another kind of
interesting if you talk about what this
algorithm is what happens if you have
multiple objective functions so you
might have some method of a balancing
them so let's say you want to yo u of l
different objective functions and you
want to minimize the max of them so can
you tryn let's say you can solve the
fractional relaxation of that however
you decide to balance them if you decide
to take the max of max of the men's then
that's just can still be solved using a
linear program can you get a solution in
which all L objective functions are
simultaneously close to their fractional
values so this is one way in which other
types of algorithms for set cover and
related algorithms don't really extend
if you have a group the other main
algorithm for set cover is a greedy
algorithm where you always choose
the set which kind of increases your
coverage the most in a single time step
but it's not even really clear how you
define a greedy criterion if you have
multiple objectives I mean you kind of
need to boil all the objectives down
into a single number in order to make a
decision about what variable to accept
and there's not any obvious way to do
that so the greedy algorithm has a real
hard time even getting started for these
type of multiple objective problems the
LP based solutions can handle this much
more cleanly so we can show is that not
only is the expected value of sia of CX
close to the fractional value but it's
actually concentrated in a very similar
way to a Chernoff bound would be
concentrated to the Chernoff bound
concentration around this value beta
times clx head so with high probability
you have that for every individual
constraint see all that X is close to
beta times see all that X hat and so
with high probability you get all of all
the objective functions are
simultaneously close to their means this
doesn't follow this from this expected
value property for the variables and the
way you do this is you show basically
abound on the correlation the bound on
the product of monomials so we
previously showed that the probability
that any individual variable is equal to
one is at most this term moroso by where
r 0 so x is equal to the probability
plus some small approximation terms so
in fact you can show that for any subset
of variables you have this bound on the
probability that they're all
simultaneously equal to 1 which is the
product of the row eyes so this is
basically the same as would be as if
they were independently drawn as
Bernoulli with probably row I and
particular this type of of a monomial
product property is enough to give you
turn off upper tail bounds
and the way you feel that is given the
set are you can build a witness for the
event that all of these variables are
all equal to one not just about a
witness that any individual variable is
equal to one and the way you do that is
if for each variable that was equal to
one you find out which resampling of
which constraint mated equal to one and
you list all the rosettes for those
constraints before that last time when
that variable got equal to one so some
of these lists might appear twice
because you might have two different
variables which would be or equal to 1
on different row samplings of the same
constraint but that's ok you just list
both of them and you can do a very
similar thing where you use some over
witness structures to get this joint
probability property another extension
you can talk about is if you're given
multiplicity constraints so in the
statement of the problem I just said
that X I has to be an integer of
unbounded size but you can also consider
a version in which they're up
constraints on the up size of the
variables not just an integer of
unbounded size but some upper bound on
its on its size D Sub I these are called
multiplicity constraints so these can be
easily incorporated into the linear
relaxation they're not very easy to
incorporate into a greedy algorithm but
the LP based approach can easily put
them in but can you still get a good
approximation ratio while trying to
preserve these multiple City constraints
so if you just analyze the algorithm
straightforwardly you will see the
solution size can be much bigger than
the fractional value so I was only
talking about the case move to the
fractional values were all small but it
turns out the reduction from the general
case to the case when they're all small
loses a lot in the solution size
possibly so coolio pose and young gave
an algorithm that you can't respect them
exactly but you can violate these
constraints compared to the optimal
solution by a 1 plus epsilon factor and
if you do so you have a term in the
approximation ratio which is basically
on the order of 1 over epsilon squared
and if you want to satisfy them exactly
you can do it but now you get an
approximation ratio which doesn't depend
on a and depends on the log of Delta 0
so we show that if you just modify a few
parameters of our algorithm don't make
any other changes just basically change
alpha and Sigma 2 new values then you
can still get this expected value
property where now the approximation
ratio is only inflated by a factor of 1
over epsilon and you can see that this
improves on the result of quill APIs in
a lot of different ways uh so these
deltas can be Delta 1 so we get delta 1
instead of delta 0 you've got one over
epsilon instead of 1 over epsilon
squared and so on and in fact you can
feel a hardness of on the order of
natural log of Delta 1 plus 1 over a min
times epsilon so this is essentially
optimal approximation ratio for this
case when you have these types of
multiplicity constraints all right so
now I've talked a lot about the integer
covering problem so I'll talk about how
to extend these type of analysis to the
assignment packing problem so recall the
assignment packing problem you have
variables which take on values and some
range jji and you have all these
constraints which are all sums of
indicated variables for the for these
variables and you want to find some
values about values X which nearly
satisfy the constraints so the usual the
first the first step is you can find a
fractional solution which satisfies all
the constraints if new integer solution
exists and if you use a simple
application of the local lemma plus some
randomized rounding you'll get an
approximation ratio looks like this
where you have Delta Z rose again the
same issue that if a coefficient is
nonzero but tiny then the local lemma it
means that that variable affects that
constraint so again you could use the
most erotic algorithm to solve two to
solve this of the local to get an
algorithmic form
what the local lemma gives you and again
you have the same issue suppose you come
to some violated constraint so the
straightforward application the most
retarded algorithm would say for any
variable with a nonzero coefficient you
have to resample that value that
variable but but that doesn't make any
sense you know fistic alee if the
coefficient is tiny and that durable has
almost no effect so you probably
shouldn't be resampled it and if that
ver and if that variable takes on a
value different than the bad one then
that variable is helping you so why are
you changing it so you can use a similar
type of resembling partial or sampling
if you have a very violated constraint
we should do is you should choose about
the square root of our variables to a
sample our is the right hand side of
this assignment packing constraint and
you shouldn't choose the variables you
shouldn't use the set uniformly at
random among all sets of that
cardinality we should is that the the
proper the probability of choosing any
set should be proportional to the
product of the coefficients the
corresponding coefficients and once you
choose this for sampled set you draw the
variables from the original distribution
and this this avoids all the problems
you least arista Caillat of the loser
tardish algorithm and again why about
square root of our well it's the
difference between what you expect a bad
event to look like and what the
expectation is so the expected number of
variables that are going to be true in
this constraint is about our it's about
our because that's what the expected
value expected value of the sum was and
usually if a sum of random variables is
bigger than its mean is about a standard
deviation from its mean so about only
about square root R of the variables are
kind of doing
being worse than you expect so those are
kind of only the guilty variables that
are that are really causing the problem
that's kind of the intuition between y
you C square root of our yeah but if you
just think of a typical bad constraint
it's probably typically bad because it
was about squared bar I mean this is
just kind of the intuition no no if it's
you don't need to pick more depending on
the violation then the number you pick
you kind of the number you you pick is
kind of optimized for the smallest
violation so if there if it's a bigger
violation you can just kind of ignore
the extra variables so you can cure that
this algorithm terminates with
probability one with a small number for
samplings and as long as you and you get
a value of x I which they don't satisfy
the original constraints exactly but
they have a small discrepancy term and
and it looks like this we're again
instead of the Delta set at the l0 norm
you get the l1 norm if you look closely
you can also see that instead of log in
the second term you get the square root
of R times log Delta one so you also are
saving a square root of our times log R
term which is relevant when r is much
bigger than Delta so it's saving that
term as well so an example application
of this is a multidimensional scheduling
problem so you have some machines and
jobs and every job has D dimensions of
cost and you assign this to some machine
and you want to minimize the total
makespan not just the makespan in one
dimension like time but the makespan
overall D dimensions so you want to
minimize the maximum the sum of all the
cost to that job that the maximum
overall dimensions so a simple algorithm
is you can first guess what the optimal
makespan is and then you can form an LP
solution which an LP which is feasible
and if some job costs more than this
makes band then then you can't do it on
that machine so you just set that
fractional variable equal to 0 otherwise
you just allowed to be some fractional
value and you can just plug this in
almost directly that feat that
fractional solution into this partial
resembling and you get a solution which
makes pan T times this extra factor
which is logged the over log log D so
you're getting a log D over log log the
approximation to the optimal may expand
this is not the the only way to get this
type of approximation ratio maybe it's
not even the best but it is certainly
really simple once you have this
assignment packing framework you can
basically just plug this LP directly
into this assignment packing framework
almost for free another thing this type
of analysis gives you is that you can
handle cases in which you have different
right hand side values that you can get
an approximation ratio which kind of
trades off between us a multiplicative
factor and additives plus standard
deviation factor this is very difficult
to do for other approaches to these
types of assignment packing problems so
again you can analyze this algorithm by
building witness trees it's very similar
to the mosser tiredish algorithm
analysis but the key idea is the only
keep track of the resembled variables
and the values they take on if a
variable was not resembled you just
ignore it when you're building the
witness tree so if you were sampled
three different constraints and you
selected three different resampled sets
you'd build a witness tree in terms of
these just the sampled sets and you
don't say anything about s2 if it's not
in the witness tree the key lemma for
this is you can show a witness tree
lemma which is that if you have any
particular witness tree whose nodes are
labeled by resampled sets and the
probability of encountering
tree can be bounded in terms of this
resembling probability those times the
probability distribution on the
variables it's a similar proof to the
current problem and the key idea is that
the total number of expected of
resemblance is the most equal to the
this sum over all witness trees of the
property the tree and by only keeping
track of the sample variables you are
greatly reducing the total number of
witness trees because most variables are
not being counted in the witness trees
so you're losing information but you're
also really reducing the total number of
witness trees are considering so this
sum is over a much smaller set by
ignoring those variables and so the sum
becomes smaller so wondering whether you
can continue to apply this type of
partial resembling methodology to other
types of integer programming problems
packing problems mixed problems involved
in covering and unpacking so we have
bounds in terms of the l 1 and l 0 norms
but can you also get any bounds in terms
of higher higher norms and also this
kind of interesting question is what is
the approximation ratio for the covering
integer problem when a men is becoming
large is it you have that square root
turn there or not IIIi think the linear
term is the correct one so I guess
that's all thank you
approach where you were using a turn of
concentration yes and when you have but
you know when you have the variables
that are very small then there are much
better concentration qualities to use
like first event give you better
estimates then just uh my impression was
that if you have you know an infinite
number of very small values then then
the chair up bound is is kind of the
crackers I mean it approaches a Poisson
which is basically what the Chernoff
bound is ok I guess it doesn't lose a
local by my comment it would be just
that the using the was accomplished out
yes yes is there a way to apply the
local levels and genesee in bronze
visited multiple times there either
there is an economy has this approach to
assignment to a similar type of problem
and we basically kind of quantized the
coefficients you know the coefficient
between half and one you do resembling
on those and then you get some
discrepancy then you look at the
coefficients in the range you know
quarter to half you get another
discrepancy but the discrepancy is
smaller and so if you sum them all up
you still get some you know small value
so you can do that it's kind of
cumbersome to use this multistage
approach and I don't know how general it
is can't you know</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>