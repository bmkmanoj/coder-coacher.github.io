<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How Eye Tracking will Revolutionize Device Interaction | Coder Coacher - Coaching Coders</title><meta content="How Eye Tracking will Revolutionize Device Interaction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How Eye Tracking will Revolutionize Device Interaction</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/92AAAo-Pvj4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay
so let's get started it's my great
pleasure to introduce Dixon Cleveland he
is president and CEO of AOC technologies
I met him last year at a you looking
company's on I movement and again this
year last month at H ha I killed
checking coffins
I have tried his system you will see the
demo little I believe this is a past
season really in available off-site
that's why I'm very excited to have him
here and demos the system and today the
talk will be very special
this is no screen this the first time I
host a talk without you know any
computer or slides
you're gonna last a new user I need your
help
what's wrong with this picture can't see
where I'm looking you need to know where
I'm looking why do you need to know
where I'm looking
why didn't want why do you need to know
where I'm looking you can see through
the glasses oh I see do it would point
my face that's exactly like so the deal
is that is really important we as humans
make eye contact that's just one of the
most important means of communication we
have I may be talking to you right now
but you're reading my face as much as
you're listening to my words computers
need to do the same thing they put up
these marvelous displays you see all
those beautiful graphics and these big
screens with all kinds of pixels and
color depth and all that sort of stuff
does that computer have a clue what
you're looking at when you look at that
stuff doesn't have a clue just does not
have a clue should it yes it's dealing
with a person we do it our eyes are
built we got weights in our eyes right
next to these dark irises so when I look
one way or another you can tell
immediately my eyes are moving around
it's a communication mechanism it's the
fundamental basis of all of this so can
computers actually see our eyes the
answer is yes my trackers are out there
they're around and I'll see technologies
happens to build a pretty good one this
can be done this whole business of
putting an eye tracker
it'll nicely one day it'll be a sugar
cube sitting right in the very bottom of
your device whatever device it is great
big screen fine sugar cubes sitting
right there all the way at the other end
of the spectrum you're going around with
your handheld device sugar cube right in
there looking back at you figuring out
what you're doing with your eyes
communicating with you as if you're a
human being that can see so I don't want
to talk today about a lot of a
physiological basis of what's happening
inside our head how our brains work such
that we can make these decisions what
what's going on behind our eyes
unclouded
what's going on inside our head so the
whole story basically starts with light
the universe started with light great
big that's the universe is the internet
light going all over it's going between
the planets it's going between the
galaxies it's going between people it's
actually happening down on a microscopic
level everything starts with light so
then human life comes love and we humans
or any life-form
has to figure out what's in this
environment well it picks up photons so
you have to have some mechanism to pick
up the first times that's the way you
figure out what's going on in the world
and so we build these things and in the
human they became our eyes and it's a
pretty fantastic device it's an
amazingly fantastic device the
engineering that went into designing our
eyes is just outstanding
what is why did why we go to all this
trouble well we got to survive we got to
find food we got to do things see what's
going on out there we got two objectives
when we're envisioned one is this big
thing you've got to be able to see
everything
if a bear's over there and that bear is
gonna come after you better see it in
your peripheral vision at the same time
if we're gonna look at something in
detail that we can use it in a way that
we really want to be able to use it
we've got to be able to see with very
very high resolution and so nature did
this funny thing it decided instead of
just building our camera with uniform
pixel density every place it puts this
really really high concentration of
cones at the very central part of your
vision and your peripheral vision it's
got 70 times less resolution than you do
in your central vision this is a
beautiful solution to the problem
because you can get very very high
resolution image when you look at
something and you still get to see the
entire world so peripheral vision versus
central vision but this creates all
kinds of problems and one of those
problems is that if you want to look at
something you have to point your eyes so
if I want to look at you I got a point
get my eyes over there if all of a
sudden I want to look at you I got to
point my eyes over in this direction to
look at you but there's a wonderful
piece of serendipity here and that is
that you now can tell what's interesting
to me I'm interested in you so I look at
you I'm interested in you so I look at
you that's cool we're communicating now
so the reciprocity of optics is a fairly
important concept here and that is if I
can see each other if I can see you you
can see me cool
let's build on that and that's why I
trackers can be built we are the best
eye crackers there are you don't need to
go by an eye tracker you got two of them
right there and you got this visual
cortex in your brain that just does our
tracking like mad it's perfect I wish we
could build a system quite there was
just that good but we're getting close
to that we start to do that today do
that today it's still pretty expensive
and it's still pretty clunky but it's
doable
the existence proof is out there you can
build those our trackers so what else
then happens in the brain let's talk a
little bit more staying with the eye for
just a second let's talk about a couple
other parameters we've said we've got 70
times the density of cones and the
center of central vision of the eye then
we do out in our peripheral vision
that's true well what is the scope of
that basically if you take that very
high central part the fovea on the
central part of the macular region and
you hold your thumb out at roughly arm's
length that foe viola covers about the
size of your thumbnail so when you point
your eyes I don't want to look at your
eyes I stick it out there that's some
covers about this much of your face
you're sitting what 10 feet from me so
your eye has to point at least that
accurately to get the information that
it wants so the eye is a pretty damn
accurate when they point well nature has
a problem now if it's good if it's let
me back up a second I forgot an
important piece of this puzzle and that
is if you were to have the same
resolution of pixels all over your
retinal vision it turns out that your
optic nerves would be about this big
around the visual cortex to process all
of that would be about a half a cubic
meter and that's a little bit difficult
to carry around just won't work so
nature really had to to get high
resolution in your central vision it had
to really concentrate down and it went
to all kinds of extremes to get high
resolution in the center out in your
peripheral vision there's a cell body
for each rod and cone out there when you
start to get in towards the central part
of the vision it can't get the density
that it needs by putting a cell in every
location what it has to do back at that
point is put the cells right around the
outside of macula region and nothing but
the wires and the in the receptors
themselves are at that very central part
where they're 70 times the cone density
is there isn't in the rest of your eyes
so nature went all this trouble to do
this and in the process of doing that it
then come up with the problem well you
got a point guys you have to move them
so then it came up with the ocular
muscle systems and those ocular muscles
are the best muscles you got your body
you don't think about them all this
stuff is kind of unconscious and we'll
get into the unconsciousness trip a
little bit later but what's happening is
that those the muscles have to be really
really precise and they have to be
really really fast when I look and focus
on you back to the photon problem I'm
only getting a certain number of photons
so there are photons coming in bouncing
off of you some of them some of those
photons happen to make it through my
people back onto my retinas how I can
see your eyes and when that happens
there aren't many photons left there are
pretty few at that point so even though
they're God's iam the photons floating
around the universal number that get
into my eyes pretty small and so
nature's got to make use of this far
times the best way it can one of the
ways that it makes use of those photons
is to put well you when when I take a
picture and get I have to hold my eyes
still for a certain period of time in
that period of time is about 250
milliseconds so I have to go over and
I'll fixate hold my eyes still for about
250 milliseconds wait for all those
photons to come in develop enough of an
image that can then go back into my
occipital lobe and get that thing get
that image processed and make sense out
of it
so nature's got a problem at this point
that is it's got a hole that eyes still
within a couple of pixels for 250
milliseconds that's a pretty astounding
engineering problem but it doesn't those
muscles do it they're like no other
muscles in your body they never get
tired they can hold your eye extremely
stable and then all of a sudden BAM they
can move your eyes 300 degrees 600
degrees per second over a long distance
and stop them on a dime and hold bloody
still that's a fantastic engineering
problem and nature solve that problem it
don't the ocular muscle system to do
that so obviously if nature went all is
trouble to design this this complicated
control system is complicated in ocular
muscle system it's important it really
is important to us and I don't mean to
heart back to this idea but computers
they're paying attention to that process
that's going on one of the reasons that
we haven't thought to pay attention to
that topic is and it's all unconscious
by and large everything we do with our
eyes is unconscious good question that's
a marvelous I don't know the answer I've
questioned I really I really don't know
I could speculate but my speculation
would be better any better newer so I
won't do that kind of speculation but I
don't know
so we've you can see now if the this one
this thumbnail is about 1.2 degrees
across that's the extent of the faux
viola and in your eyeball itself and
across that foam viola there are
approximately a hundred pixels a hundred
cones so that means that your eye has to
be able to hold still to within a
hundredth of a degree for a period of
250 milliseconds otherwise if the ocular
muscle system weren't that good it well
I have all the density of cones just
wouldn't be there so the balance that
nature finally chose is this one with
high resolution but the ocular muscle
system has to hold it that still the
reason I'm going into a lot of this
detail about some of these numbers in
the eye is that before we can actually
design a good eye tracker we need to
know what the eye is capable of we need
to build an instrument that's good
enough to measure what the eye really
does but to build it any better than
that if all we're interested in is where
people are looking then we don't need to
build it any better than that so there's
this concept of the Heisenberg
uncertainty principle of eye tracking so
there's this Planck's constant thing you
know in the real uncertainty principle
which determines exactly how precisely
you can measure something that's
emotional the analogous thing here is
how precisely do our eyes really have to
work and so that's kind of the concept
that I'm getting at here just how
precisely does the I have to work and
one of the numbers that we just sorta
derived here in this conversation was
the eye has to be held still
approximately within approximately a
hundredth of the degree for 250
milliseconds that's the engineering
design requirement for the ocular muscle
system
it's pretty fantastic yeah I can do that
so if we're gonna build an instrument we
need to be able to measure that kind of
stuff well I shouldn't say we need to be
able to but that's the ultimate target
that's where I tracking wants to go
ultimately that's the objective well
we've been talking to us about this idea
of the ID just holding still and taking
a good picture and that's true
and but even if I look at your eyes and
then my heads doing this I can still get
a fairly good feel but my eyes rotating
during that two hundred and fifty
milliseconds in order to get that nice
stable image where I can still see
what's going on so as we're driving down
the road and we're bouncing around
we still see everything fine by the way
you can tell when you're not seeing
things fine when you start to get too
blurry you get dizzy there's some
vestibular feedback that we get on our
own that says our eyes aren't working
very well so if you're playing ball and
running around like this and you're not
dizzy your eyes are basically getting
the information that they need your eyes
are holding steady and your ocular
muscles are holding your your eyes still
with respect to what it is that you're
looking at but moving ball as you go
catch it you don't think so
good question somebody ought to run that
experiment kids the goats are actually
reading in a car
around some people get carsick Larkin
every single month walking with the
cellphone yeah so there is a degradation
but there's really not quite as much
degradation as you might think
the ocular muscles accommodate a lot of
this stuff they really do they're
marvelous and accommodating well once
you've looked at something for 250
milliseconds and got a good clear image
there's generally no reason from a
photographic standpoint to continue
looking at it if that environment is
constant you're looking at a word once
you've read that word and that
information goes back in your occipital
lobe and your visual cortex processes it
and says oh that's the word something
and that word something goes up and your
frontal lobe processes the word
something you don't need anything you
don't need to look at that word anymore
you need to go look someplace else
there's very very complicated thing and
when you're actually born you have about
twenty times as many connections between
the rods and cones and your eyes and
your brains as you need and there's a
big problem with figuring out which
cones and cells how they fit together
geographically because they aren't laid
down perfectly
so nature goes through a process of
apoptosis which is programmed cell death
which sounds sounds really awful and
weird but what's actually happening is
it's figuring out which of those
connections are the right ones that make
the best use of the rods and cones that
exist and that is a process that happens
during the first several weeks of life
it begins during that period of time and
it really actually happens up through
about five years and if people if in the
process people don't
untangle that and figure out exactly
which connections are the right ones to
make your eyes work for you you have
serious reading problems
well that's beyond to cut the scope eye
tracking at this point we're going to
assume from the moment that people don't
have amblyopia amblyopia is the disease
that you get when you've got one eye
that your muscles can't control it for
example when they start doing funny
things and you and I can go completely
blind because that process of apoptosis
never does figuring out which which
cells are right and it just keeps wiping
out cells and the conductivity is until
all the conductivity is gone the rods
and cones continue to work but your
occipital lobe sees nothing it just
doesn't get any data in the worst case
but anyway that's getting a little far
afield a wonderful question actual topic
but far for you so the next thing that
the that these ocular muscles have to do
is once you've looked at one thing
you've taken the picture you've gotten
enough photons there's no more
information to be had there it's much
more valuable now to start looking
someplace else your eye then cick adds
to the next location Bing instead of
looking the ER Jews I'm going to look at
you and I move my eyes all over the
place and thence the cads the eye has to
move really high speeds and as it's
moving from looking at you to over
looking at you you don't perceive it but
the video signal if you want to think of
it in those terms
that goes from your eyeballs back to
your occipital lobe your visual cortex
stops it's a phenomenon called saccadic
suppression and saccadic suppression was
actually discovered and kind of
unvalidated in a cool way and that is
people actually flashed the light at you
while you're I was sick adding from one
fixation to the next people didn't
notice the CICC ads or notice the
flashes if they happen during that
period of time
so that's where the concept of saccadic
suppression came from do you actually
notice that when you're I've fixates or
secants from one fixation that no so
that's happening way down in a much
lower level but that brings up this
concept that your visual cortex is
processing these images but your
perception of the environment happens in
the completely different part of your
brain and that completely different part
of the brain views the world not in an
AI centric frame of reference it views
it in a world frame with Sun centric
frame what's gravity this room is out
there it's relatively stable I walk
around on a relatively locally flat
earth and so I can just set a frame of
reference out there some inertial
coordinate frame we're walked out of an
inertial coordinate frame and in your
vision sees that E a percent what you
perceive is your environment is in that
frame but your eyes are going around
collecting a little bit of data there
there's a cat over to that place with
another fixation I get a little piece of
data there and they're putting this
image together and so remember we're
doing all this because somehow eye
trackers need to be able to accommodate
all this action that's going on so
there's one other really important part
of what's going on in your brain that we
need to discuss and that is all seated
in a part of the brain called the
superior colliculus fantastic chunk of
brain the question is you can only look
one place at a time you got this
thumbnail going around you put some nail
there you put thumbnail there put but
how you choose where to put some nail
next how do you choose what we're gonna
look next that fundamental cognitive
process of ours is essential to how we
live and you can think of it this way
there's
we are always looking at when we need
visual information our brain somehow is
optimizing the process where do you
point your eyes the winner-take-all
decision where's the one place I want to
put my eyes next to get the most
important information to me right now
how does the brain make that decision
well some pretty interesting work was
done done by Doug Muniz munos up at
Queen's University and basically what he
found was and this this work is now
fairly old and that it's it's rooted
10-15 years ago and why ethereal before
that but in the superior colliculus
there's the equivalent of a map if you
were to take the folds of the brain and
the SC and lay them out you'd find a map
and if the center of that map is your
faux viola so this is an eye centric map
and in that map it starts off being
blank there's nothing in this map at all
just empty and some part of your brain
comes up and say a visual part of your
brain if you're reading it says well my
fixation right here is right now and I'm
projecting the next fixation for me to
get the mat next most useful piece of to
information is over in that chunk of
text over there so I want that next
fixation to go over there so it sends a
signal down it goes into this map and
the superior colliculus and it starts to
build a spike saying I want information
it coordinate X Y with respect to where
I'm looking right now and if there were
no other inputs eventually that spike
would reach a level and hit a threshold
and bam that would trigger your next
saccade and so you're it's a CAD would
move 13 degrees to the right
two degrees down in the depending upon
how you have the orientation in your
book in this case and band that's where
your I would go next so if you get
philosophical about this and thinking
what is going on the superior colliculus
is getting inputs from all over the
brain it gets inputs from your
vestibular system so is you sit down and
you feel something you think well maybe
I need to look at that it'll send a
signal into this map and the air see and
start building up a spike at that
location if you hear a scream off in the
distance and it's your little kid you'll
say a lot needs some attention
so that part of your brain will send a
signal down into your SC and it'll start
building a spike and this is pretty
important to you it's a hill build that
spike real fast with respect to this
some of the other ones you get if you're
walking down and you feel your balance
is going a little crazy you're gonna
trip off or something that part of your
brain will say I need visual information
here that's where I want to look next
it'll send a signal of this superior
colliculus so the superior colliculus
this map has got these spikes building
up all over the place one of those
spikes eventually goes through the
threshold that we're talking about
banging that's where your eye goes once
it goes there the map is cleared and
starts again all these pieces of your
brain have that would like visual
attention will start putting their votes
into the superior colliculus and the
superior colliculus it doesn't quote
make the decision but it adjudicates
that decision that's where the
adjudication of the decision of where
your eye goes next is made and that
process is happening how often
every 250 milliseconds exactly and it
goes on and on and on 24 hours that they
were doing that in REM sleep we're doing
that the ocular muscles ever get tired
never your eyes get tired sure you
perceive I being tired but what is it
that actually perceives being tired
it's your eyelids it's not your ocular
muscles those are kind of muscles say
I'm ready to go man I'm holding still
BAM it's the cat over there they're
happy they're just absolutely happy out
there all day long they don't need sleep
they're a lot like your muscles and
birds once they start to fly those bird
muscles and physiologically there's a
lot of similarity between those muscles
those birds just get your learner's fly
and fly and fly one of the important
things about all of this stuff so you
can start to see how this is going on
and you're kind of your brain is just
always all parts of your brain what
visual attention we've got this
mechanism for choosing where your eyes
are gonna go next and as I look at you I
can see where you choose to put your
eyes and that's very important
information to me and that then ties us
back to this place we want to have our
computers do the same thing it is
fundamentally essentially human process
and we want to duplicate that process as
best we can in computers to make them as
interactive as humanly interactive as we
can make them so if anybody's got any
questions at this point I'm sort of
finished with the idea of what's going
on
but in your brain about all this stuff
yes I don't
Micra saccadic eye do there
a theory of edge detection you need to
move your eye around a little bit small
enough in the theory there is you want
to move them at least one cone which is
a hundredth of a degree but that theory
kind of bothers me because if you do it
side to side how do you get the vertical
stuff at the same time in microscope ads
are known not to be circular too to go
in different directions they happen at
random times
it's a marvelous phenomenon and there's
another theory that says that's
corrective if you haven't set it up
exactly where you want to look then you
should go make a correction and center
that whatever it is you're really
looking at up in the central part of
your vision by the way the central part
of the vision actually has a tail
distribution that when I was talking
about the fovea or 1.2 degrees across
that's only the very central part and
the distribution drops down sort almost
like a bell curve on either side but
first off those actions are quite small
and so unless you want to get into
physiological studies of saying the
eyeball is moving in a way that you
really wouldn't expect it to move to try
to figure out whether somebody's got
some physiological issue with their eyes
but in general that's that's a different
different realm will by tracking very
important one
nice applications come out of that area
but that's not the case yeah well that
happens mostly with the rods rather than
the combs the combs have very very are
very sensitive to they're basically
considered in the black and white party
or issues they can see very well in the
dark and stuff like that but one of the
things they are very sensitive to motion
and motion is
Orton to survival generally when things
move in your environment they're more
important for how you interact with that
environment than just static things so
that's that's one of their central roles
and that's kind of a different topic
here but you do get attracted by that in
a lot of things that happen in the in
the rod systems that you detect it at
the rod level do get fed back around go
back to the superior colliculus and
you're the superior colliculus it builds
of the spike in the superior colliculus
then moves to that thing then moves your
eyes to look at that place where there
is where there is motion that's up
that's a very well-known phenomenon just
a second interestingly not know and I
don't know why that is it's a marvelous
idea
and why Nature didn't optimize it that
way I'm not really sure but basically
what happens is that your pupils stop
down to make sure you got relatively
constant amount of light and then the
ocular system continues to operate it's
at its own pace yes
I'm not I don't really know I've never
really read a lot of good papers on that
topic so I might I can speculate that
one way or another but as far as I'm
concerned there there's a lot of good
idea they're good concepts and both of
those both of those points of view so I
wouldn't say one or the other is
probably the answer is yes and yes yeah
yes that was one of the early research
done by a lotta eye tracking researchers
is trying to figure out the patterns of
newness versus and that is evolved
recently into the definition trying to
differentiate whether somebody's a
novice or an expert and so the obviously
the eye patterns pilots are a perfect
example of that
when somebody first learns to fly
they're looking at all over and who
knows what they're looking at and after
a while when they become an expert they
know what to look at and there are there
their eye patterns do change
considerably and so one of the cool
applications of eye tracking is actually
being able to differentiate when
somebody's learned something well enough
to be moved over into the into the
expert category there's another
interesting phenomenon underlying that
too and that is that when you first
learn something you learn it in your
frontal lobe you're conscious about it
you're aware of it you can't be aware of
everything so as you learn it transfers
to different parts of the brain and the
cerebellar is section is one of the
places picks up a lot of that stuff so
it just it when you walk you just walk
with your cerebellum it's in control and
it's unconscious so the learned
knowledge actually transfers from one
place to another and if you ever have
taught somebody how to drive and you're
teaching them about stop signs for
example they won't even see this
sighs then they'll start paying too much
attention to the stop signs then after
that there's actually a dip and they
quit paying attention again and you can
actually tell as the teacher while
you're watching this kid that they just
that information the frontal lobe is on
to the next task it hasn't quite made it
back to the to the cerebellar part of
the brain and that part hasn't learned
yet and they seem to have forgotten
something they're not forgetting they're
becoming an expert so celebrate that
change don't penalize them say yeah you
forgot to look at that yeah they might
have gotten in an action there's a
consequence of it but that that wasn't
slowing down their learning process
necessarily so there are the paradoxical
behaviors and that happen as well words
but your eyes it does that brings up the
whole topic that the way I've been
talking so far actually sounds as on me
your eyes are just going on or all over
the place sopping up information as fast
as they can and that is often the model
but there are often times when you don't
want visual information and in that case
you'll actually see people get to the
point where they'll close their eyes
I'll just go oh they're in their own
head their cognitive processes elsewhere
they don't want to clog up that process
with visual information that's
superfluous to them at the time and
that's one of the real problems of eye
tracking I can look at your eye but I
can't tell necessarily so I can tell
when you look here there that yes that's
the most important thing for you to look
at at the time but I can't really tell
is visual information what's really
important in the central part of your
brain at the time so there's a lot of
work to be done with
cognitive psychology to be able to
untangle those kinds of things and
really fine-tune the use of of eye
tracking information we as humans seem
to be fairly tolerant of that how we do
that the algorithms that we employ in
our heads to do it we need to go study
that stuff you had a question
so you mentioned the fatigue being
caused mostly by violence beltzman
justing that's a good question I don't
know the answer to that question my
speculation is not very much no we're
not very and that's an important topic
and our tracking is we don't want to put
too much IR light on the eyes two
reasons one is that they dry out the
surface of your eye and in that in
itself is not good for eye tracking
because the cornea and reflection image
is not as clear and can even go away and
you get congealed tears and stuff like
that so the the ability of the eye
tracker to track is not as good as it
might be and the second thing is
typically a point source of light ends
up as a point source of on back on the
retina so if there's a lot of light
that's concentrated and when one LED it
has the potential to warm up the rods
and cones that it lands on and so we
have to be a little careful about that I
actually sat on a committee in koh game
where we talked about that kind of thing
and went back to david's Lanny's work
way back in the night
70s when they were first beginning to
figure out how much light damage was
done and I trackers are fairly safe
there's really not a safety issue to
speak of we've paid attention to it at
LC technologies because we work with a
lot with people with disabilities and
the last thing you want to do is create
any kind of even even comfort problems
with their eyes that's a wonderful way
to frame that question up typically
pretty small I forget exactly what the
maximum permissible exposure number is
but when you're walking around outside
it's typically five to ten times that in
worst case but in also in that worst
case when you're outside you tend to
squint fairly seriously yes yeah so your
eye has also in that natural protection
method of squinting but as you might
guess eye trackers love to see would
love to see a naked eye ball out there
in space if it if there were just a vibe
all out there floating you could say him
a camera at that thing and figure out
where it was pointed fairly easily but
unfortunately we have these things
called eyelids and eyebrows and glasses
and stuff like that that that can
include the image of the eye and
squinting is a serious problem in eye
tracking it's just a plain serious
problem and that's that's one that we
really do need to to address more if our
trackers are absolutely going to become
ubiquitous
what's with the we could talk for a
whole hour on that topic and I really
dislike putting those numbers out
because there are so many ways that you
have to slice that up but in general we
use the bright people LLC technologies
uses the bright pupil system and that
really provides a lot of advantage to a
lot of people because we get better
contrast between the pupil and the
surround the iris and can calculate the
pupil center significantly better using
that approach and that gives better
accuracy ultimately but if somebody has
a reflective the choroidal surface where
the light actually goes into the eye
reflects off the retina re-emerges from
the eye and causes the bright people
effect if they have low reflectivity
they're about one for two percent of the
people who have exceptionally low well
it's not that high 1% of the people have
very very low reflectivity and then we
have a hard time detecting a bright
pupil in that case in those cases using
a dark pupil
eye tracker actually his advantage is an
advantage for those people some people
just have very droopy eyelids so as your
eyelids come down you can still see just
fine but instead of a of a Sophie are
looks like this and the eyelid comes
down and whacks off the top of that the
top of the pupil wears the pupil center
and if that lower eyelid comes up in the
corneal reflection is down there you
have to be able to see both the the
pupil and the corneal reflection in
order to be able to predict a person's
gaze accurately
and sometimes some of those the
percentage of people just don't have
very good the droopy eyelids want too
much of the people for them to be able
to predict their gaze some people just
plain have goofy eyeballs they don't
blink enough and so there's a surface
reflection that you get off the cornea
that isn't as nice as it might be and so
that can can get in the way so there are
any number yes yeah
the lacrimal fluid in the eye is very
complicated chemistry and you have to
blink a lot and some people in
particular with who have motor neuron
type diseases their their ocular muscles
continue to work but their blink muscles
don't and so they can be difficult to
track so there are several different
dimensions over which that can cause
these reasons and they're all fairly
small but they accumulate and you have
to be aware of all if you're going to
design for a general public for a
general population you have to
accommodate there's many of these cases
as you can yes
what regular glasses generally our
trackers can work fairly well through
glasses
yes they do if your glasses are tilted
wrong the LED it's trying to see your
the cameras trying to see your your eyes
through the glasses and superimposed on
that thing is a big reflection and so
happily most glasses are fit such that
that reflection occurs outside of the
image of the eye but not always a lot of
people have these glasses that if you
look at me from the side you see that
they're sort of tilted this funny way
and those people can have trouble there
another thing with regular glasses is
the people who have hard lined bifocals
there's a split so actually when the
camera is looking it sees your eye and
it sees the top half of your eye through
one lens in the bottom half through the
other lens and it throws up its hands
and it basically can't handle graduated
bifocals don't have that problem
explicitly they don't they they can
still find the corneal reflection and
find the people and but as the
calibration of the eye is different if
you see it through one power in through
another power so a cool solution to that
eventually is that you figure out which
angle you're looking through that
person's eyes and you put some
information into the computer about the
glasses that you're wearing and it can
figure out all those things we haven't
gotten that far on the eye tracking
industry to solve those problems so
there's a good point you mentioned
muscles in the superior colliculus
function 24 hours a day even its sleep
would you expand on no I really my point
was that it just keeps going it's it
doesn't have to sleep as much and mostly
when you see that activity is in REM
sleep when your eyes do go so I was
making a point of that's not part of
your system that gets tired or
yes what's going on there then there are
any number of reasons and the main one
is that what is involved in the
calibration procedure there are two
things that you're calibrating a lot of
times and there's two general
philosophical design stores look do you
mind if I put this topic off or just I'm
going to get to that topic I will answer
that question and I don't mean to avoid
it but what I'd like to do is at this
point is talk a little bit now about how
eye trackers are designed and once what
some of the requirements are about how
my trackers and well that will provide a
good basis for answering your question
so what are the general performance
characteristics that you want out of an
eye tracker my personal opinion is that
the most important thing to do assuming
that you've got the ability to track a
large number of people and the system
will actually find an eye in the first
place but once you've solved that basic
problem a really key issue is accuracy
there are some cases where accuracy is
some applications cases where accuracy
isn't it isn't particularly important is
somebody look at when they're driving so
they glance up outside the windshield
look down the road every once a while
you don't have to know if they're
looking at this angle or that angle very
much but in a lot of applications and
with computers
and particularly with small handheld
devices where you've got a lot of
options you want to know is he looking
at this coordinate of that coordinate so
accuracy ends up ultimately being pretty
important if you want to talk about gays
based interaction with with computer
screen one of the things that I
attempted to do in my earlier part of
this discussion was to indicate the eyes
are capable of pointing very precisely
they can point probably we don't know
this but they can probably point to an
accuracy of about in a repeatability on
that of about one tenth of a degree we
know it's at least half a degree because
the fovea itself is one degree across
and there would be no purpose of I was
just pointing with a half a degree error
because the image that we're wanting to
look at would be if outer edge of the of
that fovea olla it's just not the way
we're designed so we can point at least
a half a degree and a lot of people say
well that's the only that's it doesn't
need to do any better well there's some
nice all-night Rackers that have shown
the Purkinje eye trackers that were
developed by corn sweet and free and
back in the late 60s and early 70s and
those guys actually showered that the
repeatability is probably closer to
about a tenth of a degree so back to the
Heisenberg uncertainty principle that
really ought to be the target of our eye
tracking is to get those kinds of
matters because the eyes can do that
well why not measure it that well and in
fact when you design your screens you
put the size of your icons are based
upon how repeatedly and how accurately
your eyes can resolve those things so we
want to target for that so accuracy is
important
we is the second thing that's really
important is that we as human beings
move around I'm not standing here giving
this lecture Stan you're like this is
I'm moving all over the place and if we
don't move all this research these days
going and saying you better stand up and
will not sit at your desk for too long
why why we got to keep moving
we're just designed that way so we need
to build eye trackers that can
accommodate that so those two
performance metrics are really central
to we want to get accuracy but we want
to allow simultaneous freedom ahead
motion so what you'll see on this demo
system that will show you after the
discussion here is how Elsi technologies
has solved demonstrated that that
problem can be solved what it actually
does this takes the cameras and puts
them on a gimbal and so the cameras can
move around
was that an original idea no we borrowed
it for nice our own eyes are gimballed
so if that's how nature chose to solve
that problem why don't we just solve it
the same way just move around and so we
have the equivalent of the peripheral
vision and the central vision and the
peripheral vision is a couple of wide
field cameras in this case we don't move
we could but they're actually different
and they have a wide field of view and
they when somebody comes into the in to
sit down in front of a scene it sees
that there's a there's a face over there
and it swings those cameras around and
then these cameras are the central
vision their telephoto lenses that can
zero in on your eye and they have a
fairly small field of view if if you
didn't have the gimbal you don't have to
hold your eye in this really tiny space
but what we found is that if you can
measure the eye and get roughly ten
pixels per millimeter at the eye you
get pretty good gaze tracking so that is
one of the important things we want to
do get 10 millimeters per 10 pixels per
millimeter at the eye so if you're up
head-mounted system real close that's
easy
millimeter spans a lot you don't have to
get very much resolution in the camera
the further you get back those pixels
narrow down and it's a tougher and
tougher problem and typically if you
wanna sit 60 centimeters or roughly 2
feet away from the monitor to get that
you need a fairly telephoto lens to get
that that sensitivity you've also got to
get enough photons in the camera to get
that high resolution image at that point
remember the target is let's be able to
measure the eye within it there may be
other problems or the noise but we're
trying to get down to resolutions and
ocular sees of between a tenth and a
half of a degree so then as you move
even further back you need more and more
telephoto lenses well one way to do it
is just continually put more and more
pixels into the end of the camera sensor
well if you put too many pixels in the
camera sensor you're getting one photon
per hour on any one pixel you just
there's not enough light out there
you have to throw so much light on the
subject to get the two to get the
resolution and the pixels that you want
so then they say well why don't you just
point all your LEDs phase them face Tran
just point them at the at the eye and
not illuminate the entire field well
nature didn't design that approach with
us so we there's not an existence proof
that that's a really good way to go so
the approach that we use on our system
is to
just point those cameras use the wide
peripheral vision that's completely
different vision system to find the face
you could use that also to do the
reading of the the facial expressions
and the facial gestures but with the key
part that we for eye tracking need out
of those cameras is to find out where
the eyes are so we can point our eye
tracking cameras on them and as you'll
see back there this is a great big
monkey ugly device it's expensive it
cost tens of thousands of powers for the
bloody thing but it's a proof of
principle model it can be done it
demonstrates it's a commercially
available piece of equipment that we
have out there now it's too expensive
and too big yes but it demonstrates that
the problem can be solved and so it
really comes down now we are at the
place where we need to just put a bunch
of good engineers on this job solve the
problems with the optics use smaller
motors little MEMS devices if we make a
smaller camera we need smaller motors
and can we get this whole thing down and
do the sugar cube that I was talking
about earlier on well maybe not next
year but is that doable I don't see why
not
in fact I'm confident it can be done and
that's exactly why I'm talking to you
guys because the environment that
Microsoft is let's build this thing if
we got some reasonable confidence that
you can build this thing and have it do
what it's going to do and a lot of
computers to to communicate with us the
way people do with vision systems let's
do it
so basically that's that's that's the
chat and we can go back and you guys can
play with the demo at some time but I do
want to get back to your question now so
would you rephrase your question
the eye is a tough little tennis ball
its parameters don't change so there is
no good reason that a calibration that
you get today shouldn't work an hour
from now a day from now a week from now
a month now even years from now if your
eye ball changed if the radius of
curvature of your cornea
if the flattening of the cornea towards
the edges if the location of your foe
viola within your retina if any of those
parameters changed your occipital lobe
but throw up his hands and say what in
the world
so that doesn't change but what happens
in most eye trackers is that they
calibrate a combination of parameters
when projecting a gauge remember that
the concept of gaze prediction this is
let me get a better count of screen you
got an eye tracker down here
up here you're looking at a gaze point
out here and that's your direction of
gaze behind this thing is over this
whole eyeball and there's an LED in the
center of the lens and it's throwing up
light and there's a corneal reflection
here someplace in a pupil Center so we
have the pupil center and corneal
reflection if your gaze angle at the
eyeball is fixed all this geometry ought
to be fixed but when you do a
calibration we have to know in theory
the general idea is what's the X Y Z
location of the eyeball in space what's
its orientation in space and when you
project that line out where does it hit
the object that you're looking at so
it's a complicated almost robotic
problem
calculating the gaze point most eye
trackers do a calibration where they
throw all this geometry the optics
geometry the spatial geometry of the
environment all into one big model in
that they say that the gaze the
x-coordinate of the gaze is proportional
to some constant plus and this isn't any
one dimension one constant plus some
game K times the glute pupil vector gpv
and so do you have to come up with this
and then they multiply it out there are
all kinds of other polynomial expansions
about that this is what's called a
lumped parameter model and you can kind
of get the feeling that embedded in
these coefficients EK and all the other
higher order terms in there is all the
geometry the eyeball all the geometry of
this space and if any of that changes
then you got to recalibrate so we've
actually separated when we do a
calibration on a human we've already
calibrated the geometry and you'll see
back on that thing there's a geometry
that we've got a monitor that's actually
fixed into the into the to the eye
tracker so it the eye tracker down here
knows its relative position to that to
the environment and all we do is
calculate seven parameters your eye
for each of your two eyes so all we're
doing is getting those parameters and as
I said before because the the eyeball is
a tough stable little device the tough
little tennis-ball as I like to call it
then you don't need to calibrate again
murmurs describe the challenge oh yeah
that's right
anatomical geometric descriptions of
your eyeball in your eyeball alone
I got one other point here and here's
where it really gets down to one of the
important problems live tracking as good
as your eyeball is regarding its
physical time little stability there's
one thing in the eyeball that is a
potential problem for eye tracking and
that is the muscles that control your
people by ammeter you have two muscles
that are controlling you two people
there's one pair of muscles one set of
muscles radial muscles that propagate
radially and then there's a spanker
muscles so the sphincter muscle sits
right around the outside perimeter of
your pupil and the radio muscles are
attached at the end the traction and so
it's the counterbalance of these two
muscles that controls the pupil diameter
it turns out that this pupil center does
not have to stay exactly at the center
of the optic axis in order to maintain a
good image of your eye if it were to
drift off a little bit to the right or
to the left up or down the photons that
do get through pupil would still
converge at the same point on the retina
that get the idea there so it turns out
that as the eye is the pupil opens and
closes it does not open and close about
a precise concentric point that's
constant so as the radio muscles
contract in your eyes dilate it may
dilate more to one side than the other
at that point the pupil Center is
actually drifted correspondingly when
your pupil closes by down it might go
back to that point or
it could end up someplace else so in the
pupil sign recording of reflection
method the concept is that the center of
the pupil represents a known and fixed
location in the eyeball absolutely
absolutely there's another since you
asked the question of detail on detail
and eyeball lens of the eye cornea stick
a novel exaggerated optic axis of the
eye the first though the point of the
eye and then only exaggerate so this is
an optic axis visual axis at the back of
the visual axis right there is where the
fovea is so when we point our eyes we
pointed such that the visual axis lens
on that thing that we want to look at in
its image lands right in the middle of a
foamy olla so one of the issues with eye
tracking is what's this angle between
the optic axis and the visual axis and
in the optic field that angle is called
Kappa and it has a vertical component in
a horizontal component so if I calibrate
and measure Kappa and then somehow I
rotate my hand by 90 degrees but we've
made the assumption which is a false
assumption but somehow the eye tracker
might make the assumption that the
rotation of the eye is just the same and
it goes back to some equation like this
and then says what's the gaze point well
the glitch
vacker were these two components has
actually shifted some because the gays
doctor instead of projecting straight
out of the eye is often an angle and you
may have corrected for that angle
beautifully as long as you assume that
there's no role rotation which is also
called wheel rotation or portion of the
eye and then you extend you know what
take the worst case where rolls 90
degrees and now it's in instead of going
out and projecting well here's the
here's where the intercept of the optic
axis is and so therefore translate over
to centimeters to get out to the to the
next point
your heads rotated and so now that the
actual visual axis intercepts a
different point in the screen so if you
don't measure the roll angle you've lost
the information one of the biggest
problems in eye tracking is the range if
your eyeball is at this range when you
calibrate and then you move your eyeball
back look at the new geometry it now
sees your eye from a different point of
view and the gaze angle is actually the
last than it was when it was closer so
that means the the pupil vector actually
got smaller you're still looking at
exactly the same gaze point but the
projection the measurement that the eye
tracker is able to make on the key and
the glint capable vector got smaller
they got smaller for two reasons
double-barreled effect one is that the
angle actually got smaller the included
angle between the camera axis and your
visual axis got smaller but at the same
time the eye is further away so anything
that's further away get shrinks in the
image so it then projects a gaze point
below and conversely if your I were to
move forward and reject against point
that's too high
if you don't accommodate all-out
geometry correctly if you come back a
couple days later and you happen to be
sitting further back or sitting too
close you can get these kinds of errors
so the way LC technologies has solved
that problem is with a completely
different kind of device for measuring
range most of the eye trackers out there
have a camera in the center and two LEDs
that are offset to the side that
illuminate the eye and then so the
camera sees the eye okay and the image
of the eye there are two corneal
reflections and the distance of those
two corneal reflections is somehow
related is proportional to range and so
you can do a first order correction in
this geometry but that's kind of
fallacious because it's assuming that
the corneal sphere is a sphere it is
there's a lot of flattening out towards
the edges of the eye and that just so
the I can focus better so if you when
you calibrate if you're looking off if
you're looking at some point on the
center of the screen and then you look
at a different place even though your
range didn't change at all
the distance between these two corneal
reflections did change because the
surface is not a sphere it's it's it's
varied so the approach that we've taken
in the eye tracking here is to solve all
these problems explicitly rather than
having a lumped parameter model that
calculates the gaze point from the
calculated pupil vector we go through
many steps we do calculate the glyph
pupil vector very precisely but then we
actually do some ray tracing so as is we
find that the eye is all oriented over
in this direction it accommodates the
fact that there's a flattening of the
cornea and that geometry is act
explicitly calculated when we when we
see a games
often an angle we have mechanism in here
called the asymmetric aperture method
which is able to measure the distance to
the corneal reflection on the eye that's
a kind of a whole different topic here
but we go off on that one for half of an
or two but let's not for the moment but
we we can measure the range to the eye
without these two LEDs that are offset
from each other our LEDs are at the
centre of the lens and we actually look
at the shape of the corneal reflection
and you will see that in there but that
allows us to measure the range so we
minimize a lot of those effects by doing
explicit modeling of all of the ray
tracing taking into account that the
geometry of the of the environment the
geometry of the eyeball as we move back
and forth we've got to say symmetric
aperture for measuring more precisely
the range to the I'd find it's XYZ
location in space and therefore more
accurately predict its gaze point
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>