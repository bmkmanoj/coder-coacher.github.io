<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Verifying Constant-Time Implementations | Coder Coacher - Coaching Coders</title><meta content="Verifying Constant-Time Implementations - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Verifying Constant-Time Implementations</b></h2><h5 class="post__date">2016-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ykr9gGZavU0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it is my pleasure to welcome back to
Cambridge phone so deep as well with no
electrolyte the usage of survival is
going to tell us all about your
available sometime innovations hi so
yeah thanks for the introduction I'm not
sure I'll tell you all about it but I'll
tell you what I know about it and what
we can do as a team with these people to
verify constant-time implementations so
this will be presented I think next
month I usenix so if you're there go
there and talk to Mike will be giving
the talk Michael Amy so what is this
about and why do we need to verify
constant time so before i go into
constant time itself i'll go first into
side channel security in general and
what it means so the goal that we have
the overall goal is to defend defend a
program against an adversary it an
adversary sorry who can listen in on on
computation so your computation is going
on and involves some secret data and
you've got an adversary who is not
supposed to know the the code to the
lock but essentially you're giving it by
some side channel like a piece of paper
next to the look and that side channel
can be execution time other timing
channels like cash timing so the
anniversary shares the cash with you and
can through the timing he gets from
memory accesses infer you did on the
cash state power consumption so how much
power your computation consumes will
leak information about the data you're
manipulating micro probing with the
adversary gets a probe very close to
your processor electromagnetic emissions
sound vibrations everything essentially
leaks information up to a certain extent
and inner with a certain amount of noise
added to it and so we're trying to
defend simple programs in particular
cryptographic programs against that kind
of attacks at our assumption here in
this particular setting is that the
input/output functionality of your
program is actually black book secure so
you
already got a proof of security for the
input output relation of your program
what you want to make sure of is that
when you implemented it you did not
introduce any additional vulnerabilities
Chris I channels and having this black
box security objective which is what you
want to achieve actually informs the
tagging of input as public or private
and the tagging of outputs as public or
private public outputs and public inputs
are going to be things that the
adversary is given anyway in the black
box security definition and having this
information especially about outputs
which of the outputs are public allows
us to allow some leakage as long as it
doesn't leak more than what the outputs
actually week more than what the public
outputs leak so I'll give a bit more
details about this so this is broadly
what side channel security is about and
our particular brand of side channel
security which is in informed by the
security property you want and I'll be
focusing mostly on side channel security
against timing attacks timing attacks
are attacks essentially based on the
adversary measuring timing information
so execution time fine grain OKO screen
those can be mounted remotely or even
cache timing attacks that can be mounted
cross vm for example on a cloud server
where resources are shared and they have
been shown to break blackbox secure
crypto systems in the real world so you
have vadnais attack on IPSec and TLS
back in two thousand to lucky 13 lucky
microseconds all of those are attacks on
the record layer of GLS up to 1.2 that
were really successful than could just
recover all plain texts on primitives
like pts 1 which is RSA encryption
you've got attacks like blackened behave
and mangers that were just based on
measuring the time it took to undo
padding for example
and that would leak also break the the
security of the encryption algorithm and
then you've got attacks on modular
exponentiation that are cash based in
this setting but you've got also simpler
attacks that are usually well mitigated
and so you've got Percival's initial
attack on modular exponentiation back in
2005 and more recently the cache geotag
that was mounted despite counter
measures designed to avoid Percival's
attack um and those countermeasures yes
cash bleed I'm not sure it's being
published yet but it's been made public
definitely it was around around the
usenix deadline that they made it public
so essentially i think that was let me
check I think it was new TLS but it's in
it's in a lot of various libraries so
essentially when pessary become came up
with his attack the countermeasure that
was added in most modular exponentiation
libraries was not to make the the whole
memory trace constant but make it
constant up to cache block size so you
were hiding we're in the cache block
cache line you were hitting but you were
not hiding which cache cache line you
were hitting in and so the problem is
that's valid on some architectures but
not on some others and so the
countermeasures falls falls apart when
you actually compile it on another
machine okay so if this one is
architecture specific but it's because
the countermeasure that was put in was
designed for a specific architecture so
if or were your arms so it's a new
against intestine
we can solution yeah and so
countermeasures yet there are two main
schools so one of them is balancing or
hiding the leakage where you allow
something to happen or allow some secret
dependent timing variations in your in
your secret computations but then you
try to hide them after the fact so
that's what's used in s 2 n I think some
other crypto libraries the problem with
that is that you've got an unclear
leakage model and an unclear security
model what is that you're trying to
achieve I'm not sure you're trying to
hide and what kind of adversary you're
protecting against and then most of most
of these countermeasures are based on
adding random noise and essentially if
you don't add enough you can advert it
out and if you if you add too much then
you probably end up hurting your
application because you need to wait for
30 seconds whenever something fails
which is not usually a good thing the
other brand the other brand of
countermeasures is constant time
programming where constant time is
really really a bad name for this and
the idea is not to compute the upper
bound of execution time and then make
everything take that time it's to make
the timing in some sense with air quotes
around timing not sure if there is a
video or not there is cool the the
timing independent of the secrets so it
can still depend on public inputs but it
should not depend at all on public
outputs and the usual way of modeling
that is to consider the most leauge
prone instruction so branching which is
going to leak a lot because you're
branching and doing two different things
so the timing of those two things are
going to be different but also memory
accesses because of cash tying it and
other leaks are much smaller and much
easier hidden using noise are not
considered and then if an adversary
manages you start exploiting them then
you would try to hide those as well and
make them
independent the problem with this is
that with this brand of countermeasure
is that it completely breaks the code
you're looking at right and it doesn't
break it in the sense that makes it
non-functional but when you're now
trying to maintain it or read it and
figure out what it does is just
unreadable another issue is that because
you're doing this at source level the
compiler can break your countermeasure
completely through optimizations and put
you back in it eventually and make you
vulnerable again so the idea now is to
try and and look at constant time
programming which has a very clear
leakage model which is essentially
you've got this set of operation that
you want to make this leakage you have a
model for and that you want to make
independent from secrets and you've got
a language that you can work on so let
me give you a bit of a primer on
constant time programming so I'll give a
couple of variants of this of this code
where you have a public interface that
takes an unsigned length parameter which
is V public parameter in here and you've
got a public address out buffer public
address in buffer and a secret offset in
the input buffer and you're going to
copy something whatever is in the input
buffer starting at that secret offset
into the output buffer starting at zero
and you're copying the length bits or
bytes that the adversary requested and
so one native implementation would be
this copy function here that loops over
the entire buffer size there is a more
Navy implementation that I I didn't even
put in and then you're saying if I'm
within the range I should be copying
that I'm copying if not i just keep
looping and i'm actually leaving the
loop as soon as i'm done copying what i
wanted to copy so obviously this through
timing is going to leak a lot of
information and in particular you go
to go exactly who offset + length
operations length being public and known
to the adversary and offset being the
secret you're trying to hide so that's
definitely not good if the adversary can
from looking at the source code and
knowing the architecture know how much
in iteration takes then you can get the
offset value your secret of them so
another variation of this would be to
just keep looping until the end of the
buffer do not break as soon as you've
copied out but only do this white copy
or assignment whenever you are within
the range you're trying to copy so here
you've actually got two possible sources
of timing issues the first one is that
this this statement here the actual
assignment is going to be executed on
the offset blast length time so exactly
the same as previously but the timing
difference between whether or not you're
executing this statement is going to be
smaller than what we had before more
importantly however the instruction
cache is going to reveal if you're going
into the if or the else branch so the
first time you going into the into the
then branch you're going to have a cache
miss probably going to have a cache miss
on the instruction cache and then
possibly when you go back into the else
branch you'll have another cache miss in
the instruction cache and so that cache
miss is going to have a quite a big leak
probe cause quite a big timing leak
probably actually much bigger than then
whatever timing Lee would come from
actually executing in the sign matter up
so we fix it again and now we're going
to say we're going to prevent branching
on secret dependent data completely so
that means we need to somehow make
everything branch free and one way of
doing this is to use masks well that is
V way of doing it so you instead of
bread
changing and saying if a is less than B
you're creating a mask that's going to
be all ones if a is less than B and all
zeros if a is greater than or equal to B
and then you're going to be doing both
branches all the time I need to go down
both branches of the computation all the
time so here you're doing this
assignment all the time but sometimes
you're going to or zero onto the value
and sometimes you're going to or the
actual in of I depending on the value of
your mask and so your if I is less than
the offset sorry if I is greater than or
equal to the offset and I is less than
offset + length is now encoded into this
mask and then you do both operations all
the time but in this case you still have
some leakage that comes from the data
cache this time so here the value of J
is actually going to be incremented only
when you're within the range and so your
trace of memory accesses that does leak
into the shared cache some piece of
state that you're sharing with the
adversary that value of J is secret
dependent over time and so you're you're
going to be leaking information about
your secrets that way and then there's a
small possible leak here as well we're
on most architecture architectures
integer division it takes a time that
depends on the bit size of its
appearance and here the mask is either
all ones or all zero so the bit sizes
are widely different and that might
actually leaked information but whether
you're in range or out of range as well
and so finally a constant time
implementation where you're entering
constant path so no branching on secrets
and constant access which is no axe no
memory accesses at addresses that depend
on secret and then we're also adding
constant up in there which is constant
whenever you have an operation that
would be which timing would be input
dependent as well you can also try to
put that into your model although here
we don't really do it
and in this case you could get this
implementation that uses a constant time
equality check and that just now loops
over the whole buffer and over the whole
length every single time so now you're
in n square now the issue is when you're
looking at this code you have no idea
what it's doing there is no way you can
maintain this if there's a bug in it you
won't find it so it's it's bad and if
it's not constant time there's probably
no way you can figure it out looking at
it either and this is just a buffer copy
right so actually this kind of code is a
piece of code that was recently so this
is a piece of code not exactly this but
that was in openssl as a countermeasure
against lucky 13 and one of the many
copies of it in the openssl source code
was actually found to be vulnerable not
not vulnerable to a side channel attack
but actually not functionally correct
back in May I think or April I was just
a fact of not checking that you're not
reading past the end of the buffer that
was really really bad and now you've
you've also turned your nice buffer copy
that was linear in the length of what
you wanted to copy into a big o of n
square that's not even an N square its
buffer size times the length you're
trying to copy the buffer size could
actually be very large so that already
is not normal in that sense that you're
losing quite a bit in performance so
you're then trying to squeeze more
performance out and doing that you're
trying to get everything you can get out
of each architecture and that led to
cache bleed so you're saying on that
particular architecture I know that the
leakage looks like this so I'm going to
you know squeeze that extra cycle out
and another architecture is going to
fall apart yeah so our objective given
all of that it
to verify concern automatically by
automatically we mean with minimal
annotations and then the rest is done by
a tool automatically we want a fine
grain flexible leakage policy that could
actually allow us to look at details of
target microarchitectures so if you are
looking at a micro architecture sorry
micro architecture where the address
within a cache line the offset within
the cache line is not leaked then you
could actually model it in our tool and
then if if you're going to go for
another marker architecture that does
leave that offset then you can have a
full memory trace and leave to the
adversary and we also want to allow some
leakage if it's benign with respect to
the target security property so I mean
this constant time copy is nice and good
to have but then if what you're copying
out is an offset so the offset that
you're you're considering secret is
actually then in some cases given to the
adversary then you might not want to use
this one you might want to do a linear
copy and we want to allow this so if
it's at this point secret because you
haven't checked everything yet but it
becomes public eventually it's a good
thing to be able to optimize the N
square way yeah so one of the examples
would be trained so far bc c codes yes
what what of the compiler is going to do
with secret so so I show C code because
it's readable and you know people
understand it so our tool works on llvm
code actually this thing maybe not this
exact version of it but in open as a
sell these things are functions and not
macros because GCC was optimizing them
back into effort NL statements
any compiler optimization either a
fortune llvm or the c compiler is
supposed to preserve the input the black
box right so I mean we know compilers do
bad things to constant time
countermeasures and that's why we're
coming in after optimization so I'll i
did not say this in here well I say just
right next so we want to verify
low-level optimized code is that Louis
for conditional moon because of the way
the superscalar architecture is laid out
that has less pressure on register
renaming q so that has less of a plane
okay okay that's that's interesting to
know so so actually that's especially
interesting to know if we're verifying
directly that low-level code that C move
is actually Leslie even than other other
possible sorry I mean a bit more but I
actually realize now that I didn't put
any interesting examples in there but so
11 more interesting example then a final
copy would be if you have encrypt then
lack then when you do with the
decryption so you do verify then decrypt
that the fact that the very fishy
verification succeeded or failed is
going to run the Mac key which is a
secret want to be able to branch on that
yeah that's it attacks well it's okay to
be easily information at the end that
nothing goes so so because of the fact
that our notion of public is something
that is eventually given to the
adversary even if what you're releasing
it in the middle it has to be something
that leaks out to the anniversary that
is give
injecting boots that install 20 so so so
you have a scenario where you have
multiple Oracle's so you take one of
those polo cars that fit is the
authentication key idea yes and your
resource allocation in the middle then
the destiny can inject the message
obviously so you're not releasing the
authentication key you're releasing
whether or not the check succeeded
that's that's what i mean by something
that's eventually given to the
adversary's and we're looking at
individual oracle query right so you're
looking at the decrypt that verified
oracle for you know codes so you should
think yeah so for 41 oracle essentially
what we're considering as public input
is everything the adversary controls
through the security experiment through
the security game and public outputs are
whatever information about the actual
not oracle but actual algorithms outputs
is given back to the adversary by the
Oracle so you have this Oracle wrapper
in your security experiment and we we
consider that as for public and private
I mean that's a separate thing right
right here you could just consider
public annotations are given by the user
their trusted you tell me this is public
input this is public outputs and we're
fine with that we're just taking that as
a as an input to the tool
but yeah we didn't actually consider
things where you have experiments that
could be in several stages that would be
an interesting lead for future future
stuff you say program specifically so
I'll tell a bit more about this but
we're looking at lv m oq Mizell of young
programs and in this setting the thing
that's just before linking I see it's
great because actually so does that mean
that I will be able to use your verifier
to verify constant I'm a politician in
higher level programming languages that
see if you can compile them to our vm
yeah sure I mean there is there is quite
a bit of so I'll go to the architecture
I think it's towards the end of my talk
but I i'll i'll describe what's going on
and there is quite a lot of information
that's actually given to us by LVN so
that we can we can do our verification
and some of it comes from the fact that
it's compile from C so if you can give
us more information than what the c
compiler can can give us then we're very
happy if your computation chain gives us
less information we might have to do
more work at the lvm level which is
obviously more difficult but it's a it's
straight of you you have to come up with
so yes you do says it's not fun sometime
of you you have to change the code yeah
so if our tool says this is not constant
time first of all you need to decipher
what the error message means which is
always fun so it's actually it's
actually not so bad in most cases but
you still need to then trace it back to
the point in the sea program where where
that occurs and then figure out if it's
actually you know spurious or an actual
it all depends on how you you know how
much tooling you have but yes it might
be harder but it's then it's on the
sorry tool developer to do it you
where the IRS little calls out for
runtime library that have the time and
guarantees at all so this is also a
different issue which is how do you
model what one you know where do you
stop verifying and how do you model
leakage so if you're if you're really
looking at an inaccurate architecture
there's no information about which
instructions leak what you have no idea
what the pipeline is doing and so you
don't have all the information you need
to do this properly and precisely and
soundly so you know either you over
approximate and lose performance or you
say I assume this leakage is small
enough that is going to be eaten by
something else anyway but it's always a
trade-off you need to make between
protecting against known attacks which
you assume are the best that can be done
at the moment and protecting it against
all attacks which is always a bit more
difficult and probably impossible but
yeah so the general architecture we're
going for is you get a constant time
verifier that takes in annotations a
small amount of them a program so here
we're taking see but compiling it down
to a lot of the end so we're annotating
see just to make it readable and easy to
deal with and the leakage model
optionally and that produces either yes
this is constant time or no this is not
constant time and here is a possible
trace that illustrates it no this is not
constant time is actually I can't prove
that this is constant time it's not
complete so the first thing we need to
do before actually verifying constant
time is actually specify it so the way
we specify strict constant time where
you don't have publicly observable
outputs is using non-interference so
this is just a very standard way of
expressing that there is no information
flow from public hub public input story
to a set of secret secret outputs so
this is the the model you have a program
p
you partition its inputs and outputs
into public and secrets subsets and then
you say p is non-interfering if any pair
of executions that agree on the public
part of their inputs also agree on the
public part of their outputs so this is
P is non-interfering and then how you
can exploit this and to verify constant
time is that you mark public inputs as
public inputs and again this marking of
public is something that we trust in
this particular setting but that would
be informed by whatever security
property you want to prove you
instrument the semantics of the program
to produce leaders traces so whenever
something is meant to leak in the
leakage model you have you just give it
to the adversary and then you Marik the
leakage trace as the only public output
and then if you prove that your program
is non interfering with respect to that
setting then you have strict constant
time there is no information flow from
seeker inputs to the leakage trace if
you want to allow publicly observable
outputs the public public outputs it's a
bit difficult because public outputs
would have to be proved to be equal but
here we want publicly observable outputs
where we can assume that they're equal
to prove that the secret outputs are
sorry that the public helpers are you
and so you now partition inputs into
public and secret and outputs into
public publicly observable and secret
and what you want to prove is that any
two executions of P that agree on public
inputs and on publicly observable
outputs produce agree also on their
public outputs so here the publicly
observable outputs are what is given to
the adversary anyway in the black box
model so they could disagree because for
example on the mac check would succeeds
with a correct key but failed with the
wrong key
and you want to leaked that information
to the adversary what you do not want to
leak is if they do agree if the mag
checks fails in both cases we don't want
to leak whether the padding check failed
in one setting and not the other and
that's going into my mouth unencrypted
yes so that kind of padding Oracle's
that you could get in any II would be
captured by by this and so same thing as
I was just saying you just mark public
inputs and the outputs are released to
the adversary in the black box model as
public envy sorry yeah whatever Marek
public inputs as public mark outputs are
released to the black box adversary as
publicly observable I'll fix my slides
before passing them on and instrument
the semantics to produce leakage races
in the same way as we did before mark
the leakage trace as the only public
output so this time the blue kind of
output and run the verification tool and
voila you get a verification tool that
gives you security in this model where
you allow some leakage that would be
delimited by the information the
adversary is given in the black box
model yes still on the leakage traces
but this time with the additional
condition that the publicly observable
outputs need to be the same as well so
we're essentially pruning all the pairs
of traces that do not agree on on
publicly observable output we haven't
looked really random program so far
essentially what we would do then is
sample outside make the randomness
public a public input if it's actually
given to the adversary
have the exact same leakage dress every
time yes so so essentially in this case
so if you if you want pure constant
without any dependency even on public
inputs that would be you know pure
constant thing where everything all
inputs are marked secret and then your
leakage choices marshalls public output
and you know you can look at that and in
fact many examples are like that sorry
said liquid what were you asking just
minutes ago I think I wanted to say
something more but I've forgotten with
you yeah a random random thing so
essentially what one other thing you
could do is actually possibly more less
thought out answer is that you could
prove that the distribution on the
leakage trace is the same so you know
lift lift the Equality to the
distribution and then you're good but
that might be more difficult to to
reason about and definitely the tool
that we have wouldn't do that although
the approach might might apply quite
well to it the tool that we have
wouldn't support it out of the box it
would require a bit myself an
engineering and actually I run in I
invite you to look at the masking papers
because they're you do depend on the
randomness not to be constant time but
to be secure against probing attacks so
in this picture and these two pictures
there are two components so marketing
public inputs and marketing public
outputs and instrumented the
instrumenting the semantics to produce
leakage races and so i'll go through
both of these before actually saying how
we verify that thing so we have a simple
annotation language that we currently
have on c source code just because that
convenient to have and it actually makes
it more usable as a tool for for
programmers as well although it makes
error reporting a bit more difficult as
merkel floated up and what we have in
this annotation language is that we mark
we ask the user to mark public elements
public inputs and public outputs as
public rather than market asking them to
mark secrets a secret that means that if
you run the tool without any annotations
is going to fail by default and tell you
this is not secure and then it's your
job to think about what you're goin
attentions so for example an example of
annotations you've got this decryption
function a buffer that's used for input
and output so initially it contains the
ciphertext at the end it contains the
plaintext and that returns the length of
the plaintext and possibly 0 if
decryption failed the length of the
input buffer and buffer that contains
the key and it would be fixed size for
example and here you mark the address of
the buffer and the length as public
public inputs and also the contents of
the initial contents of the buffer there
the ciphertext is provided by the
adverse research also public you also
mark the address of the key as public so
here public in value key that's really
the pointer value that's public and not
the contents of that pointer of the
memory region pointed to and then we
would also mark the result of the
decryption is public so the length of
the of the plain text for example that's
in a setting where length of the
plaintext would be public and whether or
not decryption succeeded yes we whenever
there's a function call we will be
making sure that it's the same function
and so when their direct function calls
that's you know guaranteed when they're
through function pointers we actually do
verify that they are pointing to the
same location in memory in both
executions sorry what do you
we don't annotate anything the semantics
do that so essentially the lvm code
that's generated does jumps and does the
the dispatching and that's when we we
put assertions that the two addresses
that are called jump to precisely your
decryption function that may be as a
public argue or public argument eyes
open you specify so in this annotation
language we would not yeah so this
annotation language is really
prototypical there are many things we
can't do and in particularly in one of
the examples where we're looking at
there would be something more that we
could have done if we had a better
annotation language then if you go in
directly at llvm and add annotations
then you can you can do what you want
but it's really a matter of convenience
that we went for this so this is for a
small language for marketing public
inputs and publicly observable outputs
and then when we produce leakage traces
we just really instrument the semantics
so where you you have so here we would
capture a constant path constant access
for example if you have a jump so to a
function pointer then you're saying what
leaks about this state is the address
you're jumping to if you have a
conditional jump sorry then you'd be
leaking the condition and the address
you're jumping to and then if you have
memory accesses so here are just load
because of lack of space sorry I don't
know what's going on then you're leaking
the address and that you're loading from
or if you have a store you would be
leaking their dress you're storing two
and then if you want to extend this you
can also just add if you have a division
you're leaking a bit size of the of the
denominator dead minitage these
the divisions by zero seconds this
commercially so there is one thing i did
not say we assume safety assume memory
safety and safety in the sense of so so
this is a simple here leakage model we
assume safety of the program in which
emits correctness that's you know part
of the this is black book secure if you
have buffer overflows it's unlikely to
be a black box secure anyway is it black
box secure though so this is a simple
example but then you could essentially
extend it extend the approach to any
leakage model where the leakage produced
by each instruction depends only on the
current state so if you have if you want
to say I don't know something stupid
floating-point operations are never safe
then you could actually say floating
point operations leak the whole thing
the whole state and we need control flow
at the very least to be leaked so this
one this leakage function here these two
actually on branching operations are the
minimal set of leakage that you need to
have for her approach be silent complete
so so I'll talk about this a bit so
essentially i think i think but again
this is a claim of making a bit without
justification if you have a state
machine that encodes the leakage this
would also apply but this is again
without evidence and without much
thought
yes yeah so I mean in a real processor
you'll have the pipeline that's going to
put you in it you have the cash state
which we're not modeling here but I mean
we're over approximating it by saying
the whole memory the exact memory of us
is leaking but if you want something
more precise you might actually want to
model full cash state in there and there
are plenty of other things like branch
prediction units and all sorts of things
that come in and could actually you know
make this either in precise or and sound
but this is you know you're given a
model and within that model we're saying
there's no leakage and then everything
else that the adverse we could get he
needs to get by fighting for it so but
we could we could extend I think to any
leakage model that leaks at least the
control flow and can be expressed as a
state machine finite state machine so as
soon as it please makes you address your
jumping occur because of the instruction
cache but you assume that you just go to
the next in function you may still make
something or you will know that that
that's going to be enforced by the
content path anyway constant path by
having these two we actually guarantee
that the program counter is is preserve
is the same in both executions as well
yes if you if you have jumpstart safe
and then all the other ones r plus 1
then you should be okay yeah so leakage
traces we model like this we associate
to every instruction we associate one
function that produces the leakage from
the current state and that's what we're
working with right now and then for
that's how we specify things and now we
try to actually verify those security
properties and so we were given a choice
of many methods to verify
non-interference so there are type
systems for this however the lack
flexibility there was a verified
certified type system sorry
I'm not getting shot by Andy at least so
the type systems that have been proposed
so far lack flexibility that is a much
better statement and in particular there
was one verified type system on concert
x86 or a bit above back at CES 2014 I
think that was that we used in a
previous paper and I was just completely
unusable it required it made unrealistic
assumptions about the code it was worth
human gentle nalysis techniques obtained
propagation and things like this do not
scale well because they're not really
modular they require the whole code to
be present and looked at in one go
however there are characters
characterizations of to safety and in
particular non-interference as safety
properties by reduced by transforming
the programs in such a way that if you
can verify safety of that transform
program then you get non-interference of
the original program so i'll be talking
about two of them before going into ours
so yes so this is the solution we're
going for we're trying to find the
transformation X that uses the
annotations the program and the leakage
model to transform the program in such a
way that if well the program is secure
if and only if that transform programme
is assertion safe so here when I talk
about safety from now on it's not memory
safety it's not the program as a
semantics it's really the assertions
that we insert in it succeed so the
first historically the first such
transformation to be proposed with self
composition so you take your program
with its annotations in the leakage
model i'm using here is the one I showed
above but the only thing that leaks here
is a is a branching operation so you
take your program your annotated program
and you make two copies of it into
disjoint memories faces so you Prime all
your variables and
make sure that you don't have clashes
and stuff like this you make two copies
of your program you assume initially
that your public inputs are public so
they're the same on both sides at the
end you assume that you're publicly
observable inputs are the same and then
eventually you assert that your public
outputs so here the leakage chase which
is the negative of branching are the
same so here the low leakage trace is
just ve parity of the secret and parity
of secret crime and you're asserting
that they're equal so that's one
transformation and it's been shown back
in the early 80s to be sound and
complete so if that transform program is
safe sorry that transform program is
safe if and only if P is a secure with
respect to the decayed model l so a
secure being whatever security property
is implied by these annotations so here
that would be if you have two executions
of this program that agree on next
initially and agree on why at the end
the leakage choices are the thing I
think so itself composition with early
80s so it's soundin complete that's
great that means now all the burden is
on boogie and you guys we don't have to
do any more work however it's really
hard on the safety verifier so we're
really not fair if we actually use that
it's really hard on the safety verifier
because all relations between
intermediate variables are completely
lost right so here you would have to
infer back that y and y prime were the
same things like this modularity becomes
really difficult because you need to do
that on each function or in-line
everything and then do that loops are
really difficult because if you dissing
crenn eyes them then you're dead and the
number of that and all of that is
essentially making the number of fats in
the product program or
the self composition exponential in the
number of initial part in p so if you
have a pathway is verification tool your
dad again you lose scalability you lose
everything and so if we did that that
would really not be fair and then we
would say yeah those guys need to
improve but that wouldn't be what we
actually want to say well we want to say
is we want to be clever about it and
actually give you something that you can
work with so another solution is to use
product programs that are essentially
self composition but make more compact
so here you assume that the control flow
is the same in both settings and that's
where our assumption that the control
flow leaks comes in and that's why
constant time and that kind of
verification technique works really well
so here you've got the same program I'm
dropping the publicly observable output
for now and you'll be seeing why in a
minute so you have the same program
different annotations and the program
you're producing at the on the other
side the transform programme is you
first assert that the leakage at that
point is going to be the same and then
you only have one copy of this if
statement where each branch contains
both branches well the branch of each of
the programs so you have your branch
only once and it's fine because you know
that the branching that's going to be
done it's the same in both executions of
the program but you're doing both you're
executing both programs in each of the
branches so essentially instead of
having two copies of your programs like
this you're interleaving the two
programs and like like though like that
I guess and and synchronizing control
flow so that makes loops easier to deal
with control flow easier to deal with
and now the size of your transform
program is the same as your initial
program as well so as long as you have
no publicly observable outputs yes
to see my name for the other school
camera technique by type system they
assume that the program's is a lobster
passion and then the branch their zoom
that at the branching points there at
the same tight end the outputs if are
only in secrets the giant point says you
know so i mean this this thing is not
new all of what I've been seeing so far
starting from the point of saying we
want what we want is reducing to safety
those are things that were known self
composition in the early 80s product
programs in may like geez I think it's
not much later and definitely they've
been used before it is it's probably
very similar to lots of things that are
gone in information flow verification in
other places in particular if you're
verifying information flow and you're
preventing implicit flows in the sense
that you're preventing branching on high
values product programs are what you
want to go for so if you don't have any
publicly observable outputs then this
technique is sound complete as well
which is you know why it's very good for
that kind of things if you have publicly
observable outputs you can make it sound
and complete as long as you annotate
things so you need a bit more
annotations and a bit more work as well
loop invariant so if you have loops
they're synchronized because that's the
assumption you make and essentially that
makes loop invariant very simple to
infer so in fact what you need to do is
make sure that the loops remain
synchronized throughout and whatever
leakage the loop body has is public can
be made public and so what you have is
you can do it ain't another system on
the loop body locally on it instead of
having to do it globally on the whole
program but what we what we actually do
is a merge course our heuristic that
says all the variables that are modified
by the loop body and I'll live at the
beginning and at the end of the loop
should be equal in both executions and
that actually works for all of our
examples but one
what we need to go in and do a bit more
work so most of the of the constant time
programs that we've looked at are
actually you know very very easy to look
at even though they have loops and yeah
as I was saying product programs are
really well-suited distract constant
paths so the one that has no publicly
observable outputs and also to those
that have publicly observable outputs as
long as the control flow doesn't diverge
between the executions so if everything
that diverges is the memory access race
then you should be fine so now we're
trying to put in publicly observable
outputs where the control flow diverges
on product programs and the problem is
if you try to assert it here to assert
that the leakage at that point is the
same at the point where it occurs that
will fail because you don't know yet the
value of y and y prime and you don't
know that they're equal yet so that's a
bit problematic because we want this
program to be secure because the parody
of of the secret is essentially the
final value of y which is leaked or not
the final value of y but the difference
between the final value of y and the
initial value of x so this is bad we
lose completeness here if you do it the
other way and say I want to assert it
down here like I was doing in the in the
self composition setting where you are
sir everything at the end then the value
your you the value of y here you're
using or of y prime is going to be the
wrong one because you took a branch here
that was the wrong one for one of the
two programs so if secret and secret
prime of different parity you're not
going to go into the same branch for
both
and so here we lose probably soundness
and completeness both if we do this so
that's bad we need something more and so
the idea here is to actually combine
product programs with self compositions
so whenever you could diverge on the
control flow you say I'm fine with this
I'm going to record that I might diverge
and I'm going to verify later that it's
fine and that it doesn't leak more
information than the public output does
and then do self composition inside to
gain back soundness and completeness and
this requires a bit more annotation the
sense that you need to mark I'm fine
with this bit of control flow leaking
back to the eldership so don't
necessarily look at this trying to
understand it the paper is probably much
more informative than this bad example
oh it's just the registration I mean
just so now you have a an additional
variable L that you that you're using to
collect the leakage so here you could
see it as a leakage trace but here we
were using it as a stack of all the
assertions you would be collecting and
so that allows you to collect all the
assertions you'd be you would like to
essentially assert at that point and
then assert them at the end after having
assumed that the publicly observable
outputs are the same just vaguely it's
just a gigantic conjunction and that's a
way of that's a cheap way of doing lists
of assertions some running out of time
yes I used this method of time
explaining that a second position is
that scale and it's not Monty comfort
our tools but if you think at the
examples where you would ask we
introduce these kind of things i would
guess for instance as you said
mechanic ripped yes this this will open
at the outer know after you've done them
the sea which means that you will
actually have to self-imposed
enlargement of the program possibly so
the examples we've looked at so far
we're on the more trivial side where the
final copy would be the thing that we
allowed to branch if you're so we I did
look at and clip then Mac and that
worked all right it didn't lose too much
but again again this is if you have
something much bigger I have no idea or
if you want only if you like that's a
good question yes thanks so the way we
implemented I'll go very quickly through
this it's a whole stack of tools are
connected by Meg files so it's a bit
complicated we take annotations and
program in C to begin with compile them
with si Lang to llvm optimize that all
the way down to the final our vm output
before linking and then we feed that to
smack which is a tool by Mike Amy and as
windermere for what isn't me and others
and that turns optimized lvm assembly
into buoy code and that's a one-to-one
mapping that preserves the shape of the
code and is very very close semantically
so that doesn't the assumption here is
that transformation doesn't change
timing but really the code is you could
do a you know reading of it in parallel
and be fine with it and then we
transform this boogie code using the
product program transformation currently
the leakage model is fixed but that's
just a matter of adding an input and
input language in tool that produces the
product program in boogie and we just
run that through boogie and get the
answer we want so this whole stack of
tools is why going back from constant
time violation back to this input
program is a bit difficult but it can be
done I mean
proletarians either water or circle
position depending on whether your son
it's so it's it's that selective product
where you're doing product most of the
time and then if you have something
that's released you deserve composition
underneath it so why do we do it on lvm
I think that's a question that was asked
already but I'll go over it again so it
gives us a bit of generality in the
sense that we can generate we can do
generic reasoning over many
architectures in the sense that it gives
us a proof on many compile programs but
it doesn't give us a proof in that press
I in the precise leakage model that
would correspond to that marker
architecture but that's already a first
step right if if you're not secure at
that level you're not going to be secure
down here anyway it's convenience there
are tools for LVN that might not exist
for x86 or x64 arm assembly in
particular smack and related tools but
also lvm analysis so we rely a lot on
the shape analysis that llvm does and on
a lot of lube detection and things like
this that smack does that would be a lot
more difficult to actually reimplement
on on lower level assembly and we still
have already at lvm level that gives us
sound control flow and cash timing
modely the gap between lvm and machine
code is actually very small even though
it has some differences the main ones
that could trouble us as register
allocation but that is fine with respect
to control flow in cash timing and then
instruction selection and that is not
fine because you could select an
instruction that leaks a lot and this is
getting us to the M precise earth metic
leakage and things action / and so you
get an LVN instructions
you actually also in machine code let's
apply the same techniques that it would
not actually then the main issue is
really engineering the tools and the
main interests of doing it below is
really to have more precise modeling of
what the leakages which is something
that chip manufacturers are not really
happy to give up but if you if you're
happy with you know profiling the
leakage yourself or assuming something
about the leakage then doing it at the
lure you actually want a proof of x64
level then you know you want to do it
below and I don't think it's it's much
more difficult scientifically it's going
to be an engineering challenge for sure
real under architecture model is your
instructions dependence so we have one
one example where he has and often gets
twice as fast on x86 because it kills
false dependency No so that's definitely
the case where we would want I'm going
to pass over implementation details
where we would want to to have more
information but what the processor
actually does and that would be useful
not only for us to verify security but
there are lots of other application
domains where being able to predict the
timing of instructions in your well the
time the execution time of your program
is going to be useful I was chatting
with a someone from absent recently and
in the worst case execution time
analysis that they do they're really
impress eyes because they don't have
information about what the pipeline goes
for your uncle
like six a process the 60 instructions
that are currently in the pipeline with
your with your approach I'm not sure
this is something we need to look at so
one of the things I would really want to
look at is for some reason i didn't put
in on this slide I did maybe I can't say
it is you have an if you have a model
for your for your leakage that would be
a state machine then you can instrument
your semantics in the same way we do
with instruction instruction by
instruction leakage annotations but you
need you have an extra dimensions to
explore which is the initial state of
the ocean state machine so if you can
somehow do whole program analysis and
know in advance the initial state then
you should be fine if you can't do that
then you explode people have looked at
bigger I'm not sure if that's going to
be doable in combination with what we're
doing already but we could also do a
first step which would be refining that
state machine into something that's only
relevant to timing for example or two
time leakage or two security irrelevant
time leakage I'm not sure if it remains
us big or if it gets smaller if that's a
big reduction or if it's a minor
reduction i I really don't know so other
things we would like to look at and this
is what I was talking about we rely on a
lot of information that's coming to us
from lv m and from the fact that it's
compiled from c and that a lot of
information is preserved by llvm so if
you want to look directly at low level
code that was written by hand you might
actually need to implement all those
analysis yourself and in particular one
thing that's getting a stuck on some
examples is that you need to be able to
properly separate re memory regions that
contain public data from memory regions
that contain secret dependent data and
if you
of a sea program for example that does
weird things like said zero on a whole
structure and essentially collapses it
all into one reach think so then you
essentially get stuck and have to get
arithmetic invariants to clearly
separate those two regions which is
something if you're doing down directly
yet at lvm level you're not going to get
however if you're compiling from a
source program or a programming language
where you have this strong property or
can guarantee and preserve those
invariants to compilation then you could
use that to inform the constant time
verification as well same thing if you
have already existing annotations that
you're using to verify correctness or
safety and those could be used as well
to refine whatever analysis we're doing
so yeah look at Likud complex leakage
models like the state machines
data-dependent models that we didn't
really look at so we kind of
experimented with the integer division
but the problem there is again if you
are saying we're leaking the bit size of
the of the denominator then you actually
need RF matic invariants on your loops
to make sure that you can actually prove
something about the bit size and so then
again having these annotations would be
useful the people from escort are
presenting a paper at usenix as well do
proofs on on floating point operations
and essentially they're replacing all
floating point operations with constant
time variants except those where they
can prove none of the arguments are
subnormal and would cause weird timing
anomalies but that proof is again
something that requires strong
invariance on loops and and things like
this it would be interesting to look at
whether or not we can make the invariant
inference a bit more precise than our
course yeah this looks like what we need
let's plunk it in there and then improve
modularity so right now we're just in
lining everything and running the the
product construction on the whole
program which is not a problem because
we only have small things to look at but
then if he if you do have self
composition bits are quite big then you
might want to actually get modularity in
the sense that that way if you do have a
call to decryption inside your your
branch to a decryption algorithm then
you might actually just be able to
verify that is secure by product program
and then doing the self composition with
that call in the middle so that's
something we would be interested in
looking at it all so that's all I have I
went way over time so thank you a lot
for all the questions and your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>