<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Processing Social Data | Coder Coacher - Coaching Coders</title><meta content="Processing Social Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Processing Social Data</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/b14GWmtSs4A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let's get started Rico and I are
delighted to host devaraja today taking
advantage of his sabbatical on the west
coast to invite him here professor Shaw
is an associate professor at MIT in the
department of electrical engineering and
computer science he's with the
laboratory for information and decision
systems and also the operations research
center his interests are generally in
large complex networks the stochastics
and algorithms that underlie those
things and he is an associate editor at
the I Triple E transactions on
information theory and also operations
research and queuing systems and we'll
hear what he has to say today thanks
thanks thanks for and thanks Rico for
making this possible I really appreciate
it so just so that there's a bigger
purpose behind the visit and one of the
things that sir we would really like to
do at lids lids which is a research lab
at MIT so many of you who might know how
a mighty works there's the departments
are there and there are research labs
are there and both are important and
everybody belongs to both and at least
one lab if not more at Leeds we
primarily play role as bridge between EE
and CS so for Apple things like what I'm
going to tell you about which is a nice
bridge between statistical influence and
machine learning things people do in
optimization which by design is between
een CS people to continuous optimization
and discrete optimization control and
robotics computer vision and signal
processing and the list goes on and
these are the type of things we do there
and we would really like to engage
broadly speaking now with that prelude
let me start telling you about what kind
of stuff I do and I'll be happy to talk
to you about myself or about others if
you are interested and now our feel free
to shoot me an email okay all right so
what am I going to talk about i'm going
to talk about a few questions in the
context of processing social data these
are concrete questions that we
looked at over past three to four years
and there are some nice solutions that
we have and I believe there is much more
to be done here than just thinking about
big data as building big systems all
right so with that in mind what am I
really interested talking about by
meaning social data but its data that is
generated by us it's all sorts of
transactions electronic transactions
that you made or the restaurant rating
that you are is left on Yelp or movie
ratings at you have left on netflix the
tweets you have sent out if you
participate whether as an employee here
or an employee on Mechanical Turk which
is the crowdsourcing system or your
facebook post your two key data and so
on and I'm sure everybody uses that all
the time now the question is that what
can we do with it's well understood that
there is a tremendous amount of
information it's like our gene is
encoded here so if our genius encouraged
here maybe we can get some meaningful
information out of it and do something
useful with it here few options of
course we can do better businesses in an
operations world in business school when
we would talk about this as figuring out
how to manage your revenue it is useful
for pricing at being one would think of
how to do advertising if you are
thinking a little more generally
thinking about myself I would like good
recommendations available so that i can
go and eat at good places for example or
watch light movie be useful for a
policymaking I mean deciding whether to
add a road or not or school or not and
if Congress has access to what people
like and dislike that would be very
useful and more generally it could
change the course of societies I mean
some level this was a well understood
example of that some level crowdsourcing
are changing the way labor exchanges are
organized right I mean odesk is one of
those examples where people who
can get a quick employment that was not
possible before Mechanical Turk style
micro crowdsourcing is helping
connecting people across the world in an
interesting way things like news reports
are useful to get quickly so all of
these are ways in which we can do
something what is a basic challenge at
least from my view at the core of it
we've got lot of data which is
unstructured in highly noisy and it's a
lot of it which means what I want to do
is I want to eventually make decision
from this data in order to make these
decisions at some level I need to solve
this statistical challenge there is I
need to understand what is basic
structure there and if there is a lot of
data I need to figure out how am I going
to do it in computationally efficient
manner that is that can scale with it
and this kind of questions at some scale
being observed across across a variety
of domains / years but now it has become
really acute given the scale and given
the amount of uncertainty that we have
now for this is a big challenge I won't
be able to solve it at all but what I
would be able to do is I will be able to
tell you a few concrete questions in
which we have approached this
two-pronged approach where it's a
thinking of right statistical inference
framework along with simple algorithms
and try to get meaningful answers so
effectively i'm thinking of data as
forming an appropriate statistical model
rather data is generated from
appropriate statistical model once i
understand that i could think of taking
decisions by coming up with optimal
inference algorithm and while i have
finished most of the important part the
question would be that optimal
algorithms are not easy to implement
especially at scale certainly what i
would like to do is I would like to
develop meaningful approximations that
take me from data to decisions and
really the model
only helping me think through it so at
the end of the day I will take the data
i will apply simple algorithms i will
get some meaningful answers and these
answers are interesting for two reasons
once that's all the problem and second
if I were as an academic argue what
these are really doing I have a
statistical model to argue about those
answers okay now this is at a high level
very useful plan only if I can execute
I'm going to take proof is in the
pudding so i will show you three
examples one in the context of decision
making / recommendation here the
question is we've got a lot of data
that's telling something about people's
choice little bit about people's choice
and i want to stitch it together second
is question of crowdsourcing if you most
of you must know this and if you haven't
i'll explain precisely what it is well
what i want to do is I want to build a
meaningful answers from different small
small answers that I opt-in from people
which has noisy and stitch them together
and then finally related to
understanding trending and realizing and
Twitter now I understand that talk is
going to last for 40 yard more minutes
and this is too much to go through so as
we go through this way the information
content will decrease but hopefully I
will be able to convey what's the
question I am looking at and hopefully
the algorithm said they're looking right
and feel free stopped me here at
audiences small and interaction would be
very useful okay sure okay all right so
with that broad layout in mind let's
start with this one simple question all
right i should give grace to my
co-authors here this has been part of a
longer program that has been going on
for four to five years started with my
former students shrikant Yacouba Sylla
who is now at nyu with our my
collaborator at sloan school of
management vivek farias student amar who
is currently student at lids with my
postdoc sahan negahban and seven who was
formerly with
now is at urbana-champaign this is the
first part of the talk the second part
is with seven and David Carter David is
a colleague of mine at in computer
science to computer science part of the
art department and the last one is with
my former student ahed who is now at MIT
is a faculty member and student who
spent few rather six months with me now
because of his work he's not Twitter I
wish you as he remained with me well
then we'll get there you'll see why okay
so first part recommendation alright
okay alright so at a high level question
is something like this I've got lots of
partial preference information from
various sets of people somebody telling
me I like this restaurant so much
somebody telling me i like this movie so
much and so on and then from that what i
really want to do is able to somehow put
them this partial information together
and stitch them and provide some kind of
global ranking at the end of that is not
just ranking it's also incentive in
intensity also that matters so here's
some scenarios let's say I've got a
bunch of movie watchers somebody like
yourself telling me that you really like
inside job and then based on what other
people have told me and what you have
liked I might suggest that you might
want to watch this movie or let's say
there might be hiring decisions that you
must be doing all the time and candidate
is interviewed and different people give
scores differently somebody gives eight
out of ten maybe Rico gives eight out of
ten snip the gives nine out of ten Phil
gives seven out of ten and then the end
of it you'll be well you don't know
higher Microsoft CEO but somebody else
is hired it's a decision-making question
now these type of questions show up
everywhere Microsoft's to true skill if
you have played and got yourself
true skill then you will have a score
people are playing games that are not
everybody's playing with everybody it's
only subsets of people are playing with
each other and based on that I want to
assign scores to everybody
recommendation we just went through as
academics we think about this all the
time we submit papers to conferences and
then conferences have only ten papers
are 15 papers or 25 papers that can be
accepted out of 200 or 500 or thousand
and question is that which ones to
accept similar question for us in
graduate admissions shows up every
winter that which students to admit and
I'm sure you have in turn problems of
similar type which intends to hire and
not okay so in all of these questions
really at the end of it there are two
types of question one wants to answer it
one wants to answer one is how should I
get input from people if I can make it
feasible things like in conferences or
admissions or hiring I can tell people
that give me input in some form like
five stars or this or that that's a
design question but given that whatever
input I have for example in games people
somebody wins over somebody else or
maybe if you are playing cricket match
for five days then you can lead two
draws too but ignoring the drawers you
got pairwise winning results coming out
now there are all sorts of heterogeneous
ways in which data is coming how would
you look at it from one lens and put and
stitch them together to get answered so
really two questions that one is what's
the right way to design it if you had a
choice and if you didn't have a choice
you got all sorts of partial data coming
in a preference is coming in however is
stitched them together so let's just
look at us some of the popular
approaches one would be like dislike or
you do start writing okay this is easy
to input but it's clear whether you like
or not like this is a little bit
complicated because what does four star
mean but again no matter what these are
very simple aggregation problems I
once you have got input I will average
number of likes you have or take total
number of here for example I've got
three like let's say one like four here
one light for here and 1 dislike so plus
2 minus 1 and on inside God plus y and
then just sorting and similarly I can do
the same thing for star so it's easy to
aggregate that is once I got input the
problem is that well in this case this
is arbitrary scale because I don't know
what four star means it could be moot
dependent and at the end of the day
these are course right because as we as
it happens in our MIT admission system
that once you at the end of first round
we are left with roughly half of the
students all of them start for so now
what to do well now we have got three
sets of day long meetings and through
which we actually talked to each other
fight it out maybe there should be a
little better way to do that then that's
the really issue of coarseness of the
scale okay and as nichia say that you
know there's something beyond good and
evil so we should we should think beyond
this right now answer I think is in this
simple game and I think this is the
right time for me to entertain you as
well it's morning it's before 11 so
let's see I give you this blue color and
ask you tell me how blue is it don't
worry I've given you the the good do
yeah yeah yeah it's like I going to my
orthopedic with my back pain and saying
it an orthopedic ass so let's start how
bad is your pain well if I didn't have
pain which was not better but I won't
show up here I have better things to do
but then go to optometrist and
optometrists ask me the right question
is this vision better is that vision
better right and it's basically about
comparisons and in this case I might say
that answer is this is more blue than
that it's so really the answer lies in
compare
and whichever way you look at it whether
it's a sportswear winner losses like if
I'm a cricket fan and I like my
inclination is this way so India beats
Australia then it's like comparison
input that India beats Australia its
India is like to Australia more are
these are two restaurant that some of
you might recognize right these are two
really nice French restaurants in
seattle apparently this one is better
reviews as as far as people have
people's writing goes that's what I
found say if you want to try if you
haven't tried either and maybe
suggestion is tried this one before that
okay or if let us suppose I was writing
a paper about ranking and there was my
paper versus other paper then definitely
it would mean even though you might have
come up with ratings Esper scores I
would convert it into comparisons bottom
line is that whichever way you provided
me partial preferences I could view all
of them as bags of a bunch of pairwise
comparisons okay in the process one
might one might say that well aren't you
using losing precision here there is
nine verses 5 and 8 verses 5 have have
more information than just comparison
yes you're right you're losing their
information but it's not clear if that
information is really absolute and
meaningful so it's an debatable topic
but definitely it is absolute
information that how got comparisons
okay so question effectively boils down
to the following situation I got yes
there is a question
there's no question of the fun
because if it was six verses five yes
and you say greater yeah this is as
important as nine verses fine we're at
six verses five might not even have any
discipline excellent point that in some
sense if you didn't
no because I'm not saying the sense that
something that the right conclusion
might be equal you're now assigned it's
bigger when something that was 925 was
the same finger as the next 25 so in
some sense of what you're saying is that
lots of precision and in terms is that
if i had more comparisons between two
things then i should put more confidence
over a versus B rather than rather than
just treating that as answer once and
for all and in some sense that's a
that's exactly one would try to answer
using the model that we would build in
okay is this a great question yeah
alright so the end of it we are left
with this kind of setting right I got a
bunch of objects there are edges between
them representing that they were
compared by one or more people let's say
here a 12 for example reflects that when
one and two are compared or one and two
played games with each other out of
those many let's say this plus these
many games these many times one defeated
to and these many times to defeat 81 ok
so if actually I have got this kind of a
nice weighted graph and given this I
want to from these comparisons I want to
assign ranking or more specifically
scores to each of the object to be
meaningful based on these observations
and in some cases i will have this kind
of data in the other cases i might have
even have a choice of designing the
graph that is which pairs to compare in
which bear is not to compare so if i
were thinking of designing conference a
a conference paper reviewing system if I
assigned Lin let's say papers four and
five then he actually compared sorry
four and three then he actually compared
four and three depending on what score Z
assigned right so I could decide who
gets what okay so really these are two
questions in some cases this is possible
in some cases you
left with only this one and we would
like to answer these two questions now
again in order to answer this question I
have to give you an algorithm and before
I give an algorithm at least so that I
can think concretely I would like to
think of a statistical model okay so
first thing I would do is I would like
to tell you about a statistical model
and the model that i'm going to put as a
background is that it is underlying
distribution of a permutation that is at
the ground truth and the observations
are coming out of the pair wise
observation that are coming out are
coming out from this so let's see what
do I mean by that as he is a simple
caricature suppose i have got i have
seen got three objects a B and C I've
got these kind of data points for data
points a greater than be bigger than C
and so on really I'm thinking in the
background I really a greater than B
possibly is coming from a greater than
be greater than C as a permutation over
all objects bigger than 0 then a is a
permutation over all objects and so on
now what are these permutations
representing well the permutations are
representing a choice model a choice
model of population so effectively I'm
thinking of he has some ordering of all
papers in mind and i asked him only a
subset of them and that subset when i
asked him he revealed the answer s /
that ordering okay when i tell you about
two restaurants i have inherently in my
mind not just in restaurant case what
would make sense is not just one
ordering but bunch of ordering because
some days i might prefer chinese /
mexican and some days i might prefer
mexican / chinese is a question in what
fraction of days i prefer this over that
versus that over this okay and that is
effectively capturing this choice model
as one would call it or distribution of
permutations of the objects and that's
the ground truth and remain I'm sampling
this data points really i'm getting
snippets of that from this ground truth
now suppose given this data i learnt
what the ground truth is that's
consistent with it
in this case let us say here is one such
consistent ground truth is seventy-five
percent of the population believes in
this ordering 25 believes in this
ordering then maybe this might be the
reasonable answer okay again this is all
caricature so the question is that how
do we execute in this context okay but
this is roughly the plan any questions
yes please assuming we have some
features for each node and those
pictures are order for and those
features can help you to better anything
enough um so for example by features
what we've already mean by that oh let's
say for the movie it's a lance somebody
has 90 medicine has 120 minutes yes and
it's some other you know how how much
action within that movies anything like
that yes so if you have this feature for
fish now in this pitcher can have you
look better to do a better job sure so
okay so there are two ways to think of
that features is one is you could just
forget at some level here i'm thinking
of each movie as a separate node but you
could say they well I don't want to
learn everything in detail about each
movie but I want to categorize them in
through some features then I will have
fewer options over which I'm going to do
ranking ok and then again I will convert
data into that feature space and that
that's how it would happen so at that
level i will have more aggregation
happening of data there is more
confidence in some sense but i will be
losing some kind of precision because
now i'm comparing two movies one with
super hit and one not so with the same
feature set yes please
does your mom taking account that say
some pears might have been read several
times yes is it the same way that's
correct so i would like to design an
algorithm effectively that is taking
that into account which is related to a
little question of Rico where I said
that but if you have one pair compared
once which is like six and five ideally
and it's because of noise there were one
way versus are there if it were compared
many times maybe it would even out in
terms of my comparisons so I would like
to have an algorithm that does better as
more and more information I have and
also the algorithm should not rely too
much on one comparison when there's one
comparison it should bias your
information only so much excellent
question yeah but at sample this is my
ground truth and I want to build
guarantees through that kind okay yes
simply process is done by people making
regulations or sensors of any kind yes
in the same push themselves if have
biases and what is all the simplest
being a category of three kinds of
samplers and then there's a bunch of
samplers with positive bias there's a
bunch of neutral samplers and a bunch of
negative bison checks ensuing some good
actually is Cuba for 0 ver your
assumption that you have a ground who is
this relation door in sometimes it's
saying that to have got categories of
people each one of them is I would think
of choice model one choice model to
choice model three I want some kind of
hierarchical or classified things
similar well I'm just doing this for 11
version of them yes great so we're
trying to work a bias PDF excellent sir
that's exactly what we are trying to do
right now there are some conjectures we
have and happy to yeah excellent point
yeah all right okay so he is very very
brief history I mean this is a great
question everybody has been fascinated
by it including myself and goes
centuries back versus decades back so
here is a one of the celebrated question
that arrows impossibility result where
he said well suppose I don't have
pairwise but I have got ranking complete
rankings available and then I want to
aggregate things together then how will
I decide winner so let's have got three
objects a B and C and different people
have given me their permutations this is
what people would call ranked elections
now in 1950 1851 Tom hair a British
intellectual he came up with this
algorithm it's called hair is ranking or
proportional ranking it's been used in
all Commonwealth countries including now
currently in American Psychological
Association where you ha this is how you
elect your president so if you want to
elect a president let's say there are
four candidates in running you will rank
all of them ok and then at the end of it
we will come up with some Alrich
algorithm can never say the well if you
say that your ranking algorithms that is
these sets of properties which are
reasonable properties then there is no
such ranking algorithm possible and this
was very nice impossibility result led
to two decades of other impossibility
results and then very recently a
functionalist have got into it and they
said well impossibility result of arrow
is not just one counterexample but it's
actually present in a very broad sense
so really this is a this is very hard in
some sense we are making it harder
because now I'm giving you just pairwise
comparisons so in X ematic sense there
is no way to solve this problem so this
is the fact as a condor say had its own
criteria and since he had workin kotha
cynthia to work with at microsoft in
Silicon Valley they had an interesting
two approximation algorithm for what's
called condesa criteria as an algorithm
called Borda count algorithm which I'll
quickly mention in a second that was
young in 1974 is an economics showed
that that algorithm is nice eggs ematic
properties of course it does not
contradict this result but it has some
nice axiomatic properties right so
that's a lot of stuff from social choice
or voting theory literature from choice
model this is a thirst and he was a
behavioral scientist in 27 he said I
think that people behave like this
roughly everybody has some kind of true
skill as it's called in true scale sense
and there's some noise in their
performance every time that play let's
say let's say I and Lynn play a game and
I have my own true skill here's his own
true skill and when we play a random
variable is drawn which is adding to our
and he minor his true skill and
depending on the final answer being
bigger than one or the other one wins
versus them okay that's the type of
thing that's used in yellow ranking of
the type of thing used here this is a
whole family of things depending on how
you model the distributions of noise
that leads to different things this is a
Gaussian
this is what McFadden popularized for
policymaking which is called multinomial
logit model it's a extreme value
distribution and so on and that's also
popularized for in business school world
right so it's a long history there are
lots of exciting things that have
happened what I will do is I will relate
to these class of models to an algorithm
but first let me tell you the algorithm
because it's really its algorithm that
plays with their what you observe that
matters and then after that we will see
how well it works ok so it's a very
quick overview of lots of things there
are lot of things I'm missing out here
ok so here is my algorithm so what we
call rank centrality for reason that
it's like a random walk so remember we
have a graph with each Edge has these
kind of numbers just selling how often
one versus whether i'm going to create a
random walk as follows ok so for each
edge that's present there i'm going to
put probability like this effectively
what this probability is reflecting is
that what fraction of time the other
player defeated me ok so intuition
behind this random walk is the following
I'm going to have a random walk running
on this graph and the stationary
distribution of that walk is going to
assign me scores ok so let's suppose
that I'm always defeating everybody then
the stationary distribution of this
random walk should be a pretty strong on
me right it should make sense because my
score should be high it means now if
such is the case and if I am defeating
everybody is a part of random walk I
should hardly go to the other nodes
right and of course I should go to other
nodes sometimes if I have defeated
there's only once if I had defeated
others only once I don't have too much
confidence in the data so while I should
have bias towards me but not too much
but if I have defeated others all the
time like for over 100 times it really
is a strong bias in which case I should
make sure that I go to other nodes as a
part of my render walk with little
that's the style of design that we are
doing here so when I'm at node i am
going to go to node j with probability
that's proportional to how often j is
defeated me in the normalized sense and
these Plus Ones are taking care of this
finite error correction so let's suppose
that Kiko and I have played only once
and required defeated me once then I
will go to him with probability 2 over 3
so there is some bias i am giving but
not too much on the other hand if it was
100 games and hundred 20 then it would
be 101 / 102 right and if this is
connected which is minimum you need to
have any reasonable ordering because if
there are two sets of things i have
never compared and there is nothing that
i can meaningful do meaningful between
them and there will be a well-defined
stationary distribution for this random
walk and that will give me the scores
again if i want to learn this run this
algorithm or just do power it crashing
for example this one under gravity
you're right of course thank you yeah I
was just focusing on top thank you okay
well that's very good I'm conveying the
details also all right so that's a
algorithm any questions about it okay
not question that how well does it do
this he is just too roughly tell you
here's the type of equation you will see
which is effectively capturing that
sense that well I have high score if I
have defeated lots of people or a high
score if I have played only with one but
that one person had very high score it's
like a heavyweight championship right
the winners of only comes in play at the
end and while you are trying here brain
build your score up you will play a lot
of time
alright this is associate relation to
board account l and one way to think of
this is an iterative version of word
account all right now if one considers
this ml model which is a thirst in style
model what this model says that each
node has some kind of true skill or
parameters associated with it W&amp;amp;W twn so
ideally one would like ranking to be of
that order and those parameters
reflecting this course in this case one
could design a maximum likelihood
estimator and our algorithm matches the
performance that this is in terms of
simulations what this says is the other
algorithms after some time stops
learning well even though there's exa
matically supposed to be very good more
formally mathematically here is how
result looks like if my graph of
comparisons is a random graph then this
is the standardized error would scale
with effectively parameter k's the
confidence that i have this how many
times two pairs are played with each
other and these the degree of the graph
this is how it scales down and you
cannot do better than that there is a
this is a fundamental lower bound and
this algorithm is effectively getting
close to that okay and again this is
capturing that well if you have if you
are only so many comparisons you can
learn it well and this is capturing the
fact that you need to have if you graph
with bounded degree there's only so much
you can learn random graph has seemed to
suggest that well with random graph
structure you can essentially get as
good as the best algorithm can ever get
and in some sense it's it is captured
well because if you had any general
graph that is
didn't have a choice so if I had a
choice maybe i would use random graph
but if i did not have a choice and if i
had an arbitrary graph then it's the
laplacian of that graph will play an
important role in this case in
particular it will show up like this so
if you have a graph which is not good
not well connected in terms of this gap
of laplacian being small then it would
blow up the error will be very high
sofas amplified a line graph I
connecting to him he connecting to him
and so on that will lead to very poor
performance but if it's a well connected
graph then it would be very good okay
and this is also related to a random
walk on a natural random walk in the
original graph or laplacian okay so the
take of a message here is that if I were
to design a graph I would choose a graph
subject to constraints of example if
it's conference system there will be
conflicts I mean if I have a conflict
with somebody else I cannot be a sign
that paper but subject to those
constraints I would like to choose the
graph so that Delta is maximized it is
the spectral gap is maximized and then
if I wanted to maximize that actually as
a thing that sort of in his thesis that
was shown it's a nice semi definite
optimization problem and because of that
could be solid reasonably well yes
two stages in the ground in adaptive
manner based on answers you obtained in
earlier rounds excellent point let us
suppose that we allow Aldo shinee graph
a priori then and then you choose your
adaptive way because this information
theoretic lower bound applies to that
case also mmm maybe the best gain you
can get is up to log factor and I
believe lock factor is necessary each as
we can't prove it so my sense would be
sure maybe you might be able to improve
it but only up to constant factor okay
so there was a quick run-through through
one type of questions there is ranking
second question is ready to
crowdsourcing again ranking could be
thought of as crowdsourcing because we
are getting information from people and
this is like the world of crowdsourcing
is like Netflix prices or sending
somebody in man on the moon or micro
tasking which is five cents per task
okay and I'm going to talk about this
five cents one because I can't talk
about such a big amount of money all
right so here is a a quick motivation
why one might want to look this if you
have biological lab let's say and you
have coming up with all sorts of these
interesting images of experiment and you
want somebody to count how many red
cells are there if you hired an
undergrad in turn and might sort of deal
with 300 images / are diligently and you
will it will cost you something like
this if you hired instead put out in the
mechanical turk maybe people will
quickly count things you a little bit
noisy answer but you will get lot more
done so factory this situate the issue
would be high versus low reliability
this is essentially the type of
experiment that was done by susan homes
the statistician at stanford what you
want to do is you want to bring this
reliability high but keep
this number hi to right and if actually
this is the type of thing you are aiming
for so more out of your money and
question is that how do we do that well
we know that one way to do that is to
have structured redundancy built in so
that while we will have noisy answers
coming in we will be able to denoise it
if we have structured redundancy and
that's what we're trying to do so here
is a quick example just to set the
everything problem in notations right
the example was related actually these
images or type of thing happened in 2008
plane crashed in Nevada and people are
looking for weather plane is and of
course image processing is only so much
advance so you'd I wanted humans to do
it they released on the order of 50
thousand plus images and lots of people
are volunteered and then people started
looking at images so let's say I see
these three images and I say well looks
like there's a plane maybe a little
noisy but there's a plane here there's
no plane debris there's no plane deputy
here somebody else looks at some other
images and gives dancers there and so on
okay so you get different people look at
different subsets of image you get their
answers on that weather plane debris is
there or not and finally you decide
where these places might be a plane
debris let me send people to look at it
of course if you look at this you will
say okay there is nobody no plane is
there a very high likelihood that there
is a plane is there but then things like
this and this you don't know okay so
what you want to do is you want to sort
of build a confidence somehow from
answers that you got from things that
which ones are more likely in which ones
are less likely now if I knew who is a
person who is giving me answers and how
truthful or how not truth to that person
is it would be in religion easy problem
right because I would bias my answers a
bias that person's answers that way and
then aggregate things the problem is I
don't know second standard Mechanical
Turk sells platform I put out my task
people take on the task and the answer
and that's that really I'm not really
learning about them I don't have a
choice about them I do have some kind of
information from the platform that house
what sorts of performance they're done
in past but that's only so much limited
question is that how am I going to
integrate this thing in a meaningful way
so again I want to really do solve this
problem is reliable estimation with
minimum cost cost is just number of
edges in this kind of bipartite graph
because east coast is like each edge is
like one person performing this task an
operational question I want to answer
our task assignments and once you have
answers how to infer the best answer is
again it's very it's mirroring the same
set of questions that we had seen before
right how am I going to who is going to
compare which things and then once i
have got comparison how am I going to
infer answers similarly here how am I
going to allocate tasks to different
people and once I have answers to task
how my brain for them okay again I need
to tell you a statistical model is to
build the algorithm and then understand
it and he is very simplistic model it
said in the kind of for example i give
you the task will be binary lesser plus
ones and minus ones emotionally you
might have carry tasks well in case of
images you will have seven cells or 10
cells or 27 and so on and each person
has some kind of latent reliability as
per which the person will answer so in
this case they say this person has
probability half so real it's random
with probability half that person will
answer correctly or incorrectly and
that's how I will see the answers this
person is completely correct and
truthful so all the answers are given
correctly and I would assume effectively
that i have got reasonable positive bias
because if i did not have that then i
would not be able to differentiate all
pluses from all minuses right okay so
here is with that probabilistic model is
a quick preview of results
in that probabilistic model this is a
simulation you will be able to this is
the best performance you will be able to
draw this is the read an amount of
redundancy and the reliability higher
the reliability of course higher the
amount of redundancy you need and this
is the best trade-off you can achieve
and this logs logs this log versus
linear scale such as that error
probability with reliability goes down
exponentially this is what you would get
from majority voting that is you look at
answers and look at the majority answer
which is the natural thing to do this is
what up a popular inference algorithm is
called expectation maximization would do
and this is what our algorithm will do
and as you can see there is a similar
slope they'll offset so something
interesting is happening there okay
looking at one way that I want to
achieve let's a ninety percent accuracy
or ten percent error with in this
simulation our approach would require
amount of redundancy which is eight
verses its existing algorithm would
require 12 which is majority which e
which is 17 and if you are really
investing money into it this is the
factor loss that you are incurring or
gain your cream okay so really good
inference is very useful it goes long
way all right now I will tell you about
answers right because I told you the
model I give you the results in terms of
sir a graph now let me tell you the
algorithm for task assignment and the
inference again like before the best
hull best task assignment would be a
random regular graph remember there it
was additionally graph Esper choosing
comparisons here random regular graph
which is saying that if i have a budget
that each task should be assigned to el
things and each person can perform at
most are tasks then subject to those
constraint i will choose the random
graph
and the inference algorithm would be
like this so let's just build the
intuition towards algorithm fairer if I
have a task like that which is plus
minus minus majority voting would say
minus and that's it but well if I knew
that how trustworthy these people wear
then i would like to incorporate that
information into my answers and an
Oracle who would know these answers
would just add the log likelihood to
that and of course if everybody is
equally trusted then her best answer
would be equal to majority voting it
sort of makes sense to of course we
don't we won't have that there will be
uncertainty and so we would like to
understand if we can learn these weights
now I don't know the weights I know only
the answers if I know that Rafael these
answers are given by him are almost all
of them are correct then I should give
him very high p and now how do I know
that well maybe his answers agrees with
other person so somehow i want to sort
of stitch in this intuition together one
way to do that is to do it rest right so
here is what iterative algorithm will do
it will reliably learn this estimate for
these log likelihoods and the way it
will do is as follows so as nice I mean
this is very as natural as it gets right
let's start with giving everybody this
equal likelihood there's everybody's
equally weighted say one now I'm going
to assign likelihood for a given task
and initially these are just once i'm
just going to sum up all the answers
that i have got here i can sum up
because they are plus and minus one then
i will get the answer that let's say
there are seven people have answered
this out of which six of them answered
plus 1 and 1 a minus one so my
likelihood of being plus is plus 5 which
is pretty strong
now okay so I got these kind of
likelihood for all the tasks now I go
and try to assign the reliability for
each worker well for different tasks
there are different likelihoods that are
obtained from other tasks now I want to
look at my answer to this task and see
does it compare well with the
reliability of obtained if this was plus
5 and my answer is also plus 1 that's
good because I'm matching which plus
find my answer is minus 1 that it's
really detrimental because I'm going
against what everybody believes is true
okay and then I just sum that up and I
etrade this so if I did not exclude this
kind of in my iteration answers coming
from me from previous stars it would be
like a power iteration of this matrix a
a transpose okay I'm just excluding it
because it's actually very important for
information aggregation and when we do
that that's when the the best
performance comes out before i said i'll
give you the precise theorem and let's
say i got five more minutes or yeah yeah
exactly i'll end in five minutes and
I've got more pictures now right so we
thought well we have got simulations we
got theorem which I'll show you in a
second what about real world maybe maybe
this is meaningful maybe this is not
meaningful so we thought we'll do
experiments and this is great place
where you can do experiments because I
can sort of load up my task on
Mechanical Turk and I against run
experiments the first we thought well
maybe we should do something like this
which of these ties are similar but then
similarity is is in mind right so it's
very hard to make it objective so he
said well what about things like this
which tie goes well with this shirt
again this is object this is sorries
this is all subjective subjective things
are very hard to evaluate finally we
ended up with this
thing that is which color is similar and
that is because there are these metrics
that exist they actually do cognitive
similarity metric and that seemed to
work extremely well actually so we
showed people this kind of colours
randomly generated and over them we
learn all sorts of experiments and
finally here a type of performance we
see that is iterative algorithm starts
doing much better after some threshold
and this is there is a reason and that
threshold is effectively before I tell
you the theorem say if information is
too noisy iterating actually increases
the noise okay but if you are in low
noise design that is where you can
actually do correct then iteration
actually helps and that's what comes out
in terms of theorem this kind of
qualitative result and that's also we
saw in experiment this is just one
instance of that but this is what we see
on all sorts of data on data we
collected there's a team at MIT which
primarily designs crowdsourcing
interfaces led by Rob Miller and his
colleagues and on all of their data also
similar performance looks yes it's not a
matter of initial conditioner
so after it converges I mean so you
could initialize it with majority voting
or something and it's still like this so
you can start with majority voting and
it will become worse and that's again
it's because in model you can prove it
why it happens in reality that's what we
observed in sort of it makes sense
because of this reason all right and
then is the random graph really useful
again improve theorems about them but
really in practice also you can see that
with graphs with small spectral gap
becomes worse all right so this is some
parameter you can assign which we call
quality of crowded cyka effectively an
adjusted quadratic norm of their latent
things and that is precisely what
determines the performance this is
precise theorem let's just look at this
is look at this one is it suppose I want
to obtain an error or alive reliability
1 minus epsilon how much amount of
reading then see I need I need
redundancy that scales with that
parameter as 1 over Q and no matter what
algorithm you use you need this much and
in if you use majority it would be
quadratically off because of this
exponent being Q square and Q is usually
small right so 1 over Q square is really
bad vs. 1 over Q now again this is
another place where you can ask question
what about adaptive does it help like
for example I have answers like this now
I know that these things are well
understood maybe I should only focus my
energy and things like this and this
surprisingly it does not and what this
says is that it's only improves up to a
constant factor that is again in this
case also adaptation does not help so in
both of these class of problems right
there is a question of what sorts of
graph structure you need for task
assignment what sort of inference
algorithm you need
and all sorts of qualitative and
quantitative results come out make sense
that you need graph which is reasonably
well connected simple iterative
algorithms do very well and they get you
as good as a performance as you want and
adaptivity is not much of use okay all
right now I think that okay here's one
lot more information you can get out of
these results like you a bunch of crowds
which one I should employ depending on
their quality and amount of money that
they're asking me i can calculate that
and decide which one to so these are
very useful operationally right so that
brings me to the end of my talk and i
think this is 1130 try time to end so
really but there are a lot of data we
have great opportunity but to realize
that we need to process it at scale and
in these examples what i showed is there
is a start with thinking about
reasonable model come up with right
algorithm right algorithm helps you
solve the problem well the model helps
you understand mathematically why things
are why the algorithms are useful but
the algorithms are model independent so
they're useful just on their own okay
and I didn't show you this right so I
should you show you that so question is
that can i predict trend on twitter
before it becomes trending so here is
quick preview i should stop but I
shouldn't show you this so that's miss
Rhode Island who became Miss USA this
June so naturally what would you expect
when she becomes miss USA things would
trend on treat Twitter and so that's a
perfect time to start predicting whether
it will become trending or not so let's
see here is the real signal in terms of
volume of tweets that are happening so
through MIT has this reasons why we had
access to the fire hose of Twitter so
this was the real signal we were
backing then and it becomes Twitter
announces is trending at time let zero
we had our estimator running at that
same time and our estimator say that
this would become trending at time minus
two hours and this is this happened in
this particular case but this is not a
typical as this is very typical in
particular this is how the ROC curve
over a large number of samples that we
then basically do point to take away
here is that ninety-five percent of the
time we can predict something trending
correctly before it becomes trending
four percent of times we make an error
because you know you make an error and
when we are ahead on average we are
headed by one hour 40 minutes and so
that example was not a typical all right
I think this is where I should stop the
future you're predicting what will
become 10 on Twitter yes for example the
Google give this work on flu symptoms
yes they are trying to predict truth in
the future and they figure out that you
thinner searches for flu symptoms and
they can pin them down geographically
then probably there will be a flu break
out in this area at some you know close
point in time excellent so in some sense
what they're the point was that flu
searches are giving out information
about something that's going to happen
it will be recorded more massive later
public scale if you based on what we
observed with all sorts of these these
are factory time series is that these
time series is do I have a very simple
structure and so for many of them
actually the information about
effectively information about their
becoming popular is already there it's
just twitter is
doing it on volume based so it takes a
while for them to announce it but if you
are just little clever about it then
actually you can get in do that
prediction of course you might be wrong
sometimes but looks like you might not
be wrong too often and it's a great
analogy oh yeah yes please one of the
main things were talking I was
extracting information about the
reliability of the people giving you the
information yes the first half you talk
long example you talk about was
conference reviewing yes we've got
individual reviewer is creaming
comparison skin have you thought about
inviting those because some reviewers
are going to be better than other
reviewers assessing the quality papers
okay so what are you asking of putting
each other way is that there's a there's
a choice model see one way to think of
it is that there's a crowd that I have
and the crowd is modeled by one
distribution of our permutations effect
we going back to earlier Rico's
suggestions last question is that there
are people who are who are reliable
which means that there is a well
separated distribution of permutations
and there are people who are not
reliable which is sort of mixed
distribution of permutation question is
that how can I put them together one way
to go about it is to think of answers
coming from multiple choice models and
somehow combine them and that's
something that we are trying to do right
now so I don't have any meaningful
answer have some conjectures I can tell
you about
yes first part it was a stationary
distribution yes why is that answer to
this
good ad okay so okay at some level here
is what's happening in both cases at
both case in you got some some signal
that you want to learn you're observing
aspects of signal through these let's
call it random matrices okay and if you
look at some form of Orion corn
approximation of these random matrices
they turn out to be closely related to
the signal and in both cases really what
we are trying to do is through
iterations you're trying to learn some
form of rank one approximation in the
first kid that rank one approximation
turns out to be the stationary
distribution in the second case that
turns out to be the approximation of
that bit chopped off matrix real life
people will talk to each other and make
sort of let's say people talk to each
other pairs and I make some kind of a
person and it up and you know I love you
and then you come support dude have you
is there any way to study how how how
these decisions to be made in
distributed weight among people great so
there are two things that is a dynamic
Spartan and there's a decision making
eventually let's suppose that one way I
mean one ideal way I would model people
people's behavior I mean this is I don't
know how meaningful it is but still it's
useful to think an ideal world is that
everybody has a choice model of their
own that's there in their mind
implicitly or explicitly over the
objects of interest and every time I
interact with somebody else that
information changes my choice model and
your choice model so our time it's
evolving so while we interact it evolves
and also at the same time this evolution
could in principle lead to some kind of
global decision-making if you are
extracting those informations out how to
think about that meaningful way I don't
know but it seems like maybe a
reasonable way to go about yes it's too
complicated it's too too messy the first
hour say I enjoy the talk very much
thank you mom I mean what they signify
enjoy the fact they are you bringing
these advanced math models into
practical systems or beans it's very
interesting the most part of the work
now for the first part we try to create
a partial order of all the object in the
physical space your study
one thing I basically notices in
basically practical world many times
there's more than one orders because of
preference so in that case maybe there
is something like the context where does
about the restaurant model yes these
spaces in right I mean not everyone has
the same preference order and that maybe
cause off the background but i don't
know i mean if you sin sanity of I mean
Chinese versus basically us maybe it's a
different basically all partial orders
you have and there if you like spiciness
not likes ice honest like one on I mean
you do not drink one you may have
different orders however this hidden
context is it possible to be applied
into the order is there let's say a user
I mean this is a particular user where
you basically make the recommendation is
it possible to take his hidden contacts
into consideration okay so I think you
bring up again very interesting point in
which both of them also brought up is
that thinking of entire world as one
choice model is not the right thing
because people are heterogeneous now
question is that how many one way to go
about dealing with this is say well a
good mixture of these choice models is
given fraction is this type p 2 is this
type and so on how many types and second
is what are those peas and then third is
what are those associated choice models
it is it's a hard question again I have
some conjectures and there are some
interesting things I can say about but
not with hundred percent confidence
people have tried studying this kind of
learning or distributions from partial
information including myself there are
some sparse choice model approximations
that we know but they are not I think
practically useful at least in my mind
so what people do in the world of for
example revenue management
it's a business school world people put
some kind of some kind of structured
structured mixture of the multinomial
logit models and then try to learn those
parameters related to that structure but
again that's its ad hoc and it's not
clear why Dash is the right way to think
about it so it's a great sets of
questions which some of you should
answer and yes so I have been working on
an application really like what you did
for the first part up in the nation of
your first part in the second fine I
won't get some opinion from you so the
application is helpful how to evaluate
your patients move on movement quality
ok and how to do that we need to run get
all of the stroke patients moment okay
how do you do that we pick two of them
you can ask the therapist who deep is
better okay so then this is a pairwise
comparison and we can we can compare all
of the best rotation okay the company is
so because the time for the struggle for
the therapist is constant yes so we
should ask X less as to service this
questions so which is your second part I
think there's the household you have a
lot of service and I have a lot of
stroke patient performance I won't ask
as mean as minimum as possible remember
foot service for the security so again I
think I've been through this very
quickly but again it's what you would
like to do is you would like to do
exactly the same thing but first setting
that is you want to maximize the there's
a comparison graph that you are creating
write effectively and you want that
comparison graph to have as large a
spectral gap as possible subject to your
constraints so for example there is a
therapist who cannot rank two patients
then you can't ask that question so
effectively you got this huge n cross n
matrix and you want to assign each
therapy
test or one therapy I don't know how
you're setting is to each one of the
entries to ask them question question
how would you choose those entries one
option is you choose them at random as
per additional graph or random regular
graph another option is if you could do
you choose structure structured expander
for example and whichever way you would
do it at least this result would say
that you are getting most amount of
information out of it I'll be happy you
know yes as last time as possible yes so
this will also again it will say that if
you have this is roughly how your
uncertainty would scale so this is how
many times you are asking a given pad
and this is how your structured graph
would looks so if this multiplication is
large enough for your your your metric
of interest then thanks if you have more
questions feel free to send me an e-mail
I'll be happy to yeah yes that is a
cheese I don't know she can bring back
slides that does vents this we had one
side mountain where you have the
summations that relate about this to
tease and teach to that is an iceberg oh
yeah I see the next one yeah that's
right yeah you're right
just something that yeah exactly yes so
basically if you have a set of the on
the left side you have an assumed set of
that isn't from that you you update the
t's and then on the other side given the
t's you update the w's now i'll go back
and update peace so you keep going back
keep order to it converges until it
converges are actually I would stop
after some number of iterations which is
so I will stop after k which scales like
effectively order 1 over 1 over log 1
over log Q these things are kind of in
the class of coordinate descent
algorithm is right where you estimating
in subspaces and those algorithms have
very bad convergence properties
including that if some strong
assumptions are not satisfied they
converge to something that's not even a
stationary excellent do you run into
issues like that no it's a great point
is one is because in factory if I want
to think of that and coordinate descent
way this is unconstrained so I there is
one reason why it's not happening if I
want to think of this in more classical
linear algebraic way then the factory
I'm doing a power iteration I'm trying
to compute the largest singular vector
of a matrix and these are very well
conditioned matrices and that is the
reason we are not running into that it's
an excellent point here
go together</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>