<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning and testing submodular functions | Coder Coacher - Coaching Coders</title><meta content="Learning and testing submodular functions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning and testing submodular functions</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XDl86Gq0ucs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay it's a pleasure to have Grigori
yaroslava here so he graduated from Penn
University and he is an intern here at
MSR today he will tell us about
submarine gorilla functions alright so
welcome to today's talk and so he
started learning in testing some of the
functions and I will give an overview of
this area together with some results
that we had in soda together with Sophia
all right so let's get started so the
key property that we will study today is
a property called similarity is the
property of functions and you can think
of the bundle arity as a discrete
analogue of convexity oken cavity
property depending on the setting so do
today in today's talk it will be mostly
like concavity for functions on the
boolean hypercube or informally
similarity is often described as the law
of diminishing returns you will see why
and this has multiple applications in
different areas starting from
combinatorial optimization and ranging
to a rhythmic game theory in the other
areas so let me define what's the
molarity is so we have a function over
the domain which is boolean hypercube
and it will also denote the domain as a
collection of all subsets of some finite
universe x and this function takes real
values say in the range from 0 to r we
have some upper bound on the values of
the function so the key notion here is
the notion of discrete derivative
because I said that the similarity is
like convexity and cavity is defined in
terms of its derivative so a discrete
derivative of such function is defined
as follows so we think of the domain svn
collection of substance of summer
universe and we look at certain element
X in this universe and we try to add it
to a certain subset of element as well
it is not yet present okay so and we
look at the marginal value if the
marginal increase when we add this
to this subset minus the value on the
original substance right so this is a
discrete derivative on set s with
respect to element X and because the
molarity is some analog of connectivity
it requires that all these derivatives
should be monotone that the derivative
on a mega set T should be at most the
derivative on a smaller set so the
derivative should be monitored
decreasing well not strictly decreasing
okay so this means the law of
diminishing returns when you add an
element to a biggest upset your marginal
increases it was the marginal increase
on the smallest upset right so maybe
let's stop here and make sure that this
definition is clear here please please
ask any questions because it's crucial
that you capture this definition will
work with it a lot all right so i will
give some examples now right so when you
add the settlement it should not be in
the same otherwise your money doesn't
change it on
Jesus right jeez a big yourself started
you it's not be until you something else
as well right alright so tease a big
upset and derivatives are you crazy non
asleep like right all right it
distributes satisfy difference right
yeah sure that's why it is 90 right all
right and there are some example that it
will be useful to have in mind so the
first basic example is the coverage
function so suppose you have a
collection of sets a 12 a and over
universe of size you and your function
is just say the total size of some
collection of these sets so suppose if
we get sub collection of these sets as
any other way evaluate the total size of
their union so it is easy to see that
the coverage function is submodular
because when you have a strictly larger
collection of sets which is a superset
of a collection and you add a new set to
it then the marginal coverage increase
can only be at moes their marginal
coverage increased when you add a set to
a smaller collection because some
overlap could be already covered so and
also it's important to note that the
coverage function is monotone so it will
be crucial for us a similar functions
can be either more I feel like a
coverage function or they might not be
necessarily want to know like a card
function so the card function is
associated with the graph so suppose we
have a graph here and we define a
function over all possible subsets of
vertices of this graphic a subset like
this green set on the picture and we
evaluate the total number of edges which
cross from these green set to the rest
of the graph to the compliment and it's
a more tricky exercise but it's not it's
not hard to convince yourself that the
cout function is also so modular but you
can see that it's not necessarily
monitored because when I have this green
said being really really big mike
sighs can go down already all right so
this is actually will be crucial we have
monitored and non-monetary some modular
functions and the type of questions that
we will be asking today is how can we
approximate these functions and the most
basic version of this question can look
as follows suppose I have as a modular
function we have values from 0 to our
real values and i have only polynomially
many queries that i can ask to this
function so it has exponential size the
main but they only happen in only
polynomially many queries so the
question is how many queries do I need
in order to a proxy made this function
so so this question was first studied by
Goom owns karma button me Rockne who
show that there is not much that you can
do so the vasan you can get with
polynomially many queries is roughly
square root of the size of the
underlines universe approximation this
will be a multiplicative approximation
in every point and the construction that
they give is shown on this picture on
the right so uh basically the
approximation that they give is pretty
simple you just take a square root of a
linear function which is shown in this
first this red surface and unfortunately
there will be these spikes these are the
true values of the function and they
deviate from this surface on some points
and that's why you get some pretty bad
approximation there will be these
deviations from this approximating
surface so an actual question to ask
next is maybe we can relax the
approximation guarantee maybe we don't
really want to approximate the function
everywhere as in this work by guma so
here note that we wanted oversight for
all arguments so that's why we get this
spike somewhere
and they really a screw up the
approximation so maybe we only want to
approximate the function only one with a
sub some fraction of points instead and
maybe this epsilon fracture will capture
those spikes we will get a good
approximation on the rest so another way
of formalizing this relaxed
approximation guarantee actually
correspond to what is known as Baxter
learning with membership queries on the
uniform distribution and it can be
described as follows so okay ah Baxter
learning is defined like so we want to
construct a randomized algorithm which
will approximate the function f what
does it mean so the randomized algorithm
AE outputs a hypothesis which is also
denoted as a here at the this hypothesis
suppose we want it to actually be
exactly equal to F of s on 1 minus
epsilon fraction of points so so this is
this interior expression so we have a
uniformly random sample s from from the
domain with probability 1 minus epsilon
the hypothesis a is exactly equal to the
function value and so this outer
guarantee holds suppose over the
randomness of a with some constant
probability ok does this make sense all
right so we just want to isolate this
type some fraction of points and we want
to try to be exact everywhere else is
here
well we connect right so so probably
correct means this one half and
approximately correct means 1 minus
epsilon in the definition of back loney
right so probably means that we saw some
reveals you overlook the randomness of a
you'll be approximately correct in a
sense that you will be correct on 1
minus epsilon fraction of points right
that's an excellent alright so
unfortunately if you want to try to be
exact everywhere this is a bit too naive
to hope that we can get much better
results by isolating the optional
fraction of points and there is a result
by balcony harm either should shelter it
is pretty much as hard as this original
problem of trying to be exact everywhere
so a to use an approximate version of
back learning so the first approximation
type that we consider is multiplicative
approximation so the sudden is like this
so again you have polynomially many
necks queries and you want to have a
multiplicative approximation noun 1
minus AB some fraction of points so this
is the model introduced by Vulcan and
Harvey so we'll call this probably
multiplicatively approximately correct
learning so the difference here is that
we don't want an exact guarantee here
inside but we want an approximate
guarantee with multiplicative factor of
alpha ok so the Aquarius our members you
can query any point of your liking in
this model right so back learning right
so I'm abusing a little bit the
definition of fact learning but you can
see the back line from random examples
and from membership Aquarius right so
here for now i'm considering membership
queries but definitely there are some
results of random examples as well but
I'm just not Satan them here to simplify
the presentation a little bit
alright so in this multiplicative model
what welcome inquiry show that you can
get slightly better approximation so
actually the previous wrote an
approximation has some log factors and
it is well so they get just a square
root of x approximation the square root
of the size of Acts approximation but
not much better is known in pmag models
and related a version of approximation
is additive approximation where you
don't want to have a model negative but
rather than additive approximation here
so in this interior expression you have
a beta parameter which bounced the
additive approximation in every point so
this actually has the motivation in
privacy this this way it was studied in
10 similar and the right different
results here by goof to hurt for at
newman and by sriracha climb and scatter
in leaves so what I want to show here is
that the dependence on on the precision
here is exponential so what you get in
in terms of beta is like the size of X
to the power 1 or beta squared running
time and query complexity so r is the
upper bound on the on the maximum value
so we have a range from 0 to R
and I will just put all these together
on a single slice so that look at them
closely all right so there is this yeah
so so this slide actually fix absolute
to be a constant to simplify what's
going on so we have a this pioneer in
goomer's Fermi mattamy Rockne
multiplicative square root of the side
effects approximation we have this ball
can harm a paper which gives PMAC
learning algorithm we have these two
additive learning algorithms and today
I'm going to show a new result that we
have with a soft heroes hardening cover
but we show and actually a pack learning
algorithm so now that if you have an
arbitrary submodular function which
takes real values like this it doesn't
really make much sense to ask for an
exact learning algorithm because the
function is real valued so what does it
even mean to try to to be exact with
respect to this function clearly you
cannot do much I mean because function
kanika average really really really
values and kneecaps out with them exact
length so that's why basically there is
nothing for back model on this slide
except for this work where we can see
their discrete rentschler suppose the
function takes a discrete values from 0
to R so now it makes sense to ask to be
exactly correct on on the values of this
function and we actually show that you
can get much better learning algorithms
for this case so an actual
correspondence with the previous work
would be to try to fix a the additive
error to be roughly one over to be
basically a constant like see one third
and so once you have a negative error
one third you can also get exact values
of function which takes values from 0 to
R right because you can just run your
additive error algorithm and just round
all the values to to the integral values
from 0 to R so this is an actual
starting point what
fortunately this previous adjective
learning algorithm would give you
exponential dependence like and size of
X to the power R squared so we actually
show that with respect to the number of
values that the function takes back
learning only requires the advanced
wishes polynomial in acts some fixed
polynomial and some function of R which
corresponds to some kind of fixed
parameter tractability respect to the
number of values oh so basically the
short answer is like this so if you do
so it's only works in one direction once
you have an entity if you can get this
but in the other direction if you want
to discretize the value of a function
then our album crucially relies on the
fact that function is a modular but if
you discretize is a modular function
what you get is not less or less an
order function anymore let's make sense
I mean like several similar function
which takes a real values from 0 to R
and you ran them to integers from 0 to R
and while dry examples phone is no
longer so modular after you do this and
our algorithms rely on this so but in
the other direction it is true if you
have a learning algorithm for with
additive error then you can run this
algorithm and then round the values
cascade is it seems to be breaking
pretty badly if you try to be out to do
this yeah so the question I asked you
okay great yes yeah that's a good
question to ask all right so I know he
is always ready to say not actually so
here is an actual running time so and so
banal of queries for this for all
algorithms except for the last one is
actually pretty much as the running time
and for Raphael groans it's actually
less queries so it's not polynomial in
acts vice rather poly logarithmic
dynasties functional bar so this would
be the complexity of
of this background open the main
dependence here is our yeah basically
you're right so for the query complexity
yeah when dependencies on earth and all
right so just make sure that I don't
oversimplify things so so this is all
stated for uniform distribution and this
is all stated for learning with querias
and some of the previous work it
actually has stronger guarantees for
example the Balkan encar way upper bound
works under arbitrary distribution and
some of these additive learning hours
they also have some nice properties they
may allow some tolerant queries on some
different kind of queries and they work
so for example the chirashi relaxing
agnostic setting if you know what that
means all right so i will explain how to
get this back learning algorithm which
shines videos here in the last column
and it relies on the foley structure
resolved official is that the Aerie
submodular function which takes integer
values from 0 to r can be represented by
formula which have which has wet big o
of our so it's not going to be important
the function takes these values from 0
to R it can be any values as long as we
have at most r plus 1 many of them and
still get a representation by the small
width formula and from that pretty much
you will be able to derive those
learning out of the cell will tell later
how to go to us alright so let's
continue all right yeah maybe like this
is the signal of results maybe is
another good place to ask questions if
you haven't eaten
the one you mentioned first yeah right
lower bonds so is so for example a one
lower bound says sister so it's a more
recent result which is not mentioned
here it says that in any pretty much
learning model you need exponential in
our many queries complexity yeah
newspaper has an X to the one third
approximation lower bound for polynomial
right right yeah
tomorrow for the formula so you can
think it over it pretty much as AK dnf
so sorry our DNA or there are already
enough so the only difference is going
to be that because a function takes
integer values which won't be a boolean
formula but it will be some
generalization of our dnf to non boolean
values I will tell later we'll get with
us but yet guess imitation that's what
it is so it will be just a maximum
number of literals in every clause the
clauses like a boolean close anyway
pretty much impact you exactly since I
was the value of them right yeah so
given a productive r is constant you
only need pulling up queries to figure
out all the values exactly yes
beautiful
Oh
this be
yeah but the recent events a nap so we
shall actually hide in here so the
defense of nestle is going to be like r
to the power exponential in one or
epsilon absolutely is a fraction of
voice where you can be incorrect uh-huh
make sounds all right good all right so
yeah all right all right since our all
this previous working out what happened
the heavy in subsequent progress we
shall want to mention only briefly and
just explain it'll like tell you roughly
what the techniques I use so in our work
we use approximation by pretty much like
our dnf looking formulas so in
subsequent work wife Alma mnemonic and
Qatari we should be a very recently in
call 2013 the use approximation by
decision trees instead and further
subsequent work use approximation by
hunters to get good learning algorithms
so roughly what what happens here is
once you have an approximation by our
dnf it's not that hard to show that you
can approximate by decision trees and
hunters but if you actually use this
more refined techniques than you get get
better parameters so specifically for
the setting that I considered on the
previous slide for this back learning of
functions with values from 0 to r you
can get better results in terms of
dependence on our and epsilon and so the
flavor of the results is you try to
minimize the number of variables that
your approximation depends upon and for
example the work of Feldman one and
qatari the first subsequent work it
shows that the modular functions can be
approximated in l1 and l2 distance by
hooters by functions which depend on
exponentially many variables to
the power our reps on to the power 4
which is not very surprised and actually
given the previous results but what is
actually very surprising is that you can
actually approximated them by pooters
which have only polynomially many
variables okay so I want to actually
mention this only briefly if you're
interested in this let me know and I can
give you the reference I would we have
to talk more about this but let's maybe
continue so Before we jump into the
technical details the final introductory
slide that I want to show is that the
Summa little functions actually live in
a bigger family of all kinds of
functions which are interested
interesting for algorithmic game theory
and other applications mechanism design
so the modular functions are actually
part of this hierarchy which is shown
here so this hierarchy comes from the
Nissan Lehman and Lehman paper and
social modular functions are subclass of
fraction is a palliative functions which
engine a subclass of sub editing
functions so all the results which are
known for the super classes of sub
editor functions also implied also apply
to submodular functions and there are
some subclasses of similar functions we
have specific interests like coverage
functions we have seen on the second
slide and there are some other classes
so there is a number of results for
specific classes which I'm not going to
mention in more detail but they are
summarized here on the right so
specifically coverage functions have
attracted lots of attention and some
restrictions of submodular functions all
right so now we go to the technical
details so I'm going to describe how to
represent some of the functions with
discrete range by formulas and before we
start I will try to give you some basic
intuition why it is possible to
represent the massage formulas so the
main intuition here is a modular
functions with discrete values
can be captured by their values on the
boundary so how does it work so as I
said for the purposes of these stocks
the modularity is similar to concave it
in so let's look at concave functions
which are defined on the real line while
discretized two points from 1 to N and
which take only discrete values from 0
to R where R is less than L so and as I
said some other functions can be
monitored and for the purposes of this
talk it will be much simpler to look at
monitoring functions first and all the
results pretty much you're going to
generalize to general submodular
functions so the most basic example is
the monitor and concave function so
suppose I have a monotone gunky function
which takes her plus one many belly so
how does it look like so as I said the
key intuition is that is captured by its
boundary which is indeed the case for
monitoring concave functions so if it
function only takes r plus 1 many belly
is and this monotone concave then it
actually becomes a constant after it
reaches its maximum value which happens
at Point and the way wishes at most arm
and after that this function cannot grow
anymore because it is concave if it was
to grow here somewhere then if we
violate concave itchin only takes it
most r plus 1 many various so it has to
be maximum within the first it most
are many points let's make sense
so if it stays flat then it stays flat
it cannot grow again right so if it
starts to growth and we're here it will
violate concave it right because like I
would peak this flat region together
with this point which grows and that
will be convex instead of being gunned
game no so it has to be curve downwards
cannot curve upwards all right so that's
the basic Confucian why monotone concave
functions are fully described by the
boundary which consists so that most are
many points on them on the left boundary
of the domain all right so and general
concave functions turn out to be only
twice more complex so they actually have
two boundaries one of the lot on the
left and one on the right and in the
middle they have to be constant and
convince yourself of this is drum so
this is the first source of intuition so
the second source of intuition is to
look at the hypercube domain since
suppose we have a function on the
hypercube now and it takes discrete
values from 0 to R and suppose we
actually consider the most basic case
when r is equal to 1 so when the
function is just boolean some modular
function so let's do this case study are
equal to 1 a good answer modular
functions and it is actually known that
monotone submodular functions are
exactly monomials namely disjunctions of
boolean variables so this is a
one-to-one correspondence every monomial
is a modern function boolean similar
function and vice versa and general
submodular functions with boolean range
can be described as a two-term a CNF
they can be described as a
function of two disjunctions one of them
is strictly monotone only has positive
literals and another is strictly
negative only has negative literals
so the mark on someone else so that's a
boolean function right for you oh this
is mm-hmm and this is also one to one
correspondence so if you are a boolean
submodular function then you can be
represented by these two terms enf and
vice versa all right so uh and the
general proof of the structural result
combines these two intuitions the
intuition from the line and this
intuition from considering boolean
functions so consider the hypercube
domain which I shown this picture so the
hierarchy of here is pictured as follows
so we order all sets in the domain so we
have like the universe x and we have all
possible subsets of X ordered by their
size so we have an empty scent on the
bottom and the substance ordered by
their size in layers so what I'm going
to show is that monotone submodular
functions with our many values are fully
captured by their boundary which in this
case corresponds to the region
consisting of sets of size at most are
and general submodular functions with
will be captured by their two boundaries
one corresponding to sense of size at
most r and another corresponding to
assess of sighs at least size of X minus
R all right so let's uh see a proof so
I'm going to show the proof for
morrisons modular functions and then
give some ideas how to generalize
general submodular functions so for
mountains of modular functions is
actually quite simple
for general similar functions basically
the idea is it once you know the values
of the function of onset of size at most
are and on sets of size that lays the
size of X minus are you can fully
determine the values here in between and
all other points okay all right so one
of them similar functions so you can see
it again the hypercube on this picture
so this is a hypercube ordered by the
size of a set from bottom to top so here
on the bottom is the empty set here is
the universe and consider the collection
of SAS of size it most are this is this
bottom region on the picture so the
first idea is because our function is
monotone if I take some set here on the
bottom say set us one and the value of
the function is f of s 1 here then
everywhere above on all super sets of
this set the value of the function is at
least a first one just movement any city
of the function now if i take another
set of stool here in the bottom and i
can see that the region of points above
s to the value is at least f of s 2 so
if I intersect these two regions then
the value will be at least the max of f
of s 1 and f of s 2 in the intersection
of these two regions so the nave
approach given this observation is to
write the form following formula for
every point t in the domain I can try to
express F of T as a maximum over all
subset s of T of size at most r of f of
s and it turns out that this is actually
an exact expression so the theorem says
that for monitoring similar functions
this will give you exact value of F of T
alright so I consider some point t in
the domain and the equation corresponds
to all possible substance of tea and we
take the maximum over all assets as
substance of tea which are in the
intersection of these two regions right
so we take the maximum overall a
substance of team which have size that
most are let's make sense all right so
as I have just shown just be
monotonicity of the original function we
didn't use anything else f of T is going
to be at least the maximum taking over
this subsets of size at most are so the
key part of the proof is the opposite
direction to show that F of T is
actually at most its value all right now
consider a again this point is somewhere
here in the hypercube
and then consider a set s prime which is
a smaller subset of T which has the same
function value namely F of s prime is
equal to F of T and is the smallest one
you cannot remove any element from ass
prime without violating the property
that the value is the same so because
this is a smaller size set for every
element X in s prime if I try to remove
an element from s prime this element X
then the value of the function has to go
down which means that on every element
which wishes in ass Prime the derivative
on s prime minus X on the element X is
strictly positive this is just by the
choice of s prime
so if I set as prime is here then on any
element which I can remove from a
spray-on element element press on the
net the derivative is strictly positive
and recall that the function is
submodular which means that it's
derivative is monotone decreasing so if
the derivative is actually sweetly
positive here there is strictly positive
on all subsets of s prime on those
elements which are in s prime okay which
means in this green region which
corresponds to all subsets of s prime
all the derivatives on the elements of s
prime are strictly greater than 0 so
because the function takes it most are
many different values it means that on
every path the value of the function
strictly increases in every step and it
means that the set s prime can only have
size at most are because the function
has to decrease every time and it can
only increase at most are many times
this is roughly the same argument that I
used on the line to show that the value
of the function is fully determined by
the first are many points all right so
this means that the saddest prime is
actually fully contained with us within
this region it has size at most are and
that means that the value of the
function at point t is at most this max
because recent point which actually
makes it its value exactly f of T and
it's a point corresponding to a set of
size at most are okay make sense it's
probably the most technical points so
far so I don't all make boys kappa
characters way wohlers infinite sense
but maybe then you need the function to
be a rank of Commodore oh rly you saying
that any monsters when a function is a
rank of some Metroid these are awesome
but more or less status at least let's
say silicon cylinder 20 of Alan
basically you're clear exactly this pond
is said that you said would be exactly
the largest develop remember who I see
it's not careful if it's not like all
cylinders it's not I see oh that's
interesting is I say yeah i'll be happy
just kinda alright so what you have just
seen that we can represent the models on
similar function using this expression
which takes the maximum overall sense of
size at most r of f of s and to jump to
the learning theoretical results i will
need to switch the notation a little bit
to go from this set theoretic notation
to more functional notation so replace
the size of the universe by n and we
will replace the collection of all
subsets by the indicator variables x1 to
xn a boolean variables which indicate
whether certain element is present in
the set X in the set alright so ah and
let's recall what a boolean k dnf
expression is so a boolean k dnf is a
formula which is a disjunction of a
collection of clauses each having at
most key literals and every literal is
either a variable or negation and you
take a conjunction of this literals to
get a clause so because our functions
are not boolean will use what we call CD
boolean k dnf so instead of disjunction
here we
use a maximum and every class will have
a certain constant a sub I associated
with that and the value of a sub I is
just some value from the range of the
function so in the boolean case well
basically max is just the disjunction
and all the constants will be one so the
sooner boolean k dnf expression is a
maximum over a collection of boolean
expressions of with at most K each x a
certain constant and we say that such a
formula is monotone if it doesn't have
any negations in any of its terms so in
this language the previous theorem can
be equivalently where stated as follows
that every monitor is a modular function
f can be represented by a monotone to
the boolean rd enough where constants
are taken from the range so now that is
just a coincidence that it so happens
that both the bit of the formula and the
constants are both bounded by our in
general this can be different but so
here it so happens that the values are
from 0 to R and the width by this result
is also bounded exactly by our and in
general doesn't have to be the case
ok so for general cellular functions
that will just stay the result and give
some intuition for how to prevent so if
you have a general similar function
which takes values from 0 to R then you
have to increase your width by a factor
of 2 3 way to our dnf expression you'll
no longer be monitored but it's so we'll
have constants from the range from 0 to
R so the hint which is the crucial
observation the proof relies on the
following construction due to lavage
which we caused a modular monetization
so the idea is to reduce the general
case to the monotron case somehow and
this construction allows us to do this
given us a modular function f you can
define what we call a monetization of
this module function f monotone on every
point s as follows if you take the
minimum overall as overall super sets of
s of the function value F of T and make
this F monotone of us then it's easy to
see that it will be a monotone function
well because for for a smaller point you
take us this mean over a logical action
obsess so it will be more nitin so the
freaky part is to show that is
submodular but it's also not too hard
okay so and I'll skip the details of the
proof for now so the idea is to do some
reduction to the monitor case all right
so
so in the rest of the talk i'm going to
show how to derive some consequences of
the structure result to for learning and
dustin properties of similar functions
so the key idea is that basically you
can reduce learning this to the boolean
formulas to learn acadiana formulas in a
pretty simple way so let's denote the
class of pseudo boolean formulas which
help with k and constants taken from the
range from 0 to our sdn FK are so here i
use different parameters so we now
approves k was either are for monitoring
functions or two are for general
submodular functions and they will use
the different parameters here k and r so
the reduction is actually pretty basic
so ah so the idea is to just define the
corresponding boolean indicator
functions which we going to learn so we
call them I slices of the original
function so suppose you have a function
which takes values from 0 to r you can
define these boolean I slices which are
boolean functions as follows so f sub i
is equal to one if and only if the value
of the function FX is this threshold I
and we can reduce the problem of
learning this pseudo boolean formulas
with constant from 0 to R to learn these
I slices of these formulas so you can
represent a function which takes our
values as a maximum over its I slices
you just multiply them by their trash
holes and take the maximum or where all
of them so and just remind that we want
to do pack learning meaning that we want
to be exact on one on a sectional
fraction of points with probability
one-half so this can be done as follows
we can learn every I slice or the
function f on say 1 minus epsilon over
our fraction of arguments and then by
Union bound if we have all those I
slices correct on that many arguments
the
only make mistakes on 1 minus epsilon
fraction of arguments overall if you
take the max over all those I slices
that we learn so this is a strategy
learn f sub I which is AK dianna formula
now yeah so this is cavitation maybe
that escaped well once you have a studio
boolean ah dnf key our formula and you
take a nice slice and you actually get a
KD enough that's the key observations
all right so and thus kadian apps we
know how to learn pretty well so ah but
I want to describe the technical ideas
of what is involved in k dnf learning so
if you want to learn a que dnf with
membership queries then the key property
that allows for efficient algorithms is
the fact that KD NFS are actually very
well concentrated on a certain number of
really efficient which is pretty small
so let's define what is called free s
Bar city of a class of functions ah this
is the number of rijeka officience that
you need to recover the largest react
refuses that you need to recover in
order to have a back learning algorithm
for every function with precision
epsilon and turns out that for KD n app
this number fria coefficients that you
need to recover only depends on the
width of the KDF it doesn't depend on
the number of variables at all since
this is a very nice result by monsewer
that in order to approximate this
modular function on 1 minus epsilon
fraction of points you only need to
query a certain function key to the
power we go of K log 10 reps so many
highest Fourier coefficients of this
function no dependence on the number of
variables again I want to stress this so
and once you know this you can leverage
your favorite algorithm for learning
Priya coefficients there are different
algorithms like for example could push
Levin's Mansour algorithmic on Rick
Levin which does this in time which is
polynomial in N and the sparsity
parameter s sub F or you can use maybe
some more recent algorithms which do
what is called attribute efficient
learning which allows us to reduce the
number of queries from polynomially many
non to poly logarithmic a nun and see we
have some polynomial dependence on the
sparsity parameter and this is basically
what we can use together with the
structural result that they have shown
in order to get those learning theory
results that have been shown in the
table right because we basically get
this poly log and many queries stand
some polynomial of this S sub F and in
our case s sub F is well so you would k
equal to R you get our to the power our
log when r epsilon and this is a number
of queries that we get all right so
there are some weak lower bounds all
right so yeah so this is basically the
sparsity of the nfg are it's going to be
k to the power kill log r over epsilon
there are some wino optimizations that
you can do on top of this but that's the
main idea and another application that
you want to show is to the area of
property testing so again let's see be
the class of similar functions which
take our values now I want to solve a
somewhat simpler problem instead of
trying to approximate the function
everywhere or on one exceptional
fraction of points I want to ask the
following question given an access to a
function can we test whether the
function is actually from this class
just output a single bit whether the
function is in the class or not in the
class and the decision is going to be
made approximately in some sense and it
will explain in a second how so a
property testing algorithm is a
randomized algorithm for distinguish
into situations situation number 1 is a
function f isn't the class of this
say some modular with our many values
this is the class of functions that we
interested in so this is the first
option F is in this class and the second
option is that F is epsilon far namely
differs on at least epsilon times the
size of the domain AB some fractional
points of the domain points from any
function in c so this is the absolute
faith and there are some functions which
are Absalom clothes so does the
functions for which you can output any
any answer all right so ah so the key
idea for property destined algorithms is
that k DNFs actually have small
representations not just in the fourier
domain as for the learning so in the
learning we saw that key DNFs can be
approximated by a number of free
officials which is just some function of
K and epsilon it turns out that actually
k DNFs have small representations by
formulas namely there are some key DN
a-- formulas we have small number of
variables which can approximate them
pretty boats so there is a fairly recent
resolved by gopal mchenry in gold we
shows that for every epsilon and for
every k dnf formula you can efficiently
specify this KD and a formula so your
original k dnf formula it can
potentially have say n to the power of
key many different clauses right because
you can have all possible substance of
variables and you can also have
negations so this result by gopal mechan
ringgold it shows that actually you
don't need enter the key many clauses
you can do with the number of clauses
which is only a function of K and
epsilon again again all depends on
epsilon also sorry no dependence on em
and thus k dnf formula will be
approximation to the original formula on
1 minus epsilon fraction of points so if
actually how they are going to box it
starts with this big Vienna formula that
you have and it shows there is a certain
subset of its closest with the number of
classes is only a function of K and
epsilon which actually gives a good
approximation so and this is actually
what we can leverage for property
testing algorithms using the following
approach which is called testing where
implicit learning so now that on the
previous slide what we have seen is that
our k DN a-- formulas can we have
excavated by formulas which only depend
on small number of variables because
every clause has it most key variables
once you have a small number of clauses
you also have small number of variables
overall so key DNA formulas can be
approximated by hooters functions which
only depend on JBAB so many where it was
and the novel variables is key organ or
epsilon to the power big of key and this
is the crucial property which allows to
design property destined algorithms once
you have an approximation by small
hunters you have efficiently property
destiny algorithms pretty much so we can
test what the function is submodular
with our many values with query
complexity which is only a function of
RN absol which doesn't depend on the
number of variables at all
and in subsequent work with bleh on axis
and say video we show that actually we
can show a more precise characterization
which is exact for similar functions but
our many values and this gives a better
dependence on the number of variables in
this country approximation and I just
want to mention some previous work on
testing similarities so who's the
modularity for real valued functions
seems to be a very pretty property to
test the last upper bound that we know
is one wrap selves is about big off
route down this is from the rug by
seshadri inborn drug and the only lower
bound is linear in hannah so there is
still this big exponential gap there are
some results for special cases so the
gap is yeah pretty big and to conclude I
want to show some other directions we
seem to be interesting so in this line
of work on learning submodular functions
there is a number of results on
approximating these functions by all
kinds of representations by hunters
decision trees are formulas one
questions is whether we can leverage
this results for optimization of some
modular functions this seems to be
challenging because if you want to
optimize this a modular function it's
actually crucial to know its values
everywhere as compared to one
interception fraction of points because
maybe the option value is exactly on
that epsilon fraction and you don't know
it but maybe these factual results
actually still tell us something
interesting question number one um and
question number two is actually because
we have some approximation results with
respect to distances other than hamming
distance for l1 and l2 distance who are
interesting to study test you where the
function is close to some modular with
respect to one and little to distance so
this might help overcome this big
exponential gap on the previous slide
which is known for having distance
and actually there are some results by
Feldman and Vaughn drug still
unpublished and publish results by Baron
Arizona coma and myself we have some
progress in that direction alright
thanks so that's pretty much it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>