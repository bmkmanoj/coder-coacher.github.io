<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Information Extraction Crossing Language, Robustness and Domain Barriers | Coder Coacher - Coaching Coders</title><meta content="Information Extraction Crossing Language, Robustness and Domain Barriers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Information Extraction Crossing Language, Robustness and Domain Barriers</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IloalRDpDjo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay we should be good to go now I think
so it's a pleasure to introduce image
zoo to knee today amid is a former
colleague of mine from IBM Research he
just recently decided to make a change
and moved to Bing where he's in the
metrics group he got his PhD at the
University of Nancy after that he was a
research staff member at bell labs for a
number of years before he joined IBM
where he spent sup many years actually
working on the IBM tale system which is
their commercial system for doing arabic
and chinese broadcast news transcription
and all kinds of interesting analysis
analyses there he's also written a book
recently published by prentice hall on
multilingual natural language processing
and so here he is thank you thank you
Jeff for this thank you everyone thank
you for coming so as you just said I
just joined my microsoft sam i'm a
colleague even though that the slides
present IBM so yeah i'm gonna talk about
my work in the last let's say about six
years and mostly on tails and i will
explain what i mean by that in a little
bit if i can get this going ok this is
the outline of this talk so i will
introduce tails and the information
extraction component that's what i want
to focus on today mostly i will focus on
the measure detection part the core
reference as well as the relation i have
it i see a type of there then i will
show how we can transfer a program from
one language to another or to many other
languages so how we make cross-language
transfer and what happened if we don't
have enough data to do that to train a
new model so there i will talk about
information propagation across languages
then
and what happened when we deploy
commercial system where robustness
matter because the text or the input
signal can be very noisy I will also
talk about that and then i will show how
all this technique can be applied to a
different domain and here i would show
how we use it this in the healthcare
space and i will conclude my talk so i
have a video to introduce tales it's
better than doing that through slide so
i will try to do that now
channels of news video 24-7 generating
english translation service captions
tailgate Uriel time monitoring of the
video as it is captured each row in the
table corresponds to a show we see the
show's Network the show's title the
language the start time of the show the
duration of the show including an
indication of how many minutes of it
have been captured already the lies
column indicates that one show is still
being captured while six others have
completed processing video is processed
in two minute segments labeled according
to how many minutes into the show the
segment began those shown in blue are
fully translated while light grey
indicates segments yet to be captured
let's look at a segment of Al Jazeera
news hovering over the segment the upper
left corner shows us a slideshow of the
keyframes of that segment of video
clicking on it we see the video clip
with the automatically generated
captions below how would a miniusb any
more accurately hacking the island of La
Cava TV del camión so this is the real
time how do you enlist relation of the
nada baked spoken audio we can also use
speech synthesis to dub the translation
over the original audio let's look at a
chinese segment this time good today the
evening competition there manages
everyone's attentively plan with the
united states in this comp we can also
make captions appear when hovering over
segments these features of tales video
tools allow us to monitor the capture
and processing of current video being
added to tales a searchable database so
these are snippets that we so are
important for information extraction
because also one of the idea of this
tool is to allow for let me give this
yes absolutely you can and actually we
are we are using only fight for that and
that's you
see in to do the search and we do the
search at the level of snippets and it
was developed initially to answer the
question who did what to whom so it's
not a regular kind of search its search
based on metadata visit on entity as
well as relations between entity and and
that's what I'm going to talk about
today and during this year I was it was
it was initially so the type of project
it's Gail the the tool that IBM built on
the gale DARPA project and that is
trying to sell to show to customer
that's is tail so all the technology
behind it we develop it for Gail however
we are presenting it into under a
different framework okay correct yes
still going on it is still it's applied
to different things it's applied to
healthcare I will try to talk about that
later on it's one of the joint
development agreement we signed with no
us this is a program because after you
develop the technology I thought I
museum seller yes obviously has an
incorrect so this is the architecture
actually of debt so you have you know
from the keyframe extraction you have to
detect speech no speech detection then
the speaker segments clustering then you
will run your speech to text and here we
have arabic and chinese you will have
your information extraction component on
top of speech and also on top of text
and then the name ATT transliteration is
the part mostly use it for machine
translation because internal you need
that for transliteration and then we
have the machine translation part arabic
to english chinese to english and then
you end ex all this and you can search
them with lucy nor only fight we have so
many hives yes what is it yes what is
you mean all this so you women is is a
component that help you it makes the
integration of these components very
easy so you don't want to be aware of
what is happening here
the infrastructure it's a kind of
middleware its infrastructure you don't
have to worry about what is happening at
the different components everyone will
implement this is going going to be an
annotator that's the that's the way that
you we may use this component these are
different annotator that communicate
with each other so the advantage of that
I can take this the IBM component and
they put microsoft component it's going
to be instantly there is no issue there
is no problem of using any the advantage
of this also for our customers is we
tell them this is the architecture we
have if you want to use your own speech
recognizer you want to use your own
machine translation we can do that Orion
program the larger scale you get a
module application module by module all
right it's actually it's varrick's ml
basic so you know it helps regarding the
interfaces let me how to go from one ok
yes I got it so here I just wanted to
highlight at a couple of minutes I
highlight where my contribution really
was in the previous years I contributed
to these different components so as an
example for the speech recognition part
my contribution was mostly on language
modeling on using rich morphological
language for Arabic putting that into an
orderly framework for language modeling
using you know edition set of feature
site tactic feature semantic role
labeling and also using diacritical
serration for arabic text even though
this is was not really successful in
improving the word yo rating speech
recognition but still it was an
interesting area to explore calvinism
let's be Alexa that's the Arabic League
that's the equivalent of Arabic league
that this happened every couple of years
in the Middle East so this one is
happening in Qatar and yeah
a neural network anything so that's it
like I said technique yes so the
technique was using neural network there
is a paper here so the idea the
advantage of using a neural network you
can throw in two different kind of
information and it will discriminate
among those information to help your
language model predict you better kind
of and and the idea here so the usual
you know the suspect language model the
engrams we try to in addition at the
word level we we did it at the morph
level because arabic is high morphologic
language we did also apply the world
class we use it semantic role labor as
an additional information there and that
helped not only on speech recognition
but also machine translation regarding
the name editor transliteration that was
mostly work on cross-lingual name
spelling normalization identify name
spelling variants and all that that
helps mostly for machine translation in
the machine translation part my work
mostly was an arabic to english and here
the work i did was on the word
segmentation levels and this boundary
detection part-of-speech tagging and
language modeling and today i'm going to
focus on this part which is the
information extraction part and that
will be the rest of my talk okay so kind
in real life we start with the text like
this there is plenty of tags and you
want to process this data to extract
some of from ish from it okay so the
first step you do is to tokenize the
text so you remove all these tags and
you separate the dots from the from the
tokens the punctuation as well you do
kind of and then you do Center
segmentation you find the sentences and
also you do case restoration in a sense
that here as an example
store the capital letter because it will
help you later on on detecting person
names and all that stuff then we do
parsing we run on top of that semantic
role labeling and we do the measure
detection part the mention detection
part is to recognize object that refers
to an entity's like here be laden battle
which is an event I'll talk about that
later CIA which is an organization we
also enter sit on dates we want to
recognize the dates and so on so forth
so once we recognize these entities we
want to link the entities that was
recognized objects we want to link the
objects that refers to the same entity
like here I need an affirmation telling
me that this token leader and this
person named bin Laden and this pronoun
his refer all to the same entity Bleddyn
I also want to know that this event
killed short battle they all refers to a
Bambi event the same happen for same
thing happened for the city here or
country here Pakistan and the same thing
happened for the president which is here
you have here mention president youhave
hear Barack Obama and both of them
refers to the same person to the same
entity visit of the United States now
yes make it look like those resolving
the after pres Obama yeah that is
actually to a cluster of mention yes
actually like no it's a close it's a
cluster of mentions that refers to that
same entity and if I things yes yes but
you know that the president in that text
and the person in that text they are
they are the same so if you look to the
name admissions you may find the name of
the entity okay once you recognize
entity we want to find the relation yes
question does the other this is sort of
the absolute resolution happen later or
or is that just too hard like to
maintain a set of things you know an
accurate set of actual entity physical
entities things can resolve to and then
all police say this business resulted
the core Exynos yes that's really yes
that's that comes later now that's got
was a lot of this stage i would give
detail but that was at this stage
retardant no is at this stage no no but
that was just that was just within the
text that was a group of things within
the tangoga detects with it thanks right
but I'm saying goes crosstalk across
documents into the actual physical world
where everything refers to actually but
that's that's comes much later that's
that's gum actually no no it does
because this is what happened we end
this we and X at this level we index
every single document of this level and
then when you do your search you are
searching for the entity let's say I am
searching for the entity Obama so I'm
going to hit all the documents that they
have the entity Obama that have an
entity and the entity has mention has a
name admission that is about
now I'm indexing machines I'm index
identity and ethics in relation but when
I set by indexing entities I have ID for
them it doesn't have to be Barack Obama
its ID XYZ it's a single ID no I don't
have a single database that's it's done
on the fly based on the name admission
within the entity right and after that
we need also to look into relations what
we call the relation and the idea here
if I have entity code which is an event
I have the location Pakistan I'd like to
know that the killer happened in
Pakistan and the name of that relation
is located at so we have set of around
hundred relations predefined and such as
manager off located at patient off and
and we try to do all this together at
the relational I'll come later in how to
do it just now I'm trying to and
introduced the problem all the time yes
but is the time normalization thank you
that's actually the idea if you have
yesterday in the text the yesterday has
to be converted to the exact date if I
have Sunday May first 2011 if I have
today today this isn't this happened
today has to be translated it has to be
converted into the exact date and that
basic arm if I don't have any
information that visit on the date of
the document and if I have some
information related here within the text
then again it's a kind of relation that
helped me the relation always they have
they have a specificity so they have
what we call Nene made specific generic
kind of relation and when it's a
specific usually it refers to to a
specific date like this vs generic which
can be yesterday tomorrow next month
things like that okay laters
that it's kind of tough going into but
not like other day
you know that that that's not correct
yes that's any better yes at the time
the time normalization is mostly related
when last year it happened that someone
went somewhere and that last year you
convert it into exact date and again the
whole idea we have in mind is this is
part of the gale project with the
distillation which is we want to answer
who did what to whom when and where and
the when is is is usually date interval
from that day to that date and most of
the document they have this information
that you have today's date and they tell
you the article about what is happening
last year so if you need to know what
last year means based on the document
happening today do you appreciate the
time too late for example will the
oldest person has another patient or by
present and so how you have shield the
time will begin I I don't associated
with the entity here I just replaced
last year with with the exact date and
if if if later on I have a query saying
that what that what's the occupation of
that person last year I again I will
translate last year into the exact date
and if both exact date matches then I
will consider this as as a head is the
envelope in translation system right
excuse me so you're included is an
output of translation system you know
this is here I'm during during training
well you are talking about decoding that
is to think during training it is it is
not translation is it is the source
language it is either Arabic or English
or Chinese during decoding it can be
both it can be the output of a
translation system or a speech
recognizer it can be also the original
text
yes so how we do measure detection
similar to many other applications such
as part of speech tagging shanker we
consider this as a sequential classifier
and the idea is to process the text from
left to right or right to left it
depends on the language you are
considering and for every token you make
a decision whether it begin beginner
mention it is inside the machine or it
is outside the mansion it's nothing so
and you run your classifier and you take
you can take either top end or you take
the best answer for that I'll get into
more details in a little bit so similar
to any other techniques including speech
recognition you know you compute the
probability or empty in that matter the
probability of the sequence of tags
given the words and and and and for that
we use again the base rules similar to
other applications and what we do for
that we use the maximum entropy from
work and actually I think that you know
this this is actually was investigated
by delle pietre as well as burger in
96-97 where they find an interesting
relationship between the maximum
likelihood estimates and the mod of
models and the maximum entropy models
and and they said the relationship that
happens here so actually founding visca
I mean estimating these probabilities
can be viewed as the maximum entropy and
also under the maximum likelihood
framework you know we choose those that
it maximizes the entropy over a set of
consistent models and maximize the
likelihood over a set of models and and
here so the whole idea what I'm trying
to say here is that
the maximum entropy model will not
assure anything except the evidence
right so if you have all the data in the
world available you are sure to converge
to the perfect model you want in reality
that's not true because you don't have
all the events it happen unless you have
a dead language if if we take if we take
the Egyptian languages and we take all
the data happening there and and there
is no new data happening anymore and we
train on that Mexico Metropia model any
discriminative training model we can
claim that we have a perfect ball in
reality we we live in in in live
languages that evolves and change over
time so that's not possibility so that's
why we try to exceed my estimate the
maximum likelihood so this year we
enjoyed Sierra we tried maxent we
believe i believe personally that what
matters is the used features more than
the approach itself so you know if you
look to be well that's also this happens
in my group and i have also this idea
maybe i'm wrong really the difference
between CRF at maxent is if you look to
the loss functions if you change the
loss function of them accent you get
Sierra it's I understand one is looking
to local the other one maxent mostly try
to do optimize locally and you hope that
that will be globally CRF who looks
globally so it's time consuming it takes
more time to do ceará excuse me you got
a tools 40
oops yeah and truth you mean yup yes the
tools are the tools yes we do have it
yes yes yes yes the the reason we yeah
and we offered for me it's for
historical reason as well IBM believes
that you know they they believe that
contributed to the invention of mexi so
they want to keep using it and since
there is no big difference in
performance so it's good claim to keep
using Mexico I feel like Fernando and
there's somebody here you big that would
say that way two percent drums here make
a big difference and faster yes the
weighted perceptron so again all these
thick including yeah you can go further
than that and you include them to that
SVM as well a support vector machine all
these techniques they are good they they
are comparable that's what I'm trying to
say they are based on the scrimmage of
training kind of approaches really the
main difference between them is the
usage feature use it the information
that you you flew into them so if if you
go basic information set us or using
only lexical input nothing else I would
assume that you will get comparable
results and carry guess right I didn't
try the perception we tried CRF we tried
support vector machine we tried the
hidden Markov model you know the
Markovian path the the challenge with
Markovian path is like it's very hard to
include additional features the
advantage of using Mac sentence CRF it's
like easy to implement additional
features so it's for convenience more
than for the theory behind it so yeah so
the fee here I'll go to the feature use
it for this so you know we use the
context of the world if I'm trying to
predict the tag of this word so why use
the context the previous Wars the next
word I use it in the we are using maxent
but also we are using the Markov
assumption of Max and so we use the
previous two tags we keep
we keep the history that's very
important because I saw this in many
papers that people telling you CRF works
better than maxent but the
implementation of MaxEnt is not the
Markovian maxent and and this makes a
lot of difference to catch that okay we
use dictionaries we call them gazetteer
so we have huge dick series of person
names location organization all that we
threw these as a features as a gazetteer
information document trigger level
features it's interesting to know at
what what is the document you are you
are handling and at the output of all
the semantic classifiers and this is
what i mean by symantec if you have
other classifiers you have if you have
the CRF classifiers I can use it I use
the output of this classifier as the
input to my model if you are working on
a different project and this has
happened so Fergal so we work it on ace
they have a different kind of tags I
will not throw that away I can use it i
use i have a classifier training on that
data i use the output of that classified
and threw it in as a feature it helps
yeah all right it helps a lot these i
mean there is there is two points F
major by doing this kind of things and
22.5 matter of the tails when you are
already of the 80 it's important the
training the data yes the number the
number of classes is around hundred
twenty alright so that is the very
sparse excuse me even the word in
context yes you used just anger maybe
yes we use the context of the two next
and the two previous five is five gram
contacts we did try we did try something
else actually we use also kinda see
partial information and semantic
rollerblading information so you have
the headword information you have in
semantic role labeling
will tell you more you have you have
this idea of you have the argument who
is doing the action who is getting the
action so all these information that we
threw as features here it helps as well
on top of the parser so you must have
something oh I see well we use yes
that's why we use the conditional
generative iteration scaling with the
Gaussian prior we need to do that yes we
cannot yes I I see your point no we
cannot train on yes true we need to do
that this is the entity that's why I'm
saying so this is the set of any to use
we have is a little bit different from
ace but it covered what we want so we
have hundred sixteen and that when I
said time three because we are
interested on named admissions and
nominal mentions and pranaam dimensions
we differentiate between them the he is
a pronominal president is a nominal
barack obama is named and this is an
idea about the performance so i know
that many people are familiar with
happening in ace and that's why the same
technique is used in s and i'm showing
the performance here in terms of
precision recall at F measure so this
performance in terms of s is very
competitive the same model was ranked in
the top in the AC valuation applying it
to terms because we have many many
classes the data is maybe a little bit
sparse here or that so the performance
drops a little bit
no good my own 10 28 x direction once
the sea is Compton is extraction and the
a is automatically takes action it's run
by NIST and yeah so this this is the
number of mentions we have and that's
the number of documents we have now we
talked about mention i want you to do
the core f part so how i do that so I'll
be brief here but we have a paper at ACL
for those that they want more details so
actually really I take all the mentions
here and my goal is to group them into
entities with different IDs and what for
that I use what we call the bell tree
algorithm so I start with the first
mention and then I decide so i have this
you know i have all these mentions in
here i take the first one i put it into
one class and then for the second one I
have two choices right at the first
choice is it belongs to the same class
the second choice will be that it starts
its own class all right and then when i
do that for the third one again the
different choices i have even this
belongs to the same class or it starts
another class the same thing in here all
right so and again i use my classifier
my maximum trippy classifiers with the
some some threshold because i can't rain
on all this of course we and and we try
to so i try to estimate the probability
of linking linking meaning put it in the
same class and estimate the probability
of starting in your entity
okay and again for that we use the
maximum entropy we are using the same
classifiers are the same framework we
don't have to do and the difference is
on the feature we use here when you do
entity it makes more sense to use we
using lexical features such as the
string match acronyms special match we
are using distant features how far they
are the two mentions from each other's
in terms of number of words also the
mention entity attributes we we did
recognize the mention we know the type
we know the level we know the gender we
know the numbers we can use that and
what is interesting actually this is
kind of almost most people use this what
maybe make the difference for this for
our approach is using the syntactic
features base it on the parse tree and
the semantic role labeling and yeah so
compared to what everyone else is using
the performance in terms of Karev is at
this level using syntactic features it
helped for English it tablets for Arabic
didn't help for Chinese the reason
because I don't know how to read Chinese
I don't know I was not able to debug
this I rented my features i got that
numbers I said done I was able to debug
this I was able to debug that and find
out how to improve things again we see
the Deaf set we see what's going wrong
in the deficit we try to improve the
performance tune the features to a deaf
set and then you have a blind test said
that this is on the blind test set right
but again when when you do your training
you always look to some data to see the
effect of this kind of feature on that
date all right
they came forward Intel in terms of
performers the portal is injected
creature yes be more than we doing with
our victim you should be more noisy than
vested true it should be more noisy
however remember that the fact that it
has a high morphology and all that so
you are capturing plenty of information
that you don't capture here so if you as
an example when use the context and
grams there and if you are doing it at
the stem level or at the morphs level
you are picking the morphs so maybe your
context get diluted a little bit however
when you add parser and semantic role
labeling you you get additional
information that you are losing before
because of that details of the morphs
and and that helps relation to the
clustering howdy mr. by an expression
search of you
excuse me bidding at you I mean this
this algorithm described yes however the
class does that three years that seems
to be an explanation for how do you have
it oh it's exponential problem so let me
go back so during training you have all
this so it's not the explanation it's
only during decoding go right during the
coding the issue you have if your path
is is low you just get rid of it is
similar to what we we do in speech
recognition all right you don't exploit
you when you do your Viterbi even hear
the bell tree you have a kind of Viterbi
so so so you discriminate not
discriminate you eliminate the path is
that they have you you believe that they
will not get you there it's the same
techniques applied here you are using a
viterbi anyway it's a bit three but
think about it as a kind of you know you
are exploring this path you have a
probability here you are you are
following that you are getting another
probability here if the cost of this
path is low I'm not going to follow it
it's a big shot yes yes ace v44 so so
this there is ace the nest that provides
some data and and also for some our
application we do have annotators
in-house that also human is you that
that provide the cetera what they have
how much data okay you know the rules is
the the more the better but the more you
have the better but at the level of
mushy detection we did find out around
the million token you get reasonable not
mentions text you you start to get a
reasonable model you can improve it the
improvement over that is is limited so
if you look to the ACE data a state it
wasn't the range of 400 k tokens and we
did add on top of that that's not right
it's not only 120 classes of average is
class who have how many samples so
roughly speaking 400 k if you divide
them bar 14 so that we are talking about
I mean if ya fue y a few thousand few
thousands yeah it's right so the
baseline is the your mother without
discipline picture picture and the
dualities Sutekh deterring the
experimenter yes um and the base a
spirit Max and paste yes and I remember
a couple other rappers reference
resolution papers for example from
Andrew background super B Newman's you
did you consider there was a space right
or was there some keep pressing yeah so
so that there is two techniques there is
there is so compared to Mac alone I know
we are using similar techniques that I
firstly the features nothing be on the
technique itself he tried to Sierra we
use it Mac sent but that again I believe
it's not big difference because I
believe that the features make the
difference there there is other
approaches where they try to do mentions
and koreff in parallel try to take it
both of them we we didn't explore that
path we know that the overall numbers
here is better we didn't implement that
because we think that there is a noisy
step in the middle maybe that's the
right way to do to do we joined approach
where you can learn from kind of real
from your mistakes we didn't explore
that so we did them into two stages once
nations that recognize it we we keep
sometimes that we try to keep the end
best but we didn't try to do the joint
approach between machine and Karev
relation so it's similar to the correct
part is really i have two mentions and
they try to find out if i have a
relation between them yes or no and to
do that i am training classifier that
for every relation for every two
mentions for all the features that they
can fire I detect is it does this
relation does exist or not and the non
relation is why itself relation okay the
club the number of them if they are
around the 50 I I will have more details
a little bit so again we I'll skip this
either we are using maximum entropy as
well so I discussed that so what is
important here you know this is the kind
of feature we have if you have a person
entity person visited a location that's
a good indication if you have a person's
person that's also like the father's son
that's a good indication that they are
correlated the organization person those
kind of feature that we are using of
course for for binary features you
understand for numeric features such as
the distance between the two and all
that it's better to bend them we cannot
really use real I'm accent it's better
to to find bins and and that's how we
use them so again these features at the
parse tree we find a path from here to
here at the dependency tree we find the
head words and this is other kind of
features so the organ is I have the
organization I have the person I
mentioned that and there is two
approaches to do relation we can do it
sequentially or we do it in a cascaded
way sequentially cell dies we take every
pair and we recognize it does exist or
not so as I mention here in the Cascade
approach so you you first of all you
find because there is many male
in relation anyway we need to detect the
tents the modality and the specificity
of the models right so one way is to
separate all of them to detect one at
the time or to predict all of them at
once and one issue with the relation is
the number of class non relation is a
huge in the data compared to the
entities where there is a relation and
we have to deal with that that's a big
issue here when we call that you have
the same pairwise you have centers yes
you have a sentence you have machines or
entities within the sentence and you
don't you want to know if the if if a
relation does exist or not sorry oh now
you do that at the Centers level only
because you know the at the whole
document you have the entities the
mention that our group it in entity that
they will help you but you do it at the
sentence level and even at the sentence
level there is many entities that there
is no links between them because they
are not related so you end up when you
are looking to the training data the
number of non relations label is huge
compared it to the number of relation
and that's why we we did this bagging
approach and the idea if to sample the
data create different samples trained
many classifiers and then and then if at
least and vote then you call it a
relation so we use the majority vote
that's the non bagging and also we can
use if at least n then it is a relation
and we do it twice actually first of all
we need to detect do we have a relation
or not and once we know that we have a
relation then we detect what kind of
relation it is okay
relational matheus and then if you have
my person person subset of the religion
me yes before everything so so person
person you have it can be related to
this is the kind of relation we have
right it can be planet off because that
supplies four percent person's it can be
relative when the second stage you will
detect the specificity of the relation
okay and now what happened if you apply
to this in Arabic so I only see one
Arabic speakers I'll just I'll give
brief idea here if I have this is the
English text and if you assume that guy
is Arabic it you increase ambiguity
because there is no vowels so if you
take a text and you remove the walls you
get something like this in Arabic now
there is a lack of capital letters that
also adds another level of ambiguity and
because of rich morphology few words are
attached to each other like here alright
so that adds another level and so a
sentence that was initially like this it
becomes like this if you see it from
arabic respect and you need to hand them
that letter oh not at the letter level
it's at the world level but it happens
that this gets glued together here you
remove the vowels you remove the capital
letters yes this is the procession of
this sentence is the same this is
English I see it's English it's English
that I wrote it arabic way so if you
cannot read this that will tells you
that's exactly how hard is the problem
because this is English but that's how
Arabic speakers read Arabic right so
what we do we segment the text I go fast
for this we segment the text we take the
text we separated into segments like
here and then we run the same classified
into segments to detect the entities and
then the relation
and and here i am showing the
performance we have on arabic this is
the feature use it what i want to show
here is if I don't have enough data for
a specific language what I can I do I
have a rich language english is rach we
have a bunch of resources that we have
plenty of annotated data that is not the
case for other languages such as Arabic
how much time is still help good so so I
the idea was how to use rich language
Josh English to improve other languages
such as here our target is Arabic
Chinese a German French and Spanish
these are the language that we want to
handle with tales where we do have some
data it's not we don't have data at all
but the data we have it depends so it
depends on the language and it depends
so so I'm not going to muti I did
already the motivation so what we did is
the following I have a rich set of
language let's see here English and I
want to know how can I have my Arabic
model benefit from the English model so
what we do you know you train your model
you use all the features you have the
data so you do the usual things ok so
that ends up with you have the English
side with all the tags you have the
alignment between the two you can use
even the publicly available like Jesus
gives or something a liner between the
two languages if if you have the
translation of that ok and you propagate
this mentions to the Arabic side or in
that matter to spanish or anything else
and you will get your mentions in the
target language now if you get that what
you can do with them you can assume that
this is the results if you don't have
any data and rotated at all in the
target language so you say this is my
results or you can set ok i will use
this as an additional features in my
framework i can build
dictionaries from this data if I have
here a huge amount of data because I
don't know if I you look to the European
Parliament where they have plenty of
languages language pair and they are all
aligned and you can take this is a huge
so you take all this extract get the
tears from that and use that as a
feature you can also build the model if
you hear you you take it your training
data you will get a training data
annotated so you will use this a
training data to build the model and use
that model as a feature in your target
language here the same example i'm
showing for showing for a spanish so
again i tagged the english part and then
i propagate that into spanish so the way
as i said the way we traded it so we
have if i don't have resources at all i
just use that as the output of my
classifiers if i have some data then i
may use that you use that data to train
a classifier and use it as a feature and
here it's a gazetteer building
dictionaries and using them as
dictionaries and i will compare this
shortly this we try this on ace data
because this is publicly available and
we can publish it so the area feature
use it our lexical lexical widths and
tags and with semantic features and the
idea the idea we have here we want to
show that the game performance decreases
with the amount of resources used in the
source language so if you have a donor
language which is hearing English you
have a plenty of resources if you want
to help another language and that
language has already interesting
resources you were not going to help it
if that language has poor sources yes
you will help and that's what we will
prove soon so in our case we use it
English this is the performance of
English we have other classified this is
there is no language propagation and
here you see the performance on arabic
chinese and spanish where there is no
resources at all
in these languages so it's only
propagation it's only the results from
propagation oops excuse me it
language training yes there is no no
target language this is only English i
have only english data and this is the
performance i get in in Arabic in
Chinese and in Spanish and the reason
here the performance is better makes
sense because Spanish is a closer to
English then here between Chinese and
Arabic it's hard to make a claim because
both of them are different but for
whatever reason Chinese looks what is
the error rate for English that is
eighty percent eighty percent f-measure
and this is when we use lexical features
so already we have some data in the
target languages so we see that the
performance go like for Spanish it goes
to 77 so we are getting a close to the
81 of English here cent tax and hear all
the information so this is so until
stick right here to see Arabic that
Arabic we are less than one point behind
English so all you do is that you just
propagate the intrinsic and pretending
that they're correct yes a little
training model on that and extract
dictionary from that and I trained and
and they feed that yes so I use the last
five minutes in here so so all these
models that I'm showing when we show
this to to customer and the customer you
know customer usually doesn't expect to
have a clean text right here you have a
clean text you have the system behaving
properly this is the likelihood from the
max and the probability for every token
so everything's fine here we may get
confused between number one and number
two but still the performance is
reasonable now our customer sometimes he
feeds text like this we have english
model but he still feeds this kind of
text and he expected the model to behave
properly and that model that i just
described it this is what is doing very
bad okay and the same we have the same
text we have another customer from the
finance sector and this is the kind of
data whether they give to us this is
actually not the exact data because I
cannot show the exact data but I try to
mimic a little bit how it is and he
expected to have a behavior like this
but again the system doesn't behave the
proper way so plenty of techniques
actually are proposed how to overcome I
mean to to to to deal with this noise we
get inspired from these techniques but
what we did here we we first of all we
run the system to know
what is English versus non English with
some probability we process the text and
then we try to remove to find the sgm
tags and all that part and then how do
we know that the text is English or not
Angeles we are using the perplexity as a
measure use it in here so we find out
that the perplexity is a good indicator
to define if this sentence is a good
English sentence or not and the
perplexity is not a binary decision good
or bad is like perplexity is low enough
so this is a good English perplexity is
confusing I don't know publicity is very
very high this should be really very bad
english and and and because of that we
recreate these different models the in
the clean text makes it model in a sense
that it's traded on this noisy data and
again and we have also models that are
using only gazetteers only dictionaries
and this is the performance so this is
the baseline this is a clean txt we see
here how it falls miserably when we turn
it on when you decode a text that is not
English but we see that with the
technique with the proposal even though
we lose a little bit but that's okay for
commercial purpose because in
counterpart we catch up on this kind of
data yes this this is a link go this is
a la lengua this is English it is not
funny continue to the information
no no this is has nothing to do with the
propagation this is if I'm sorry I am
back to 22 only one language the
affirmation provocation is done require
para that is that part is done now I am
I am back to my monolingual part okay so
yeah I I'm on time I think so all things
that I mentioned before the recent
projects I work it on is to apply Tom
healthcare healthcare looks to be very
similar problem doctors dictate a lot of
documents using a sr technology they
also type text and when they try to send
that to the insurance actually the
insurance really what worry about what
the insurance wants is the icd-9 or the
icd-10 code that matches the procedures
that happened that they don't want to
read the entire document they are not
interested on that and right now they
are this coder icd-9 icd-10 codes that
are trying to do that manually and with
this project we are trying to help them
do that automatically so this is a kind
of text you get in a medical and it will
be nice to know that the shapes pain is
a problem probably is the hatch you need
to define because probably this this is
not probably there is a big difference
in the health care area so the terms
like probably not really not sure by
themselves are mentioned that we need to
detect so we call that hedges and the
relation between this and this so we
have chest pain noncardiac and there is
a split utter so that means it's they
belong to the same attribute but they
are splitted and here modified by so
this probably modify the meaning of this
so base it also this is so again we do
mention detection it's a sequential
classifier we are on that same
classified detect relation and and and
here another example where she is on the
medication the dosage is 40 milligram it
takes by ivy it's enter
yeah push daily so you have the
frequency the dosage you need to
recognize all those and base it on that
you will give the ICD once you have that
it's a trivial a kind of to find the
icd-9 code and send that to be insurance
again you have my new taters you have
coders that that's right now they do
that job manually so we are using that
data trying to take advantage of it yes
alright so we have a no taters doing
that and we are training model based on
that we are using kind of active
learning on order to expedite the
process but that's the idea this is also
again with sometimes the same mentions
belong to two different relation we need
to take care of that because here does
not consume cigarette or alcohol that
means does not consume cigarette and
does not consume alcohol so we need to
really to get both of it not only one
its details but it's important so anyway
to conclude I try to present and to end
statistical approach for affirmation
extraction and showed how we can this
same technique apply it to different
language and if we don't have enough
resources in the target language how we
can use information transfer across
languages and again the if the receiver
target language has enough resources
this approach will not be that nice so
we need to have different kind of
resource rich and resource poor
languages then you talk about robustness
too noisy text and how we can apply this
to other areas such as health care all
these gave very competitive performance
in the healthcare space it's it's
already used in commercial purposes for
tales it was it is also ship it for a
couple of customers and and that's about
it thank you
each of you sometimes it features they
they were useful to not all the syntax
is mostly the headword affirmation is
very important that helps to find the
the split attributes however the parser
itself we use some chunking information
to know the limit where the chunker's
stops that's it but also actually here
our parser is a trainer on the same kind
of data as well so we are not using a
parser on a regular English text</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>