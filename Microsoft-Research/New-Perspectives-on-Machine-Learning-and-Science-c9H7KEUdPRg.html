<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Perspectives on Machine Learning and Science | Coder Coacher - Coaching Coders</title><meta content="New Perspectives on Machine Learning and Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Perspectives on Machine Learning and Science</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c9H7KEUdPRg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
next speaker rich Caruana so risk ariana
is a senior researcher at microsoft
research and the his research focus is
actually quite varied and wide he's been
working and he's interested in learning
for medical decision making deep
learning adaptive clustering
computational ecology and an actually
prior to microsoft also what he's been
doing has been also quite diverse he did
he studies at carnegie mellon in the
center for learning and discovery also
worked at cornell university and UCLA at
the Medical School and today is going to
tell us how deep dip Nets need to be
three hey Mike working ok so I'm rich
caruana microsoft research and i'm going
to tell you about some work we've been
doing on trying to better understand why
deep learning deep neural Nets works and
this is work that started last year with
an intern from University of Toronto
named Jimmy bah so okay so here's a
three different kinds of neural Nets
these are the sort of shallow neural
nets that all of us trained 25 years ago
in the early 90s we have inputs a
nonlinear hidden layer typically one or
most two layers and then the outputs and
we train it with vanilla back problem
this is a deeper neural net in the
middle here this neural net has three
hidden layers between the inputs and the
outputs these are typically nonlinear
units like sigmoid just conventional
stuff it's really just this model with
extra layers in it and then you might
use some extra tricks when you train the
snare layers you might do some sort of
pre training or maybe you're going to do
some sort of regularization leg drop out
but this is a deep neural net and I
guess I consider sort of three nonlinear
hidden layers to be the sort of smallest
neural net that deserves to be called
deep then this is a more complicated
deep neural net on the far right here
this also has three nonlinear hidden
layers but before that
between those hidden layers and the
inputs there's these convolutional
layers so in convolutional neural nets
is something that was created in the
late 80s and it's still something that's
very powerful and I'm not going to go
into what they do most of you are
probably familiar with them but
basically they make it so that instead
of having to learn to recognize a face
or a feature of a face in one region and
then having to separately learn how to
do that for every other region say the
upper left hand corner of the image
instead we can build one feature
detector to learn some piece of a face
and we can map that scan it around the
entire image and that's effectively what
convolution gives you at least in images
so these are shallow neural Nets this is
a moderately deep neural net and then
this is an even deeper neural net now
I'm going to show you some results on
the timid speech recognition data set
timid is Texas Instruments MIT dev said
it's been around 20 plus years it's been
around a long time so many people have
worked on this data set and tried to get
good results on it this is a picture of
what timid data looks like this is time
on the x-axis and these are frequency
bands on the vertical axis and basically
this is a speech utterance I'm told that
people who work in this all the time can
actually recognize what you're saying by
looking at these pictures I I cannot do
that we turn this into features and what
we end up with is an input vector that's
about 1,800 input features and then
there's a hundred and eighty-three try
phones that we're trying to predict from
this data and on this particular data
set this would have been considered a
very large data set for the early 90s we
have about a million frames in the
training set there is about 450 speakers
in the training set another 50 speakers
and the development set and then finally
24 speakers in a test set so this is a
considered a fairly large data set and
that's one of the reasons why people
have been able to use this data set with
deep learning so now let me show you
some results so as I said this data sets
been around a long time these are our
rates so this is a twenty-five percent
error rate and this is back in 98 this
is about the best people could achieve
after a decade of working on this data
set so this was considered extremely
good performance circa nineteen
98 it took a tremendous amount of effort
to develop the models that exhibited
that kind of performance so this was
really good stuff and then what happened
is nothing people continue to work on
the data set and very little progress
was made that is it was very hard to do
better than this Beijing hidden Markov
model and these numbers basically stood
for about a decade so people did not
make much progress and then deep
learning happened so the first attempt
combined deep learning with a hidden
Markov model in 09 and suddenly a sort
of two percent reduction in error rate
so that's really significant when people
haven't been able to move this number
for a long long time suddenly you see a
two percent reduction then people try to
variation on this adding dropout a
regularization procedure a huge
additional improvement another three
percent improvement in the error rate
then people tried convolutional nets
another kind of deep neural Nets yet
another percent improvement and the
rumor I've heard is that maybe it's
geoff hinton has a recurrent neural net
that is now doing incredibly well and if
you look at this realize this is the
result of a decade of work and then this
number stood for a long long time and
all the sudden people start applying
deep learning to this problem there are
dramatic reductions in the ER of these
models this is really tremendous
tremendous improvements so let's talk
about that so you know why is it that if
we take a humble neural net like this
which is kind of shallow and add a few
more layers to it and do a few extra
things that all of the sudden we started
cheating accuracies that were undreamt
of before we did that so why is that
working well there's a number of
possibilities so one possibility is that
you get from these deep models some sort
of extra expressiveness in the functions
that you can express and learn but you
get that without requiring some
Exponential's say increase in the number
of parameters so this is this is one
argument for why deep learning works and
this is true for certain classes of
functions certainly it's true for parody
functions that a deeper model is
actually more expressive in that class
than a shallow model with more
parameters another possibility is that
these sort of feature
hierarchies that get learned in the deep
models is really critical to learning
well from from limited data so in image
processing when people look at what's
learned in different layers they
sometimes see that we learn sort of edge
detectors sift light features in the
first layers then we start learning
things that you know if this is a face
recognition problem are start learning
pieces of faces in sort of middle layers
and then in final layers we actually
start combining those pieces and
learning things that are like faces
maybe it turns out that learning a
hierarchy of features is just critical
to getting good performance on the kind
of tasks that were applying deep
learning to it's possible they're
working just because the learning
algorithms we have right now are just
very well suited to these deep models so
it may have nothing to do with this
hierarchy of features or with the
expressiveness of the model class it may
just be that our current learning
algorithms and regularization methods
such as dropout are just very well tuned
to deep learning now it could be that
it's some combination of all of these
effects it's possible that it's none of
these effects you can actually come up
with additional reasons why these things
might work so what we're going to do is
try to look into this a little bit and
try to figure out a small piece of what
might be going on here so now many of
you will know if you know the history of
neural Nets that we had representation
theorems you know from the 90s which
basically said that a single layer
neural net with nonlinear units if you
put enough hidden units there is capable
of representing any reasonable function
so we're not talking about you know we
know that a single hidden layer could do
it it's just that we observe empirically
it doesn't do it so we're really more
interested this is really a
representation theorem it says if you
were to make that layer big enough there
exists a neural net which would
represent the function that you're
interested in but we observe that when
we make those layers big enough we don't
learn that function so it's important to
realize that this representation
theorems are theories of what's
representable not theories of what's
learnable from data and we're much more
interested in given the kinds of
functions that we need to learn given
that we're learning from real data often
limited real data
do the deep Nets really have an
advantage they seem to have a big
advantage do they really have a big
advantage of rachelle in that so that's
what we're going to talk about okay so
let me show you what our new results are
this is a summary of where we're going
so remember these are the error rates
this is a shallow neural net with one
hidden layer when we trained it we get
our rates of about twenty-three
twenty-four percent if we make it three
hidden layers we reduce the r8 about two
percent so this mirrors the results that
I showed you in the other table if we
make this a convolutional model this is
a lead dangs convolutional model we drop
our wait another two percent so this is
a very big reduction in our and then
what I'm going to show you is that we've
been able to train shallow neural Nets
if we make the number of parameters
small so that it's like this we're able
to train a shallow neural net that has
comparable accuracy to this even though
it has only one hidden layer and if we
make the number of parameters in that
shallow neural net larger we're able to
make that neural net is accurate is this
convolutional neural net even though it
has no convolution in it so I'm going to
show you how we how we do that okay so
I'm going to just describe three of the
tricks that we use to make that happen
one is a model compression another is a
trick for speeding up learning and then
the final one is this ensemble of
convolutional neural nets let's talk
about model compression first actually
will spend more most of the time talking
about that so it's very very simple this
idea has been around about ten years
what you're going to do is you train
some massive very smart model then you
pass a bunch of unlabeled data then you
pass some huge amount of unlabeled data
through that very smart model and you
collect the predictions that that very
smart model makes and then we're going
to Train another model often a smaller
model on this synthetic data set to
mimic to learn the function that was
learned by that very smart probably very
big model so so we're doing quote a
compression trick because we're trying
to get a small model to learn the
function that was learned by bigger
model so that's the game we're going to
play it's very simple let me show you
this as a result from almost ten years
ago so this is root mean squared error
so down is good
square root of 0 would be down here this
is a neural net that was trained on this
original data set it's the best neural
net week a trained trying many different
architectures many different training
procedures on the original data set it
has a squared error just above point one
it's a pretty good model but we were
able to train a better model with this
ensemble selection technique that I'm
not going to talk about but this is a
much smarter model than this neural net
and then we did this trick this
compression trick which is we took a
bunch of unlabeled data passed it
through this very smart model collected
its predictions and then we trained
neural Nets of different size this is
number of hidden units of this mimic
model on the x-axis we trained neural
Nets of different size on this synthetic
data collected from the very smart model
and it's trying to mimic this model and
you can see that by the time we get to
say 32 or 64 hidden units this small
neural net in this case with one hidden
layer is able to yield an accuracy
almost as good as the model that's
teaching it that's being used to label
its training set and the remarkable
thing is not necessarily that the neural
net was capable of reproducing this
ensemble the amazing thing is that we
couldn't train a neural net on the
original training data to do that in the
first place that's somehow by going
through this smart model as an
intermediary using it to label data and
then training a neural net on that
synthetic data to mimic this model we're
able to achieve better better accuracy
so so that's the game we're going to
play why why did that work let me just
give you some quick reasons why it might
work this is a very intuitive
description if there were some errors in
the original labels of the data if the
smart model didn't make those errors
didn't over fit to the data those errors
are now gone from the data that's being
used for the mimic model so that that's
an advantage similar to ours maybe
there's just some labels that are
incredibly hard to learn there in sparse
regions of the space where things are
very complicated it may turn out that
the smart model basically didn't learn
to do this complex function
approximation in that region of the
space so it presents a simplified
smoother version of the function for the
mimic model in that region of the space
also the function that
mimick is trying to learn is learning
friendly because it was already learned
by another learning method right so in
particular while the original data the
labels may depend on features that were
not available to the learning method as
inputs at this point once we're trying
to learn the function that was learned
by this other model it is just a
function of the inputs so any dependence
on features that were not available has
been washed away by going through this
intermediate model that means again the
mimic model the student model has an
easier learning problem so finally we're
going to use soft target so often the
initial data is 0-1 boolean labels that
that can be hard to learn from instead
we're going to use soft values like
probabilities you know point 20 point
point 90.7 s and we're going to do soft
learning here and that also seems to
make the mimic model easier to learn
okay so and here's the objective
function so instead of doing the usual
softmax 01 log lost kind of training
that you do on 01 targets instead we're
going to take the probabilities before
they go through the softmax that the
smart model has predicted and we're
going to learn those probabilities with
the mimic model using squared error okay
this is just a simpler objective
function in many cases for learning now
we don't on timid actually have any
extra unlabeled data so what we're going
to do is we're just going to take the
training set and we're going to throw
away all the labels after we train the
really smart models we'll just throw
away all the labels and we'll reuse that
as our unlabeled data and make
predictions with it with the smart model
and this is a hard way to do compression
you'd actually like to have lots of
unlabeled data which we don't has have
so we're going to use some trickery to
sort of overcome the loss of accuracy
that typically happens when you do this
okay so let me show you the results on
timet this is really going to be the
heart of the talk now it's the the next
few slides okay so let me show you this
graph so here we have accuracy on the
timet development set so up is good this
is the number of parameters on a log
scale in millions that are in the model
that we're training so these are pretty
small neural Nets over here and this
line is for a shallow neural net just
long hidden layer okay this is a pretty
small networks over here these just have
you know five ten one hundred and it by
the time you get in here one to 10
million parameters these are pretty good
size neural nets these have somewhere
between a thousand and ten thousand
hidden units here so these are getting
to be pretty big and then these are
pretty big neural nets so these neural
nets are starting to have two hundred
thousand four hundred thousand hidden
units and in fact these that neural net
is just the largest single hidden layer
neural net week a train on our GPU so
that's why the graph stops there now
let's look at the graph so what do we
see remember this is just a shallow
neural net so there's no deep learning
here there's no mimicry this is a
shallow neural net being trained on the
original data set so this is how well
you do well if the net is too small you
don't do too well we've known for a long
time that having a lot of extra capacity
and neural net seems to help we sort of
do well when we start getting to maybe
10,000 hidden units in the shallow
neural net and our accuracy peaks at
about seventy eight percent and then
maybe there's even a sign that we start
over fitting if we go beyond that ok
that's a shallow neural net if we had
the horsepower we could have trained
this neural net back in nineteen ninety
so there's nothing nothing unusual
happening there this is a deep net this
is three hidden layers no convolution
just three hidden layers deep net being
trained on that same data and again
we're counting the number of parameters
here so the deep nets that are around
here are sort of two K by 2 K by 2 K
that's how they have a similar number of
parameters to something that might be a
TA or 10k hidden units if it's a shallow
net and what do we see well you know
nobody does very well out here when the
models are too small once we get to
models that are large enough the deep
Nets do significantly better two percent
better than the shallowness right so
this is the magic of deep learning this
is something we didn't know about until
the last five to ten years okay so this
is great and then performance levels off
once we get to a network that's about
this size which I think that might be
the 2 K 2 K 2 K model basically there's
no extra advantage to making it bigger
here's a convolutional deep net so this
is lee dang's model so this is a this
blue line this is a convolutional layer
max pooling layer then
followed by three nonlinear layers as in
that model and I haven't done this
versus the number of parameters we
haven't actually run the expensive
experiment to let us vary the number of
parameters plus it's not really clear
how to vary the number of parameters
right we could change the convolutional
layers so that they were different we
could change the number of parameters in
the first nonlinear layer that's
attached to the convolutional layer vary
the number of parameters after it's
actually more difficult to know how to
vary number of parameters in a
convolutional model so we haven't done
those experiments instead it's just a
sort of target that we're hoping to
reach right this is already a deep model
this is a deeper model and this is great
right again we're seeing a sort of two
percent increase in accuracy by adding
convolution making this model deeper now
I'm going to show you another line this
purple line where we've got yet another
percent increase and what we've done is
we made an ensemble of nine of these
convolutional deep nets so this is
really state-of-the-art performance I
mean this is very good performance this
is sort of excellent performance and
that's sort of nell state-of-the-art
performance that's performance that's
hard to achieve very few models maybe
the recurrent neural that is the only
other model we know of that achieves
that kind of performance so that's an
extremely high quality model and now
what I'm going to do is I'm going to
train a shallow model so it's like this
red curve but I'm going to train the
shallow model on the targets that come
from that really smart model so again
I'm going to take the training set throw
away all the original labels I'm going
to label the training set with that
model that ensemble and then we're going
to train a shallow model just like this
but we're going to train it to mimic
that model this is sort of remarkable so
let me show you what's happening again
you know nothing is doing very well when
the models are too small in fact this
model doesn't doesn't do very well at
all when it's small too interesting
regimes here one is in this middle area
where the deep neural net with three
hidden layers are starting to level off
this model for remember this is number
of parameters in the model this model
for a similar number of parameters is
doing just as well as the deep neural
net right so here we have a shallow
neural net with the same number of
parameters as
deep neural net doing just as well or
even a little better than that deep
neural net now it had what I would call
an easier learning problem which was it
didn't have to train on the original
training set like this model it got to
train on this synthetic data set that
came from that model it's interesting
that this model presumably makes more
hours in the training set than the
original labels did but somehow it's
still an easier to learn function and
that easy to learn function is better
learned even by a shallow model so this
is very interesting it means that for
the same number of parameters were
basically matching a deep net in this
regime if we go further and we're
willing to increase the number of
parameters in this shallow mimic model
it does even better than the deep model
even if you had given the deep model
lots of extra parameters it doesn't know
how to use them this shallow model is
able to do as well in fact as the
convolutional model and the amazing
thing is that the shallow model has no
convolution in it it really is just a
fully connected feed for jello model so
so this is just remarkable in it if you
remember anything from the talk I think
this is the slide to remember so okay so
let's just go through that a little bit
we get exactly the same results of that
was the dev site i was showing it we get
very similar results for the test set so
there's nothing there's no overfitting
happening here that's making it happen
did we get exactly the same results no
matter how we run the experiments so let
me just show you some things that we've
learned one is the fact that we're
training two squared error when we do
the mimic model means that we're much
less prone to overfitting than if we
were training to the original 01 targets
with log loss and I won't go to this any
detail because we don't have much time
but basically we found it's almost
impossible to overfit the shallow mimic
models and in fact we did everything we
could to make them learn faster as I'll
show you in a second so they're very
they're very immune to overfitting so we
don't even have to use dropout or any of
the regularization techniques that you
would normally use that's a surprise to
us I'll skip that because I've already
said it let me tell you about this speed
up trick that we had to use so this is a
shallow neural net
here's the inputs the nonlinear layer
then the output layer what we had to do
it it's so resistant to overfitting and
so slow to learn that what we had to do
was put a linear layer between the input
units and the nonlinear layer to act as
a sort of linear easily learned
bottleneck layer and we did this it
turns out it actually hurts the accuracy
of this model this model is slightly
more accurate than this model the beauty
of this model though as it trains almost
ten times faster than this model so this
is not a trick that we're doing in order
to improve accuracy we'd actually have
higher accuracy without it but we are
doing it to speed up learning it turns
out it's very important for that and
then the interesting thing is because
this is a linear layer you can just
absorb the weights of this linear layer
into the weights that are already going
to the nonlinear liar you can afterwards
get rid of this you can compile it into
the rest of the network if you want so
and I think I'll just skip and I'll skip
and then I'll show you this ensembles
trick so basically we're able to train a
teacher net of different kinds of
accuracy depending on how we train the
teacher net we could make the teacher
net be just a three layer deep neural
net we could make it be a convolutional
three layer deep neural net or we could
do an ensemble of convolutional neural
nets and every time we did that the
teacher got smarter and it turns out if
we would train a student to mimic these
models every time we made the teacher
smarter the student would get smarter as
well so we didn't see any evidence that
the student is sort of lagging behind in
its capacity or ability to learn what
has been learned by the smarter a
teacher model so so that's an
interesting result and that's why we
went to the ensemble's the reason why we
have this large gap is because we don't
have any unlabeled data in this domain
if we had 10 million or a hundred
million unlabeled examples we could
close this gap and force the student to
learn essentially the same function as
the teacher but because we don't have
the unlabeled data we're not able to do
that so the way we played the trick of
making this martyr this model very smart
was by making the teacher very smart by
making ensembles so that's one of the
tricks we had to play so it looks like
he
you squint maybe deep Nets don't really
need to be that deep that is it is
possible that a shallow model with a
similar number of parameters could
actually learn these functions that are
being learned by the deep deep models
and learn it not just represented but
actually learn it so that's really
exciting maybe that's just true for
timid though so let me just give you a
peek at some more recent results that we
have on c4 10 one of the image
recognition problems so these again our
rates and if you take a pretty shallow
neural net a two-layer shallow neural
net you get pretty high a rate this is a
10 class problem so getting an error
rate of fifty percent is not too
surprising so this is not it is a very
hard learning problem this is not a very
very good model if we take that shallow
neural in that we make it very large we
do some pre training to it and then we
add drop out as a trip for
regularization we do get some this is
basically we start doing deep learning
kinds of things we do get a dramatic
reduction in the in the error so that's
great now if we go all the way to a
full-blown convolutional model that is
three layers of convolution and max
pulling followed by three nonlinear
layers then we get a dramatic further
improvement in the accuracy of those
models and what we've been able to do is
train shallow models just like we did
before to mimic an ensemble of these
guys and depending on the size of these
shallow models were able to get either
the same accuracy as this deep
convolutional model or slightly better
than it and i should say that this
shallow model we had no convolution in
the shallow model for timid we did find
that we had to add add a single
convolutional layer when we are trying
to mimic these models so we don't have
three convolutional layers we have a
single convolutional layer but it turned
out that at least one layer of
convolution then followed by one
nonlinear layer what was important for
forgetting those kinds of results okay
so it it sort of suggests we can play
exactly the same game on a domain other
than speech recognition other than timet
this is image recognition and get very
similar results so what's a summary it
kind of works shallow models are in some
cases capable of learning similarly
accurate functions as these
eep models and in fact if we make the
shallow model very large it's capable of
learning from not any extra data the
same kind of models that we can get with
these complex convolutional models it's
the ugliest algorithm in the world right
the first thing we do is we go ahead and
we train the deep complex model we train
lots of them make it ensemble of them
pass a bunch of data throw it and then
we train a small model to mimic that
model so it's not something that we
consider to be an attractive algorithm
it's more to just give you an existence
proof that it is possible to learn
shallow functions that have this kind of
accuracy they don't really have to be
deep functions the ensemble's themselves
are great they represent a new high
watermark on on some of these problems
and then we're getting some mileage out
of using these sorts of tricks to train
let's say smaller models to mimic very
accurate much larger models so that
these smaller models can be run say on
your smartphone or on servers at real
speed so and I just want to thank some
people including reading who's in the
audience and let me just just stop there
thank you thank you
so we have time for one question I guess
we have a panel here so just why do my
goes there oh oh yeah so with your
conclusion when you say that they don't
need to be deep however you do use the
output from the thick model to train as
the alternative country to train the
shell model and in this sense you cannot
just you know conclude that you don't
need to eat they don't need to be deep
right right right now we have no we have
no idea how to train a shallow model to
be this accurate from the original
training set we have no idea how to do
that so you're correct if the only way
we know how to do that is to go through
these deep intermedium model
intermediate models collect their
predictions and then train the mimic to
mimic those deep models it's the only
way we know how to do it and Yahshua
Benji Oh makes the argument that well
maybe the inductive bias of these deep
models is just better for current
learning algorithms and our current data
sets and it's just easier for them to
learn the right function and then
somehow this compression trick makes it
sort of not too difficult for us to push
that function into a shallower model
yeah yeah but you're exactly right right
question right but but I I guess I
believe there's a at least a fifty
percent chance that there exists a
learning algorithm which could train the
shallow model from the original training
set without going through the deep
intermediate fifty percent chance hi I'm
Katella Desai University of Washington
Tacoma there's a lot of work that
happened in the early 2000s on evolving
neural networks with genetic algorithms
or you know so on so has anybody looked
at deep vs shallow neural networks and
compared them with evolve like neuro
evolution so and there's even work that
goes back further Scott almonds work on
cascade correlation and c2 nets which
also grows a sort of wide deep model
starting from something small and grows
it bigger my understanding is that
neither of these methods are capable of
duplicating what is currently being
learned by these deep
that that you have to go to the quote
new deep model technology to get these
kinds of accuracies and we're just
showing that at least in principle it's
possible for a shallower model to learn
similar functions okay last question as
we transition so in an early slide you
had this visualization of what the
layers actually represent for these new
very shallow nets do you have some kind
of visualization of what they mean what
that means it's a great question so on
the speech recognition of course it's
very difficult to do visualization so
that's one of the reasons why we're
pushing now on the sea for 10 problem in
a hope that will better understand how
the shallow model is mimicking the deep
model whether it's learning different
units that would map to different layers
of the deep model or whether it's
learning some completely different way
it's a great question it's what we're
looking at next okay
alright sorry last question so in some
sense you're doing regression in the
last stage when you're knowning a
different target function are doing what
regression yes it absolutely is
regression now suppose instead of using
a shallow network you can use deep
neural network or regression what
happened so so it's a very good question
I mean you know you'd like to think that
some of what we're doing can actually
make the deep neural Nets themselves
better and it is possible to train deep
neural Nets with regression but we don't
know any trick right now see the mimicry
process if it works incredibly well it
means that the student is exactly
equivalent to the teacher so there's not
a sort of bootstrapping method here that
would necessarily let the student sort
of jump ahead of the teacher it doesn't
mean we haven't tried and it is
occasionally you observe that the
student is actually better than the
teacher but not by much but right
right so we have tried train the deep
model the usual way then replace all the
targets with the predictions of a deep
model and then train another deep model
with regression to mimic that model and
we never achieve accuracies that are
better than the original deep deep model
yeah yeah we don't know how to do better
deep learning using these tricks yet but
it we are trying alright fascinating i'm
sure you'll get more question
interactions at the break let's thank
our speaker again our last peak of the
session is Isabel vo she's the president
of shall learn which is a nonprofit
organization which is dedicated to
organizing challenges she's also the
vice president of the unit pan
foundation and adjunct professor at New
York University her expertise and her
areas of interests are in specialized is
a statistical data analysis pattern
recognition machine learning and prior
to that Isabel vo was a researcher as
AT&amp;amp;T Bell Labs and she pioneered some
applications in the area of neural
networks and is one of the co-inventors
of svm Isabel Thank You Evelyn so thank
you very much for inviting me and giving
me this opportunity today to talk to you
about some of the results of the
challenges that we've been organizing so
after doing research in machine learning
for a number of years i started doing
another way of performing research
crowdsourcing research that instead of
preferring research myself I put out
problems and I let other smart people
solve them so I found that an amazing
way of contributing to making machine
learning solve hard problems out there
in data science and today I would like
to share with you the results of which
are organized in a causal discovery
after organizing many challenges in
machine learning and particularly in
causality
most of which delivering either non
conclusive results or negative results
we finally in 2013 organized the
challenge with delivered amazing results
I'm very excited to share that with you
today and we keep organizing challenges
one of them I won't be talking about
that I will be talking about today this
challenge in cause effect pairs which
will run or remake this year on the
Microsoft colada platform which is a
platform that has been developed in
collaboration with Microsoft's in
stanford university that Evelyn has been
the head of the development and thanks
to her we've been able to make even
bigger difference in this field I would
like first to acknowledge the many
people who contributed that effort and
starting with the people who give the
impulse to the project by pointing out
the problem to my attention some people
at the Max Planck Institute your
response Dominic jansing and Bernard
Shaw cup and many people contributed the
early algorithms on the problem and who
made their code available and that was a
great stimulation to the challenge
participants and I would like to thank
also very much the people who helped
collecting data and putting out the
problem on the website and the people
who tested the protocol of the challenge
and finally I would like to thank also
the sponsors who made this research
possible and particularly Microsoft who
has been a very supportive of our
challenges over the year and you are
continuing to support effort another key
type of actors in it in challenges are
the participants themselves and we have
hundreds of people who are donating
their time to solve these hard problems
and they've been truly amazing because
the prices are only a tiny little
motivation they are looking to solve
interesting new problems to learn some
people are just working during the
daytime and spending some time at
trying to solve these hot problems so
what affects your health the economy
climate changes etc and which actions
will have beneficial effects there are
some of the central questions that
affect or well-being and our happiness
and wouldn't be wouldn't be nice you
know if we had a means of mining
existing data sets and finding potential
answer to those questions the scientific
method proceeds as observing first
correlations between pairs of variables
then hypothesizing causal relationships
that equals V or because a or is our a
and B the consequence of a comment
clause and then perform experiments to
put these hypotheses to a test however
experiments are generally costly they
can be unethical they can be impractical
or even infeasible so it's really
important to prioritize them that is to
find those candidate code effect pairs
that are that have the highest priority
for us to experiment with you keep
hearing during the news all sorts of
causal claims so this is these are
result that I've been extracted from a
recent paper published in the new
england journal of medicine that found
high correlation between chocolate
consumption and the number of nobel
prize for capital now shorty thereafter
on the internet there were some claims
that eating chocolate produce novel
Prize winners but if you look a little
bit longer on the internet if you find
other claims like geniuses are more
likely to eat lots of chocolate
obviously either claim could be correct
and the correlation doesn't mean
zation but our objective is to take lots
of such correlation found in big data
but go beyond the simple idea of
correlation and rank those pairs of
variables in order of a Ledge causal
relationship having at the top those
causal relationships that are most
likely to be valid and then putting them
to a test like we did before and finally
take action so the second step is what
we are focusing on formerly the program
consists in taking pairs of variable so
say a is the chocolate consumption and B
is the number of normal prices per
capita and we get samples from these
variables so this may be amazing to you
that we are talking of the causality and
we are throwing away the time component
but think of that as a cross-sectional
study in in medical in the medical
domain where are you taking a population
of patients and you're measuring for
example whether patients are smoking or
not and whether patients develop lung
cancer is so in a cross-sectional study
you're not taking time into account so
each sample here in the case of
chocolate consumption and number of
normal price per capita was a country in
the case of a medical study each point
here would be a patient and we can plot
each data point in in a scatter plot and
the question is whether looking at these
cards you can determine whether a is a
function of B be a function of a or
neither and it is in that sense that we
define causality is one variable a
function of the other variable and we
formulated the problem as a
classification problem one class is the
a caused B so B is a function of a and
maybe some other variable and the other
class is everything else so and we are
independent and B are caused by a third
variable that is the latent variable
unknown to us or be the cause of a and
because of the symmetry of the problem
of course you could consider separating
because it's a from everything else so
in the end what we ask the participants
to do is to create what we call a
coefficient of causation a coefficient
which is large positive for a Cosby
small negative 4 because it's a and near
zero for all the other cases and using
that coefficient of position you can set
the threshold on that coefficient and
then classify this the scatter plots in
a cosy vs everything else or if you set
a different threshold you can specify
because as a versus everything else so
by bringing that back to classification
problems we can use the usual metrics
like area under the ROC curve to measure
accuracy of classification and Q will
actually average to a ucs one for the
problem of a causes B versus everything
else and one for the problem of because
is a versus everything else so this
course I will be reporting in the rest
of my talk are always in a you see so
these are scores that are for the best
values is one and random guesses is 0.5
so one solution actually said this
solution one solution to the problem is
to put it out in a form of a challenge
and to have a lot of participants try to
solve this and on that graph I'm showing
you that moving from random gas to using
you know plane correlation or even
fancier independence test doesn't bring
you a lot of performance improvements
the baseline methods before we start the
challenge were around 0.6 and during the
child we've got these big leap in
performance torrez 67 participants
entered and at the end of the challenge
we had performance
above 0.8 and that is truly amazing at
least two other the first time in all
these years we've been organizing
challenges that we had a really really
really big improvement compared to
baseline methods and I'm glad that's a
rich Quran I gave its own before because
I feel less embarrassed to present a
method that is a very heuristic and
that's kind of a dirty solution the
participants came up you know with this
big leap in performance but their
solutions kind of ugly but we are we
going to we going to see that in the
rest of the talk the data are quite
complicated and challenging there are
all these scatter plots and we included
binary variables included categorical
variables and continuous variables and
so each of these milk our blood is one
of the problems and you have to say that
equals B because a or neither the human
eye doesn't do very well on this problem
although you know if you really really
concentrate and after ten minutes you
gets extremely tired if you have only
continuous variables you can kind of
guess and you get a performance baby
0.65 if you if you're really good at
this and the data set was created from
both real data where you had tears of
variables for which with human expertise
we could determine with some degree of
confidence that one was the cause of the
other for example age and wages CT
elevation temperature or in genomics a
transcription factor that triggers
protein induced them an addition to that
we had lots of artificially generated
data that were created by taking real
variables by putting them through known
functions so we had truth value of the
causal relationship so we had twenty
percent of the real data and eighty
percent of the artificial data and here
are the results so this course are in in
a you see and you see that performances
are better for the artificial data
all data which probably are very noisy
because it's human judgment on and with
very tedious to label the causal
variables and then we broke up the score
into a dependency score which is
separating the independent the pairs of
independent variables from everything
else and for those we have pretty good
results so it's easy to determine
independence a confounding score which
in which we separate the tears that are
generated that are actually dependent
but there is no causal relation between
a and B there is a third common cause to
alien be and if you separate that from
everything else you get relatively poor
scores and then the causality score
where you separate a cosy be from
because as a so graphically we can
represent that and show here the top
ranking participants so each point here
is one of the participants and we're
showing the score on real data versus
their score on artificial data and we
showing the squizzle it is score the
confounding score on the dependency
score so we are doing better at simple
dependency so these are the higher right
corner point the blue points the harder
is the black spots the confounding score
and the quasi were kind of in the bit in
between but the very interesting thing
are all the lil crosses all the little
crosses will present the results that
you obtained by random guessing so by
taking by permitting randomly the target
values and see whether we can make a
prediction for for random guessing and
you see that the predictions that are
obtained are very significantly better
than chance them so how how do we do
that now in the rest of my god I'm
trying to give you an intuition how you
know you proceeded to build causation
coefficient and why the participants
improved so much upon the state of the
art so everybody knows about correlation
and we really knows that you know
correlation doesn't mean
and also that you can have some
variables that are dependent and have
zero correlation like at the bottom so
the zero correlation problem for
dependent variables can be solved by
independence tests and if you do that
you move from you know very poor
performance to slightly better using
tests involving for example mutual
information or the Hilbert Schmidt
independence criterion but that doesn't
tell you how from the scatter plots you
could determine causation so he is one
of the little artificial examples that I
bought from Wikipedia here the input is
a uniform distribution the noise is also
uniform distribution and B is a function
of a plus noise so you can quite easily
determine that these variables are
dependent even though they have zero
correlation using mutual information and
this HD test them here high between
information indicates dependence and a
low value of I which is the p value of
the independence test means also strong
dependence now in order to determine the
cause of direction what people did prior
to the beginning of the challenge is
that they did fit into the action so you
can fit a as a function of B or B as a
function of a and if you have a good fit
in one direction in a profit in the
other then you say okay I think I found
the correct causal direction and the
second type of feature people very often
used is to look at the residual so the
residual here is represented by the
error bars of the feta and if the
residual is relatively constant
throughout the curve then it means that
you probably have the right cause of
direction because the residual is an
estimate of your noise and the noise is
essentially another latent variable that
you haven't measured and so the
noise is probably going to be constant
if you are in the right course of
direction so people use these two
criteria the the goodness of fit and
then the independence between the input
and the residual so if the residuals
constant then it's a consider to be a
clue of the right cause of action now
unfortunately these methods don't work
always because you have cases are
completely symmetric like the linear
case plus Gaussian noise where you could
generate the exact same scatter plot
with a as a function of BB as a function
of a or a and B generated by a third
variable so this is a so-called
non-identifiable case the good news
though is that if you break one of these
conditions so if you go to nonlinear
model non-gaussian input or non gaussian
noise then you break the symmetry and
this is an example of a linear linearly
generated example but with known in with
the uniform distribution for a and
uniform distribution for the noise and
in that case if you go through the same
exercises what I did before you see that
the fit is not the same in both
direction in the correct direction then
you have a residual that is constant
whereas in the bad direction you have a
residual that varies and therefore is
the independence test fails so there is
no independence between the residual and
the input so this is how you could
detect the right course of action so
this is where we were standing when we
started this challenge and after a few
months of work of the participants we
were there how did this happen so
typical research methods worked as
follows people restricted the the domain
of all possible cases to a subset of
cases for example they considered own
continuous variables only non Gaussian
noise only non linear cases with no
noise there's cases like that and then
they studied a particular model for
example the additive noise model the
post nonlinear model Gaussian processes
and finally they came up with a test and
then they studied the identifiability
the consistency etcetera etcetera but
each paper that was published focused on
just one restrictive aspect and one
particular model and came up with one
test what the participants did is that
they consider the problem of the pattern
recognition problem each gara plot is
like a little image and how about you
know extracting features from that image
and then once you have lots of features
apply a machine learning algorithms such
as some of the decision trees random
fries or great entry boosting and coming
up with a you know big learning machine
that's where I said you know the
solution is kind of ugly right because
it's much less easy to understand now
you have you know a huge number of trees
you know hundreds of trees that are
basing their decision on a very large
number of features and the feature is
however they borrowed from all these
papers that were published in the
literature so they consisted in
independence dash curve fitting results
and other other features derived from
the conditional distribution information
theoretic features and some data
statistics so what I did is that I went
back to this big forest that were built
by the participants and I tried to
extract one simple kind of consensus
tree using the most informative features
and with the purpose of trying to
understand how these systems work so
using the results provided by the winner
of the second phase and who also a
second in the first phase I came up with
that tree and at the top there was a
feature called
conditional distribution similarity that
I had never heard of before and after
that were differences in marginal
entropy and maximum of the marginal
entropy so I went back to the paper and
try to figure out what is this
conditional distribution similarity what
the authors did is that they conditioned
on one variable after discretizing the
list of the variable so they kind of
Bend you know the variable and then they
looked at the conditional distribution
and observe whether the shape of the
conditional distribution was constant
while sweeping along the introduction so
this is kind of a generalization of this
idea i mentioned before to look at
whether the residual is constant so it's
trying to infer something about the
noise that is something about this
latent variable that we don't know of
and we want to know whether in one
direction the distribution the
conditional distribution that is
essentially the distribution of the
latent variable is more constant than in
the other direction and that would be
indicating that we found the correct
direction so this actually is a true
example of two variables so the a is the
aspect which is the orientation of a
slope in a hill and b is the hill shade
at 3pm and so we can play the same
analysis as before and we found you
there is a strong dependency between the
variables but when we do the fit it
actually doesn't work well amazingly
enough if you do a fit the fit in the
wrong direction as the smallest residual
and in both directions you have a very
strong dependency between the residual
and the input variable the reason is
that you don't have an additive noise
model here you have actually a noise
which is pretty multiplicative or you
know some some more complex type of
noise combination so the idea of noise
model doesn't work how r this idea
of using the latent variable and the
Independence of the distribution of the
independent variable with the input work
still if instead of just using an
additive noise model you use the
conditional distribution and this is
what I'm representing on this slide the
plot in the lower left corner here are
representing the conditional
distribution and this CD s feature that
was computed by the participants who
provided this this really performed
model have an amazing difference in
value here for this particular case and
allow us to discriminate between the two
directions when in fact there the
additive noise model was failing so what
happens is that you can buy conditioning
on variable a you can find that the
distribution here is in fact
self-similar all across all across the
spectrum in once you have rescale with
respect to the to the span of the
distribution alright so that's just to
give you a flavor of what the
participants have been doing they have
been creating you know hundreds and for
the actually top ranking participant
West thousands of features automatically
generated there is much to do now to
keep trying to understand what these
features do but the amazing thing is
that challenges can help making big leap
in in data science and they provide
solutions that open up entirely you know
newer fields of research now that you
know the the new baseline is so high the
entire research community can set up to
work on this problem and try to
understand it better and analyze the
features the I theoretical results and
tests on other data so next challenges
a dress code 18 time series and entire
network reconstructions but we also have
a lot of other types of challenges in
the pipeline and we also invite you to
come and join the community of challenge
organizers and put up your own
challenges on the corolla platform
everything is actually making a demo
just after this session you follow us
and to show you how you can post your
own problems and have lots of
participants will contribute and make
big big leaps forward thank you for your
attention Thank You Isabelle</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>