<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Semi-Supervised Learning for Acoustic and Prosodic Modeling in Speech Recognition | Coder Coacher - Coaching Coders</title><meta content="Semi-Supervised Learning for Acoustic and Prosodic Modeling in Speech Recognition - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Semi-Supervised Learning for Acoustic and Prosodic Modeling in Speech Recognition</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gsJGDSUvC6E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's my pleasure to introduce r18
playing today rating is graduating from
the University of Illinois at
urbana-champaign where she's one of Mark
Hasegawa Johnson students she was an
intern here in 2009 and did some very
interesting work with Holly at that time
basically they did language model
training in order to maximize the mutual
information between the acoustics and
the language model when all you have to
start with is the language model itself
and a phone confusion matrix from then
she she moved on to work on her thesis
work which she's going to be talking
about today and that focused on mixed
supervised and unsupervised training or
semi-supervised learning and she'll tell
us about that now so today today I will
talk about my thesis research
semi-supervised learning for a cookie
and parsley modeling in speech so I will
first give a brief introduction of beef
brief for take one semi service learning
then I will present two kinds of
problems where I investigate the use of
unlabeled data by some proposed
semi-supervised learning algorithms
followed by some conclusions so we know
that the scale of audio data sorry
actually there are a synchrony here
so it's the talk outline and yeah so the
skill of audio data that we can collect
is increasing dramatically these days it
turns out that speech data without
transcriptions is easy to collect but
time-consuming to transcribe this
motivates the research and
semi-supervised learning which tries to
use unlabeled data in addition to a
limited amount of label data to improve
the supervised model so the ultimate
goal here is to improve speech
applications where recognition
performance was limited by the amount of
transcribed data and there are a few
related methods proposed for speech
applications most most related approach
can be categorized into self training
methods which we conventionally called
supervised or some sounds that the
supervised training which can come with
confidence to hold meaning that what it
does is that for an transcribe speech we
run in existing recognizer and on this
set of on transcript speech and document
the training set with confident enough
transcription automatic transcriptions
and retrain the model while it has shown
the demonstrate the potential use
demonstrated the power of own transcript
data to improve the system there is no
systematic way there are some drawbacks
for example there are no systematic way
to determine the threshold of competence
measure and on the other hand there is
no theoretical foundation about the
convergence property so in my work I'm
taking another perspective trying to
propose a principle framework that we
can train our models by some
reasonable training objectives
considering the influence from post
labeled and unlabeled data so let me
give a formal definition of semi tsubasa
Danny
meaning that we know that in machine
learning the classification problem is
given the data described described by
some signals some features we want to
map it into one of the predefined class
categories which is called supervised
learning because we would need a
training set of label tokens and then we
learn from we learn a mapping from them
so semi service-learning meaning that in
addition of those table tokens we are
also given a set of only potata to learn
the mapping so in our framework the
models are the models I'm looking at are
either Gaussian machine models or hidden
Markov models or by learning I mean the
estimation of generation parameters so
the basic idea is to use unlabeled data
to generate the applicability of
classifier for bouncing data there is
impressed prerequisite though which is
we assume the distribution of unlabeled
examples is relevant for the
classification problem so in in this way
we came that because if we have large
quantity of unlabeled data we can
leverage the data distribution from the
unlabeled data and connected with the
Colet classifier and there are different
perspectives that we can use to make of
use of this connection why is that the
first kind is kind of treat unlabeled
data in the context of missing data
problem meaning that the the unlabeled
it has kind of like incomplete data
misses class label information
and uh and this incomplete data can be
one way to treat it is the EML given
another con is we trying to impose some
measure of unlabeled data to regularize
the the overfitting problem cost by
using label data only so in the
following i will my methods is basically
based on these two perspective one of
these two perspectives are you see for
you measure the relevance
using of our makeovers and pose is
called magic
yes there is no con counter
quantification on this relevance by
relevant I mean that for example if we
treat the assign say if we treat the
missing beta if we have a generative
model then the unlabeled data is
relevant here because generative
training is going to find the better
likelihood to describe your data then in
that case its nature to incorporate
unlabeled data into your generative
training and if you have a better
generative classifier generative model
and use which you are using for
classification problem then they are
relevant yeah so and also by there are
another relevance for example here if we
think the if we think the distribution
of the unlabeled data can change our
thinking about can change our thinking
about the the the matter we learned from
only labeled data which I will show
later then it's also relevant yeah so I
think the the point I raised here is
that it's known as it's not it's not
necessary that unlabeled I can help you
need to have correct context correct
model to use it
so it's so in particular I proposed a
training framework where speech models
are trained to up optimize an objective
that reflects reasonable assumptions
about post label and unlabeled data and
in particular there are two kinds of
training criteria the first kind is
generative training criteria and then we
will find that its ability to discover
unseen class and we apply this a bit
ability to the problem of porosity brake
detection and then I go to I hope go
into the acoustic modeling program and
the proposer semi service maximum
likelihood criterion for GM's of atrium
trending and then the other kinds of
training criterion is regulated
discriminative training criteria meaning
that the original discriminative
criteria will be augmented with certain
measure on unlabeled data regularization
and we were applied to acoustic modeling
so the first part I will I will show how
we use generative model to deal with
detection paucity in Mandarin speech so
part of the let me briefly introduce
varsity varsity are those variations in
pitch darkness tempo and reasoning human
speech to convey extra information in
speech communication there are many
positive events of course many languages
and here I'm interested about positive
break or positive boundary first
boundary meaning that instead of speak
and utterance inflate inflate temper
we usually well tend to pause in several
locations within an utterance and the
boundaries of win/loss the band
boundaries between natural group of the
words in speech is partially break here
is an example in Mandarin
speech where each symbol here represents
a syllable corresponding to a Chinese
character so the then you know the now
playlists male speaker and you will find
that he instead of his speak it in a
fred temple he will try to slow down
slightly in some place as i will
indicate by a blue line Nadia what alien
Lily Aldrin you HOT your horse again pdn
so so those blue lines is the part where
the past prosthetic breaks are then
locating those locating those parts of
great natural group and finding the nose
locating those party breaks in speech is
useful for finding natural group for
speech synthesis and also it appears
that the the segmentation cutting to
paucity corresponding to syntactic
structure in speech and which can be
used for speech understanding
furthermore you we could also have build
prosody dependent acoustic modeling
meaning that the acoustic model will
change depending on the positive event
currently so there since it's useful we
are interested in the automatic the type
of automatic positive break detection
which is essentially a classifier which
receives acoustic correlates and
clarifies the event as number ik a break
and because it's a classification
problem supervised learning problem and
the we which will require the positive
labeled data we will need a paucity of
notation and it's usually done by the
inquisitive experts on the who are
sensitive to those those acoustic use
and then
and can be efficient to mark those
prosody marks for coppers
so because it's it's even more harder
than the word transcription which
motivates our goal which is to
automatically locate parts of the break
in men during speech without any
particular label data it can be done
because because of to two things
one thing is that we will first identify
reliable representative from the number
class using some simple textual let's
cook useful Mandarin then given a set of
a small set of label data we rely on
some of same asuras algorithm to learn a
classifier using both stable and the
rest of the unlabeled said and it's
worth noting here that here we have a
special setting of the problem where the
label data can is only available from
only one of the two classes and we will
see that we are able to take care of
this scenario in the fund and the unseen
class the break class by our algorithm
so here is an example how it works in in
general meaning that if we have if we
have utterances then a speech recognizer
will generalize text recognize text for
us in here it's a character traffic
sequence of trans characters then for
every syllable boundary here
corresponding to each character is where
we need to decide if there is a porosity
break on that and according to some rule
which I will explain later
we are able to identify some
class representative for the number-8
class and I indicate as MB here and we
collect this data as the label set the
rest then the rest of the silver
boundary with unknown a class identity
go to the unlabeled set then our semi
service learning will use this to set to
estimate a passport underlying model and
predict the most likely prosody for
those unable boundaries or even ahead of
a set so let me let me show us the rule
is to find the class representative we
leverage some information from the text
which is output from the recognizer and
before showing the rule let me introduce
the concept of less code word in Chinese
meaning that Bianca beyond a single
character the lexical world is actually
the combination of multiple characters
which can convey different less
convenient than its character components
and it's renewable for a recognizer to
output a word sequence rather than a
character sequence for example you can
have a word based language models then
the let's call Q we are relying on is a
literature search study that preceded
products do not exist within a shortest
code word by short we mean that the the
lexical world contains less than three
or equal to three syllables then so we
see that according to this Q for those
within word syllable boundaries we can
we are pretty sure they are known from
number class so we mark them as MP
that's how we found the class
representative and the rest go to the
unlabeled data then how do we the
from this mixed set first let me show
the generative model we are using and I
show how to retrain the model so we are
using a mixture model type mitra models
that has initial components that can
generate data X Y I sub n X is the first
multi dimensional parts of the features
contain containing and acoustic
correlates Y is the class information
and I sub n is an indicator of level is
missing or not so then our the joint
probability of the data is described in
this mixture based model where we have
weight in the gaussian gaussian for each
component but because it's at admixture
to classes meaningless to classes shared
a sample of gaussian components so the
class discrimination comes from the soft
class membership defined by this
component dependent clause probability
also one important feature here is that
we have crossed dependent label missing
probability to match closely to the one
class scenario meaning that if we have
known that from the training data we
don't have label information from the
break class we simply apply the
constraint that the the the minute level
missing probability for the break class
is 1 and then we learn the rest of the
parameters so because it's a generative
model its nature to have makes my
likelihood changing criterion to
estimate you have all of your parameters
and the low the likelihood is computed
by by the by the combination of label
set in the only
said and that it can be solved by en
algorithm then the classification is is
simply finding that the the one that the
class that maximized the class posterior
probability which can be computed in in
in the following way using the
parameters we just learned so which we
show our if we evaluate our unsupervised
approaching an inner-tube
you know duration corpus which is a rest
page by a single male meandering speaker
and in this capacity of notations are
available but we just use it for our own
choice for evaluation and there are two
kinds of break labels two labels break
labels but we treat them as the same
break class then feature regarding
features for each syllable boundaries we
extract eight acoustic features
including pitch duration and energy
related features yeah so here
yes it's just one speaker because these
coppers just contain one speaker it make
difference because then we will need to
take care of speaker normalization
regarding the the prosody features and
the it could be equal beam it could be
eliminate eliminated to attach them to
Saxton but now it's a simpler situation
I think you have to tell whether there
is some silence research
yeah sure so I yes I totally totally
agree with you and so one thing is that
we observe this thing we also observe
this this fact however the number of
silence that cues the number of
locations that can be identified by
silence very account vary depending on
the corpus it's not necessary to have a
sufficient number of sufficient number
of syllable boundaries marked by
silences depending on the corpus so we
think identified class representative
from number ik class is more general
than finding the boundary by silence but
it's possible it's possible to add in
those feet those data's recalled by
silence and then then now then we are
back to a normal setting on semi
service-learning where the label said
have information about two classes yes
actually I have that result but I didn't
show here for complete comparison
classification so it's a classification
you know the word and you classify
yes in this case it's a prosody kind of
positive event classifier so I know the
city of boundary so meaning that yes
they are all I assume that I gather
information from the recognizer output
more complete modeling you may need also
you know doing our sequence yes and also
maybe using immutability word specific
boundary information is there too
yes so I so I will show that for
acoustic modeling this secret sequence
model but here for party modeling I
didn't consider I haven't considered the
sequence scenario yet
so in the experiments and our
transcription comes from the ground
shoes
yes and another thing is another thing
is it's maybe you can discuss it later
so okay so here we here we I want to
show the demonstrate the models ability
to discovery new class by using one of
our features saying example so the
feature here is the difference of
average energy of the syllable after and
before a boundary a syllable boundary
the cut most plot is that the histogram
of the features in the training set
regardless of the class information the
length to plot our respective histograms
for this one's for num break class this
one for break class we see that the
break class has a lower mean than the
nominal break class then we show the
estimated data distribution given only a
subset of labeled token from nombre
class with different number of gaussian
components and we see that if we have
sufficient number of gaussian component
like system gaussian components look we
are able to identify the he this is the
joint probability are computed using a
model and you will see that given
sufficient parameters we are able to
identify the
the the contrastive class that we we
haven't seen they both said before so
let's let's look at the detection
accuracy result so let me show to
reference numbers first by fully
annotated approach I mean that assume
all the labels is fuzzy labor available
so this is kind of the upper bound
performance we can get 91% and then by
translate I mean because the school
class distribution of the data if you
simply assign every syllable boundary
osnabrück we get 77 accuracy but but
it's it's not a reasonable way with our
method according to a precision and a
recall then by using the non break data
in the label token we are able to given
a reasonable accuracy but it's more but
it's more meaningful to see that we have
reasonable precision that we call
summarised by F score here so show some
range that we are able to discover
procedure X by a tester Q test oq and
the semi summarize algorithm
now we now move to the problem of
acoustic modeling and we will start with
generative approach that has shown its
effectiveness in reciting model so in
the context of cosy modeling with the
generative criterion such as makes my
likelihood criterion and label data a
special and it's that it's all GM's or
assurance our generative model given
this context unlabeled data can be
naturally incorporated into generative
framework that is we extend the semi
civilized version of ML criteria in
particular the model parameters will now
to aim to maximize the likelihood of the
joint label and own label set and it can
be done by en or language for GM's of
GM's there are some issues here first
raised by by Kohan that is incorrect
model assumptions can cause unlabeled
data to degrade performances but in
their paper they assume like they assume
one Gaussian distribution per class in
our case we using a more expressive
probability model which is GM and it's
already a better model assumptions than
a single Gaussian distribution and we
don't observe that equating effect so
far and also it's well known that
generative model may not up Mies the
classification accuracy well and as
people usually go on to do
discriminative training we will see how
do we extend the supervised
discriminative training to
semi-supervised version so so
the diss current track criterion is
simply if we are using maximum mutual
information it's a finding the Maxima of
the average log posterior probability
and since discrimination is prone to
overfitting the training data it will be
helpful to include an regularization
term to reduce the problem of
overfitting then our semi service
equation are essentially using certain
measures on unlabeled data as
realization to the supervised training
criteria therefore we propose to there
are two organization measure the first
one we propose to augment the supervised
in our criterion which makes my elect
who measure our own label said mmm it's
so interpret in the interpretation here
is its balance between average evidence
from the label data and also clustering
information from the whole input data
so isn't there a prior on the label
sequence a prior on weii that needs to
be in there somewhere
if you look at the mutual information
with probability of x and y over the
probability of x by itself in this
probability of y by itself and then use
a chain rule and you have the
probability of X and probability Y given
X and then P of X cancels in the
numerator and the denominator so you've
got T of Y given X over the field wine I
think I don't follow you so than the
expansion but maybe this is like what is
the meaning the expansion is fine
tequila and why
what happens to it why isn't that the
Phenom they doesn't matter
yeah it's if you look for discriminant
training via wires so yeah so here it's
a here is Falak it's the classification
version of the criterion and yeah I
think as you said there is a denominator
there is a P of Y term but
there's anything you couldn't Senate
give that student light of the pay apply
yeah I think yeah the fact that I have
to say that the so I will talk I will
look at the classification case and the
recognition case and this is the this is
the the measure that I use for the
classification trending and I think it's
June
it's generalized to the and I the mi
criterion for the recognition this one
is actually conditional makes my
likelihood for class of classifier we
usually call it that and I'm just saying
that they are equivalent if you
generalize it to make manual information
for the recognition case
so so I first talk about the hybrid
training objective then because if you
look more closely at the automation
problem the objective is actually
composed of multiple terms of likelihood
and the corresponding parameters per
meter updated formula can be derived
based on a modification of course
transform or extending powers so
essentially it's a modification of
original and I training by aiding a
additional statistics from the unlabeled
data like this way and the L and the U
is just analyzed normal normalization
due to the amount of the training data
effects the balancing factor so this is
the version for classification where
each model is represented by Gaussian
model so this is the mean vector for
Class E and the M component M so I want
to discuss first a relation to other
work so there have been some several
techniques have been developed to
mitigate the overfitting problem for
example the criterion globally
interpolates MI and the ML objective and
also as smoothing is used to interpolate
with Gaussian component dependant priors
based on the ML estimate our criterion
is different in that we leverage
information from unlabeled data and
there are different interesting
perspective proposed by IBM researchers
in this year I guess this year the
proposal makes my entropy major on
unlabeled data to oakum comment which
the supervise an item which essentially
results in
- turn here - nah here flip Allah sign
here for the unlevel major I so I
haven't I
they have slight improvement but they
combined that method with another
paradigm which is multi view learning by
combination many systems so I mean it's
it's interesting to try why I will get
if I flip this time in words correct me
if I'm making a mistake today I think in
words with what the first one means is
that you want to maximize the likelihood
it's a supervised data and make sure
that you've got a hypothesis data that
gets high likelihood
and what the second one does with the -
is you want to maximize the likelihood
of the of the supervised data while
saying as little as possible about the
unsupervised a yes and I think if you
have very strong supervised model maybe
it's reasonable I mean maybe you can try
to use unable to penalize a serpent on
the likelihood and competing hypothesis
a penalisation but here you will see
that in my experiments I assume we have
limited amount of label that I mean that
my supervisor model is it's pure in the
first place it is poor in the first
place and the in that case we will I
rely on the unlabeled data to to capture
the data distribution then in that case
the hypothesis regardless of the it
might be some errors in the hypothesis
recognized by recognizer but in that
case in this context where I don't have
good enough distribution model in the
beginning maximum likelihood
regularization my help
but here you for the use that - why that
mean service they don't want to have any
impact astronomers are way over there
huh
so what's the new sea-doo open are they
were there now we need the tool terrible
the Watson ahead to the likelihood of
harm ever again I misread although I
make it submitted yeah doing the - seems
bizarre he there I'm really wondering
why - because the man should be that
would have been appear there would have
been liable to me
he is peel over here the apartment by
the balcony : there's no team look data
it's just pee like people maybe Union I
was thinking about people assuming that
was at the root of town it's just like
this right it's a lot of life there's no
class information here
and yeah but to compute P of X you have
to hypothesize class information
because your class based models yeah you
kind of you have to you're gonna have to
break that down into somehow a sum over
all the possible facts label
actually I I have actually I yeah I I
used as you say that to approximately
the usually you rely on the prior
information here yeah so I think Thank
You Louis anything I use a language
Amado and their acoustic model to
general to a passing memories yeah so
here in this equation
the first line is the unlabeled data if
you find some data that gives any of few
P of theta
you know effects very low probability
that your maximum maximization will get
hugely penalized so you would not allow
you know those densities and gives very
low power P
there's no like
does that help explain why it helps my
babies so you try to make the you know
so every X I make them almost equal
thank you
that kind of smoothie toward average
I'm wondering if there's some kind of
Incan something consistency like if you
have a pile of label data and no
unlabeled data you use of you maximize
the probability of the label paper and
now imagine that you chopping your label
they didn't half and half of it into the
unlabeled data and throw away the labels
it's the same thing that same data it
used to be trying to prove its
probability Allison you're trying to
decrease its probability
it seemed kind of inconsistent and so
they so yes yeah so that's interesting
and I think they they also think it's
kind of corresponding to La Mesa entropy
idea so I think it's worse discussions
I already talked about this okay so the
second regulation I have is conditional
entropy regularization which is the
identity of class prediction different
features the paper first proposed this
idea is based on the assumption that
unlabeled data are beneficial especially
when classes are well separated then you
are trying if you have a model prior
that creates also late when they
estimate a model they apply of model
prior that prefers minimal class overlap
which is equivalent of minimizing label
entropy unlabeled data the
interpretation in other interpretations
that are the negative conditional
entropy term encouraged the model to
have the greatest possible center T
about it's leveling decisions so it
reinforces the confidence of the
supervised class classifier we learned
from the labels so to
computer-to-computer the conditional
entropy because we don't know the real
data distribution we approximate it with
the empirical distribution from the
unlabeled data so the formula is
essentially like this and so meaning
that we have for discriminative
criterion for label data and the
negative conditional entropy for
unlabeled data and it's in a sense
unlabeled version of this creative
criterion so we have coherent Li this
creative objective then we cannot use
because of this term we can now use the
extending powers so we optimize it by
gradient based methods based on the
quality and computation of this term
more we use precondition
conjugate gradient method to accelerate
the convergence rate of steepest descent
so this I'm just showing that gradient
of overall is the gradient of these two
which looks like this in the
classification case but I'm going to
forget the question so can ask it at the
end you're probably going to compare
results using different techniques and
one is and one of those techniques is
done not the optimization but one of the
techniques so maybe it's just plain ml
which presumably you've used p.m. to
optimize
then there's going to be this other
technique where you use the precondition
conjugate gradient descent and optimize
and there's going to be a difference
between those two techniques and so the
question is how do you know that
difference comes from the objective
function versus the optimization method
like if you didn't if you did
precondition conjugate gradient for the
regular maximum likelihood objective
function maybe that would give a
different answer
I see that okay is there some reason you
can tell me how you don't have to worry
about that I think as long as I'm sure
the objective functions are not
optimized so well then yeah yeah I
though then just getting local optima
and I think it's more likely to put you
into a sea watching me voice so for
example for for this method I have for
this method I have I plot a training
objective and then it looks reasonably
saturate after several iterations I
haven't tried to do the same thing for
the EML reason but usually I mean and as
soon as it's actually he also have the
it actually converges very fast aright
but I cannot comment on if the
precondition conjugate gradient is
better than finding I think the nature
of the others and finding local Maxima
not now issue are finding better local
maxima and so I guess they are fair - so
it's fair to compare them I think that
because the way I used precondition
Annika Guardia is to speed up the speed
up the training I didn't have I didn't
I'm not aware of any optimization
probability of it that can find can help
to find a better local Optima so message
maybe just a commercial cinema as well
you know somewhat advanced the resident
would jump out and start local minimum
and then give some
otherwise your religious kind of same
caste
I disagree okay because this is level
reason why people have like inertia and
they're cut in their gradient descent
techniques something you know over these
little bit of them it really depends on
how you follow the gradient where you're
gonna end up I seen before time if you
look at a different feel like a old mess
yeah people put a huge amount of effort
and exactly how they're gonna optimize
these things and whether learning rate
is and whether they precondition the
dumb precondition or what the starting
point in it is and then this any it's
not the point
yeah anything even though the data is
the same and the objective function is
the same the details of the optimization
at least in that case not seem to make a
difference
look how much better here we have a
fairly good week instead think about
whether position one things that are
pitiful Amaya compared with gradient
descent method and extended from Welsh
method they give almost the same number
oh I I mean in my I did that so I mean
without the negative we saw that turn if
I do that with gradient descent method I
have the similar I have the same result
with the one that are optimized used to
extend the power well yeah I don't like
that yeah for my astounded it's similar
because extended baggage is an extension
of power so probably it's provide some
proof evidence a part of the
organization doesn't matter much here
so I'm just saying that uh yeah so it's
deep it's a negative conditional entropy
it's different from the previous
interpolation term and the and then the
negative conditional entropy has been
applied to other discriminative
classifiers such as logistic regression
and the CRF and here we apply a
regulation to discriminate training of
gm's and ATMs so I look at the two
specific test wines classification the
earliest recognition classification is
like poly I don't need to explain
whether it's Angie I'm assuming I
I already given the second I just want
to recognize the phone identity by the
by the by the posterior posterior
probability and which has a components
that is a phone dependent distribution
described by Gaussian mutual models then
I used Timmy coppers and to create a
semi tourists setting labels of certain
potential teacher of training set are
kept and the rest of unlabeled set
becomes the unlabeled set then I
extracted segmental features for each
segment which is a fixed inspector
calculated based on the pupae Fisher's
PVA features from within the different
regions within the forum segment and
also pre and post the segment and the
plus the log duration so this is the I'm
showing that it has a so I think that
Lego is not clear but the blue line is
objective value and we have saturated
it saturated in 15 iterations and the
coin lines the phone accuracy I
calculated on the deliverance set so you
see it correlates well it's not exactly
a cent because it's not trending said
accuracy it's kind of testing accuracy
but still regulation helped us to have a
nice correlation between accuracy and
the objective then here we show the
classification accuracy for two
regulators
why is my it's my organizer the other is
conditional entropy the x-axis is the
decreasing amount of label data with
increasing amount unlabeled data
therefore the best time performance we
can see which is the dashed line here
the quizzes are along the axis and to
have a clear view I subtract the
supervised model than performance of
service number with two regulation
approach and you can see they have very
different behaviors the AMIA
organization helps very much for the
case where we only have 20% of labels
available and it this is Phi the the
rightmost number is 5% of label so it
can all eat our perform the conditional
entropy regularization gives a very
limited amount of label data on the
other hand the conditional entropy
regulation consistently improves over
and I am i changing regardless of the
weather supervised the the quality of
the supervised model
it's uh I have any shorter
let me show show a graph very quickly
and see if I can go there right now it's
just a vise between a vice at the range
of the range of 10 and I I have the plot
showing that it's it's quite insensitive
to is in sense that so the performance
insensitive to the value alpha so so
it's not change as much no I have I want
to show that let me see if I can
very very quickly so I have the labels
few labels two more labels you see that
they have the similar behaviors by
tooling a number of alpha okay let me go
back to
okay then about phone recognition now
the task is to recognize the whole phone
sequence given an utterance and and we
know that the written generate
recognization will based on the score
from both language model and the
acoustical model in your setup so when
you have only just training with
whatever label data you have yes but in
all cases it's the same size model by
using like a smaller model yes this is a
G mm yeah yes those about curiosity
change number count the number of
parameters are fixed
even our eating amazing unlabeled data
points in that crap it's always the same
yes
and yeah I will show that actually by
adding a label data we are able to grow
the number of parameters by generative
training and I will show that shortly
yes
okay so so in the current contest that
we we were interested in optimized um
parameters for Acacio model and first of
all let me go back to the generative
training I mentioned in the first place
where I have missed my likelihood
criteria criteria for post label and
unlabeled data and to approximate the
distribution of the X I I have confused
confusable set respect to each other ins
X I and which is the Rev by recognition
and in in the real implication it's
essentially can be summarized it's
summarized by a denominator model but
it's innocent in the same sense that
this wine is also summarized by a
numerator model encode encodes the for
acoustic and language model used to
according to the label phone sequence
and for unlabeled data we have
recognition model use such that a person
netted the distribution then is then
essentially the update formula for for
example one of the mean vectors in your
state model which looks like this where
we just ate in the occupancy strategy
from the denominator lettuces for
unlabeled data
for discriminative training we have to
find a way to compute the conditional
entropy for each utterances here we are
here we approximate it with the best
hypothesis so now H is actually the best
hypothesis produced by only recognized
each utterance I then we apply the sin
of Malaysia methods for finding
discriminative training so here show the
result I here we assume we only have 5%
of training set available and unlabeled
data is the rest of 95% and I compare
with the self training methods meaning
that the totally the conventional on
supervised training way where we just
find the confidence enough
transcriptions to add in the label set
then we train the model then we see that
the best number of gaussian we can get
from this country said this amount of
label data is only 8 components and then
by adding unlabeled data we are able to
grow and we see that the the semi
service learning is slightly better than
the self training ml and the another
disadvantage not just another advantage
is that we are not necessarily using
confidence measures here for our methods
it's more for the second results for
discriminative training where we
starting with the initial model was the
best matter we can't render from the
semi service relative approach and it's
not from this graph from this plot and
then we I also come here with the self
training and I and as we can see again
the confidence measure have to use to
try for
and as you can see Sims was a mice
better than the self training and I but
it was mentioning that I when I just use
n paste to a two computer the the
conditional entropy it has its
limitation in that I only have a few
hypothesis competing hypotheses
associate each utterance it's a it's not
its type Optima to approximate the
sequence distribution okay so some
take-home messages that we see that
unlabeled data are useful for finding
more accurate distribution or a better
classifier depending on training
criterion so for example we find that
imposter D modelling we are able to find
a more accurate likelihood functions and
corresponding to chocolates
distributions and so we are able to
discover unseen classes and by applying
a label data under the the regular eyes
discriminative training framework we are
we can have a better classifier and one
of the Vantage of here is confident
confidence measure are not necessary in
a framework for a label they are to be
useful but of course you can combine
this with try to filter out your data
and by some tuning I you can get better
results and one current limitation of my
thesis work is that I only have
experiments on Timmy said and it's
worthwhile to try on the large
vocabulary dataset to see how can it be
generalized thank you
any questions and comments questions see
the self changing approach use the
confidence to select better yes um so
there's a kind of theory but if you
select hi Compton think and the
transcription will be more reliable but
adding the data which you already know
very well
or you see you're thinking about this in
confidence
maybe for those speech segments yeah
I'll cigarette not confident about what
they are they may give you more
information even super video signal is
enough to that effort - if you are using
just using the the confidence call I
found a lot I founded us the similar
conclusions you for self training I
would get because I need to try
different value of confidence to see
which one gives the best performance on
the on the development set and it's like
as if confidence to hold it's like if
this is the accuracy the diversity it's
kind of like this so I usually find
something like here and I think it's we
need so that's why I think it's it's
it's very I mean it will be helpful if
we have some algorithm that can make use
the paper that lies below this threshold
now there is yeah and so I think with my
framework as I said it's one way to make
use of the data you can is confidence
measure and so I guess
Commodus major is a method for the very
straightforward unsupervised training
but if you apply in self different
method of Sammys were supposed only not
necessarily my method but also maybe the
matter if you learning meaning that you
you are able to the the the one that has
no competence is not necessary the one
have flow account competence from one
recognizes necessary have the same door
combatant from the other reckon
another system so this is where if you
can't buy money whole system you are
able to find them all changing interest
in changing transcription that it can be
complementary useful for different
system do you see what I mean
so I hear plots were
the total pot of data with the same size
and we just sort of changing the tile on
labeled versus public yes so what would
the plot looks like if the product
labeled it was fixed then you had a
variable enough
and maple did it would always get better
yes I have
I have a lot about that so yes it's a
for example I have a fixed amount of
five percenter unlabeled data and I try
to add the number of of the unlabeled
data it it really like this in our TV
set and this is for this is for I am and
I haven't had a plot metaphor and I
ain't seen yet
so I sure I should including my thesis</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>