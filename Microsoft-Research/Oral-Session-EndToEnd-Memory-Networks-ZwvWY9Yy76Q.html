<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: End-To-End Memory Networks | Coder Coacher - Coaching Coders</title><meta content="Oral Session: End-To-End Memory Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: End-To-End Memory Networks</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZwvWY9Yy76Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right our next talk is on end-to-end
memory networks this is presented by st.
marys oh but our and this is joint work
with Arthur slam Jason Weston and Rob
Fergus so thank you for the introduction
so in deep learning we have a good
models on certain types of data
structures such as we have a RN n for a
temporal data we can use a cognate for
spatial structures but be still struggle
with the sums or some types of
dependencies in the data for example if
data has to be accessed after order also
the long term can be long term defensive
dependencies also the input can be an
order sir so on those kind of
dependencies we the current model
doesn't really work very well let's take
an example so here we have a question
answering task so there's a short story
followed by question and the question is
what was the Apple after the garden so
to answer that you have to first look up
Apple so that's the left the last
sentence now you know that Sam had the
Apple so if you look the first sentence
you you will know that Sam was in the
garden at some point so now you just
have to track that Sam enter the kitchen
so the answer will be kitchen so you can
see that the optimal solution to this
problem is actually you're accessing the
sequence out of order way out of order
and also there can be many sentences
between those three sentences so there's
a long term dependencies so somewhat
matimak motivated by this we come up
with the model it's a neural network
model actually it's a so it's a model
with external memory that has a that can
reads from the memory with the soft
attention and it performs
multiple lookups on the memory I can be
trained with end to end with a bad
preparation so we call them all instant
memory Network or for short men men -
and so it's a it's actually based on
original memory network proposed by
Weston Chopra and boards so their model
had a hard detention and it required the
explicit solution on the attention for
training but that kind of supervision is
only available for a simple task and
that limits the application of the model
so our mode can be taught as a soft
attention version of that model and so
you can train because it's
differentiable you can train with only
sub-region on their final output so
let's take a look at the models
architecture so it consists from two
modules you have a memory module and the
controller module the inside of the
memory module you have a memory vectors
it's a set of vectors so it's unordered
set so it comes you put you can put the
vectors into your memory and the size of
the set can worry on the other hand you
have a internal state in the controller
so that can also can come from input or
can be fixed vector so you would see you
will use that vector first to get to
address the memory and in return memory
will give you a vector and then you can
add that vector into your state to get
the next state vector you can repeat
this process several times
fix it number of times and finally you
use a decoder to get the final answer
and to train all this the whole model
you only need supervision on the output
so actually what how are we doing the
other thing in the memory let's explain
that so as I said there's a memory
vectors
in the memory and there's addressing
signal coming from the controller and
it's actually a state vector of the
controller so what we do is the refer to
take a dot product between those two
that will give you some similarity
scores between the those two we passed
that through a soft match to get the
probability distribution our memory
locations and those numbers can be
considered as a attention rate or soft
addressing once we have that we compute
weighted sum of the memory vectors using
those weights so that will be the output
of the memory so that will go to the
controller and add it will be added to
the current controller state so that's
how the module works that's it but so
far I didn't explain how the memory
vectors vectors are how we put things in
memory ok so there's many way to get the
memory vectors also it will depend on
your input so for example if I had the
image you can put the image features as
a memory vectors but here I will give
you explain to when you have a sentences
so let's say we had some drops of Apple
that's a sentence what we can do is
embed each word into vector and add them
together so it's a bag of word to get
the memory vector and that will go into
your memory but sometimes your input
might have a structure for example there
could be temporal structure in your
input lag in this story there's a three
sentences but it's ordered in time they
so there are time stamps in that case we
have to include that structure into the
memory vectors simple way to do that is
create a special words for the that time
step stamps and include them into memory
bag of words so you have the special
I'm embedding in the memory writers okay
so that's the model description although
normal can be applied to many things
let's take a question answering as an
example to actually see how it works so
let's say you have three sentences
followed by a question so we can't we
will embed each sentence into a memory
vector so we'll have three memory
vectors in the memory we do same thing
for the question to get a controller
state then you do we can do dot product
and softmax to get the attention rights
because the first two sentences have the
location of the Sam
it can attend those two but because we
have a time embedding in it it can
actually pick the one that is more
decent one so hopefully you'll get a
higher attention on their second
sentence we will do weighted sum of the
memory vectors added back to the
controller state and decode that with
the matrix and you should get there well
it should get give a kitchen as answer
okay so obviously our model is related
to the original memory Network model
that model had a hard attention that's
the main difference from our model so
they had a means they have a max instead
of a soft max in the memory module so
they need the supervision on that max to
train the model and you need to written
on every memory hops another related
work is a RNN search by button at all
it's a encoder decoder RNN with
attention and they applied into machine
translation and got very good results so
our model can be considered as an
attentional model but it multiple hops
for every output there's been other
works on the external memory also so one
is the stack memory for an and another
one is a
newer machine which had also random
access memory with lead and also write
equations well historically there's been
many works combined in neural networks
with memory and even in last three
months there's been many interesting
works coming on the top coming on the
top okay let's go into the experiments
now first we did experiment on a QA toy
dataset called baby it has a twenty
different tasks so for each task you
give in a short story followed by a
question aha because it's artificially
generated it has a small vocabulary and
simple language but what makes this that
is that difficulties that all the all
the time all the 20 tasks require
different reasoning so if you look at
the second example you have brownies
lion julissa's lion Julie she's white
and Bernard is green and the question
was what color is prime so to solve this
you know that both Brian and Julie sees
lion but Julius is white so maybe all
lines are white so if you do that
induction you can answer white so each
task has a different requires different
kind of reasoning so this is the result
we got this chart showing the number of
failed tasks out of the 20 tasks and we
have two versions of the data set the
blue one is when we have a small
training data and red one is when you
have a large training data so the
original memory talk performs very well
but it also requires a strong
supervision also if you train lsdm on
this story
followed by question to predict answer
it performs not that good if you take
our model and use simplest bag-of-words
features it outperforms the other stem
if you do keep tricks you actually can
get up too so you only fails on three
tasks okay so what are those tricks so
first we replace bag of words with the
position encoding so in the memory
vector you actually have some
approximate position of the words so you
know the position of the words in the
memory of there we also tried removing
softmax from the memory module for
initial fever it was that helped also
also it tried at injecting a random
noise into the time embedding that also
helped finally be trained on all the
paths all 20 tasks jointly but that only
helped when the training data was small
in the last graph it shows the it's
showing the by the way the bottom to
graph is showing the mean error not the
number of file tasks so the last graph
is showing the performance when we
increase the hops from 1 to 3 so it
shows that actually the number many hops
is essential for the solving the task
but we didn't get improvement beyond
three hops ok let's take a look at its
how the model solves those tasks and the
example so this first task the question
was where is the milk so you can see in
the first table model attended model
attended on the first two sentences
because it had a Miskin in it but in the
second hop it more attended on the
second one because that's more recent so
it's more relevant so now it knows that
John took the milk so in the last topic
it looks that looks up at John moved to
the hallway so it answers hallway which
is a correct answer
so next experiment was on a language
modeling so this is to test the core
model can scale to more complex
real-world data so we have two training
data entry is commonly used for language
modeling and slightly larger text
dataset so what the model for controller
we have a linear level followed by
non-linearity in the but in the memory
we put each word into a memory vector so
let's say if you have a sentence Yun
says your model must be and you have to
predict the next word so you would
encode each word into memory vector so
you will have six memory vectors in the
memory but because the ordering matters
here you have to include the x timing
into their memory weather and you will
do several memory lookups and produce
the answer so this shows the test
perplexity how formal compared to Alice
team that's just single layer one a
large team so if you can see that if we
increase the hops the performance
actually gets better and it's comparable
to Alice TM also we increasing the
memory helps the performance so we can
do we can see the attention also and on
this task so this is a showing the
average attention of the model on test
data set so the y-axis is a number of
hops so you have a six hops there and
what x-axis is the position of the water
on the on the left side you have all the
stored and right side you have most
recent work so you can see that in the
first hop model attends on recent most
recent words but in the next it intends
a more broad range of words but it
alternates between that two we thought
that was interesting behavior
we also observed samba favor on the
other data set or so so that's the
experiments we're done but currently we
were also working on several ways to
extend our model so one way was to add a
writing capability so we found out that
you can use a same attention ism to
write into the memory vectors so if you
if we did that we can actually like I
use it for like certain numbers sorting
the numbers another the direction was
like playing games with memory networks
using reinforcement learning so instead
of embedding time you cannot embed the
locations so each each item will be
going to a memory as a bag of words but
it has a location inside as a word so it
has to learn the topology from the
experience so conclusion people post a
neural network model with external
memory it has a soft attention over the
memory locations and it can be trained
and to end with the backpropagation we
showed budros of some toy cue a dataset
on language modeling tasks we our
performance was comparable to Alice TM
and we showed that our models versatile
that it can be extended to writing or
playing games so you can actually
download the code from github and run
those experiments by yourself so if
you're interested in more detail you can
come to our posture today thank you very
much
all right before we take questions the
spotlight presenters I invented to come
in the front on my right to prepare for
presenting their spotlights so question
on my right yeah hi my name is Yomi tuba
from the Swiss Halley IDs ia and of
course in the past quarter-century there
have been lots of methods for learning
programs that control internal memory is
external memory is soft attention heart
attention and I'm sure some of them are
going to mention them the and the RAM
workshop on Saturday I think now I saw
that you compared your assistant to lsdm
but did you also compared to any of
these other methods there so we compared
the lsdm only in the data training in
this paper yeah we didn't compare to the
other memory ma memory architectures
yeah so I think that is a whole treasure
of literature out there and a lot of
this stuff is directly applicable to
questions exactly like the ones that you
are addressing but maybe in the ROM
workshop on Saturday is it on Saturday
yes we will have lots of discussions on
that ok thank you very much
all right next question on my left like
we talked generally from hobby
technologies quick question on the
performance study you you have to give
antenna sets 1 K and 10 K and have
various mechanisms and compared with
cigar bar just wonder why you mentioned
that but try to wonder for the joint
test of 20 times why is it doesn't like
behave better is it because of the 20
tasks are not distinguished distinctive
to each other or the 10 K data set and
not a large enough any insight on that
so the question is that why you get
improvement on joint training why not
why not on 10k
yes sir so I think it was just that our
model wasn't large in a large capacity
to learn all of the tasks yeah I think
if he increased the capacity it should
be at least as good as a individual
individual tiny yeah thank you ha Taemin
for mines hi
very nice I want to say that I really
like this
this constant addressable memory thing
I'm obviously it is you know linear in
the number of facts that's being stored
in terms of runtime which is fine for
ships such short stories as you have
here but we use content addressable
memory every day when we do a web search
and then itself billions effects then
they use indices and hash tables and all
sorts of lookups to to make that run in
a time that is far sub linear to the
size of the collection effects do you
think that at some point in the future
your model could be extended in such a
way to work with a very large collection
effects without you know scaling up the
runtime linearly with that size okay
that's very good question
frankly I don't think it can be extended
to that scale but I think we can use
like hashing tricks to get relevant you
know get like Tom topmost hundred
relevant things first then I maybe do
soft attention on those two I think that
would be more sensible approach it
sounds reasonable and you would be sub
linear sub linear so sounds good yes
yeah thank you
okay thanks all right so we do have a
bit of time last question so last chance
yes
alright let's say our speaker
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>