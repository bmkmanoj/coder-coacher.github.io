<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Edge of Artificial Intelligence - Cognition – Pushmeet Kohli | Coder Coacher - Coaching Coders</title><meta content="The Edge of Artificial Intelligence - Cognition – Pushmeet Kohli - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>The Edge of Artificial Intelligence - Cognition – Pushmeet Kohli</b></h2><h5 class="post__date">2016-08-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qgEOSO8ZlOg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
right so we're going to start again so
we'll invite you to take your seat and
our next speaker is push meet Kali Kali
is in the microsoft research lab here in
redmond and prior to that in cambridge
and i think you've even been in the one
in india years ago so it's a real
pleasure to have push meet here with us
he works on the development of
intelligent machine to teach computers
to understand the behavior intent of
humans and to correctly interpret
perceivers the objects and scenes
depicted in color depth images or video
so I think just from that you can guess
he comes from computer vision and he
actually worked so he moved to Redmond
recently well recently I guess a year
ago where he worked with Rick rush it as
his technical assistant and since
februari he's actually starting a new AI
team so great we have now a cognition
team at Microsoft so let's welcome a
push mid please so thanks Evelyn for the
for the introduction and for the
invitation to give this talk so there is
a lot of excitement about deep learning
and AI and machine learning like is
every sort of day no day causes goes
past where you don't hear an article
from the New York Times or The
Washington Post or the BBC talking about
some cool sort of a a project and then
we we listen to experts saying away I
will take over and sort of the it will
be a threat to humanity and so on so
behind all this sort of hype what is
what are the real sort of challenges and
what do we really need to sort of
resolve so I my approach to sort of
machine learning and AI research is very
pragmatic I started off as
a verification systems person power
methods person like when i joined MSR 12
or 13 years back it was in the software
engineering group sort of testing of
sort of device drivers and so on and
then moved into discrete optimization
computer vision perception game theory
machine learning deep learning problem
stick programming and so on so I've had
a chance to sort of reflect upon the
developments happening in various areas
of computer science not just in machine
learning so this is sort of in some
sense inspired from the experiences
throughout through those years and what
I see are some of the challenges that
all these machine learning sort of
systems will face when they end up being
deployed so to start off there is a
reason to be very careful right there
have been a number of great successes we
have real-world systems like Skype
translator which like 10 or 15 years ago
would not have been imaginable right a
system being able to in real time
translate speech into first but text
convert it convert the text and then
generate speech again right that's a
that's a pretty impressive feat we have
self-driving cars although we know what
are the challenges there we have systems
like the Kinect sensor which can a
single sensor can estimate the human
pose of the person right in real time we
know the advances in object detection
object classification now these types of
systems can outperform a human on these
perceptual task we all heard about alpha
go and of course the Atari games work
from deep mind so behind all these sort
of successes which are a great sort of
success for the community there are also
a number of stumbles right so we have
the racist so the gorilla gate as i call
it we had teh right and then we have the
tesla crash
right and the question is was it really
surprising was it's really surprising to
the community that these things would
happen like many people were expecting
this to happen in some sense right our
software systems if you look at the
history of software we all sort of
remember the blue screen of death and
when it when it used to happen but in
some sense software was always played
like this and suddenly we we think that
Oh humans were writing the software disk
over these sort of thousands of lines of
codes or millions of lines of code and
we were able to not get and we were able
to resolve all the bugs but suddenly if
a machine sort of rights this this
program then all these sort of bugs will
will go away well that's not true as we
have already sort of discovered so the
question that I think about is how do we
sort of solve this right one example is
how brittle are we what what are the
foundational sort of things that we are
building these systems on top of right
so this is the example from again folks
at NYU and Google and shows that here is
a state-of-the-art image classification
system which correctly identifies the
correct label for the left side of the
images these images and these images
were given it given as input to the
system would correctly sort of classify
what the objects are and if you add some
random sort of very minimal
perturbations and create these new
images suddenly everything becomes a
ostrich so the label the system would
give you is the ostrich for all these
three all these three images that seems
crazy like forget about intelligent
sister girls like the most stupid system
I've ever come across right and we are
we are cheering this we are writing New
York Times articles on this that's
that's the most stupidest thing I've
ever heard or seen right so this is the
current state of the art this is the
current state of the art I'm not making
this up this is like the papers mention
these sort of results so now it's clear
that there are a number of challenges so
of course we should be proud about about
all the developments that have happened
in the field but we should also be
thinking about these sort of issues
right and behind all those accidents and
all the sort of the racist sort of bots
and photo classifiers are these
fundamental limitations in the machinery
so let's think as scientists to and look
at the crux of the problem right so when
we encounter these problems while
developing software what did we do so ms
are for example invested a lot in
software verification back in the day
like like when i started in as an intern
like in 2004 the software engineering
group was huge and it was basically my
first project was testing device drivers
for long con and it was a long on
basically what went to be windows yeah
so it was it was it was a big deal right
testing these systems and so what do we
what sort of the machinery we will need
to build to test these systems so the
questions in my mind are in 10 years
time in 20 years time what is the
programming language we are going to use
for developing these systems are we I do
you think we will still be using C sharp
C++ Java or like torch tensorflow what
what will be that language what what
type of debuggers will it come with what
sort of compilers will come with what
sort of visualization techniques it will
come with right 20 years from now like
will we be still doing C++ or MATLAB or
whatnot right to run these experiments
that's the question and how do you then
verify these things should we will be
using unit testing integration testing
how do we even quantify what we are
testing against software has high level
specifications and then we basically
test the implementations on the basis of
those high level specifications in which
in the case of machine learning we only
have partial specifications input-output
pairs the things that are given to
machine learning systems are partial
specifications of the behavior that is
expected of these systems and not even
partial they are noisy partial
specifications because sometimes we
don't even know that the labels are
correct okay so let's look at the
software engineering version of it so we
were given high level specification you
go to a software engineer and say here's
a high level specification of what I
want you to sort of build and then they
build the implementation and then you
test whether you are the implementation
has the properties as specified in the
high level specification the machine
learning system has to now we can sort
of the only way to test is on the
partial specification so that's what our
test data is the fastest partial
specification and we have a bunch of
tests that we we can only sort of do
statistical sort of guarantees there are
no formal sort of proofs that we can
give about the properties of of the
machine learning system okay so how do
we now sort of resolve this issue how do
we resolve this issue and then what
supervision should we sort of use should
we just use should we continue to just
use this partial noisy specification
which is input-output pairs or is there
certain sort of properties these sort of
Asimov's laws of robotics that we need
to ingrain in or what is the formal
structure of how do we actually embed
these types of supervision or invariants
into the mapping right so these are the
questions that I am particularly
interested in okay so let's look at the
second question first about data so I
like if I if he had time we could I
could have shown you a very interesting
post a video from yes prime minister
it's a British sort of comedy sort of
program in which
the bureaucrats explained to the
minister about service right so you
essentially keep on asking the question
in different way until you get the right
response which is what people are
blaming drags it for but but the idea
but the question remains right if you
look at the data how data is collected
to train these systems we know you in
social sciences that there are big
biases there are framing biases and then
there are subject biases right so in
behavioral econ econ economics we know
that many sort of empirical theories
have been tested on undergrads at MIT
you are however because that's where
faculty sort of try to test these things
right but you cannot expect the same
sort of things to hold true from some
sort of random person from India or
China or South Africa who comes from a
completely different social sort of
background they might not basically give
you the same results then when you ask
them the same question as an MIT
undergrad or a Harvard undergrad or a
Yale undergrad odd or a Chicago
undergrad right so how do you basically
sort of collect data and how do you
basically how does it affect the
performance of the system so i will
basically just show you one example of
how this happens in the case of gesture
recognition so the problem is you want
to build a gesture recognition system
you want to collect data to make the
Machine understand whether a person is
sort of throwing an object or want to
increase the volume of the music player
or want to move through the next track
or something like that right so here's a
bunch of things that we did we invited
30 people into the lab and this is the
time when we were building a sort of the
connection recognition system and we
wanted to say get training data for
different types of gestures like these
were gaming gestures but were also music
player gestures so gaming gestures would
be through sort of something duck or
kick or music player gestures would be
increase the volume
or move to the next track and so on
right so how do we sort of get data for
to train the machine learning system
that when they see that these movements
and they should sort of think that this
is a gesture that the human will also
perform and so there were different
types of modalities that we could sort
of think about so first of all we can
tell the other people ok this is in text
this is the thing that you should do
what will be the gesture you should do
for moving to the next track right so
this is one of the things that we we
said raise your arm to increase the
volume right and some people did this
some people did this some people did
this some people there's a lot of
variation right so then somebody said ok
don't that's so ambiguous let's use
images right so in the case of images
you could we basically said ok to in
order to go to the next track what you
need to do is basically here's a bunch
of sort of canonical poses this is like
you would see in manuals for by while
traveling on flights they will say these
are the set of poses that you need to be
in so you need to be in this pose I
suppose this suppose this was and there
is the arrow saying you essentially need
to do this gesture right to move to the
next track and sixty percent of people
did it correctly the rest basically they
did this right so it's crazy right they
were still ambiguity so then people said
oh you just just collect videos of
people doing it right you should just
basically give them instructions give
give a canonical sort of video show them
a video and then let them do the gesture
so one of the gestures was increased a
sort of the tempo of the music and the
way to do it to just do this right and
their tempo of the music will basically
increase and so I had an intern who was
collecting all these instructions for
the participants and he made a video of
himself and then showed the video to
all the participants and recorded what
they were doing and when we looked at
the data all of them had one hand in the
pocket and then what they were doing
this now the one hand in the pocket is
not needed right but the intern was had
one hand in the pocket so they just
copied the whole thing so it's showing
you how difficult it is to get training
data and how difficult it is to sort of
look at to even instruct people to
collect data right so and so and it
actually makes a big difference so this
is the the the gesture recognition
performance if you collect data by
showing textual instructions this is by
images this is where videos and this is
why images plus text and videos plus
text so it this basically slice has a
significant sort of effect on the final
output of the of the system so and and
and the effect is different for
different types of gestures so for the
music player gestures you saw that
videos and videos plus texts were very
important in the case of games where the
gestures people already knew there was
less ambiguity right kick there is no
sort of there's only one type of cake
people know what what to do with a kick
in those cases basically just having
images and or text was was fine right
videos did not add that much so this is
the question right that we are always
looking at when we are collecting this
data we are looking at a few people
right few subjects and behavioral
economist were doing this also computer
scientists are doing this but we need to
sort of look at people a larger crowd of
people and to make sure that they are
that these systems are able to sort of
conform to those preferences so we did
another study here we were doing a
recommendation system and the idea was
that when we when we look at
recommending say what type of program a
person should watch v
we base it all individual preferences
but that is not necessarily true because
people generally watch things with other
people so this is the Preferences if say
an individual male was watching
television alone right so you will see
general documentary comes up high news
comes I general drama comes high an
individual female sort of general drama
comes high news comes I situation comedy
comes high and sword right now any idea
on what happens when you have an
all-male group what will come high
absolutely right it's different right
it's a different bias right it depends
on basically who are you with so it also
changes the context in which the data is
collected all was when two females are
watching things together hmm general
drama sort of comes higher but like it
does not change much right but these
things are not sort of these things have
to be studied but what happens when
there is a mixed-gender group now the
the female sort of references dominate
so so so with these are interesting sort
of questions right like when we think
about individual preferences when we
think about recommendation systems we
will just look at individual sort of
training data and just feed it to the to
the classifier or recommendation system
right it does not take into account what
are the complex dynamics that are in
play when a group is making a decision
together okay they're like I can go on
and on about basically this stuff so now
coming coming to Center other part like
how do you how do you sort of create
tools or programming interfaces to
actually build very complicated
perception or intelligent systems so
here is one sort of example problem that
we could think of so so suppose you want
to solve computer vision one way of
solving a pedo vision is to say I have
sort of a rendering engine right I have
a graphics engine which given some
representation of the world gives me the
image right it creates the image and
then in order to understand the image
what I just need to do is basically
solve this inverse problem to recover
the parameters of the world right so the
idea is that I will I've been I've been
shown some image X and I have this
rendering function so i will try out
different parameters different things
about the world until i get to a point
which sort of matches the observed image
and that's what i will output that is
the representation i will output so many
very interesting problems can be
formulated in this sort of way including
camera pose estimation where the
rendering engine is just you are doing a
raycast right you have a sort of 3d
model you have a sort of camera position
and then you can create the depth image
and then the inverse problem is given
the depth image you want to generate the
camera position right the the 3d
position of the camera or the 60
position of the camera the 3d position
as well as the orientation you can do it
for human pose estimation where you are
given some 40 dimensional representation
or 26 dimensional representation of the
human polls you can generate the image
and then to solve the both estimation
problem you're solving the inverse
problem so the problem with this
approach has been that it is very
interpretable right it's like this
mapping is actually known this is
written by human right this is a
generator this is sort of a rendering
engine which is written by human and so
this is very well specified the mapping
from outputs to inputs is very well
specified but in the same way the
mapping also it's also providing a
specification from the input-output
mapping right so here the specification
is very well specified but it's very
difficult to actually optimize it right
in order to get to the back mapping so
the way we we did it is we've had some
way a number of sort of different
techniques we said okay
if this optimization problem is
hardwired Odin features basically use
machine learning to generate hints and
from those hints we will start local
gradient descent right that's the sort
of technique that we used for for
connect in fact in fact we went further
we said why not just why use just one
hint we can use a bunch of hens right
and at least one of them will be good
and then there was a lot of machine
running as to how do you sort of
generate these multiple hints such that
at least one of them will be good and it
sort of all worked right we had systems
which in real time could get users like
this you have a depth average and you
could classify the body parts and you
could estimate the body part positions
so this sort of system worked but the
challenge is now and of course I can
show you another example this is
something that Erik Wemple yeah this how
the computer was founded in 20 years no
so the idea is here are the proposals
that the computer is being shown right
so is shown this sort of image and it
quickly finds out which is the right one
the initial sort of prediction you will
see by that the program produces those
those samples the best of those samples
is not that great right but after sort
of and you can sort of see the
reconstruction error but after
refinement it will now look much better
each of those images so this is the
depth image that has shown that is given
to the input that is given to the system
and let me just go back
so this is the model that we are trying
to fit to the observation of the
observed image so what happens is you
are shown this image and a bunch of
proposals are generated by the system
right and the first one is is is
selected this is showing the best one
right that is that is proposed by the
system now this proposal will be
improved by local refinement using the
gradient descent step that I was I was I
was mentioning right so and this is
showing the projects or of fitted model
overlaid on top of the observation the
red is observation right and green in
the fitted model okay and now this these
are sort of the results that you get
from the system and you can see that it
is really robust because it's now not
relying on tracking it is at every
moment it's generating a lot of
proposals and it's able to sort of fit
those proposals correctly so at one
point of time gem like he remove he
changed the lighting condition and still
the still the system was working and at
one point of time he will just now go
out of his office and still the thing
would would be working even though like
at this point of time his hand probably
occupies only around 30 or 40 pixels so
this sort of method is very very robust
it works on a variety of different sort
of subjects and so on so the question
now is that required a lot of
engineering right that required sort of
hundred man years of work to actually
get get it to that stage how do we do
this for arbitrary sort of systems so so
the we were generating this sort of
pipeline so the forward mapping was
given the generative the rendering
engine and then the back mapping
required creation of this portfolio of
discriminative predictors which each
proposed a proposal and then which were
which were refined so can you now
a programming language or a new type of
programming language which just
formalized this you write the generative
process you provide the specification
the the back mapping in one way and then
and the and the backward sort of process
is automatically generated as like a
comp like a like a compiler so we had
this paper last year which won the West
River which was another for the best
paper award cvpr it's called picture
prob listing programming language for
seen perception where you have an
approximate so you will have a language
you have a way of specifying the world
and then you have a renderer which sort
of takes in that representation of the
world and generates an image and then
then there is basically this compilation
stage which takes in the observed image
and learns what would be the the right
mechanics to invert that process okay
this is another sort of direction which
sort of aids the interpretability which
is there's a lot of work on
representation learning like whenever we
have operating on high dimensional
signals the idea is to somehow convert
them into a low-dimensional either
through a pcl through a sort of linear
mapping or a non linear mapping and
because it aids sort of supervised
machine learning from there on but
suppose we want this intermediate
representation to be disentangled as
well if it is disentangle yet super
useful for example in this case suppose
you have images of faces and and there
so it's a 155 150 in of a face and you
want 200 dimensional representation of
course you can do a pca and just look at
the first 200 eigenvectors right 200
eigen values to encode the the actual
image but those eigenvalues would not
correspond to any semantic concept
suppose you want to disentangle that
vector and say that the first n
dimension correspond to pose the second
andaman dimensions correspond to light
the last sort of 50 domination
correspond to the shape of the face
right that would be super useful why
because now once you have this
representation suppose I want to do
different types of tasks see face
identification I know if therefore face
identification only the shape matters
right the
the lighting condition should not matter
if I am sort of comparing two faces and
saying two images and saying whether
they belong to the same person or not
only this part really matters the other
parts don't matter so I can sort of just
use the representation that I want
similarly if I want to do a gesture
recognition then all this is all this
information is nuisance I just need to
use the first ten dimensions so finding
a supervised mapping from these ten
dimensions through the post space is
much more sort of efficient than finding
the mapping from a 200 dimensional sort
of feature vector to the PO space ok so
this sort of these are disentangled
representations are very sort of useful
so so suppose we train this data and
this model and we show how to train this
model then given a new image if you have
a single image you can take this
representation and just perturb one
piece of it and see what is the output
that you generate so here is one single
image right and we get this intermediate
representation for this single image
using this encoder and then perturb one
piece of it and see what is generated
output so this have nth sort of the pose
in the z axis right so this to sort of
thing is done this is when you change
the illumination in X this is when you
sort of change the pose in sort of the y
axis so the model has not seen any of
the the things from other dimensions
other directions or other sort of
illuminations but it is still able to
disentangle in and hallucinate what the
what the data should look like there's
also some other work that people have
started sort of doing in the in the
computer vision community and even in
the natural language community is to
build these sort of do question
answering save is there a red shape of a
circle in this image by a sort of not
solving that problem directly as an
end-to-end classifier but building
modules which are then sort of composed
together to answer the question and each
of these modules can be unit tested in
some sense right and the idea is if you
can sort of test easier the
unit separately then their compositions
can also be correct so there is some
interesting work that is sort of
happening in this space so how much time
do I have okay so just to sort of
conclude by mentioning a point that was
mentioned in during Eric's talk regarded
regarding game theory so traditionally
machine learning systems have been
considered as the following you are
given an input and you give in a set of
outputs and you are trying to learn the
parameters of of the machine right of
the machine learning system but that's
not how it is going to happen in the
future in the future the machines are
going to constantly learn from humans
right and humans are not constant they
are like humans are not a stationary
distribution right they will basically
sort of the preferences change over time
the social norms change over time what
the society considers as verbal changes
over time and so on right so in fact
this user population also has some
parameters and it is also evolved when
it's also even going to react to the
developments that are happening in the
intelligent systems so it's about co
adaptation or core learning of these
systems right where you also have to
think about what is going to how are the
user is going to be affected right and
how should I basically think about it
even before I start making changes to my
parameters right so that's sort of it
goes into really deep questions of
equilibrium analysis and so on so by
that I mean I'll just conclude by saying
it's very exciting time for machine
learning but there are lot of challenges
that we need to be careful about and we
need to give a lot of thought or or to
thank you thank you
questions months
so I'm trying to understand you you you
you specified some high level goals and
then you started talking about sort of
an individual project and so I'm trying
to understand like are you trying to
come up with some sort of completely new
kind of formalized methods for designing
training sets or I'm still not clear you
had this vision like we we did all this
stuff with programming now we're going
to do it for machine learning and then
you went into a lot of stuff about how
the training sets are very hard to
design properly so what is the link here
yeah so what so what I started off with
is basically sort of trying to motivate
where do where are the challenges in
deploying machine learning systems right
or even understanding machine learning
systems and so one approach that I could
take is basically okay let's throw all
the research in machine learning out and
sort of we start from scratch and build
and take the right pieces the other sort
of approach is basically say what are
the things that people are already doing
in machine learning right which which
try to address some of the concerns for
example the question of interpretability
the question of sort of making sure that
the specifications that there is some
high-level specification in mind that
the the mapping needs to conform to so
for example the the idea the the thing
that I talked about with regards to the
human pose estimation part or the hand
pose estimation part they're one side
the forward process the process from the
the the joint angles to generating the
image that is a very well specified and
well-understood mapping right because
it's like a rendering engine that has
been hand specified has been written by
a software engineer right so that
mapping is perfectly well defined the
specification is very clear but that's
not the mapping we are interested in we
are interested in the reverse mapping
given the image what is the pose right
but this mapping the mapping from sort
of pose two images provides a
specification it provides watch
the back mapping should be because we
can now generate infinite data to test
the to test the reverse mapping right so
the idea is okay can we now generate
high level specifications for all the
problems we are interested in maybe not
absolutely right so so it's a question
of how do we sort of provide if we are
going to work with machine learning the
machine learning that we see today most
of it is works with partial
specifications partial certification in
the form of input-output pairs right of
course there is there is a new types of
specification which is if you have a
simulated world right and you have to
sort of create tasks that's also one
form of specification of behavior that
you would that's more of a goal-directed
sort of a form of specification of
behavior right not it's not in the in
the form of input-output pairs so the
idea is can be now sort of go towards
the the time where we will be able to
marry the concepts that have been
developed in the software engineering
world in testing and verifying the
behavior of software systems very large
software systems and can we sort of make
machine learning make them to translate
those things in the machine learning
world right and I was giving you some
examples of how we did it for say
connect or sort of some of the initial
sort of work in for picture but that's
not solving the whole problem it's just
giving you a taste of what it it would
look like or an example of how it sort
of works with regards to data that's a
different matter altogether it's about
saying that even generating a partial
specification has its own challenges how
do you know that you are how do you
generate specifications anyways for the
tasks that you are trying to solve right
so even even generating a partial
specification is problematic because of
the framing and subject biases so when a
system designer basically goes in and
says okay I want to build at pose
estimation system or a gesture
recognition system and it's supposed to
work in Japan as well as the US and the
UK and india where people have various
different notions of a sort of movements
and so on like
where do we start right so so it is all
about basically specifications how do
you if machine learning today is working
with partial specifications input-output
pairs then how do you generate them
properly right in the future we might
want we would hope that we might be able
to have more richer or more sort of
stronger notions of specifications right
so we can test those systems to say are
you conforming to the specifications
because today I don't know what the
Tesla system is going to water what are
the specifications on the teletype lane
sort of assist system right nobody knows
right it's just based on some input
output examples but how what are those
input from which distribution are those
input output examples sort of sample
from who knows right there must be a lot
of biases maybe it's all sort of Tesla's
driving around in a lawn musk says house
right or the hive is around is sort of
around Silicon Valley so it's that's
that's the the real sort of problem
that's why I basically have wanted to
just also mention some of the work on
subject biases and framing biases in
collecting training data because that
sort of goes to the problem of even the
partial specifications that we used in
put out prepares those are even not
correct I would so I'm sort of bias and
by training so whenever we look for
example the problem that you show with
neural networks you know the tubers rub
a little bit some pixels and then you
get something completely different to me
it sounds like we would actually need to
have you know some sorts of Bayesian
model in there or some sort of
quantification of the uncertainty and of
the day of the noises there is in there
and in general I think that so my point
of view is that in general most of deep
learning methods that we see nowadays
they essentially create deterministic no
representations from which you cannot
really extract information about
uncertainty you don't know if what you
know that if what you're seeing is
really ninety-nine percent accurate
because you have already seen this
before or simply because you know there
is some strange things in the model so i
was wondering if
how does fit into the sea or your vision
so being patient is not enough that's
that's the answer right that the point
is is not just about modeling
uncertainties it's about having a good
notion of what is the model because say
i am doing logistic regression and i am
going to say i am going to solve
everything with logistic regression and
I am vision because I am going to use
sort of the l2 regularizer over the
weights Sam patient because I have a
prior over the weights it's not enough
right because that prior might not make
sense right why does the basically the
l2 prior over the weights makes any sort
of sense it's if as we are talking about
here it's all about prior distributions
right like but priors define more
sophisticated mappings on offer of more
sophisticated form rather than the
general priors that we see
indiscriminate machine learning the l2
norms or the l0 norm or the l1 norm it's
about having more that's how I basically
like factor graphs are a language for
specifying priors right prob listrik
programming language is basically a
language for specifying priors right so
bayesian is being patient is not enough
sort of having uncertainties is not
enough it's the models will might all
might still be misspecified so we need
to also work towards more sophisticated
models of the world right off of the of
the tasks that we are sort of trying to
solve so sort of handling uncertainties
is important but that's not the only
thing that we need to consider by
looking at this problem but in the
moment where we try to specify more
stronger priors then we also need to
make sure that these are correct which
means that we need to know more about
the system that we want to learn from
which means that you know essentially
it's sort of a circle we want to have a
ton o matic system because we don't want
to have strong priors otherwise you
would already have the models there yeah
so it's it's like saying that if we
don't have a higher level specification
right then what is the software system
via building that so that's basically
the
there is there needs to be some sort of
domain knowledge if we are to sew
interpretability in is in this in the
sense that if we are not able to explain
what the system has learnt we need to
basically be able to explain what the
system has learned otherwise we have a
wide trust the system you can trust the
system based on just statistical
guarantees over over at the input-output
distribution but the but you need to
have some way of specifying sort of of
interpreting what the model has done and
that comes and there comes a notion of
how much what can you sort of say about
the behavior of the system and that's
basically where these structural priors
come in that I believe that this should
be true by this in this with this
probability with some high probability
thanks alright thank you push meet thank
you let's thank our speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>