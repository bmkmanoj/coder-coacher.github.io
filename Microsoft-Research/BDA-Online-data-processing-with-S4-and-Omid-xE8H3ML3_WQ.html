<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Online data processing with S4 and Omid | Coder Coacher - Coaching Coders</title><meta content="BDA - Online data processing with S4 and Omid - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - Online data processing with S4 and Omid</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xE8H3ML3_WQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
cool ok so I'm Claudia and today we'll
be talked about online data processing
with s for a moment so those are
projects that that I haven't done during
my time at Yahoo this in cases don't
know I have joined sorry they my time at
Microsoft as you can tell I recently was
with was with Yahoo research that's what
I have done I have done this work with
some other folks or still there when
Dylan talked and invited meet you up to
participate and give a talk in the
inviscid and the first thing I kept
wondering is if this topic was was
really suitable for for Big Data
workshop and so I went out there and try
to look for references or definitions of
what big data actually is so weak pedia
says that a big data is in information
technology a collection of data set so
large and complex that becomes difficult
to process using on hand database
management tools I'm not saying that I
agree with that definition but that
drags me down a bit because this in the
context of this projects online data
processing stream processing doesn't
quite fit in that definition now some
guys doing doing big data Hortonworks
they say that a big data system is
supposed to have four properties and and
this is a lot more vague and so makes me
a lot happier you use local storage to
be fast and inexpensive uses clusters of
commodity harder uses free software and
an open source so that's great a lot of
stuff falls into that category right and
some happy now so now i can i can
pretend that I'm you know so this is a
topic about about big data even if you
don't agree with it so assuming that
it's it's about big data let me show you
this graph where some guys try to
project the utilization of of Hadoop in
inside yahoo over the years it
apparently started before 2006 around
2006 that was use most
for research around two thousand seven
people start publishing these things
using using a using Hadoop I generated
sorry using results generator of Hadoop
2008 goes into production like
applications using it using Hadoop 44
for their stuff and and that's that's
what that the hype starts right there's
this cool technology if you if you're
not using that in your application your
product then you're probably a loser
right so you need to use it and at some
point at some point every application
was in some sense touched by Hadoop in
in in some way so run 2008 was that was
that was possibly the peak of the high
prank everyone had to use it at least
inside yahoo and again take it take it
as my personal perception of this i'm
not employ of yahoo anymore so i'm not
representing yeah oh here now back to
2008 so it turns out that not all
applications could live with Hadoop at
least in the way it was it was implement
an end was Rainier that and some some
applications needed some sort of
scalable real-time processing in general
for things like direct feedback so you
take you take events you process them in
and you know what you know what you
would like to know what to do next
things optimization so you're tuning
parameters of a model or something and
you'd like to do that in an online
fashion instead of waiting for the next
the next psyche of a Hadoop job or of a
data pipeline processing one concrete
case was ranking ads using click through
analysis so you're processing that the
events that correspond to a two two two
clicks of users and you try to determine
and you try to use that to leverage that
to help with your ranking so the
solution that folks proposed for for
those kinds of problems was was to
implement a platter for a platform at
this video platform for processing
strings at the time there was no generic
platform available at least that we were
aware of as I became a research project
but now let's try to understand first
what is that what is the stream
processing platform at least according
to my definition so this is a platform
that enables you to processes streams it
could be one or more streams this
streams are composed of events events
could be could be tweets right it could
be as i mentioned clicks of users right
clicks on the applications on the on the
web pages you you're serving desirable
properties for for for this platform
this is the part that is probably
arguable but according to what we
observe it was online a mini low latency
was is an important one I suppose that
everyone would agree with that another
one was that we felt that he should be
best effort best effort in the sense
that you we're not going to try to do to
process and reflect the state of
processing every single event that comes
across the system we wanted each of East
gable is capable not in the sense that
we going to have applications deployed
across thousands of nodes it's cable
more in the sense that if I if I need to
increase my capacity by adding more
notes I will be able to do that that
easily fall torrents we also felt was
important but not to the extent that
that would hinder performance or prevent
us from from implementing an application
that that they would light you but some
limited form of photons would be nice in
desirable it finally flexibility with
respect to the way you use the resources
that are available to you we could have
clusters of alpha glasses with notes for
such a platform and and we'd like to use
the results resources in a flexible
manner so all that was was projected in
transform into S 4 S 4 stands for simple
scalable streaming system this is kind
of a note a historic overview of what
happened to the project so the project
is started around 2008 this was actually
not started by me I was not part of that
team that developed at that first the
first version of the platform and and at
that point there was internal code via
an internal code base available that's
some
groups used to in with their
applications they're deployed
applications around 2010 that same group
decide to open source it so they put it
on github you got you got some traction
people got interested and you see
comments across the web about that and
between that and beginning of of 2011
that that's when I i personally joined
the project and started started helping
those guys with some of the features
that yet they needed another point we we
identified two things that would like to
do one was was redesigned the system
maintaining some of the concepts but by
adding some some of the things that we
felt the first version list and also
move that your patch it because we felt
that in Apache we'd have more visibility
then then on github so we made the move
from github joe pesci at the same time
we start working this new version that
was that was those called piper which is
the current version so this figure tries
to explain or illustrate the main
concepts that we have in the in the
current piper version on the left-hand
side we have we have the incoming
streams which I'm calling the external
string so these are the raw events that
you receive that you want to process
what's think once you receive those
events you feed them to to a nap a nap
is is a an implementation of an s4 class
that that does what you were essentially
what the operators that that they would
like to use to process your events and
and those operators which we call
processing elements we connect them
through streams so when we did defining
our application we have to say what up
what processing elements we have each
implement them and we need to define
streams that connects those those
processing elements so we have the
external streams that correspond to the
events are we receiving and then being
turn of streams which correspond to the
streams that connect the processing
elements in an application another thing
that we allow is that you connect
application so you can define a
an external stream so an extreme that I
stream that an application exposes and
other applications may come and
subscribe to receive those and the nice
thing is that now you don't have to
define once when you implement an
application you don't have to to define
all in the same in the same app you can
have multiple and connect them in
different ways so you can couple
multiple apps to define your over
application it also allows you to have
different applications connecting by by
subscribing to the streams of others now
looking more closely at what an app is
we have again that external stream of
events coming we typically have what we
call an adapter that receives those
events right so those are Roy on sand
and they're not as for events yet and
that adapter what you will do it will
take the event and and transform into an
s4 event transforming to an s4 event is
essentially mapping an event to a key so
you define the keys for a a key four for
four day event and and you can add
fields to that a 22 to that event at
fields that you're going to use in the
processing of the event now on the exit
of the of the adapter we have the choice
in this particular case you have the
choice of 72 notes so that's because we
partition the key space so we partition
the key space and and the adapter is
going to send to the one is going to
hash the key and it's going to decide
you which one is going to go so in this
case because I have two notes the
expectations that each node is going to
handle fifty percent of a of the keys
and for each key that I ever process
there will be a w processing element
responsible for processing all the
events of with that particular with that
particular key so so interesting points
to keep from here is that an application
define the key so you you tell what what
what your kids actually are and and we
have one processing element perky that
having one processing element perky
means that each processing element has
its own state there is no sharing of
state across that the elements when it
comes to deployment so you have
implementing your application now you're
ready to run it what you do is you
package your application you post that
on some on some blast repository so that
blast repository can be can be really
anything except you to define what what
that is you can use a simple disability
file system or you know I don't know
some something like NFS to distribute
the packages now you inform the people
that you have published a particular
application and then zookeeper notifies
the nodes that have been allocated for
that given application so those know
this is what i'm calling the logical
cluster that corresponds to our give an
application in that case the nodes are
going to fetch the package and they're
going to execute the application the way
i have described before with respect to
fault tolerance we allow failover so if
you if you're deploying an application
and you define a number of of node
status superior to the that is greater
than the number of partitions you have
then you're going to have spare notes
it's a very simple final crashes I
replace I replace that node with one of
the spare notes and I can keep doing
that as long as as I have spare nodes so
related to that we also we also take
checkpoints but the checkpoints are
taken in an uncoordinated manner and and
an asynchronously on top of that we do
we do for these checkpoints we do lays
recovery so we take checkpoints of the
processing elements and in the case I
need to recover that state I would do
that only upon receiving the first event
for their processing for that processing
element which means that when I was
restarting a note I don't have to pull
or existing checkpoints you have
probably noticed that this scheme
selassie because I'm making synchronous
calls to sort this checkpoint
I may I may lose some events because the
state might have changed by the time he
gets he gets to disk this is okay so we
did that on purpose so we didn't want to
we didn't want to affect performance but
at the same time our main goal was to
prevent loss of accumulated of state
accumulated over extended periods of
time so suppose you had been run your
application for say I don't know a cup
of months in your training Ahmad or or
whatever you don't want to lose state /
/ all that time it's okay for you
perhaps you to miss the events in the
past 30 seconds perhaps not okay to lose
again thus it accumulated over over two
months so the next thing I'd like to do
is is to give you a sense of what is
writing an app with with s4 there are
three particular components you have to
you have to you have to implement so one
is the adapter in fact you don't have
you don't really have to to implement
the adapter you could do that directly
in the in the app class but it becomes
cleaner if you it's easier if you just
do that in an adapter class to make the
transformation of incoming events in 2
in to s four events so that's why we
typically recommend that that you do it
so for this adapter that is a lot of
rapper Cody here for you establish in
connections and so on I omitted all that
the important part is that is that you
create a new event we put a few there
with looting a label name and that is
the the content of the event and that's
supposed to be the name of a person by
the way it is not supposed to be a very
useful examples is to give you a sense
of how you do that now once you have
that adapter the next thing to do is to
implement the application or tell s4
what is the actual topology that is
going to process those those events and
in this case we have one processing
element that is called that's called
hello p
and we have one stream so this is
streams coming is coming from the
adapter as I as I have just defined and
and that processing element which I
haven't shown you yet will will process
it one interesting point here is that
when i'm defining the strain i have to
define what we call the key finder the
key finder essentially extracts the key
from from from from the event and that's
again what we use to hash and define to
which partition the event is going to go
so finally we have the hell ope it does
something very complex which is printing
out hello + that the name in the event
so in this particular again in this
particular application all all you're
doing is taking a name sending that as
an external event processing that
through the adapter for something that
each one s for event that contains the
name and and and bring it out so there
is one single p but the idea that he
could have multiple in yukon that you
can connect them in in various ways so
about Piper I mentioned that we decided
to go with with a different design
compared to what we did it well compared
to what was done initially one of the
points that we realize as as as I
mentioned is that state laws upon loud
crash was was a problem that could have
been state accumulated over extended
periods of time and and that was a
problem a rigid communication layer so
everything was implemented with with UDP
and then we would like to have at least
a possibility of using TCP so we
implement a different communication
later that allowed for 44 different
protocols the third point which is
pretty subjective is it would it was
hard to use the bug and deploy it's hard
to say exactly why that's the case but
we have what we have done is we try to
automate a lot of the problems that
people have identified and have have
told us about in particular with respect
to our future deployment how to run in a
in a put an application to run in
necklace and the last one it which is no
regression test and you shouldn't be
like that at all and so we have fixed
that so this is our the improvements i
have mentioned so we now we have dynamic
coupling of applications so you can have
an obligation exposing a a stream and
and and other applications connecting to
that stream and processing and
processing those events we can have
communication via DCP so that helps with
things like throttling with transmission
flow control etc and with respect your
fault tolerance we provide as I
mentioned some limited form of all
torrents so we have checkpointing in an
old failover but let me let me stop
talking about as for now and ask this
question which has a pretty obvious
answer which is all other ways that are
out there of achieving low latency one
of one of the other ways that we have
investigated what was this incremental
processing along the lines of what
percolator proposed in in percolator
that the idea is to use essentially two
concepts distributed transactions and
observers running on on top of big table
to to implement incremental processing
the use case dimension is related to
just the search index and essential
preparing documents to be inserted into
a into the live index they want to
choose the daryna in an online manner
because previously and I suppose that
most search engines we're doing that at
some point sorry to tell these days but
you would you would you would do your
collection and every now and then you
would you know perhaps in cycles of
several hours you would regenerate your
index so the delay between getting a
document crowing a document and make it
available for for search queries was
pretty long and so with this you can you
can reduce that time between crawling a
document or a new version of a document
and make it a very in the index 22
seconds the project omid that that are
that I will talk about is about is only
about transactions
so we focus on the transactions aspect
of that incremental processing but why
transactions so with traditional
traditional MapReduce we have we have a
data set with process that data set and
we generate an output what if what if
instead of doing that I just do my
processing on top of the data itself so
i generate an initial data set let's say
and as I receive updates I I can just
run them and modify directly my my my
shared state so and that's the whole
idea so I can I can do the updates
directly into a into into this yard stay
and and I'd like the properties of a
transaction because I might have a large
number of clients and there is
concurrency into into making this into
making these updates now if I compared
to the way that s for does the
processing of events alright so again
you can see what we have just discussed
as the processing of events i'm
receiving these events and i'm i'm
running whatever result of the
processing of that event a modifying
implementing it directly into into the
shared state and with s for data lives
in memory so each of those processing
elements i mentioned has its own state
and that's that's not shared and so as
four platforms along the lines of s for
a very good for for applications that
can live with that with that constraint
because we can do that can do things
very fast I with this incremental
processing we bring the computation
closer to the data and we have this
large data sets that are very operating
they were operating with so we have this
disadvantage compared to ask for that we
have this large state but we are
probably paying the penalty of a a
slightly higher latency when we're doing
this processing of a of events and as I
said oh meet targets lower latency 444
transactions a this is roughly the omen
architecture we have we have chosen to
avoid having a a Java heading locks we
use a centralized Oracle that we call a
status article so
that entity that guy there is
responsible for making decisions about
what transactions commit and what row
success do not commit we do not we do
not make modifications to to the
database we're using in in our
implementation we have used HBase which
is a clone of big table and there is a
third a third thing that we do which is
we try to replicate as much as possible
the state of the Oracle to the kind so
that the client can make it sound
decisions about about how to read in how
to modify state in in the database so by
doing that by moving part of the state
by removing the sellers Oracle out of
the critical path we are able to support
a higher number of coins because
obviously the satyrs Oracle becomes a
becomes a bottleneck right so it's one
single entity and everyone has to go to
it for for processing transactions so
I'm not going to be talking about about
the protocol so if you're interested we
can talk more about this other offline i
can give you pointers the one thing I'd
like to show is this is graph which
illustrates that we can get free high
throughput with with this game data that
we have shown the different curves
correspond to two different sizes of
transactions expressed as a number of
roles that we do we update one use case
that has been mentioned in in in a
recent talk by one of the guys that I
used to work with during hadoop summit
was this case of a news Rica use
recommendation so the idea is that users
with the similar interests are closer
and when you receive a new article you
check which of those clusters might have
might have interests in that in that
particular article and you essentially
recommend that the article to the users
in that particular cluster the problems
that transaction solve here our first
concurrent operations reconfiguring the
cluster so you could have multiple
clients multiple clients and
reconfiguring the clusters of the
application and that could lead to
inconsistent
also increase grease come while the
clusters are being a being reconfigured
okay so so that's pretty much how I had
to say to summarize what I had talked
about I discuss online processing the
goal is is in very few turns to receive
events and make them ready for
consumption fast you don't want to wait
for a next psyche of a MapReduce job to
make it available for for consumption in
particular discuss two techniques stream
process where events are processing
against a small amount of local memory
and we can make that very fast i think
the last time we rechecked you have
we're getting like over 250,000 events
per node per second and and other
technique was incremental processing
where instead of having one small amount
of local state / processing element I
have a large shared state where
transactions operate against and and I
could and in that case that i have that
shared state i have to pay the price of
a slightly higher latency but i can
still call that online because i can do
that in a in a within the order of
seconds so some acknowledgments these
are folks that have put a huge amount of
effort there and and have done actually
most of the job so so yeah thank you and
I'll take questions if there is any
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>