<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning for Mechanism Design: Learning Payment Rules via Discriminant-Based | Coder Coacher - Coaching Coders</title><meta content="Machine Learning for Mechanism Design: Learning Payment Rules via Discriminant-Based - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning for Mechanism Design: Learning Payment Rules via Discriminant-Based</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w0AXa0Ca5vI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so we're very happy to have today John
ley from Harvard University he's a
postdoc candidate for this upcoming
academic year and he's going to be
talking about machine learning for
mechanism design all right thanks yep so
I'm going to be talking about some work
we've been doing that ties in machine
learning methods to mechanism design so
here's just a brief outline for the talk
I'll start out with some background on
mechanism design then I'll move to the
machine learning side and talk about
structural classification and support
vector machines and then in the third
section I'll tie together the structural
classification with mechanism design
I'll move on to some experimental
results and some more recent work that
we've been doing on trying to make the
training problem more tractable in our
formulation and then I'll end with some
brief remarks on my overall research
agenda so to start off one way we can
think about mechanism design is that
we're doing optimization but the inputs
to our optimization problem are private
and in particular they're possessed by
self-interested agents so some practical
examples of mechanism design we can look
at the faa's problem of trying to
allocate landing slots at congested
airports so here are the self-interested
agents are the airlines and the private
information are the airlines values for
different landing slots or combinations
of landing slots so similarly we can
look at the problem of the FCC trying to
allocate wireless spectrum so here the
self-interested agents are the cellular
companies and the private information is
how much value the companies have four
different portions of the spectrum and
the FCC school might be to try to
maximize the total value of the spectrum
but they don't know the true values so
we need some way to to get the companies
to elicit these values and then finally
in the university setting you can think
about course assignment as a mechanism
design problem so here are the
self-interested agents are the students
they want
try to get the best course schedule as
possible and the university is trying to
get the students to simultaneously
submit their preferences and also
perform some optimization over the
submitted preferences so more concretely
we can look specifically at the landing
slots example so here we have three
airlines and these are their sort of
private values for different landing
slots at a given airport so here we see
that we also give values for
combinations of landing slots so these
landing slots could exhibit
complementarities in which case if I get
to if I get both landing slots the total
value is higher than the sum of the
individuals now the mechanism asks each
of the airlines for a report of their
values so here American Airlines says
these are my values for the landing
slots and the other airlines also
provide their values so the mechanism
operates on these reported values so it
doesn't know the private values and the
reported values could be different than
the actual values and what the mechanism
does is it looks at these reported
values and it computes two things so the
first thing it computes as an outcome so
it assigns the landing slots to Airlines
and the second thing it computes is the
payment for each airline so in this talk
we're going to be looking at the setting
of mechanism design with money so we
assume that we can use payments to
incentivize the participants to report
their information truthfully you can
also think about mechanism design
without money but that's that's going to
be beyond the scope of this talk so the
main goal here is we're going to try to
make it so that the mechanism does two
things we want this outcome to maximize
some objective function all right so for
the government we might want to find an
assignment that maximizes social welfare
or the total value that these landing
slots are creating the second thing we
want to do is that we want these reports
to align with the true reports we're
going to want to create the mechanism so
that airlines are incentivized to tell
us their true valuations
yes we get a soft is it depend on this
father be yeah right so um I'm going to
be looking at the setting where the
payments are done after the reports so
you don't have to pay anything to submit
your evaluations at the end I'm going to
compute two things I'm going to compute
the assignment and then how much you pay
for whatever you get you can also think
about mechanisms where people are
required to put up some amount of money
in order to even participate in the
mechanism but in this setting we're
going to be looking at payments after
the fact so more formally in the talk
we're going to use visa by to denote an
agent's prep private information so here
V sub 1 might be and Airlines values for
all the different landing slots and
combinations of landing slots we're
going to use omega to denote the space
of possible outcomes so in the landing
site example this is an outcome will be
an assignment of landing slots to
Airlines we're going to use F to denote
the outcome rule and T sub I to denote
the payment rule and as I mentioned will
want two things we want that F has good
performance with respect to some
objective function so here we might want
an outcome that maximizes total value or
welfare and then we want F&amp;amp;T to work
together so that we have good incentive
properties in particular typically we
want we want that it's always in an
agent's best interest to report its
valuation valuation truthfully so here
what we see is that this is the agents
value when reporting truthfully minus
its payment this is the agents value
when reporting something else and it's
payment when reporting something else
and we want the its value its value
minus payment when it reports truthfully
to be greater than greater than its
value minus payment under any other
reports this seems like a good thing but
why is it actually important
if we get better social welfare out of
some scheme is not incentive compatible
why isn't that better right so so you
can that there's sort of a couple
answers that question so one answer is
that suppose you have some mechanism
that is in truth or that isn't incentive
compatible so then now you have to start
thinking about well what will how will
agents actually believe behave when they
participate in this mechanism so you
need to start thinking about the
equilibrium of whatever system you've
created so that's one thing so in terms
of analysis it becomes harder to
understand exactly predict what's going
to happen in your mechanism right so
depending on information different
agents might behave differently another
another advantage is that if you have
incentive compatibility then you're sort
of not a you're not giving an advantage
to more sophisticated agents it's
actually pretty straightforward to
participate in your mechanism right so
if I'm one of these Airlines I know that
the best thing I can do is to just tell
you my true valuation I don't have to go
and think about well what what are other
agents valuations and things like that
makes life simpler for everybody right
so on the issue so as I mentioned
mechanism design is looking at this
optimization problem when the inputs the
optimization are private and so it's
challenging because we have this
simultaneous optimization over an
outcome roll and a payment rule they
have to work together to create the
right incentive properties and so on in
terms of theory we have good
understandings of sort of simple simple
settings right so we if an agent's
valuation can be described by a single
parameter then we have a good
understanding of what what mechanisms to
use what good mechanisms are or if the
objective is welfare maximization so if
we want the outcome role to maximize the
total value we can use the
cg mechanism and that says that will
select the outcome that maximizes total
value and the payment role will be
something that charges the externality
that an agent is imposing on the system
and if you combine these two things
you'll get you get incentive
compatibility now one thing here is that
we're assuming that this problem is
computationally feasible which in some
cases it's not so finding an allocation
that maximizes welfare can be a
computationally hard problem and so
things break down when you look at say
algorithms that approximately maximize
welfare you can try that and use the
same payment rule but then that breaks
the incentive properties that you have
so you really need this exact
maximization here for for incentives to
work out exactly in practice things are
typically more complicated we have we
have valuations that are multi
parameters so in the landing slots
example Airlines their evaluations
aren't described by just a single number
they're described by these different
numbers for different landing slots and
combinations of landing slots and also
we might have different objectives
objectives that are different than
welfare maximization so even if you
think about the government the
government might want to try to maximize
welfare but also preserve some minimum
level of competition so when we look at
these different objective functions it
becomes less clear on the theory side in
terms of what what sort of mechanisms we
should use so the goal of this talk is
to try to use more computational
approaches to design mechanisms for
these more complicated settings so it
just has a couple of motivating examples
the one example where the theory doesn't
give us an answer about what to do is
the setting of multi minded
combinatorial auctions so a
combinatorial auction is a setting where
we're auctioning off multiple items at
the same time and multi minded just
means that each agent is interested in
specific subsets of items so here
American Airlines
is interested in this schedule and has
value 5 million for this schedule and
this other schedule it has 2 million for
this schedule and then 8 million dollars
for this schedule so there's sort of
these subsets of items that each agents
interested in and they have a value for
each subset of these items now let's
suppose that our objective is to
maximize welfare so we can use the VCG
mechanism but even in this sort of
restricted combinatorial auction setting
finding the welfare maximizing
allocation is np-hard so as I said you
could use you could try to use the VCG
payment role with an alkenyl that
approximately maximizes welfare but
that's no longer incentive compatible so
the question here is well what mechanism
should we use if we're operating in this
setting as a second motivating example
let's suppose we're looking at a landing
site example where we're out of rural
airport so there's only three three
slots a day and the government wants to
make sure that there's a competition in
this at this rural airport so it wants
to say that each airline should be
allocated exactly one slot so what we're
looking for is an assignment from
Airlines to landing slots and the
objective is we're going to try to
maximize the minimum value obtained by
any airline so we're going to look for
the assignment that maximizes this this
minimum value which we term the a gala
tarian assignment so in this setting vcg
doesn't apply but for a different reason
the objective is no longer welfare
maximization we're not summing over the
values of all the airlines we're trying
to maximize the minimum value obtained
by any airline and so in this setting we
can again ask the question about what
mechanism should we use if we're looking
at trying to use this looking at this
objective function
so here's a high level view of the
computational approach that we're going
to take so we assume that we're given an
outcome rule so we're given some
procedure where if i give you reported
valuations i get the outcome that's
computed and i also assume that I'm
given a distribution on the possible
values of the agents now we're going to
apply machine learning methods to learn
a payment rule that works well with this
outcome rule so one caveat is that we
need this distribution on values here
but the advantage is that we'll be able
to handle these al criminals that may
not be that you might not be able to
specify analytically so they might just
be some heuristic algorithm that finds
an allocation with good welfare so we're
going to treat the outcome role as a
black box will also be able to handle
the case where there there are certain
al criminals where you just can't make
them incentive compatible there are no
payment rules that will make certain
alcaman rules incentive compatible so
we're going to define in a specific way
sort of relax relaxing away from exact
incentive compatibility so I'll get into
the exact definition later on but we're
going to try to learn something that has
good incentive properties but may not be
exactly incentive compatible and by
drawing these connections between
machine learning and mechanism design
will be able to leverage the
advancements that are going on going on
the statistical machine learning side of
things agents cool right so we we
basically have we have to assume that if
we run our mechanism the agents
valuations are being drawn from this
distribution and that we know that
distribution so yeah so they're sort of
you have interesting questions about
where do you get this distribution and
things like that but we're going to
assume this in our work
you generally need that or do you mainly
need that for objective some of
subjective factions have these action
articles right so we yeah we it is def
it is something that we will need in
order to apply the this machine learning
type approach so ultimately what this
ends up doing for us is this will let us
generate training data in the machine
learning sense so we will need it
regardless of the exact form of the
outcome role so there has been other
work that takes computational approaches
to mechanism design in particular
there's work called automated mechanism
design where they frame the problem
mechanism design in terms of an
optimization problem where you're
optimizing over both the outcome role
and the payment rule simultaneously so
the main difference here is that we're
assuming that you give me an outcome
rule and I'm just trying to learn a
payment that works well with it so
brendan has some work that looks at
taking any approximation algorithm and
turning it into a Beijing incentive
compatible algorithm with the same
approximation factor so the main
difference here is that the objective
function here is welfare and we're sort
of looking at cases where the objective
function is coming through via the
alcohol that you give us there's work
that looks at learning voting rules from
examples so given examples of how a
voting rule decided the outcome of an
election kind of recover the form of
that voting rule but this work doesn't
really look at incentives at all it it's
a pure sort of machine learning
perspective on the problem there's also
work that looks at using kernel methods
for pricing and clearing combinatorial
options so in terms of technique we also
use kernel methods from machine learning
but the problem is that we're trying to
solve are very different we're looking
at a mechanism design problem whereas
here they're looking at take given given
a instance of a comment or two are
locked and I want to find market
clearing prices
so so the the techniques should the
techniques share similarities but the
the targets and settings are very
different so just some more details on
exactly what these what our
computational approach involves so we
assume that we have this distribution on
values we're going to sample a number of
values so given this distribution will
sample M possible value evaluation
profiles so here V superscript 1 that's
a value for each agent so this is like a
report an entire set of reports for the
agents now we assume that we're given an
outcome rule we're going to apply the
outcome role to each of these valuation
profiles that we've sampled so we'll get
a valuation profile outcome will get a
pair of valuation profile outcome and
we'll have all of these pairs now we're
going to treat this as training data in
some machine learning problem and we're
going to try to learn this mapping from
valuation from valuation profile to
outcome so ultimately we view this as
training data we're trying to solve this
prediction problem of given evaluation
profile what's the outcome that this
that this outcome role would have
computed and then from the form of this
learned classifier will back out a
payment rule for that works well with
thee with the outcome rule that we're
given um so I'll get into more specifics
later but this is sort of the high-level
view yes get inside the black box but
you want something the particular
formula that you construct
yes exactly so you can think of it as
well you know it's it's cut one thing
people ask is well we already know what
this outcome rule is why are we solving
this sort of prediction problem to
figure out the alkenyl well the answer
is that when we do this learning we're
going to restrict the form of the thing
that we can learn and then we're going
to get the payment role from that
restricted form and if that restricted
form allows us to predict the actual
alcaman role well then we'll have
something that has good incentive
properties if it doesn't then then we
won't have access to the distribution
only as my sampling because if you're
near the distribution you would not need
something itself you could just solve
this proc summation problem of I mean
close something close to F which has the
form that you want right you also
assuming that you were access to the
distribution is through sound right yeah
we're assuming that basically yeah we
can ask ask this black box give me a
sample from the true distribution so in
the second part of the talk i'm going to
talk briefly about the machine learning
side of things just give you enough
background so that we can have enough
information to draw these connections
that we're trying to make so structural
classification is the problem of
learning a mapping from some input space
to some structured output space Y where
Y has some structure in that it can have
a large number of elements but it it has
some structure so the easiest way to see
this is through a couple of examples so
in part of speech tagging we're trying
to give in a sentence we're trying to
figure out the parse tree for that
sentence so in this case the out the out
why is going to convey that will be the
space of all possible parse trees for a
given sentence you can think about OCR
in the context of not just a single
character but an entire word and so here
we're given an image of a word and we're
trying to output exactly what we're
in the image so in both of these cases
the output space is large here we have
parse trees and here we have possible
words but they're also structured right
so here we're going to only be looking
at valid parse trees and here maybe we
only look want to output words that are
in some sort of dictionary so one way to
solve these structural classification
problems is the following so we assume
that we have some training data X
superscript K&amp;amp;Y superscript k so we have
some inputs that we're trying to learn
from and we're going to learn a
discriminant function so for every given
input and every possible output will
learn something we'll learn something
that assigns a score to that pair the
prediction then is if I give you some
input the prediction will be the output
that has the highest score all right so
I have this I have this discriminant
function that says given this input
output pair how well do they fit
together and to make prediction I'll
just say that I'll output the the target
that has the highest score so in
particular what is the form of this
discriminant function it's going to be
the dot product between some weight
vector and some feature vector so the
feature vector is something that we
specify so this encodes our domain
knowledge it combines the input with a
given output and the weight vector is
what's ultimately learned by the
framework all right so we design we give
you the feature vector we give you some
training data and the framework should
output a weight vector which will then
define your discriminant function and
give you give you a classifier so now
given an input I can run my discriminant
function over all possible outputs and
tell you my prediction and as is typical
and machine learning will want to avoid
overfitting which means that we're given
that we are trying to learn from this
training data but ultimately what we
care about is performance on data we
haven't seen before we don't actually
care about how well we do on the
specific training data
so one way to avoid overfitting is by
using the idea of maximizing margin so
if you look at this optimization problem
well this is saying is that we want the
scores for the correct output so these
are our training data pairs will have to
score for the correct output to be
higher than the score of any other
possible output so we want our
classifier to be correct but not only
that we want we want the two to be
separated right so we want the score of
the correct target to be much higher
than the scores of other targets and so
if we can try to maximize this this we
we can we can avoid we can avoid
overfitting so as an example of what we
mean here by margin suppose we just have
two classes of data we have this we have
this the input can be either negative or
positive so we're going to define the
margin as the distance from the boundary
to the closest point on either side so
in this case if this were our decision
boundary everything on to the left is
negative everything to the right is
positive our margins going to be the
distance from this boundary choose the
closest point on either side now what
you see is that if we rotate this
boundary we can make the margin larger
so increasingly if we rotate it this way
the margin becomes slightly larger and
then if we rotate it even more the
margin increases even more and the
insight we're going to use is that we
prefer we prefer boundaries like this
two boundaries like this we want
boundaries that have higher margin and
the reason is that and this will help us
avoid the overfitting problem and one
intuition for that is that we're sort of
making less assumptions about the unseen
data when we use this boundary versus
this boundary like if you look at this
boundary or implicitly assuming that the
points up here r minus and the points
down here r plus but here we're not
making we're making fewer assumptions
about the data and by doing that we'll
be able to avoid overfitting
so it turns out that this optimization
problem is exactly equivalent to
maximizing margin but and I won't go
into the details of that but if you if
you look there's one problem with this
formulation which is that it might be
infeasible right so there might not be
any weight vector that exactly that
always gives the correct targets higher
scores than all the other targets so one
thing we'll want to do is we'll just
want to introduce errors you can make
errors you might not always assign the
highest score to the correct target but
will penalize those errors so if you
want to make errors you can increase the
slack variable but will penalize the
slack variable in the objective function
and then this parameter controls the
trade-off between sort of how well
you're fitting the data and and how
large of a margin you're getting and so
this is something that will have to tune
when we actually run experiments so one
thing to keep in mind is that if you
look at these constraints there is
actually a large number of constraints
so I said why could be this large space
of possible parse trees or words in a
dictionary and we're saying that we have
to have one of these constraints for
every possible why and so if you want to
be able to solve this in a
computationally tractable manner when y
is large you'll need some sort of
separation Oracle you'll need some
efficient way of efficient way of sort
of finding constraints that constraints
that are currently violated and in
dealing with those so I'll get into that
a little later but this sort of right
now it doesn't right now it's not clear
that this training problem can be solved
in a very efficient way because of this
large number of constraints
so the main takeaway from this section
of the talk is that we have existing
methods for predicting large structured
output spaces and in particular these
max margin approaches have been
successful so on the two examples that I
mentioned max margin approaches are able
to improve upon the state of the art by
significant percentages so now I'm going
to talk about our main contribution
which is tying in these machine learning
methods to the mechanism design problem
so just as a reminder our goal is that
we're given an outcome role f and we're
trying to learn a payment rule t that
has good incentive properties now for
the rest of the talk I'm going to make a
few simplifying assumptions the first is
that there are no externalities so what
this means is that an agent's value for
an outcome only depends on the part of
the outcome it receives so American
Airlines they only care about the
landing slots they receive they don't
care about sort of the allocation of the
rest of the landing slots so that's
definitely a simplifying assumption but
it's something that'll make the
presentation easier we also assume that
the outcome over given is agent
symmetric so it is a black box but it's
not treating any of the agency in a
special way so if we rename the agents
and reran things we'd get the same
outcome if we make these two assumptions
we can look at just a single agent
classification problem instead of
predicting the entire outcome we can
just look at agent one and try to
predict the outcome to agent one will
learn a payment rule for agent one and
then we can apply the payment all to all
the other agents as well so if you our
methods do extend to the case where if
you don't want to make these assumptions
but it'll be easier to think about for
the rest of the talk we're looking at
this single agent classification problem
that will end up solving the problem
we're trying to solve but we'll be
focusing on focusing on agent one in a
special way yes the fact that you are
super symmetric
is symmetric how does it mean that you
can do the prediction separately for
instance if F is maximum over all agents
or minimum of religions who doesn't only
depend on the value of any one agent
even told symmetric right so so we're so
we're still we're not we're not just
looking so we're sort of still looking
at the we're looking at f so f is given
all of the values and it's going to
compute some outcome we're just going to
look at the outcome for agent 1 so we're
still looking at all the values we're
just looking at a specific part of the
outcome so originally we're so saying
given the values I want to predict the
entire outcome distribution is symmetric
right yeah yeah that's right so okay so
I guess you don't ok so you don't
actually so the answer to your question
is you can't tell if there's so much
they could have been drawn from the same
distribution right or they might be
drawn from the same distribution but in
general you don't you we don't need that
we don't need the distribution to be
symmetric you can run this problem for
every agent all right for every agent I
want to predict your outcome for the
second agent I want to predict your
outcome we do need that the we do need
we do need no externalities for that we
don't we don't need agent symmetry for
that the problem if you don't have agent
symmetry then you need to do a learning
if you don't have agent symmetry or you
don't have the symmetry in the
distribution you have to do a separate
learning for every agent if you have
both of those you can just learn for one
agent and apply it but you could the
methods I present it's not going to you
can think of it as sort of i could apply
this to every agent and it'll solve the
asymmetry in the distribution or
asymmetry in the alcohol
so we're going to use omega to denote
agent ones allocation and just drop this
Omega 1 just to make the just to
declutter the notation so here's sort of
a well-known a well-known
characterization of incentive compatible
mechanisms but it will turn out to be
very useful in our setting so let's fix
the valuations of other agents and let's
see what american airlines can do by
varying its report all right so it could
report its true value V sub 1 that'll
give it some outcome and charge it some
payment it could report some other
valuation v1 Prime it'll get some
outcome maybe it's the same and it'll
get some other payment now you can go
through this exercise for all different
possible reports and basically by
changing your report you can change your
outcome and change your payment so the
first observation is that if your
mechanism is incentive compatible these
two payments have to be the same right
so it can't be the case that you can
report two different things get the same
outcome and be charged different charge
different things because in this example
if I'm if my true value is v1 then I'll
just report v1 prime I get the same
outcome and I'm charged less so this
payment up here has to be the same let's
just say they're both 4 million so
that's the first observation so the the
second observation is that now we now we
basically have a way by changing our
report to get different outcomes for
every outcome there's a fixed payment
right we said the payments can't differ
for a given outcome so essentially what
an agent's the signing is like okay this
is my evaluation I can get all these
different outcomes at these different
outcome payment pairs there's basically
thinking well what should I report to
get the outcome payment pair that
maximizes my utility so now if the
mechanism is incentive compatible when
the agent reports V sub 1 the outcome
that I the outcome payment
pair that I output has to be the outcome
payment pair that maximizes agents one's
utility when its valuation is V sub 1
alright so suppose suppose not suppose
that when the agents value is V sub 1 it
actually prefers this outcome payment
pair well then it will report V 1 double
prime instead of the one and your your
mechanism is not going to be incentive
compatible so the the main takeaway is
that if we know suppose that we knew for
this outcome the payments 4 million and
this outcome the payment is 8 million
now when an agent makes a report all we
have to do is we have to pick the
outcome and payment pair that maximizes
that agents utility under its report
alright we have these i can give you
this outcome for this payment this
outcome for this payment and you're
telling me this is your evaluation
function for F to be incentive
compatible I have to I have to give you
the outcome and payment pair that
maximizes your utility so in some sense
you can think of this payment function
so this payment that map's an outcome to
a payment you can think of that as fully
determining your outcome rule if you're
in if your mechanism is incentive
compatible like if I know the payments
i'm charging then i have to then I know
the outcomes that I have to give you so
in some sense you can think of this
payment role as a classifier for the
outcome rule and so what we're going to
try to do is basically reverse this
right so we're saying that if I knew the
payment rule i could tell you what the
outcome rule would be if you were
working with an incentive compatible
mechanism well the problem we're trying
to solve is we're given the outcome rule
and work we're trying to find a payment
rule so we're going to try to say that
if we have an outcome rule we're going
to try to say that if I can learn
something that predicts your outcome
real well then I'll have a payment roll
with good incentive properties so we're
basically trying to burst as observation
that the payment roll pins down the
outcome over when your incentive
so as I said we're trying to learn this
predictor we're trying to learn a
predictor for this outcome rule that
maps valuations to outcomes for agent
one we're going to use the structural
classification ideas we saw before and
we're going to try to learn an
admissible discriminant function so what
that means is we're going to learn a
discriminant function that has a
particular form so the discriminant
function will take the value eight the
valuation profile and a possible outcome
that agent one could obtain and assign
some score to that we're going to say
the discriminant function has to have
the following form it has to be
decomposed as agent ones value for the
outcome minus this term on the right and
this term on the right we're going to
say it can't depend on agent ones report
so the intuition here is that for it can
depend on the outcome but for the same
outcome this term on the right shouldn't
depend on agent one's report and that's
sort of the previous slide we saw that
and at the payment for an outcome
shouldn't change if the outcome
shouldn't change depending on an agent's
report for a given outcome the payment
has to be the same and then so this is
this is something that we know this is
the feature map that we specify this w
is what we're learning in our framework
and then the payment rule that we use
will just be this term on the right so
we're going to learn this weight vector
that parameterize 'as this discriminant
function and then the payment rule that
we that we end up using will just be the
right hand side of this discriminant
function
and the claim that we want to make is
that if you learn a classifier of this
form and it's an accurate predictor of
the outcome of Aleph then this payment
rule that we back out will be a payment
rule that has good incentive properties
and sort of orthogonal to that the
payment role will also be agent
independent right so the payments will
depend on the outcome so if you're an
agent you can change your outcome and
change your payment but for the same
outcome you can't change the payment so
sort of local changes in your report
don't make any difference unless they
change the entire allocation that you're
given so this in some sense gives us
local stability of her payment rules so
if you think about the auction setting
like the second price payment rule has
this local stability property but the
first price auction doesn't because you
can you can keep winning but lower your
price in a first price off so we have
theorems that make the relationship
precise so the first theorem is that if
our learn classifier is exact then our
learn then if we pair the outcome role
we're given with the learn payment role
that pair is incentive compatible so the
proof is pretty simple so suppose that
when agent one reports truthfully the
outcome is omega and when agent one
reports some something else it reports
v1 Prime the outcome is omega prime we
want to show that under the payment rule
that we learn the agents utility under
outcome Omega and reporting it's true
valuation is at least its utility when
reporting v1 prime so the first step is
to notice that this is the exact form of
the discriminant function that we've
learned the second step is to use the
fact that our classifiers exact so exact
means that the score for the true
outcome Omega should be greater than
should be at least the score for any
other outcome Omega Prime and then we'll
just reverse the definition of the
discriminant function and this and this
this is the right hand side that we're
trying to show before so basically this
proof just relies on the way we we
forced our discriminant function to have
this form and if the discriminant
function is exact then we get this
relationship the score for outcome Omega
is at least the score for outcome Omega
Prime
so you but it's not it's not
distribution free we need the classifier
to be exact so it's sort of it actually
predicts the correct outcome for every
every possible report right so yeah
right right yeah so you can think about
it about it as distribution free or it
in some sense has to hold for every
distribution the second theorem that we
have just works in the opposite
direction so it says that if you have an
incentive compatible outcome rule then
there exists an exact admissible
classifier for some feature mapping so I
didn't get into too much details about
exactly what this feature mapping is but
this is just saying that if you have an
incentive compatible alcohol if you have
a very rich sort of family of functions
that you're learning over your you'll be
able to learn an exact classifier so i
won't go into the details of this
theorem but i want to get to the third
theorem so the first two theorems look
at cases where we have exact incentive
compatibility and exact classification
so here we're going to look at cases
where the classification is an exact but
it's sort of how tries to low tries to
minimize some sort of error function so
in particular let's define our the error
of a classifier as being the difference
between the value of the discriminant
function on the on the thing that the
classifier predicts minus the value of
the discriminant function on the true
target right so if we're predicting the
true target then this error will be 0
now these two terms will be the same but
otherwise this term will be positive if
we're making an incorrect prediction
there'll be a there'll be a difference
between the score of the prediction and
the score of the target that we're
trying to predict
I'm the mechanism design side we can
look at regret which we can say is the
utility of an agent for its best report
so it could report anything what's the
utility of its best report- its utility
for for reporting truthfully and the
theorem is that if we have an admissible
classifier that minimizes this
generalization error so this error
that's dependent on the discriminant
function then we'll find a rule that
minimizes expected regret the payment
role that we learn will minimize
minimize expected regret so even when
you don't have exact classification if
you minimize this error function then
you'll recover a payment rule that
minimizes this for grin which is which
quantifies how much an agent can gain by
Miss reporting and again we we still
learn payments that our agent
independent which has this nice local
stability property and I've yeah as I
mentioned will be able to relax away
from exact incentive compatibility in
this sort of specific way of minimizing
expected regret so just as a review
we've given a method to learn a payment
rule for any outcome rule F and this
allows the mechanism designer to focus
on the outcome Bowl so you give me the
alcohol as a black box in addition to
the some way to sample the distribution
and this computational approach will
give you a payment rule that has some
reasonable properties and we have and we
move smoothly away from exact incentive
compatibility
okay so i'll briefly tuck your last
slide about sort of so you're really
nothing about a designer who is given
the black box and has to come with
famous but but you're trying to simplify
either their task in designing a
mechanism by saying by saying you can
design just the ephemeral instead of
buying both parts right so is there some
some way we can think about I like if
I'm designing an outcome rule for me to
know that there has two going to be a
payment rule that makes it approximately
instead of compatible and I have my
outcome Valastro satisfy certain
property right it has to be managed all
right and once I've done that like
coming up with the ones I've got now can
roll the satisfied that then coming with
the right payment roll it's pretty
straightforward so is there some some
way to think about situations where sort
of having this having these kind of
result will will where it's easier to
design just the outcome role than to
design both at once right so I so so one
thing is so in I think it's true that in
sort of simple domains recovering the
payment rule so if you have these
monotonously properties you can recover
the payment roll easily I'm not sure if
that's true in sort of multi prep like
we know what the payment role should
look like but it's still not clear
exactly what it is is that right it's
out of sight certain properties also
seems like the reasons why it's hard to
actually figure out with a payment role
should be are also the reasons why it's
hard to figure out whether there even is
a payment rule that may come along so
it's under compatible right so so I'm
wondering whether there are situations
where designing an outcome rule where we
think that good incentive properties
exist right but we can't actually find
the payment rule directly but it's
actually any easier than just coming up
with a book or the ones or to put it
another way I hat does this scheme give
me any guidance right so I don't
I don't think it gives you give you a
really crappy anka right you'll still
producer yeah I'll still produce the
payment right know how good a job I did
right now i think that's that's a good
question i don't think this gives you
any guidance beyond what's already known
in the economics literature about
properties like mana to nisti of your
outcome rule and things like that
experimental framework right that's true
I could say I have some heuristics I
don't know how it works I could design a
bunch of outcome rules then I could roll
you can look at what grati and gable I
get a good classifier right not there
right that's true you sure distribution
I didn't see talk about dependence of
your results on the distribution I mean
the result that he did give was
dependent maybe on support of the
distribution really had to all right any
other distribution on the same support
so is there any result it depends on the
actual distribution yeah so this so I
didn't I didn't exactly right out what i
mean by expected regret here but this
will depend on the distribution so this
this is going to integrate over the the
distribution that were given
distribution is all the true values
right right so you basically assume that
they're either way to value of the
agents right right so there's um i guess
there's a difference between knowing
their true values in every instance and
knowing the distribution that their
values are being drawn from right so
there there is there there's definitely
yeah i mean it's true there there's
definitely this problem of this question
of where do you get the distribution
from like you could argue that you sort
of get the distribution from the past
where you've maybe run some incentive
compatible mechanism that doesn't have
good properties
you can imagine doing that but in
general when we're thinking about I mean
it's a sort of knowing the distribution
is a pretty standard assumption in the
literature on designing Asian incentive
compatible mechanisms but no I I agree
there is this question about where does
this distribution come from I'm Brendan
Oh family parametrized family about
companies but i use i wrap this scheme
in another layer of machine learning or
in the parameters yeah presumably you
can because out of this thing you get
some measure of how well you're doing
and there are lots of methods for sort
of doing local search over parameter
spaces and things like that so that's
definitely an interesting direction to
think about okay so I have five minutes
left so i will quickly go through the
experimental results so we implemented
this for the two settings that i
discussed earlier the multi minded
combinatorial auction setting and this
gala tarian assignments heading so the
multi minded combinatorial auction
setting we use an outcome role that is a
greedy algorithm that tries to maximize
welfare so it's not going to exactly
maximize welfare it does so
approximately the egalitarian assignment
setting we look at this objective where
we maximize the minimum value that any
airline receives in both settings we use
what's called an RBF kernel so what that
does is that that map's this feature
space that we're learning over into a
highly nonlinear feature space so it
allows us to learn over very complicated
payment rules even though we're sort of
specifying something that's simple so
this is a standard thing that's used in
machine learning the benchmark we use
will be these vcg based rules
so well we mean by VCG based rules is
basically we measure the externality
that's being imposed by agent 1 i'll go
through this very quickly the
externality of the american airlines
will just be how much valued do the
other airlines receive when american
airlines is around and then if we
removed american airlines how much more
value to the airlines receive so here
the payment will be 11 million which is
here minus the 7 million which is here
american airlines pays four million
dollars now this rule is going to be
incentive this payment role is incentive
compatible if you're out criminal
exactly maximizes welfare but it's not
incentive compatible in either of our
two settings so here we're looking at
the regret of the payment well that
we've learned versus the regret of this
VCG based rule for the two different
settings so what we see here is that in
the multi minded combinatorial auction
setting we're learning a payment rule
that has regret that's similar to this
VCG based rule and in the egalitarian
assignment setting we're learning
something that has that has
significantly better regret and so one
reason why this might be true is that
here we're still trying to maximize
welfare so maybe this externality based
rule is still doing something pretty
reasonable but here the outcome role is
really not maximizing welfare it's doing
this a gala tarian maximizing this a
gala tearing objective and this vcg idea
seems to break down break down a bit
here another thing I wanted to note is
that in the combinatorial auction
setting it's actually computationally
intensive to train to learn this weight
vector W so if you if you remember we
have all these constraints so you have a
constraint for every possible element of
the set Y so in combinatorial auctions
that's going to be every bundle of items
that agent one could have received so if
you have n items you're going to have an
exponential
number of constraints so training is a
bottleneck here and the the last section
of the talk was going to in the last
section of the talk I wanted to talk
about sort of overcoming this training
problem so I'll just give a very brief
overview and then conclude so right when
we're looking at combinatorial actions
we have an exponential number of these
constraints in our optimization problem
and what we really need is an efficient
separation Oracle we want something that
picks out the element the subset of
items maximizes over this exponentially
large set but does so in an efficient
way so it turns out that we can do this
if we consider a class of valuations
which in the literature which has been
studying in literature by a couple of
papers and this is a valuation that's
graphical in nature so on each node we
have the items and then we have edges or
hyper edges between items and the edges
and hyper edges say that if I have both
of these items i get of 1 million dollar
bonus so the structure is completely
additive so if i give you just 9am you
got two million dollars if i give you
both 9 a.m. and 11am you got two million
three million plus 1 million i gave you
all these items then you get all the
numbers in this graph so if you look so
we have a way that if you work with
valuations of this type the training
problem is still you know you still have
an exponential number of constraints an
agent can still receive any subset of
items but we can solve that maximization
problem tractive Lee so we can do we can
have much more efficient training so i
won't go into the details of that but
i'm having to talk about it later if
anyone's interested and so i'll
summarize this part of the talk
we've drawn a relationship between
incentive compatible isms in structural
classification we've said that a good
predictor for the outcome role gives you
an approximately incentive compatible
payment rule and we've applied this to
multi parameter settings and settings
where the objective function is is
complicated and maybe not welfare Maxima
maximization in general i'm interested
in work sort of at the at the
intersection of machine learning
optimization and mechanism design i've
done some work on looking at the cake
cutting problem and looking at both
incentives in cake cutting and
extensions to cake cutting that consider
trying to optimize welfare in addition
to fairness which is typically studied
in cake cutting I've done some work on
this mechanism design theory trying to
understand can we have truthfulness and
fairness in combinatorial auctions and
more recently looking at the design of
optimal options where where agents can
choose whether or not they want to
reveal their identities to the system
and then finally I've done work on
computational mechanism design which
includes this work and also some work on
taking branch and bound search and
trying to use it for the purposes of
mechanism design by designing a
procedure that takes your branch and
bound search and turns it into a
monotone search procedure I've been very
fortunate to collaborate with the number
of people during my time at Harvard and
I just wanted to especially thank my
advisor David parks and thanks thank
everyone
question about the general setting off
using payments to get incentive
compatible results okay it seems like
this undermines the basic think that you
want to get by some subjective for
instance objectively to be a glatorian
so that everyone gets a minimum of
something but then you ask them to pay
for it which means that actually get
less than that so it's not the question
really they're good kids you're not
question all this sort of way of solving
the problem of truthfulness right you're
my object to that you're trying to chew
right so I think it depends a little bit
on your objective so if you're looking
at the gala tarian objective I think
that's a very valid critique if you look
at social welfare I think the critique a
social offer ultimately what does the
government want right the government
wants these items to go to people who
value them the most they don't actually
care about the exact utility of the
companies that they want to maximize
value right they don't care about sort
of value minus payment which is the
utility that these companies are
receiving so I think to your question
there are certain objectives that where
payments might not be as natural so the
segala tarian objective payments might
not really be the natural thing to do I
mean we looked at it here because it's
this interesting setting where we have
something that really isn't welfare
maximization and we want to see how well
things worked but you're right that in
practice you might not it might not make
sense to combine like a gala tarian
concerns with payments if you're really
caring about fairness like much of the
cake-cutting literature doesn't look
doesn't use payments at all okay
yeah I'll ground</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>