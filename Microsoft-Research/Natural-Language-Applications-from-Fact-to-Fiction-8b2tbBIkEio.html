<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Natural Language Applications from Fact to Fiction | Coder Coacher - Coaching Coders</title><meta content="Natural Language Applications from Fact to Fiction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Natural Language Applications from Fact to Fiction</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8b2tbBIkEio" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let's get started so as a former
student of Kachemak units my immense
pleasure to welcome hurt microsoft
research so where is not need of course
to introduce katieb she's done a lot of
work in generation summarization and
question answering so she's a professor
at the columbia university also the
director of newly founded institute for
data sciences and engineering and so
today should be talking about natural
language applications from fact to
fiction getting okay thanks the shell so
I'm going sorry I have a cold so I hope
but hope I won't cough too much I'm
going to be talking today about a work
that's been done in my group over about
a ten year period where we started
looking at data that was primarily about
the world fact and we have moved sort of
full circle and have recently been
looking at work that is fiction so i'll
be talking today about the work of some
of my current students who i have here i
want to acknowledge them up front but
you'll see occasionally they'll pop up
at different points during the talk it
also builds on the work of my past
students who are situated at various
places around the country and actually
around the world so i'm going to be
talking about work that we've done with
data that falls along a continuum from
fact to fiction so at the top you see
tax kinds of texts that are more factual
in nature they refer to things that
either happened in the world or they
report on fact as we know from
scientific experiment and as we move
down the continuum we come to data which
is more subjective
nature we're likely to get things like
opinions expressed it's likely to be a
bit more informal and when we get to the
bottom of the continuum then we have
data which is actually not about the
real world at all in any way and has to
do more with what people have written so
one could argue about whether scientific
journal articles or news are more
factual but in order to align the
continuum with the order in which we've
done the work i'm going to put news
first and if we look at the different
kinds of genres that we have here one of
the things that we can see is that the
language of genres is remarkably
different i'm going to be using a lot of
examples from references to Hurricane
sandy which for us in New York was a
pretty big deal and we still continue to
talk about it if you if you look at all
that comes down you can see that it's
quite different and even from the the
language that's used you can probably
identify which Donna is which but let's
just try it as we go through so you can
call out whether it's a social media
scientific journals news or novel so the
first one what do people think that
social media the second one scientific
paper the third one news yes and the
fourth novels okay now this just in
overview before we get into the talk
itself is a word cloud of the kind of
work that we've done in my group and so
you can see summarization is their right
front and center that's been a lot of
the focus of
work this comes this word cloud was
created from the abstracts of my groups
papers over the last I think it was
about 15 years in addition to
summarization we can see some other
things like generation we've done some
multilingual work so translation shows
up a little bit of work with patient I'm
not sure why speech is so large we I
guess that's all due to Michelle galit
here in front so our vision is to be
able to generate presentations that
connect things from events from these
different sources so that be able to be
able to connect information about events
opinions about those events personal
accounts about the same events and their
impact on the world we'd like to be able
to link to supporting science and
ultimately to link to fiction that's a
little bit more in the future so let's
start with news and as you all know
there's been a big effort on news within
the natural language community a lot of
work within the linguistic data
consortium on connect collecting news
annotating it making available to
research groups to use for their
research and for that reason a lot of
the initial project progress both in my
own group and in other groups around the
world was on news certainly in my group
a lot of the work that we've done has
been one summarization and shown here is
a page out of our news blaster system
which was developed a cheat almost 15
years ago a little bit less than that we
made it go live on 2001 and this here we
are looking to generate summaries of
multiple articles about the same event
so here you see one on hurricane sandy
and as we saw this is where the initial
example came from Hurricane sandy true
about 290 miles off the mid-atlantic
coast Sunday night and so forth now the
first question you may ask is why is
summarization hard well it seems to
require both interpretation and
generation of text furthermore it seems
to require doing that in unrestricted
domain we don't know ahead of time what
topic we're going to get text on and
what kind of domain information we need
we need to be able to handle those
documents though rebut robustly and thus
it seems that we need to be able to
operate without full semantic
interpretation and in the summarization
field this has led many researchers to
use what is called sentence selection
where sentences are selected out of the
input documents on the basis of salience
or importance and then the sentences are
strung together to to form the Sun
summary and this is a way to get a
system working quickly it also though
can lead to some problems where
sentences placed side by side may create
some misconceptions or they may have
missing information for example who is
being referred to in the article may be
very clear but when you put it in the
summary it's not so our approach at
Columbia has been to use sentence
selection but then to edit the selected
sentences and the kinds of things that
we worked on have been to correct
references to people that are in
felicitous so the first time you have a
reference to a person you'd like to have
a full reference so that we can
understand who the person is and if we
continue referring to that person
throughout the summary then we might
have a pro down so this was work done by
aunty nan cava compression
is still a big topic and perhaps even a
bigger topic now because when we extract
sentences especially for news they can
be quite long and so what we'd like to
do is to be able to remove extraneous
material from those sentences to make
them shorter so we have a concise
summary and particularly if we think now
about doing summarization on mobile
devices we might like something quite
short just a single sentence to appear
there this is a work that we an area
that we've worked in for quite a while
from hung engine in two thousand from
Michelle galley and his dissertation on
Anna's something that we're continuing
to work on now we given that a lot of
the work that we've done has been done
in a multilingual environment and we've
had to generate either summaries or
answers from translated texts which are
often dis fluent as a matter of fact the
first yes the first time we began
working in this area and we saw what we
were going to have to generate answers
from we were like are you kidding but of
course things have gotten a lot better
nonetheless when we do summarization or
question answering from translated texts
we have a lot more context from the task
that was not available at the time of
translation and we use that information
to make fluent sentences from dis fluent
translated ones and then we've been
doing work on generating new sentences
by selecting phrases from the input
sentences through a fusion now this is
walking a fine line it's easy to make a
good sentence bad if you extract that
somebody has written it there has been
quite a bit of work though now in the
field of text to text generation and I
won't go through it but you can see that
a lot of different people have been have
been looking at this problem in our
current work which is the work of couple
sarani were modeling text transformation
as a struck
should prediction problem so the input
is one or more sentences with parses and
the output is a single sentence with a
parse and we're doing it in a multi-view
structured prediction so we can take
different kinds of informations in into
account at the same time simultaneously
as we reword the sentence so we're doing
joint inference over word choice
constraints of that word choice using
information from Engram ordering and
dependency structure we do this in a
supervised fashion so we start with a
data set for compression and you can see
here one of the very long sentences is
in input and then an output we have
removed all of the information and white
so flying towards Europe yesterday which
may not be as important for the summary
as just the fact that Air Force fighters
were scrambling to intercept a Libyan
airliner so the framework that has been
developed can be used for different
kinds of text the text generation pass
it can be used for sentence compression
here the input is a single sentence and
the output will be a shorter sentence
with sale salient information and we use
as data the kind of data that I showed
in the previous slide so we have a lot
of cases where we have a longer sentence
in shorter sentences and we're using a
baseline from clark and lopata in 2008
who have done a lot of work and you you
can see here the two different
perspectives on the sentence the n grams
which are shown in yellow so this shows
you the pairs of words which are likely
to occur together and dependency
structures so we take dependency
relations between pairs of words and
these also form constraints on the
output we can take the same framework
and use it for sentence fusion simply by
changing the data set and by changing
some of
features that we use so here the input
is multiple sentences and the output
will be a sentence with common
information from both so it's a way of
taking multiple sentences in finding
what is most salient because it has been
repeated across sentences we use here a
data set that is created from
summarization evaluations I don't know
if people are aware of the pyramid data
which has been used for evaluating
summaries but here we have a case where
we have many different summaries of the
same data and it's been coded down to
the phrase level of what is common
across human summaries and therefore
should appear in an output and we use
that for the for training or fusion and
then we have changed the features in
some ways so we have some fusion
specific features for example repetition
maybe one of them and you can see an
example here where we take part of the
first sentence another part of the
second sentence go back to the first and
end up at the second to get a sentence
like a six years later independent
booksellers market share with seventeen
percent we're also using sentence fusion
for machine translation and here we sit
at the end of a pipeline where different
systems have done translations we've
been doing this in a joint project with
Martha Palmer Kevin night dangal day and
language way but I'm showing it here
with translations that are available off
the web and in fact we do often
experiment with that so you have a
Chinese sentence at the top the
reference sentence so you can see what
should have been translated and then you
see three different translations by
online engines Google Bing and systran
and no one of them is perfect but
nonetheless there are phrases in each of
them that are good and so our goal is to
be able to fuse these three sentences
and to take phrases which
can be used and improved the final
output so we do too tight types of
sentence level fusion one is what's
called a sentence level combination and
that's where we look at the output and
we want to choose the sentence that is
best for that we have developed a
structural language model that we can
use on the translations this is a super
tag language model and so it can capture
sort of long-distance dependencies
between the phrases and we use this to
rank the different outputs and choose
the one that is best followed by this
now that serves as our what we're
calling the backbone sentence and now we
use phrase level combination given this
best sentence to pick the phrases from
the different other translations that we
have and we use a variety of different
kinds of future based scoring functions
we do look at consensus in the different
translations on phrases we have
syntactic indicators on whether phrase
works well and then we have information
about the semantic role labeling the
source and the target translation so if
we go back to this combination we're
first going to choose one of them as the
best using the language model and this
would be the sister and one and now
we'll choose phrases from each of them
to decide how to put it together and we
use a paraphrase lattice which shows the
paraphrases between the different
systems I'm showing just this part of it
where we've got translations from each
of them and we're going to choose the
top as the best sort of n grand as we go
through and as we go through the second
part of it is unlikely to will choose
this because it gives us the best
it the syntactic features show it to be
the best in that case so what have we
learned from this work with monolingual
compression we get a five percent
increase in Engram recall when we use
joint inference with dependency
relations over previous baselines in our
case of multilingual fusion we get an
increase of one point in blue score for
combined empty output of course things
can go wrong as I said we're not relying
on how people wrote the input and in
fact one of the things that often goes
wrong is when we have multiple people
from the same family who appear in the
news so this example is from way back
when when news blaster was first picked
up by the press and was appearing in the
press and of course the journalists
wanted to find everything that was wrong
with it and on the death of the Queen
Mother and England news blaster had
Queen Elizabeth attending her own
funeral when we rewrote the references
okay so let's turn now to scientific
journal articles we're dealing with a
number of different kinds of articles
some are from nature they tend to be
mostly scientific we have a number from
elsevier as well but it's it's full text
the first thing that you might ask is
how our articles different we saw that
the language of genres was different and
we can see right away just from the
beginning page of it that we have some
structured information that we didn't
have before we have titles authors and
abstracts is shown here below we have
citations typically and of course the
language is different so one of the
first things we can see that we can get
out of a set of scientific articles
which we can't get elsewhere is a
citation network so if we're if we're
working with this group we we can start
at the bottom and see that in fact from
your ASCII quite a bit of we're
working out of that same genre this is
all citing back to papers other papers
which eventually say Yurovsky on the
same topic if we look in at the text we
can see that we can get things out of
the language of the text which is not
available otherwise so in the project
that we're working on with scientific
articles monito joyful from cambridge is
working with us we can identify for each
sentence in the article the purpose of
that sentence in relation to the overall
article and in relation to how
publications go so here is the aim of
the article one of the points or
contributions that it's trying to make
given that there are citations within an
article we can also discover the text
around that citation and what kind of
sentiment it has towards it whether it's
aligning itself with it whether it's
positive negative or whether it's simply
its objective and so one of the problems
of course is identifying what scope of
text around the citation refers to it
but also sentiment so here this is a
negative one we argue that this approach
mrs. biologically important phenomena
and the citation would be in there where
the dots are so in this project we're
working on prediction of scientific
impact our input is a term to represent
a concept in the field and or a document
we extract indicators from the full text
of documents that are related to the
term so we first generate a set of
documents that all have to do with the
particular concept and then our goal is
to predict the prominence of either the
term or that particular scientific
article and one of the kinds of features
that we use is time series and you can
see here time series for two different
terms climate change and
climate model you can see that climate
change shows a burst in this particular
feature later than climate model and
continues to climb while a climate model
goes down so that's just one of the many
clues that we use for this although our
work is primarily focused on indicators
from full text as part of this project
we're generating we want to be able to
explain why the system came up with its
prediction and we do this in sort of two
parts one is a summary which defines
what the technical concept was that
corresponds the term and then we also do
a justification of the prediction using
the output from machine learning so
that's an original generation task with
it where our input is basically the
features or attributes that were used
and how so this gives you two examples
in what is tissue engineering this is
sort of a one of the terms that we
gotten in input and this is done by
doing summarization query focused
summarization and this is a very large
number of articles we will typically
have at least a thousand in our input
and from that we have to go down to a
single sentence which will tell us what
the concept is for tissue engineering
this gives you an idea of what our
output looks like this is a very small
portion of it it's actually relatively
long and we're continuing to work on
this here we have we predict the
prominence of point seven three we give
information about the most prominent
indicators and then we provide some
description about each one of them so
overall the sentiment in the set of
documents is overwhelmingly objective
which yeah so you would compare the
result that you've got
summarization of this top compared with
the Wikipedia any preference for
subjective a little bit of a taste yeah
we we did not when we run this system we
do not have access to we only have
access to the scientific documents but
we could use that as a sort of ground
truth for how well it's how well it's
done we actually do use Wikipedia to
construct an ontology of the concepts
that will have and we use that in the
well we haven't measured it that way but
it would be a good way to do it I'll
just say a word about our summarization
approach here it's an unsupervised
approach we begin by selecting non
subjective sentences from the text and
we use the argumentative labels to do
that so background sentences we then
ranked by similarity and centrality and
a similarity graph so sentences appear
at the nodes and they'll be connected to
sentences that are more lexically
related and then we choose the most
central of those and then weary ranked
the top candidates by using definition
specific heuristics so what have we
learned well different forms of
summarization are definitely needed when
we change on rus' we want information
about terms and justifications and while
I didn't talk about it we have looked at
this in medical scientific articles as
well where we needed work that was
tailored to the reader whether a
physician or a patient we have
information we can exploit which we did
not have in lose the structure of the
article itself and the network so we
have explicit networks based on
citations and sentiment towards cited
work plays a role so I'm going to turn
now to our work on online discussion
forums
and this will be the kind of data that
we might have available to answer
questions a Twitter but we're also
looking at online discussion where from
discussion forums where we would have a
bit more information so how is online on
discussion different from the kinds of
genres we looked at so far first it
provides an unedited perspective from
the everyday person so the language is
going to be more informal it's often in
the form of dialogue so we will have
some back and forth about what people
are saying it it contains a lot of
opinions viewpoints emotion and of
course the language of social media is
not the same as the language of news
we're looking at being able to answer
questions about events so this would be
and we're looking at sort of these
open-ended questions this would be a
case of what we also refer to as query
focused summarization so here's a one
case this was in the course of
developing the system one answer that
the system generated at one point in
time we don't always get such nice
answers this is but it's yes just like
novels final writing you know
yes but this is one where it shows a
larger chunk so a lot of it comes from
the same chunk and I can't claim that
all system output looks like this here
what is the effect of hurricane sandy on
New York City and and you can see it's
dark there's minor price gouging their
restaurant selling hot food through
their babe windows the police are doing
an amazing concern job with traffic
concerns many stores have set up
recharging stations that was actually
kind of interesting when you where they
are because when you walk through that
dark part it looked like there were
fires and people were all huddled around
them and when you got close he saw that
it was actually electrical outlets with
the glowing was from the phones some of
the things that are hard here is that
very often there's no word overlap
between the question and the answer so
we had hurricane sandy New York City
affect none of that appears in the
response so we need to be able to do
some inference about how things are
related so our approach has been to
start with a small amount of manually
annotated see data where we have query
sentence pairs where the answers were we
use Amazon Mechanical Turk to get
sentences from the documents that were
actually related we then augmented this
with nine years of unlabeled data from
News blaster and we made the assumption
that the summary headline was a query
about the event and the summary was
approximately an answer of course it
would contain many sentences that are
relevant to that query and a small
number of irrelevant sentences and then
we also look for query answer pairs on
sites like yeah yahoo answers or cora
then we developed a semi-supervised
method that use multiple simple
classifiers for example keyword overlap
name dented the overlap and we
experimented with different kinds of
semantic relatedness we also have
expanded that earlier approach
to use more unlabeled data that we can
get on the web and and so we've gone to
features that we draw from dbpedia so we
have over 1.8 billion facts that we
extracted that were extracted from
Wikipedia infoboxes these have been
encoded in the Semantic Web and you can
see we have these sort of triples here
and this let's a see relationship for
example between hurricane sandy and
location which helps us to determine in
some cases relevance so our goal here in
moving forward is to be able to
decompose articles or online discussion
into a mean of anthis initial impact of
the hurricane and sub events what
happened afterwards the Manhattan
blackout breezy point fire public
transit outage of course you may ask if
we're constructing answers like this
from online discussion when can we
assume that an individual posts or
pieces in it are reliable enough to be
able to answer a question one factor in
this is influence and that's something
else that we're looking at in the
context of online discussion so the
research question is we want to be able
to detect online influencers what
conversational features are important
towards that task and how can we
identify situational influence that is
in influence that is made apparent by
the conversation not by the links
between who follows who so we don't
particularly want to identify Justin
Bieber for example so in our work an
influencer is somebody whose opinions or
ideas profoundly affect the listener or
the reader we're doing some of this work
in discussion forums like Wikipedia
discussions so these are online
discussions that take place between
editors of the Wikipedia forums and
there's a lot of sort of back and forth
about how editing should take place so
here we have we can see we have
conversation we have a person who makes
an initial post responder person who
responds to that and so forth and the
the discussion here is about whether
Ahmed in a job was lying about having
served in the iran-iraq war and he's
recommending to work in a certain piece
of information into Wikipedia the woman
replies that's a very weak source I'd
like to ignore it and the original
poster agrees thanks I guess we'll have
to wait and see so here the influencer
is is the woman we're doing this with
cascaded machine learning so we have a
number of features at the right some of
which are fairly complex and we need to
learn them themselves and then those
pass up into a machine learner for
influence so if we look at a couple of
examples for dialogue patterns let's say
we have a structure of posts like we see
on the left our pattern here the feature
irrelevance would tell us that posters
the posts that have no replies in which
they're foreseeing irrelevant are coming
from people who are less likely to be
influencers that would be our intuition
and it's one feature where as someone
who takes the initiative or insights a
lot of response would be more likely to
be the influencer now agreement and
disagreement is another factor in that
and this is something that we also work
done fairly early with Michelle galley
and we're continuing to look at it's
hard you can see here with disagreement
that's a very weak suicide endure it at
no point in time does the speaker say no
so we have developed some machine a
machine learning component which will
look at the kinds of
features that we need to be ordered in
order to determine whether we've got
agreement or disagreement sentiment
plays a major role in that so what have
we learned here in our work so far we do
significantly better than a baseline but
detecting influencers is really hard if
we look at F measure we were still we
have a long way to go we can gain
intuition about language use in social
context and so for Wikipedia we found
that agreement is is more useful than
the dialogue patterns but in some of the
other online discussion that we've
looked at some blogs it's the other way
around and we can validate some
hypothesis that we have about which
conversation or features are more
important for different genres yes are
you doing the uploads and downloads and
various discussion media where viewers
can give a posting of it not so
reminders vote yes in the inlet in this
particular case the data that we've
looked at yes we did we did ignore that
we have gone on to look at we're looking
right now at so each of the forums are
different and the kind of information
that you can get right now we're looking
at create debate which is sort of
interesting because in the data you have
pros and cons and so from that we can we
can easily see agreement and
disagreement and we can gather a lot of
data about it so follow one comma ters
are typically okay so going forward the
question that I showed is a kind of
question that you might normally get
when you're looking at the news if we
were looking at other kinds of questions
that we might want to pose when we're
looking at online discussion forums
there of a different nature so what is
the reaction and if we look into the
blog's we can see things like emotion
plays a role I'm still speechless at the
widespread damage how do people expect
sandy to impact the election here we
have a lot of opinionated
information I can only imagine how this
will make the nightmare of voting even
worse and then experiential questions so
it gives us the opportunity to look at a
lot of different kinds of language
analysis than we have when we are
looking at news so I'll move now to our
work on personal narrative the
autobiography of malcolm x would be an
example of the kind of thing but but not
so long we're not looking at novels here
we're looking at short online but we're
looking at things that provide a
coherent telling of a story they have a
component that is particularly
compelling almost shocking in terms of
of what happened so it's grabbing your
attention unlike the online discussion
forums we typically have a monologue um
but like the online discussion forums we
have informal language so here's an
example of what we would get and we can
sort of chunk it into different areas so
in the front we have some information
that Orient's esses what was going on we
were sitting down to a late night dinner
on Monday night when the storm was
supposed to hit we then have sort of a
sequence of events that is happening and
we end up with this very compelling
almost shocking element to the story he
went upstairs to get a tool and in those
few seconds ocean waves broke the steel
door lock and flooded the basement six
feet high in minutes so our goal in this
work is to be able to look at these kind
of narratives that occur online and
predict when a narrative is interesting
when when will it go viral or when
should it be selected as part of a
larger story and we're approaching the
work initially by analyzing the
narratives through William LaBeouf's
theory of narrative and this theory
proposes different structures that
should be present in a narrative which
will be interesting we expect to find
background information we expect to find
a series of these complicating actions
and most importantly we expect to find
this reportable event the sort of really
compelling shocking statement about what
happened I'm not going to say much about
how we've done we're still at very early
stages but we have developed a
supervised approach to be able to
identify a structure that is consistent
with LaBeouf's theory and we're getting
about 70.4 f-measure you'll note that
the kind of labels that we're putting on
structure if you're familiar with the
discourse relations that come from the
pen discourse treebank it's it's
different because we're not looking at
relations between sentences rather we're
labeling blocks but we do use the
relations from the pen discourse
treebank to help us in that process okay
so let me close by looking at our work
on narrative and here we have looked at
a corpus of novels that come from the
19th century and if that's because
they're available through online through
google books why we chose them if you
look at the language of novels again
you'll see it's quite different we do
have a lot of conversation with
different people speaking and often
talking to each other so here what is
the matter I cried a wreck close by i
sprung out of bed and asked what wreck a
scooter from spain or portugal loaded
with fruit and wine hey so we're using a
corpus of novels from the 19th century
and we're working together with faculty
from comparative literature uh and we
looked at whether we could use the
analysis of novels in order to decide
what theories of literary theories are
of interest we discuss what collections
we could work with in order to
provide evidence before or against them
and there has been quite a bit of work
in the comparative literature on these
various theories but using what is
called a closed read one or two novels
in a lot of detail so what we wanted to
do here was use what is it Colton called
a distant read look at a lot of novels
to make our conclusion so we were
looking at whether we could provide
evidence for or against literary theory
we did this by using social network
extraction from from literature and as I
said this corpus of 19th century british
literature where the network was based
on the conversation that happened in the
speech so we want to have a method where
we identify who talks to whom and then
extract features from the graph to
evaluate hypotheses about literary
theory so each node would correspond to
the characters a link will come between
them if they're talking to each other if
we looked in literary theory the kinds
of things that this particular person in
in comparative literature was interested
in with hypothesis that had been
developed that said that as the novel
moved from rural CENTAC settings with a
very small number of characters to the
cities with very large number of
characters then the network tends to be
less connected and you can see this in
quotes from very various people Franco
Moretti who said at 10 or 20 characters
it's possible to include distant or an
openly hostile groups Terry Eagleton who
says in a large community most of our
encounters consists of seeing rather
than speaking glimpsing each other as
objects rather than conversing so we
want to should be look at whether we can
show empirically that conversational
networks with fewer people
are more closely connected so to
construct the network the nodes are
represent people who said something and
we did this work in triple-a I with
quote attribution which we could do with
eighty-three percent accuracy so the
idea is can you identify for a
particular quote in a novel who said it
even that is not trivial and our edges
are people who are talking to each other
and here we use quote adjacency as a
heuristic for detecting conversations so
what I should mention that even goes to
the point often you have in novels where
you have conversation where it
alternates between one person and the
next you have no identification of who's
speaking but you have the quote quote
quote and so that was something that we
could pick up and then we set the edge
weight to the share of the detected
conversation so we sort of have a
information on how much talking they did
we're able to identify these links with
very high precision but only fifty
percent recall so that's something that
could still use some more work
nonetheless as you'll see this should
work against us in in what we aim to
provide when we look at the network size
as the number of named characters
increases given our hypothesis we would
expect to find same or less total speech
and in fact we found that but with a
week yes the number normalized number of
quotes was flat we would expect to find
a less lopsided distribution of quotes
among speakers and yes we did find that
the share of quotes by the top three
speakers decreases as the number of main
characters increases we would expect to
find lower density if our hypothesis is
true that is each person would have
fewer conversational partners as a
percentage of the population
we did not find this we found that
larger networks are more connected we
would expect to find same or fewer
clicks like smaller of these groups that
actually have conversation and we did
not find this in fact we found that the
cliq three click rate increase and
larger networks form cliques more often
as the number of speakers increases we
would so that was with main characters
now let's look at just the people who
are speaking perhaps that would give us
the information we were looking for so
as the number of speakers increases we
would also expect to find less overall
dialogue this is glimpsing rather than
speaking and we find that not to be the
case larger nor networks are more
talkative and we would also expect to
find lower density and again we find
that not to be the case in larger
network speakers no more of their
neighbors we did find an alternative
explanation in the data that we were
looking at and that was a tax
perspective which do not dominates the
network's shape so in third person
tellings as opposed to first person we
did find significant increases in the
normalized number of quotes the average
degree that is the number of people they
were each person was speaking to the
graph density that is the percentage of
the population that they were we saw
them speaking to and in the rate of
three clicks and this was with no
significant difference in the number of
characters or speakers our hypothesis
here is that when we have a first-person
narrator they're not privy to the other
characters conversations with each other
they see things only from their own
viewpoint and we can see this in the
graph so this is a network from a
third-person narrative novel Jane Austen
persuasion and we can see it's not an is
talking to a lot of people when we go to
what is called the close
so it's told in the third person but
from the perspective of one character we
can see that shape changes Robert
dominates and most of the links are
between Robert and other people we're
not seeing who else they're talking to
and when we go to the first person
narrative we get a dramatic change so
you know everything is seen through I so
what have we learned here well we've
learned that high precision
conversational networks can be extracted
from literature and I think that while
natural language has avoided looking at
novels I think the time is right now to
do more exploratory analysis of fiction
and we're beginning by combining the
work that we've done by looking at the
social function of characters and see
how that plays a role okay so to
conclude we've looked at a lot of
different sources of data and what we
can do with them our goal is actually to
do some integration I'm not sure yet how
novels fit in but we'd like to bring the
rest in then we're at early stages with
personal narratives so I'd like to show
you now a mock-up of where we want to go
with this
of course things that work when you
start I may have to just do it like this
huh
you can see it
well I had this all set up
we may just have to go through by hand
here is a sort of timeline of Hurricane
sandy where we have the introductory
information about hurricane sandy
approaching and then we move through to
provide information along a time in
space which will tell us about what hat
what has been happening so here we have
the personal narrative the compelling
event we then move along tuesday a p.m.
where new york city has become dark
again drawing from social media a little
bit later in conversation on postpone
the vote and finally a month later where
people are beginning to work on the
impact of it and so drawing from
scientific articles
okay so
so to conclude that's that's our goal
and I'm at this point just ending with
showing you a picture of our research
group so thank you enough there any
questions yes we actually got from the
literary yours
how they react to you know in the third
goal well I was we were working with the
chairman of the comparative literature
department at Columbia and he was really
interested in this in fact he did not
worry that our evidence came out against
the theory he found that very
interesting and and I have to say the
evidence is only against it doesn't
disprove it it just provides some
information that suggests that it may
not be as true as people thought the
field of comparative literature is
moving towards doing more empirical
analysis of text Franco Moretti at
Stanford who provided one of that those
quotes that is what he does they tend to
use less sophisticated tools that you
have in natural language processing so
you know I think there's a lot of room
for interaction and since we did that
work the department at Columbia hired a
faculty member in computational english
and i thought you know how cool have
Columbia who is so conservative higher
in that area and he's a person who
actually does he is fairly sophisticated
tools he does topic modeling and you
know various other kinds of things yeah
but there is another 19th century what
we're studying right so I'm thinking
shows the tour guides of the second
person you'll be back you that it
basically puts you there and that
nothing of death if you look at
different authors discuss how much more
of a difference there is between 900
authors by themselves versus the type of
novel with lots of characters fewer
characters there's a whole lot you can
do I mean so you could look you could
look by we talked about a number of the
different things that we could look at
for example we talked at one point
before starting about whether we would
look at difference between genres and of
course when you're working with someone
in comparative literature they have a
lot more nuanced view of genres then
than I do but there's
three novels and weather but we didn't
have enough data on each kind of genre
to do that and that was one of the
reasons you know you need to be able to
get a large enough corpus that is
available online we had about 60,000
novels from that time period it's pretty
interesting presentation one of things
that I want to ask you for the insight
about is literature is very structured
in a way that you and very little
sarcasm is very much mobilize lot of the
sort of the social postings today or
Twitter or something like that there's a
penis ohms a lot of sarcasm very
negatives so I wanted to ask you for
your thoughts about how you think this
type of approach may work in you were
sort of media types of language um well
we are I mean we are working with social
media and different portions of the work
that I talked about we use different
techniques so what we're doing with
novels we are not doing with online
discussion forum in our work with online
discussions sentiment analysis plays a
big role in in order for example whether
somebody has influence can depend in
part on positive or negative reaction to
what they've said there is work at
Columbia going on on detection of
sarcasm we my group is not yet using it
but it's being done by weiwei go in with
Mona da bands and there's another
researcher Miranda Maris on we're
looking at that we're working together
as a team so it's something that we
could eventually fold in I think sarcasm
is hard to detect but of course it
negates whatever sentiment you know the
person has expressed so it's important
jaylene sites about lots of context
whether these or the message is sort of
short because a lot of the things
mentioned these things depend on knowing
all the stuff around it and you have
these are very short of the bill context
around it it's sometimes very difficult
to determine like I wouldn't even talk
it out yeah so I have to say that's why
I stress a little bit that we are
looking more at online discussion forums
where the context is longer so in our
work on influencer detection we we found
a number of very nice sights on on
political discussion even in our work on
disaster we find some but with we have
also done work on Twitter and within
Twitter we've sort of focused on finding
threads where there is conversation so
you can see some back and forth as
opposed to individual posts because
other otherwise I I do agree with you
like I'm obviously I'm not of the
younger generation and I'm not always
sure of the value of Twitter although
clearly in the context of disaster it is
important when you know things first
happen and sort of to see the
progression of events yeah
distinguished do versus controls so in
general
that's a good question we don't right
now I end and I can't give you a good
answer about about how we would do it
it's we're having a hard enough time
with the influencer so we're just
assuming for the moment that that
doesn't exist research that just came
out talk about trolls and how to align
with the doc tetrad at cycle well I was
actually wondering with a analyzing it
takes to identify that
yeah so identify problem that's a
question that I've been asking inside
the science so I was chi about how we
might be able to identify and sideline
the influence of troubles by recognizing
gee these are the kinds of psychological
models and then what kind of language or
behavior alliance with thugs I think
that's a long time I did have a question
about influence model I was curious the
what you do is it only on online
discussion forums within one forum
because there is when you evaluate the
influence and the authority of someone
if they come from reddit I evaluate them
very differently to they come from
somewhere else so I'm curious so we are
looking across different kinds of
discussion forums I agree they can be
quite different we had earlier started
out with livejournal blogs mainly
because they had a lot of metadata
available about the posters so we could
get more information we then move to
Wikipedia discussion forum so those are
very different we're now looking at
online political discussion forums and
we have worked some with Twitter and
part of what we're doing is a bit of
domain adaptation so with our different
features which are learned weekend as we
test as we move from one genre to
another how well it how well we do for
example do we need to retrain on the new
genre can we use a combination of
training material from both and so forth
but we haven't looked specifically at
what you're raising now which is where
the person natural habitat is yeah what
percentage do reader
Air Force miss you with US air force why
did we drop that right so in other words
why did we drop US Air Force yeah
weren't you know I can't I can't give
you a logical rationale as to why it's
it's learned so it happens in the data
that we're looking at when people did
summaries they dropped US Air Force and
yeah so it may have been assuming the US
leadership I think all of the people
doing the data the majority of the
people this came from data also that was
done by people must have been from the
US yeah allows that you show on the
literature constructing network of the
people of honey to do you have for this
thing down look ye may help to the full
reading of Moby know of you go getta be
snow yeah I sort of I you know I I don't
do patents as much as I should if at all
and I know it's very different from
being in a company we actually end up
putting a lot of our work online for
free I have worked with our patent
office and it was enough to make me
decide no I don't have to do it i'm not
going to russian work so just started
from the dock beta right from the
pyramid evaluation so have you looked at
what would be needed if your convert
so what would you need no from news
annotated to have similar quality
because that kind of date I want you
have right because you have to have more
visas and running I know and human
evaluators getting the loop and it's a
big effort I am sort of energy used
right I I think we need to move more
towards semi supervised and unsupervised
approaches either that or also using
data from the web that we can find that
can serve so our work on making use of
all of the news blast or summaries to
serve as question answer pairs which is
you know it's not as accurate as if you
did it with human an annotation but it's
huge and so you know that that that can
compensate for some of the errors that
are there now for we have thought a lot
about how to find good data that we
don't have to sit and annotate to get
nothing is perfect so we went with the
pyramid data which actually is quite
nice we thought about and have looked at
using simple wikipedia where you go from
longer to shorter sentences I which was
partly okay but wasn't clean data like
we were having to do a lot of work in
figuring out which were good pairs in
which we're not in previous work this is
medical where we've gone from you know
Reuters posts news releases when a
journal article is released and so there
you have a pair with a short lay version
of the note of the of the journal
article it's helpful but again not
perfect because the data is sparse so I
think that would that's the direction
that we're going in is how can we look
to find the kind of data that we need as
opposed to annotate it
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>