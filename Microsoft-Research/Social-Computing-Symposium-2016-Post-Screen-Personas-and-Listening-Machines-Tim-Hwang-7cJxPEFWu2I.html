<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Social Computing Symposium 2016: Post Screen Personas and Listening Machines, Tim Hwang | Coder Coacher - Coaching Coders</title><meta content="Social Computing Symposium 2016: Post Screen Personas and Listening Machines, Tim Hwang - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Social Computing Symposium 2016: Post Screen Personas and Listening Machines, Tim Hwang</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7cJxPEFWu2I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
well good morning everybody really
excited to be here my name is Tim Wong
I'm a researcher at data in society and
my talk today is entitled Dashboard
Confessional data moderation and privacy
and it's a new talks I'm excited to get
people's feedback particularly for this
group that we have here today and so as
the title suggests I've been only
allotted ten minutes so I'm going to
bite off way more than I can chew cover
a lot of ground with an eye on surfacing
a bunch of questions for discussion
which I think this group will jump into
quite well so let me go right into it
which is I want to address two quick
questions the first one is under what
circumstances is our interest in
moderation conflict with our interest in
privacy and then the second one is what
relevance does is have to the social
spaces of the web and I think these are
really critical questions because our
interest in moderation and if our
interest in privacy are in deep conflict
I think they're both values that this
community really appreciates and thinks
are important then we have to debate
those trade-offs as we push towards a
new vision of what the web should look
like alternatively if they're only
sometimes in conflicts we need a better
understanding of how they're in conflict
and when they're in conflict so one way
of I think getting into this question
addressing this question is to start
from the idea that moderation implies
listening in fact moderation implies
listening and put differently moderation
implies data collection right so we can
think about this even in a non online
context so if you think about moderators
right or mediators right these are a
third party that monitors the discussion
between parties and is there to
basically enforce a set of rules but a
big part of what a moderator does is
listen to what's going on and respond
accordingly this also occurs in
situations of self moderation as well so
we can think about the confessional for
example as a kind of primitive Amazon
Mechanical Turk listening device right
you speak into a microphone and a render
certain types of judgments back at you
and again data collection is key to the
moderating force of the confessional
right you need to receive what someone
has done or to give them advice on what
they should do next and this rule
applies not just for one person or two
people but also scales out to groups or
even thinking about moderation at the
of a society so listening to social and
creative activity becomes a critical
piece of implementing a system of
moderation so that's you secured a
traditionally in the form of
bureaucracies right listening machines
that take the form of human
organizations and processes and rules so
I think here about the production code
administration right which enforced a
sort of system of censorship on film in
the United States for a period of time
and again this was a very kind of large
group scale high costly listening
machine right you have to submit a film
get their approval and only then could
it kind of go out and enter the public
sphere so so far so good right this is
actually not very original stuff this is
sort of basic panopticon design 101 and
we can make the kind of links pretty
explicit as to why listening is really
key for moderation right on one hand it
creates the basis for action right you
can't moderate without knowing what the
activity is and then assessing that
activity and then like to pen off the
Conrad it serves a notice purpose it
lets people know that they're being
listened to and therefore they might
want to think twice before doing what
they're thinking about doing it's also
potentially not very original on the
level of political theory as well right
this is sort of a Leviathan type
situation where we've always been
confronted with situations where we need
to sacrifice liberties and freedoms in
order to ensure the third party has
ability to kind of regulate societal
balance on some level so I think the
question and this is the second question
I asked is there anything different this
time in the context of the large-scale
social spaces of the web when I think
there's two right so one of them is the
extent to which activity takes place on
a platform mediated by code and is
automated to deal with scale right in
short these platforms implement
moderation with true listening machines
with actual code and automation and I
think that has a number of implications
for how we think about moderation
impacting sort of our interests in
privacy and particularly our exposure to
surveillance so one of them is to
automate and scale moderation
effectively you need to structure
activity to be visible and legible to
machines right that means a couple of
things that means fixed identities right
that you can deactivate to reduce
control or access to a platform it also
implies persistent logs of activities
that allow you to evaluate whether or
not a user's
actor or a bad actor and for
particularly harmful activity that
breaks the law you may also want to
index this data with real-world data
right think about the the sort of
violations of rules in the real world
you know you can imagine spam craftsman
there's a sudden connection a certain
pressure to move sort of this data and
connect it with what's happening offline
and real world identities it also means
that in some cases adjudication takes
place only over a given set of variables
right the data points that the machine
can see and that renders potentially
unfair results because the system only
sees a limited set of data points about
any individual or any set of people at
the same time improvements in that might
require more disclosure for more users
right so that are interested more fair
results in moderation may actually lead
to less sort of privacy being given to
any particular user on that platform so
this is just to me that the code base
nature of moderation may push us towards
more privacy are more moderation than we
would otherwise prefer as users so
that's one difference right what's
what's potentially another difference
about the fact that you know we've got
moderation happening in large online
social spaces well so one of them is
that societal us these social spaces are
in private hands right so it's different
from sort of the classical Leviathan
framing where you say okay we're going
to get together and form a government by
the people and this has another
presentable set of pressures on how
companies may approach moderation so one
of them is that platforms may have
really perverse incentives to seek out
what you could call the emo threshold
the engagement maximizing outrage
threshold that's to say that there's a
relative potentially lack of moderation
that might be beneficial for a business
that's based on attention advertising
engagement from the users and obviously
this can't be allowed to bubble out of
control or else you'll alienate users
who will then go away but there's
potentially an optimizing level of
outrage and a lack relative lack of
moderation that you might want to
implement in the system to to boost
profits essentially there may be a
profit motive towards the the line that
we draw between sort of privacy and
moderation another is that platform see
themselves as platforms in many contexts
right they may just see themselves as
passive infrastructure with limited
responsibility for it what takes place
within it so this is a point that's been
made by
Zhang and Tarleton gillespi and a number
of others and this might make them worry
about taking on the responsibility of
differentiating between types of content
differentiating between types of
activity and then may actually be
reinforced by the law right CDA 230 and
dmca put in certain structures that
enforce this kind of approach to
thinking about what a platform is and
what its responsibilities are to the
content that moves through the network
so this may create hesitancy to get
involved even in light of obvious crises
happening on these platforms so this
suggests to me actually another really
interesting thing which is that the
commercial quality of these platforms
may have an impact again on the
trade-off between moderation and privacy
to wit it might pull platforms more
towards the privacy direction than a way
that we want if we were users creating
the platform ourselves so I think we're
left in a really interesting situation
when we think about the future of the
network in the future of the platform
right one of them is at the center of
gravity of commercial entities is that
they will systematically moderate less
than we might want them to do but the
challenge of moderating at that scale
might be that we have to produce
diminishment of privacy that we're also
uncomfortable with right that like
automating moderation at scale may may
create effects that we don't really want
either so then I think obviously the
question is how do we find middle pads
that provide better trade-offs right
some of them have actually percolate
around a little bit already one of them
is it might be contextual right we may
say let's take the platform at its word
and force them to put it in a level of
moderation that corresponds to what they
present themselves as so if you present
yourself as a public forum we may say
okay we're we're okay with you
potentially providing less privacy in
this forum for the interests of
moderation another one might be a call
for fragmentation right giving people
tools to balance themselves this balance
between privacy and moderation in the
way that they would want right and
there's an instant question about what
that fragmentation leads to across
platforms of this scale you can think
about filter bubble problems and a
number of issues along those lines so as
I mentioned at the very beginning I
figured at surface a lot and actually
not have any really great answers but
figured it might set the stage for how
we think about the links between
listening machines on one hand privacy
on the other and I think our
overwhelming interest in increasing the
level of moderation online so we'd love
to chat about more here's the obligatory
contact information and figured we would
just go straight to Q&amp;amp;A
yes so thank you let's see if this is
working hooray um I wanted to take
cheers privilege and start with a
question for you first and one of the
things that I'm also finding really
interesting about the nature of the
listening machine is that it's now
become part of the key driver of a I so
a really really early statement from
bill gates in the 1980s he said I want
to build machines that see hear and
understand and we certainly haven't
really got to understand yet which is I
think part of this difficulty of
automated moderation why it's so hard
it's extremely hard to understand when
someone is actually engaging in
harassing or bullying behavior it can be
so coded but we're getting really good
at designing machines that can see in
here so I'm going to curious to hear
from you like as we move more towards
these a I moderated environments as that
is being seen as the shining light for
where this is going where do you see the
kind of potential for both leaps forward
in understanding and potential new forms
of misunderstanding sure I mean I think
one aspect of a lot of deep learning
systems and sort of a i right
particularly when you think about like
addressing and understanding what the
content of text is is that you're
unleashing like incredibly data-hungry
processes so there's one aspect where
the sort of dream is well if you only
had enough data perhaps you could create
the magical system that be able to take
a tweet and say that's wrong you know
and automatically remove it right those
making moderation very cost-effective
the problem is the amount of data you
need to collect in order to get there
might present their own kind of like
interesting outcomes that we might be
uncomfortable with and so part of the
problem is that the smart systems again
that we need may themselves kind of
raise these issues I mean I think that's
one thing at least to get us started I
think there's many others yeah yeah
great question of you so I was struck by
your comment about listening in data
collection and I was wondering if you
could talk a little bit about what the
difference between listening and data
collection is and whether we have the AI
tools yet to do the kinds of things that
a great moderator often does in reading
between the lines and into emotional
context things like that sure right yeah
I was saying skate this morning it's
sort of funny because the panel is
listening machines but in some ways the
talk takes liberties with what listening
is and what machine is so so yeah I mean
here at least for the purposes of this
talk like I think like I use listening
to effectively mean data collection but
one thing that's often been pointed out
is that like listening has this
additional component of comprehension
which I think goes to your question
which is how close are we actually to to
creating machines that effectively
moderate at least from what I've seen I
think we're still a long way off right a
lot of the the subtleties of harassment
I think our difficulty at caught by
machines even though we can catch like
the maybe the extreme versions of it and
so so I guess it ends up becoming a
question of what's the degree of what's
the degree of invasiveness or the degree
of granularity we want to have around
dialing up or dialing down our
moderation and I would say right now
we're at a state my sense of the
technology is we're at a state where we
can do it on a sort of a crude basis but
I think normally we have to resort to
saying okay well maybe we can have an
army of volunteers participate because
at the the more subtle NZ may be more
difficult to pick up thank you that's a
great point and maybe one way to tackle
it is to rather than think of the AI is
having autonomy in the moderation think
of it as a flag in the moderation
something that can point out to a human
moderator here's something you might
want to look at right right and I think
this is really interesting about what
are the relative economics of any
particular moderation system we might
set up and there's a problem which I
think like the scale of these platforms
makes certain moderation systems very
difficult to invest in which is one of
the reasons that companies have
systematically under invested in it and
so yeah I think there's actually really
Justin question about like okay and
these hybrid machine human systems are
there are there are there things that we
think can do this at scale effectively
so my question now sure into a follow up
because I was going to mention that my
wife and I developed a system /
university of phoenix cool that based on
some analysis we found moderation needed
to be really timely for academic
purposes if it didn't come in within
roughly 11 minutes it wasn't worth
biol anymore mm-hmm and it had to have
specific semantic content it had to be
semantically relevant to where the
current discussion was so the moderator
couldn't come in and just simply say
here's the topic you guys should have
been talking about you have to lead them
back in and and so we built systems that
would you know the way I think of it is
to me AI really ought to be augmented
intelligence not artificial intelligence
and how can we essentially allow someone
we can think of in terms of cost or you
can just think of it in terms of
allowing the faculty to lead a life that
just signals them hey this this this
discussions going off the rails and you
need to intervene right now in this
space but the in the space part was
useful also because when we simply
signal people to intervene if they just
came in and we're not you know you have
limited time you don't 11-minute you
don't have time to read everything
that's been said so far so we found real
value in mining in detail what was being
discussed and leveraging a lot of data
but i think the flip side is then you
have to be respectful which you are
forced to be in an educational context
of we don't own it we can't do anything
else with it then provide an educational
tool right yeah definitely any last
question sent out when he fantastic last
one I was interested that you talked
about the spectrum of sort of privacy to
moderation remain focused on moderation
as potentially a driver of where our
choices would be about what we feel
comfortable with I'm interested in your
logic and doing that because it seemed
to me just as a in a 1 i'm more
concerned about the listening turning
into something like my data being sold
to an insurance provider that impacting
you know my insurance rates or my credit
score which is not really about
moderation it's another form of
surveillance so is your point that you
think moderation on these big platforms
is really the thing that will drive our
choices around privacy or how do you see
that interplay oh yeah no I mean I think
this is just a single dimensional
relationship right like I don't think
it's necessarily say that like okay the
only thing that affects privacy is are
interested in moderation I think my
selection of identifying that dynamic is
just
to kind of address the the topic of the
session right our topic of the event
really which is thinking about okay we
think that moderation is really in a bad
space online right now how do we improve
that and I think the the talk is a call
to think about like what are they all
the other values that we might put value
in that might be eroded by our choices
towards greater moderation now I think
there's a really good interesting
criticism of the talk which is to argue
that like okay the privacy but really
whose privacy right like we could say
look there's a really strong interest in
protecting the privacy of people who are
being harassed and in fact if you don't
act what happens is that like people
will harass and vox people and that's
actually a privacy violation and so I
think that that's probably the next step
of this talk is to then start to think
about okay what are the privacies that
we're thinking about and what are the
actors in this space and we may say like
look by and large the large group of
users were okay making them submit this
extra piece of information just because
I think our interest in protecting
vulnerable groups is so high and I think
that that ends up becoming a really
interesting discussion actually becomes
an empirical question then which I don't
have the answers to but but I think that
would be sort of the next step could I
agree right like the insurance one is
very much a commercial thing right we
feel weird that businesses are using our
data for certain purposes there's also
we feel bad when someone's personal
information and their address is
disclosed right and so quick come in
here so what's the social contract
mm-hmm I mean we have terms and
conditions we have privacy policies
which are very much a one-way contract
that we have when we decide to become
users of a particular platform but do
you think that part of solving this is
creating a contract that's really you
know between two parties instead of just
one way like a negotiated you mean yeah
or at least something that you know
society in general has agreed to in the
same way that we have you know a welfare
state to some extent we're going to
protect the vulnerable by giving up
certain things either monetarily or from
a privacy standpoint you know whatever
that is right yeah I mean that someone
mentioned this morning right I think it
becomes difficult to create those kinds
of arrangements in part because of the
perverse economics of scale right so
what I mean by that is like a company
like Facebook could say yeah well
merge a couple million users from this
choice whatever I'll see you later right
and and that becomes a really difficult
thing in creating sort of a balance of
negotiating leverage so then we I think
go to this question of like do we need
regulation do we need something else to
kind of like specifically apply to these
cases where we feel that like the users
and their ability to just switch to
another platform won't really be an
advantage that will force platforms to
come to the table and so I'm a big
supporter of thinking about like okay
what is that social contract and does it
have to be enforced by an entity that's
even larger than these platforms
themselves so that is that that would be
my response fantastic thank you Tim and
I'm glad that we're getting questions in
there about the sort of insurance and
discriminatory aspects of this potential
data stream and that's something that
Suresh is going to be talking about
later</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>