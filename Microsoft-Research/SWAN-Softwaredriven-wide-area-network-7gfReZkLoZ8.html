<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>SWAN: Software-driven wide area network | Coder Coacher - Coaching Coders</title><meta content="SWAN: Software-driven wide area network - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>SWAN: Software-driven wide area network</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7gfReZkLoZ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so I'm gonna talk about project
we've been doing over the last I'd say
18 months to a year and it's sort of of
you building a big system and and I just
want to start off with recognizing I
think the set of people we work with
this is a mix off essentially I think
people who are in our ops groups or
engineering team like Vijay Gill Dave
Mosley WA and Mohan
some of us I think Shrikant and Ming are
with me in the networking research group
Roger watt and Hoffer were visiting us
from ETH and and then we have a bunch of
interns working with us right okay so to
set the problem context the context is
the following so Microsoft like any
cloud provider basically operates what's
called the inter DC well this is the
wide area network that connects our data
centers as you might you mad this is why
the way I made a picture just so you
know don't go looking for Microsoft data
centers there and and what's there are
two things to this like this is as you
might imagine it's a it's a highly
critical resource for any cloud provider
it's highly critical because all
services send traffic for both
performance and reliability across the
data centers and so if you think of
things like geo replication in Azure
every write that comes from a customers
gets replicated to another data center
if our users are doing queries and the
entire index is not present in one data
center that essentially gets queries
essentially go across this network so we
needed both for high performance and
high reliability so to speak at the same
time it's a very very expensive resource
it's expensive because we have
essentially hundreds of gigabits per
second capacity going across oceans and
going across continents and this
capacity is very very expensive
so unlike the capacity within a data
center capacity across data centers it's
actually very hard to even upgrade right
now there's a big crunch about the
optical capacity that's available across
the Atlantic or in the Pacific or even
in some parts of the US so it's not just
about cost is also availability issue so
given that this resource is very very
expensive by the way the amortized costs
of this network runs easily in
to hundreds of millions of dollars per
year and this is taking up taking up all
the cost factors into account so here's
the problem we kind of became interested
in and the way we started working on
this was actually Vijay came to us about
ear almost over a year ago and he's
basically said like you know unless we
can make this network highly efficient
we are basically have losing competitive
edge to other players that can operate
the network la cheaply and in particular
basically what goes on in these networks
and this is just the its problem it's a
problem that's not unique to Microsoft
if you strands if you read about studies
of people operating IP backbones
it's not uncommon for the average
utilization on the backbone to be very
low so so it's so low in fact that even
the busiest links the average
utilization aren't the busiest links is
less than 50% the median utilization
tends to be around 30% or so so
basically you built we built up this
expensive resource and we can use even
less than half of it on a good day on
good links and that's essentially what's
happening so the question is like why
does that happen so one one one reason
is this which is that I think the way
the technology stack of how these
networks are built works today is that
we have different sets of services using
the network these services they send
traffic whenever they want however they
want however much they want the result
of that is we tend to see these diagonal
patterns that you see here the traffic
essentially Peaks then it drops such
that peak to mean ratio is about you
know it's more than a factor of two so
the reason this is happening is we have
uncoordinated transmission by services
now if you don't want the network to
drop traffic you provision your network
for the peak capacity so you basically
provision for peak and then in average
case you have really low utilization so
you may look at this graph and go like
well what can you do here but the thing
is because we run the network and
because we run the services in a data
center and because we understand the
nature of the traffic we are carrying we
can actually take a careful look at what
what exactly is happening input
a very simple heuristic we can separate
the traffic into what you might consider
background traffic or non background
traffic so non background traffic would
be things like you know queries going
across data centers or a huge your
application background traffic might be
things like entire clusters being
replicated from one DC to the other so
now average what you see here this is by
the way like real traffic trace from 1 1
DC 1 introduced e-link
in our network so what you see here is
that the background traffic's there and
a mixer of non background traffic and
they're essentially that's what kind of
determines the peak but the nature of
the traffic is such that the background
traffic can actually be slowed down so
what we can do is that when the truck
when the non background traffic is
peaking to not carry a lot of background
traffic and we can carry background
traffic when there's not when there's
ample capacity in the network so by
doing that if we can do that
successfully so what's going on here is
that we can actually either now carry
more red traffic which is non background
or we can essentially delay any kind of
capacity upgrades that may be needed in
there so that's kind of like one source
of inefficiency that we like to get rid
of the other thing that's happening is
that the way these networks are operated
today they essentially do what you might
consider like highly local highly greedy
allocation mechanism so this is using
MPLS for those of those of you
understand MPLS but it doesn't really
matter if you don't know how MPLS works
here's what MPLS roughly approximates
right so let's say given this network a
flow comes in from A to E what MPLS will
do for you it will find the shortest
path that has available capacity to
carry the flow so in this case is that
now if a new flow comes in from C to D
because the link CD is taken it will
find the shortest path with available
capacity because it because it's doing
this local allocation it's it does not
have the freedom to move the blue flow
in this case or the router C does not
have the freedom so now C G flow starts
and you basically end up with this so
what ends up happening essentially I
mean that you get into this jumble of
traffic where flows are going over
longer past than necessary because you
are doing this local greedy allocations
if you had a global
view of the network here's what you
should do or you could do essentially
not only your performance will be better
you're using fewer links in the network
and a lot of other links as you might
see here have been freed up to carry
even more traffic in the network if you
could do that so these are essentially
the two sources of inefficiency that
we're going to try and tackle in our
design I call Swan which is stands like
set for software driven van and we what
we want is a highly efficient van so by
the way one thing I did not mention
efficiency is only one side of a coin
here it's very very easy to eat up the
utilization of the network all we have
to do is to let lose all background
services we have and the network
utilization will go 200 percent in no
time and the reason you cannot do that
is because you have other traffic to
carry as well so this basically is the
second goal or what we call like support
flexible sharing policies in particular
when we talk to operators and we talk to
service developers what matches our
context is be able to strictly
prioritize let's say things like
interactive traffic over non interactive
traffic non background traffic or
background traffic and that kind of
formulation maps to strict priority
classes as to how resource allocation
should happen and within a priority
class there's an expectation by service
owners and operators of fairness that if
you belong in the same class like
interactive class or background class
fairness is expected from the network so
we're going to take that as a flexible
sharing policy so what we want is a
highly efficient and highly flexible
wire and the key design elements of
course are basically I'm going to
coordinate the rate sending rates of
services and we are going to do
centralized resource allocation because
of the things I mentioned here quick
show of hands like how many of you know
what Sdn czar 1 2 okay so I'm gonna skip
this anybody does not know what as dnase
it's ok to raise your hands ok I'm gonna
skip this then and sometimes you know
what's happening in SDNS we are
replacing beefy doubters by commodity
switches and we are centrally
controlling essentially what these
switches do by directly programming
they're forwarding information
forwarding tables that's it
ok so here's an overview of so on is
what happened so the way to I
understand is is basically we've got two
ends to the system one is the service
host themselves and then the network
what we gonna do is in the Swan
controller this is like logically think
of it as like one controller in the
network we are going to get traffic
demand estimates from these things we
call service brokers so service brokers
stick a service is basically from our
purposes a connection of hosts doing
something jointly let's say they are
replicating traffic or they are serving
user queries and so forth so we are
going these service brokers are going to
estimate the demand and they're going to
tell the Swann controller how much they
need so this happens for each service
across each pair of data centers that we
have we are going to have these things
called network agents that talk to the
switches and tell us what the topology
of the network currently looks like and
what traffic currently happens what's
one controller does is that it takes all
of these inputs and essentially computes
how much each service can send at this
particular point in time and it
reconfigures the network to carry
exactly that so we have a demand matrix
we compute the allocation matrix and we
reconfigure the network so basically at
the and by the way the final thing here
is that the bandwidth allocation gets
pushed down to the horse and any rate
limiting happens at the horse because in
the network we don't have any capability
to rate limiting on commodity switches
they are no not enough queueing capacity
or number of queues there is very very
limited so that's essentially what we're
going to do essentially the thing that's
happening here in case it wasn't clear
is that every five minutes or so what we
are doing is redoing this computation
and changing the network's configuration
to match the demand matrix as best as
possible
so as you might imagine if we try to do
that these are the kinds of issues we
run into and the technical part of the
work is essentially focused on so that
was kind of the architectural slide in
terms of the algorithmic work for the
work focused on solving challenges like
these one was like how do you scale ibly
compute bandwidth allocation and network
configuration every five minutes so we
are talking about hundreds of services
across at least tens of pairs of data
centers and basically we have to do
fairness as well as well as strict
priority classes so that's one challenge
the other is avoiding network congestion
during updates what I mean by that will
become clear when it has slides
explaining what the challenge is and the
other thing is commodity switches have
essentially very limited memory and
we've done some back of the envelope
calculation is that even the next
generation of switches will not let us
essentially be freely not worried about
memory at all so this is this is just
part of the pain you have to take in if
you start working with commodity
switches okay so the scalable allocation
computation it's it's basically what we
do as you might imagine set up the
problem so we've got demand matrix from
if services we set it up as a path
constraint multi commodity flow problem
a path constraint version is essentially
we route over a set of tunnels that are
preset rather than letting flows go
anywhere and this makes the computation
more scalable and this also helps with
the memory stuff in there and the other
thing we do is I'll talk about fairness
in a second but solving this at the
granularity of individual switches and
links tends to be very complicated so we
actually solve it at the granularity of
data centers so forget that there are
three switches in the data center we can
emerge that do abstraction of the
topology and solve it at that
granularity but then we have to
essentially map that configuration down
to the network and for that we're going
to leverage this inherent symmetry that
exists in how the switches are connected
so that's basically let's the
abstraction gets implemented as is on
the switches by leveraging the network
symmetry we have so talking of max-min
fairness so max-min fairness what we do
is we achieve within a class within a
priority class and the reason max min
fairness is computationally hard is
because so for a single resource it's an
easy problem you just start from the
bottom and start allocating but
network-wide essentially it becomes
really hard because you need to do
progressive water filling across the
network when links are moved and then
you have to be willing to move the flows
it kind of struck me as surprised I
didn't know that this max min fairness
problem was so hard but it tends to be
and so there are like not enough there's
like very few algorithms that would
scale to the level we need so we kind of
made up our own and essentially what we
do take the entire space of allocations
a service
have and we do geometric partitioning of
the space and then we allocate in there
so what we have is fixed number of
iterations and we have bounds on how
unfair we might be so this is like an
approximation of fairness that works
within like a fix compute time in there
okay
other way I'm happy I know I'm kind of
glossing over details there and but I'm
happy to talk to you about these things
offline or if you have questions speak
up
this for me was this for me was the
favorite part of the work so this this
is a problem that only arises in central
allocation and was like essentially I
didn't even realize network
configuration would have this problem
here's the problem so suppose you wanted
to change your networks configuration
from the one on the left to the one on
the right and you may want to do this
for any number of reasons for instance
you want to free up capacity on the link
r2 r4 as an example now the thing is
like making this configuration change
requires changing the configuration of
r1 and r2 at the very least given that
these are two different switches you are
not going to atomically switch them or
at least it's not easy to atomically
switch both of them because you cannot
do that easily you may end up in the
state of one of these states possible
where either r1 moves first or r2 moves
its flow first so no matter who moves
first you basically are now in a
situation where you have a congested
link and the link is over subscribed by
huge amounts and lots of traffic will
get dropped in a burst and this
difference in the burst would is a
function of like what the time delay
will be between r1 moving and r2 moving
and so to speak so and this is this is
somewhat like like I said this is a
problem unique to central allocation
because on the top we are dictating the
network traffic when it's going
north-south and we say okay I don't like
that I want it to go east-west and you
want to do that essentially without
dropping so how do we address this yeah
two-phase commit is not going to help
you here order your changes can we order
our changes you may
not be under arbitrary circumstances you
cannot order your changes so imagine a
situation where the network is
completely busy all the links are used
100% of the time any first move will
congest some link right and and I think
with your ordering thing I think you you
are you're barking along the right line
of thought so that's what I think will
come become clear but yeah so this is
what we do so instead of trying to it so
we want to do congestion free network
updates so what we do is we compute
essentially a congestion free network
update plan so instead of going directly
from left to right we essentially take
multiple steps what I did not mention
was that assume the capacity of each of
the links is 1.5 times the size of the
flow so instead of going directly from
the initial to final state what I'm
going to do is move half of FA the blue
flow then move half of F then move FB
and then move the other half of FA so
basically don't take don't go directly
but we compute a plan by which we don't
actually rely on any kind of
simultaneous move in there now the
question is like can we always compute
such a plan does it always exist or not
here's what happens so what we can show
is that if we leave a scratch capacity
of s on every link in the network let's
say think of s as being 10% we can
compute a plan that is at most 1 over s
minus 1 so if s is 10% this bound is
basically 9 steps on top of that we can
actually have like an algorithm it's a
linear programming based algorithm by
which we can compute the plan with
optimal number of steps so what we find
in practice is that can we can have a
plan that is about two to three steps
most even though when we leave s is
equal to 10% so here's what we're going
to do but the beauty of the table the
beauty of the problem here is that we
actually have lots of background traffic
so we are not going to waste the 10%
scratch capacity instead what we are
going to do is to use that scratch
capacity for carrying just the
background traffic and once we throw the
background traffic
we can provide the guarantee that the
amount of congestion that the background
traffic faces is bounded and no
congestion is ever faced by non
background traffic the shunt and I
assume we move them in units of 1/5
double at a time so you don't get
multipath we build multipath through
tunnels we don't move them 1/5 trouble
airtime we so think of it we basically
map let's say we move based on volumes
and that volume essentially could be
decided by so we decide ok there's like
a 10 gigabits per second going between
say Eleanor New York and I need to take
five of this and park it let's say on a
path that goes through Houston as an
example and then we change the network
configuration such that that actually
maps to five gigabit per second and what
that actually means depends on the
capabilities of the switch itself if the
switch offers us like a multipathing
capability that lets us directly program
the fraction of traffic that it should
send directly versus indirectly we'll go
reprogram that current commodity
switches don't let you do that so we
change we basically compute the prefix
distribution essentially a description
point within the ten gigabits per second
that lets us move approximately five
gigabits per second James the
possibility of not having simultaneous
update and there's consequent packet
loss against the cost in terms of
latency and throughput in you know you
know this is the optimum but we're not
going to do that yet what does that
trade of I mean it I'm just a tiny bit
skeptical that you can't get two
switches to synchronize their clocks and
change behavior at the same times it's
not about synchronized clocks it's also
about when you apply a rule what we are
learning is depending on how busy the
switch is it can take any anywhere
between after I send the switches
command it can take anywhere between two
to three hundred milliseconds for the
effect of the rule change to happen so
so
the variation you might see is not
purely limited by a clock
synchronization it also limited by other
things and what we've also done even if
you forget this variable delay what
we've based on real traffic what we've
kind of learned is that anywhere between
zero to hundred megabytes of extra
traffic can arrive at a link in these
differential time differential periods
and if you have router buffers that are
off the order of 10 megabits 10
megabytes you will essentially see a
huge drop in traffic
we're actually highly bursty now you
have huge TCP windows they'll see a
burst that goes through and they'll back
off so that's actually gonna that back
off and then it's gonna TCP is gonna
take time coming up so that's the other
thing to be kind of worrying about those
second-order effects that will happen
due to highly bursty losses if you
atomically synchronize the switch change
in Los Angeles for change in Tokyo
there's still an awful lot of bytes
traveling across the Pacific that are
gonna arrive at the other side after you
already know they're already in flight
ok they're just going to slam into the
traffic at the other side background
flows will be bounded congestion seems
like a very nice property probably
relying on priority support at these
commodity switches yeah the strict
priority queuing that so we so I think
yeah what do you get as priority on
forwarding
but not priority on buffer sharing and
if you don't get priority on buffer
sharing which applies to almost all
commodity switches we have experimented
with what do you find is that the
buffers will be filled with background
traffic which means that the foreground
traffic comes and you see losses so I
think in the experiments we've done we
haven't looked too much I should say
like to tell apart my understanding of
prior to queuing is that basically that
essentially
to buffer priorities as well if a packet
arrives there's inter if it's an
interactive traffic it basically gets
priority it gets its own cue and it gets
sent and this is strict priority this is
not weighted priorities these are strict
priorities so what that means is like as
long as the switch has an interactive
trap packet to send it will send it if
not it'll move on to the next category
next priority class and next priority
class and it's very hard to have
dedicated buffering for the class but I
can probably okay so I'm sure it's a
corner case or no I don't think we've
seen that but if it happens we can go
looking for it actually so that'll be
good point to make yeah okay okay so the
third thing was working with limited
switch memory I'm gonna touch only kind
of briefly on it so here's what we do
so the challenge here is that we are
directly programming these flows and
these flows they can have highly
complicated l3 filters so you know a
service could be described for instance
by I don't know like 10 prefixes and
source and destination and we kind of
get limited by how much memory these
switches have so we do two things one is
that we use tunnel based forwarding so
that essentially means that we are not
gonna use IP based filters IP address
based filter anywhere but where the
flows enter so take that filter table
and then from there on we essentially
say tunnel one tunnel two town three
it's a label that gets carried in the
package and the intermediate switches
just work off of that label instead of
like in a hundred dual description off
the flow so they just work off that even
that's not enough
in fact so because that's not enough so
now we essentially think we are
basically kind of like take the working
set idea that it's not like you need all
possible tunnels at any instant in time
yes you need a lot of tunnels in the
network across time but at any given
point in time you don't need all of
these tunnels so think of like why
working set up a process is a lot
smaller than solar memory allocation so
kind of same thing actually happens in
this network traffic because the demand
matrix has temporal variations and so
forth
so what we do is that we basically
greedily pick the tunnels that
essentially can carry the traffic right
now and which is kind of the working set
and we keep only the working set in
memory of the switch and we rotate this
and we've got mechanisms to rotate the
working set this problem I'll be talking
about because if you have n data centers
you're talking about n square maximum
possible times right if n is 50 is in
that 2500 if n is 50 no because if you
have n data centers you need tunnels
between n square pair of air centers now
it's a matter of how many tunnels you
need between two data centers so if you
want 10 tunnels so we currently we
operate our MPLS with 10 tunnels between
every pair of routers so you see where
the scaling problem is coming from yeah
and in general we find like even like if
you wanted like the total allocation
continuing the process analogy it maps
to basically that we need K where K if
you need K shortest paths we need about
15 tunnels to essentially fully use the
network and we cannot support 15 tunnels
between every pair of DC's but the thing
is the beauty of that temporal locality
of traffic is that we can actually do
with only two or three at any given
point in time as long as we can quickly
identify and keep those two or three in
memory and there so the way we rotate
this is actually very very similar to
the way we do stuff to way we do like
congestion free updates by the way the
problem with changing the tunnel side is
we must add a tunnel before we can
remove it because every tunnel is
carrying traffic at any given point in
time so if you remove the tunnel all the
traffic will get dropped so we need
spare capacity in the memory to first
week you should add and then we should
delete the tunnels so you got to retain
that and to do that quickly we keep
essentially a scratch capacity lambda in
the memory of the switches and then we
get symmetric results that you know the
number of maximum number of steps to add
a tunnels would be essentially 1 over
lambda minus 1 in there so again lambda
is 10% we can in in maximum of 9 steps
again in practice it's a lot smaller but
1.diff
between congestion free updates and this
one is this is essentially a discrete
problem versus the flow problem so
essentially the algorithm we have is
greedy but not optimal but in the
congestion in the congestion free update
case we have an optimal algorithm
this one's just greedily select stuff
yeah okay
in case now you're if you're all
confused what all is going on this is
what happens in the entire system every
five minutes we compute bandwidth
allocation network configuration we
compute how the rule change so this is
the tunnels in the memory will change we
compute a what I call like the bounded
congestion plan to change we notify
services that have so this is steps one
two three are purely computational at
step four we tell services who have a
reduced allocation to lower their rate
step five update the network step six
some services would have higher rates we
tell them okay now you can go up now and
this is what time if you're curious
about how how much time it takes each
step and this will tell you like
essentially our bottleneck right now is
distributed rate limiting so if you see
like the two biggest the long pole
analysis is like how quickly can we get
an entire service which could be
thousands of force to speed up or slow
down the other thing is and this is
essentially data driven simulations so
we've taken real demand matrix and stuff
and it's not it's modeling switch delays
but it's doing a predictable model or
switch delays so but that's zero point
six seconds so 600 milliseconds is the
number when the network is actually in
flux and this is kind of reflective on
how quickly we can actually update the
network on the congestion plan is
waiting for the services we have
produced yeah yeah yeah yeah right and
the second ones yeah waiting for it them
to ramp up so if we had you know you
guys have been working on faster rate
limits so if you can get like that
control loop going faster we can
actually do this a lot faster than yeah
yeah times the steps all eight steps on
all the steps on each steps yeah what do
you mean eight steps so you said there's
a bounded number of steps right you have
not yet so because you'll do the steps
separately right yeah so is this point
six for one step you know the entire
change the entire change you run into
the listen these are averages yes which
is you know that point six seconds the
amount of time to send stuff across the
Pacific or you know the longest links is
a significant fraction of that so when
you change switch rule you need to wait
for all this all the in-flight traffic
to finish before you we don't have to
wait for in-flight because the way we
compute what happens in a step is based
on the worst case analysis for on every
link so this is where the scratch
capacity helps us the worst case
analysis is that basically so they'll be
so we are taking a step right so what's
Canon asks is say okay the traffic on
the link will not have moved away before
new traffic arrives so all of these
updates essentially within a step get
applied in parallel and we have a
guarantee that congestion will not
happen then
step one ends we get all the acts back
then we begin step two within the update
plan and and that step two goes in
parallel comes back step three starts
and typically we need only like about
two or three steps I'll show some
results yeah
so here's what we built essentially we
have we have a system running in a lab
close to MS are building 99 we work with
the the first generation of prototype we
build work with open flow switches a mix
of blades and orestes blades are
horrendous by the way these are like
ibm's open flow switches i don't
recommend buying them or Easter's are a
lot better
if you're in the market for open flow
hardware and we worked with this big
switch open flow controller we started
using thirty-two servers with each
server running 25 VMs and so essentially
we get a lot of statistical multiplexing
going to do the prefix based tricks
we're talking about and essentially
they're the prototype also contains some
number of routers and stuff okay okay so
onto onto results so no so failure
notifications come from the switches to
the network agent and we react to
failures right away so that's what
happened there like so what we find is
right now the biggest delay is
discovering essentially the link has
gone down on the switch itself that's
just a limitation of the way lldp works
but the faster notification nothing
failures you have to react right away
otherwise you will end up dropping a lot
interactive traffic as well which is
completely non-starter I think it's the
fastest we could go and evaluate at that
scale in the sense what we've done is
some experiments on that if we changed
five to ten what we learn is that we
lose about two to three percent
utilization which you know from a
research perspective not more but from
an operational perspective it translates
to you know three full-time employees
like in terms of the amount of money so
what we so what we know is that like
going from five to ten we lose
efficiency what we don't know is that
whether we can go faster than five and
what the gain would be and part of the
reason for that is that we have traffic
statistics there are a five minute
granularity we don't know what they look
like at sub five-minute levels so we
can't even do that evaluation until we
have something kind of deployed yeah
are there net flow butBut the grand
light is five minutes yeah or s flow the
standardized net flow
yeah I don't think it's going to be 200
data centers think of think of 25 is the
number
awfully long time and going back to your
arguments about you know we have to do
this funky thing for maximum yeah I'm
sure it seems as an I thought the trick
was very neat yeah 25 descendants seems
like it's not 25 data centers it's
between each pair of data centers there
are hundreds of services so the
allocation is happening at that
granularity so the fairness is between
services right max min if you do the
water filling it's it's a function of
the number of nodes in the graph and the
number of flows I just find it given
that you're dealing with angry high
flows I just found it hard to imagine
the simple progressive feeling would
take forever but maybe you talk about
reserves I'm not gonna show the fairness
computational results I think the the
numbers we show for computation the only
thing that was indicated there I think
we tried to do it this the the direct
way and we could not get the computation
to scale maybe if we talk to LP experts
there was way to formulate it but the
thing is the problem is progressive
water failing has the following problem
let's say you have take take the case of
25 data centers so now you have what is
25 25 like 625 so you got 625 flows
thing is you pin each flow at a time so
you pin the lowest one then when the
link fills up you have to be open you're
basically doing almost factorial
computation because you there's this is
some network-wide allocation here's
what's going on like you pinned a link
fills up so you know that's the max
capacity but when you take the next step
you have to be open to unpinning this
flow and moving it somewhere else and
that's where a lot of the complexity
comes from progressive water filling in
in a network context and you never have
to do that if you're doing single
resource allocation in there but I think
there are entire theory papers on trying
to solve this problem I think we we
implemented one of them it was too slow
for us but if you have better ideas I
think it'll be helpful yeah yeah so this
is a result on on one of the networks
since this public forum can't say which
one so we took real topology
real traffic on this network and what
I'm showing here are three bars so MPLS
te is essentially the state of practice
today and if it worked really well this
is what the red line would be
and the blue line is essentially Swan so
what we see is that we can actually
carry 60% more traffic on the same
network essentially so this is like you
we have some deployed capacity and how
much more traffic we can carry so we can
carry 60% more the green line is
somewhat interesting because it shows
like a variant of Swan where we only
have centralized resource allocation but
no ability to rate limit at the edges so
think of somebody like I don't know BT
or AT&amp;amp;T they don't have the ability to
shape traffic but they do in theory have
the ability to centrally control
resources so if you had just central
control of resources where you configure
reconfigured your network based on
whatever is coming in you're not
controlling what comes in you still see
like about half again so what that means
is that off the total gain that's
happening with Swan based on
coordinating rates of services and
central allocation the gains are roughly
equally split from central planning as
well as quota coordination across
services so you could take this and
apply to 18t and they'll see this much
gain but of course in in a Microsoft
Google kind of context we will see even
more gains because we have the ability
to control what enters the network this
is now beginning to touch on essentially
what happens why we actually need
congestion free updates what is what the
Left graph is showing is that what is
the over subscription ratio of a link if
we were to do one-shot updates just
don't worry about don't worry about
congestion just send all the updates go
from initial to final state in there the
orange line is what happens to
background traffic with one shot updates
the red line is what happens to
background non background traffic with
one shot updates so what you see here is
that if you look at I think the non
background traffic is kind of more
interesting what we see is that our
subscription ratio is for non background
traffic which includes the last
interactive traffic can be about 20% and
if you do the math what that means that
20% over so 20% here means that 1.2 ty
the link capacity among traffic is
coming on the link if you take that one
point two times and we actually
multiplied by that by the time that over
strip sufficient lasts what the right
graph shows is how much traffic ends up
at the link how much more traffic than
the link and carry ends up so what you
see here is this even non background
traffic in this case you'll basically
see cases where 50 megabytes of non
background traffic ends up and in the
background case can go up to like 200 or
300 megabytes of traffic will come and
all of this traffic will essentially get
dropped in a big bundle so this is kind
of like why we had to do some kind of
congestion free planning in the first
place okay this is essentially where we
are right now I think what I just
described where we are headed with this
is we are in the middle of getting
optical capacity to do this deployment
in the wide area so go outside the lab
and get optical capacity and start
running these things in the wild because
we believe we learn something extra
there most of the tactical focus of the
work at this point is and which I showed
like the I think the basically this
these are the kinds of topics our
interns are working on right now and and
they're all related to resilience to
failures and uncertainty what I mean by
the description of failure what I mean
but by uncertainty is the following that
like you know so far we've assumed we
have perfect knowledge of the demand
matrix we have perfect control that we
when we tell a switch to update a rule
it actually updates a rule I think the
reality is a lot more complicated that
we will essentially have highly
imprecise control so if I ask us which
to send red colored packets left we
don't know how much time it will take
for the switch to do that or whether the
switch will do that or not so that's
what I mean by current precise control
so right now I think we're doing a lot
of kind of algorithmic work on dealing
with this imprecise control and
uncertain information in this context
and that's where we are at this point
and in terms of I think a deployment
plan it's it's rather it's rather simple
because of we can because we can play at
the optical level essentially what we
can do is just play it off the optical
capacity and deploy of an essentially in
parallel to the one we have today
slowly move prefixes from the top which
is a Juniper based ran to the commodity
van so without having a flag date we can
have essentially parallel networks and
slowly move all traffic from the
traditional wider network to to so on
essentially okay to summarize
essentially talked about so on the goal
was to yield highly efficient and highly
flexible wire a network and we achieved
that by coordinating transmissions of
services and centrally allocating
resources what was hardest for us
in trying to build the system was to
managing transitions
I think computing allocations was a
computational issue but how to actually
every five minutes compute bandwidth
allocations and every five minutes
change network configuration and do that
in some sane way without losing traffic
or without blowing away the memory are
individual commodity switches the last
thing is generally I think to me it was
kind of revelation in the past I have
not worked on efficiency or performance
related things I think one thing at a
very high level one thing I've kind of
learned from this is like I think if
you're talking about there a whole bunch
of interesting issues now in the space
of efficiency with cloud services
because it's not purely about efficiency
a lot of this stuff is beginning to
touch on competitiveness and running
large-scale cloud services is at this
point is turning to cost whoever can't
deliver the cheapest infrastructure is
going to win out and there so I think
there's a whole bunch of work to be done
in as not just Microsoft a lot of
companies are building other cloud
services and eking out efficiency from
the infrastructure so I think this was
just one example but there are lots of
other examples within a data center as
thing or a server utilization and so
forth where it directly touches the
bottom line of the company and which is
kind of which is great for for systems
researchers at least okay with that Alan
thank you
from scratch without looking at which
rather located before a tively home so I
would like to have a an idea of moving
that is without you switch configuration
completely you can stage it and with
other is a way to continue I think
that's a that's a great part if we can
get that without program it programmatic
complexity we'd love to have that
ability right now I think you're right
we solve from scratch and there are
mechanisms you can take LP to its dual
form where incremental computes become
easy the reason we did that is
essentially it was easy to program that
we could throw it at regular solvers at
the same time I think we have talked to
some people who specialize that and I
understand that we can essentially get a
lot of the gains by solving the LP as we
are doing essentially take its to its
dual domain where we can without much
programmatic complexity we can get get
back the benefits in there
we have not we have not built that yet I
think because once once we figured out
we could do the allocations in in under
a second in some sense I think we had
less of a motivation to improve that
computational aspect in there but maybe
if we start scaling or if we start doing
things we will look at it again but if
you have kind of good ideas what we
might do I'm kind of happy to talk to
you talk to your offline I think I think
what I will talk about I think the
topology abstraction bit we have is
right now has a cost so one of the if I
had a super fast solver one of the
things I would do is to not work on an
abstract apology but actually work on a
real switch and link level topology but
that blows up the number of constraints
and links so a number of constraints and
variables in our LP by a factor of 16 or
so and that increases the LP complexity
by you know if LP is you know n to the
power of three so then
it's too much I think if there was some
cheaper way I'd write if it was so cheap
that I can actually do solve the LP on
link level topology that'll be great we
can go there and if I'm happy to talk to
you offline for any ideas on that front
yeah five minutes for that or for
actually estimating traffic and making
sure you have inputs for the LP and all
that solving the LP right now takes 1.3
seconds
this is doing solving the LP actually
solving it's not just one LP it's like
even like congestion free update
planning is an LP it's a sequence of LPS
all of that is 1.3 seconds right now we
are just using off-the-shelf solver I
don't know how to paralyze that no so we
are not so that 1.3 second number that I
said we are not trying to improve right
now but what I said like if I actually
had let's say two orders of magnitude
faster way to solve this I would solve a
different LP that is higher fidelity so
right now we solve the LP at the
granularity of data centers if you gave
me a method that was two orders of
magnitude faster I would start solving
an LP that was high fidelity and worked
at the level of switches and links
that's what I meant yeah so this one
point three second number we're not
interested in improving now yeah James
it's the same solution like we those
already acted soon so what would be an
example of an analogous problem yes take
sort of a wireless mesh network or
you've got sensor flows or whatever
going around and things changing you're
changing the routing to cope with that
right it's a similar sort of setup so
here yeah so I think the different
series like I said I think I don't know
if a system I think I think the problem
will come up there if you start doing
centralized planning and running your
Lynx heart a couple things go on with
the wireless context I think this it's
also unknown right like what does it
mean to run like a wireless mesh network
heart the capacity models are incomplete
so you don't know even know how much you
can push it and the other is like most
of the work has happened with
distributed resource allocation they're
so distributed resource allocation we
haven't seen that problem but I think if
people started doing wireless mesh
networks with centralized planning and
they got an idea of what a capacity
there even means then I think some of
the same things will apply in the
senders of the challenges they will face
and maybe some of the solutions will
work out as well I don't know yet I
don't know yet
yeah I think there are I think that the
domains we've kind of looked at I think
the the details end up being so
different that I will know I think we've
I mean there's certainly a problem
within the data center to updating
traffic but the scale we are talking
there is so huge that my sense is people
will end up designing approximate
schemes rather than the precise
theoretical guarantee we are offering
with updates I think that kind of our
kind of computation will not work there
I think this thing carries over I think
is in the new work we are doing I think
this way of looking at things it's
actually helping us even at least in
this domain in reasoning about imprecise
control as well because like I said
nearly two in the construction of the LP
there's this notion of worst case so we
can embed that and it's it's helping us
solve other problems within this domain
but I haven't like I don't know if now
the domain where the same thing directly
applies Thomas
so what is the wait for a team meeting
exactly waiting for is it like you know
it's I didn't mention it it's basically
the depends on the period with which so
what's happening is that we don't so
service brokers they don't call back
into the hosts that change your rate
because we didn't believe we could build
a system if you are controlling
thousands of force you could reliably
contact each one of them to slow down or
speed up so instead what happens is that
these hosts they sit in a loop which is
10 seconds every 10 seconds they go to
the service broker and they say how much
can I send how much can I send so after
the Swann controller tells the service
broker hey your capacity your throughput
should go down from 10 to 5 the service
broker knows that immediately but
because the hosts are coming every 10
seconds it can take that much time for
that thing to get implemented yeah I
mean you guys are experts at this rate
limiting so that's one of the things I
think we'd love to improve if there's a
practical way to fit that in</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>