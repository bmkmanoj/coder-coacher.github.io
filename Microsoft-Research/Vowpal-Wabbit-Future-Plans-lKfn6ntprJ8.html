<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Vowpal Wabbit Future Plans | Coder Coacher - Coaching Coders</title><meta content="Vowpal Wabbit Future Plans - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Vowpal Wabbit Future Plans</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lKfn6ntprJ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so I actually do have slides I
think it takes a little bit more fun to
look at this right so the green is the
two things that that the people wanted
quite a bit and the blue is the stuff
that Hal and I were talking about so a
better documentation apparently we
should check out I Python and notebooks
and some way to automatically search
over hyper parameters so that people
didn't have to learn that all hyper
parameters are would be very helpful and
then in terms of okay so this is my
handwriting and this is house
handwriting over here in terms of
directions that we're thinking about
research wise there's some issues with
finishing up the reductions because
right now reductions are kind of
instantiate from the bottom up but they
think they should be insensitive from
the top down so from the complex to the
simple so that needs to be fixed and
others another the second issue
reductions which is sometimes you want
to reduce to more than one kind of
problem so examples of this come up in
contextual bandit learning where you
want to both reduce to cost us to
classification into regression and in
the combination of the two is better
than either individually okay so this is
kind of finishing up the reductions work
and then we want to create an
exploration library because the ability
to kind of choose which data you're
getting can become very powerful and
have many applications so this is
something which is pretty high priority
for us we want to create a global
testing framework I mean not a unit
testing framework here right so unit
testing you have a bunch of small tests
when you have a reduction stack that can
map a large number of very different
problems
onto the same base learning algorithm it
becomes possible to try to apply this
based learning algorithm to a lot of
different problems and then you can
optimize the space learning algorithm
across a very wide spectrum of different
problems right so if we can create a
global testing system that would be
helpful now the thing which is a little
tricky here is it's kind of a trade-off
between accuracy and time if you only
emphasize accuracy and then when you
start optimizing you're going to see
global testing framework you're going to
go crazy because it will take forever so
exactly how to deal with that trade-off
is not entirely clear to me and then of
course playing with GPU seems like a lot
of fun lot of people have had funnels
GPUs recently and then uh what was he
talking about for that one go back let's
go this way oh yes so reductions take a
complex problem and then they decompose
it into a bunch of little individual
problems but multitask learning takes a
bunch of little individual problems and
then solves it jointly right and we know
that multitask learning can be extremely
helpful in many situations can see all
the big deep learning successes are
multitask learning of one sort or
another so it would be nice to think
about based learning algorithms which
actually have a multi task nature and
then that's potentially very powerful ok
so then there's there's how's things so
how's worrying about learning to search
l2s and one of the things he's working
on right now is so you have a joint set
of predictions that you're trying to
make and some of these predictions
depend upon other predictions when the
ones they depend upon or just the thing
that immediately three preceding
predictions it's pretty easy but you
would like to be able to specify I
depend upon this prediction than that
prediction in that prediction so that
you can do more
complex things easily and okay so that's
one thing another is he's creating a
Python interface for so as you can
actually create like that little piece
of code that he showed you for for part
of speech tagging you can create that in
Python which I think a lot of people
find more intuitive to work in for good
reasons and then you can you can fit
that into the system and then it will
run and then this is a dependency
parsing example of learning to search
and then arbitrary ordering to the
search seems potentially very powerful
right now this is a specified order to
the search but the search process could
be abstracted and that could allow you
to kind of choose to label the part of
speech and sort of the most sure thing
first which means that the things you're
least sure about has everything around
it labeled first so that you have more
context information to label things
appropriately right so these kinds of
search strategies could be could give
you a substantial jump in terms of how
high of accuracy you could achieve
automatically and then better I think
it's a better base algorithms I don't
know but I think that's the rest of the
reduction stack there which is used to
be further optimized
ok so questions
there's a the core issue okay so this I
think is kind of creeping number of
different things that needs to be dealt
with the issue which keeps coming up
with documentation is do you want to do
research today or do you want to
documentation today that's definitely
what the trade offer keeps coming up in
my mind but yeah I do i do understand
that better documentation is needed
making some of distributed or do you
think gpus is just strong enough Oh VW
is already distributed yeah so uh so the
impactors yeah so I i personally regard
linear learning on a big cluster is a
solid problem the terascale learning
paper I think does it very well yeah
rationalize and report oh right we need
to pester me row about L bfgs in
particular because it fails in ways
which are not very clear but I think one
of the problems is when you're writing
the code you don't know what an
international air statement is so it's
actually very helpful if people just say
that's an irrational error just I will
pester Mira about that yeah to be
rephrase my question yeah do you have an
engine for distributing stuff which is
kind of generalized which could be used
by US open source projects yes I do
there's a separate library inside of our
and inside a VW called called all reduce
and that that I'll reduce library allows
you to use all reduce on 222 paralyzed
learning algorithms so you in that that
with that that's designed to be
compatible with Hadoop clusters as well
as as well as you know your own personal
set of machines or whatever so that i
think is at something which i think
children could easily use to paralyze a
lot of different algorithms relatively
quickly because the number of lines of
code / algorithm which you need to
change he's like to typically
all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>