<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Critical slowdown for Ising model on the two-dimensional lattice | Coder Coacher - Coaching Coders</title><meta content="Critical slowdown for Ising model on the two-dimensional lattice - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Critical slowdown for Ising model on the two-dimensional lattice</b></h2><h5 class="post__date">2016-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kWu_Nf8XVcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I I happen I happen to be a member of
the scientific committee so as a part of
the general introduction let me say that
we are grateful to Microsoft Research
and the pacific institute for
mathematical sciences for financial
support civil arm and well I guess the
only thing that remains for me to do is
to introduce the first pickle a
landslide who is a postdoc at Microsoft
Research and you will see who speak on
the Ising model thanks Chris and thanks
for the to the organizers for giving me
the opportunity to speak this morning so
I'm going to be talking about joint work
with ER lovetsky my colleague from
Microsoft about the two-dimensional
easing model so the easing model is a
model that comes from a statistical
physics it's a model of magnetic systems
kind of the magnetic particles in
something like iron when it's one of the
most studied models in statistical
physics and there are literally
thousands of papers about it so you can
imagine a bunch of particles each butch
will all either be plus or minus and
they're given and their positions are
given by some graph structure so the
graph G and in for today's talk you can
always think of this as some subset of
the D dimensional lattice and so it's a
probability distribution over
configurations where a configuration is
just an assignment of either pluses or
minuses to each of the vertices and
these pauses or minuses I'll call spins
and so the probability distribution
weights configurations according to how
many pluses and extra pluses and minuses
next two minuses sort of favoring though
configurations that have more like spins
in neighboring vertices according to
this Hamiltonian type formula and so
when
and the strength of those interactions
is governed by the parameter beta which
are called the inverse temperature and
so when beta is a zero this is just the
uniform distribution and all the spins
are independent and when beta is large
sort of there'll be a tendency for most
of the spins to be next to most of the
vertices to have the same spins as their
neighbors okay so there's been a lot of
work on this on the easing model going
back well since easing easing started
working on it in the 1920s and for the
two dimensional model that of since the
work of unsung in the early 1940s it's
been known well established so the exact
location of the phase transition of the
models so when beta is small in the high
temperature phase sort of all the spins
the pluses and minuses are sort of the
correlations between them decay very
quickly sort of exponentially in the
distance and so there's no structure its
kind of looks like random noise whereas
at low temperatures you have a lot of
structure mainly either and you mainly
have pluses next two pluses and minuses
next two minuses and and the reason why
you have plus on the top and minus on
the bottom is because of the boundary
conditions imposed here and then and
then the point at which you have a
transition between these two regimes is
at the critical temperature which is at
a beta which is exactly nine ok and some
of the most interesting mathematics
happens at exactly at the critical
temperature and i'll talk more about
that layer later in the top so this talk
be about the grabber dynamics which you
can think of as a dynamical version
the easing model so it's a markov chain
used for sampling from the distribution
and and so you could which you can think
of as have the spins randomly evolving
over time so each vertex gets a rate one
pass on clock and when the clock rings
for that vertex you look at the
neighboring vertices and work out the
conditional distribution of that vertex
given the neighbors and this will just
be a function of how many pluses and how
many minuses occur amongst the neighbors
and the the interaction strength beta
and so then you just resample the vertex
according to that distribution and so
this is just one up updated the dynamics
and the the distribution is stationary
and 0 gotik and reversible or the the
markov chain is 0 gothic and reversible
with respect to the stationary
distribution so eventually if you run it
a lot enough time it will converge to
the stationary measuring you and so the
main question for this talk is when you
have a large system so the number of
vertices is large how does how long does
it take for this Markov chain to reach
that sort of statistical equilibrium so
what's the mixing time for it so there
are a couple of different ways you can
measure the rate of convergence of the
Markov chain one will be the spectral
gap and this will be the main one I'm
I'm going to be talking about for this
talk but you can also talk a bit about
the mixing time the in terms of total
variation distance so this is
essentially how long it takes for the
Markov chain to be close to its
stationary distribution from a
worst-case starting point say within
epsilon in the
variation distance and the total
variation distance is just like the l1
distance between the measures okay and
and so the question is the different
values of beta the inverse temperature
how do these parameters vary as the size
of the system grows okay so there's been
a lot like a lot of work on this topic
and there's a general picture for
different values of data so when when
beta is large there's the the strengths
of the interactions are strong there's a
bottleneck in the for the Markov chain
and it takes exponentially long to mix
so if we if we fix sort of our graph
being a box in the D dimensional lattice
of side length n then the met with say
either periodic or free boundary
conditions then the mixing time will
tend to grow light exponentially in n to
the D minus 1 on the other hand it's
expected that at the critical
temperature it should be just polynomial
in the size of the system and then at
high temperatures the the spectral gap
should be of constant order so not not
growing with this well not growing
beyond a certain point with the size of
the system and the mixing time should be
of order log n and it should also have
have the so-called cutter phenomena
which essentially says that if you look
at the distance of the Markov chain from
stationarity it in the total variation
distance say it goes from close to one
to close to zero over a very short time
period so this is kind of so some of
this picture is known for some models
like the easing model or parts of it
on different graphs and so I'm going to
be talking in particular about the
two-dimensional lattice so I'll tell you
so this is what's kind of expected by
physicists and in is sort of in line
with which what rigorous results have
been proved so far okay and for the for
the easing model a lot and yeah a lot is
known and so in the particularly by work
in the late 80s and early 90s by de
Bhushan and sloshed man eisenmann struck
zeglen ski and culminating and work by
Martinelli and his co-authors both the
the high and the low temperature regimes
are quite well understood so in
particular at high temperatures when bed
is small and the interactions a week the
the spectral gap is known to be
uniformly bounded in the size of the
system and the mixing time is known to
be order log n for any beater less than
the critical value B to C when you're in
two dimensions in higher dimensions this
is only known for sufficiently small
beta and you've all asked the question
does do the dynamics obey the cutoff
phenomena and recently with y'all we
established this and show that the
mixing time occurs that the inverse
spectral gap times log n up to the lord
of factors okay and at low temperatures
sort of work of Martinelli and others
confirmed sort of exact pretty much
everything you could ask about the
mixing time the inverse spectral gap
that their ex for natural in n and even
the constant in the xfer natural is
known explicitly so
oh so the remaining question then is
what happens at the critical temperature
b2c and does this obey the sort of
polynomial growth that was predicted
okay so first of all it's known I it's
been ma yeah it's known that the it
grows at least as fast as a polynomial
and this comes from sort of results on
correlation decay which go back to on
saga in the 40s and you can also work of
Holly in the 90s so it's at least
polynomial and numerical estimates by
physicists seem to confirm the
polynomial picture and show that the the
inverse spectral grap seems to grow at
something like n to the 2.17 so and this
is this seems to be universal in the
sense that it doesn't depend on the
square lattice it also seems to be the
same exponent for say the heck set
two-dimensional hexagonal lattices or
triangular lattices as well and yeah but
but this 2.17 this isn't a prediction
from some sort of non rigorous that or
techniques or heuristics this is just
what comes up in the numerix and there's
no actual conjecture about what the
critical dynamical exponent should be
for this and in terms of rigorous work
sort of before the work that i'll tell
you about in a couple of slides there's
there were no sub exponential bounds for
the for the mixing time
so so it was an open question as to
whether or not it was actually
polynomial it could be anything up to
exponential this was the question has
been answered on the complete graph and
the regular tree and even in these cases
it was a very challenging result okay so
so the first time I gave this talk
someone asked the question it's like you
know what happens in high temperatures
and low temperatures that's like leaves
only one particular value of beta why do
you why do you care so much about this
and and it's not just a matter of like
completeness and trying to get it for
every value some of the most interesting
mathematics occurs when beta equals B to
C and in particular it's part of the
sort of revolution of the last decade
abbott's that have been sparked by the
advent of SLA and so so recently in the
last few years smirnov described the
full scaling limit of the critical
easing model in terms of if you look at
the boundaries between the plus clusters
and the minus clusters saying that these
converge to the conformal loop ensemble
which is like a collection of SLE type
curves and for this another work he
recently won the Fields Medal and and
this convergence to SLE we don't use
directly in our proof but it motivated
some of the ideas of it and in but we do
use Russo Seymour Welsh type estimates
which I'll explain later on which were
part of this program of convergence to
SLA
okay okay so the main result is that yes
indeed sort of yeah this inverse
spectral gap is bounded by some
polynomial in N and this holds the
arbitrary boundary conditions and the
yeah and saying arbitrary boundary
conditions here is not just sort of
trying to be as general as possible it's
really important for the proof that we
prove it not just for one particular
boundary condition but for all of them
because it's a the proof is in a sense
inductive yeah so so yeah the inverse
spectral gap grows as a polynomial and
fairly standard estimates show that this
also extends to the mixing time in total
variation distance and that's also a
polynomial but we don't actually have we
don't actually know what the constancy
here is just that there is some fixed
constant okay this also gives a
polynomial upper bound for the sort of
coupling from the past sort of approach
to sampling to perfect simulation for
the easy model at the critical
temperature and this was one of the
examples in David's with Jim props
original paper on coupling from the past
and perfect simulation so we can say
that the expected running time is
actually polynomial and we can we also
give a an improved lower bound on the
mixing time of into the 7 over 4 and so
this doesn't
the sort of predict or the numerical
experiments but 7 over 4 is maybe not so
far from 2.17 but we don't yeah we don't
know how to sort of improve this even
heuristic alee okay so so the main
techniques are a multiscale approach and
so you can say what does it what does it
mean to have polynomial have the inverse
gap growing at a polynomial rate what
means say you double the size of the box
it means that the inverse gap should
increase by at most a constant
multiplicative factor so you have to
remember where breakfast and the food is
and if you want to know the wireless
code you should probably write this down
so
so we know let's say we know that if you
look at the bottom two thirds this mix
as well and the top two thirds that this
also mixes well how can we take this to
sort of a result in terms of the mixing
time of this whole box and that will be
the the main the idea of the proof and
this is this is essentially what people
do in high temperatures and and the
point is there to take advantage of the
fact that at high temperatures you have
sort of strong decay of correlation so
the effect of vertices here is going on
a vertex here is very weak if if the
distance is large and you can
essentially because it decays
exponentially you can in a sense just
add up the effect for each particular
vertex but this approach just doesn't
work at the critical temperature because
rather than having exponential decay of
correlations you have correlations
decaying at some slow polynomial rate
and so in a sense our proof is about
saying what not what is the effect of
conditioning a single vertex but what's
the effect of conditioning the entire
face of a box and proving some weak
spatial mixing results about this and
that will make the proof go through so
yes this slide I'm going to just explain
it in pictures so suppose we have a box
of say roughly equal side lengths and
two boundary conditions sigh and easy
and they agree on three of the slides
the site is not equal to ETA on the
bottom boundary what can we say about
the two measures inside this box well it
could be that one is all plus on the
bottom and the other is all minus so
towards the bottom of the box the
measures will be very different
potentially so you can't hope to say
that they're similar everywhere inside
the box but what if I just want to say
that if this is if this is distance are
here above some level row times are what
if I'm only interested in the measure
above here and so just to make it even
simpler just the measure in the top half
of the box how much does changing the
boundary condition on the bottom affect
the measure here and our result is that
if you look at the projection of the
easing model on the top half of the box
for the two different boundary
conditions then the total variation
distance is sort of less than or equal
to some function of Rho here and it's in
particular strictly less than 1 so
another way of formulating this is you
can say suppose we take these two
measures we can couple them so that on
the top half of the box they agree with
some probability bounded away from 0 and
and so this this statement is just a
something purely about the static easing
model there's no gravity amex at all in
here but now given this statement I'll
tell you how we
do the inductive step to show to compare
the spectral gaps on different scales
and this is done using the block
dynamics which is sort of the standard
and a multiscale approach to
understanding spectral gaps for the
easing model and under other spin
systems and so the idea is you so let's
say lambda is the rectangle that we want
to know the spectral gap foot 4 or 2
bound it on and so break lambda up into
some possibly overlapping sets bi then
you can bound this the inverse specter
of gap of on lambda by the inverse
spectral gaps of the on the BI on on the
sub blocks but also taking a maximum
over all boundary conditions on these on
the blocks and this is this is why we
need a proof to work for all boundary
conditions so that this induction will
work okay and then there are correction
terms as well the first one is just the
how many times vertices appear in
different blocks and in our construction
you can just take this as being a two
because they'll only be two blocks and
the other one is the inverse spectral
gap of what's called the block dynamics
and this is a this is a generalization
of the global dynamics where rather than
taking in vertex than updating it
according to its stationary distribution
you take you take a whole block delete
all the spins in here and update it
according to its conditional
distribution given the configuration on
the compliment of the block and this is
again reversible with respect to the
stationary distribution
and it will be simpler to analyze so in
particular will take just about the
simplest decomposition into box that you
could ask for so that's not very good
when the temperature is critical yes so
this this is all about I mean this
result holds for any temperature but the
analysis i'll tell you now be relying on
the fact that it's at the critical
temperature because it will rely on this
spatial mixing result which is for the
critical temperature which is so i guess
i should have said this is false if
you're at low temperatures and so in
particular that's why you won't get
polynomial mixing at low temperatures ok
so we have our box and split it up into
two blocks the bottom two thirds of the
block and the top two thirds and our
dynamics will go pick one of these
blocks delete all the spins in it and
update the rest according to the
conditional distribution given the rest
of the block so and what I want to do is
show that this has a uniformly bounded
inverse spectral gap and I can do that
by showing that it has a uniformly
bounded mixing time for any external
boundary condition and I'll do that by
showing that given any starting
configuration the probability of
coupling with in two steps is uniformly
bounded away from zero so so let's say
your first step is you update the top
block and so now when doing updating
that you delete all the spins in the top
two thirds and then look at the boundary
condition you have three of them are
given from that by the external boundary
condition in the problem but the bottom
one comes from the spins in the
complement of the block
and so these could be different in the
two copies of the chain but the spatial
mixing results is that with some
probability that's uniformly bounded
away from zero we can couple them in the
top half of the block yeah yeah the top
half of the block and so the top third
of the of the whole thing and now say we
update the bottom block then if we've
coupled the top half then they have the
same boundary condition in both copies
of the chain here from coming from the
top boundary and the other two three
boundaries are just the same because
their fix external boundary conditions
so given that we coupled the top part
the top third then we can in the next
step couple the rest as well so so this
gives a positive probability of coupling
it in just two steps and this and so in
particular the mixing time is uniformly
bounded and the inverse spectral gap is
also uniformly bounded okay so so then
well if you plug this into this form
into the formula so we can take the
block dynamics to be have an inverse gap
which is uniformly bounded this n will
just be too so the inverse spectral gap
on the larger block is bounded by a
constant times the inverse spectral gap
on these sub blocks so we can now just
do induction and each so each time the
volume decreases by two-thirds and
there's a multiplicative factor of some
constant in the
in verse spectacle gaps so you do this
sort of order log n number of times and
you get some constant to the power of I
guess to log base 3 on 2 n which will
just be in to some constant so this this
gives the polynomial upper bound on the
inverse backdrop gap so so the key thing
is proving the right spatial mixing
result and and and that we can indeed
say that when we have two boundary
conditions we can couple the top half of
the block with the deposit probability
bounded away from zero so it would be
easier to say say how to do this first
rather than for general boundary
conditions I want to say how you do this
for two specific boundary conditions
because in some sense it this was the
motivating idea so suppose on the one
hand we just have the all plus boundary
and on the other hand we have plus on
three sides but minus on the bottom okay
this is yeah so the spins giving the
boundary condition will be yeah plus
here and minus year okay so now you can
ask well I want to try and couple these
distribution so that they agree in the
top half let's let's reveal the kind of
cluster of minuses on the bottom she'll
be some which will give some random
curve and with minus minus is on one
side and pluses on the other
okay and above above this curve you now
have about plus boundary conditions on
all of this all of this space and so by
monotonicity I can couple these two so
that above this random curve okay i can
put pluses along here as well and above
this random curve i can couple them so
they're they're equal because they just
have both have all plus conditions
everywhere so the question then is in
terms of whether or not the top half is
the same is does this random curve
exceed the sort of half the halfway
point and and the point now is that
smirnova kind of results in theory say
that this curve should converge to some
SLE type curve well to an SOA curve and
in particular well I'm not going to tell
you all the properties of that but one
thing will be that with positive
probability it will stay in the bottom
half of the curve or in the bottom half
so in the scaling limit the probability
of this curve staying in the bottom half
will be strictly bounded away from one
and probability of it staying the the
chance of it yeah okay yeah
okay yeah so the chance that it crosses
the curve will be less than 1 okay so so
we can so for these two particular
boundary conditions we can couple the
top half with some probability strictly
greater than zero independent of the
size of the system but our inductive
proof really needs the result to hold
for every boundary condition and there
isn't sort of the theory of the we're
also using the monotonicity of these
particular boundary conditions but also
there isn't really a theory of what the
scaling limit should look like for
completely arbitrary boundary conditions
and so instead to deal with this we pass
to the FK easing model so there's okay
so in the app yeah the FK model or the
random cluster model is like a just
briefly like a kind of like a dependent
model of percolation so so you do
independent percolation but then re
weight the probability according to the
number of clusters you see and in the
easing model case it's just according to
2 to the power of the number of clusters
and then the Edwards socal coupling
gives a way of relating the easing model
to the FK model but again the this
arbitrary boundary condition that we
have is problematic because it
conditioning on a particular boundary
condition in the easing model is like
conditioning on certain wirings and the
boundary but also on the event that
there's no crossing on the FK model from
a plus vertex to a minus vertex and and
this could happen with and the chance of
this event could be
it's been naturally small for particular
boundary conditions okay like in
particular if your boundary conditions
were plus minus plus minus all around
but but this is this is a decreasing
event so we can use monster necessity in
the fkg inequality and you can translate
the problem into asking if you look at
the corresponding FK model does the
cluster connect it to the bottom exceeds
say the halfway point and and this is
and to this we can use the new russo see
more welsh estimates developed by dunal
copÃ¡n Hong laura nolan and which show
that this is this probability is
strictly or the probability of exceeding
this line is strictly bounded away from
one okay and yeah and so what we can do
is outside of this component that's
connected to the bottom boundary we can
couple everything else and then transfer
it to the easing model and by the edward
socal coupling and couple everything
there as well okay so a couple of open
questions would be and these are
probably not so easy can you calculate
the precise sort of universal critical
dynamical exponent we don't even really
have a guess for what the correct answer
there should be and also what can you
say in high dimensions like and in
particular in dimension 3 okay thank you
okay good you again how you are you
couple Dean pads on the bottom okay yeah
so so first of all you reveal you sort
of reveal the sort of minus component
and just by looking at one vertex at a
time is this minus or plus and do that
until you can't go any further and then
so by monotonously argument you can say
that well along this curve because this
is even more plus it should be at least
as at least this plus which is all plus
and then and then in what's remaining
you have all plus boundary conditions on
both and so then it's just yeah yeah
yeah exactly so you sort of already
assume that you have the militant couple
and yeah
right we you mentioned that the
numerical experimentation suggested that
there should be some kind of universal
exponent around 2.17 do you have
intuition for why that shouldn't depend
on the type of graph I don't have
intuition yeah
no and and I think businesses probably
do and I not not really sure
all of it
yeah it's not but for a two-dimensional
system have swear ladders and the
hexagonal lattice Wow questions
I got a quick question concerning on the
metal of simulating the process
basically you know you're asking
question about convergence two of the
stationary distribution or from yeah if
I understand correctly of the whole
procedure would be done at a critical
temperature at the simulations but the
estimate also armed to the mixing time
assumes that you can start from any
initial configuration right yeah so my
point is this what it makes sense to
start your simulations at the high
temperature we have exponential
convergence to something so we get
immediately from some extreme or unusual
initial configuration to something that
looks maybe more standard yeah or normal
and then switch the temperature so this
is this is a goes by the name of
simulated annealing and burying the
temperature as you go and in some models
this does improve the mixing time I mean
definitely you can show that say in the
he looked at the complete graph for the
easing model you could prove that this
that it gives you a faster way of
rounding the mixing time at low
temperatures because there there's a
bottleneck between mainly plus
configurations and mainly minus
configurations and if you do it at high
temperatures then it kind of lets the
Markov chain choose which one to take on
the other hand there are there are other
models like the pots models for which
it's been proved that this kind of
technique doesn't doesn't work so it
depends on the model and the graph on
your results just to the square that is
ok you handling that
do the certs okay do the resource email
well she estimates at Y and those ones
as well
so if as long as we know these are
resource in my welsh type estimates then
everything else goes through
and they don't
so it depends on that and I'm not
exactly
if they don't
reduce the single worst yeah
to get back to you about okay I mean
yeah follow our messages yes so your
results don't use the fact that in this
case you know explicitly what the data
sea is that you don't need to use that
land yeah yeah it's and the result
depends on crossing probabilities and
make a attack relations because I mean
they're selling three balls like
surprisingly fought for every easing and
for easing they don't any lattice all
these confirm their theory and
population isn't but try to get yeah
yeah so some some of the results the SLA
results it does hold in general I'm not
not exactly sure whether or not sales
like wrote it down or after the ones we
needed
but but really the important thing is
crossing probabilities
so your purpose is certainly robust to
the left yes yes
ok that's the speaker again so our next
speaker is a way ma who needs no
introduction to people who've been to
the seminar frequently and um yes so
thank you martin and thank you i have to
also thank our local organizers from
oregon who do my head out this time to
be a speaker from oregon and also not
only maybe I don't need an introduction
for people who've been here before but
this is also a topic the last time I
spoke here I think it was the same kind
of topic because there'll be different
material new results but but similar
topic and its joint work with colleagues
that this is a graduate student to Lanka
Elohim alaj who's here today and
vaishali bakili and Me Kate Oman and
Brian Wood we're all in math except for
Brian who's in environmental engineering
and so forth and um this is work that
was supported by her one of these
Geoscience math Geoscience grants from
NSF I should mention that too I suppose
and I'm going to start by just sort of
describing three different examples this
first example this is this right here is
brian's lab and it's a little bit this
right this is the actual apparatus what
it is is it's a column that has 22 glass
beads packed in and one is this coarse
grain glass beads and the other is very
fine glass and so on one half the top
half say is is the coarse grain and the
bottom half is the fine grain sand and
it's saturated with water and then you
know I always envisioned these things
until I saw his lab I always thought
they would be horizontal but his
vertical but basically you put dye in on
one end and then you the dye runs
through and and and you measure how long
it takes to get up come out the other
end and so basically what you have in a
as a model of this sort is a a fixed law
of a flux law basically the standards
like create law or flux law but the
diffusion coefficient is different in
the course in fine grain so you have and
you have an interface there where
there's an abrupt change in the in the
diffusion coefficient and then usually
there's a there's also a flow through
the constant rate through the through
the system this is these are results of
an experiment that were performed at the
Lawrence Livermore Berkeley Lab of a
very similar nature in 2009 and in fact
what I just showed you was Brian's a
confirmation of these results in the lab
at OSU so what they what they are are
basically they're called breakthrough
curves and they measure how long it
takes two to go from one end to the
other but they do it in two different
frameworks one is that you put the you
inject on the coarse grain in and you
you retrieve it on the fine grain in or
you can inject on the fine grain in and
retrieve it at the coarse-grained in a
completely symmetrical situation however
what these curves are showing and now we
the probability problem that you get out
of this is that if you have a diffusion
coefficient D minus here and a fusion
coefficient D plus here and the
interface is here and you inject at
minus 1 and you see how long does it
take to come through out here versus if
you inject here and see how long it
takes to come through here with the
velocities completely reversed which is
more likely to be removed first that
becomes sort of the question and what
the experiments are showing is an
asymmetry in
have breakthrough to curve so I'll come
back to that but that's the sort of a
motivating question involving a problem
with an interface the second example is
looks quite different it looks like a
geophysics or oceanography type problem
however it's actually it is that but it
is really the point of emphasis is more
from ecology and the reason that it's an
ecology problem is that if you look at
this string of lights here this is off
the coast of Argentina this string of
lights here is a string of fishing
vessels that are have been there
routinely forever I as I understand and
what's going on is that that this
there's a break in the shoreline that
where the where the the slope off here
is fairly flat and shallow and then
there's a there's a steep break and at
that break is exactly where those
fishing boats are lined up because
there's an upwelling that's occurring
there and it's bringing all these
nutrient rich water from cold water from
below and bringing it up so it's sort of
a a foodservice mechanism for for these
fish and and it's a very very important
upwelling occurs I'm told in like one
percent of the ocean is it it's and
however upwelling is responsible for
like over fifty percent of fisheries
that's the kind of thing that you can
read about these these things so and it
does upwelling events do have a big
impact on fishing and fish food as a
mean as a resource and what what's
characteristic of this are somewhat
unique about this particular one is that
it's due to this break in the in the
slope and normally these cup these
upwellings are caused by winds this is a
model this is just a show I don't want
to go through any of the details it a
derivation of a model which describes
the free surface where you have a break
in the slope down here that that that
represents sort of the coastline of
of Argentina and when you derive for the
free surface when you derive using just
geostrophic balance physics equations
you get an equation which looks like
this and this term here because it's in
the southern hemisphere is negative so
if you put that over on the other side
you'll have a minus times that and this
this is going to be again a constant
roughly it's going to be a constant here
and a constant there to indicate a sharp
break and so again you have a diffusion
equation for the free surface with a
with a well that should be a minus sorry
yeah that's that you have two different
diffusion coefficients at an interface
the interface being caused by that break
so that's my example number two of again
a diffusion equation of where there's an
interface and the third example is an
example that it's literally from my
backyard in some sense i'll tell you why
the fenders blue butterfly is a
butterfly that's on the endangered
species list and and in fact it's in the
Willamette Valley of Oregon where where
there's weather where where they live
and what they do is they live in these
what they like or these pets these
patches of lupine here's what when Lupin
looks like but they like these big
patches of Lupin and the ecologists
because they're on endangered species
there's a lot of literature statistical
literature about them and I just quoted
here one just to give a sample of the
kind of questions this is from a ecology
journal given past research on fenders
blue and the potential to investigate
response to fetch boundaries in this
system we asked to central questions
first how do organisms respond to
habitat edges and what are the
implications of this behavior for their
residence times so I indicate this just
as another example what what's going on
is there's a distribution of these Lupin
patches throughout the region and when
butterflies are in these Lupin patches
they're very happy and they tend to kind
of just wander around and and not move
too quickly but when they get outside of
their
you know they kind of worried and they
kind of fly around all over the place
they have a different kind of dispersive
ax t rate when they're outside a patch
is what vs. when they're in a patch so
this is sort of just three examples to
show the the somewhat natural
consideration of interface problems
what's different about the three
examples however is what what the what
the what happens at the at the interface
in the first example because of just
conservation of mass the natural
interface condition is continuity of
flux across the interface because this
is consistent with with what's observed
is consistent with this ficken sort of
model of diffusion and so forth however
for the for the free surface in the
oceanography example the cut it's the
continuity of the of the of the
derivative that that is imposed by the
physical considerations in the case of
the butterflies and so forth this is
sort of a new example to us that kind of
getting interested in but I'm not quite
sure how to do it to address what the
interface condition actually is and that
will require looking at more data and so
forth but in general these the interface
conditions are in conditions on the at
the interface of this form and um you
know you can by dividing this by d + + D
minus you can think of a lambda there so
it's a and here it's lambda is equal to
a half so you have different conditions
of that form so this is probability so
let's let's kind of at least go back to
a little bit of probability and if you
if you think about although one can
argue whether fellers classification of
diffusions it is certainly motivate
about probability but but it's more or
less than analytic resolution of that
question but I just kind of reminding
you hear that feller according two
fellers theory you know given these
coefficients just measurable
coefficients there's a there's a
diffusion corresponding to that because
it will determine a speed measure
and a scale function and so you can ask
yourself or you can try to work out what
would be the interface condition in the
case of fellers diffusion that you would
get from this we can talk about that
later similarly one could go to strip
marathon and you can pose a martingale
problem so stupid on given those
coefficients and give it would for the
given generator would say you know you
you look for a martingale for all F and
C infinity for those given coefficients
and if you apply that then what happens
as a consequence of the stroke marathon
formulation you get a you get a solution
but because of the the test functions
are all seen in the domain or else the
infinity functions you get a interface
condition which is the continuity of the
derivatives so then you can ask you know
what do you do about other interfaces
from the point from this point of view
and one approach to answering that
question is just to say well you know
you can buy transforming space you can
make if you have a given interface
condition so I denote that by if
belonging to I lambda that's just
notation to mean that you have this
interface condition well then you can
transform space so that the composition
is is with one half so it's continuous
derivatives and then you can adjust time
and then solve for the the process as
it's true queried on problem using this
and then sort of go backwards and get a
solution that for the given problem but
again this is still kind of analytic
machinery it's kind of an analytic
approach to this problem I mentioned
here out nine who also has worked in
this in this area and and and he by his
approach has been one of them was to
look at I wrote that operator in
divergence form and you can and then you
look at it as a diffusion with
generalized coefficients and I believe
that he has a very similar resolution of
this what i call this remedy to to the
martingale problem anyway so i just
mentioned it here but i'm going to put
up here this is sort of mice the
students from osu know this one my
favorite quotes and it's a quote from
that you know I don't know that how many
probably see this but it's a from
jean-baptiste parens little notebook
called atoms which is just a wonderful
little thin book which is the notebook
that he wrote well he was doing
experiments to determine Avogadro's
constant after Einstein had shown that
that diffusion the diffusion coefficient
could be viewed either statistically or
physically and and and so they said
about to do all these experiments and I
just want to just I for me I I think I
you know he got a Nobel Prize for for
determining Avogadro's constant well
that's an important thing you know so
it's worthwhile but this to me has
really shows his genius and he says at
the time trajectories are confused and
complicated so often rapidly that's
impossible to follow them to ejector II
actually measured is very much simpler
and shorter than the real one similarly
the parent mean speed of grain during a
given time varies in a wildest way in
magnitude and direction and does not
tend to a limit at this time taken for
an observation decreases as may be
easily shown by noting this is really
primitive camera technology that they
had but they it's amazing that they did
do these kinds of photography and they
went from twenty a second every five
seconds and and or minutes 25 seconds to
20 and seven seconds and it is
impossible to fix a tangent even
approximately at any point on a
trajectory and we are thus reminded of
the continuous under I've functions of
the mathematician so you know I was
always kind of you know method we always
get scooped by physicist but in terms of
what people really know and so I just
really amazed by this this quote so but
i really put it here because now i want
to get away from really the analysis
problem and asked more the question of
you know what would / in see if you were
looking for
Apple at this solute under a microscope
and the first step toward the
probabilistic solution to that is given
by the notion of skew Brownian motion
which I did talk about here once before
and probably is familiar to most
everybody here but basically you have an
interface condition on the functions in
the domain of the of the laplacian and
they asked the question well how do you
construct the stochastic process which
would have this as it's a Markov process
with this as its generator on this
particular domain and the answer is that
you take a reflecting Brownian motion
you start with that and then you and you
enumerate the excursions and and then
you go through and you toss coins and
and if you get a hit a tail you flip it
down otherwise you keep it up so plus 1
is head- one's tails and you go through
and so then this gives you a formula in
terms of the reflected Brownian motion
ANS being the plus or minus ones and and
and j ends being the excursion intervals
so you get a nice formula 44 Brownian
motion and then this was this was
actually in a paper of Ito and mckean
and and eventually then appeared in
their book as an exercise is it I those
of you in this there's a wonderful
presentation by Henry mckean at the
Osaka spa meeting and he really conveyed
this idea that that with Ito that that
it would all of the all of the analysis
about whether he could understand
something I'm paraphrasing quite a bit
here he said it much more eloquently was
really or whether you know what was the
particle path doing he wanted to know
what the stochastic process was in order
to understand analytic results and and
so so this this also is in that spirit
of trying to understand the solutions to
these problems in terms of the
stochastic process so so now with with
with Ito on the key with with it give
in the the existence of skoo Brownian
motion that doesn't really solve the
problem of what is the particle process
that you're going to get in these
various examples because you have to
tell me what alpha is for one thing and
probably if you can tell me that then
you know we can scale it by square root
d+ when it's on the that regime and
square root d minus the scaling is going
to be relatively straightforward but you
have to find out what alpha is so what
we're going to just consider here that
same problem is just very much like the
e-toll mckean problem with an interface
condition here that's given by lambda
and and pose the proper martingale
problem and the proper martin gore
problem which would correspond to these
conditions is given here and then I want
to now take advantage of our guests and
a very early result of his i think is
PhD thesis i guess where jean-francois
showed the existence of solutions to
stochastic differential equation in this
form where this is local time and you
this is so you apply jean-francois
theory and with a little ito Tanaka and
ultimately you get a solution to that
martingale problem by observing that the
process satisfies this stochastic
differential equation with this local
time term so all you have to do really
then is make this equal to zero and you
then you make this equal to 0 and then
that tells you what alpha is in terms of
lambda and the diffusion coefficients
and so the solution is given here just
doing the algebra alpha is given by this
combination of diffusion coefficient and
and interface parameter if we apply that
to the examples the first two examples
really the only two examples that I have
where I know what the interface
condition is in the first one if we in
the first one the lambda was a d-plus /
d + + D minus that's because we wanted
to have continuity of flux so you just
put that
this equation up here and then you find
out that the proper alpha alpha star is
given by this ratio for the continuity
of the derivative of lambda was one-half
and you substitute that in and you get
this is the proper alpha to use for the
representation in that problem there is
a one of the really nice consequences
after eat or McKean of the introduction
of skew Brownian motion was a paper by
john walsh where he was observing i mean
at the time the questions i mean other
than reflecting Brownian motion where
behalf about having continuous semi
martingales with discontinuous local
times and john walsh pointed that out as
a basic property of skew Brownian motion
and one of the questions what are the
questions that we've been trying to deal
with ultimately we would like to be able
to talk about these phenomena and higher
dimensions there are many higher
dimensional problems the butterfly
probably being an example but there are
others where you'd like to go to higher
dimensions and we're trying to find
other ways of thinking about how you can
arrive at this parameter are these these
kinds of proper these kinds of x
transition probability transmission
probabilities alpha or transmission and
um and one of the things that we
wondered about was what would happen if
we go to example one in fact I think the
last time I was talking about this stuff
after the meeting I was talking to
someone yeah and wondering about you
sort of out loud about whether one is
there was some physical reason for maybe
you could get continuity of local time
as a way to to also characterize this
alpha and it does turn out that that in
example one that the modified local time
is continuous if and only if alpha is
equal to alpha star so this is another
characteristic characterization of this
particular
a value of alpha is continuity of what
we're calling your modified local type
so what is modified local time modified
local time means that in defining the
local time typically you you integrate
with respect to quadratic variation of
the process so you know for Brownian
motion it doesn't matter because it's dt
is DT and it is a little big measure but
for these processes it turns out that
the usual local time that you're
considering is where you integrate with
respect to quadratic variation and of
course for skew Brownian motion it's
also lebesgue measure so but in the case
of the of this so-called physical
diffusion for this given constants if
you integrate with respect to lebesgue
measure then in place of the quadratic
variation then you do get continuity and
I hope I'm making this as clear as I
want to make it the point is is that you
know there's a physical process
associated with these problems and that
physical process is not skew Brownian
motion it's got it hits scuba
emotionally with this alpha and scaled
by those coefficients so it's a very
special structure that way and and it's
the local time of that process if you
define local time by integrating with
using the big measure instead of
quadratic variation that will be
continuous and the reason that that's of
interest as I say to us is that we're
just looking for other ways to
probabilistically characterize these
choices of parameters and I must say
though that Brian Wood is the the
engineering I the environmental science
person in this project he was really
happy when we changed over to that
definition of local time for this result
because it had physical units that made
sense to him that didn't make sense to
him ever when we talk about if you think
about the units of local time it's not
time in the case of when you define
integration with respect to quadratic
variation anyway so let's come back I
want to now use this to look at this
problem
that you you get let's suppose it so I
have the course of defined grains glass
on this side and the coarse grain on
that side and I could either inject and
retrieve it over here or inject it here
and retrieve it over here and the
question is which one would get out
quicker do people have intuition about
that I won't take a vote but speak up if
you do okay so the answer is that
according to the experiments that if you
inject at minus 1 it will arrive faster
one then when its reach injected here so
find the course is faster than course
defined and I just want to kind of go
through a little bit of the exercises to
explain that so the what one would first
dude is 22 well here's the theorem it's
I guess it's a little bit stronger
there's a constant times that actually
that is less than but there's a
stochastic ordering of the breakthrough
times in going from minus y 2 y versus
going from what we're here y is equal to
minus 1 or Y is equal to 1 sorry yeah y
is equal to one and that so that so
there's a stochastic ordering of the
first passage times in going from left
to right versus right to left and it
really is a strict inequality and
there's actually a constant there but to
prove that and I don't want to I don't
want to go through the I just it's too
much here on the screen but the obvious
place to start is with skew Brownian
motion right you just take skew Brownian
motion and you take alpha bigger than
half and then you see if there's a an
ordering here and there is an ordering
here between the first passage times for
just skew Brownian motion and there's a
fact this is a factor of 1 minus alpha
over alpha in in that ordering so this
is less than that so that there is an
actual stochastic ordering and and
the proof is based on looking back at
this there's a natural coupling that you
can you know you can write there's plus
or minus eens all in terms of a single
sequence of uniform random variables so
you have this so it gives you a way of
comparing the effects of alpha and
drawing the curves and and and so forth
so there's a way of actually using that
representation and to to make this
calculation so at least for skew
Brownian motion you show well there is
that kind of stochastic ordering of the
price each times but you know for the
physical diffusion process it's it's a
little bit different because they're
also you have to scale that and the
scaling kind of has the opposite effect
as alpha and and so you so I it's shown
here in the calculations because they're
appearing in reciprocals of these of
these scalenes however nonetheless I'm
just going to have to believe me but
because i'm not going to go through I'm
not even good at explaining like on
these slides like this anyway but I want
to give enough here to show that they're
they're competing things going on
between the Alpha and the end and the
diffusion coefficients over in the Fast
and Slow regime so you basically work
with four different inequalities sorry
and and and you end up showing that the
that there is a stochastic ordering of
the passage times so that's consistent
with the experiments
this is that this is actually my my last
slide here because there's an
alternative and I think this is more or
less what I talked about last time when
we were here an alternative to this
stochastic ordering and this is a more
of the first passage times is actually
to compute the the resident
concentration I mean to solve the PDE
and one can do that by a change of
measure and and and it converts the
problem into a problem or ultimately
what you really want to know are the
distributions of a skew Brownian motion
position its local time and an
occupation time of the positive axis you
have to know how much time it's spending
on the positive axis and we were able to
derive a nice simple density which i'm
just showing here for for for the triple
density in the case of these these three
processes that basically extends there
was in the case of the case of Brownian
motion this triple density was known by
who was a i can't remember now the
formula had been known in the case of
where this alpha is a half but in that
case you can plot this is actually just
the flux and then rescaled by a
cross-sectional area and the velocity so
this is really just the flux and you can
you can buy a plot of that you also see
these curves which are showing the same
kinds of shifts that are showing in the
experimental curves the question of
whether or not those our first passage
times or these kinds of flux curves you
know from an experimental point of view
is a little bit i thing can go either
way because it's probably it's really
neither because of you know
experimentally when you when you measure
are you really measuring to measure
actual first passage
is is a non-trivial object as well as
measuring this actual flux so they're
both consistent with experiment and
finally I'll put a problem up that that
we can add to you Vols list in the case
of first passage time distributions for
skoo Brownian motion recently to Lanka
and Dan Sheldon both then is a postdoc
at OSU and to Lanka they were able to
compute the first pass each time for
skoo Brownian motion by analyzing
excursions of Brownian motion and that's
a that's on the archive but what so
what's the open problem is that to try
to find a formula for skoo Brownian
motion with drift and you know unlike
the case of Brownian motion where that's
an easy problem because you do gerson
off and and so on in this world of
discontinuous coefficients and so forth
you have to contend with local times all
over the place and and it makes it not
so obvious at least to us so I think
that's I think that's the end of it yes
thank you we'll make what excuse me one
more thing so I usually try to help out
with Bernoulli society these days and I
have in the back forms one is for the
next elastic processes conference which
will be in June in Oaxaca Mexico and I
put the flyers back there and also
especially for graduate students because
membership is free and Bernoulli society
it's subsidized it's subsidized by the
originally Society for graduate students
and so there's membership forms the
membership forms for everyone but in
particular graduate students have an
opportunity to have free membership and
there are benefits for that thank you
good
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>