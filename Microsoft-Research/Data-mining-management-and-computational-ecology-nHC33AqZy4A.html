<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data mining, management and computational ecology | Coder Coacher - Coaching Coders</title><meta content="Data mining, management and computational ecology - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data mining, management and computational ecology</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nHC33AqZy4A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so it's my great pleasure to introduce j
lu j is a principal researcher at
microsoft research his research
interests root in understanding and
managing physical properties of
computing and he's a distinguished
scientist of ACM since 2011 j has been a
partner of several of our projects and
it's it's really a pleasure to have him
here and he's going to be presenting low
energy location sensing for mobility
okay thank you so I'm going to talk
about some system support for for doing
science in particular one property that
a lot of data are relied on and we use
it every day the notion of location a
long history of location sensing if you
like but things really changed since
1977 when the GPS system is launched and
after that it goes through this normal
from bulky too small to smaller devices
from military applications to civic
asians from dedicated devices for gps
navigation to a more ubiquitous service
that we see on our phones location is
everywhere today for example we look at
Windows Phone apps 10% of them used
location services and then there are
dedicated devices like photo tagging
like healthcare scientific applications
like water flow management animal
tracking lots of them use GPS as the
fundamental basic location service
unfortunately they all treat GPS as a
black box this is a trip a module you
buy from off the shelf and you integrate
into your sensors and you're done with
it and that suffers a lot of issues for
example
the energy and the availability issues
my GPS modules they consume a lot of
energy we all know from our experience
if you turn on your phone for continuous
GPS sensing for six hours the phone will
just run out of battery the other issue
it has is availability if you're indoors
you don't get gps location and you're
lost track of the entity wanted to track
so this work is really tried to solve or
tackle those two challenges to give a
low-energy GPS sensing that you can
integrate into your sensors and to look
at another way of doing door location
sensing so you can have continuous
sensing through your applications in
order to describe it I'm going to go
through a little bit of how GPS works we
know there are 32 satellites for the
u.s. GPS system the 32 satellite in the
sky the time synchronized to nanosecond
level accuracy and their ground stations
that manage them and they send up these
trajectory information the so-called
ephemeris which got broadcast down from
the satellites and the time and
trajectory of these these parameters are
sent in a very slow data rate these are
sort of outs by outer space
communications right the data is at 50
bits per second and that's the key
limitation of why GPS modules takes so
much energy to run because you have to
decode all those information in a very
low speed the receiver estimate location
using essentially two parameters one is
estimate the time of flight from the
satellite to your device and it solves
the equation given where the salad is at
that moment and the time of flight is
solve the equation in the least square
sense typically to find out your device
location and that computation process is
is relative simple but estimating the
time flop flight and the exact locations
of the satellite that takes a lot of
effort in particular your typical GPS
we will go through the following
processes the first step it does when
they receive the signal in the baseband
or intermediate frequency is to do the
acquisition process this to find out
which satellite they can see and in
particular of the Doppler and code phase
of that the signal coming out of the
satellite so what is essentially looking
for is a spike in the in the Doppler and
code phase space if you see the spike is
say okay I i see that satellite if it
doesn't see a spike it says I don't see
that satellite and that information is
useful for calculating the the final
occasion then they go through the
tracking tracking can do so you can do
acquisition every millisecond you cannot
you continuously do tracking every
millisecond in order to be able to
decode this 50 bits per second
information that come down from the
satellite every six second you got a
time stamp so you get better chance of
getting the time Stein information your
your every 30 second you got the fully
framers so this is our experience with
car gps it takes 30 seconds when it
first start up to acquire satellite and
that's what it what it was doing to
decode all that information your phone
is a little bit smarter it has a firmer
asst information from the network I only
Tico's the time stamp which takes six
seconds so this is your experience when
you turn on the phone to use GPS it
takes about six seconds on average to
get your first location so all those
information feed into the least square
computation and that's a little bit
competition on the way on the device
itself to get you the lat/long so we
look at this process we think what can
we what has the device has to do versus
things we can offload for application
that we don't need turn-by-turn
navigation we can hopefully to do it in
the cloud so we can save the device side
of an energy consumption for example
code phase you have to come from the
device this is specific to that position
but other things like ephemeris which is
the trajectory of the satellite NASA
published this in the cloud service you
don't really need that in the device
time stamp is a tricky question
traditionally people have to decode it
from the
message that come down from the
satellite but there is a trick the so
called the course time navigation that
will avoid you to decode anything from
the satellite the way it works is to
leverage the speed of light is so fast
if you have a reference location you
have something nearby you know its
location already then the propagation
delay for the RF signal leaving from the
satellite to your unknown device versus
to that known location it's going to be
the by and large the same up to the
millisecond part and the sub millisecond
part which is what the code phase will
give you and that we need to decode from
the device device themselves so given a
reference location we don't really need
an end a rough time stamp this will so
called the course time we don't really
need to decode anything from this outlet
that saves all this process of decoding
information which has to keep the GPS
module always on but what if you don't
have a reference location this is
something you tag onto an animal you
don't know where it goes then this a a
deeper method you can do based on the
Doppler angles that you can do
intersections so each of the sunlight
when it moves with the speed and when
you look at the satellite from the from
the ground you see a relative speed NASA
also publish the absolute speed of that
satellite the two-speed give you an
angle and that defines a cone leaving
from the satellite going towards the
Earth and you do multiple intersection
of those you get a reference rough
reference locations in order of
kilometers but that's good enough to
give you a reference to to use the code
phase to refine it so that's what we did
for the device ID I only logs raw GPS
signal as small as two milliseconds
since the time synchronization issues
and so on and then it has a reason about
the time stamp that attacked to that
chunk of data that got sent into the
cloud we have a cloud service that would
do the acquisition like what typical GPS
would do but in software and then it
gets the satellite ID I look into the
NASA orbit database to get a firmer us
using Doppler and intersection we've got
reference locations using the code face
a frameless reference location and
device times then we plug into this
course time navigation process and give
you the lat/long locations so that's
what we did we build a sensor some of
you may have seen this last night which
looks like this right now which is
essentially a gps logger with a very low
power microcontroller to log the data
onto a SD card if you look at the power
consumption it's dramatic so this is a
trace may not be very clear from your
view we have two milliseconds of
sampling time of taking the GPS samples
and then about 30 milliseconds of time
to write it on the flash so if you
calculate the current usage of that on
average we use for milliamp if your time
3d like 10 milli watts of power and 30
milliseconds to to log the data what it
translated into is even with five of
these chunks for better accuracy we use
two million joules of energy versus your
phone gps use about two jewels for every
location lock in the sense we are
thousand times more energy efficient and
in theory this translated into if you
have a pair of double A batteries it
will last for a year in half to do
continuous GPS logging and one second
granularity we build a web service i'm
going to show you for processing offline
data this is a data log we collected
this is about one megabyte of data that
i took when was writing from from san
fran sounds the airport to one of the
microsoft labs and right now in this
moment we're sending that to the cloud
service and the location resolved that
service comes back this give you a sense
of the the performance through this
wireless network and so on to the cloud
service we have four instances of the
service running for every individual
data points right now it takes about
four seconds to resolve
and I give you also a sense of the
accuracies and so on this one outlier so
I'm going to give you more statistics on
the performance we evaluated over 1,500
traces both them moving traces and
static traces the median error is about
11 or 12 meters from the ground truth
the mean error is about 20 the maxxair
is pretty big we're still looking at
ways to to reduce that if you look at
the distribution a 30 meter is sort of
what we we believe the system can do as
more than eighty percent of the the data
is within thirty meter accuracy we also
break down to the number of satellite
that's a key parameter that determines
the accuracy once you have seven or
eight satellite in view actually goes
the thirty meter mark goes to more than
ninety percent so more than ninety
percent of the samples we get better
than 30 meter accuracy so it really
matters how you define or design the RF
circuits and so on one of the other
things we care about is the device time
synchronization accuracy how much times
time error we can tolerate on these
devices so this is a valuation of that
so we can tolerate up to one minute of
time ever so you've synchronize the
device at the beginning of your
experiment you let it run as long as
within a month you have a GPS sample the
clock won't drift more than a minute and
then after every GPS resolution we get
the new timestamp and we can use that to
correct future samples so time
synchronization is really not an issue
in these in these devices we're going to
screw ski or a little bit don't have a
lot of time I'm going to talk a little
bit about indoor location sensing there
are lots of applications people are
looking at now to study human beings and
behaviors and so on one of the key
location metric there is the indoor
activities and indoor location indoor
location has since GPS doesn't go
indore has been primarily done in a very
different way using signature based
approaches so basically a profile indoor
space their RF signals or other signals
you can use as as signatures either
dedicated infrastructure you send out
beacons we're opportunistically based on
Wi-Fi and other things Wi-Fi is probably
the most popular one right now and then
if another device sends the similar
signature then you say aha I've been
here and that's the location it suffers
server limitations Wi-Fi is a bad signal
it's absorbed by human body it's with
more or less people the signature
changes it changes over time or not that
we in this particular work look at a
very different kind of RF signal which
is FM radio this is sort of your
standard 50 year old technology even if
you go out of this building actually you
see one of these towers this is a AM and
FM tower as I figure out today so the
good thing about FM radio is they
actually penetrate building very well we
all know we can receive FM radio indoors
a human and small furniture movement
like the arrangement of the chairs in
this room for example won't change the
FM signature there are a lot of channels
they give proper the possibility of
defining a diverse signature that we can
use to identify a place the other
benefit is the receivers with all these
years of refinement the receivers can be
done really really low power in the MIDI
milliwatt order so but on the other hand
there are not that many FM's towers out
there this is the FM towers near Seattle
area there's about 20 of them people
have tried this before you use FM radios
for outdoor location signature and the
performance was really bad it was lucky
if they can get city block level
granularity what we're leveraging here
is the building structure actually
create unique signatures for FM radio
that from one room to an
because there's a war in between you can
accuracy very different FM structures or
signals so we did an empirical study at
three places one is this office building
our beauty 19 I and Microsoft a mall in
the Seattle area and a apartment in in
Maryland interns house and we use a FM
development board that allow us to log
for different metrics signal strength
signal-to-noise ratio multipath
indicators and frequency offsets and we
use 32 channels to to build a signature
database and this is what we get with
RSS alone we got eighty-seven percent
accuracy for room level location
repeatability meaning if your profile a
room with certain signatures we come in
there again eighty-seven percent of the
time we identify which room agree if we
combine all for FM signatures we got
about eighty-one percent so this is not
that dramatic from Wi-Fi so Wi-Fi alone
also get eighty eight percent so roughly
at the ballpark we're dealing with RF
signals and they're noisy and so on but
the good thing is the signature of the
the the error distribution of Wi-Fi and
our FM is very different if we combine
the two we actually got ninety-eight
percent accuracy of determining which
room you're in in a fairly complex
building give you some intuition of why
they works good a Wi-Fi give you a sort
of good granularity in tens of meters
range because there are a lot of more
Wi-Fi stations and they're dense but FM
give you much finer granularity based
within that tomita range because of the
wall and so on so if we combine it too
because the two signatures behavior
differently you actually get better
chance of identifying where you are we
did more evaluation in terms of time and
in space Wi-Fi behaved really bad if you
don't profile it very often so this is
after a month of of so we collect data
in one month after months we did the
same using the same database and try it
again Wi-Fi goes down to about fifty
percent this basic says Wi-Fi have to
profile every week but FM doesn't go
down as much right so you can profile it
in a much lower frequency and with a
more database you've built actually the
performance goes up so as long as your
profile it accumulates and the
performance goes up just to summarize
the energy consumption piece if you if
you use like mobile devices like cell
phones as you're sensing platform cell
tower scanning is the cheapest way but
it gives you probably kilometre level
accuracy GPS the cloud upload GPS
receiving the way we're doing is very I
gave you about 30 meter range accuracy
but it's very power efficient FM radio
give you a few meter accuracy indoors
you spend a little bit more Wi-Fi
scanning actually you take more time and
energy to scan Wi-Fi than FM radio your
phone a GPS one is successful it takes
about few seconds to get the location
but it can be very accurate if you fail
to actually take a lot of time to
realize it doesn't lock to any satellite
and fail so there's a sort of spectrum
of location sensing capabilities based
on your energy requirement your accuracy
requirement you can mix and match
combine them into your application so
looking ahead what we're doing on about
this I'll we are building the next
module is about this size which is this
is the gps receiver module that's sort
of refined from this sensor and we
expect people to use it so we're going
to open source the hardware design that
people can
use it integrate into your own
application and your own platform we're
going to open the cloud service as long
as they send us the data with the right
format we're going to process it and
give you a location we'll keep working
on from the research side improving the
accuracy and speed so really the message
is there really a spectrum of location
sensing that has the opportunities for
us to build devices that can be lighter
smaller they were last longer and have
broader range of applications than just
the standard off-the-shelf GPS modules
you can get today right what's that
thank you any questions any questions
yep
one of the most important things for
environmental sensors is being
weatherproof so it's great that it has
very long lasting and it's super that
you have you can identify exactly where
you are imagine if I were to say deploy
this at River and want to see how
potentially you know the river flows
down an association one maybe with
imagery data so how well does it work in
that type of condition so uh this is
what people do today right that's what
people do in river this is a project at
Berkeley civil engineering department
and what they did was they build a
floater that sort of sealed and they put
a cell phone in them they throw it in
the water down the river have this
fishing net to capture it it lasts for a
few hours that's a month at the amount
of data they can they can collect with
capabilities like this you can have much
smaller ball of floaters rubber duckies
if you like right throw them into the
water catch them at the downstream or
have them at certain point upload the
data right if you if it went past a base
station or something trigger it to
upload the data then you can have these
disposable sensors just let them flow
that's a much finer granularity today
well this is very expensive to do any
one of those sensors and the granularity
because of the weight and the size you
can you can sense the final green a
water flow but with a smaller sensor the
granularity be much better
and we do a lot of work with anyone
tracking with gps gsm VHF a transmission
the old one is VHF and now you have
satellites that can give you rated
colors at a good price although
expensive what is the main advantage of
your technology compared to the what is
used now broadly on wildlife tracking so
one is weight right it's very small and
the cost because it's not a full bloom
GPS module it's just a front end it
doesn't need that much silicon to
process it on the device and because
it's low energy you don't need that big
of a battery which is most majority of
the weight where the majority of the
weight come from and so all those are
benefits are gonna have something
smaller and lighter and cheaper and you
can add your own radio to it right in to
collect the data wirelessly right now in
this particular device will build your
pop of SD card if you plug it into a PC
or a mobile phone and you upload the
data obviously for wow that wild animal
tracking you don't necessarily want to
capture the anymore repeatedly and
upload the data but you can have this
short range radios like zigbee like
Wi-Fi but only turn them on
optimistically when they're near a base
station and upload the data that way any
other questions I do have one so we kind
of touched it when you mentioned the
river and you know a little bit of
Christian scenario but what are the
scenarios that were not possible before
or you know that you would enable
through this technology Oh or at least
one that you think it's gonna be rightly
benefit so again has to do with the
granularity of location sensing things
that so one of the things we collaborate
with was scientists in in Australia
they're trying to track so called the
flying fox which is
kind of bat that can travel like hundred
kilometers per night to eat fruits and
spread seeds and so on they're
interested in tracking them and the
challenge there is for one the god and
night so even solar panel won't work
right and for but the good thing is to
actually come back to roughly the same
location right now they can only get one
sample per 30 minutes because of the
energy constraints and the weight they
carry about 30 grams of weight so with
with this we're looking at much more
finer granularity of tracking and solar
solar panel to charge during the day and
use them at night will become feasible
which it was not feasible before great
any other questions so thank you to and
now we have Kristen bonacic with an
associate professor in the School of
Agriculture and Forestry at cooked chili
right thank you very much first of all
I'm not a computer engineer but I
brought one with me and Draya so for any
problem she will defend me from your
questions basically what i want to tell
you today is about our software which
has been developed for the last two
years thanks to the support of relaxer
and Microsoft Research first I want to
say a few words about what I believe are
the main problems to comply with the
fourth paradigm in Latin America in
terms of how we share data how we know
and use I see technologies and how
difficult is for the case of wildlife
conservation to collect the data from
the species in an era of intensive data
generation we have the problem that we
still know little about in the new
species can we work backwards please can
you move backwards or not another one
now there sorry a we still know little
about endangered species many animals
are not easy to detect are cryptic or
nocturnal and population density of many
in dangerous witches are very low also
in Latin America due to the constraints
economic constraint and the vast areas
of wilderness that are still remnant is
very expensive to conduct surveys
somehow incidental encounters Oh
tourists or even professional from other
countries that come on visit can
encounter a wild animal and take a
picture of the end or take a record that
information is gone or missing is not
used by conservationists on the other
side we have the lowly change when you
talk to people who are not from the ICT
world they are shy or they don't want to
explore tools that could be complicated
for them
so they have gix experts in one office
to help database experts in other and
they have the managers in another place
but they don't talk to each other and
the things and the tools are very
specialized so how we how we break that
barrier and the last barrier that we
have is that you can have it in a single
country in Brazil or until a for example
the Ministry of the Environment owning
data and using their own databases the
Ministry of Agriculture using data and
known in their own little databases I
they don't talk to each other and when
they transmit information between
different bodies governmental bodies
universities or even NGOs the way that
they transmit information is through a
PDF file or a comment or a phone call so
we want to break those barriers by
having something collective and open
also cloud computing is not really well
used or used in a massively and many
historical information is storing papers
emails or without being utilized so
citizens saying science may play a role
on this and that's why we developed life
on this but life and this compared to
other citizen size software around the
world there are many is different in the
way that is building blocks from local
to global and it's also a tool that
allows you to upload share and visualize
and do analysis with information for
management purposes so it's not just fun
and a learning process as we havin a
multimedia encyclopedia buck word please
as we have a multimedia encyclopedia
embedded within the software can you go
backwards once thank you and again sorry
Run Run button backwards again backwards
please
they're so the the system has the
problem now that we need to think how
how we share information but make this
information available for everybody and
everybody mean scientists Power Rangers
conservationists and people at large so
in order to accomplish this task this
challenge we we have thought that some
Microsoft tools can be useful to
integrate in a platform with mobile
devices and a web-based platform the way
that we can enter information share
information by uploading sharing and
visualizing visualization of the
information we also did some study cases
now we collected and rescue historical
data from environmental impact
assessments that were done in until it
for the last 25 years and we encounter
that some of the only a few amount of
data more than 600 report only we could
encounter 350 records of reptiles and
less from amphibians that we're located
with a timestamp and allocation from
many many studies so but that baseline
information is useful for us to know
where the species are distributed and if
we have that information we can do for
forecasting so for example this is the
case for the reptile this is the
database was loaded from store achill
data we found that two species are the
most common fortified species in 14
regions wondering has more records than
any other and the most common species
reported in these environmental impact
assessments and the exact location of
the H sighting as is now available in
life on this and that information is
building out by users now in our country
we have more than 2,000 record
and it keeps running for 840 species
which is our entire biodiversity for
birds mammals amphibians and reptiles we
have more than 300 users and the next
steps are to move I expand life on this
to North America to by national park
with Olivia's a small study case and in
the future we expect to increase the
number of species that we can handle on
survey and integrate into insects sea
life plans we have several challenges on
in order to jump into the next step
database expansion how we move into a no
relational database and I will live
Andrea with you to explain some of the
challenges for from the point of view of
computing
life on this application has two main
components the first one being the
website and the other is an APA of
windows communication foundation APA
that aids to use mobile devices
connected to live on this sharing a
common database between the website and
the mobile applications levantas
mainland strong point is the data
visualization it allows Microsoft
Silverlight and it's five of you where
have been essential pieces of the
visualization that live at the supplies
also with the available information
using Lync and it's curry capabilities
we have been able to do great data
analysis also with Bing Maps we've been
able to display all the information
that's that's geocoded in live on this
database this is the constellation of
Microsoft tools that live on these uses
and its main components for sightings we
use asp.net with sequel server using a
polygons to detect where the species are
located for visualization we use mainly
Microsoft Silverlight but now we are
moving to each html5 and also bing maps
and lick you for statistics and data
mining of the signings right now live on
this is being used by Park Rangers
scientific community and decision makers
of wildlife preservation in Chile but
future steps includes Bolivia our North
America so right now we are facing a big
challenge how do we make live and this
growth levantas in this context is about
to migrate to Windows Azure platform to
ensure scalability and availability this
migration brings two main challenges for
us
our database will be adapted to see
Usher and because of the scalability and
availability of it and we need
considering this new scenario to tweak
how we use Bing Maps today to take
advantage of this faster database also
is it possible to use sequel usher in
conjunction with sequel spatial to
enrich the yo coded data that we are
uploading to date you live on this how
do we adapt a windows communication
foundation API that we are using today
to run with Windows Azure as at its
maximum possibilities how do we make all
these mobile devices phone surface to
communica municate with this new life
and ease summarizing live on this is now
facing the challenge you use all the
cloud power to this new stage we need to
enrich our geocoded data may be using
all your video and sound in conjunction
with the GPS data we are now receiving
one awkward place
no that was a surprise so we are now
facing the challenge of me migrating
live band is to Windows Azure we want to
enrich our geocoded data with new type
of information to attach to every
signing also we want to take advantage
of the cloud services to extend live and
is to other mobile devices and also we
want to try to develop an algorithm for
image processing to recognize the photos
being uploaded to the system using
windows azure processing capabilities
sorry about the mistakes with the remote
control and just to finish me we want to
tell you that our next step is to do
life and this for for my hatton
particularly for Central Park we have a
request from a colleague in the works in
conservation in New York and they want
to do a citizen science prize in Central
Park so we're very proud to say that we
hope that in the next few months we'll
have life and this completely renew and
written for a English speakers to do a
citizen science praying in in New York
we are open to stablish new
collaboration with other countries and
start block by block building life on
this from local to global in order to do
that we would like to suggest the same
partnership that we have developed in
our country which is conservation
biologist or wildlife ecologist like us
working together with computer scientist
building this application according to
the needs that we need to solve in the
ground from simple to more complicated
from local to global that is the key for
life on deck success we have new
partners now in Mexico we are hoping to
start something as well in Colombia and
we are receiving several requests to
start expanding life and this as a tool
for conservation
finally I would like to say that we have
a little video four minutes that lacks
ear and Microsoft Research prepare and
perhaps the organization committee could
show that video at the end of the day if
they want to thank you any questions
what are your thoughts about handling
the fact that reporting's from from
tourists and so on may not be very
accurate in some cases they may be very
accurate because you have eco-tourists
who go to see birds or things they are
fairly well trained but then you have
other tourists who are not very well
trained so how do you deal with the
accuracy problem within the system we
have options to allow and to assist you
in the identification process and you
can also we can interact with the users
and sending messages or asking for
revision but we need to understand that
the whole concept of sharing data and
using using citizen science is not
necessarily always correct what means is
that the cloud of data gives you a trend
for example for the case of Chile you
cannot enter a monkey or a crocodile
because it's not in the list of species
that you are constrained and you are
also constrained geographically to enter
only species and animals in the
terrestrial land of chili so there are
some barriers that impede you to make
mistakes but we have also the fact that
you will always have some mistakes
so for example it happens fairly often
that birds will show up in places that
they are never expected to because a
storm will blow them or something like
that right so if you if you limit the
input that citizens can do it improves
accuracy in some ways but it may mean
that you will miss rare events and that
was the whole point right was to record
rare things and take advantage of the
crowd you're quite right that doesn't
look very important point somehow we
need to be aware that we could encounter
a new even a new species right so the
system of reporting that has to be
inside the software regardless that is
not in the in the list that's
particularly important in dynamic
ecosystems like like in Latin America
with so many places that are still
unknown so in that sense every country
who will have life on this as a set of
curators who are specialists with within
each taxa because you don't have one
specialist for every single living
species in the world but in order to to
also argue in favor of these
restrictions I have to say that some
other citizen science or open software
allows you even to invent a scientific
name or to put an elephant in the middle
of Sao Paulo and then the whole concept
of contributing to conservation and
science is more difficult to grasp as
something that it will be useful but
this is a compromise any other questions
I thank you and we appreciate what
you're doing because I have the
experience working natural reefs with
the part Minh does your system includes
some kind of report for example we had
like a tiger that no one has seen that
for 40 years but they used to see the
trace you know the footprints do you
have this kind of capability that
someone report this kind of know exactly
life and this gives you a mean you a
menu of opportunities you can register a
track you can register the actual animal
you can even enter a that somebody told
you that an animal was there so then we
can then with people viewer we can
search and separate different kind of
information when you create your account
also you also self assess your level of
expertise so the idea of life and this
is to create a community around the
species and to close the gap between
nature and people and the multimedia
encyclopedia that is builded in is done
only using being resources for searching
with an algorithm that allows you to
find exactly the animals that you are
using for example if you put in any
search engine in mountain lion or cougar
you will find the Cougar I American
football team as the first heat so we
develop a way that when you enter into
our encyclopedia you will have pictures
videos the IUCN information for the
species and the factual information for
the species and not for the American
football team I'm sorry if somebody's
from the Cougars team okay okay we may
have time for one more question yeah so
thank you Chris who knows but are very
interesting there thanks Christian
and now we have professor Geller surely
he has an extensive biography but I'll
just say that he is a professor and
researcher at the biology institute at
UNICAMP and he will be presenting the
use of database and information systems
to improve public policies of
biodiversity conservation and
restoration well thank you very much for
the invitation I think that Juliana has
set up this very well having Christian
bonacic speaking before about the life
and is and how to track and register
endangered species I'm going to talk
about a system that we developed for the
stage of San Paulo for biodiversity so
when we talk about biodiversity in Latin
America we are talking about big data
it's the continent with the largest
amount of species of all vertebrate
groups and all plant groups so it's a
lot of information to be organized we
also have a very large community ethnic
community that gave origin to the people
from Latin America and obviously
interfer with biodiversity by selecting
species or selecting varieties of
species like other places in the world
europe in the 1500 us in the 1800 symbol
estate in the 1901 to a spirit where
most of the forest was cut and replace
it by coffee was the main confrontation
state so together with their the forest
we also have this areas of cerrado this
is savannah's of brazil and what's left
from the cerrado are moralized eight
thousand little fragments
there are less than 20 fragments that
are bigger than 400 hectares that's the
minimum size for keeping population of
some of the large mammals so the
simplest eight we've lived with three
realities will have very large cities
like San Paulo campinas the metropolitan
area will have a very well-developed
landscape for coffee sugar cane and
cattle and we will also have some
priests in areas were preserved so when
we started the discussion about
developing a system for biodiversity in
the say to San Paulo we face it this
challenge that we have a lot of theta
but very to of understanding and even
less of sustainable use of biodiversity
so the challenge was to develop a system
where I could bring together the
information train people in biodiversity
and also use these for policy
improvement so we started a program for
possibility by auto program was started
in 99 based on the convention on
biological diversity it was sign it in
92 and working with all kinds of
organized from my quiroga needs to hire
plants and vertebrates terrestrial
freshwater and also marine and all kinds
of studies inventories taxonomic reviews
to landscape ecology and also the human
dimensions of biodiversity conservation
and sustainable use so the first step
was to be able to share data that's
important to have I standard way of
collecting that data so we had two years
of fight among biologists because we
needed a single record system for people
that were working with micro organized
with birds or with thats so in the end
we managed to establish
data banks structure were on your right
you have all the techs anomic
information of the species and never
left you have all the metadata the
projector collected it the researcher
when it was collected where it was
collected the coordinates and we also
developed a digital map of the remnants
of vegetation in the state of sample
that allowed us crossing the information
from fuel observations with remote
sensing to map 34 different types of
vegetation in the state we also recover
crossing data from soil climate and old
maps of vegetation the distribution of
vegetation original distribution of
vegetation in the state so oops could
you go back mice regard so this is the
original vegetation you have the
rainforest sir along the coast you have
the pine forest so the monkey puzzle
forest tree in the southern in the limit
with piranha then you had the symmetry
series forest in this blue greenish and
then the yellow is the cerrado so this
was what could be the vegetation the
state before human interference and
that's what's really what's left so
almost all the sum of the Cedars forest
is gone you have only one large bit here
that's the state park most of the pine
forest has gone as well and you have
left some of the rain forest along the
coast because the mountains were too
steep to blend coffee not because
somebody thought it was important to
conserve so having the maps and having
the database of species we develop the
tools to cross information so we could
get a name of species look it up in the
map map the distribution of the species
where it's been collected in sample
and you could from the map go back to
the data bank and get all the
information who collected it when it was
collected all the coordinates and all
also the list of species that was
collected in that area so after ten
years we had a data bank with more or
less 12,000 species and 100 and more
than 100,000 records for this species
the vegetation maps and a system that
was integrated working with linen server
postgres and using standard protocols so
any query will go to the data bank of
species and also the data bank of maps
and the vegetation information we also
working with their biological
information that was collected in the
past so what was in museums air Byrom's
and connected these collections though
the system how now has 200 collections
almost 4 million registers online from
315,000 species occurring not only in
San Paulo state the system has been
increased for the whole of Brazil and we
also started to develop a subprogram to
look for molecules of economical
interest one of the main products apart
from publishing and training people
we've traded more than 200 PhD students
in a period of 12 years in 2008 we
decided to work with the state secretary
of environment and also with the some
NGOs to try to produce a map of priority
areas for conservation in the state
so we realize that we were very naive in
the beginning thinking that by having
all the information available in data
banks of open access the manager or the
politicians of the state secretary
employees would use that information to
improve policies it doesn't work like
that you have to work through the
information in a way that they can be
used so we what we did was we went to
the data bank we've clean it all the
data with make absolutely certain of the
notification so using taxonomy trainhard
in each group to ratify the notification
so we ended up with a list of more than
5,000 species of plants we have around
7,000 species in the state and large
numbers of other groups as well crossing
that with the maps where we had the
areas of the remnant of the cerrado of
semi deciduous forest and index of size
and format of that remnant proximity
with other fragments and so the
connectivity the possibility of
connecting with other fragments together
with biodiversity information on lists
of species endemic species endangered
species and also the presence of
invasive species or risk of invasive
species we produced with a set of maps
that not only showed the areas where it
was important to establish new
conservation units and to preserve but
we discovered that in the state of San
Paulo to recover population of large
mammals mainly from the sera de encima
de sitios forest we needed to restore
corridors of forest reconnecting
fragments it was not only conservation
was not enough so we needed a program of
restoration so this map together with a
book that came with all the data behind
a chore the indications started to be
used by the government to improve
policies
on biodiversity conservation either by
establishing new conservation units or
by establishing rules for the
restoration of forests using the number
of species that should be used and so on
so also for planning the expansions of
sugarcane so everything that's rad in
that map is aiready where it's forbidden
to plant fruit sugar cane in the state
so all there is we pointed as important
for conservation and reconnection of our
here in red and also all the area that's
of that the rainforest so now we have
more than 20 legal instruments this goes
from decrease laws and resolutions that
start saying based on the results of the
biota program the governor decides to
establish a new conservation unit of
those so it's been actually used and now
we are moving up I step forward on that
and still this year we are going to make
a joint call for projects with the state
secretary of environment on targets
species target species invasive species
and also better knowledge some areas of
the state that we don't have enough
information for policy decision and the
people that were looking at molecules
that have economical possession also did
a nice job we have now we have more than
five patents and they still tell me they
did that in 2002 to convince us to start
a program that in 10 years we are going
we are not going to need for past miss
mommy anymore you're going to leave on
the profits from the patents we have
they SAT it in 2002 we didn't happen yet
sometimes I have so this history of the
program has been published by science in
an article on the policy forum and all
this together made us strong enough to
propose to fap SB that they should go
renew the support to the biota program
for another ten years starting in
nineteen
9 for 10 years and now we managed to get
it renewed until 2020 so we have another
10 years to run this is the amount of
money that capacity has been investing
on the program and since it's been
renewed that amount of money won't even
app so we are last two years we are over
five million dollars being used in the
program restoration became one of the
main targets for us so in the standing
restoration it's possible to recover for
us the oldest area here of forest has
been planted 10 years before but if we
can do this with plants we don't know
how to do this with animals we don't
know how to keep genetic diversity how
to reintroduce species on the nature and
they are as important early plants
because there are pollinate resist and
disperses and so on so the living forest
will need to animals there as well we
are putting a lot of effort and
producing house material for education
primary high school education something
that it was on the original plan but we
didn't very well in the first ten years
and one of the TV activities that we are
having is a cycle of conferences that's
focused on high school teachers so they
are coming we had one on biodiversity
the concept and values one on the
Pampers this is the extreme south of
Brazil one on the paper now this is the
flooded area and tomorrow we are going
to have one on the cerrado and the next
month is going to be the cut kinga that
the dry areas of the northeast and then
we stopped in july because holiday and
restarts in august with the atlantic
forest the amazon forest and so on so
this is still going on we also are
putting a lot of effort on marine
research that we did lito in the first
ten years and this included
sharing the whole town for work boys the
reason for that is sharing the costs are
with the Climate Change Program of a
pathway to buy a new oksana graphic ship
working with ecosystem functioning
ecosystem services this is how to really
show important biodiversity ease and
also developing a new system the system
the information system we developed in
99 is old and needed to be reviewed
using YouTube's and crossing information
if all the information that's available
from barcoding to climate change models
interoperability with international
initiatives and also national
initiatives and this new system is being
developed under a partnership with
Microsoft Research this was a two-year
project that we developed it will call
it symbiotic 2.0 it was a well together
with Professor John madonnas that's from
the ceiling from attic and also the
computer incident Institute of unicamp
so this been developed and it's being
tested now so using Big Mac a catalog of
life a much more modern interface you
have it it's possible to have it on your
phone from the field if you are
recording things also the question of
typing different names of species or
spelling of species all that you have
dictionaries behind that and the
dictionary will tell you that you are
writing it wrong but you have the option
to keep it if you think so this question
that was raised about new species or
think this if I say are you sure that
you want that yes I'm sure so this is a
different from what's in the data bank
before and it will include
the next registers so this is now being
used now for six months I think all the
books were soft and finally we're going
to put it in full use so that's what I
have to present then if you have any
questions any questions I was just
wondering I was curious about the five
patents what were they about the patents
you guys have the five patents the other
there are three patents on drugs that
are for dengue one is we are looking at
diseases that are not studied by large
labs abroad so so thank you one is on
malaria and one is for control of some
beetles on sugarcane so the different
areas and who owns that is ha pest be or
is shared by fap SP the researcher
himself and the university or institute
where he comes from there's a an
agreement between philosophy and that we
all sign when we get some support from
purpose p
one of the speaker's yesterday mentioned
that they were working in collaboration
with the International Union on
conservation of nature and I was just
curious do you have any connections to
them as well with this particular
project yeah we are connected with with
them so we are using data they they they
use for classifying the species and they
are using our data bank to improve the
classification of species endangered
species distribution of species and so
on we also made a last year and this
year again a joint call with NSF
dimensions of biodiversity we had one
project approved last year now we have
tree on the competition still being
judged and made one call in
collaboration with natural environmental
research council nerc from UK that's
what one project is in the final stage
of being evaluated and partnership with
diversities and now with the new
internet intergovernmental platform on
biodiversity and ecosystem services
that's equivalent of the IPCC that's
been established for biodiversity to use
it our data bank share our data bank any
other questions
is it the data available to ya it's open
its open so if I on the first slide
there was that the address of the best
piata dot org BR so if you go there have
one of the options is to enter the data
bank also have more information about
the program if you want any other
questions Giuliana will have a question
I will in a comment well I'm a big fan
after the iata project and I think it's
an exciting time you know as a scientist
to be able to do research and advance
the state of the art and to influence
legislation and you know impact Society
my question is do you consider
incorporating to the biota any scenarios
that can be enabled for my sensors for
example like maybe to create in terms of
Education it would be very interesting
to have a virtual lab where you can
track animals and plants and humidity
and all of that that would be super all
right that would be super and not only
that the tracking possibility that you
said but also the citizen that Christian
is developing on life and is I mean to
have people participating on identifying
are registering species and so on that
not only for the scientific purpose but
also for the educational purpose that it
will have the outreach that you have
something that we surely would like to
share more information with ok ok ok so
thank you professor
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>