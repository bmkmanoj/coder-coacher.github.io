<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Culture-Aware Approaches to Music Information Research | Coder Coacher - Coaching Coders</title><meta content="Culture-Aware Approaches to Music Information Research - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Culture-Aware Approaches to Music Information Research</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KRLU-F1NoeU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning I'm Gopal i am from music
technology group which is located in
barcelona i'm a third year PhD student
in that group in this talk i would like
to go through the work that is going
doing in the department and also in the
within the project and at the end of the
talk like in the last part of the talk i
will go into a little bit of detail into
my thesis so i don't know how many of
you are familiar with this domain music
information research it's a it's a
domain which deals with information
extraction from music data music data
doesn't essentially mean just the audio
data but also the metadata associated
with it and all the web data which are
social networks the discussion forums
wikipedia information any act any
literary stuff anything so having to
deal with many of these diverse
modalities it's inherently an
interdisciplinary domain so that these
are the most frequently seen domains in
the conferences and of course the
machine learning signal processing
dominate pretty much everything else and
there is a problem with this I mean not
problem not with the machine learning
and signal processing but the way they
are used exclusively without much input
interpretation these are the tasks that
feature in Mirax which is a music
information retrieval evaluation
exchange it's like track which is run
every year with a conference where
people submit their systems for
evaluation and so this pretty much
should give you what kind of information
extraction tasks that are usually seen
in this domain and so as I said there
are some problems with the current state
of this domain which is like in the
first case the information models that
are being used for past 15 years
they have they haven't pretty much
changed they have remained the same and
which is to say that whatever started in
western popular music which is at that
point of time I'm even now driving the
markets the schemas and the information
models everything is modeled after that
music and it stayed the same and now
when people are starting to process
other kinds of musics they are not in a
find it people unfortunately are forcing
those schemas on new musics and some
some used to broader concept to describe
something very specific and even in the
case of methodologies as I said you
because of this using signal processing
to extract some features and then run it
through a machine learning model to
learn some features that will help in
doing some task like genre
classification so but we don't
essentially know what exactly
constitutes a genre for example so we
are not in a position to define or tell
what what features or what properties
the music resulting saying d marking it
as a genre or a particular music or
anything that depends on the task force
on reclassification it's mfc CS and
others timbrel spectral features and
also if you remember the first light it
shows you three for data modality but to
be frank most of the approaches which
use this each of these data modality
they are pretty much independent and
they don't essentially use one data
modality to reinforce the information
from other one so these are some of the
problems with the current state of this
domain for example an example is as
fundamental as this melodic framework
even this concept is has a very big
schema problem so usually the papers of
the literature
which were existing till now they talk
about scales and when when it comes to
melodic frameworks of other musics like
Turkish Macau music or Indian classical
music they are very differently
formulated and they need a different
information schema and a different way
of approaches so scales are basically a
set of nodes and which are fixed
frequency points and well-defined raagas
on the other hand depend on a different
formulation for ax which is a frequency
region owing to the melodic movements
maybe I can describe it later and in
scales ornaments which are designed
those all the Brazos everything is not
inclusive in the scale but in the case
of gragas and even in the case of macomb
music for example they are inclusive
they are not ornaments as such they are
included within the definition of raga
and fication schemes are different so
this is where our project comes into
existence so maybe I can tell up briefly
about my project before it gets on so
when my prof visited IIT Mumbai at three
years three and a half years back he was
presenting all this state-of-the-art
approaches for music information
research for the audience there and one
of the things he noticed is that most of
the concepts used in Western classical
and Western popular music they are not
very I mean the audience here we are not
very well accustomed for example how
many cards and everything else even the
concept of genre is are pretty new here
so it's not very well defined as such so
going back
he applied for this project called calm
music which is to say that these music
are these different musics need
different approaches different
approaches that are based on their own
cultural context and which take into
account all these differences well so
that's where the confusing project
started and right now it this is again
well so this project includes all these
countries mainly India China turkey
Morocco and Spain southern spot southern
part of Spain the so this is the team
which is now 50 strong this is two years
ago and here in India we collaborate
with iit madras for Carnatic music and
IIT Mumbai for Hindustani music and we
have collaboration with musicologists
and musicians to help us validate our
approaches so these are the different
music genres or styles that are
identified across the globe for this
project the key point is that peach of
this culture is still in practice and
they have a music logical literature
thorough music logical literature and it
they are there in practice so maybe you
might have heard him the sign in kinetic
so I did not include any audio samples
there sorry but maybe we can listen to
this sounds to give a sense of what
this music it started from southern part
of Spain Andalusia when and muslims
ruled Spain and then when they were
kicked out it moved to Morocco Algeria
and other parts and it's the
compositions are not a new I mean they
are all old and nobody composes anything
right now but it's a practice it is
practiced in weddings and other ritual
stuff so whatever is there whatever we
can get we got everything so we have
hundred percent of the collection of
this music so the compositions the
number of compositions are 65 I guess
and each span a day and they are divided
into sections and they are some by
different people and whatever we got
they are not recorded by any label it's
recorded by a musicologist and he
privately shared it with us so there is
no recording culture in this music
nobody records this and and this is
beijing opera there are different kinds
of operas in China the reason we
selected this is because we can reach
out to Beijing easily and there are
music there is a musicologist from Spain
who went there and learnt the language
and everything and we can easily
collaborate and but this is quite
representative of the region in general
the former music it's a kind of petrol
phony where they follow a single melody
but each have their own embellishments
and all but in this music you can see
it's a polyphonic version and the lead
single sing something but the
instruments play something else and
there is a strong percussive strokes by
melodic instruments which make things
free and we cater and this is maca music
so in that music as well it's a kind of
hydroponic but there is a different
melody line there are subtle differences
but it is a different melody line to me
do they have quarter notes like the
kitchen I don't know I am Not sure we
just started work on this and sound like
yeah but we just started work on it
nobody is working on things yet we are
still collecting the data I think will
come to know soon so this is a broad
picture of what we are doing in the
project with all these music reporters
we start with data gathering which
involves all this curation of I mean
selective curation of audio from
consulting musicologists and musicians
and putting up everything on musicbrainz
which is a repertory where all the music
metadata is shared publicly and then oh
sorry and then we consult use these
musicians and musicologist to figure out
what in each of these tasks what should
we do in each of these tasks so just
give a brief overview of the data we
have this is some time ago these are not
up to date figures but right now we have
plus 50 something so this will be around
300 series that will be around 280 CDs
and so on so this is a smallest of all
as I said it is a privately recorded
collection and once we have the audio we
extracted all the audio features and
made them publicly available through an
API and if it's for a research purpose I
think the audio is also available and
all the editorial metadata as I said is
on musicbrainz and we also collect and
call all these sources of text data and
here is where we shared all the data
sets and everything
in India so after consulting the
musicologists we figured out some of
these stocks and maybe i will present
some of them not not everything for
indian classical music the most
fundamental task and music information
research is tonic identification the
tonic is something which you hear in the
background of a classical concert in
India it's played by the drone and
usually it has the lower path the middle
saw sorry come up with Aneesa this for
us which are notes analogues to notes
and the lower pi is the lower fifth and
saw the tonic and upper upper power so
these are the nodes that are played by
the drone and this is from the
spectrogram and you can see that these
are these lines the steady lines they
they they represent the drone and there
are three or four different approaches
that are that are in vogue now and it's
pretty much we believe that it is a
pretty much solved problem as long as
there is a background tempura and and
also these are not strictly machine
learning based approaches because we
know the rules why it works and we know
why it doesn't so the way they did it is
that they have a purely data based
approach and there is a rule based
approach and then they correlated
whatever is working and whatever is not
working and they have some understanding
of why this data based approach fails in
some instances and the next a
fundamental problem is international
description so a raga is a collection of
for us and each swara has its own melody
context so let's say there is there are
5 4 s in iraq and the second suarra s2
let's say is nearer to s1 since it's
nearer the movement which is a which is
played or sun doctora it's it's
restricted on the left part of the other
left spectrum of the frequencies and it
starts maybe on this second suarra and
move
more towards the towards the right if
it's restricted on the both sides then
it's pretty much Sun at a constant
frequency so these so the base dog based
on the melody context or the their
positions in the frequency spectrum they
are either shaken too much or sung with
different contexts so basically each
suarra given suarra in different ragas
will have different contexts based on
that we say that it's intoned in a
different manner so that is what
intonation means and we are developing
some methods to capture that information
and this is what the first part of my
PhD will be and I will be talking about
a couple of methods for that the next
goal is to extract the melodic patterns
so in a ragga ragga is strictly not a
scale and strictly not a melody at fixed
melody it's defined to be somewhere in
between so there are some fixed phrases
which give a feel of that raga they
arise because of these restrictions on
this for us so there are some movements
which I have to be sung in a particular
way there is no other way you can sing
them and they are very characteristic to
the to the raga and now so finding them
and so imagine a raga sphere there are
space and you want to save witch saga is
similar to which other raga and one way
to do that is the International
description where you say these were us
in these two raagas have similar
internation so this they are similar and
the other way to do is saying that these
motives are present in both draggers and
hence they are similar
and on that's all melodic analysis those
three things and then we have this
rhythmic analysis the work on this
started last year there is not much yet
AJ who spoke a couple of years here
before here and he is now working on
that I hope this is okay yeah so the
goal is that there are these red lines
and there is there are this green or
blue or gray lines and so this is a
downbeat and these are accented beats in
the taller so this whole thing comes as
Atallah and is it as you can see the the
stroke doesn't necessarily say that
there is no obvious way of from the
strokes along there is no obvious way of
saying it
so there is a stroke here as well so the
goal is to attract Allah which is to
track the downbeats the accents on the
pulse indian music does not have a score
as such it's it has some notation which
is limited to lyrics and thus far as
which are sung at that point but not so
much the the movements and everything
else so this is specifically this code
to audio alignment is specifically being
done for Makka music of Turkey which has
even though it has this many movements
they are not as well defined and
formalized as in Indian music and people
still are okay with the hard-coding the
music and then keeping written a
notation form so that happened mainly
because of political reasons but today
they have scores for most of their
compositions and so here the alignment
the currently we are working on is a
section level and also at note level and
we are using some part of this to partly
align phrases in Carnatic music which i
will talk about in the second part and
another interesting aspect is that is
that these lyrics and music Janey's
music beijing opera because chinese is a
tonal language the same word can mean
many things in when you in turn it in a
different manner so for example here ma
based on how you say it it can mean a
mom or horse so and when people compose
lyrics in this language for their music
it interacts with the the music flow and
it creates an interesting interplay so
one of the hypothesis our hypothesis is
that they try to sing the lyrics in a in
a in a tone that they learn from
language and so it's still being tested
then the next part is are designing
ontologies and learning automatically
learning and populating the ontology
so we have developed some top-level and
some concept specific oncology's for
Indian music to start to start with so
is to be how we already have raga
ontology and top-level ontologies for
karnataka in the sunny by top level I
mean ontologies encapsulating all the
main concepts in that music for example
performers composers raga start as
everything else and in terms of studying
communities we started analyzing some
data from discussion forums and the only
discussion forum till date we found is
of carnatic music which has a very
strong community that maintains this
discussion forum and it is quite active
it's called a receipt cast at all and we
have done some analysis which which gave
us some insights into who is a guru of
whom and who sings what and maybe like
the associations between the kind of
instruments in the concerts and so yeah
these are these are the main n Sun
symbol of a carnatic music and the
associations between a composer and the
tallest composer and Ariah so what raga
he composes frequently in these kinds of
details and at the end our goal is to
integrate everything into a system that
allows for exploration and navigation of
this music repertoire and we already
have a prototype of that and maybe at
the end I will give a demonstration of
this prototype it's called duniya and
it's the idea is that we have all the
concepts here and if you can filter out
this concept by selecting multiple
versions of them and querying them and
you will be shown different kinds of
results that are that match this very
and once you go to the recording page
that's where our all our work comes into
play so you can navigate the recordings
navigate the album's artists by the
analysis which we do so as I said
similar ragas by international
description or similar raagas by motivic
analysis
so on and so forth so that's the first
part of the talk it that's the broad
view of the work the kind of work being
done in in the project and in the second
part I will talk about some details in
my thesis which is to develop similarity
measures from all this information and i
will be mainly working on these two
information extraction tasks one is
international analysis and the other one
is ontology learning so one comes with
this this part and the other comes with
this part and finally when I when I have
some similarity measures built on top of
this multimodal knowledge base then we
integrate that to the system so maybe to
quickly understand what this thesis is
about I gave an example of the result
the kind of result i want to see at the
end so till now the system say okay
something is similar to something else
but in this thesis we want to say it is
similar by a quantitate a quantity and a
similar because of some reason so we
want to juxtapose the modalities so this
comes from the audio description and
this comes from the from the information
extraction part and we want to establish
these correlations and associations
between these two modalities and
similarly between other entities DISA
raagas and these are artists this is an
overview of the thesis these are the the
data sources mainly the unstructured
text which comes from the web Wikipedia
and other sources metadata which we have
curated on musicbrainz and audio
notation files and this part is when
where I deal with information extraction
mainly ontology learning I use the idea
is to use this new information
extraction paradigm which is open ie
that intends to scale to the web and
from here I will have an ontology and a
knowledge base and from audio analysis
this is the only part that is part of
this is this which are indicated as
predefined processes those are not part
of my thesis but what but i will be
using them and here i will have some
descriptions for some relations between
artists and raagas in a quantitative
manner and at the end we will merge them
to multimodal knowledgebase and compute
some relatedness measures the similarity
measures so the fundamental hypothesis
of this species and the project as well
is that music cultures differ and there
is a lot of discourse that music is a
universal language and so these concepts
can be generalized and to what extent
can we generalize and so on so we try to
validate our hypotheses saying that we
need different kinds of approaches
because they have different concepts and
the formulation is different and the
structuring of the music and the valley
they listened to everything is different
so by musical characteristics we mean
these things I mean in anything that
comes under the lingo of music and the
salience by aliens we mean how important
is that particular characteristics a
raga how important it is for jazz music
how important it is for indian music and
an example is if you take dance it's it
forms a very core element of flamenco
and it coexists with it and defines
flamenco whereas in Carnatic and
hindustani they help other dance forms
but dance is not an integral part of
them and with baroque and jazz they are
not associated at all so this is the
process for validating our hypothesis we
started with wiki pages and the I'll
explain the reason why wiki pages so we
stripped all the structured content from
the wiki pages and kept only the text
part and then we linked the entities
that the title of the page is the given
entity and the links will be the
references in the page and then we did a
page rank and got the most relevant I
mean the most important entities in that
music
then what we did is to find the
characteristics of each of that entity
so for example here there is a page for
TM Krishna he's an artist and he will be
associated with some of the Wikipedia
categories he's a performer he is a
Tamilian he is born in some village and
so on and so forth these are all the
characteristics of that entity TM r ich
nur and we use TM Krishna's rank here to
compute the relevance of the categories
he is associated with so that's done in
this way so let's say pages or entities
is E and P of e is page rank and c is
the characteristic of the category and
what we did is to take inverse document
frequency of the sea so that if TM
krishna is both composer and a performer
and there are too many performers then
it's likely that he is identified as a
performer it now in a broader way so but
this this resulted in some erroneous
results which like so theta c
rajagopalachari is a he's associated
with a carnatic music but he is a
politician and he is the only person in
the entire graph who is a politician so
but he has some medium relevance in the
whole graph so all the political
categories stand up so we have we have
we need to do a pruning by filtering out
saying these categories which occur
certain minimum number of times so this
gave us a score for each category yes so
I think there are around 20,000 pages
for jars and it comes down to 300 pages
for flamenco so there is a lot of
variation but we use the entire
wikipedia for each of the music cultures
these are the results maybe I can read
them out so in chaotic music the most
salient ones are that its association
with the religion and the terminology
which is very important and the
terminology doesn't occur in baroque or
jars for example on flamenco it occurs
only in here and here in Hindustani and
there are raagas
and yeah in hindustani you can see this
gharanas here the Bihar Uttar Pradesh
and in jars for example there is this
Buddhism and the record labels and
baroque the the saints which which
contributed to the music and the German
area and so on so forth so we have some
good result and on top of this we have
to validate if this is really helping in
anything we build mantic distance aliens
of our semantics distance so what it
does is if a two artists let's say two
entities to musical entities a feature
in one of the top categories that are
identified for one of the salient
categories in that music they are
recommended as a closer than another two
artists who feature in together in a low
low set off in a low salience category
so this in fact we did a pilot survey on
this using this and comparing comparing
it with a link database distance and we
found out that this in fact has a very
different complementary set of
information that that that is from the
structured information which is already
present on the web so yeah so this
pretty much supports our hypothesis and
also helps us to bring build some
distance measures on top of this
knowledge to enhance or get some
different kind of information which is
which is not there in explicitly in the
current structure data so the first task
after validating the hypothesis is a
description of intonation obtaining it
automatically from the recording so till
now the the this intonation is studied
under and under the name of tuning and
the tuning usually a constraint is
constrained to studying the location of
the nodes but here we don't want to see
the location it's a region so we don't
we have
broaden the approach and include this
particular definition in our analysis to
to just okay so this is the this is the
usual setup of carnatic music I i
started working on carnatic music for
international description and violinist
usually follows very closely the
vocalist and there is a very small lag
between the move of the order of
milliseconds so usually the mono
monophonic pitch track which tracking
algorithms they go have when switching
between them and to give just a sense of
what it sounds like so she never stays
on a particular place I mean she keeps
moving all the time so this is what I am
talking about so when you say yeah
intonation of maybe this particular
suarra it it's not just this this part
but the entire part so so the first
approach well well by now I think you
know the intonation so the first
approach we follow these this an
aggregate approach just to check if if
we are able to capture this
international information somehow and
that's which histogram parameterization
by that I mean so these are two
different ragas and so they look that
the peak locations are very similar if
you also and they look also very similar
the only thing that can differentiate
these two is the shape of these Peaks so
that's what we did we we restricted the
bounds by Valley points or minimum
threshold of a maximum threshold of
fifty cents and computed these different
shape parameters apart from position and
amplitude this is a state-of-the-art in
raga recognition until then it was three
years back and we included these and so
this is a whole process so we started
with
the audio and then took out all the
vocal segments and then extracted the
pitch for them and using the tonic we
normalize the histogram and got the
histogram pick the peaks and
characterized the shape parameters so as
I said there is a slight lag between
violin and voice and this green contour
here it shows the pitch extracted using
in algorithm and this is the blue track
is using a predominant melody extraction
proposed by Solomon and Gomes so you can
see that this is a while in part and
this is the vocal part it keeps
switching between them the violin is
usually tuned an octave higher and once
we have the histogram you have been
normalized it using the tonic you can
see that this is this is a not very
smooth I mean there are many erroneous
Peaks that are that can be extracted
from it and for that what we did is to
introduce two parameters which is LP and
DP they are depth and look at parameters
we pick a peak if there is a depth of DP
on either side of the peak in points at
least LP points ahead so well this this
constrained the number of peaks to the
mostly the valid Peaks which we wanted
but still there are a lot of erroneous
Peaks so what we did is we be computed
an average histogram for the raga for
taking all the recordings of the raga so
this is the average histogram of the
raga and to just juxtapose that with the
individual histogram this is the one and
you can see that there are many
erroneous Peaks here which can be
discarded comparing it to the average
histogram so that is what we did our
goal is not raga recognition yet all we
wanted to check is that these new
parameters somehow capture extra
information which is not in not there in
position and amplitude so the way we
checked it is
ragga recognition task there are two
other tasks which I did not include here
and in that this is three years ago at
that time we did not have a bigger data
set so I tried it on three ragas 42
recordings and I noticed that there is a
good improvement I mean it's not
statistically significant because these
numbers are too low but there is a a
study improvement using any classifier I
don't exactly remember it I can refer to
that but it's longer yes yes yes yes I
think I think since we are using we are
comparing against position and amplitude
I think I chose both like there are two
rockets which have which are alight and
there is another raga which is not
allowed to these two raagas and so the
next next approach builds on top of this
and the kind of pause like the problems
which it addresses is the first one is
this studies for avi check with
transition so these these both these
conditions they can be sung with the
same for us but in different ragas so
this can be same as this when they
pronounce it but so as we see this is
completely different and we need to have
a method that that can capture this
information with it whether it's what I
study or whether it is moving up down
and another problem is that we are
rejecting the the peak to a stick bound
here if I draw a line and say this is
the end of the peak that's not true
exactly so this peak the pitches of this
water this pan all the way here and when
they sing this water the it goes all the
way here so there should be some way to
segregate these peaks in a in a
intelligent manner so what we did is
this is the contour we started looking
at a particular segment and we have a
window starting
at one end of the segment and we move
the window till the till the other end
and we computed the means of the window
so it gave us whether these means are
raising or falling or there they are
pretty much steady so we have an
information of whether this particular
suarra this particular segment and this
flora which it which it is sunk in where
exactly it belongs in the hole which
histogram so we finally have yeah this
this kind of output where we say that
the pitches here they go to this were
the pitches here their son for this were
so on and so forth so we have some way
of saying which pitches in the melodic
contour are some for which suarra and
these we tried on more recordings this
the result here the task here is of on
six riders with 69 recordings and it is
compared against the previous approach
and the difference is pretty big and we
also validated this result with the
musicians and there are many
discrepancies and they said that okay
your algorithm captures some information
of this for us but what the cactus is
wrong but it scattering in a very
consistent way across the recordings so
what we had to do is to have a ground
truth to test this thing yes we're also
marking sweaters yes and you will be a
peak picking so initially we know the
kind of far as it seemed and so in the
first approach we are peak picking so we
know this for us in the second approach
we we just do so we just have these
lines where the pitches belong to yeah
check this one with experts like whether
they will call this as well no no this
they won't yes yes they will know it
just fell estas right right yeah they
were not they will not so what we did is
to have a ground so we started
collecting some Barnum's which in
Carnatic music are pretty pretty much
sum to the notation and we say me
automatically aligned it to the
recordings which we have recorded
privately from the singles and yeah the
possible improvements in this step that
we have more accurate which class
assignment and more accurate estimate of
the melodic context so these are the
kinds of plots which we got after this
this task so these are they agree with
this so this is the same suarra in for
raagas so the kind of pitches they sing
in these different ragas is same so this
is what I i was mentioning in the
beginning if the soiree is too close to
the neighboring for us it is pretty much
Sun on the frequency and if if it has
some so this is thus far that is being
sung and it is being extended all the
way here so all the way up to 400 semi
tones so that is the that's the movement
and here they are singing this water and
they have both thus for us on both ends
and yeah so
sorry so we have TM Krishna and his
student of Ignatius were and in
Hindustani we have soo vanilla thorough
ma'am in ncpa and a student of rj
chakravarthy costume ganguly this is
1200 sense is one octave is it yes so
hundred will be like one note yes some
weigh 700 enjoyed fighting off the scale
right right right but still they calling
the signal right so yeah the movement on
that knot spans it it's not that they
call this note as that but the movement
on this not expands killed RBI yeah
right right right right so yeah that's
where we are on international
description so we are now trying to
automatically align phrase with a
notation and then somehow get the
sweater boundaries so once we get that
then I think it's a pretty
straightforward theme from here to
relate artists and ragas so a hypothesis
of our say the second hypothesis in
international description is that it's
also characteristic of artists and the
school's they're honest so yeah we did
not get test that but once we are done
with this I think that that would be
more interesting and so this work
quantifies the relations between artists
and Dragons and now we want to say why
there is a particular relation and why
is there a particular number on that
relation using some description human
readable description so these are the
broad tasks in ontology learning that we
are working on entity identification so
entities are all the instances of in the
in the music all the artists by
individuals all the raga individuals in
everything the individual elements of
all the concepts and the concepts are
these different classes of the
categories and semantic relations are
the artist sings songs so sings is the
semantic relation so these are the kinds
of information which we want from this
unstructured text so to validate
whatever we do here we need some
ontologies some manual engineered
oncologist with the help of musicians
and their reviews of whatever we
generate and also we are comparing
against sorry we are comparing against
all the already structured information
that is available on the web or the
dbpedia and other communities generated
linked data so these are the this is a
top level ontology for Carnatic music
and this is currently being validated by
x RM and in romana than is a
musicologist an IM krishna so this is
not still agreed upon so i might be
wrong so at the wavier we are
approaching it is that we want these
ontologies for navigation and not to not
to be as a domain ontology we may not be
the experts but so our fundamental unit
in the navigation is a concert so we
model everything after that so concert
has a performer it has a venue it is
performed in this concert and so on and
so forth
yes so when you relate gragas for
example let's say I relate in Korea and
why reveal and so there is some
similarity but this doesn't explain the
international description doesn't give
me a description like why they are they
similar it can I at best say that this
number of for us have similar context in
both the raagas but it won't tell me
beyond that so they are allied ragas
that's what that is why they are similar
so so I extract that that knowledge from
the text so at the end when I recommend
by our way for mockery I would also be
able to say that there this much similar
but they are also similar because of
this reason
you can expect in one single yes yes can
you all say yes yay and this is a
concept level ontology by concept I can
go down to rocks para or stop at praga
so an example is as far aware it has
servants for us and each for eyes as
again its variants and they have their
properties and they have property shared
with different well this is not exactly
from the metadata but the metadata is
modeled after the yes so yeah I'm we
have I mean for each of the concepts in
the top-level ontology there exists
already an ontology which further
describes that concept or we are
developing the ontology and our approach
to extract information is basically to
use existing open information extraction
paradigm so it differs from the
traditional information extraction in
the sense that it scales to the web and
it does not use the vocabulary of the
domain it's unsupervised that's what we
want because we are dealing with
different music cultures and they have
different terminology and we can't adapt
every the system for every model
manually so we we picked up open
information extraction as the as the
paradigm and the main challenges here
are examples of open information
extraction include this never-ending
learning from CMU and know-it-all from
Washington University so the main
problem with them is that they're recoil
is very low very very low like I don't
know ten percent or something and the
position is pretty high they depend on
redundancy in the web but in this
context we can't afford to do that since
there is not much data available for
each of these music cultures so
we are trying to improve the recall and
also identify the domain-specific and
culture specific information that we
need to that that is being missed out so
one example of this domain specific
information is this our 100 hour Ahana
so these are usually lists or they have
different grammar of presenting them so
this is not used this is usually
considered as a spam in in a sentence
and they're usually excluded from the
information extraction systems but we
want to change modify the grammars of
existing systems and see if we can get
them alpha and yeah these are the
systems which we have which we started
benchmarking and we already have a
system to benchmark them we use two
different kinds of benchmarks one is
based on the volume of extractions they
give us given given some data and also
the quality of the sessions or
extractions so three systems are the
first one it it's purely based on
dependency parsing and the second one on
shallow semantic parsing of cementec
semantic role labeling and the third one
it's a it's not exactly an open
information extraction system but it's a
diplomatic parsing system which we are
modified to give us some triples which
are which account two hundred percent
representation logical representation
and then we use those triples as we
assume those triples to be assertions so
we compared all these three systems on
volume and quantity and there are too
many figures so I just present some of
them here a couple of them these are
qualitative results for relation
extraction on conniving and hindustani
data so as you can see this semantic
parsing based system it has a lot of
redundancy in its assertions because of
that it has I mean higher number of
relations types that are that it's very
confident that they are there of the
domain and yeah and open ie it comes
close second and the reverb
it's the performance of reverb is low on
most of the aspects then so we chose to
adapt this deep romantic passing based
system I am NOT an expert in NLP doing
so I am collaborating with a couple of
researchers in edinboro and so we cut we
will change this the grammar behind the
system and adapt that particular grammar
to whatever domains which we are working
with and the main things that while
benchmarking the systems the challenges
we notice the reification of sentences
like TM Krishna said that an Ramanujan
ramanathan is a good musicologist so so
there is the there is there are two
sentences in that sir so this kind of
reification is a complex thing and often
these systems miss out on that and so we
can't afford to take that data let go
because it results in contradictions
when we finally put that in ontology and
they are often these sentences in
Wikipedia and other discussion forums
which are not strictly scholarly very
long they span a paragraph and too
complex to get information from so we
are trying to see if there is something
which we can do to segment those
sentences and as I said there is some
domain specific information which we
want to get great across and this is so
we still need to do a lot the first task
which we are done with is or
international description which we are
almost at the end of it and the second
task which is the label the relationship
established using international
description motivic analysis and every
other audio description task is still
under protest so we are we are just done
with benchmarking and now we are moving
to actually working on this open
information extraction
systems so that expertise okay thank you
booty any call you
i'm so gay
I'm trying to connect it with the music
information retrieval and what
information detection think you are
doing does it help there or yeah
basically it's the same field we it's
getting a rebranded as informational
research because we are including more
data I mean more different modalities
modalities of data so in four
internation the the previous work
related work is mostly on tuning so
detecting or seeing what what what scare
or what frequencies the intervals are
tuned to a particular in a particular
recording and there is a lot of work
being done on folk music traditions
which doesn't have documentation and
they are trying to see what kind of
intervals there are in those kinds of
those kinds of music so most of the work
that existed prior to our work in this
domain they are limited to say
identifying the frequencies in the song
and but they did not they did not talk
about how these frequencies are sung and
their context nothing else i think i can
safely claim that this is the first work
that that talks about the context of the
interval so what would be yes in idea
where a user is creating something we're
using both the information both
information as well as NLP kind of
segmented it and on to you no answer the
query better right yes maybe I can start
and I can i connect to the internet
maybe I can show the demons the
prototype we have that really answer
this question okay so it is really okay
so there is a this prototype and the
idea is that we we model some some
interaction interaction user
interactions on this interface that will
allow this so let us assume that you are
listening to a recording
and you would like to say you you have
seen some phrase which which reminded
you of some other song and maybe you can
click there and then the system directs
you to different kinds of phrases or
different relations based on not just
the audio description the phase is
identified and shown to you by audio
description but the relations that that
we show further they can be shown using
all the all the analysis not just the
audio description so yes yes so well
this interface is to test our all our
analysis this is a research prototype
this is not a user product as such but
if we we did not yet think about how to
take these two uses right right
music synthesis is right now not part of
the project but there are some people
working on music synthesis that attended
our workshops in chennai last year and
yep the main problem is this gamma cars
and how do they synthesize mostly they
are working on which bending and
questions like you be similar to think
as this phrase is yes yes some play this
artistic yeah for that you can or can
you also see it I've given this to music
if i mix them all those that that's not
how goal but yeah it's definitely
possible I mean music synthesis in
Indian music is still a very open
problem the question so in linguistics
we have various kinds of databases like
one example is worth linguist language
atlas when you really know all the
languages or most of the languages of
the world if they're all properties you
are trying to do something similar
across musical cultures is there any
such resource like there is no standard
resource on Martin this that would be a
very cool thing to do in early and about
exposing seems very alike to that kind
of the classifying different systems of
music using different kinds of features
and then you know getting a map of the
world like music here is these features
music there but it's not organized
effort but there are some ad hoc
conversations on mailing list saying I
mean comparing against one music against
the other
description of all these different right
right right and then you can actually
very easily yes so I initiated something
like this on there is a music ontology
so it's called the music ontology
research thesis of e Freeman and he he
came up with an ontology for general
ontology that can describe workflow
processes of music how its produced how
it's how it's on media and everything so
when when I approached saying that I can
modify this ontology for shooting the
Indian kind of music then that's we are
like so these ontologies they don't
exactly put everything in a single place
but when I model an ontology let's say I
say sabha in Carnatic music it's a
top-level concept but it will also refer
to some place similar to that in music
ontology the music ontology so in a in a
very decentralized way these ontologies
are doing that to an extent so they are
somehow relating the the concepts in
both the music cultures so it's not
exactly the same as relation in an
ontology but it's like a related concept
kind of thing so there are some links
between ontologies which are very
decentralized but then I mean this kind
of effort is known of
similarities with some distinct shoes ok
ok so for instance suarra that no feast
fit for node is a very distinctive
feature of same account using an attic
and induce them but not found in most of
the other cultures right right so that's
why the feature which is turned on for
these and okay okay okay and similarly
you can define some say 100 or maybe
hows it features okay and that can
describe all the world music listen
tally for languages okay okay very
ambitious thing it takes long time it
will be very good I was thinking on some
of their lines I don't know if you are
trying to address the popular music
he'll be this to me or not because
mostly everything was
but if you bring in the popular music
for ya Molly would all be the only ones
worth something opening ones then you
have their music and then there is a
phenomenon of music plays racism it
strikes to Train like freedom hey Coco
we have some seed lexicon which came
here is a shared asking permission I
think there are works doing back there
is no ship does they believe what that
years of addiction in music I haven't
seen music plays okay yeah I don't know
if it's called by the same name but this
task here is that there is this so my
one of my colleagues he has done a cover
song detection so they take a song and
they build some version of it so
ultimately this this work it combines
all the cover songs and identifies the
original version and the associations
which is inspired by water and so on
what would be slightly tricky they are
very careful all over this done okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>