<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Demis Hassabis | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Demis Hassabis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Demis Hassabis</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zvBoOF01MY4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let me start have the pleasure of
introducing very briefly demis hassabis
and this is continuing a tradition that
I started with Christophe is much better
known that I am and he was a postdoc
with me Davis and and of course is ideal
for today because it's a mixture of a
neuroscientist and a technologist and an
entrepreneur talk a little bit about
because of the symposiums about
neuroscience and brains and minds as
well as machines I'm going to focus sort
of most of my talk around systems
neuroscience and how it can be maybe
helpful towards in helping us progress
in our quest for building AI so I'm the
CEO of deepmind the mine was founded in
2010 and then we joined forces with
Google in 2014 to sort of accelerate
progress towards our mission of solving
intelligence one way you can think about
deep mind is a kind of like an Apollo
program effort for AI so we have over
150 research scientists now so I think
it's the largest collection anywhere of
sort of machine learning researchers on
this single topic but also one of the
things we try to do with the mind is
come up with a new approach to
organizing and doing science so we've
tried to combine and take the best from
the best academic Institute's and
combine that with what's best from you
know that the greater sort of Silicon
Valley style startups and we tried to
kind of create a hybrid environment that
maximizes and it's been sort of
optimized for research progress to try
and make it as efficient and productive
and collaborative as possible
so what we do at deep mind is we're sort
of interested in algorithms we call them
general-purpose learning algorithms and
we're only really interests in
algorithms that can learn automatically
from wall inputs or directly from walk
experience so not pre-programmed or
handcrafted in any way and we're also
interested in this notion of generality
so the idea that the setter seems to a
single system or single set of
algorithms can operate out of the box
across a wide range of tasks and in fact
this relates to our operational
definition if you like of intelligence
that we use a deep mind which is we
define it as the ability to perform well
across a wide range of environments so
while I agree with Tommy and I think I'm
some of the other speakers today about
the intelligences quite an amorphous
term we found that operationalizing it
like this and putting generality at the
center of it works very well for us and
you could argue well what about all
these different properties that that
doesn't cover you know creativity cetera
but actually then it just depends on the
tasks that you include in the set of
tasks that you're going to try and
attempt so the wider and the more
diverse the better so we call this type
of AI artificial general intelligence or
AGI and the hallmark of this kind of
these kinds of algorithms is that
they're flexible and adaptive and
perhaps even one day inventive and
they're built to deal with the
unexpected from the beginning now
contrast that with a lot of what's
called AI out there which we mostly term
narrow AI which is often handcraft
doodle special cased for specific
problem and therefore can be very
brittle if something unexpected happens
it will just catastrophic if you fail so
one of the other sort of philosophies I
guess that we commit to at the start of
deep mine was the idea of grounded
cognition so this is the idea that all
the notion that a true thinking machine
has to be grounded in a sensory motor
data stream now this kind of gives rise
to notions of end-to-end learning agents
you know the perhaps start with raw
vision so it may be pixels on a screen
and go all the way to making it
about what action to take and where
should in that entire stack of things
that's involved all the way from
perception to decision-making now
usually when people talk about grounded
cognition on body cognition people start
working on or thinking about robots and
of course that's the ultimate grounding
robots in the real world but actually we
decided to start off with simulations
and specifically games and we think that
games are a pretty perfect platform for
developing and testing AI algorithms
especially these kinds of grounded out
guerrillas so firstly obviously you can
create unlimited training data by
running the game as much as you like
there's no testing bias because the
games were designed to be challenging to
for humans obviously they were designed
by game designers for the purpose of
entertainment not for testing a is so
often I think one of the things that's
held back the AI field is that usually
is the algorithmic developers who the
algorithm developers who also develop
the tests and that can lead to a kind of
unconscious bias about what the tests
really test for of course compared to
robots you can parallel test these
algorithms you know run millions of
agents at once all playing the games and
it's also quite convenient in most games
that have scores so it's very convenient
for us to measure incremental progress
if you make some small change to an
algorithm you can quickly see and
quantify the difference it's made so we
really love games and of course we also
like robots and we're very interesting
robots as a as a sort of application of
AI but not you know in terms of
development we mostly focus on
simulation so I guess our most sort of
widely known work to date is I work on
deep reinforcement learning where I
think we showed for the first time quite
an impressive use of the reinforcement
learning when we used it on the classic
Atari games from the 80s Atari 2600
games I'm just going to run a quick
video of some of the of the agent
playing these games but just to be clear
for those you haven't seen this sort of
this video before you know what the
agents getting here
are just the raw pixels as inputs around
30,000 pixels per frame and the goal
here is to maximize the score everything
else is learnt from scratch and here
it's a single system we insist on a
single system that plays all the
different games without any tweaking of
the hyper parameters so we called the
system DQ n after DQ d / q network and
here's the sort of little medley of the
same system playing of a mere half a
dozen games and you can see for those of
you don't know about Atari games how
different in style they are visually
also in terms of their objectives
incredibly diverse and and the same
system could you know effectively master
all of these games now when we sort of
published our nature paper on this we
were better than human at just over half
the games and now with superhuman level
at all the five of them so if you if
you're interested in so here's a book
the boxing example where you know here
that on the list it's the red boxer on
the left hand side playing the inbuilt
AI and so if you're interested in the
details of this you know it's all them
in the in the nature article including
our code which is available online so
you couldn't you can play with DQ and
yourself so given we're here talking
about brains and minds as well as
machines what about the brain and in
fact DQ n and why one of the reasons it
worked so well was partially inspired by
neuroscience and specifically
hippocampal replay the idea of replaying
your experiences so that you can
maximize the use of that information and
now it's critical the use of replay was
critical in making dqn tractable in a
reasonable amount of time when it was
training on these games so I'm gonna
spend the rest of the talk now just
discussing my own view on you know how
systems neuroscience should be
integrated with AI so first thing to
talk about is you know often I get asked
well why bother with the brain or
neuroscience at all you know there are
probably many other ways of building
intelligence and that might well be true
but I think there's a couple of
arguments to take neuroscience seriously
first of all I think it's likely that
the
of possible solutions is actually very
small compared to the size of the search
space so if that's actually true then
it's probably worth honing in on and
trying to reverse engineer at least to
some extent the solution that we know
exists and after all the brain is the
only existence proof we have that
general intelligence is possible at all
now that might all be very well but if
it would be no point talking about it if
we didn't have amazing now new
techniques and data streams to actually
analyze and to give us information about
what's going on inside the brain and
pretty much yearly now amazing new
techniques get developed so away from
opto genetics connectomics two-photon
microscopy you know that the list them
goes on and on now and you know there's
a huge proliferation now of amazing
techniques to get ever closer to what's
going on in the brain and this is all
resulting in a kind of exponential
increase in our understanding of the
brain of course we've got a long way to
go you know I'm not saying we have a
good thought or even close to full
understanding of what's happening with
the brain but there are many clues now
many interesting nuggets of information
that if used in the right way I think
can be helpful for AI development so I
see there's sort of two you know kind of
two buckets if you like of you know the
purpose of neuroscience in terms of how
it can help a eye development so firstly
maybe more obviously it can provide
direction so research Direction
inspiration for new types of algorithms
architectures and even analysis
techniques for analyzing these machine
learning systems and I think we should
be looking to neuroscience where we have
the least idea or the most uncertainty
if you like about what to do in machine
learning to solve a particular problem
or to have a particular capability and
also I think something that some we're
pushing very hard on at deepmind
is building and taking a inspiration
from the analysis and visualization
tools that are now quite mature in
neuroscience say for analyzing fMRI
images and then applying that in some
analogous way to analyzing machine
learning systems and another interesting
thing is actually the experimental
techniques in biz
techniques that are kind of standard in
things like fMRI which in terms of
controlling for what you're looking for
in an experiment and that's something
again that in machine learning you know
hasn't come across yet and I think could
be very useful this sort of idea of
designing these kinds of experiments
that we do in neuroscience and the
second way I think neuroscience can can
end up being useful is why I call
validation testing so if you have some
idea of our notion of you know your
favorite type of algorithm maybe it's
reinforcement learning you know and
you're sort of arguing with another
machine learner that actually this
should constitute or could constitute a
viable component of an overall a GI
system you know how much ever had you
decide if this is you know if this is a
reasonable conjecture and you know so
maybe you go away and you stop trying to
build a system and probably it doesn't
work straightaway so then you've got to
decide like how much more effort should
we put into that you know is it just a
question of another few years and then
something viable will happen and
something interesting will happen so
it's these very difficult decisions to
be made especially if you're running a
large group or you have a large team of
where should you put your effort and I
think if you can point to a system in
the brain that that analogously does
that sort of mimics that algorithm or or
indeed the algorithm mimics that part of
the brain then I think that can give us
confidence that we should put more
effort into that area of research and
that ultimately it will yield some fruit
and in fact that's what we thought about
with reinforcement learning and why we
committed to that so heavily because the
brain indeed and most biological systems
use forms of reinforcement learning like
TV learning in order to learn about
their environment but it's you know the
next question then is you know if you're
going to take neuroscience seriously
there's so much neuroscience so what
parts of neuroscience should we be
paying attention to and so here I like
quoting sort of David Mars three-level
analysis or it should be called Tommy's
Tommy's a three level analysis as well
so I think Tommy was involved in heavily
in this and David Marr used to say in
the 70s probably one of the founding
fathers of computational neuroscience
that to fully understand a complex
biological system you need to understand
on three levels the computational level
which is what the the what if you like
the goals of the system and algorithmic
level so the how so the representations
and algorithms the system users and the
implementation level you know ie the
sort of physical substrate that realizes
the system so those are the three levels
you need if you want to fully understand
say the brain but actually I think for
machine learning and AI development
really the top two levels are the most
important so at the mind we focus on the
kind of computation on algorithmic
levels when we come to analyze
neuroscience in the brain so
collectively I refer to that as systems
neuroscience and really what we should
in then Ben is the algorithms and the
representations and the architectures
that the brain uses and we're working on
all sorts of sort of cutting-edge areas
where we're using systems neuroscience
ideas as well as the course machine
learning ideas to try and make progress
on so here's just a sort of a kind of
summary of some of the areas that we're
looking on at the moment and currently
focusing on representations memory
attention concepts planning navigation
imagination and in fact in terms of
parts of the brain Christoph was talking
earlier about you know prefrontal cortex
and and and high-level cortex but
actually one of the old parts of the
brain the hippocampus which is this bit
of the brain in pink here in the middle
is it turns out to be quite critical for
a lot of these capabilities so let's
talk about some of those in in order
let's still talk about memory first so
we've been experimenting a lot with
memory and adding memory to neural
networks so one way you can think about
the work we're doing is you take a
classical computer we implement a
sophisticated recurrent neural network
perhaps with some LS TMS and then we
give it a huge memory store that it can
learn to control and then that whole
system is differentiable end to end and
then what we end up with is what we're
dubbing this neural chewing which has a
called a neural chewing machine in the
sense that it's it has all the
components now that a Turing machine
would need but it
new rule in the sense that it's learnt
by from input and output examples so
here's a sort of cartoon diagram of the
of the architecture so you've got the in
the center there and the CPU the
controller is the recurrent neural
network and then there's the input and
output tape you can think of and it
learns by example input-output examples
and it learns to control this a very
large memory store on the right and it
learns to read and write from that now I
haven't got time to go into this in
detail today and in fact I think Alex
graves is giving a talk on this right at
the moment but also Greg Wayne is giving
a talk about this five o'clock tomorrow
in another one of the workshops so he'll
be going into much more detail about
this work if you're interested I'm just
going to give it to show you a couple of
videos of sort of some of the latest
things we've been able to do with your
two machines over the last year so we
looked at this one of these classical
problems from old-fashioned AI I guess
back from the 80s shrewd aloo which is a
blocked world problem and the idea is
that you're trying to manipulate these
blocks you know put the green block on
the red block or put the blue pyramid
inside the white box and you can also
answer questions about the scene as well
now we're not ready to tackle for shrewd
Allu yet but we created a kind of mini
version of shrewdly liu if you like so a
mini blocks world where now we're
looking sidon onto this block world and
the sorts of tasks we created was
imagine a configuration of blocks here
on the left you have this starting
configuration and you've got a goal
configuration on the right hand side and
the system has to convert the start
configuration into the goal
configuration by moving one block at a
time so the only moves that are allowed
are moving the top block to the block to
any of the columns adjacent columns so
it's a little bit like a complex Tower
of Hanoi problem and it's actually quite
difficult for humans to do in some sort
of you know reasonably optimal way so
I'm just going to run this video and
show you how the Nutri machine does so
in the middle here you see it trying to
change the star configuration into the
goal configuration and no obviously it's
never seen this type of configuration
before so obviously the stock if
aggression the goal configuration a
totally new it's
it's learned this from example seeing
other examples of solutions from other
positions you can see there it's it
solved the problem and then we have
actually a language a sort of mini
language version of this where we
describe now the constraints in terms of
little language instructions rather than
an example n board and so we if you see
down at the bottom here hopefully you
can see that the little constraints so
like block three needs to be down from
block five block four needs to be up
from block to block one needs to be up
from block four and so on and so we give
a bunch of these constraints and then
from the start position the you're a
machine has to reach an end position
that satisfies all those constraints so
I'll show you that again here so it
reads in the constraints
letter-by-letter
and then when it sees the question mark
it knows that it's supposed to execute a
solution so we can see there at the end
there that that final ball configuration
actually satisfies all those constraints
three down from five four up from two
one up from four and six down from three
and it does that in in a in a in an
optimal way
so that's certain memory so what about
concepts so we would like to have
abstract concepts because it's going to
be key for so many things including
transfer learning which you know I think
is one of the keys to flexible general
intelligence so we defined transfer
learning is the ability to apply
previously learned knowledge
appropriately to a new situation to help
your performance in that new situation
and that breaks down into at least three
sort of sub steps one is you got to
identify the salient features in your
current environment and more importantly
ignore the irrelevant features then you
need to re represent those features as
an abstract concept so sort of divorced
from the perceptual features that you
learned them in and then once you have
these library of concepts you need to be
able to select and appropriately apply
those to any new situation that you
encounter so probably step two of this
is the probably that I mean all these
things are very challenging but
posterity is probably the the most
challenging and the most critical
and really I think from the beginning of
deepmind we've identified learning
abstract concepts as one of the key
breakthroughs that are needed towards
getting us to AGI and you can see this
diagram you know this is sort of missing
gap on the left hand side you know you
can think of information in the brain
crudely split into three levels like
perceptual information this abstract
conceptual information and finally
symbolic information like words and
there's been a lot of work obviously
amazing progress the last decade on with
things like deep learning in terms of
dealing with perceptual information and
obviously there was a lot of work in the
80s on logic networks but there's this
missing piece in the middle which would
allow the the symbols to be grounded and
also form to go from perception
information up to truly abstract
information now I think as Gabriel
showed in Gable and you know when I was
doing my PhD in neuroscience in London I
was this was one of the key results that
I noticed that were quite amazing to me
in terms of clues about how the brain
might do this so these are Jennifer
Aniston neurons I think they will cover
this earlier you know these in Europe
this is a neuron that only fires to
Jennifer Anniston pictures and only when
she's on her own not when she's with
Brad Pitt so so so you know so this is
very specific and here's a Halle Berry
neuron you know where here it can be you
know drawings of her not particularly
good drawings in fact and you know
there's one where she's dressed up as
Catwoman so and you know not only whets
just heard the text of her name and so
you know these are really abstract
neurons in some sense that are
representing the concept of Halle Berry
Halle Berry so then you know we've done
a lot experiments I did something hit my
PhD with my colleague Josh and Kumaran
where we looked at conceptual learning
in the brain with fMRI studies and so in
this study here we had people learning
about fractal patterns where so they saw
two fractal patterns on honest on a on a
TV screen and they had to predict the
weather outcome of the next day whether
it be sunny or rainy and they learned
this by trial and error and eventually
you find what you learn is that there's
some underlying pattern so certain
fractals you can ignore where their
position
and others the other fractals you can
ignore their identity and it's just
important where they're positioned on
the screen so these are kind of
underlying rules that were independent
of the fractals themselves and we scan
people learning this task while they
were learning the first task and then
once they mastered that there was a
second task where now we changed the
fractals to new fractals but the
underlying rules were the same were kept
the same and we scan them learning the
second task and they were much faster at
learning the second task and what was
really interesting is that the part of
the brain that correlated with this
increased learning in the second task
when we were scanning in the first task
was actually the hippocampus so the
amount of activity in the hippocampus in
the first task predicted later transfer
to the second task and being improved on
the second task
so it seems quite strong evidence that
then the hippocampus here it was quite
difficult for me to point at it from the
state podium here hippocampus here is
very critical in learning concepts
here's another famous study actually of
a sleep study also related to probably
hippocampal replay and the learning of
concepts so in this one the concept
you're trying to learn is this linear
hierarchy that a is better than B is
better than C that are de and F but you
you don't see that whole hierarchy at
once you only see the individual pairs
so you see that you you get shown B and
a together B and C together and so on
and you have to learn by trial and error
which one of the two is better but you
never see the whole hierarchy and the
joins between so your brain kind of has
to infer this from those individual
premise pairs and what's interesting
about this study is that when they did
this and they tested people 20 minutes
later on the separated pairs so maybe
like was be better than B then people
are a chance level 20 minutes later but
after 12 hours or 24 hours and a night
of sleep they're now up to 80% or 90%
successful on the these sort of
separated pairs the inference pairs but
even after 20 minutes there's no
difference on remembering the premise
pairs so it's really this in this sort
of extraction of
actual information that's happening
during the sleep so what about
representations so here's another thing
that I saw that made me think about this
is a huge clue to how representations
are structured in the brain so this is
very famous effect in psychology called
the drm effect and what happens here
just briefly is that you get shown as a
subject lists of words so maybe let's
take that top list dark cat white and
coal and you get you get shown all these
study words and you're told to memorize
them and then later you get tested on
these words and you get tested on like
did you see the word cat and you say yes
no but there's also critically some law
words so like here that word black or
the word river or the word cold which
are related to those lit word lists but
were not shown during the study phase
and actually people get em fooled very
reliably to say that they see they've
seen the word black or they seen the
word river when they didn't actually see
it during the study phase this is an
incredibly reliable effect so it's been
known since the 60s it was rediscovered
in the 90s and it's been repeated like
you know in thousands of psychology
experiments it's one of the most cited
studies but interesting no one had
thought about doing this with fMRI so
what we decided to do is look at this
with fMRI where now you know you can
think about what's going on in the brain
and why is this happening and one of the
reasons we thought was that a partial
priming effect so the idea is that you
know these the neural representations
underlying these words are maybe
partially overlapping perhaps they were
learned because of the way they were
learned through similar experiences and
we thought maybe the degree of overlap
would predict how how much the brain of
that individual would be fooled into
thinking that they see in that world
because perhaps all these partial
priming of these partial overlaps would
end up creating a full a priming effect
on the word which is maybe the way the
brain judges whether you've seen
something recently or not and that's
what we found is that actually once we
started scanning people looking at these
lists leave these word lists and being
tested on a drm effect we found that
afterwards there was one part of the
brain that predicted whether reliably
whether individuals were going to get
confused about whether they saw
it worked and the part of the brain that
kept that was predicting that was the
anterior temporal lobe here highlighted
in the yellow and that's actually known
to be the area that's involved in
semantic dementia so that's the part of
the brain that goes wrong when you have
semantic dementia so it's actually where
we predicted these types of conceptual
semantic representations would be and I
think this study now shows a little bit
about how those representations might be
organized so with imagination now then
there's the final part you know we this
is something I also did in my PhD was
looking at how people imagined and plan
for the future and one thing we decided
to test on was hippocampal patient so
patients without hippocampus as damage
to the hippocampus could they imagine we
know that there are music we know they
don't have episodic memory but they
could they imagine things about the
future and so we tested always imagined
things like cues like imagine you're
lying on a white sandy beach in a
beautiful tropical Bay describe what you
can see and when we looked at the
patient descriptions they were hugely
impoverished specifically in their
spatial coherence so what that means is
that they couldn't bind the disparate
components of a scene into a coherent
whole and you can see their performance
here they're the dark bar this is a
measure of kind of richness of their
description and they're massively
impoverished compared to age and
education matched and and verbal matched
controls on the right hand side we then
took this in the scanner and we found
that of course the hippocampus is
critical but there's also a whole brain
network a very reliable one that
mediates imagination and is also
involved in episodic memory so then one
is anything you can think of and we did
we haven't touched on this is also about
animal intelligence you know there's not
only human intelligence I think we can
also learn a lot from animal
intelligence so I was fascinated to see
whether rats can actually imagine and so
we designed this study but it was quite
elegant and quite simple to show I think
categorically that rats can imagine so
we first of all we looked at play cells
so for those of you don't know play
cells are neurons in the hippocampus of
a rat that signal where a rat is in a
location so here we got top-down looking
at a box that rattles in and and these
are two cells a and B that fire only in
that position in the box what we can
also we also know from
previous rat studies is the sequences of
these cells play when a rat runs along a
trajectory like a linear track so here
ABCD in the in and it replays in the
order that the rat moved in we also know
that when the rat goes asleep these
trajectories are replayed a very speeded
rates probably to aid learning so what
we did is we designed a teammate and so
now again we're looking top down the rat
there's a barrier on this teammate that
stops the rat reaching the arms but the
barrier see through so the rat can see
the arms it just can't move there so
initially in the first phase the rat
runs up and down the stem of the
teammate and we give it a reward we show
it a reward on the right hand side I can
see the food pellet but it can't reach
it then the rat goes to sleep and we're
recording from the rats brain at this
point and it's very important this
because we're this is this is the
critical data that we come back to to
analyze then in the second session we
remove the bat that the the barrier so
now the rat can freely move up and down
the teammate and so now on the arms of
the teammates we can find its place else
so here a and D and what we find is that
when we go back to this analyze the
sleep phase that in fact these place
cells were being replay or pre played if
you like during that sleep even though
the rat had never experienced it yet so
it only had only seen it maybe it was
thinking about moving towards the food
pellet so this is the first time that
anyone's ever shown imagined play cells
and what was very cool was that there
were significantly more pre plays if you
like to the right hand arm where the
food pellet was then to the left hand
arm in every one of the four rats so I
think that's pretty categorical that
this was actually behaviorally relevant
as well so we're now looking at
imagination based or modal based
planning in for our Atari agents and
this is a very this is an early version
of some of the models that were trying
to build in and use in planning so I'm
just going to end with integrated agents
so if you now combine all the things
I've been talking about into a single
agent then perhaps we have something
that could be deemed rat level AI so be
able to do unsupervised vision attention
have memory episodic memory navigate
round mazes and
perhaps even do imagination-based
planning and I think you know rad is
pretty smart so if we can get to this
level that we pretty spectacular and
maybe you can think of the Atari agent
as a sort of lizard level so here here's
a little sneak preview the kinds of
things we're working on so this is now
true feed 3d environments called
labyrinth this we've extended it from an
open source 3d engine and this is our
agent navigating around finding
rewarding apples and and also exits out
of the maze just form pixel inputs so
just from from the raw pixels so we are
doubling down on systems neuroscience at
deep mind and I think there's an
incredible wealth of information and
ideas and clues if you look you know
where to look and I think we're actually
just at the beginning of the influence
of neuroscience and AI on each other and
one thing we're specially excited about
is developing new tools and techniques
borrowed from neuroscience to help with
the analysis of machine learning systems
so I think we're sort of entering a very
exciting era an hour and we're doubling
down on these efforts and part of that
is actually I'm you know very excited to
announce that Professor Matt bot for
Nick who many of them you know from
Princeton is joining us full time from
February to head up the neuroscience
team and join our incredible sort of
in-house team and you'll know a lot of
his work I'm sure already from decision
making and control and working memory
and we also have a bunch of
collaborations with Oxford and of course
at MIT with CBMM program Harvard UCL and
so on and obviously we're expanding the
neuroscience team we're putting more
into this so if you're interested in the
work that some of the work I've been
talking about then come and have a chat
with me or Matt so I'm just going to end
by saying you know that actually we've
talked a lot about how neuroscience can
help AI but I also think equally
interesting maybe we can discuss in the
panel is how AI might help us better
understand the human mind especially
questions like their Christoph is
interested in like what actually is
consciousness and is it necessary for
example for intelligence and as Richard
Fineman one of my all-time here I'd say
what I cannot build I do not truly
understand and I think that's true of
intelligence too Thanks
thanks a lot Dennis we have time for a
few questions so thanks a great talk
imagination based planning I guess one
of the problems you have with with that
kind of system is how does the agent an
agent who acts how does it know when
it's thinking about something that is
not currently doing versus when when it
should be thinking about things that it
is actually doing and it seems like in
humans at least we have some sort of
cognitive level access to our own sort
of attentional mechanisms so we know
what we're thinking about and we know
perhaps the context and why are we
thinking about it you know in space and
time so if I think about my my birth
date a few days ago and I'm not thinking
about you know my I'm not confused in
that sense yeah sure I mean we work on
attention both for sort of internal and
external attention I think that's going
to be very important I think you know
actually the biggest issue with our
model based planning stuff is that and
what our our genitive models are pretty
good so if you measure them on
compression or other things measures but
the small errors compound very quickly
and so if you're trying to imagine you
know hundreds of steps out the errors
get quite large I think part of the
issue is is probably you know you don't
want to actually be imagining on the
pixel level ideally you want to be sort
of measure you know imagining on some
high level feature level which again
comes back to sort of concepts so I
would say that's the bigger problem then
not knowing whether you're imagining or
not I mean in an artificial system we
can make that distinction quite clear so
does that work yes it does a quick
comment and a question
you mentioned in the beginning this
blocks well problems that yeah you hide
a mess are you doing that
that you were able to solve and you also
mentions that you think this is a more
complex version of the cause of annoying
problem I think that's not the case
simply because in the Kasavin way the
problem you ask is additionally
constrained where you cannot place a
larger disk on a smaller one which means
that if you have n disks you really need
to 3 to 2 the N Mo's which means if you
have 30 disks you need a billion moves
to
the optimally which is not the case and
that particular reduced blacks were on
setting what is maybe more important for
the topic of this symposium yes in the
beginning you mentioned that you were
inspired by neuroscience as you did the
Atty game thing and and and you
mentioned the action replay as an
example but that of course goes back to
in 1991 know where that PhD thesis on
reinforcement learning about action
replay and which we did saw it with your
class yeah you sure yeah guys and at the
pointers however I said this was driven
by engineering and so the questions do
you have something well you really can
show neuroscience inspired us to do
something better from standard
engineering which would maybe address
exactly why I think that's a very
difficult bar because I think what
you'll find is that neuroscience ideas
have sort of seeping around everywhere
and help formulate certain ideas and of
course you should be doing both at the
same time so I think engineering should
go hand in hand with the neuroscience
and lots of ideas have been thought of
in the machine learning and lots in the
neuroscience and I think it helps they
help each other sort of confirm that
you're going in the right direction
I mean even comp nets with you-know-who
bum and weasel you know which ones
influenced who I mean I'm sure you know
you could say that the the commnets sort
of came up you know with little
influence format but actually I think
these ideas are kind of all mixing
together and help with this progress so
I don't know if you're asking for a
specific example of exactly a
neuroscience thing that was done and
then interpreted in machine learning is
that we were saying well confidence is
actually a great example because that
was in my point of view more or less the
last time when your science really
produced an important inspiration for AI
which because fukushima's conclusion
Anette works and the 70s were inspired
by a human and Wiesel whose idea of the
fifties I think this is actually a good
question for the panel
where other people might come in so
let's do this letter and maybe one two
quick questions we're here I try to be
very quick um
I'm surprised that you rule out an
eighth structure more or less altogether
and I wonder whether that's a
methodological commitment you said at
the beginning you want to do everything
kind of from raw pixels you don't want
anything built in there's a lot of
evidence from biology that the genome is
spending half of its effort so to speak
trying to build a very carefully
structured brain so why sure so in fact
I should clarify that so we're
interested in sort of learning as far as
possible sort of end to end but we're
not we're not against having modules or
I mean clearly evolution has given some
initial constraints so we're looking at
those kinds of things as well but we
would like the prior knowledge that we
build in to be as minimal as possible
and as generic as possible that's maybe
another longer way of saying it's an
empirical question with the right amount
should be cool yeah very brief
collaboration and competition have some
neural basis in the frontal cortex so
like this the agents are very
independent what role do you think
competition and collaboration play
between learning systems so we arranged
in things like meta control where where
there's you know more than one type of
control system and then you've got to
decide which system to trust or which
system to switch to so the brain you
know definitely does that between modal
free model-based perhaps between
episodic controllers as well so we're
looking at that and you know
implementing something similar to that
maybe you know deciding around the
uncertainty of each of the controllers
so we're looking at that moment should
say thanks to all my colleagues of
course that deep mind all the amazing
people who done all the work on on the
screen here and many more thanks thanks
let's thank Larry is again
you
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>