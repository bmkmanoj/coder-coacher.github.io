<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UW - MSR Machine Learning workshop 2015 - Session 1 | Coder Coacher - Coaching Coders</title><meta content="UW - MSR Machine Learning workshop 2015 - Session 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>UW - MSR Machine Learning workshop 2015 - Session 1</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1QcCQ_vVnSU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning
it is a great pleasure to open the third
uws our machine learning workshop we
started this workshop this series mainly
to facilitate the interaction between
researchers from u-dub and emma starr we
were expected to see the same usual
suspects year after year but the foam
has expanded considerably from from
u-dub alone we see almost every
department being represented here and we
see other universities such as western
washington being represented here from
microsoft will see we see any group any
product almost everyone is being
represented here as well as other
companies such as people from Google
Amazon Facebook sup zillow we are very
glad to have all of you here today in a
sense we are having a Pacific Northwest
machine learning summit here today
unfortunately one of the chairs here
today will remain unspoken for Ben
Tasker who was one of the pillars of
this community is not with us here today
Ben was one of the organizers of this
series of meetings and was speaking in
front of these audience just last year
in our last meeting he passed away
unexpectedly just a few weeks after the
previous meeting we will never forget
his contribution to this community we
have a busy program for the day we will
spend the morning in this room we will
hear ten talks and nine spotlight talks
from students in the afternoon we will
move to the McKinley room on the other
side of this building for a poster
session and a reception please check the
program on our website you see you have
the URL over there since the program has
changed in the last moment because one
of the speaker couldn't make it we were
lucky to get sponsorship from audra for
research to award prizes to the best
student posters therefore when the
poster session starts we'll open some
polls and you can vote for your favorite
poster and we'll gonna announce the
winners at around five-thirty note that
not all posters are eligible for the
competition
and due to conflict of interest rules we
try to build a program that will give
the opportunity for more people to
present their work we have added the
spotlight talks and extended the poster
session we also ask you in your
registration form to suggest the titles
for paper that have not been written yet
and some of you took it really seriously
and and during the breaks we're going to
present this these titles then you can
use them to inspire you for your future
studies I'm just going to give you a
sneak peek one of the title suggests the
deep purple machine this is an algorithm
for classifying rock and roll music or
the paper about the empirical evidence
against the no free lunch theorem at the
uws our workshop and indeed we're going
to have a free lunch today it is free
for us but someone in ms are actually
signed the check thanks Chris if you
specified your dietary preferences in
the registration form your lunch box
will be waiting for you at the
registration desk otherwise we will have
an assortment of lunch boxes out here in
the hallway you're welcome to enjoy your
lunch here and they already told you or
more take it outside however please be
respectful of other meetings in this
building today since we have many
speakers and full schedule will have to
be strict with respect to the timetable
if you are speaker please introduce
yourself to the session chair at the
beginning of the break before your
session starts when coming back from
breaks we will signal for the beginning
of the next session using the bell and
the clock courtesy of my kids when you
hear these sounds please find your way
to the lecture room we'd like to make
sure that this is a productive and
enjoyable day for everyone if you have
any issue during the day please see
Sierra at the reception desk or one of
the committee members marina jf
Sebastian or myself
I think we've covered all the
formalities and we can get started with
the real thing our first speaker will be
a leaf or Hardy from u-dub who will
speak about scalable visual recognition
ok it's a good minute good morning
everybody so I'm going to talk about
scalable visual recognition today the
stuff that I'm going to be talking about
would be basically joined work with a
set of wonderful people Thursday hameed
Santosh and the last guy I don't know
this but probably not here today ok so
let me start by showing you the current
paradigms of recognition what do we do
in actually recognition these days the
current paradigm recognition sort of
follows loosely this this pipeline we
start with a set of categories that we
want to recognize we have our own way of
either extracting a hand design features
or learn our deep features or
discriminated features and then we have
a different ways of actually training a
set of different kinds of models for
recognizing your chat those categories
and at the end of the day we have a
basically very explicit way of
evaluating how well we're doing on
benchmark tasks and it's each year we're
actually improving upon and asked and in
the last five years we've actually seen
dramatic improvement in the performance
of these category recognizers or object
detectors over the years now let's look
into let's look under the hood under
this pipeline and let's actually start
with a very beginning of this pipeline
we're going to decide on what are the
things that we're going to learn and how
do we actually start this learning
paradigm so what if you look closely
into this paradigm what's going to
happen actually under the hood is that
somebody or a group of folks actually
have sat down row bunch of category
names that they thought are interesting
image search basically search for images
of those categories that's is basically
my running example would be horse so
we're going to start basically searching
for images of horse people are going to
go over each any of those images this
cart non horse pictures among the
retrieved images draw bounding boxes
around horses and that would be
basically our way of curating a training
set and also a test set for our
recognition algorithm and one problem
that we almost always face in
recognition special and visual
recognition is a well-known phenomenon
called intraclass feelings what it is
basically members of the same category
might look very very different in pixel
space so eventually I one may want my
recognition algorithm to assign chair to
all of these instances despite the fact
that actually they don't look they don't
look much alike in the pixel space so to
handle this intro class variance we have
a pool of different mechanisms we either
try to basically learn features that are
robots to this variance or be built we
build models that can actually handle
this much of intro class variance and to
make sure that basic our algorithm most
of the time this creative algorithms are
aware of what's happening basically
we're going to make sure that we're
going to feed the algorithm enough
coverage of what's actually happening
within this category meaning that if I
want to build actually a really wide
horse detector what I need to make sure
is that my horse detector actually seen
multiple different cases of horses under
different variations how do we do that
basically we asked these people who have
actually annotated those images to be
creative come up with different queries
to search for other than horse so that
you can actually put horses in the
context of different things for example
we might actually search for racing
pictures we might actually search for
horses in the beach things like that to
get basically a variety of different
kinds of horses so that's basically
under the hood on this pipeline and
that's that these two pieces are the two
pieces are going to actually give us a
hard time when we try to scale up
recognition we want to actually go from
hundreds of categories which is what we
do these days hundreds no thousands to
hundreds of thousands of categories or
actually in my way of talking about it
to anything but by anything actually
mean that anything that's visual so how
can i scale this up so that actually my
recognition by
you can also learn to recognize about
these verbs actions emotions professions
many different different countries so
how can I do that how can I learn a
recognition algorithm that does not
require a human annotation in the
pipeline to be able to learn those kind
of things so and basically the question
is that how can we learn about anything
that's visual and since we want to
actually handle inter-caste variance the
question that I'm actually asking is how
can I actually learn everything about
anything and the bottleneck for us in
this process are these two steps hey I
need somebody to come up with a list I
can actually expand my query so that I
see different varieties of horses and
also I need somebody to label images as
relevant irrelevant and draw a box
around relevant pieces in this case
basically the greenboxes corresponds to
basically the pixels that we believe
correspond to horse and once I do that
basically the rest of the soft actually
can follow dramatically but there are
issues if I want to scale up recognition
I cannot actually stick to that paradigm
why because there are serious
fundamental problems with actually
having human in the loop one of them is
that I cannot actually I asked human to
be exhaustive or comprehensive in terms
of a vocabulary that explains the with
in class variants me I cannot actually
ask people to list all possible things
that can happen to horses that modifies
their appearance it's going to be hard
for people and it's gonna actually be
extremely biased if I might be a horse
expert and my vocab or it might be
actually very different from your
vocabulary and it's very very hard to
actually be comprehensive are almost
practically impossible so that's one
fundamental problem that a we might be
able to actually draw bounding boxes
around whatever that you give me but
actually being able to ensure that we
can we have enough intraclass variance
in our data set I cannot rely on human
annotator lists the other issue is that
basically some of those vocabularies
might be actually category specific for
example grazing might apply to both
sheep and horses but
we basically horses do rain we do like
basically barreling with them horses do
roll I actually didn't know that for
ships ships grace but they don't do any
of those things so hopefully we not
we're not going to try to basically do
raining with horses we might want to cut
we might wanna basic shear ships but
hopefully not horses so there are
different vocabularies that apply to
different categories and actually what
makes it even harder is that may be
exact same ward like cut might mean very
very different thing with in different
categories a cutting horse is actually a
very specific movement in that sport but
cutting Shipman's shearing ships and the
last basically to me the most
fundamental problem that we have is by
adopting the current paradigms of
recognition we're actually committing to
a unnecessary hard decision early on so
when we pick an image net to actually
perform recognition we're actually
adopting implicitly to a very
fundamental restriction that we believe
that basically the boundaries provided
by Ward net are actually visual
boundaries in the world and in most
cases they are not so in image that we
actually spend a lot of our times
recognizing between very specific very
detailed breeds of horses where have we
knew have no clue about for example
actions for context for parts so how can
we avoid this issue so basically we have
fundamental issues in scaling up
recognition if you want a bill automated
system if I want to build a fully
automated system how can I do that and
that's basically the topic of a system
that will be built at you Robin ai2 it's
called Levin we call it learning
everything about anything and of course
by everything I really don't mean
everything and buy anything I really
don't mean anything but a lot of things
everything that's visual and they see
you're gonna see limitations of what do
I mean by everything and anything so how
do we do this so what are the problems
just to remind you basically these two
are the problems how can I be exhaustive
in terms of discovering your vocabulary
that explains the width in class
variance and how can I actually
alleviate the need for human annotation
in labeling what's in the image where is
the interesting stuff happening in the
image
and it turns out that actually the
solution to this actually might be using
language if I go to the language and ask
basically language resources what are
the modifiers that can ever modify a
concept of interest that's a horse well
the answer to this actually can be
extracted from textual resources in this
case we went to Google Books use Google
engrams with a little bit of dependency
parser on the top of the Google Ngram we
can actually get a very very large list
of things that can modify a concept of
interest in this case horse and we have
this this is actually twenty thousand
dimensional these are basically sort of
almost all of the modifiers that we've
ever used in written English about
horses in all the books and of course
not all of them are visual for example
well reining horse might actually be
visual meaning that there is actually a
consistent visual signal among the
images that corresponds to the reining
horse whereas they'll something like a
last horse is actually not visual
there's not much visual structure among
the images that returns for the last
force and I'm not going to go into the
details are basically how to how to deal
with this problem there are actually
simple scalable tricks that you can do
in an unsupervised fashion to get rid of
those and once we figure out what is
usual what is visual on what is not
visual then the next step is basically
learning the model but will still face
the problem of intro class variance but
the nice thing about this is that at
this point my intro class variance is
actually manageable why because I'm only
concerned with learning a model for
jumping horses in this case I'm going to
actually query for jumping horses
instead of horse the set of images for
which actually want to learn the
detector or a visual record water
recognition model actually expose a
limited visual variance that means that
actually I might be able to alleviate
the need for pruning out the images and
provide a bounding box how can I do that
well whatever whatever is your favorite
model learning algorithm I'm going to
introduce couple of latent variables
into that and those latent variables
will basically tell me where the boxes
in those images i start with actually
the actual box and basically within the
algorithm what i'm going to be doing is
I'm going to basically asking the
algorithm
to go and find the two to find what's
common between them and once I'm tuned
to the box of interest then i can
actually let go and learn my favorite
the recognition models whatever it is if
it's TPM if it's a deep model or what
your choice of classified and now what's
going to happen is that now I'm going to
actually learn a different independent
DPM model for each of those rows and I
have thousands of those rows so
basically I am I end up learning
thousands of different category models
for just one category horse and those
are basically sort of tune to all of
those local local modes in the
appearance variation within the concept
horse now let's see what happens but
actually I wanted I started by building
a horse detector now I'm in this mess I
have thousand detectors for my horse how
do I actually make sense out of that
space so you can actually think about
this space as a huge gigantic graph the
nodes are the detectors I have thousands
of those the edges would be basically
the inter detectors relationships I can
get the detector a running on the
training set of detector be and vice
versa and cook up a similarity between
these things what's happening in this
graph basically there are so many nodes
that are very very similar to each other
and there are nodes that are very
different so what I'm interested now I'm
facing a community optimization problem
where what I'm interested in is actually
picking a subset of these nodes where
we're actually I can maintain a good
coverage of the space of visual variance
within horses and at the same time
maintain a subset of high quality
detectors and you can actually think
about it at the standard basic
optimizing for quality and coverage it
turns out that I'm going to wake Jeff up
it's a modular objective function
actually works here so basically we have
a nice sub-module objective function
that finds a subset of those nodes that
covers the space of visual variance
within horses and maintains a set of
high-quality images again if you're
interested we can talk about it offline
if not let's actually move on to the
result so basically what i'm showing to
you right now is a set of nodes that
actually ended up merge together because
of that optimization problem so the
optimization thinks that basic eating
horse and gray
horse should not be two nodes in this
graph actually should have just one note
for example a cantering horse and the
looping horse should be the same angry
mob angry protestors and angry crowds
should be the same note that virtually
were starting actually walking away
walking outside the boundaries that we
were comfortable with an object
recognition which was cat and dog and
horse now we're actually starting to
learn learn phrases learn detectors for
those phrases and now we can actually be
basically merge them together to have
one big mixture model that represents
horse or a concept of interest and once
I do that basically I have my horse
detector that and I can actually run it
on what are the different things it's
not even it's not it's not objects
anymore right I can actually start
learning detectors for Christmas so I
can basically my Christmas model has 800
different these mixture models and
within it including the beer the tree to
wrath the sheep and all of those things
kitchen standard Becky we can have a
kitchen pantry detector we can have a
kitchen sink detector to notice the red
boxes are basically the latent variable
I was talking to you about that
basically I'm going to end up converging
to that it might be a country it might
be concepts like electricity it might be
even weird things like DNA so we didn't
actually start this with these set of
objects so this is a system that's
actually it's been up and running it's
called love and it's actually you can go
to 11 at CSL washington edu and we
started the standard 20 categories in
pascal we open it up and people actually
have started submitting queries and what
I'm showing you is actually the right or
the left top corner right top corner of
the page so right now basic appears
started expanding the models and right
now we have almost hundreds 15,000
different detectors in this model
annotated more than 100 million images
and provided basic more than 22 or 23
million annotations and the DNA
electricity and all of those weird
concepts are actually entered by users
of this system so next time you are
actually learn the object detection
model that needs to expand the coverage
of you of the visual variance within the
skin within categories basically I
recommend trying
11 so now that I have this scalable
recognition models basically i'm going
to showcase too neat applications that
comes out of a scale or recognition
system one of them is basically a
segmentation model so segmentation is
basically a problem of figuring out
which pixels corresponds to the object
of interest which pixels does not
basically a finer representation other
than the bounding box but requires
extensive supervision what I want to do
segmentation what I typically need to
provide my training set I need to
provide exact masks boundaries of
objects of interest to be able to
perform segmentation but with actually
web-scale recognizers that are carefully
and semantically grouped together what i
can do is i can actually end up with
clusters of basically really well
aligned set of images and once i
actually have these kind of things i can
actually reformulate the segmentation
pipeline and now my segmentation
pipeline looks like basically go within
any of those semantically constraints
group of well aligned set of images
perform a co segmentation meaning that
automatically without any supervision
figure out what's coming between this
group of where align images this in
practice is not possible but in those
set of carefully curated images
automatically this is actually possible
may I can actually go ahead and segment
out what is common between any of those
groups and now for horses i'm going to
end up with thousands of different
segmentation ask now when you give me a
new image what i can do is basically i
can figure out where in this graph
actually your image appears and use the
corresponding segmentation map pull them
together to perform a segmentation and
now actually we can actually text
segmentation to an ex richer level
because now our segmentation masks are
not just cats and dogs we can
automatically without supervision
without explicit human supervision we
can start segmenting things like a sad
dog a running dog a who for the horse
with notice that basically there is
basing no explosive human supervision
the system can actually go ahead and
learn it off the web why because
basically the images are grouped
together carefully using semantic
constructs and now we
actually go ahead an image and segment
imagenet without any explosive
supervision so what I'm showing you is
basically images the state-of-the-art
segment or an image net that is
supervised and our stuff which is
basically not using using any kinds of
supervision and that means that
basically we can figure out what pixels
corresponds to what objects without
extra supervision but just being careful
about how can I use web images so that's
the one we want that's the first
application that I wanted to talk to you
about the second application that
basically requires scalable recognition
is actually whiskey unfortunately not
this kind this kind we're still working
on them we haven't get good results yet
but basically these kind of whiskeys so
this key is a system is a knowledge
extraction system that is based on
visual information so later in the
afternoon I'm actually I pretty good
Larry's going to talk about the
importance of actually common sense
knowledge and recognition why do we need
to extract common sense knowledge and
why is this useful i'll leave that part
to him but basically what i want to talk
to you about right now is that basically
the knowledge extraction is an important
problem people in language processing
have been starting this problem careful
for years have been dramatic
improvements over the course of years
but there are some pieces of knowledge
that are really hard to get out of text
why because a they're not frequent
enough be there typically actually
appear in a convoluted form or a very
complex centex sentence structure that
makes it really hard for actually
parsing or semantic understanding to
work on them and this key actually aims
at extracting the knowledge of the world
hidden within pixels so if I ask you do
you guys think that do dogs eat ice
cream answers somebody says no somebody
says so you so rambles yes right how do
you say yes so basically you've seen
them actually eat ice cream in the real
world and if you haven't how do you defy
this well you have two choices right you
have computers in front of you will go
to google and start reading documents on
dogs
the ID restrictions and figure out if
they eat sugar and milk and maybe if you
figure out that you actually might eat
ice cream or go go to google images or
Bing images and just search for dogs eat
ice cream and what you're going to see
is something like that i'm going to show
you this you're actually confident that
yes dogs do it ice cream why is that
because you're going to see the dog
you're going to see their ice cream
you're going to see the ice cream in the
right end of a dog and dog is actually
in the Oh engineering position right so
these are basically you how you're using
q very important cues to do that one is
appearance models the other is very
careful spatial relationships between
them and that's the essence of whiskey
basic vissa t is a fully automated
system that basically the input to this
key is a textual query like do dogs eat
ice cream do snakes lay egg do men right
elephant whatever you want to like in a
simple and linguistic form and the
output is basically the score that says
how much do we believe actually this
statement is true based on the
consistencies that we get out of the
visual recordings of this phenomena on
the web how do we do that basically the
way that we do is basically let's start
with the running examples of do bears
fish salmon or not basically the input
would be this query we're going to go to
the web we're going to go to the 11
basic you learn automate in an automatic
fashion very different set of detectors
for all possible combinations of these
actually these detectors I have two
minutes I'm going to actually go fast so
basically I learned the detectors for
all possible combinations of salmon bear
fishing some fishing salmon and bear
fishing salmon detectors then I
basically designed this factor graph
that basic is concerned with the spatial
relationships between these detectors
and to me a score that again i can
assign to basically a statement would be
the most probable explanation of this
factor graph and at the end of the day i
do the most probable explanation and
that gives you a score that score tells
me that how much visually i believe this
phenomena is true and basic using that
you can actually just you need to it's a
fully automated system you type in a
statement here it comes the score
so what I'm showing you is set around
those segments the x-axis is basically
the confidence score out of the MPN PE
and basically the green is actually true
statement the red news wrong statement I
would like to see green stem on the top
red stain at the bottom unfortunates not
always the case but mating is sort of
like that on the top we see things like
horse drawn carriage person right horse
a horse hen lay egg and at the bottom we
see things like man mandroid sheep horse
each bone horse lay egg and things a guy
Betty that would give me a way of
verifying statements and I can actually
start walking into this state of
actually answering questions now I can
actually new married all of those things
automatically I can start answering
answering questions like what do dogs
eat the top row is basically what comes
out of this key the bottom row is what
comes out of a language model for
example the division based system thinks
that dogs eat rabbits sorry grass fruit
cheesecake food not and of course
breakfast and bone and the language
based smaller things that basically dogs
eat contest grass cat food bone
watermelon dinner man and meat and I can
actually start asking questions about
basically different things what kind of
animals lay egg a turtle is snake if
frog a bird unfortunately beer cat and
the language based model believes that
basically a hen lay eggs and the rest of
them I actually don't worry okay so let
me wrap it up so basically what i
believe is that the next actually future
of recognition would be in the
scalability so we need to rethink the
pipe on the recognition with actually
scalability in mind we have to make sure
that we understand every detail pieces
of actually recognition algorithm to be
able to to to scale things up visual
knowledge extraction is actually by
prologue of scalable recognition we're
going to see many many appearances of
actually these kind of research within
research community with vision research
community hopefully sometime soon if
you're interesting any of those topics
basically we're actively pursuing these
topics so if you love and also at AI too
so talk to me for instance thanks Ron
pushes part of you talk you you talked
about getting the the data for training
but how do you test accuracy right so
you have the same problem yeah so it's
actually a very good problem question to
ask so we in within within recognition
community that the bad news is that we
honestly don't know how to evaluate
large-scale recognition we still have
our limited benchmarks like Anna
Jeanette and things they've got so one
thing I can do is I can actually go off
to web learn my detectors on the web and
just test it on standard benchmarks and
I didn't basically wanted to bother you
with the details of axial of the
evaluation but we have those kind of
evaluations so basically there's this
scale or recognition results are sort of
on par with supervised methods it's not
the best detector that you can get but
it basically scales to everything but
how can I evaluate in large scale
actually there is there is no elegant
solution so which in Turkey and then you
basically can start walking to the
paradigms that we already have for the
benchmarks but that's just the way that
we actually do these things right now
this is the best way unfortunately no
questions so you probably get this
question a lot but given you know the
fact that the population of the planet
is seven billion and it's growing by
about a billion people every 10 years I
mean the obvious answer question is why
not why can't use crowdsourcing to
essentially get everything especially
given the ubiquity of network access and
everybody has devices and can solve
small captures perpetually on their
portable devices to to do annotation
tasks and maybe maybe unlike what you
said at the beginning maybe it's
actually quite possible to get all of
these different categories and the fact
that there are different semantic
meanings are different for the same tags
or different objects doesn't necessarily
cause any problems it just means that
the matrix is sparse God so there are
actually several questions within that
question so one of them is that well
there are actually if I open up a
dictionary there roughly in the order of
300 thousand words in it right so if
it's three hundred thousand why can't we
just ask Google to annotate everything
in that their issues right so one of
them is that for if you're human
annotator is actually sort of very hard
to be a comprehensive and exhaustive for
with in class variants that's one
problem but the actual problem that were
questions why is why necessarily is that
the case because given that there are so
many people who potentially are going to
be in the future willing to annotate why
it necessarily is that true I don't see
why doesn't so 11 what ya 111 approach
to that problem would be basically just
scale it up across annotators so that
each person might not be exhaustive but
a collection of actually annotators can
be exhaustive and the issue basically
these kind of paradigms would fall apart
when we go into the space of
combinations because as you see
basically we're going to go to the
phrases I'm going to go to the longer
phrases and now not only I don't need to
i don't all you have to basically
annotate 300,000 or million different
objects in the world but i also i need
to start basically labeling combinations
and that's where things actually get
unmanageable even on Mechanical Turk or
even with Google budgets because then
you can not start this again attaining
all possible combinations within million
entities and that's basically where
we're scare wall basically fully
automated
Systems comes handy once again on a
sneaker is a sudden one where she and
she's going to talk about user brought
machine learning so hi I'm thanks Ronny
I'm as Roni said my name is Salim amurri
I'm a researcher here at Microsoft
Research in the chill group which stands
for computer human interactive learning
and today I'm going to talk to you about
some of the work that our group is doing
along with several other collaborators
at MSR on how we can work towards more
usable machine learning okay so what do
I mean by usable machine learning so
when we think of machine learning we
often think about it in terms of the
problem that machine learning algorithms
are trying to solve which you know what
the most basic level is something like
this right where you're given some data
and the goal is to learn a function that
can predict the labels on new data right
losses lease for supervised machine
learning however in the practice of
machine learning what actually happens
is there's this guy right there's a guy
who is responsible for collecting some
data creating some features tweaking
parameters and then sending all of that
information to a machine learning
algorithm in order to learn the function
and you know this is an iterative
process usually in the in the sense that
this guy's not going to get it right the
first time right he typically has to
somehow inspect the algorithms
performance and then try to improve the
model by supplying it with more and more
information until it's working the way
he wants it to so this is you know sorta
typically how we use machine learning
and this guy here you know really is you
you know well this guy's are on ebay
like you you know as one of the workshop
organizers today you know you represents
people in this room you know the people
in this room who use and interact with
machine learning algorithms everyday so
you know while we talk about advances
and when we talk about advances in
machine learning we're often talking
about how we can improve you know
machine learning algorithms because
people are so integral to the machine
learning process there's also also a
really important opportunity to advance
machine learning by improving the
abilities of the people who are
interacting with and using machine
learning algorithms every day
so this is what I'm going to talk about
today I'm going to give you a couple of
examples on how we can you know help
this guy which you know i will stop
picking on Ronnie you know really how we
can help people like you better use and
interact with machine learning and you
know there are many sort of interaction
points between users and machine
learning systems you know from
collecting data the labeling to
featuring and so there are many
opportunities to improve the interaction
here but today I'm just going to talk
about a couple of them I'm going to talk
about labeling is actually very related
to previous speakers on talk about how
to how to label data appropriately and
performance analysis okay so labeling so
a previous speakers talked about you
know people are often recruited to
manually label large data sets and then
those data sets are then used to Train
machine learning algorithms the problem
is that labeling is not really as easy
as it seems so when people are labeling
data they're trying to label data
according to some target concept that's
in their heads right so imagine I was
you asked to label web pages as being
related to cooking or not about cooking
so what I would do is you know I start
labeling some examples that you know are
clearly about cooking so these are
examples you know contain recipes so
they're clearly have to do with cooking
then I've label some negative examples
examples that are clearly not about
cooking so these definitely don't have
anything to do with cooking but then
what if i came across an example that
look like this right this page is do you
know about catering which you know is
kind of like cooking because it's about
sort of preparing food so maybe i'll say
you know yes this is about cooking so
then I you know keep going ice keep
labeling some positive and negative
examples and then I might come across
another example that's kind of like that
catering page right this is about you
know some chef coming over to your house
and cooking for you so at this point you
know I might say that well you know this
is actually not about cooking it's not
really about like me learning how to
cook or cooking myself it's about
someone else cooking for me so I'm going
to say no this is not about cooking so
that's the problem right the problem is
that I just labeled this page as not
about cooking but what about that
previous page right where did I put it
you know do I need to find it are there
other pages like that that now I need to
find a new
move and change my labels for the
problem is that the concept that I'm
labeling for is evolving as I'm looking
at data but the tools that we have to
label don't easily allow us to revise
and refine our concepts as we go along
and you know this was just a few example
so you can imagine how difficult it
might be if we have hundreds or
thousands of examples to label in fact
we actually did a study where to
investigate how much of a problem this
concept evolution problem was where we
had nine machine learning experts label
the same set of data in two sessions
about four weeks apart and what we found
was that even machine learning experts
were only about eighty percent
consistent with themselves from one
labeling session to the next on the
exact same set of data and in fact six
out of the nine people's labels change
significantly from one labeling session
to the next so in order to try to sort
of help people with this concept
evolution problem we came up with an
interaction technique that we call
structured labeling where we essentially
allowed people to explicitly organize
their concept via grouping and tagging
within a traditional labeling scheme
okay so let me show you what this looks
like so so this is the traditional
labeling scheme where we sort of force
people to categorize data at a high
level and we put replace it with
something like this where we allowed
people to create subgroups within these
high level categories okay so for
example if I came across this catering
example and if I was unsure about what
to do with it what i would do is create
a subgroup about it and then i could tag
that group and say you know this is
about catering so i could remember how i
labeled examples of that kind so what
this does is that it sort of services my
labeling decisions to myself so that
later on you know when I came across
that other example that was kind of like
catering I can group it together I can
say you know this you know actually is
the same as part of the same subgroup
and then later on you know if I do want
to revise my concept I can easily go
back and do that by moving this whole
sub group at once and changing what I
consider as part of my concept of
cooking or not so these groups and tags
not only sort of you know let me recall
and revise my labeling decisions as I go
along they actually let me defer
my decision-making great which is kind
of important right because a lot of
times people agonize about how to label
data you know is this pay this catering
page about cooking or not is this horse
you know really about horses or is it
you know not really something I want to
consider as part of my concept but what
this does is it allows people to sort of
tag things that they're unsure about and
then change their minds later on as they
observed data um so we also came up with
an assisted version of the structured
labeling tool where based on some pilot
studies that we had done with an early
version of the system and we noticed a
few problems so first of all we noticed
that people didn't always provide tags
when they came across groups that they
wanted to to put in subgroups but then
you know later on when they wanted to
remember what was in each of these
subgroups then they'd have to manually
go in and look at them and that well
that was you know a big pain so what we
did was we added automated summaries to
the groups based on the items that
people group together so in case they
didn't tag it manually we they could
help we could help them recall what they
put in each group we also found that
when people were labeling they'd often
you know remember creating groups of
similar items so that if it came across
an item they'd remember like oh I saw
something like this before but I don't
really remember where I put it so what
we did was we provided recommendations
of where they should put these groups
based on item similarities so you look
you measure the similarity between the
item you're currently trying to label
and all of the previous groups and then
we recommended which group to put it to
put these items in using this little
sort of star we also found something
interesting was that people didn't want
to you know make labeling decisions or
didn't want to put in effort and making
labeling decisions for items that they
thought were outliers so you know if you
thought that you know if there was only
like one catering example in all of my
data then maybe it doesn't really matter
whether I label it as positive or
negative but if that example is very no
representative of some concept that i
want my model to really recognize then
you know I'd probably want to put in
more effort to label them correctly so
to help with this we what we did was we
showed people similar examples to the
page that they were currently trying to
label so then they could recognize
whether it was part of
if it was part of a larger concept then
they could decide whether or not it was
worth sort of structuring their labeling
for so we did an evaluation we had about
15 people come in we you know did a
within-subject sign we had them used
each of the different tools as
traditional labeling manual labeling and
structured us manual structured labeling
and then the assisted version we had
them do three different tasks you know
did all the counter balancing all that
and what we found was that people in did
in fact youth structured labeling when
it was available we found that people
created groups and revise their labels
significantly more with the structure of
labeling tools then with the traditional
tools that means that they were they
were allowed to evolve their concepts
more easily with the structured labeling
tools we also found that people labeled
consistent more significantly more
consistently whipped the structured
labeling tool and preferred it over
traditional labeling and they stated
things that you know it help them
organize their concept and decide what
was part of their concept and not and
helps her to reduce the cognitive load a
cognitive burden sort of the traditional
labeling task so so this is just you
know sort of one example of how advances
in our interfaces and tools can actually
really help people better interact with
machine learning algorithms and so now
I'm going to talk about switch gears a
little bit and talk about a different
example which has to do with sort of
another critical step in the interact in
the machine learning process which has
to do with performance analysis so with
performance analysis the idea here is
that the performance that you see and
each iteration of model building should
really directly influence your
subsequent actions right that's the
whole idea right if your model is
performing poorly you're going to try to
figure out what's happening and you're
going to try to fix it the problem is
that most of the time what we see at
this stage is this right some some
number that summarizes the overall
performance of your model the problem is
that you know what does that really tell
you right it can sort of signal that
there is a problem but it doesn't really
tell you anything about you know where
those problems are what you need to do
about them and even if you're lucky and
if you have several summary statistics
or a different you know charge and
standard you know performance metrics
charts and graphs like precision recall
curves you know all of these techniques
will convey the
sense of errors but they don't tell help
you figure out where those errors are
and what you need to do about them in
order to do that what you usually then
have to do is go to a different tool or
switch to a different mode and actually
pull up your data to look at it and try
to gain insights of where the problems
might be so the problem here is that the
switching between looking at summary
statistics and different tools for
looking at your actual data can actually
be really disruptive to your primary
task which is actually to build your
model right and in fact there was a
study done a joint study actually by
people from both eww and MSR where they
studied this and they found that this is
having to switch between summary
statistics and looking at ax was
actually shown to discourage people from
looking at their data and actually let
them lead to a more trial and error
approach to model building where people
were essentially randomly trying to
check tweak parameters of their models
or change features in order to make
these summary statistics go up and then
then they would only sort of look at
their data what to do any error analysis
when those trial and error approaches
failed so what we wanted to do here was
sort of come up with try to come up with
a way to encourage people to more
regularly look at their data during
model building and so we came up with a
tool which we call model tracker which I
will attempt to demo this
all right okay so this is model chaga
the model checker is down here at the
bottom and I'm just going to show you
this in the context of a system we call
ice which is an interactive
classification and extraction platform
that our group here is developing at MSR
and I won't be able to talk about it in
a lot of detail but just for the
purposes of this talk you just need to
know that up here sort of where a person
would label data so i can label these as
positive or negative and over on the
left here is where i could sort of add
features so in this case i've actually
already created a model I've already
trained a classifier to recognize
whether web pages are about cycling or
not cycling and I've given my system
about 511 examples and about six
features at this point now in ice what
we used to have is this at the bottom
here this is sort of the standard
summary statistics and graphs that you
might see in order to try to understand
the performance of your model so we'd
have people look at this and then if
they decided that maybe there's a
problem then they would switch to a
review mode and then try to sort of
figure out where those problems are they
might sort of you know sort or filter by
arrow errors and then try to look at
their errors to try to gain some
insights about what to do so what we
wanted to do sort of like encourage
people to more to more easily look at
their data and so we replaced all of the
summary statistics and graphs with this
visualization here at the bottom so I'll
explain what the visualizations showing
and so each of these little boxes here
corresponds to an example that I've
already labeled so in this case I've
labeled 511 examples so they're 511
boxes here the color of the box is the
label data gave so if I said something
was positive it'll be labeled green if I
said something was negative it'll be
labeled red and now the boxes are laid
out horizontally according to my
classifiers score so low scoring items
are to the left and high-scoring items
are to the right so what that means is
what I want this visualization to look
like is that I want all my greenboxes to
be on the right and all my red boxes to
be on the left right that would mean
that that my system is very much
agreeing with what I said so what this
is doing is is actually showing you
performance right it's actually showing
you not only like where the system is
performing well where it's agreeing with
you it's showing you your errors rate is
showing you these the redbox is over to
the right our false positives the green
box is over to the left are false
negatives so it's showing you your
errors and you know unlike with standard
performance statistics and graphs where
all errors are sort of treated equally
this is actually showing you also the
severity of your errors right so the
very egregious errors are the ones that
are at the far end of the visualization
because that's where the system and you
very much disagree that's in contrast
sort of the two the errors that sort of
error in this uncertain middle area so
what this allows is that it allows me to
actually directly access my errors
because I'm actually showing the
individual examples here so I can
actually click on these boxes so I can
click on these areas so let's look at
one so so this is a false negative it's
a green box that has a very low score
I'll click on it and I can click on it
in its directly brought up to me so I
don't have to go find where my errors
are try to understand what the problems
are so I clicked on it and it's actually
brought up for me and so you know if
your recalls trying to create a
classifier that can recognize whether
web pages are about cycling or not I
actually labeled this as positive but
it's actually it's a recycling place in
off-site night not a cycling page so
that's actually a Miss label on my part
so but I can do is I can easily go ahead
and fix it and update my model and then
my system will retrain and then show me
the results after fixing that problem so
um by doing this works not only allowing
people to see their performance but
there were allowing them direct access
to the heirs and letting them prioritize
their efforts and debugging now this
visualization actually subsumes the the
same information that you would see in
all of these traditional summary
statistics and graphs so for example
confusion matrices which show you
agreements and disagreements there that
you know like i said is what we can
already see in model checker we can see
the agreements are the green boxes to
the right red boxes to the left and
disagreements are the red boxes to the
right and green boxes to the left but
unlike with confusion matrices which are
threshold dependent meaning that if you
wanted to see what your degree Minh sand
disagreements were at different
thresholds you actually have to change a
threshold and create a new confusion
matrix model checker is at
threshold independent meaning you can
see the accuracy you can see the
agreements and disagreements at all
thresholds simultaneously and and you
can see what the threshold is so we have
a vertical line to show you what the
current threshold it is but that really
just serves that as a visual aid to the
user because you can see that if you
know I if I increase my threshold I have
fewer false positives because there are
fewer red boxes to the right of the line
but it's clearly at a cost of my false
negatives because I have a lot more
green boxes that are to the left
similarly we can also see precision and
recall right precision is just on the
proportion of red boxes or green boxes
that are to the right of this line right
so in this case you know I'm doing
pretty well in precision because most of
the boxes that are above the threshold
are green there's a few red boxes but
I'm actually doing pretty bad on recall
because recall is the number of green
boxes that are to the right so there's a
lot of green boxes that my system is
unable to see that are to the left of
the line so i can see that if I you know
increase if I if I decrease my threshold
I would be increasing my recall but
clearly at a cost of precision because
now also more red boxes are to the right
so we're showing you the same
information you'd see in all the summary
statistics and graphs while allowing you
direct access to your data okay so
there's a lot more that we can do with
model checker there's a lot of like
annotations that you might be seeing
here because we're showing things at the
item level but I won't be able to talk
to you about that today there's a lot of
things you can do but to help people
sort of debug the problem their their
individual errors they by using a
display like this okay so let's get back
to the talk ok so that was mild tracker
we did a couple evaluations with it to
see sort of how people used it and
whether or not you know what helped them
first we did a six-month field
deployment where we had real users so by
real users I mean people sort of outside
of our group who are using ice with
model tracker for real for building real
models so for both research and product
deployments and what we found was that
model try people use model checker so
using 35 out of their 40 sessions and
more importantly they use it through
when they were using it they use it
through
model-building so this chart shows you
how often people interacted with model
tracker throughout building a model and
so they're interacting they're clicking
on these boxes they're looking at their
data throughout model building with a
slight increase in interaction towards
the end we also did a controlled
experiment where we sort of looked at
when people were looking at their data
were they still able to sort of debug
airs as they would if they were just
able to sort of sort and filter they're
the they're already labeled data and
what we found was that people preferred
model tracker referred preferred
interacting with it without a loss in
debugging ability okay so this is really
what we want we want people to look at
their data more regularly and when they
look at their data we're still want them
to be able to debug and gain insights
from from what's happening so that they
can take a more informed approach to
model building okay so these are you
know just a couple of examples of where
advances in interfaces and tools can you
know really impact our ability to
interact with and use machine learning
algorithms but there's a lot more
opportunities in the space you know two
other examples of things that our group
are working on is on the data collection
side you know how can we help people
sort of effectively explore large data
sets so that they can find a variety of
data to teach their system about so it
can generalize better you know featuring
is another thing that we've been looking
at is like how do we help people come up
with these new features that might help
them build their models there are a lot
of open problems in this in how we can
sort of help people being Iraq better
interact with machine learning
algorithms and you know you know I
really only focus sort of on the
interface side of this problem but
really what you know advances in
interfaces on the interface I can really
go hand in hand with advances in
algorithms right the insights that we
gain from understanding users can really
impact machine learning algorithms and
vice versa so I think there's you know a
lot of opportunities for collaboration
and for a user machine learning
partnership so thanks at this point I
can take questions
what kind of stuff can you do with model
what kind of stuff can you do with model
tracker if you have saved more than 500
data points like you know a few million
for a lot of these image sets yeah so so
we built models hacker for I swear it so
it's an interactive machine learning
platform so people usually collect about
500 to 1000 labels we also use it for
energy extraction I switches can amount
to about 10 times as many labels because
you have it at the token level and for
that we use subsampling so it's only to
show you a sample of the data we're also
sort of exploring on some grouping so
instead of showing individual examples
we can show sort of group of examples so
that you can scale to larger data sets
more easily so is the software available
we're hoping to be available somewhat
sometime soon yes hi I was curious how
you deal with bias in the label
recommender and so when we help to to
how people group them right similarly
yeah so it was interesting we found that
people people felt that the grouping
recommendations turned their labeling
task into a verification task so instead
of deciding how to label they would just
verify whether the recommendation was
correct or not I mean oftentimes they
actually just went with the with the
recommendation as opposed to sort of
deciding on their own so so it may be
causing some of that and we haven't
actually looked at like how that
impacted the performance in any way for
the structure learning you showed
results for label inconsistency do you
have accuracy results like how helpful
is that to be more consistent ya know so
we we we measured consistency as so we
had we looked at the pairs of items that
people to put together and whether or
not those items should have been
together based on some ground truth data
we didn't actually build models with
them because
the models you would need we would need
features along with it and the features
you would create for the different
subgroups might be very different and we
would want to have the users be involved
in what features they were going to use
so we didn't actually build the models
but we looked at just the consistency of
the labels that they're providing to the
system cool and sorry just not a
question for this last visualization how
do you do if you had like multi-class
classification yeah and this is a good
question and we actually use it also for
multi-class and for any attraction which
can be thought of hierarchical multi
class and for that we turn multi-class
into like a binary classification so one
classes versus all the rest it's not
necessarily ideal I think there are ways
that we can improve it so we can show
all of the classes at once and we're
looking into that but right now we've
been using it for by turning it into
binary and it seems to be people are
still seem to be using it like Howard
Johnson okay so my name is Tyler and
today i'm talking about bullets which is
a very fast in principle to algorithm
for solving convex optimization problems
involving sparsity this is joint work
with my advisor Carlos custard ok so
here is a convex objective involving
sparsity it's very famous on the lasso
problem we have squared loss in this l1
regularization term we know a lot about
this problem for one the solution is
sparse so the solution has many values
that are exactly 0 we also know how to
solve this problem using methods like
coordinate descent in gradient descent
and there have been a lot of papers in
recent years improving this method using
words like stochastic parallel proximal
accelerated in dual we understand these
methods very well at this point we
always about those have a problem and
that problem is that they treat all the
features equally in the problem and
we're doing feature selection so the
features are inherently not equal all
the features with value zero
are in fact irrelevant to a solution so
what the fastest solvers do is they use
something called working sat methods the
idea is very simple you first choose a
small set subset of variables that you
believe are likely to be relevant then
you optimize only over these chosen
variables and ignore all the other
features and then once you converge on
the subproblem you repeat the process
until you converge globally so this
works very well in practice and the
fastest libraries use this very
successfully but there are a lot of
questions to be answered about this
problem which variables do we choose how
do we know we're selecting a good set of
variables how many do we choose and how
long do we converge for each subproblem
these are all questions that are not
answered adequately by a current theory
so what we've done is we propose blitz
which is an algorithm that does have
such theoretical guarantees it makes
progress in all of these areas so we we
selected through theoretically justified
subproblem we can apply our theory to
optimize the convergence criteria for
the sub problem and how large it should
be we also have a method for
progressively discarding variables as
the algorithm runs so it's kind of like
pre-screening but you can do it as you
progress in convergence and most
importantly the tile rhythm converges
extremely fast in practice we have a lot
of results from a single machine setting
also out of cord learning and also in
the distributed setting and we hit or be
state-of-the-art and all these settings
so if you're interested in this please
see me at my poster about blitz late in
the day thanks
so supervised learning has been one of
AI and machine learning is greatest
success stories so it's really not
surprising that everyone's and trying to
use crowdsourcing to label their
training data in order to train really
powerful classifiers now the problem
with crowdsourcing is that the workers
are noisy so what people generally do is
they ask multiple workers to label each
example and then use some sort of label
aggregation function to figure out what
the true label is given the multiple
noisy ones this label aggregation
function can be something like majority
vote or something like some some work
that the machine learning community has
output in the last couple of years but
all this leaves one crucial question
unanswered and that question is how do
we best direct labeling effort in order
to maximize the accuracy of the
classifier not the accuracy of the
training set and to that end I want to
talk to you a little bit about our
efforts in what we're calling reactive
learning which is a generalization of
active learning to the setting of label
noise and so so what is active learning
well active learning suppose you're
trying to train a classifier as
distinguishing between birds and humans
and suppose you've trained this
classifier using three pictures of birds
and two pictures of humans in tuxedos
and the question active learning asks is
what is the next next example I should
label in order to best improve the
classifier and the answer to that might
be well it might be the most
disambiguating example which might be a
picture of a bird in a tuxedo an active
learning would pick this example but
what happens if your training set has
mislabeled examples what have you
labeled this bird as a human well now
now you have to make a trade-off that
didn't exist before and that trade-off
is should I real able this example in my
training set or should I get a new label
for an example that the classifier has
never seen before in other words should
I denoise my existing training set or
should I expand my training set with new
examples and that's exactly reactive
learning reactive learning is trying to
understand how do we balance the
training between more
data and less better data now to further
understand how relabeling affects
learning we did a little experiment we
looked at 12 data sets from the UCI
machine learning repository and we
simulated workers that are fifty five
percent accurate that is these workers
are very slightly better than random and
we compared a bunch of different
strategies for training these
classifiers so we compared you labeling
that is labeling every single example
once and we tried relabeling strategies
so for instance taking a third of the
examples only-- labeling each of them
three times we're taking a fifth of the
examples and labeling each of them five
times and what we found was like you're
labeling was better in over half the
data set and this was extremely
surprising to us right because people
don't do this people don't you know
label especially when the workers are
fifty five percent accurate and so we
wanted to learn a little bit more and
after thinking really hard we found sort
of three characteristics of learning
problems that would suggest that
strategies with more relabeling might be
better and these three characteristics
are classifiers with weak inductive bias
moderately accurate workers and small
budgets now I won't have time to talk to
you about any of these but I would like
to give you at least a little bit of
intuition about why classifiers with
weak inductive bias would suggest
strategies with relabeling would do
better so suppose that you're trying to
learn this concept that everyone 65 and
older is a senior citizen and now
suppose you throw in some noise now
suppose that you're learning is strong
inductive bias classifier that is one
with low express sniffs low
expressiveness so for instance a linear
classifier like logistic regression well
you see basically that it's going to get
the decision boundary pretty much
correct no it doesn't affect it because
the noise will get averaged out but
suppose you're trying to learn a weak
inductive bias classifier one with high
expressiveness so for instance a
decision tree you'll base you see that
it will basically over fit the noise and
this suggests that you have to go and
real able examples in your training set
so that your classifier your weak
inductive bias classifier doesn't over
fit the noise and so we have that weaker
inductive bias increases relabeling
power so if you'd like to learn more
about what we're doing with reactive
learning please come see us at our
poster thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>