<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Decision Jungles | Coder Coacher - Coaching Coders</title><meta content="Decision Jungles - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Decision Jungles</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q7BwufUxteE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good afternoon hope you enjoyed
your lunch I'm Jamie chaton I'm going to
be talking today about decision jungles
and this is joint work with Toby push
meets last year in John and Antonio and
we presented this at nips last year so
decision trees and forests have been a
great tool for us we've used them for
all sorts of different applications
connect body tracking interest point
detection and images organ recognition
segmentation even camera relocalization
and why have they been useful well
because you can you know you they're
sort of general very general purpose
classifier classification and regression
tool and they can be implemented
extremely quickly especially at test
time because you have this sort of
conditional execution you you execute
what you one feature according to what
happens there you execute another etc
and that leads to a very efficient
runtime however there's a fairly large
fundamental problem with decision trees
and and forest which is that if you give
them enough data and grow them deep
enough you know they'll get very very
large they grow exponentially with depth
doubling each time you train an extra
level in your in your tree structure and
this is actually a real problem because
we're now in an age where we have large
data and for some applications we have
infinitely large data we can for example
with Kinect we rendered synthetically as
many images as we wanted as training
data and so memory concerns really are a
practical limitation for the adoption of
this kind of technique in in practical
applications especially perhaps mobile
and embedded devices and so what we were
really focusing in on this work is how
do we avoid this exponential growth with
depth what what can we do to avoid that
and the solution we hit on was
the use of decision tags and a jungle as
i'm calling it here is really just a fun
name for an ensemble of such decision
tags now a decision dag really is
exactly the same as the decision tree
except that each node can have apart
from the root node can have multiple
parents potentially have multiple
parents and what we're going to do in
this work is investigate a couple of
ways of present just one of them today
in the interest of time but a couple of
ways of trying to learn such structures
in a way that we can jointly optimize
both the the features that the dag is
learning and also the structure the way
the the layers interconnect and what we
found is not only did these dag
structures help us with our original
problem which was thinking about the
memory consumption but actually on the
experiments I'll show you today they
seemed to help perhaps improve
generalization and reduce overfitting so
there are a couple of reasons why that
might be one is that you because you're
because the way we're limiting the size
of our structure it means that the the
data doesn't get sort of recursively
diluted as you go further down the trees
you have more training data essentially
for each node here's another possible
sort of intuition of what might be going
on and this is a very simplistic example
and it's certainly not meant to be you
know the be-all and end-all of what's
going on inside these things but it
gives you some inkling of what might be
happening so here's a very simplistic
feature a two dimensional feature space
we've got these three classes here and
it's going to be a sort of image patch
classification task for for the sense
for this example and so we've got these
graphs down here and down here this
happens to be a sort of washed out in
ensure that the grasses has come out
quite white we've got sheep that looks a
little bit like look grass and we got
up here so we can imagine having some
more examples of these classes and data
might live you know roughly in that
space a bit like that now if we're
trying to learn a tree well what you
might find that it does is it might
split first vertically like that and
then on each side it will split these
these guys like that and then as we keep
going down it will keep subdividing and
start over fitting as trees I want to do
and so you'll end up with this region of
feature space being classified as I
think this was cow and and this one here
being classified as grass whereas we
might maybe have wanted it to be the the
grass class down here what we might do
instead with a dag what might happen
again this is a very simplistic example
I realize but just to give you some
intuition we might start with the same
kind of initial split we then
recursively split the the next level
down and this is exactly as it was
before but now what we can do is we can
think about merging these two disparate
regions of feature space into one node 1
into 1 region and then if we continue
training from there then we are we have
the ability to do slightly different
things such as have a split across this
way and so you know it opens up the
possibilities that we can represent in
our in our data structure and so what
this might look like in a sort of graph
diagram is that you know we have this
initial root node where we did this
split and then at the subsequent level
we're able to merge these two nodes
together and then we'll whatever we'll
keep training down here and and do
whatever we want to do so the the
generalization of the tree structure two
graphs allows us to get both the
tradition
and ending of the feature test as you go
down but also the whoring between
different feature different paths in the
tree in the graph now of course we're
not the only people who have been
looking at decision graphs and decision
DAGs and here's a selection of some of
the related work that we found I won't
sort of talk through that in any great
detail I think where we differ from
these pieces of work is that where we're
really able to jointly learn the
structure and the features at least
within an individual level at a time and
our experiments showed that that was
actually quite an important thing to be
able to do fine so let's have a look at
what we actually do so how do we train
these things so let's introduce a bit of
notation we're going to have we're going
to train this thing a layer at a time
and within this layer here we're going
to have the parent nodes up here the set
NP the child knows the set NC and we're
going to index the parents with I and
their children with Jay and for each
parent node we can have a set of
parameters theta I that specify what
function you're going to evaluate of
your input feature vector and according
to that parameter theta I that will
induce a split into left and right
subsets of your of your training data s
so it will induce a split saying that
this we get this set of examples going
left in this set of examples going right
and we're also going to use this
notation at you know Li and RI to mean
what what are the allocations of the
parents left and right children in the
inner sub in the lower layer and so all
of this allows us to then define what is
actually quite simple which is to say
what is the set of examples SJ that
reaches a child
no Jay and it's simply of course the
union of all of the branches out from
all parents that were allocated to point
to that node so this just gives us some
some notation to say first of all we can
we can work out given a the response of
a feature and comparison and a
binarization of this into left and right
we can decide which examples went left
which went right and then at the child
layer how they merge together and what
that allows us to do then is to define
an energy function over jointly over all
of the parameters theta at the parent
layer and all of the left and right
branching parameters L&amp;amp;R and to keep
this somewhat consistent with the
standard fairly standard information
gain criterion that we've used in the
past for training trees it turns out
that you this is essentially just a
generalization of of that to the case
where you have you have this graph
structure up in the tree structure and
that that in turn allows us to you know
and now so now we have this energy
function and we're able to think about
optimizing it in general are we going to
allow for a sort of paranoids because
there seems like that would be quite
important yeah it is now can free
parameter exactly so so far what we have
done is is really investigated one basic
structure which is what we call calling
a decision house so it has a skyscraper
so it has it goes out with you know
exponential growth to a certain limit
and then go straight down so we're
specifying a fixed width on each fixed
maximum width essentially layer below
any of the children can be children of
any of the pair
there's no restrictions on that you
could have both children from apparent
going to the same node in fact that's
one of the initialization strategies and
in Curt you doesn't have to be this
skyscraper structure it could you know
you could even imagine merging back down
you know if you could learn this thing
would the perfect thing would be to
learn back down to the number of classes
and have a perfect classification for
each so how do we optimize this and here
I have to you know confess that this is
incredibly simplistic and I'm sure there
are much better ways of doing this so if
you have any ideas do do let me know so
right now as I say we're fixing a
particular budget of nodes for the child
layer and this sort of allows us to if
we know what our memory budget is and we
know roughly how many layers we're going
to need we can sort of define what what
this M parameter should be we
investigated a couple of very simple
optimization strategies one which I'll
describe in a second and the other based
on a clustering algorithm using Bregman
divergences and I refer you to the paper
to look at that in a bit more detail but
they both worked about the same so local
search how does that work well first of
all we start with some feasible
initialization and there are various
ways of doing that one that seemed to
work quite well is you essentially
allocate both child branches from each
parent to the same child and you do that
as as much as you can and then if you
run out of budget then you start
allocating where you least increase the
energy and there are there are other
strategies you can try and then you just
simply iteratively make a locally
optimal change update to to the
allocation to the features or to the
left or right allocations of the
children oh so you're just making it's
just the stochastic search algorithm and
you could imagine a kneeling this or all
sorts of things as well but even even
this was seeming to give us pretty
decent results
so one of the beauties of this is when
it comes when that that's it for
learning that that's really all there
all there is to it when it comes to test
time you you just don't have to train
change anything if you've got decision
tree code lying around that it's highly
optimized for you know GPUs or whatever
you can just reuse that assuming that it
allows you to have pointers rather than
just assuming that you know a particular
allocation of nodes of the the child
layer another nice thing which we
haven't probably explored yet and you
know this may or may not have a
practical benefit is that because you've
got much smaller insan balls they might
well fit in your cash better than a
large decision tree would have done and
so this indecision forest would have
done and so that may result in in speed
increases okay so let's have a look at
some experiments we did so we've got a
few baselines obviously a sort of
standard vanilla implementation of
decision forests then we investigated a
couple of variants of fixed width trees
so the first of these is that rather
than just growing every node as you
would in a normal decision tree you you
rank them by their their energy their
entropy and you optimize out you only
continue growing the ones that the
taught the em over to most energetic
ones which means that you end up with
the same size structures we have with
the dags that we're growing the other
one is to sort of rank them and see
which one's actually reduce the
objective and do that so there's a sort
of variance of the same theme and then
we also investigated priority schedule
trees where we you know you do the
standard every node gets the opportunity
to grow but you grow them most energetic
node in order and then we can sue so we
can rank all possible trees that you
could get at all stages of that growth
against the accuracy and you'll see that
in a second I just want to be
sure I understand the most energetic
intuition so a node is going to have a
high energy value if it has a lot of
data points associated with it and if
the classes don't agree yes in that node
agree if they all agree then the entropy
is zero exactly and so the energy
exactly lots of you know unhappy yes if
you're unhappy you will get you will get
split first in one of those strategies
okay so here's some first results on
three different data sets here a B and C
let's start by I guess looking at the
Kinect data set so this was a Kinect
body part classification task we're
showing the test segmentation accuracy
as a function of the size of the the
number of nodes in the ensemble which is
a sort of proxy for memory consumption
and so few things to note first off the
decision trees the blue curve here does
a reasonable job but it starts to
overfit at around whatever this is here
fifty thousand or so interestingly those
bass lines that we investigated actually
do quite a lot better especially the
priority scheduled one that's quite we
found that quite intriguing and maybe
worth exploring in a bit more detail but
we were able to you know get
considerably better performance up here
now it was it was not surprising that we
could get the the horizontal gap here
that we could save memory what was a bit
surprising is that we we got this extra
generalization out there and I'm not you
know I don't want to claim that we fully
understand why that is I've given you a
couple of Inklings of why that might be
happening but I think there's plenty of
scope for more investigation there
similar effects perhaps even more
pronounced on these other datasets here
I mean if you're wondering why there's
this kink in the curve that's when the
merging starts kicking in when you
get to a certain certain width now that
was memory consumption what about
compute time yes a question pieces so
the features in this data set I think
all three of them are the same kind that
we used for the kinect body part
classification which are essentially
Atta pics you're trying to classify
pixels and at a pixel you're allowed to
look at any offset neighboring pixels
relat defined relative to that pixel and
sort of take differences of their
intensities no no it's not just
immediate neighbors it's it's the
feature space is very very large
hundreds of thousands of dimensions it's
never actually explicitly computed so
that saving and memory and potential
extra generalization does come at some
cost so now I'm plotting the same y axis
but at an x axis which is saying how
much computer you doing how many of
these features are you actually
evaluating and so it's not you know that
that improvement is not coming
completely for free we are let's focus
on this graph for example we are having
to do more work quite a lot more work to
get that generalization now my hope is
and again we haven't fully investigated
this my hope is that that might be
offset a little bit by the fact that
this is smaller and will fit in cache
and and thereby might be end up being
you know similarly efficient just a you
know a very quick investigation we did
here on the the size the width of these
these tags it looks like you know
clearly this is going to depend on your
data set in your particular scenario but
there is there is some some effect going
on there that you would need to it you
would need to think about some image
results so this is the this is maybe I
should have shown this first this is the
kind of tasks that we're dealing with
here we're trying to classify from depth
pixels into these body parts here we're
trying to classify RGB face images into
these
kind of face part images here and all of
this is to show I mean these aren't the
best possible results you could you
could get we could have thrown tons more
data at it etc but what was slightly
interesting is that you start to get
these somewhat less noisy results out
here and out here possibly again because
it might just be the better
generalization means that that's what
you get it might be something something
slightly more interesting going on I'm
not sure some results on you see I
generally we didn't see as big
improvements on you see I but that on
some cases there was we didn't see ever
it get worse than a standard decision
tree that's what you might get if you
visualize one of these structures this
is I think depth 60 and we should have
color-coded it with the different
classes so you can sort of see some
structure coming out there fine I better
wrap up so in this piece of work we
looked at generalizing decision trees to
decision dyak dags and jointly
optimizing each layer in the dag
structure over both the structure and
the the features that are extracted from
the feature vector and it seems that not
only can decision jungles help you with
memory usage but also potentially get
you better generalization so thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>