<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bedrock: A Software Development Ecosystem Inside a Proof Assistant | Coder Coacher - Coaching Coders</title><meta content="Bedrock: A Software Development Ecosystem Inside a Proof Assistant - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bedrock: A Software Development Ecosystem Inside a Proof Assistant</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BSyrp-iYBMo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so it's a pleasure to welcome Adam
Tripoli from MIT I think many of you
know Adam and his work on using cock to
prove various things and we had the
pleasure of having him his into his
student Jason intern with us in the
summer so oh yes I'm sorry yes of course
as well right pane yeah
peri so Adams going to talk about his
bed lock system which is a framework for
doing everything so it seems so Thank
You Adam thank you so I'm going to be
talking about bedrock which is a system
we've been building inside the the cock
proof assistant and I want to start out
by setting the stage for the general
kind of problem that bedrock solves so
in theory all the things we do with
computers we could do by building a new
piece of hardware for every program but
that's no fun at all so of course we'd
rather be using some sort of universal
computer we'd like turing-complete
systems but we're not done yet because
Turing machines have this problem that
it's hard to achieve very good
performance running things on Turing
machines so we rely on modern hardware
platforms that have assembly languages
with efficient implementations and this
also is not good enough because if we
all have to write all of our programs
directly in assembly we would never have
nearly as interesting programs as we do
today so we really depend on ecosystems
of software development tools that
provide a higher level of abstraction
now in particular what those ecosystems
provide is a number of different good
things we might put in a few different
categories potentially with some overlap
I'm not quite sure of the boundaries in
particular between the first two here
abstraction and modularity and in the
abstraction category we have things like
structured programming and abstract data
types there's sort of ways of thinking
about programs at a higher level of
abstractness and and for instance
viewing a complex implementation through
a simple interface and we also care
about modularity or ways of helping us
break a big problem into smaller
problems and get some guarantees about
the way in which the pieces will come
back together again and examples of this
include safe type systems and garbage
collection where you don't have to think
about memory management and also another
important pillar of programming today is
automation where for instance we rely on
compilers to translate from some of
these high-level abstractions to through
lower levels and there are a number of
other things that you might also call
compilers but are also referred to by
other names like linkers and code
generators so we need all of these in
our ecosystem to be able to build the
things you build today
yeah I'd say it's sort of in our thuggin
ille de mention but all of these these
elements or many of them are in support
of performance and we wouldn't need them
if we didn't care about performance all
right so so concretely how does one of
these tool ecosystems look well at the
bottom we have a machine language which
is what the hardware knows how to run we
might write our programs in some
mid-level language I'm sort of thinking
of that as like a C style of language
and we have a compiler that knows how to
lower the mid-level language to assembly
and an assembler that knows how to turn
that into an object file and the an
important thing here is that there's a
standard format of object files that
many different tools can produce for us
and we might have a whole other language
with a similar pipeline that leads to
object files in the same format and then
a linker that knows how to combine these
together into one machine language
program we might also prefer to write in
a higher-level language and have a
compiler that plugs into this pipeline
perhaps through one of these mid level
languages and we might link in a runtime
system written in that same language and
we might even do things like use a
parser generator and have a declarative
description of a program and a code
generator that translates it into some
other part that we have implemented in
this pipeline so we so all these pieces
are essential to the kinds of software
that people are building today we might
also consider having a common type
system throughout this this this picture
for instance
and the the the COR system with a
language of types that applies to
bytecode in the end but has consequences
that are pushed back through the rest of
this diagram and now we we have sort of
a fixed abstraction and modularity
discipline that all the levels of this
picture need to respect and that's a
benefit for understanding libraries and
so forth so the first picture let's call
that l0 for bear assembly language and
then we can add conventional types into
that picture and arrive at l1 and of
course it's interesting asked as n
approaches infinity what is the limit of
this sequence of languages where we're
adding more and more expressive
abstraction and modularity features into
the core of everything obviously the
limit is distributing assembly programs
with proofs of functional correctness so
if nothing else that seems like an
intellectually interesting thing to
study so so let's see how this might
work pragmatically so why is this worth
doing well one reason is that it's good
to know that our programs are correct so
obviously it's a good thing to have
rigorous proofs of software correctness
and therefore we'd like to have the
tools that help us build those proofs
more easily when we've constructed
programs and the way that we're used to
building them there's another reason
that I find may be even more exciting
than that one which is that maybe we can
use formal methods throughout the
programming process and enable new kinds
of abstraction and modularity that we
wouldn't have been able to get to work
otherwise because some programming tasks
might just be too hard for people to
understand when they have relatively
weak notions of interfaces and
abstractions so hopefully by applying
formal methods we can actually lower the
cost to build the program not just say
this team's going to build it and then
this team is going to prove it correct
as a bonus let's try to make those two
processes into one and here's sort of a
restatement of of that hypothesis that
by by using formal methods we can come
up with superior interface formalisms
that help people understand complex
tasks in new ways alright so in
particular I want to do this inside of
the call
proof assistant where we have an
assembly language at the core of things
and probably most of you are familiar
with with the work that Benson and
Kennedy have been doing here about sort
of what happens below the assembly
language point in this picture and also
some some parts above but let me focus
on describing the the things that
they're doing that we're not doing in
the bedrock context they're dealing with
semantics and verification from machine
language
working with with popular instruction
set architectures in detail and
verifying some of the lower level tools
like assemblers and linkers
so these are all important things to do
and they're also complementary benefits
at the top of the picture what we're
working on in our system which is called
bedrock it's also a library inside the
proof assistant
it supports modular verification of code
libraries we've done some work on
extensible low-level languages with
verification support we've done verified
compilers for sort of sea level
languages for domain-specific languages
and also we've done some work on
deriving programs from their
specifications so we're sort of trying
to fill in that the higher levels of
that that picture of a software
development ecosystem but like in the
work going on here bedrock is for
programming and verifying and compiling
all inside of the Cauca proof assistant
you don't have to step outside of that
world until you actually have your final
program and you'd like to actually run
it on real hardware then we have to
export to the outside world but
everything else goes on inside of the
proof assistant oh yes compiler it's
plural multiple compilers inside of
college ok writing and actually running
them inside of coq not extracting tuvo
Camel first yeah language to be sort of
a class of things or one particular one
or sort of we have a particular one I
also think of it as a class of things I
mean what does it take to be an assembly
language I mean does that matter or do
you just sort of take Intel some fudge
it a bit or take our
but should've been tour I mean how does
the silver language different from see
if it has local variables it's not an
assembly language that's one necessary
but not sufficient condition and I could
try to go into more detail but I haven't
thought of a concise answer ahead of
time so maybe I'll put on the back okay
so we had these three pillars of
effective software development
ecosystems and all of these are going to
come into play in in bedrock we care
about abstraction and this takes forms
like abstract data types with semantics
with with specified interfaces of how
the data type is supposed to behave the
general idea of specifications in higher
order logic to provide interfaces to
components and many other parts on the
modularity column we have program logics
and semantics linking where we want to
verify libraries separately and then
combine both the code and the proofs to
provide a theorem about a compiled a
whole program and in the automation
category now we can have not just
automation of compilation and
programming but also of formal proof
generation and we apply this automation
both at the very lowest level to proving
pointer based programs correct and also
at a higher level to derive programs
from specifications and I'll say more
about each of these pieces as I go but
really I'm going to I'm going to skip a
lot of elements in this talk that are
sort of well known in the community at
this point but weren't at all well-known
say 15 years ago basically the the
challenges have how you verify programs
that that deal with mutable recursive
higher order data structures using and
predicate of quantification over
specifications and do it all in a
modular way that supports separate
verification of libraries and we
actually want to implement this in Cocke
so we'll need nice notations for
programs and specifications and we'll
need good ways to automate the proofs
about these programs each of these
points is non-trivial but I want to
focus on some other parts of the the
picture for this talk we have stories
for all of these in particular just a
quick example of a verification that
we've done using bedrock
this is diagram of the modular
decomposition
addition of a cooperative
multi-threading library and each of
these dashed borders is an encapsulation
boundary where there's some private
State inside the border and code outside
the border can only get at that state by
calling the the exported methods of that
module and each of these methods has a
specification so in particular at the
bottom here you can see maybe the most
interesting part of this diagram that is
a four levels deep nested encapsulated
thing the outermost one is called the
scheduler which is the main module that
controls waiting for IO events and
waking threads up to respond to them and
letting thread suspend themselves again
and it has a set of file records
describing the open can the open network
connections that this library knows
about each network connection has
associated with it a queue of threads
waiting for input on that connection and
a queue of threads waiting for output
and those to say the in queue it out
queue arrows point down to this next
encapsulated component which is the
thread queues component that is the idea
of a set of cues that support in NQ and
DQ and each thread each of these queues
is itself using a separate thread queue
object which is implemented in terms of
the traditional queue data structure and
and the the queues here are storing the
program counters and stack pointers of
the suspended threads so there these are
recursive mutable higher-order data
structures and each of these levels
exposes just the right kind of
abstraction for its specification so in
particular code outside this library
sees the say received from socket
function it's just a regular function
with a specification that hides the fact
that the thread might be suspended into
a data structure instead of just having
the function returned normally and this
is all implemented specified verified
and compiled to assembly inside of caulk
and what we use this library for is a
case study of running code on autonomous
vehicles where you have a variety of
interesting things plugged into a little
robe
like that one there's a Joyce joystick
potentially sending messages remotely
and somehow those messages should
influence what the wheels are going to
do and so we've we've built this the
server that drops into a popular
robotics platform called our OS robotics
operating system and this is a server
that's basically like a name service
that helps the different components find
each other's network addresses and so
the different physical things connect to
the the processor and we rely on a set
of non-blocking system calls for network
access but we're not verifying well
we'll assume those are given and we'll
assume specifications for them then the
thread library is built on top of that
this library provides blocking
abstractions which are more convenient
to work with it also relies on a memory
management library and a number of
threads are running handling requests
from the different nodes of the system
they're connected to an in-memory
relational database which will store
information like the address of the node
that knows how to handle turn wheel
requests is 1.2.3.4 or something like
that
and so the the joystick can ask the
server where do I find the wheels and
the server says here's the address and
the server can connect to the wheels and
start sending them commands so
everything except of the non-blocking
Network syscalls Box is implemented and
verified in the bedrock system okay so
let me say a bit more about exactly what
we're assuming and what we're providing
what the the theorems that we come up
with are so we're running on top of the
clock proof assistant I've put the caulk
logo in the trusted side of this diagram
to indicate as usual we'll assume that
caulk is giving us correct answers we
ask it does this proof really establish
this theorem on the untrusted side we
have a kind of semantic module system
for assembly language where we have
packages of code specifications and
proofs that the code meets the
specification and in general we will
have many of those in one program and we
need to be able to compose them together
and we do that using a module system
which has a soundness proof in caulk so
you don't need to believe that in the
soundness of that system and what that
will allow us to do is take a set of
modules compose them together let let
one module satisfy the dependencies of
another until we get to a closed
assembly program that is no longer
importing any more code labels from
others and a proof that this module
meets its specification it is it is
pretty hard yeah so in this case this
piece in a way write the specifications
of composed modules are computed from
the specifications of the inputs so you
don't have to write it over again but
you might want to inspect the output to
make sure it was computed in the way you
expected specification of the closed
assembly program well the way it works
is that each function has a
specification so when you link two sets
of functions you get a larger set where
each function has the spec from
whichever input provided that function
there's no new specification that is
sort of an emergent phenomenon of
linking it's just the union of the
original the function specifications you
started with closed assembly program
plus poof appears on the slide what is
it approve all it's a proof that every
function satisfies its specification in
terms of a precondition and post
condition D we're not saying anything
about interaction with fiscal world
we're only for the moment modeling this
internal state of the machine he's
nothing but hey this is not super hey
but whether you put it all together you
get anything useful is no part of your
code it's an interesting question I
guess I've started it to me I feel like
the specification style that we're using
tells you something about the program
overall because a bug in one function
could easily break the state that
another function depends on and you have
to prove that that interaction doesn't
happen in a bad way
checking okay it's it's I feel like that
might suggest a weaker notion that we're
actually implementing we are proving
functional correctness it just looks a
little different maybe then people are
used to seeing because we're at the
assembly level and there aren't as many
abstractions built in yes there can be
arbitrary recursive dependencies across
modules every module has a list of
imports of functions and specifications
from others all right so once we have
all this stuff in the untrusted part of
the picture we can throw it over the
wall into the trusted part where we have
a formalization of the syntax and
semantics of an assembly language and
I'll tell you which one in a moment and
we have to choose some safety property
that the final program should satisfy
this is actually done in a generic way
based on the specifications of the
functions basically think of it as every
functions precondition is always
satisfied when you call that function so
ask caulk do you believe all these
together do you believe that this
program has this property according to
this semantics and that's the basic
operation of checking a program and then
we also have a traditional compilation
tool chain everything that happens after
producing assembly language we assume is
given to us and for the moment we're not
proving anything about it so here's our
assembly language here's a one example
of what I apparently mean when I say
assembly language it's it's not too
interesting but I think it if you take
the time to read it it won't be
surprising there's a we're working with
with 32-bit vectors and we have a small
finite set of registers and addressing
modes and operations and at the bottom
we have a code module is a list of
blocks of code each of them each of
which has a label it's a basic block so
it as as a set of straight line
instructions followed by a jump
instruction jumps include indirect jumps
via expressions and also conditional
direct jumps so it's a one of the many
small turing-complete languages you
could come up with if you wanted to
actually be able to compile it easily to
any
the common assembly languages today and
run things with reasonable performance
which we do in this case translating to
32-bit and 64-bit x86 and arm and maybe
will be inspired to add some more in the
future
for now no proofs about those
translations but this this I think this
really is an assembly language it's not
like an LLVM IR kind of language as a
very direct kind of macro expansion
translation to all the the popular
assembly languages so then this the
semantic module system that we put on
top is it's called X cap or it's a
variation in X cap which is a system
suggested by nee and Shou in 2006 and so
the notion in this framework of what is
a whole program it's just a set of basic
blocks every basic block has a set of
straight-line instructions ending in a
jump and it also has a precondition
which is a specification in higher order
logic of what the memory and registers
should look like every time we reach
this location so a program is a set of
such things and the correctness
condition is that when we run the
program every time we reach a basic
block the precondition of that block
actually holds so by picking rich enough
preconditions you can encode all kinds
of functional correctness properties or
you can choose to encode type safety
kinds of things it's it's up to you and
then this notion of modules is that a
module is a set of code blocks each of
which has a specification or a
precondition and the the proof
obligation of a module is that if ours
if our some assumptions that we make
about certain code labels existing in
other modules with specifications that
we give if these assumptions hold then
in any whole program that includes this
module sort of we will never be
responsible for breaking a precondition
all of our preconditions will always be
true as long as no one jumps to us
without satisfying the appropriate
precondition of the block that has
jumped to and finally there's a semantic
linking operation that takes two modules
combines them into a bigger module that
includes all the code labels of the
constituents and also connects together
the the proofs in an appropriate way
and we'll also remove imports
that were present on one side that have
now been satisfied by the exported
labels of the other side so by repeating
this operation we can converge towards a
closed program with a a proof that all
of its basically all of its assertions
will will always be true whenever
they're reached right so somebody
language is really continuation passing
style and there's no such thing as a
post condition here it's one abstraction
you can build on top if you're in that
kind of mood yes
there's condition reasoning you have
some connection to the pre and post like
logical variables or a relational post
condition how does someone FS itself in
this style of presentation by do by
using higher-order assertions about code
pointers like the return pointer of a
function and there will be quantifiers
whose variables are in scope for both
the precondition proper and the
higher-order assertion about the post
condition and we define notations that
make this look like the normal style but
are compiled to that right it's all
partial correctness it's only safety
properties the the usual usual
restrictions for for most work in this
area there's implicitly some kind of
blindness because you have to pull
through the jump once you enter it right
all the instructions are so simple that
there's no interesting liveness it's you
can write an interpreter and obviously
terminating interpreter for a single
basic block to be sound I mean for total
correctness would require rather more
because there's an implicit in the
induction argument that goes on so if
you were to actually say it look we're
gonna prove liveness then you have to be
more careful right this is what
definitely this framework would need to
change to do live in its properties and
I don't have any insights into exactly
how it would change the relation of the
programs it's not so hard
all right so the so that's the
foundation of bedrock and on top of that
we're going to build up two languages
and programming tools sort of like the
ones we're familiar with but with this
formal methods correctness proof angle
everywhere so as one example of a layer
of abstractions we can build at the
bottom we have the assembly language
that I just introduced we call it the
bedrock il then on top of that we have
the bedrock structured programming
system which lets you basically do macro
assembly programming sort of looks like
C programs then we have on top of that a
language called seto which is another C
like language but with data abstraction
support built-in and with a more
conventional compiler than in the
previous step on top of that we built a
language called posad which is a very
thin wrapper around seto that is getting
a little bit higher level and has a
simpler memory model without any
aliasing in it then we have Galena which
is the normal functional programming
language of caulk it's a pure ml style
language and we're actually going to
compile that into facade and therefore
push it down further towards assembly
and finally we will produce Gallina
programs from Fiat which is a language
or framework for program derivation that
will let us start from relational
specifications in higher order logic and
connect to this framework where we will
take those specifications and translate
them to assembly programs with cock
proofs that the assembly programs
preserve the meanings of the original
specifications and I'll make each of
these layers more concrete as I as I
step through the rest of the talk and as
I do that I want to just keep returning
to one example program its life cycle as
it goes through these different layers
and talk about how we reason about this
program at each layer so the example
that I'm going to follow is let's say
we're calling some function f with two
arguments 42 and 17 here's some assembly
code we might write for that function
call what it's doing is it's saving the
two arguments and to stack slots so the
brackets are for memory dereference via
a computed address based on the stack
pointer register
we'll also take the the address of this
code label ret right after the function
call and let's store that in a return
pointer register and then jump to the
function itself so this would be how we
would encode this in art than the
calling convention that we've we came up
with for for bedrock IO and we would use
a proof rule like this one to justify
just the last of the instruction at the
jump instruction and the proof rule is
saying so first this capital psy is our
assumptions about which blocks of code
exists in the program and recall every
block of code has a precondition SCI is
a finite map that tells us for a code
block address what is its precondition
so let's say we wanted to prove that p
is a valid precondition for jumping to
address f what we have to do is look up
F in the specification map find its
precondition Q and then prove that P the
current precondition implies the
precondition of the function that we're
jumping to and this rule in its full
generality would need to support
indirect jumps where the the target of
the jump is some computed expression the
rule gets a little more complicated but
not terribly so and we would certainly
expect that this function f relies on
the more general rule when it actually
returns by jumping to the value that
we've stashed in the are P register and
the system supports that but I won't go
into the details here to avoid
cluttering up the slides so we can
imagine that we we have some
precondition in mind for this block of
code that would be annotated on the
basic block that we're seeing here so
you think of it as some higher-order
logic function from memory values and
register values to predicates or
propositions though it's a little more
complicated than that in the way that
these things tend to have to be when you
have these this combination of mutable
state and recursive definitions and
higher-order stuff and then the this
label ret is the beginning of a new
basic block and it itself has a new
invariance that describes how memory
should look after we return from this
function call
and so these are the ingredients that we
need to make all this work alright so
let's move up to a higher level of
abstraction we have an extensible c-like
language implemented within what's
called the bedrock structured
programming system and so the the pitch
for this system is based on the general
idea of code generators which we all
know and love in the context of
programming parser generators are a
classic example where you can describe a
language to parse in a nice concise
formal language like context-free
grammars throw it into your parser
generator parser generator does the hard
work of finding some c code that is
going to implement this parser
efficiently your c compiler produces
assembly and the great thing about this
approach is that we are paying some
performance cost for using a higher
level of abstraction but if it's a good
parser generator the performance cost is
only applicable at compile time it's a
it's what we pay while this partial
generator is thinking and producing the
c code for us at runtime we get
hopefully code that is actually close to
hand optimized code in performance
unfortunately the downside is that
conventional implementations of this
parser generator box are hard to get
right because they're they're doing ad
hoc manipulation and generation of code
and the programming language doesn't
provide much help in detecting bugs in
that process so another way of going
about this problem uses a good idea
called embedded domain-specific
languages for instance maybe your parser
generator is not I don't know a perl
script that uses regular expressions to
mind strings and output strings maybe
instead it's a Haskell term of type
grammar arrow C program and now that the
Haskell type system is going to help you
avoid certain mistakes so what we're
basically doing is we're taking
advantage of the type system of the meta
language in this example it would be
Haskell to guarantee properties of the
translator in terms of how it works with
code of the object language which might
be C in this example so what can we get
this type system to do for us well we
can get it to check basic syntactic well
form in this for instance if we
represent code with algebraic data types
we can be sure we actually build
syntactically valid code we can get some
help with checking variable binding
patterns and this would for instance
take advantage of higher-order abstract
syntax or something like that that helps
us get the compiler to check that there
are no out of scope variable references
we can get the compiler even to do type
checking for us using things like GA
tt's in in Haskell where the type system
of the object language is reflected in
the type system of the meta language and
we might even want to try to get some
help from the meta language in checking
functional correctness of these code
transformations if we have a semantics
in mind for the input language we'd like
to show that that semantics is preserved
in the code that comes out so that's
that's what we do in the bedrock system
and the big picture there is so of
course we're running everything inside
of caulk a program in this structured
programming language is a functional
program that uses dependently typed
Combinator's to put together the pieces
of a program think of these Combinator's
as standing for things like if and while
and function call and all of the
features we know and love from c
programming but as libraries not as
built-in features of a language and so
when we run one of these programs it
produces some assembly code and we can
take that assembly code and translate it
to all of the the popular ISAs but
there's another way to run a program
that actually produces the ingredients
for a correctness proof in higher-order
logic and this is things like
verification conditions and the
strongest post condition calculators and
the the goal in setting up this
architecture is to make it possible for
the programmer to use a set of
combinators to build a program while not
actually understanding how those
Combinator's work internally it should
still be possible to verify a program by
dealing with relatively straightforward
proof obligations that hide the
implementation details of a Combinator
so the quick sketch of the formal
details is that the bedrock structured
programming system is based on what we
call macros which are these these code
generating and and formula generating
Combinator's and that what a macro does
is it produces a chunk
which is one of these control flow
graphs inside an encapsulation unit it
has one entry point and designated exit
point and then arbitrary control flow
inside these are all assembly basic
blocks and in the middle and also
conceptually inside this box is say a
verification condition that tells us
what the programmer needs to prove for
this to be a correct program so we might
call that chunk a there might be chunk
be produced by some arbitrary other
sequence of of macro invocations and
then we can give those as inputs to the
if-then-else Combinator which then
builds a new chunk that adds some new
blocks and then drops in the blocks of
the original inputs to this Combinator
and plumbs it all together properly and
we can take the verification conditions
of the inputs and can join those
together add some new stuff and now we
have a new chunk built parametrically in
the details of the two inputs and we
have our generic if then else construct
without baking it into a programming
language and I'll leave out a lot of the
details here including exactly what it
means formally to to connect these
control flow graphs together the details
of another the other important dimension
besides verification conditions which is
predicate transformers corresponding to
strongest post conditions that we don't
actually require that they be strongest
and also there's a formal connection of
all this to hoar logic that we use to
justify the soundness of this whole
system so you don't have to trust it to
believe that your your verification
means what you want but I'll quickly
connect this to the running example that
I had of calling a function f on the
arguments 42 and 17 so now we can
actually write the function call that we
met directly instead of having to expand
it into assembly we don't have to write
any program labels and we still have a
precondition before this this function
call and a some sort of invariant
afterwards that expresses how the state
should look after the function returns
and we have to give rules that explain
how to calculate a post condition and
how to generate a verification condition
for this call the post condition rule is
taking as an input this dictionary that
map's program labels to specific
and also the piece of syntax that we are
verifying and finally the precondition
that we should assume as we go into this
piece of syntax this notation is meant
to suggest we have a function call with
some arguments and we also have an
invariant that must hold afterwards so
the proper post condition of this call
is just exactly the invariant that we
said should hold afterwards and the
verification condition should be some
formula whose truth guarantees that this
is a valid program and well in this case
we have to say there exists some
specification R which is associated with
the function f by this maps I so R is
the specification of F and then we say
for all lowercase R which is a this is a
first-class code pointer when we look up
the specification of r r is the return
pointer that that tells the function
where to jump back to you when it's
finished that specification should be
exactly this invariant that we asked to
hold after the function call so this
used to be the precondition of the basic
block of the the return address now that
that pattern is sort of encapsulated
inside this rule so we we have some
return pointer R whose specification is
the one we asked for and then we have
this finally this implication which
basically says assume we're in some
state satisfying Q which is the
precondition that held before the call
and then sort of symbolically executes
the operation of storing all of the
arguments in the stack and the return
pointer in the return pointer register
this will any state that can be
described by this this sequence of
operations is actually it also satisfies
the precondition of the function f that
we're calling so there's a fair amount
of complexity in these few lines here
but the point is it's a relatively short
formula and actually we can we can
completely automate proving conditions
like this for concrete programs and it
relies on higher order logic and we're
also using separation logic under the
hood here somewhere yeah just balance
this question but
perhaps have extended sign with a
mapping from R to P and then have the
same is the rest of your constraints but
you didn't do that and I wonder why
because that would reveal the
implementation of the function call
operation to the client code that's
using it that's how we implement it but
we want to expose a more general
interface so that the programmer doesn't
have to know that the return pointer
corresponds to a new code address I mean
the qualification of the universal
overall small R it's really limited to
the domain si so I mean prove that
implication you could just do case
analysis and say if there are only
finitely many good points let me
enumerate them all and check each one
but is that really what you want this
thing to me one complication that I've
left out here is that these functions
aren't really being passed side that
it's actually internalized inside for
the language of predicates that are
allowed to to be used here and
everything is set up to support modular
proofs where you're going to verify a
code module without knowing the full
universe of of sy and later you'll link
it with others and it should remain
valid when past pointers to code in
those other modules and just use just a
quick example of all these pieces in
action this is a program at
specification and it's proof inside of
caulk of the this code is processed by
the normal caulk system taking advantage
of the extensible parser and the the
flexible dependent type checker this
turns out to be an implementation of
imperative linked list length
calculation here's the specification up
here I won't go into too much detail
about what it says it's using
higher-order separation logic and
abstract predicates here's a predicate
capturing the idea of a linked list
representing a mathematical list with
the given head pointer now the next
thing is a definition of the actual
function to calculate lengths it has
assignments while return sequencing the
usual low-level imperative stuff the
only unusual thing for a programmer is
the highlighted part here which is a
loop invariant that we use to generate
verification conditions and then down
here is the proof where we say this code
up here
is an okay module and it's this proof is
done in one line with automation from
our library VC Gen tactic you can
probably guess what that does generates
verification conditions based on the
annotations like this one and then we
call a tactic called SEP that is a
separation logic automation procedure
for symbolic execution and entailment
checking takes an argument called hints
which is a small set of lemma as we
proved about the sll predicate and
there's a one other bit of tactic stuff
here that we've defined in a few lines
but mostly this is an automated proof
the same proof works for a larger module
that contains several more of your
favorite list functions and can change
the code and often not have to change
the proof yet you wind up with a proof
from first principles about assembly
language in the end alright so this is
the end of this abstraction level i'll
just want to take a quick detour and
talk about it on another language that
we've built on top of this one which is
a domain-specific language for xml
processing and database access with
which was used in that case study that i
briefly said something about earlier you
can write programs like this again this
is code processed entirely inside of
calk it's doing things like exposing an
RPC notation and manipulating an
in-memory relational database with
delete and insert operations it's
running a kind of query over that
database and running some command for
each one this happens to be an HTTP
based remote call to another server for
every one of the iterations of the loop
this is all d sugaring into a more
general language for pattern matching on
and generating xml and as well as doing
database access we prove that this is a
well-formed program which is basically a
type system level property and then we
when we have this property we can feed
it into a compiler correctness theorem
which tells us that the code that will
will be compiled from this program is
basically it's it's it's type safe it
maintains invariants on the states of
data structures and will not trample
over the state of other components of
the system and
this compiler is actually implemented as
one big macro in the structured
programming system that I just presented
and the way it's the implementation is
structured as we're going to do it in a
feature modular way where we will divide
the language into different features and
give each one its own macro which could
in theory be used in some remix of
another programming language that
chooses just a subset of these features
and adds some others and here's a quick
diagram of the structure of the
different macros we've built for
instance one that implements querying a
database table one that implements
pattern matching and XML trees each of
these has a separate encapsulated proof
and they can be composed together to to
build different kinds of languages that
have the same level of verification and
support without redoing any of those
proofs all right so let me tell you a
bit about the language that we build on
top of the structured programming system
it's called cito and it the thing that
it adds is first it has a more
conventional compiler instead of just
being a macro assembler so it can
support traditional program
optimizations in the compiler and it
also has some support for data
abstraction built into it which is
important for our motivating scenario
which is multi language programming so
imagine we have programs in two
different languages each of them has a
verified compiler where we mean compiler
correctness in the usual sense and each
of these compilers produces assembly
we'd like to be able to link together
the assembly programs that came out of
the two compilers now the wrinkle is
each of the programs we started with has
some kind of specification and proof and
these might be rather different if there
are very different programming languages
with different models so how can we then
get those specs and proofs to come
together to accurately describe the
linked program that combines these two
pieces of assembly and that's the the
challenge that we looked at in the
design of this compiler so imagine as a
practical example language one contains
an optimized data structure
implementation with manual memory
management for performance maybe think
of as a C and then language too it has
the implementation of some intricate
algorithm that uses that data structure
but really you want to think of this
algorithm in a higher level notation
maybe it's it's Java or something like
so in more detail we might have this
program that has sort of the header file
associated with with the C
implementation of finite sets of machine
integers we have some abstract type of
sets and we have operations to create a
new set D allocate a set we can add an
element to a set add this key to this
set and compute the size of a set and we
can use those in some nice higher-level
code to write a function that takes in
an array of integers as an argument and
then it's going to compute how many
unique values appear in the array and we
can do that by allocating a lot a finite
set as a local temporary and doing a
loop over adding all the elements of the
array to the set then computing the size
of the set D allocating the set and
returning the count that we found so
we'd like to be able to verify this
program without knowing anything about
the low-level language that provides the
finite set data structure and still be
able to link code together and get a
final theorem that this program meets
the natural functional specification of
counting unique values in the array and
the way that we do that is we use
abstract data types with formal
specifications as the the glue between
languages so we don't want each
languages semantics to have to say
anything about the syntax or semantics
of other languages instead we use the
preconditions and postconditions as the
cross language interface so we have
these precondition post condition method
invocation triples so that for instance
then list the set new operation has no
precondition and then on return it
guarantees that the return value is a
representation of the empty set that's
what this notation says in general the
programmer can define new constructors
like set for new mathematical domains
that should be used in the in these
specifications the delete operation is
sort of the opposite there's a set at
the beginning afterwards there isn't the
add element goes from a set s to a set s
with the the key K added n and the size
operation preserves the set and also
returns its size
sort of independent of the particular
language right again and some have come
across the system right so there must be
someplace in which you have to say look
if you've got a set built with these
functions you can't manipulate it with
some other random thing in a different
language the right foot representations
but it's still manipulate sense right so
this set function is actually associated
with a particular code module it's not
it's not the general idea of sense this
is yeah no sense in general yeah I'm
sorry I should have know what's it been
clear about that so an implementation of
a signature like this provides not just
the code that implements these
operations but it also has to define
what does this product it really mean as
a representation invariant that connects
the abstract state like the mathematical
set to the actual pointer based
representation of this data structure in
memory and different languages might
have different ways of implementing this
kind of concept we still want to use a
uniform abstraction mechanism for each
of those languages so returning to the
running example of calling F with the
arguments 42 and 17 now we have sort of
two different proof rules that might
apply to this call corresponding to
calls within the language and calls that
might go to a different language so this
is the conclusion of this proof rule is
using a big step operational semantics
for this programming language I'll use
Sigma to stand for a local variable
environment and Mew to stand for a heap
which actually contains those abstract
abstract data type models like the the
set constructor from the last slide and
so this rule says if you're in this
state of Sigma and mu and you call F on
these arguments it will terminate and
provide that in this new variable
environment in this new heap and the
this rule says in the context sy look up
the function you're calling find its
definition it looks like this it has the
number of arguments you're expecting in
this body statement then let's run that
function body s in the heap we assume
that was active at the time of the call
and in a variable environment that map's
each formal parameter to the value of
the actual
in the collar that's all this notation
says and then this will produce some new
local variable environment and some new
heap we copy the heap down to the result
of the call but we we throw away the
local variables and just in the color we
let it keep its original local variable
values so this is just a standard
operational semantics kind of way of
defining a function call so we'll call
this kind of mapping of the size
function and operational spec and we use
this mapping for calls within a language
but what's more interesting is the the
other case for calls across languages
where the the side mapping might also
say this function is associated not with
code but with a precondition and a post
condition expressing the contract for
using it and now we can say the way to
evaluate this call is check that the
precondition P holds of the Barratt the
argument values and the initial heap and
then then non-deterministically pick a
new heap new prime and check that the
post condition holds of the argument
values the starting heap and the final
heap and then we copy the final heap
down here as the result of the function
call so this is kind of mixed
operational and axiomatic semantics to
let us stay operational when we're
within one language and then take these
axiomatic steps to model the interface
between languages and ER and remember
these prep these P and Q predicates can
be talking about abstract data types
through these constructors like set and
that lets us reason about calling an
external function without knowing the
implementation details of how it
represents things in memory because
although the abstract predicates have
formal definitions and separation logic
somewhere but our proof can be
parametric in those alright so I think
I'll skip through some of the details of
this slide but it turns out that we can
use that semantics to state a
traditional compiler correctness theorem
in terms of a whore triple that every
invocation of our compiler should
satisfy and what we do is we translate
this kind of hor triple into a
precondition for the assembly code that
we output by our compiler and basically
the the compilation correctness theorem
works
in terms of showing that every assembly
program that comes out has a derivation
in an appropriate logic this logic
supports semantic linking between
modules therefore we get for free that
we can link the outputs of different
compilers that follow this kind of
strategy because the the final theorem
or formal interface of a piece of
compiled code is independent of the
compilation details it only focuses on
the interface as accessible by other
programs or other modules all right so
we add another language on top of this
one called facade that is exactly the
same syntax but has slightly different
operational semantics so recall that the
old one had this explicit notion of a
heap mu in the semantics facade has as
its state exposes only the values of
local variables in a function and though
local variables that store abstract data
type values will be represented in the
end with with pointers into the heap we
will hide that fact in the definition of
the facade language and by imposing some
extra rules so now when we went run a
function call I'm just showing the
axiomatic case of the semantics it says
check the precondition on just the
variable values no more heap involved
check the post condition on
before-and-after versions of the
arguments to the function and
furthermore we can't allow the same
variable to appear twice in this list
because that would create aliasing in in
the Kali function and to preserve the
fiction that all the state is in the
local variables and there all these nice
well-behaved things like finite sets we
have to prevent aliasing from ever
coming up so by adding restrictions like
this one that that will prevent anything
that would create aliasing in the
implementation we can allow the
programmer to pretend that the variables
really are directly storing finite sets
without having to expose any concept of
pointers or aliasing all right and the
last steps of this picture are we extend
up to compiling functional programs to
that facade language which is fairly
close to functional programs compared to
where we started and we'll also derive
functional programs from specifications
at the very top level of this picture so
we have this framework called Fiat that
that does
that derivation process it's it's
applying a well-established idea called
program derivation by stepwise
refinement so what we do in general is
we write a mathematical specification
written to be as clear as possible
without worrying about performance then
we apply what we call optimization
scripts that refine that specification
into an efficient program and the
refinement process is generating proofs
that witness the correctness of the
derivation and show that the final
program really has the specification we
started from and compared to past work
in this area we're focused on achieving
both high levels of automation and
program derivation but also high
assurance about the process where we
have a very small trusted code base to
check the final theorems so so briefly
we can start with code like this which
is an SQL style query over a data schema
that I haven't shown here like in
earlier examples this is all parsed and
type checked inside of caulk so that
that process will tell us if we have any
errors in referencing non-existent
fields and tables or something like that
using a syntax for database queries
inspired by monadic query comprehensions
and then so this is our spec it's not
actually executable directly it D sugars
and to set theory kind of specifications
and now we have to say how do we
implement this efficiently and we do
that by writing an optimization script
where we basically start by suggesting a
data structure to use to represent each
one of the tables in that database this
is similar to specifying an index in
standard SQL but we do it by using first
dependent types that tell us that this
is the type of a correct index
implementation for this relation from
our specification and then we run a
tactic in caulk that builds the data
structure and also the proof that it's a
sound representation of the
specification so we do that for both of
our our database pieces then we define
an abstraction relation that explains
the connection between the specification
and the data structures that implement
it and then we just pass that relation
to a tactic from our library called plan
meant to evoke the idea of query
planners in SQL and then this tactic
will build the efficient functional
program for us just using the hint that
we gave by suggesting a data
representation
and in particular the code that comes
out is like this it's a standard
functional programming program using old
favorites like fold left and at
appropriate points also calling methods
like be find and be count which are for
a data structure from our library that
implements these indexes as nested AVL
trees so we get good asymptotic
performance from this code and of course
we also generated a proof that it
preserves the behavior of the original
program and the basic framework this all
happens in is representing computations
as sets of possible values or we're in a
non determinism monad or or a power set
monad where we have the usual return and
bind Combinator's that we can define in
appropriate ways for instance return is
just the singleton set constructor bind
uses quantifiers in the right way and it
turns out that in this setting the
subset relation serves as a natural
notion of implements our original
specification can be a very
non-deterministic program in this monad
the final efficient program can be a
very deterministic program that has just
one possible value we will gradually
refine the spec into the efficient
program step-by-step so for instance if
we know that our specification as we
want some number X that is even we can
run one step of refinement and replace
that goal with with we've chosen some
predicate P and now what we're trying to
do is pick some value n that satisfies P
and then return two times n obviously
because we're returning two times
something this is a correct
implementation of evenness so we can we
can refine the specification like this
we potentially reduce some of the the
non-determinism and then repeat that
process until we get to a term that has
just one possible answer and it also
turns out that this this subset relation
is a pre-order and has the right
congruence is so we can treat it as a an
an oriented relation for rewriting so
for instance if we're trying if we start
from the step we reached in the previous
slide if we have the fact that the set
of X that's the P can be implemented
with the set of X that's the queue we
can rewrite within this this big formula
replace the X that's the P with XS that
Q and so in general our derivations are
just sequences of steps like this
applying rewriting rules with within the
terms that we use to summarize what we
know about
goal and eventually we get to a point
where the whole the whole term has just
returned something
it's a deterministic program and we can
just run that in in Cocke or even oh
camel which we extract too so that's the
big idea we write scripts that
automatically find the right rewrite
rules to apply for different
specification domains and therefore
thereby we produce programs in Gallina
the functional programming language of
Kok and we can extract those two oh
camel to to run them but we might be
able to get even better performance by
going through bedrock and actually
trying to produce assembly programs from
the the Gallina programs that we
synthesized and what we do for that is
we reuse the refinement framework to
give it a functional program find a C++
style program in our facade language
that has the same semantics as that
program then we can use our verified
compiler to lower that to the the c c
like level of the seto language and the
verified compiler there to produce an
assembly program which is one of these
semantic modules that bedrock supports
and then we can also say implement a
b-tree data structure more manually in
one of these low-level languages do a
more manual proof that it meets some
specification of an abstract data type
and then we can link together our
verified B tree with the synthesized
code and we can get a closed program
with a proof that this combined assembly
code actually meets the specification
that we started with here and this whole
pipeline accepting this this data
structure that we built manually this
pipeline is is automated while still
generating a proof from first principles
and just briefly I'll flash up some
examples of what different stages and
this derivation process look like we can
start out with us with a relational
specification like this one which
captures the idea of computing the sum
of the unique elements of a list this
this harpoon down operator is for
computing the set of elements of a list
and we can fold the plus operator over
that that's set and get the number of
unique elements in there we can
translate that to an executable
functional program that uses a finite
set as an intermediate data structure
much like in an example I showed before
an important thing about this code is
that it uses some implementation of the
finite set abstract data type the proofs
are all parametric and what that
implementation
is so we record that this code is
dependent on some finite set
implementation we can then translate the
code to a C++ style program it does a
similar thing using a while loop instead
of a fold left with allocating a new
finite set and doing imperative mutation
and the allocation of the both the set
and the list at the end still has a
finite set dependency we can compile
that to an assembly program that
maintains the same dependency and then
we get a final theorem in separation
logic saying that the specification for
this function is that it starts out when
you call it there must be a linked list
in memory of starting at the the
argument L and in disjoint part of
memory there must be the data structures
of the malloc library then afterwards
the return value R belongs to the set of
legal answers that our original
specification dictates and the malloc
library state is still there and so we
can take a verified implementation of
finite sets at the low level and we can
link it with our assembly program and
get the the code and the proofs for this
this closed program showing that it
really does implement this specification
right and I think I'll probably skip the
details of this slide which just
explains a little bit about the internal
process of deriving a C++ style program
from a functional program using the same
refinement framework that we use to get
the functional program in the first
place we just have a notation for state
arrow state means the set of programs
that would transform this state to that
state and we can we can use the
specification you use the functional
program to write down and an initial
description of this state and then keep
refining it into sub computations until
we get to the point where all the steps
are actually constructs that the target
programming language supports and we've
done this in a way that where we have a
proof that the final program really
preserves the behavior of the original
one alright so I think I'll wrap up here
this is a URL for the website for our
project on the web that it's a release
to as open source a quick summary of
what bedrock provides it's a small
trusted code base for doing proofs of
functional correctness it's it supports
modularity and proof automation and
verified compilation all for one common
assembly language that with
the possibility to write programs and
derive programs and many different
higher-level languages Thanks couple of
slice back you said about the
performance purposes of the extraction
in camel versus going through your tool
chain is it faster or have you sort of
actually got any numbers for everyone I
think we haven't done all the
engineering work yet that would be
needed to make it actually faster but
you you think the old camel extraction
and call caching but not as much
okay that can also be true in I can't in
theory it seems obvious that it should
be able to be faster but we haven't
actually demonstrated that empirically
yet the lowering abstractions to to do
more optimizations right so you want to
do register allocation stuff to keep
some interest so since I'm silver have
you done any of that work or is it just
you just creasing cookie-cutter code
from each mode multi we don't do
register allocation yet but we do
constant folding and dead code
elimination as two token examples we
picked in our verified compiler for the
two directions of dataflow analysis
other direction should be compatible too
but we haven't implemented or verified
them you
you know yeah so when he went from the
Galina down to the C++ or whatever
though whatever it's called me actually
inserting memory management there right
because you're deleting so how do you
justify those right so we have to prove
LEM was like this one that characterized
how to implement functional programming
constructions in terms of lower-level
idioms that might involve memory
management being introduced explicitly
well the whole compilation process is
just applying this kind of rule so
here's one four fold left and there
might be others for for other kinds of
operations</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>