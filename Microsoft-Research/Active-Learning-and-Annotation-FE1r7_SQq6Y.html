<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Active Learning and Annotation | Coder Coacher - Coaching Coders</title><meta content="Active Learning and Annotation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Active Learning and Annotation</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FE1r7_SQq6Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
welcome everyone to this session again
so it's a great pleasure to introduce
Anju - Gupta Sanjay is a professor at
UCSD he has been there for around 13
years before that he was at AT&amp;amp;T Bell
Labs as a researcher and before that it
is did his PhD from UC Berkeley
sanyo has been very active and has done
some amazingly good work and active
learning clustering and also some
machine learning problems so you know
it's great to have him here and I'm
looking forward to all his lectures for
the next two days okay thank you very
much
so right now I'm going to so I'll be
talking about various different things
but right now I'll talk a little bit
about active learning and this is meant
to be pretty understandable so you know
if there's anything that is a little bit
confusing or anything please just ask
straight away okay
so active learning is rather different
from the usual supervised learning model
and it's motivated by this vistas
observation that in a lot of cases data
is is unlabeled and and it's easy to get
lots of unlabeled data where it labels
are expensive okay so for example if
you're building a speech recognizer it's
easy to get speech you just leave the
microphone on and connect it you get as
much speech as you like okay but in
order to label that data
somebody has to sit down with the speech
signal and look at it and decide what
the phonemes are and exactly when each
phoneme begins and ends that's extremely
costly okay and so there's this sort of
asymmetry between the unlabeled data and
the labels that's really not part of the
supervised learning model at all and and
so the question is you know what is
what's a good way to take this into
account okay so the raw form of the data
is just unlabeled this is what you get
and you can get as much of it as you
like basically you know this is free
okay and what usually happens is that
you pick a few
please you say well it's going to be
really expensive to label these
I can only say a four afford eight
labels so you pick eight of these points
at random and then you get their labels
and then you throw everything else away
and now you have these points and maybe
you fit a linear boundary to it okay now
there are these other models
semi-supervised an active learning they
try to leverage the unlabeled data a
little bit okay so what happens in
semi-supervised learning is that you
pick those eight points at random and
you get their labeled but you keep the
other unlabeled points around and the
hope is that somehow the unlabeled
points bring out some kind of cluster
structure okay that give you an idea of
where the boundary ought to be you know
kind of between the clusters or
something like that
okay active learning takes things a
little bit further it says that you have
all this unlabeled data but instead of
picking eight points at random try and
pick them to be as informative as
possible okay you have you have only
eight things you can choose why don't
you pick things that are you know give
you as much information as possible
about the actual classifier okay so just
a quick example of how this you know
actually pans out this was a you know so
active learning it's used a lot just
because it's an you know it's necessary
and this was an example where they
wanted to do pedestrian detection and
they put a camera on top of a car and
the car drove around the streets of
Paris and collected many hours of video
okay and so there was just a huge amount
of video there's no way to label it all
and so what happened is that a few
frames were picked out at random and
then a human put a box around the
pedestrians okay and you know that's
very small amount of label data was used
to train some classifier and then that
classify was applied to all the frames
okay and you know if there was something
that the classifier was unsure about the
human was asked to label that okay and
so that was the way in which active
learning was done build an initial
classifier
have it make predictions but also have
the sort of notion of uncertainty or
confidence and then only ask for things
that you know you are not confident
about and then retrain and iterate okay
and in fact a lot of active learning
methods look like this okay so this is
the typical way in which active learning
proceed so you start with this large
amount of unlabeled data you pick a few
points at random and you get their
labels just to kind of get the lay of
the land just so you know roughly where
the boundary is gonna be and then you
begin the actual active learning loop
okay where what you do is you've got a
fuel able to fit a boundary to the label
data and now you say well the points
near the boundary are the ones that
really I'm unsure about these are the
uncertain points and so the next ones
I'm gonna ask for are these let's say
and then you get those labels and then
you retrain and then you keep iterating
like this okay and there are many
different criteria that are you know so
sometimes you ask for the mode the one
closest to the boundary or the one that
is most uncertain or the one that is
will most reduce the overall uncertainty
there are all these variations but
basically it's kind of a same thing okay
and and so the question is how good are
these schemes okay because they
certainly sound very sensible and how
can we analyze them because in
supervised learning you know we have all
these you know we have this really nice
understanding of what what the methods
are doing and so what really is going on
here how good at these active learning
methods okay and so how do we analyze
this and so we might as well do this and
just the usual setting in which we
always analyze supervised learning as
well which is the statistical learning
theory framework okay so in this
framework you say that there's some
unknown underlying distribution on the
day time label for example the
distribution of all street scenes in
Paris okay or the distribution of all
documents from your document generating
source or whatever there's some
underlying distribution from which the
training data comes and from which
future test data will also come you have
no idea what this distribution is but
it's there
okay and so there's this mysterious
distribution people now you're gonna
build a classifier and it's going to be
from some particular class age like
linear classifiers for example okay and
what you would like therefore is the
classifier that does the best possible
on that underlying distribution okay be
optimal classifier the one that you know
does the best job on the streets of
Paris for example okay so the linear
separator that does the best on that
underlying distribution now this is
something that's well-defined the only
problem is that we don't have access to
it we can't find it because we don't
know what this distribution is and all
we ever get to see are a few points from
that distribution okay so we get to see
endpoints from this distribution and
then we want to choose the classifier
that does best on those endpoints and
then we hope that you know as we see
more and more data this classified that
we've got will eventually converge to
the best possible thing so does that
make sense so this is the general
statistical learning theory playbook so
with this framework in mind we can go
back to active learning okay so let's
see what's happening over here okay so
now we have some unlabeled points and
what are these points from there from
this underlying distribution B okay so
these unlabeled points have been
generated from this underlying
distribution okay
so let's see we pick a few of them at
random within this case we pick six of
them at random and we've got their
labels these six points that we've
chosen at random are from the underlying
distribution P so we have six labeled
points from the underlying distribution
and so what we do with those label
points is meaningful okay and so we get
this initial boundary now we enter the
active learning loop and so we query
some points near the boundary and as
time goes on the points we are querying
are really concentrated near the
boundary and so we get a data set that
looks less and less like the underlying
distribution it's really a very weird
data set that has no clear relationship
with P and it's not at all obvious that
optimizing with respect to this strange
data set will actually do well on the
under
distribution at all okay so it seems
like there's a big problem of sampling
bias okay and that really is the sort of
central problem in active learning okay
you try to be really clever and you know
to pick these maximally informative
points but what you end up with is the
sample that's super biased you know it
doesn't look like the underlying
distribution at all and now you're
trying to optimize with respect to
that's all okay so that's the basic
problem and so let me just give you a
more concrete example because I've just
been explaining this problem in a
somewhat abstract setting so let's just
look at a simple case where we had data
that's just on the line okay so this is
1d data and let's say that that's
basically in four clusters and they're
two classes the Reds and the greens so
there's a big cluster here of Reds
there's a big cluster here of greens and
then there are two clusters over here
okay and so we start our active learning
procedure you're gonna pick a few points
at random since you're picking a few
points probably they'll be here and here
okay and so your initial boundary let's
if you're using a support vector machine
will be somewhere in the middle
somewhere around here okay and so now
you'll begin your active learning loop
you're gonna ask for points near the
boundary which means that you're going
to be asking for points around here okay
and as time goes on you will converge to
this thing okay so you're gonna end up
converging to this boundary but that's
not the right boundary at all this
boundary has got a five percent error
rate okay where is the right boundary is
this with the 2.5 percent error rate
okay so in active learning we try to do
well with very few labels okay but this
is a case where even if you get
infinitely many labels this active
learning procedure is converging to the
wrong thing okay so does that does that
example make sense and this is in 1d
what's basically happening is there's
this cluster that is far from the
initial boundary and because it's far
from the boundary the algorithms
confident about it oh I know what its
label is but it's never seen any labels
in that cluster
so it's confident but in this case that
confidence is misplaced
okay it was wrong and it really should
have asked for some labels in there but
that's not what these procedures do okay
and so if this is happening in one
dimension you can imagine that this
problem is much worse in higher
dimensions where there are lots more
places for these clusters to be hiding
out okay and so and you know so they've
been nice studies for example where
people doing text classification have
noticed this sort of thing you know they
use active learning on this corpus of
documents and there are these entire
clusters of documents that never get
queried and about which the you know the
system is completely confident and it's
wrong okay so so what we look at is is
just ways to manage sampling bias and to
come up when so what we what we really
want our algorithms that are first of
all at least consistent you know so
given enough labels they they do
actually converge to the right pane okay
and then our you know some idea of you
know how many labels we end up using
okay and so there are three methods that
we'll talk about and and it's it's sort
of non-trivial to deal with this
sampling bias so before we get into that
I just wanted to address the question of
I mean is it worth the trouble
okay so active learning clearly is not
that easy
okay so sampling bias is a real problem
is it worth the trouble okay and and you
know so I'll just show you a canonical
example where you know this sort of
illustrates the power of active learning
okay so suppose that again we have data
just on the line for simplicity and that
the clock the concepts we're looking at
are just thresholds okay so the data
lies on the line and there's some
threshold and to one side of it it's
plus and to the other side it's - okay
so really simple okay and the data is
actually separable so there really you
know it really does look like this okay
and so we just want to find the right
threshold so one thing we know is that
of course we can never find exactly the
right threshold with a finite amount of
data but if we want error
than epsilon then we need about 1 over
epsilon points okay so if you want era
let's say at most 1% you need about a
hundred points ok because if you take
the hungry points and the gap between
you know adjacent points will be roughly
1% of the distribution ok so so that's
what we know we need in the supervised
learning case now in active learning
instead of 1 over epsilon labeled points
you can start with 1 over Epsilon
unlabeled points and then you pick the
point in the middle and you ask for its
label and if its label is a plus well
then you look at the 25 percentile point
and you ask for its label and if its
label is a minus then you look for the
37.5% eye point and so on and in this
way you do binary search and you locate
the boundary we just log 1 over Epsilon
labels ok this literally you know so in
the supervised setting you might have
say needed a million labels where is
over here you would need log of a
million ok which is 20 literally there
are no constants even you know so so
there is this sort of exponential
improvement in label complexity ok and
so this is this is the reason why people
are hopeful about the active learning
okay do you know in order to achieve
benefits of this kind now this is a toy
example ok and so there are lots of
problems the first is that in this toy
example the assumption is that the data
is really separable you know that they
really are a bunch of minuses followed
by a bunch of pluses ok and in reality
nothing is ever separable ok but in this
case it turns out that there is a sort
of noisy binary search that you can do
that would work just fine ok so that's
not a problem the real problem is other
hypothesis classes this is just 1d and
we know how to do binary search in 1d
but in higher dimension you know we're
looking for linear classifiers and
things like that how do you do a binary
search through this sort of high
dimensional space you know what is the
method for doing that and it turns out
that that's also something we can deal
with ok and so these are going to be the
three methods that we'll look at method
one is just to label everything
and that seems I mean it's not it's not
as naive as it seems ok
so let me explain what I mean by that
but this is method number one for active
learning okay
and so here is one of the earliest
active learning algorithms it's just a
really beautiful algorithm it's from
1991 so that means that you know if I'm
still talking about it in 2015 it means
must be really really good right it has
withstood the test of time okay and so
this is an active learning scheme that
is only for separable data okay but it's
not just for things on the line it
really works for arbitrary hypothesis
classes okay but it does assume it's
separable so if you're doing linear
classifiers it assumes that there really
is some classifier that separates the
pluses from the minuses okay so it's a
separable data and it works in a
streaming setting so you have this you
know oodles of unlabeled data millions
of unlabeled points and the way it's the
way you get them is just one at a time
okay and he's sending it an unlabeled
point you just decide on the spot
do I want if label or not okay and so
this is how it works it starts by taking
the entire hypothesis class like
decision trees or linear classifiers or
whatever your class is okay and then it
receives the next unlabeled point and it
has to decide do I want its label or not
okay and well it looks at the hypotheses
it looks at all these classifiers that
that are in a type office's class and it
says do they all agree on the label do
all of the possible classifiers pink
this should be a plus if they do I'm not
going to ask for its label but if 999 of
them think it's a plus and one of them
thinks it's a minus then I'm not sure so
I'm gonna ask for its label okay and so
if there's any disagreement at all about
its label then you ask for the label
okay and when you get a label then you
shrink the hypothesis class to include
only those things that agree with that
label okay so and then this goes on so
let me illustrate okay so let's say
we're running this procedure we've
already seen seven points and this is
the eighth point okay and let's say so
this in 2d and let's say we're using
linear classifiers okay so we get this
new point and now we have to decide do
we want
label or not okay well based on the
points we've seen so far these are some
of the candidate hypotheses okay so our
current version space H sub T consists
of everything that's consistent with the
labels we've seen so far and these are
some examples of them every one of them
thinks this is red okay there is no need
to ask for its label okay and so the
only points that you would get labeled
in this case are points that lie
somewhere in here okay and that's what
we are going to call the region of
uncertainty okay and so if the unlabeled
points lies in here then you will ask
for its label otherwise you won't okay
so this is certainly an active learning
algorithm it kind of intuitively makes
sense but it's not the you know it's um
it's a very mellow algorithm okay it's
not really looking for maximally
informative points all that's saying is
that look if we can be certain what this
thing's label is then we are not gonna
ask for the label but if we have the
slightest doubt we're gonna go ahead and
get the label okay so it's a completely
non aggressive method okay but what
we'll see is that it actually does
extremely well despite you know not
being super intelligent okay but there
are some issues okay so the first thing
is that maintaining the set of
hypotheses is in general intractable
okay so let's say you're dealing with
linear classifiers you have linear
classifiers you've seen some labels but
okay and now I just want to keep the
classifiers that are consistent with the
labels you have so far that's like some
sort of weird in a body in high
dimensional space it's not the sort of
thing you want to be dealing with okay
the second is this only works if the
data are separable okay otherwise this
inference rule doesn't make sense and
finally you know how many labels
actually does this use okay we would
like abound okay we'd like to know how
does it compare with supervised learning
for example and so we'll be able to deal
with
all of these things and so and so let's
do this one at a time okay so so the
first thing we'll do is to make it
agnostic and agnostic is just the word
that's used for non separable situations
which is the default situation okay
nothing is ever separable okay so you
have data we're no longer going to
assume that there's some perfect linear
separator that separates the what is
from the minuses
it's the agnostics a situation okay and
we're gonna make sure that everything
gets a label okay but some points get
labeled because we asked for the label
and some for the remaining points get
labeled because we have inferred the
label okay but everything is gonna get a
label and that's how we avoid sampling
bias okay so the problem with sampling
the reason sampling bias occurred is
that we would just lay we were just
querying a few points and maintaining
those labels we're going to avoid it by
giving everything a label okay either we
query it or we infer it one way or the
other everything is gonna get labeled
okay now the invariant is that any
labels that we figure out on our own you
know so there's some point which we
think really should be a plus and we
fill in that label by ourselves that
label will agree with the optimal
hypothesis we'll make sure of that okay
okay it might not be the label that we
would have got if we queried the point
because the data is not separable but it
will agree with the optimal hypothesis
and we will never need to maintain the
version space okay and to make this
happen we are going to use a black box
that takes these this label data and
then finds a linear separator that
agrees with all the points we've
inferred and minimizes error on the
query points and so let me just show you
what this algorithm would look like okay
so this is the agnostic version okay
so initially we stopped so all our
points are gonna get labeled some will
be inferred and some will be queried
initially both sets are
okay and now you keep getting these
points and for each one you have to
decide whether or not you want its label
okay so here's what you do you get a new
one label you get a new one labeled
point and what you do is you try all
possible labels okay you say well let's
first try giving it a label of one okay
so you give it a label of plus one and
then you feed you feed it into the black
box along with all the other label
points you have and you get back some
classifier now you say let's try it with
a label of minus one okay and again you
feed it into your black box black box
learner along with all the other label
points that you have and you get back
another classifier and now you look at
the error of those classifiers on the
label points that you have so far and if
one of them is much better than the
other if h plus is way better than h
minus then we know the label must be
plus one and if H fineness is way better
than H plus then we know the label must
be - otherwise we ask for the label okay
and so this is the way it works okay so
you keep around older you know your you
keep getting these points for some of
them you ask for the labels for the rest
you fill in the label okay and each time
you get a new point you just try both
labels you try it with a plus one give
it to your black box along with all the
other data and get back a hypothesis you
tried with a minus one get back another
hypothesis and now you look at the
relative performance of these hypotheses
on the data that you have and if one is
much better than the other then you
don't need to query the label
right so right so so what happens is
that so you get these two classifiers
and if one of them is much better than
the other then you don't need to ask for
the label okay like if H+ is way better
than h- then you say okay the label must
be plus but the question is like what is
that gap how large does that gap have to
be and that gap just comes directly from
generalization bounds for supervised
learning okay so it's like 1 over square
root T or you know one of those standard
generalization bounds for supervised
learning now and and built into that
will be you know any so you know so so
so these bounds hold with high
probability but you but you know as you
pointed out you're using it these in
bounds every single iteration right and
so these these these the failure
probabilities are adding up and so you
have to work that into the bound okay to
make sure that they add up to something
small
right so for calculating the error yeah
so for some points you have the actual
labels and for other points you just
have the label that you filled in okay
and it turns out that that's good enough
because you're looking at the
differences between these errors so even
if you filled in the wrong thing you're
shifting both by the same amount okay
that's sort of it does seem like
although you're labeling everything you
are introducing sampling bias by filling
in some of the labels yourselves right I
mean those one of the labels you fill in
might not for instance be the label that
you would have got if you'd queried okay
so that does seem like it's introducing
bias into the labels but it turns out
that whatever bias there is and there is
bias cancels out because you are taking
differences but I I'm not I wasn't going
to show that but yeah we will you know
the things right up no random alright so
this is always for random ordering okay
and anyway so so basically at the end of
this procedure I mean throughout the
procedure the guarantee ends up being
that the label to infer might not have
been you know what you would have got if
you actually asked for the label but
they are consistent with the optimal
hypothesis okay and so in this sense the
inferred labels are better than the
queried labels there is some optimal
hypothesis the inferred labels always
agree but agree with that hypothesis the
ones you query sometimes do sometimes
don't okay
right the high probability is over the
randomness so one way to think about it
is that each point is each point to see
is generated at random from the
underlying distribution so it's with
high probability over that this
algorithm is does not have other
randomness in it
right and that's why basically um you
know we're assuming that the points are
generated at random so it's not like
you're seeing all the pluses and then
the minuses so you know if you would
have if you do have a data set then what
you would do is to start by randomly
permuting it okay any other questions
yeah
yeah so I'll get to that I'll get to
that so right now this is a
time-consuming thing because for each
data point that you each time you get a
new point you are trying it with a plus
label and a minus label so you are
calling your you're learning to
classifiers for each new point that you
see right so it's polynomial but you
know it takes a long time and so we'll
we'll talk about that later okay okay so
this is well and good and it's
consistent which is good but what we'd
actually wanted from active learning was
to have a very low label complexity to
show that you can get error epsilon by
asking for very few labels okay so what
how many how many labels do you need in
order to get error epsilon we know the
answer for supervised learning so that's
been studied very carefully we know very
good answers what is it for active
learning okay so for supervised learning
sample complexity depends on the VC
dimension of the concept class okay so
if you want to get error epsilon and
your concept class if your linear
separators and D dimensions the VC
dimension is D or D plus one okay and
that's what it depends on okay if you
want error epsilon you need D over
epsilon labeled points roughly okay it
turns out that for active learning
there's another parameter that matters
or not a parameter of the concept class
called the disagreement coefficient okay
and this comes up in active learning
okay so let's look at that original
algorithm from 1991 cow and that was
only for the separable case the cone
Atlas Ladner algorithm only for data
that's actually linearly separable or
whatever your concept clauses okay so
for supervised we know that we need d
over epsilon for that for that algorithm
this is how many you need d times that
other constant times log 1 over epsilon
so it really is that same exponential
improvement okay
the dependents are next on epsilon is
just a log so that's long is 1 over a
million this really does become 20
instead
instead of a million okay and so and
that's that's a pretty remarkable thing
okay because that algorithm doesn't seem
like it's trying to be too clever okay
and it turns out that the similar things
holding the non-separable case and their
lower bound
okay so so these are algorithms that so
as I said in active learning the main
problem is sampling bias these are
algorithms that deal with the sampling
bias and they have these very good
bounds okay so everything seems good but
there are problems in practice
okay the this and so here are two rather
big problems okay the first is that
these generalization bounds for
supervised learning normally the way in
which we use them is just to understand
learning procedures mathematically okay
so we have our want to know we the
decision tree versus neural net which of
them needs more data you know we just
want to we unisys for there's sort of an
intellectual understanding of these
algorithms in these active learning
procedures the generalization bounds are
being used inside the algorithm okay so
now generalization bounds are known to
be pretty loose okay and using loose
bounds inside the algorithm leads to
methods that are overly conservative
they end up asking for way more labels
that are actually needed okay so that's
the first problem and the second problem
is that you know let's say you're
dealing with linear separators on each
step it asks you to find a linear
separator that's consistent with the
data so far or makes as few errors as
possible on the data so far that's an
np-hard problem okay so finding a linear
separator that minimizes errors on the
data you've seen is np-complete that's
not a problem we know how to solve okay
and in machine learning the way we
usually deal with that is to is to
abandon that as our goal we pretend
that's not what we really want okay well
we didn't really want to minimize the
number of mistakes we made we instead
wanted to minimize this convex surrogate
okay like the squared also the logistic
laws or something like that okay and so
and so that'll lead into the second
method for active learning which is
importance waiting okay so so let's look
at more general loss functions and as I
said this is you know one of the reasons
we do this is because the most intuitive
loss function is the 0 1 loss where you
get you get a penalty of 1 for
everything you get wrong and a penalty
of 0 if you get it right that that loss
function is unfortunately intractable to
deal with in general and so people end
up using these convex loss functions
like the squared loss and so on okay
where it's kind of a gradual thing you
know zero if you get it right and then
loss okay so okay so then the setup
changes so now you have an input space
and you have a label space which might
be plus 1 or minus 1 but now your
hypotheses like your linear classifiers
instead of outputting plus one or minus
one might say output a real number where
you would interpret a positive number as
being a plus label and a negative number
is being a negative label and then
there's a loss function for example for
logistic regression this is the loss
function over you okay and this is the
convex loss function okay so previously
we were talking about zero one loss but
now this just look at a general loss
function okay where the thing we want to
minimize is that we have this underlying
distribution we have the true label we
have the data point we have our
classifier and we want to minimize the
loss of what our classifier says with
respect to the true label okay and this
could be zero one loss but it can be
also one of these other convex losses
that make optimization easy yeah
Oh what are the worst case
right ya know that there are definitely
I'm free even for linear classifiers for
example there are some very bad
distributions and right and so though
then and we have a pretty good idea of
what those are and for those
distributions so this parameter depends
on the distribution okay but we have
nice bounce on it for any smooth
distribution say right okay basically
the worst distributions you can think of
are discrete distributions where like
let's say you just have a certain number
of points and each point could you know
just have a different label say or you
know that there's just it's just
constructed in such a way that you're
somehow forced to query all of them okay
okay so so any questions about this so
this is exactly what we were doing
before we get a bunch of data and
previously we just wanted to minimize
the fraction of time our classifiers
disagrees with the true label and now
we're saying let's just do something
more general okay we just want to
minimize the loss of our classifiers
output with respect to the true label
okay this is the true label that's what
our classifier says we want to minimize
the loss between these two okay so we're
now allowing loss functions that are not
just zero one okay so this is how active
learning can proceed in this case okay
so we are no longer going to label
everything we're just gonna ask for a
few labels okay we're not gonna fill in
the rest we're just gonna ask for a few
labels that's it the difference is when
we ask the labels we're going to put
weights on points okay so we're gonna
use weights and so we'll use important
some point okay so initially our label
data we haven't asked for any labels and
now we start this loop where we get one
unlabeled point at a time and we decide
whether or not to label it okay so you
get a new one labeled point now instead
of making a hard decision
like oh I definitely want its label or I
don't want its label you instead choose
a probability so you get this new on
label point and you say hmm let me give
this the probability of 0.8 which means
that you're pretty likely to label it so
then you flip a coin and with
probability point 8 you ask for its
label and with probability point 2 you
don't ask for its label okay
now if you do end up getting its label
then you add the point in the label and
you I give the point a weight one over
that probability okay so it becomes
randomized okay and this is an
importance weight so you with each new
point you don't decide I'm definitely
gonna label it or not you just pick a
probability 0.1 or something like that
okay and now you find the classifier
that minimizes the loss on the points
you actually labeled but where each
point is has got that weight one over
piece of T and one thing you can show is
that this is an unbiased estimate of the
actual ones okay it's the unbiased
estimate of the loss using all all
points okay and so let's just see how
exactly this works okay so um so for
each point that arrives you've chosen
this probability and so there's some
probability that you will actually query
that point okay and so let's call that
event Q sub T Q sub T is the event that
you've been actually gonna ask asked for
the label of the teeth point okay
now the expected value of Q sub T is
just P sub T that's the probability you
can ask for that label okay and the loss
function that you your loss function at
time T is this okay because if you don't
ask for a points label then this is a
zero and so that point does not appear
in the loss function if you ask for its
label then this is a 1 and it appears
with weight 1 over P sub T so this is
the loss function you're working with
okay this is the loss function you're
working with and it's expected value
will the expected value of this is piece
of T so this that cancels out and so the
expected value is the actual was
okay so the probabilities have been
rigged so as to remove the sampling bias
that makes sense so this is sort of a
pretty easy way to to ask for just a few
of the labels okay and and then you can
show some sort of generalization bounds
for this and the nice thing about this
scheme is that you can use any way of
setting the probabilities for example
one thing you could do is to you know
fit a classifier through the data you've
got so far and if a point is near the
boundary then you assign it a low
probability if it's far away sorry you
assigned it a high probability if it's
far away from the boundary you assign it
a low probability okay it doesn't matter
how you choose the probabilities this
will always be consistent okay so this
this weighting trick removes the
sampling bias okay now there's a
specific way of choosing the
probabilities I should say that actually
gives you the kinds of bounds we were
talking before and I won't really well
let me yeah let me not get into the
details right now but there is a way to
choose those probabilities that you know
that gives you those kind of nice
results we will you know the log 1 over
Epsilon type of results that we were
talking about before and it involves
solving two convex optimization problems
for each new unlabeled point that you
see okay but that's still better than
what was happening before okay where you
had to solve two zero one minimization
problems for each label point so I'll
just give you a quick example with this
just to see what some of the curves look
like okay so this is an example with two
gaussian classes where so you're
learning a linear separator in in ten
dimensions okay and the two gaussian
classes in which the overlap is 5% so so
there's so it's not separable okay but
it's really well-behaved gaussians are
you know really nice okay and so this is
not this is not separable but you know
as far as non-separable goes is pretty
okay and so you run this procedure and
this is the number of unlabeled points
seen and you know it seems to do fairly
well you know by the time you get to 500
unlabeled points it says for maybe 65
labels you know it sounds good so it's
it's something that you know can
actually be implemented okay and I'll
say a little bit more about that in a
second but I wanted to show you another
curve okay so the algorithm looks very
nice when you look at this girl okay
because here look that we we we got five
hundred we had a data set of five
hundred points we ended up labeling only
sixty five of them you know that's
pretty nice
but this curve is actually a little bit
more interesting okay because this
comparing what happens with supervised
learning okay so what this shows is that
initially active learning has
essentially no improvement over
supervised okay so and the reason for
that is initially you just have no idea
what's going on it's only when you have
a fairly good sense of the boundary that
this so-called exponential improvement
or anything like that kicks there and so
it only kicks in towards the end and you
know when you draw the picture like this
the difference doesn't look anywhere you
know as impressive as it looks in the
other picture okay so you can pick one
picture or the other in mind I actually
think this picture is is the more
interesting this it kind of shows what's
happening you know you need to get the
error down to a certain level before the
active learning really kicks in okay and
okay any questions about that okay so so
this is a lot more practical than the
previous scheme but it still is based on
loose generalization bounds and okay but
it turns out that you can actually
develop a very practical version of this
by by just replacing this day
but so that the basic importance waiting
idea is solid and easy okay there's not
you know that's that's really easy to
use okay but the problem is that in
order to set these probabilities in a
way that would give us these these
querying probabilities in a way that
would give us these these very good
label complexities we were doing all
sorts of complicated stuff okay and so
but you know there there are ways to
just sort of approximate that very
crudely and for example the current
version of John Lankford's optimization
package has this built-in you know where
instead of solving a convex optimization
procedure on each step you just take
whatever you had in the previous step
and you do one greater you do one
stochastic gradient descent step based
on the new point okay something like
that
okay so very crude approximation and
that seems to work okay although there's
very little theory about it okay
okay so finally I'll get to the last
active learning scheme and this is a
very simple and practical method okay
and so here's the here's the motivation
okay so you have a lot of unlabeled data
and suppose it looks like this okay so
you look at it and there really seem to
be five clusters over here okay um well
then maybe it's easy maybe you can just
ask for one label in each cluster and
you're done okay so you ask for a label
here turns out to be red you say okay
this is the red cluster okay so this is
kind of a dream scenario but you know
it's like you know overly optimistic so
what are the problems okay so there are
three issues the first is that the
cluster structure is usually not so
clearly defined here the clusters are
just really far away from each other
okay the second and this might be
another way of restating the first is
that typically their structure at many
different levels of granularity okay
there's a reasonable way to divide the
data into three clusters but then you
could also divide them into ten cluster
you know there's this like many levels
of structure and it's not clear what
level of granularity you should be
operating at and finally even if you do
have a cluster that seems like you know
pretty coherent and far away from
everything else there's no guarantee
that that's gonna be pure in its label
okay so how can we sort of accommodate
all of these and do something reasonable
okay and so here's the basic primitive
which does not work on its own but we're
going to put something around it that
makes it okay okay so we find some
clustering of the data okay
who knows how and now what we will do is
that within each cluster will randomly
pick a few points and get their labels
so in this case we've decided that we're
going to pick five points inside each
cluster and for some reason for inside
that one and we're just gonna you know
ask we're gonna randomly pick five
points get their labels and then we're
just going to assign the majority label
to the rest of the cluster and when we
do that we get a fully labeled data set
and then we can use that to train
whatever classifier we like okay so
really simple so now let's see how this
would actually work out okay how we
really need to do this so we have on
label data now we find a clustering okay
let's say we find those two clusters
over there now we're gonna ask the
labels and now the most important thing
is the following rule you are not
allowed to ask for the label of a
specific point you're only allowed to
ask for labels within a cluster so if I
want six labels I have to pick six
points at random inside this cluster I'm
not allowed to say give me the label of
that one okay so I just choose a number
like six and let's say I get six points
inside that cluster six points are
randomly chosen for me and I get their
labels and in this cluster let's say I
ask for eight points eight points were
randomly chosen for me and I got their
labels okay now based on this I'm
feeling pretty good about this cluster
I'm ready to make it the alread cluster
but this one is looking a little bit
more if he
okay so what do I do
well then I just refine the clustering
okay and so now I partition this further
okay so let's say I divide it into these
two regions and now the key the key
thing is to see the points these these
points over here were randomly chosen in
this cluster which means that the ones
in here are actually random within this
cluster as well so there's no sampling
bias okay because I randomly chose
within the original cluster when I split
it still remains random okay
and so the sampling bias thing is taken
care off and now at this point I might
be happy with this and I might need to
split this further okay
right so right it depends on whether
that label was used both for the
splitting and for the and for the
accounting yeah so the each label has to
serve one function or the other okay and
so and so one way to see this is that
you just start with a hierarchical
clustering of the data okay so you start
with a hierarchical clustering either
based on no labels or based on a few
labels okay so you start with a
hierarchical clustering and then you go
to the top of the hierarchy which is
just one cluster containing all the
points you asked for you pick a few of
those points at random if everything
looks pure you're done there's only one
label everything is that label okay but
that's not going to happen so then you
go down to the next level of the
hierarchy and now again you are allowed
to sample at random if you're a sample
at random in here and if something looks
pure then you know you can leave it
otherwise you go down to the next level
of the hierarchy and so on okay so it's
a very simple method yeah
right so what what's okay so so okay so
the two different thresholds the first
is that you know if at any given point
I'm forced to assign a label to this
cluster I'm always gonna use the
majority okay so that's easy right the
other thing is what threshold am I happy
with okay like over here should I be
happy or not okay and that is like you
know like I don't I don't know you know
how much juice is there sorry there
might be label imbalance or so so the
thing is that that is one aspect of the
problem so this scheme has been studied
quite a lot okay because it's so simple
and nice and so on but this is one
aspect of the problem that has not been
figured out okay and so basically the
you know one way to think about this is
that at this point we might decide we're
happy with this okay and work on
something some other region of the space
where everything is much more mixed up
okay where you have an equal number of
pluses and minuses so we work on that
other region we divide it up once that
region becomes more pure we might want
to come back to this one and then say
okay we were happy with this but now our
overall level of accuracy is so high
that we have to come back and work on
this okay and so the exact protocol of
doing that has now been worked out
properly okay
oh you just use whatever your favorite
clustering method is mine is average
linkage clustering okay there's no I
mean so you you use some yeah you just
use whatever clustering method you like
okay
you know some of well these you know
these these hierarchical schemes
actually you know they give you an
entire tree and so you know so you don't
have to make any decisions the only the
main decision you have to make is what
the distance function is that you're
going to use on the on the instances
themselves and for that you use you know
what whatever domain knowledge is
possible one thing that would make sense
is to use a few labels and constructing
the hierarchy but that's something that
is not known how to do you know people
don't have a good way of doing that
suppose you you know you suppose you
have a million points and you have
labels for a few of them and now you
want to construct a hierarchy that
somehow takes those labels into account
as well as respecting the geometry of
the points how do you do that you know
unclear yeah
oh yeah so that that could be determined
by what level of confidence you want
just from like just from standard
statistical confidence intervals you
know if you want a 95% confidence then
you need so many points and so on so the
statisticians would do that just based
on a normal approximation
so the scheme like that would so over
here I mean all those various nice
simple scheme the key feature of the
scheme is that somehow sampling bias has
been taken care of and if you were to
recombine clusters then that would again
become an issue that would have to be
dealt with somehow okay
yeah so what you would do well for
documents the default thing you would do
is you would start by fitting a topic
model to the corpus okay with lots of
topics right so like 200 dogs or
something like that so you fit this
topic model so now every document has
this nice 200 dimensional representation
and now you build a hierarchy on that
that you know that would kind of be the
default but there's probably better
things one can do okay so so the main
things that water the way for building
the initial hierarchical clustering you
can pick whatever you like rules for
deciding which cluster to query you can
pick whatever you like and when do you
move from a cluster down to its children
again you can pick whatever you like and
different people have looked at
different rules and you can prove bounds
and so on and so forth okay so the
bottom line is I think the fair thing to
say is that the right way to do this has
not been worked out but it's very
promising I mean it's it does work quite
well I'll just give you some a little
bit of intuition based on this data set
that everybody knows this is the the
data set of handwritten digits like
sixty thousand handwritten digits 784
dimensions are 28 by 28 okay so so this
is a case where you have ten classes
okay and there are 60,000 points you
have 60,000 say unlabeled points and you
have ten classes and you can use so what
the first step is to build a hierarchy
to do hierarchical clustering of 60,000
points okay so let's see what could we
hope for that hierarchical clustering
well the best case is that we build this
hierarchy and right at the top of the
hierarchy are these ten clusters there's
a way to to cut the tree so that you get
exactly ten clusters and they're all
completely pure right that's the dream
scenario the worst-case scenario is that
you keep going down the tree and it's
just everything's mixed at every level
and the only time you get pure is when
you get to the leaves to the sixty
thousand leaves so the question is like
you know
so clearly a scheme like this is gonna
do best when it's more like the first
scenario than the second okay and so
what does it look like you know well
what does it look like in and here's the
graph you know so and it shows that you
know when you get down to say 50
clusters it's actually pretty pure you
know it's it's below 10% okay and so so
it's not the dream scenario you know
it's not just ten clusters that are pure
but you know once you get down to about
50 everything is pretty close to pure
and after that it just tapers off pretty
gently okay and so this is a scenario
that is in which one can expect to do
well okay and sure enough one does you
know draw the curve and it's you know
way better than you know as one would
expect you know so so this is an area
where you get 250 classes or these you
have big clusters that are relatively
pure and so a method like this gonna do
well okay and so okay so so one last
thing I wanted to point out is that I
talked about a bunch of methods but
they're actually solving two different
problems the first two methods are
solving what I would call active
learning where the final product is a
classifier okay the final method is not
producing a classifier at the end yet
it's just labeling the data set okay and
I think that that's much better than the
first okay and that's a problem that we
can formalize you know I sometimes just
call it the finite population annotation
problem so just get classifiers out of
the picture altogether so you have some
data set each of each point has got a
label which is missing you're allowed to
query any label you like you're given
some epsilon and Delta and you want to
label the entire data set so that with
so that at most epsilon fraction of
those labels are wrong okay so you're
given a data set you're not asked to
produce a classifier you just ask to
label the whole data set and you're
given a threshold you're told okay you
can ask for whatever quit you can make
whatever
queries you like query some labels fill
in some other ones but at the end of the
day I wanted more 2% of the labels to be
wrong okay and the reason I like this
more than the traditional active
learning scenario is that the usual way
one uses this I mean okay if you get a
new data set and you don't know anything
about it okay so you have this new data
set you want to build a classifier and
so what is the first thing you're going
to try maybe a linear classifier so you
said okay let me get a linear active
learning algorithms you have this active
learning algorithm that is fine-tuned
for linear classifiers and you know is
asking for points near the boundary so
you fit a linear classifier but then you
realize that the linear classifier is
not so good maybe you needed a quadratic
classifier so now you pull out a
quadratic classifier active learning
algorithm which asks for a completely
different set of queries okay because
it's optimizing the quadratic boundary
and then that's not good enough so then
you decide okay well maybe I need a
neural net so then you pull out a neural
net active learning algorithm which asks
for a different bunch of queries so
somehow the you know the traditional
model of active learning is not you know
does not work well with this sort of
multiple rounds of of refinement in
which you know we we often end up
dealing with data sets and so this would
work much better okay we just label the
data set and now you have this labeled
corpus with a nice error guarantee okay
some of them are wrong but at most two
percent and now you can apply whatever
method you like okay but this is
something that I'm so sort of just
formally dealing with this problem and
coming up with good algorithms for it is
very much an open area okay okay well
thank you very much
sorry
you know I mean I active learning is
very widely used you know and in all
these vision NLP and it's just because
you know the labels aren't there so it
has to be used yeah
it's used across the board it's
especially important in in cases where
labels are super expensive like in some
biology settings where each label is a
lab experiment
you know something like</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>