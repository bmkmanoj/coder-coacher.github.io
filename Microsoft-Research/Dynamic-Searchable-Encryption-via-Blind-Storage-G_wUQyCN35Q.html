<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dynamic Searchable Encryption via Blind Storage | Coder Coacher - Coaching Coders</title><meta content="Dynamic Searchable Encryption via Blind Storage - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dynamic Searchable Encryption via Blind Storage</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G_wUQyCN35Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so it's a pleasure to have a mom in
the vide speaking today mom it is a PhD
student at University of Illinois at
urbana-champaign and where he works on
security and crypto and today he'll be
giving a talk about a new paper of his
on search of encryption Thank You Sammy
so I'm talking about I will work on this
blind storage and how we can use the
blind storage to create searchable
encryption scheme this is joint work
with my advisers both from University of
Illinois and so in in a pollute 2013
Amazon s3 reached like the total number
of objects stored in Amazon s3 reached 2
trillion and each object in Amazon s3 is
like from 1 by 2 5 terabytes it's like a
huge amount of data that is being stored
and this like this reasonably says that
people are using cloud storage services
to solve large amount of data and
another is just like how many students
are using it so this was like Dropbox
has a promotion offer a couple of years
back and if a particular number of
students enroll in this and then
everybody gets 20 gigabytes free storage
so if you can see like thousands of
students enroll there like just from
University of Illinois 7,000 students
enrolling so like a lot of people are
using cloud storage services and
recently Google Drive reduces the prices
are just like $2 per month 400 gigabytes
and 15 gigabytes is just free so people
will use it more and more because the
prices the prices are also decreasing
and just like another way to look at the
popularity is if you just do a Google
search for public cloud storage it to
achieve almost 350 million record search
results and then if you
do it for the privacy it like something
like 94 million and same with like
security issues is like 184 million so
people are worried about security and
privacy issues so now this is like a
funny story how like people think about
cloud storage so there is this video I
on YouTube I'm sure you don't want to
see it you know what cloud computing is
but the comments on this video were very
interesting
so this first guys say that if you want
to if you value your security and
privacy you don't need to use cloud this
other guy just replied and this is all
on YouTube the other guy replied that
banks are using it and you use banks so
it's secure I mean why are you worried
about the other guy comes in and he's
very pessimistic he says that it is like
a massive form of always-online DRM and
they can take all our privacy if data is
stored on the cloud and this other guys
is that even high stuff like he's saying
once our data is stored on the cloud
authority can just deny us from
accessing our data and then they can
take control of all of our data and this
is Africa this is saying that you can
just have two to three local drives and
then you just store everything locally
you just back up and he's think there is
no way that all three can fail at the
same time this other guy replied that
this all three cannot fail at the same
time but they can burn melt or they can
damage or they can disappear if someone
stole it so and then there is other guy
that is saying that I mean Jesus is even
more reasons and what's funny about it
is that if you want real security you
can just ship your hardest to your
parents every couple of months just like
so people have like different opinions
about this this is just like a funny
story on YouTube so now I'll talk about
like just storage outsourcing so first
how the our storage looks like now when
we store like in on
our own premises so they're like the
user and he is direct access to the
infrastructure storage infrastructure he
users can do whatever they want they can
write it free delete search and they can
do pretty much everything and they don't
have to worry about any security and
privacy because it's every everything is
in the infamous now when you move to the
cloud then the client stores all the
files with a cloud but now and the cloud
can be any company and but now they have
access to your data so that what is some
people so and the problem is that this
user can be hospital and these records
can be electronic health records and
there's then is even a serious concern
and it's not even that this hospital
might want supply this is like a law
they need to do it in a special way like
they need to encrypt it and do it in a
set special way so HIPAA is a law that
guarantees like you cannot store
unencrypted data or like identifiable
data on the cloud yes yes that's true
yeah I'm just like that's what you if
you just encrypt data and store it in
the cloud it satisfies him yeah but
still it leaks a lot of information so
maybe in future they regulate that nodes
so that another problem is like for
example University of Illinois a few
years back they moved from Gmail to now
now they maintain their own my exchange
server for email and it is just
available for graduate students and
faculty and they are just worried about
that Google might be seeing our research
and this type of issues so a knife
encryption if you just encrypt all your
documents and store it in the cloud and
this does not work because you can see
that some files are bigger than smaller
like this show that the size of the
files are leaked and and the other major
issue is that you cannot now search over
this data so
and if you if you want to hide the file
size you can just just combine all your
files encrypt it and store it in the
cloud but still the search problem is
there but now it's even worse you need
to download everything so now if you
want to search with this knife strategy
the only option is you just get back all
of your data you decrypt it and then you
search locally so here the problem
begins with that like the only reason
that you outsource the data was that you
were not able to handle it I mean that's
why you outsource oh you don't have
enough resources to handle it so you
outsource so you cannot get it back all
all all your data so that's not that
will not work there are many other
trivial solutions like with local
storage you store and X locally and
things like that but that also does not
work because and X is comparable size to
the data itself so can we do better so
yeah an answer is in there are different
ways we can do it we can use property
reserving encryption function encryption
fully homomorphic encryption or AM and
searchable in captions so some of them
are less secure some are more and some
are inefficient but yeah we believe that
a good approach is using searchable
encryption which provides good fair of
bid paper between performance and
security and privacy so for the
remaining of talk I'll talk about
searchable encryption so how searchable
encryption works is that like searchable
encryption have two phases in the first
phase the client has a set of documents
that it wants to upload to the cloud but
it wants to have the capability to
search over this data and without
telling the server the content of the
files so it just creates an inverted
index of the documents then it just
encrypts the files using simple ears
like simple encryption scheme and it
encrypts the end X using searchable
searchable encryptions
scheme send the sense it sent the
documents to the server and the annex
also do so so this is the setup phase
after that now climb one to search for
all like for some keyword like Illinois
then it will create an encrypted token
and send it to the cloud now on the
cloud from this encrypted token cloudy
that cloud will look into this encrypted
index and from it using some procedure
it will figure out that these two files
are required and it will get back these
files to the client and the client can
then decrypt the files locally and read
this is just the high-level idea what
over what searchable encryption does and
now if you want to add a document like
you have this document what you do is
you create you first extract all the key
words you encrypt the file using simple
encryption scheme you include you
encrypt the key word is in a clever
scheme using searchable encryption
scheme and you send the document encrypt
it and then you send this to the cloud
and then the cloud will somehow update
the index to reflect so now later if you
retrieve it it will ink you will get
this new file lots a same procedure for
delete also you just you'll just delete
the index from the index file there is
some leakage to the server in this
process and what's being leaked here is
that firstly the access pattern that is
which documents contain a key word like
if to copy if two documents contain same
keyword that this fact is leaked to the
server when you search and the other
things that that is leaked is the search
pattern like if you search for this
keyword once and then little and then
later you searched again and you use
search for the same keyword the server
knows the fact that you are searching
for the same keyword again and this is
for the search add addition leaks a lot
of information in most of the prior
schemes and including ours it leases it
leaks the hashes of all the keywords
that are present in the new document and
delete actually in the prior best scheme
it leaks the hashes of all the keywords
in the deleted document plus it leaks
some more information about other files
that are not even deleted so leak during
updates is more than during search so
there is a lot of prior work in this
area and most of the schemes are not
paralyse able and almost all the schemes
require server-side computation like you
need some computation to be done on the
server and you need some storage on the
server and as I explained earlier it's
the leakage is much more during updates
and so this access patron yes
yes no you don't litter yeah but as you
search then you leak some information
you really the exit access pattern and
search patch yeah so slowly it reveals
information and most of the five schemes
are very complicated schemes and if they
want to make it dynamic then it makes
that then they becomes even more
complicated so linked list with scheme
so most of the SSE schemes searchable
encryption schemes uses a linked list
based approaches so the the basic
approach is something like this so the
node of the linked list looked like
something like this that it has three
components one is a document ID it has a
key that is used to decrypt the next
node of the linked list and it has a
pointer to the next node so for every
key word what you do is you create this
linked list and and each node of this
linked list contains one of these
document IDs that contain the keyword
and then you encrypt the first node with
a special key that is stored in a
different data structure and you encrypt
the second node with the key that is
stored in the first node and then it
goes on and then in in this way the
whole data whole linked list is
encrypted and you do it for all the
keywords and after that you stored the
first address of first key to decrypt
the first node in the special data
structure and the elements of this data
sector that is an array has a like key
to the key to decrypt the first node and
pointer to the first node and then now
if you and for a keyword so in this data
structure this corresponding element is
stored in a position that is just like
hash of the key word so the array
position where this element for for for
a linked list needs to be stored as hash
of the keyword so you can little
determine it again
so after that all of these nodes are are
randomly put it in a link in in a big
array and then you do it for all the
linked lists and then pretty much this
is the two data structures that are sent
to the server now later if you want to
do search for example for this keyword
to you just apply the same PRF to and
then you get the location in this data
structure T and if you get this location
this this contains the key to decrypt
the first node and pointer to the first
node right so from this you can go to
the you can decrypt the first node of
the linked list you just remove this
lock and then this node looks like this
it has document ID so you recover the
document ID of the first document it has
pointed to the next one it has key to
decrypt the next node so from here you
can go to here and then you can now
decrypt this node and in the same way
you go on and you decrypt all the nodes
so this is how search works in this
linked list B schemes so I mean if you
if you follow it I mean you can see that
this means that it is sequential like
you cannot paralyze it because in link
list you would decrypt the previous node
with the key stored in the you decrypt
the next node with the key stored in the
previous node so it cannot be paralyzed
about yeah and even a addition document
adding document and deletion is even
more complicated so now I'll explain
blind story yes
I know yeah it does not matter I mean
you can just leave it in the clear
because there were once the server knows
it then yeah the leak already happened
at some point you might want to do it
again but so we use a different
technique to achieve black-ass
searchable encryption that is based on
blind storage and so first we so first
I'll talk about our blind storage scheme
so what blind storage should look like
is something like this so the client has
a data structure that we call blind
store it's just a simple array and then
client has a bunch of files and a client
put all these files in using some
algorithm in this data structure and
then it encrypts it in a clever way and
send it to the cloud now now and and
this just needs to guarantee these two
things that a total number of files I'll
is not lead to the server and the size
of the individual files are not late and
then later when we want to access a file
what we do is we just we want to read
the file we have a file name we just
encrypt it using for example just
sha-256 send it to the server server
we'll retrieve the file encrypt it send
it back to the client the client can
decrypt it and if the client wants to
update client can update this file and
in encrypted again and send it back to
the cloud so this this way client can
like this is a general access it can
read write update and delete in the same
way
now scatter store is our protocol which
we uses to achieve yes yes yeah I mean
I'll explain later we build search on
top of it yeah so scatter store is our
protocol that we use to achieve blind
storage so we need to guarantee these
two things that it should not leak the
number of files initially index and it
should not leak the file sizes of the
files that are initially indexed so our
dine storage scheme is based on block so
our block format is something like this
so we have header in every block and
some some space for data so Heller has
two things one is the hash of the file
ID and the other thing is version so
version is used so every time we re
encrypt the block we use a different IV
so for that we keep track of this
version so we use a new nonce in the
encryptions and the first block of a
file is some is is a little bit special
it just stores the number of blocks
that's extra total number of blocks of a
file is stored in the first block of the
file and if we have a file it is just
passed into these blocks and then these
blocks are stored in the blind storage
so how our schemes work is that if we
have a blind so assume this is just a
simple array we call it blind store when
our client wants to store this file with
file name one it has just two blocks I
mean just for simplicity now if the
client wants to store this file in this
blind storage what it does is it first
create a seat by just applying a hash
function or any PRF to the file name it
gets this seat and from this seats it
generates it will pick some it will
generate a pseudo-random sequence so it
will generate some random blocks from
this blind store
and then among these it can store its
file in two of them like this and now
just two of them are mark occupied rest
are just free and later if the client
wants to store another file file name
with file with with file name file name
- it can just do the same process again
it it get it generator seed and it
randomly selects another another set of
blocks and now it can store it in the
available blocks in those that is
selected randomly and now these are mark
occupied this process this this process
goes on and this blind store is slowly
filled up and now when later the client
want to access the block it is actually
then encrypted this thing is encrypted
each block is encrypted separately sent
to the server and now the server as
third word has each block encrypted and
now client wants to access it
for example file name - it just
generated the seat and with that seed it
can figure out which blocks to retrieve
and then it just retrieves all these
blocks and now if the client wants to
update these blocks the client can
update these blocks and write it back to
the server in the same positions so full
access is little bit tricky that it has
two rounds in the first round what we do
is this similar we just send the seed
but because we do not know the file size
of the file in advance we just download
a fixed number of blocks for example -
here so we just download these two
blocks in the first round the first
block as I showed you in the block form
it contains the number of blocks so we
retrieve the number of blocks from the
first block once we have these number of
blocks we can do the second round we
just
to use the same seed to generate the
rest of the pseudo-random sequence and
we download remaining six blocks and now
the client can write back again if it
wants to so this is how our blinds were
scheme works any questions so this
pseudo-random sequence that we need is I
mean we have different parameters but in
in our implementation we use like four
times the blocks in the file so if the
file has four blocks we select 16 blocks
yeah and the overhead is something like
we need four times more storage else and
that is the whole tricky part and the
product will how we reduced the blow up
in the storage yeah that yeah we so yeah
I actually we have a detailed proof in
the paper in which we show that if you
do it in this way the overhead is for
like very small
yes the it way yeah there will be an
issue I mean so we do it like here the
first round in our protocol the two
blocks I just use it for like explaining
yet it's not - it's like 80 blocks so in
the first round we download 80 blocks
always so that take care of small files
so the probability that like for small
files you you'll be able to find enough
blocks yeah but in the people we do
prove that like this probability is
something like 2 to the power minus 4 T
or 2 to the power minus 8 okay yeah yes
so the probability that you select
blocks for a file and the probability
that you are not able to write your file
in these blocks because they were
already occupied he is negligible yeah
so that is why like why we are reading
more is actually just because of this
that for example if you have if you just
select two blocks to write your file
then then it's like the first file is
good but when you want to write your
second file now the problem is that if
you select randomly these blocks the
probability of overlap with with the
block that is already occupied is non
negligible and if you just select two
then you need to select another block to
write your file and then it does leak
the fact that there is one file that is
already present there is one file that
is all already present in the clock in
the in the storage so we do not want to
reveal this also so one one thing that
more that we require is that accessing
single file should not leave anything
about the other files
so now I'll explain how we build our
Tessie skin on top of blind storage so
we again have the setup freeze so what
we have is we have our our blind store
and then we have a set of documents we
create an index just simple pinata index
and each each of the so we considered
like this list for each keyword as as
our file in our blind storage so we
create file for each keyword so this is
like file with contents one and two and
we create with this for all the file all
the keywords and then we stored this in
in the blind storage then we encrypt
this using the blind scheme that I
explained and we encrypt the documents
using simple yes we send both to the
cloud and now when we want to do the
search what we do is that we send this
encrypted token to the server and has
this it just looks up in the blind
storage and will give us the file using
the same strategy for the blind storage
it will give us this file that is
actually the list of document IDs that
contain this keyword we retrieve this
file and the client can now decrypt the
file and it can read the document IDs
that contain that keyword now the client
needs to send this document IDs back to
the server and the server can pick the
file two and three and send it back to
the client kind clean just to grab the
keys and now at this stage we need to
write back this file and recall and we
and we needed for lazy delay that I'll
explain later
but every time for every search we need
to write it back
so we have this storage that we yeah
that yeah that is allowed to leak in
searchable encryption schemes yeah that
is the allowed leakage like we define
this leakage we allows yeah I mean this
is this is the reason why all searchable
encryption seems an efficient that you
would do leak something we leaked this
so when you search you leak this access
pattern that which documents contain
which encrypted keywords and you also
leak that if you are searching for the
same keyword again repeatedly
yeah somehow Witek
okay so now oh I had a so we need clear
store because as I explained earlier in
searchable encryption schemes when you
add document the hashes of all the
keywords are already leaked to the
server so we don't need to put it in our
blind storage scheme if we do put it in
our lines well scheme is very expensive
but so for that reason we have this
another store in a clear store that so
new files that we update files then when
when we add files that are stored in
clear store and not in the blind storage
so it stores the files and encrypted
because for example whenever you add a
document then you know which keywords it
links to because you added a document in
your blind storage system and you also
need to update the index files for the
keywords that this document contains so
that information is leaked to the server
anyway so we do not need to use blind
storage to protect that so for
yes yeah so that's what we know so this
clear store is just it's toward the end
Xbox so for example what it will store
unencrypted is like the keyword and then
which document IDs it maps to in blind
storage we need to encrypt these and use
it in the clever way because it does
leak anyway yes is the document IDs not
the document IDs are so keywords are
still like you need to do this PRF
evaluation yeah yeah but the document
IDs are stored and unencrypted because
the so we don't know we do not need to
pay the overhead of blind storage yeah
actually one limitation we can use blind
storage but the problem is that we want
that the server do not do any operations
if the server can support append
operation then we can like just it can
just append something to a file and then
we can just append like when whenever we
add we can just send to the seven it can
append it yeah so so it supports the
like constant time append operation that
is like if you have a file you can just
add if you can append something to the
file and because many cloud api's do not
support the append operation we
implement it by just downloading small
number of blocks and uploading small
number of blocks so like if we have this
document list we just download the last
block and if it is full we download one
more block and we need to download the
first block walls so we just didn't need
to download three blocks and upload then
just to of
so now whenever we need to add a
document what we need to do is we have
this document and then we just extract
keys keywords and all the key words and
now we encrypt the document with a
simple encryption scheme and that is
like a yes and then we encrypt these
keywords using like sha-256 and then we
send this key word then the file the
file is then for all these keywords we
will get these files that is actually
the index that corresponds to this key
there's like the list of documents that
contain this key word and like just as
as I explained earlier these are just
like this is not the complete index that
we download we just download three
blocks every time and upload two blocks
so like they are downloaded all these
files and the client can just update it
like add the document ID and then send
it back to the server so delete is free
in our scheme we so what we need to do
is if the client wants to delete a
document to just send the document ID to
the server and the server will just
delete like for example if this is a
file that needs to be deleted so we will
just delete it from here and that's it
that's what we need to do for delete so
it's pretty much free like you just need
one operation to delete the file from
the file system but then we need this
lazy delete strategy using search time
for example when you search for keyword
you get this you get this file and at
this point if you when you send this
document IDs to the server and and at
this point you see that file tree does
not exist it will be the server can tell
to the client that this file does not
exist and now the client can just update
this document IDs two two
and then send it back to the server so
this is how we handle lazy delete and so
this produces leakage also when we
delete a file we do not leak anything
but when slowly when we start searching
then we just reveal the fact that this
keyword that that has been just searched
for is present was present in that file
that was related something so we do leak
access pattern and search pattern and
just like all prior sse schemes and so
we leave nothing when the file is
deleted
at that time we leak nothing but slowly
when when when we start searching for
the keywords that were present in the
file that was deleted we leak the fact
that these keywords were present in that
file but it is gradually like if client
does never search for the keyword that
was present in the document that was
deleted the information is not leaked so
for the updates our scheme leaks
strictly less than all prior schemes
except this stuff enough at all in DSS
2014 scheme that that achieve a stronger
no privacy notion but they need to pay
log read log cube or head on top of our
scheme
so now I'll explain some of the features
of our using blind storage to implement
searchable encryption so the first is
our scheme as you have seen is very
simple and it's very efficient so it is
dynamic and still the whole scheme is
very simple and the other advantage we
have is that we have a computation free
server which means that all prior
schemes need some kind of processing to
be done on the cloud and then they need
some storage services so and what we
need is that our scheme just works with
any computation free server like this
can be Dropbox so we the interface that
we need from the server is just upload
blocks and download blocks so why why is
it important like this fact that we do
not that we do not need any computation
on the server is that first it can work
with any cloud service like Dropbox or
Google Drive and clap cloud storage is
more widely used by like normal users
maybe enterprises uses compute cloud
also but and so there is bandwidth cost
like for example if you use Amazon ec2
and Dropbox then you need to pay for
bandwidth cost between them and also
there is latency issues are well known
in cloud services even even if you use
Amazon s3 and ec2 they are in the same
so in the same data centers they are
separated like if you need to get data
from easier s3 Amazon s3 you need to go
through the data center network and this
will include some lot of delay
so other features of our sse scheme is
that its support compression we can
compress the files in the blind storage
so if the index files are like not
random then we can reduce the size of
the index so we just put the index in
the blind storage at the like another
way is that we can put all of our
documents also in the blind storage and
then we do not reveal information about
document salts and it is inherently
parallel like because we just need to do
this decryption on blocks that can be
just paralyzed very easily
we leak less and we just our security
just depend upon like standard
assumption that are like PRF assumption
and security of PRF and hash functions
and our delete is like just zero cost we
do not pay anything for delete so now
I'll talk about the performance of our
scheme so if I mean if our scheme is
used cleverly it will just cause like
four times four times a es cause to in
to encrypt the end X this is not always
true you need to like use it cleverly so
this is the processing call of cost this
is so the previous best scheme it took
like 15 hours to process sixteen
gigabytes of data this is just
searchable encryption cost excluding the
time it takes to generate the plaintext
index and our scheme just take 41
minutes to do to process the same amount
same amount of data so we evaluated our
scheme own and run data set and you can
see that search is also this is
pre-processing so we ran the experiments
to tell like to
six megabyte of data and it's pretty
fast just like take like 35 seconds to
encrypt to encrypt 256 megabytes of data
using our searchable encryption scheme
and search is also very fast like in
like so here we searched for the most
frequent keyword that is like V so we
search for the word D which pretty much
which which pretty much means that you
need to download everything because it's
present in almost all the emails and for
that is like we just paid for example
like half and second half a second for
256 megabytes of data and to add files
we edit files of different sizes so a
file with four thousand keywords take
something like quarter second to add to
our time storage scheme and then we also
evaluated our scheme on the document
data set so the documents are like Doc
PDF XLS that are not like that are like
rich text so there was no data set
available so we just collected one
gigabyte documents from google buy and
then we just filter them for just like
English documents because there were
other languages and so and then the
pre-processing for it is like for one
gigabyte document we just need to pay
something like 30 seconds and search is
also very fast for one gigabyte
something like seven millisecond to
search for the keyword D and adding we
added like 20 27 megabyte PDF file with
10,000 keywords and it also takes
something like 600 milliseconds and
delete is free because of our laziness
allergy and the search performance that
I showed it it includes the search
overhead that we pay for delete at
search time so in conclusion I would
like to say that the we have
find a more basic primitive that is like
the blind storage scheme it might have
some other applications the whole scheme
is simpler it's more scalable and secure
I mean we also believe it's more
practical because it does not need any
server-side computation the only and we
can just deploy it on Dropbox or any
cloud storage service with just using
the API that they provide now and for
the same and and as it's very simple
it's very easy to write code for it's
very easy to deploy and it can be used
it can be deployed on any commercial
cloud services like Dropbox so I have
written Dropbox interface for C++ so I
mean we can just use that to store files
in the Dropbox for any questions
it's a scheme has yes can you just get
rid of it around the first time so so we
cannot like get rid of it completely but
with parameters if you set your
parameters cleverly most of the time it
will be one round but for very large
files it might be more round
there are two rounds because the purpose
of the first round is back so if we can
say if if we can I mean one way one
trivial way of doing just like storing
the size locally which is very small
yeah yeah yeah I'll do we is like I mean
if we allow the so actually one problem
is that we don't allow server to do any
computation yeah so that is the end that
is a problem
yeah
yeah should be looking for advice yeah
and then what we can do is we can give
just just a seat to the server
any other questions so whistle gorgeous
it is it was 256 megabytes and under
skin yeah yes so I tried to run it on
the bigger data set but the problem is
that so
I have implemented it in in memory I
mean like we just store everything in
memory yeah for like one of the reason
was to compare it with your scheme
because yeah yeah so that's why we
stored in memory so I try to run it on
Wikipedia but so we have a machine with
128 GB memory but it was not work yeah
so all these comparison that I showed
you was done on my laptop so we need
more yeah
because you're gonna get like 16 blocks
out of that that's how big the bottles
are so you know the entire data second
huge
problems
so I guess many small so for many small
files we have a problem and we have
alread also but but if we have so one
thing we can do is like if we really
want to store like very large it is that
we can increase the block size such that
we compensate for that yeah yeah so if
we can't increase the block size and we
can compensate for that yeah yeah it's
something like 180 megabytes how do you
need to make it right yeah so so that is
one of the question like I mean we
cannot store index low P because this is
almost the day so why so why was it so
big because every document ID in the
implementation has four bytes just like
in an integer so then that there are so
many keywords and Enron data set that it
takes that much space so that depends
upon how much key word pairs yeah so key
word pair is like key word file pairs so
you need one integer for byte integer
for every key key word file pair because
you have this key word and then you need
to store a document ID of every file
that that contains this given
so we're trying to
but meant this with your standing thing
from
we didn't think about it carefully but
we but we are just like we believe that
it will work like I mean this scheme we
if we can adapt this scheme to their
setting Lots yeah and also to the and
and also to the multi-user setting where
they have so there's one data owner but
I mean so here the like this scheme all
most searchable in encryption schemes
they have just one user like he stores
it and then he literally he searches but
I mean they have this other model in
which you have one data owner but then
he allows multiple people to search what
they did so we hope we can also extend
it with that I mean like it it can be
extended to that but with carefully
that is cool yeah yeah we really
achieved adapt a second
you encrypted implying storage potatoes
yeah yes so we need yeah that's yeah
yeah we get it
hit this interaction but like I mean yes
so so from the start we want to have
some interaction to achieve get a ticket
yeah I mean that like then in so like if
we believe that the scheme that we have
is like if you if you pick these the
number of blocks that need to be
downloaded in the first round
cleverly then it for the most time for
the most of the time it will be a one
round yeah but like it is two rounds
yeah
back to the index sighs did you
experiment with like throwing
yeah yeah we did not because yeah I mean
we wanted to compare with other schemes
so we did not like we just tokenize it
simply like just using space yeah that's
why it stick I think so
yeah yeah I mean you can just get rid of
like numbers symbols and yeah I mean
that is one of the issue with this is
that we include head results because I
mean we cannot compare with other
schemes if you do not do that so but the
headers like take up like email headers
so if you just include that it they have
too much like unique keywords yes so if
you like just remove it I mean this is
so the searchable encryption can be very
efficient for example if you want to
store health record and the only thing
you care about is like diagnosis code
you just want to search for like some
tagging so diagnosis code is like for
each disease have some code so if you
just want to search for that then you
don't you just throw you all the data
and you just put that like in the index
me that's one thing so my advisor is
giving this as an assignment to
undergrad students to do this to see how
much local index is and how much locally
effect</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>