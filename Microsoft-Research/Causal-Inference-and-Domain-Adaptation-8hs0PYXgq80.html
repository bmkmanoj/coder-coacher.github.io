<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Causal Inference and Domain Adaptation. | Coder Coacher - Coaching Coders</title><meta content="Causal Inference and Domain Adaptation. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Causal Inference and Domain Adaptation.</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8hs0PYXgq80" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so it's my great pleasure here to
have Eunice Peters from ETH Zurich so
actually I got to know Eunice in the Max
Planck Institute in tübingen
where we boasted our PhDs Eunice has
worked on causality and it worked with
the greatest minds working on causality
all around the world so in you were
working with smart in Wainwright in
Berkeley with Peter spirits in at CMU
and also is our very own Leon Mbutu at
Microsoft Research and now you're in
Zurich but I think you will you soon
have your own group working all on
causality so it's very exciting to have
yonas here as an expert on on causality
and in particular because he he will
give us a sort of tutorial on on where
causality stands and I hope we all can
of profit from that inside so thank you
very much for for coming here yeah
thanks a lot for the invitation so it's
a great pleasure to be it's a very nice
building I didn't know it before it's
very nice town so I'm enjoying my stay
already I'm meeting a couple of you
afterwards I'm looking forward to this
oops there's a can you hear me I guess
it should be fine um okay so I I will
give sort of an overview talk maybe
about a few methods at least it's
limited time and let's see whether I
will have time to talk a bit about
domain adaptation which is something
that we recently looked at but most
importantly so please interrupt at any
times I know that it caused influence
especially if I'm not used to it sort of
it's a bit a funny way of thinking and
if something is unclear it's most surely
because of you because of me as a please
please ask me and I will repeat what I
said or try to rephrase it so it's
definitely not because of you
okay um so this sort of contains a
couple of work so these are the
collaborators from ETH is Peter boom and
Nicola mines housing from the NPI and
tubing and Dominick and pan hats
euros moyers now at the University of
Amsterdam and Siobhan Martin from
Berkeley
okay so and here's what I think about
causality and I'm more than happy to
discuss
if you disagree so I think they often
worked with iid samples from some Joint
Distribution P and my claim is at least
for this talk that in a lot of
situations we are actually interested in
a different distribution P tilde and to
give an example you may have seen this
before but it's sort of my favorites so
that I that's why I showed it again so
what you see here is the study from 2012
where people look at the chocolate
consumption in kilogram per person for
different countries and at the same time
at the noble number of Nobel Prize
winners per ten million and yeah
apparently there's a strong correlation
so you might figure out that Sweden is
sort of an outlier this tells you maybe
something about the quality of the
chocolate in Switzerland in Germany yeah
but in fact so this is in this is a
sample from a joint distribution in my
argument is that although this might be
interesting for prediction so imagine
you are country and you don't know how
many Nobel Prize winners you have you
may just measure the chocolate
consumption and then you get an idea of
how many Nobel Prizes you have won in
the last year's but this is a sort of a
prediction task but my statement is that
often we are interested in a different
distribution P tilde and what this what
would this be in this example so here we
are interested in sort of the causal
relationship between chocolate and Nobel
Prize winners so you can phrase this as
saying we are interested in a
distribution P tilde where we randomly
distribute chocolates so we take like
let's say we have a UN meeting and all
the presidents from all the countries
are coming towards us and then we
randomly tell them okay your citizens
have to eat that much chocolate every
year so now if you do this then you may
look at the data and if you still see
that there's a correlation in this new
distribution P tilde where you have
randomized over the chocolate then this
is sort of a quite a different statement
because then it really means well eating
chocolate helps right so this is a way
to phrase those those sorts of
interventional distributions so saying
well in fact we are interested in some P
tilde and then of course the next
question is well how do we relate P and
P tilde so I gave you an example
but it
obvious that if P and P tilde
arbitrarily different you cannot learn
anything about P tilde just by absorbing
P and so this is my argument so I would
claim that causality is sort of a very
useful language in this respect so
causality gives you some tools or some
phrases that are very well suited for
defining the relationship between P and
P tilde and this I would like to spend a
couple of minutes on sort of to make a
bit more precise what I'm what I mean by
this so the language we are using a
structural equation models so maybe
you're familiar with this if not it's
something very simple and I define it
basically on this slide so what is a
structural equation model we say that
the Joint Distribution has been
generated by a structural equation model
with respect to a certain graph let's
say this one if the following holds so
if we can always write a random variable
as a function of its parents in the
graph and some noise variables so here
x1 for example in the graph has one
parent namely x3 and the corresponding
structure equation says X 1 is a
function of x3 of its parent and some
noise
x3 on the other hand doesn't have any
parents
so it's just a function of noise so then
we have to make some more assumptions
and we can also discuss them so you can
relax them a bit one is that the noise
variable should be independent and the
other one that we are concentrating on
now is that the graph doesn't have any
cycles which is a strong assumption in
reality I think so one of the vation and
this is not so difficult to see I think
is that with this definition oops sorry
so this really gives you some sort of
distribution so if I tell you the
structure equations and by this I mean I
also tell you what are these functions
and what are the distribution of the
noises then you can generate a
distribution with this information a
simple way to to see this is where you
just think about simulations and you
write a program and what you do is you
first simulate from this noise variable
you pluck it through this function and
you get samples for x3
so then you take the next level let's
say this one for x1 you already have the
samples for x3 you generate samples from
it and one plug it through the function
it's good centers from x1 right so this
this tells you if you're given a
structure equation model you have a
Joint Distribution okay so the example
that we have seen before and so here I'm
making the assumption that we know the
ground truth namely that so this is at
least my guess maybe you have other
ideas the correlation between chocolate
consumption Nobel Prizes probably comes
from some economic strength of the
country so in rich countries people
spend more money on chocolate and
research so this is my guess so this is
the example and now if we assume that we
know the distribution P here's what we
can do so this is our structure now we
observe some data so let's say we also
have access to this these economic
indicators now what we can do is the
following we can compute statements in
the new distribution P tilde so how do
we do this so first of all I specify
what I mean by the distribution P tilde
and so what I am interested in is by a
distribution P tilde they replace this
first structure equation so here I'm
saying well I'm not interested in sort
of the chocolate that is eaten because
the country is rich but I really want to
so you can think of a randomized study
here so I want to break this dependence
so what I do is I replace this
structural equation and I think this is
a pretty neat language so what you say
is start with the distribution P we
consider like a certain structure
equation model and then we are just
replacing this structural equation here
and now this is something that I will
not talk a lot about but now you can do
all kinds of inference so if you observe
data for example from the distribution P
but in fact you are interested in the
distribution P tilde but you know the
structure equation models so here for
example you can try to estimate these
functions and the conditionals if you
like so then you can make all sorts of
statements and then you can ask ok is
eating chocolate really helpful in
getting a Nobel price or not and you can
do proper statistics so if you
have access to the full distribution P
but only two samples you can construct
confidence intervals ends of and so on
okay so this is another example it's
very simplified that we have been
working on at Microsoft advertisement so
this is another structure equation model
for advertisement placement so again
very simplified but you you may think
that one a user has some sort of
attention so he wants to buy some
organic apples whatever and then he goes
to a search engine like Bing and puts in
a query and you also have some user data
like the location of this person or the
time of the year or whatsoever so then
this is the mainline reserve I should
say so this is parameter that you you
may be familiar with so basically it
tells you how many well it has an
influence and how many ads do we want to
show on the main lines so this the area
above the search results and then at the
end of the day you have something that
you're interested in namely whether a
person clicks or not okay so now you can
gather information so you can gather
data from this system and this is again
a distribution P but again you might be
interested in a different situation
namely in the distribution P tilde there
now what you do is you replace this edge
here so you place this conditional and
why is this because this is something
that being for example it has full
control about rights so this is
something like the the parameter setting
so given the user data this is a
parameter you can think of a value that
takes oil parameter that takes real
values so this is something that you can
change you can control about and because
you want to make money at the end of the
day or you want to get clicks you can
ask questions like well how many clicks
would I get if I replace this structure
equation so if I replace my parameter
settings by something else
so maybe yeah designing an experiment
that allows you to figure out what
pillars
yes and no I think so because here we
don't that's sort of the nice thing
about this is that we if we only have
data from the distribution P but we are
interested in this new distribution P
tilde we actually don't need data from P
tilde to infer something about the
system's behavior in this new
distribution so I I don't have a
blackboard I guess but
so in principle what you can do is this
is sometimes called inverse probability
weighting for example so now imagine a
specific situation that something we
looked at you're interested in the
expected number of clicks in the new
environment P tilde so what you could do
is you could take the the old samples
that you gathered in the distribution P
and now you rebate the samples because
maybe some of the samples because of
some randomization here some of the
samples look pretty much as if they
would be coming from P tilde and those
are the samples that you want to give
higher weights to so in a way you are
using the information that you gathered
in the old environment to make a
statement in the new environment but so
you are right in the sense that if you
want to sort of check this for example
you should be able to also physically do
this right because at the end of the day
maybe you're saying well I can propose a
new advertisement system that uses a
different parameter setting here and
that seems to work much better and sort
of it's nice if the people can really
try it alright so if you say well we
will get much much much higher number of
clicks if the users would like to buy
more things then this is sort of maybe a
valid statement but how do you make the
users to buy more things it's sort of
it's a bit vague there okay is this sort
of roughly clear okay so now sorry I'm
again is this is another example but I
don't go into the details here so I
don't tell you precisely what are the
best statistical methods to sort of
estimate the behavior under the new
distribution but in a way this is so
what I wanted to stress here is that we
are using the causal language to go from
P to P tilde and the rest if you like is
it's more standard statistics okay and
now there are a couple of questions so
one
what do you do if there a lot of hidden
variables and this is for example
something that you can learn a lot about
if you read Judea pearl the book say I
found this this book amazing because no
matter how often you have read it you
read it again and you find some new
stuff in it but this also has a bit the
problem it's very difficult to read it
when you don't know about causality so
there are lots of things that seem a bit
weird and one reason is that sort of
he's dealing a lot with these questions
so can we infer something and this is
the case here if we don't know anything
about the user intention or if there's
another variable that sits up up here
and influences a lot of other things so
can be still although there are some
hidden hidden variables that we cannot
measure can we still make sort of this
this causal inference and this is if you
like again I'm I won't talk about this
right now but this is something you can
read for someone should be a pearls book
the next question is and this is
something I would like to address in the
rest of the talk is the following so
what happens if you don't know the graph
right so what can we do then because I
as I said if we are given the the sort
of the cause of knowledge meaning this
structural equation here so they
structure the the graph and the
distribution P P we can use both of this
these things to infer something about
the new distribution P tilde but what
happens if you don't have the the
structure equation model and this is
something so they they only have the
distribution and this is something I
would sort of maybe briefly tell you a
few ideas what you could do then sort of
the problem clear okay otherwise just
interrupt yeah because I I read the
pearl book and I did encounter the
difficulty that you were just mentioning
is very good I mean maybe other people
say different things but I think there's
none so I'm I'm giving I'm giving a
lecture at the moment at the ETH or I'm
starting to give a lecture so I'm
preparing a script and I'm I mean it it
also depends a bit on your background
right but if you're coming so I studied
statistics and machine learning and if
this is your background I think there's
there's no good book at the moment yes
but the working on it sense of yet let's
see I mean they're trying what would be
the background and the look okay so
maybe put it this way they are M the
Judea pearl book it's sort of its
specializing on the question what
happens if I know the graph okay and
then he's answering as I said what
happens within variables there's another
book from the CMU people so Peters Burt
is Clark Lima Richards shyness so what
they are addressing on is M so they have
the constraint based methods that I will
also briefly mention in the talk later
and they have sort of this focus they
focus a lot on independence ease and
conditional independencies and then you
have another book that comes from some
other books actually that come more from
a statistical point of view where you
may have heard about the potential
outcomes so this is a slightly different
language and this is also a bit a
problem that people are inside causality
are fighting a lot about which language
to use where is it would be nice if they
just say well this language is very well
suited for these kinds of problems and
these for these kinds but they are not
so they're really fighting which
language to use and they're this is
really this is quite a different sort of
group of people working on it and in UK
for example if you go to a UK causal
inference meeting there you have a lot
of people they'll work with instrumental
variables and this is something probably
don't have time for this I also have a
few comments on the last slides and this
is something that also comes from really
like a practical application so where
people have medical studies they have
like treatments where you have like
different phases and you have to
consider what happens if people figured
out that they take a placebo what do and
then stop taking it so what how do you
deal with this so in a way you have
these people that are quite different I
think but it would be nice to sort of
merge this a bit more okay so and so for
the rest of the talk I would like to
present you three ideas that have a
different flavor of how you could try to
learn the structure of
the graph and the first one is something
sorry we're a bit surprised so this got
up and this got actually picked up by
slash dot I'm not sure whether this is a
good idea or not
so the comments are a bit funny to read
but so now I will tell you about this
revolutionary new statistical test
actually not somewhat revolutionary it's
very simple and it's based on the
following idea so again I told you about
this fractional equation models this is
yet another graph the same story as
before but the problem we are trying to
solve is the following so assume we are
given access to a distribution that has
been generated by this structural
equation model now the question is can
be somewhat infer sorry for the
abbreviation here deck means directed
acyclic graph can be somewhat infer the
structure of this graph from the Joint
Distribution it would be nice of course
if we can infer like all functions and
all noise variables and so on like the
distribution of the noise variables but
now let's sort of make it a bit easier
this and say can be infer the structure
of the graph and the answer's no and so
I don't prove this year but intuitively
the reason is that this model class is
extremely general so imagine that you
have a fully connected graph so with a
lot of edges then sort of these these
functional models they are extremely
general so also imagine now is another
way to look at this imagine we know that
x4 has two parents namely X 2 and X 3
and we know that X 4 is a function of X
2 X 3 and some noise now try to fit this
model so it is it possible to find this
function f 4 and then it will be very
difficult so this is also another way to
look at it why this model class is so
incredibly rich ok so we this sort of is
the bad results it's sort of negative
the question is whether it's hopeless
and so here's then something that we
looked at in the last years for quite
some while because it turns out if you
reduce the complexity of the model class
so if you make the assumption that the
noise adds in an X in an edited way
rather than the function X jointly on
the on the variable and the noise then
it turns out that the answer is positive
so as long it may be stated a bit
frontier but as long as you are not in
the linear Gaussian case so if you have
Gaussian distributions and now the
statement is as long as the functions
are not all linear then you can identify
the structure so this is what's written
here so we assume that everything is
Gaussian distributed then V the
statement is the deck can be recovered
from the Joint Distribution as long as
the function is not linear and also I
mean I don't state this here but this is
something that's known under the name
lingam so you also have this statement
if the function is linear a linear then
the Gaussian distribution is the worst
case so this means in a way the linear
Gaussian case is not identifiable but
the rest is this is sort of a bit funny
because the general so you saw that the
very general model is not identifiable
and the very specific model is not
identifiable but a lot of things in
between are okay so again because of a
lack of time I won't go into like the
intuition why this is the case but I'm
happy to discuss this afterwards as well
okay and now what we did is the
following so in a way in the causal
inference domain I found that what is
also a bit problematic is that if you
look at real data and you try to
estimate a graph structure then in many
situations you just get an output and in
most cases you can somewhat reason that
this is a very good output because if
you look at variables like I don't know
income and height and education then a
lot I think a lot of results would sort
of look very good so what we tried to do
is we collected data sets where we know
the ground truth so these are now I
think something like 88 data pairs of
random variables where we know which of
the variables the course and which is
the effect so this is a specific case
because we only have two random
variables but we would be very
interested in looking at higher
dimensional data sets as well and so now
I know that this sort of causal
inference is a bit maybe a hoodoo part
of machine learning and some people
would say well how is it possible to
infer something about the causal
structure from just observational data
and so if for those people I always like
to show this this scatter plot here and
this is 41 so I won't tell you
- because and which one the effect right
but if I would ask so maybe we can do
the test so who of you thinks so let's
call this variable Y and this variable X
so who of you thinks that X is the
course of Y who thinks then Y is the
course of X okay so we have a few said
just okay because of the amok recei we
have we have perforated to that X of f
decided that X because so I mean you're
right in a sense that it could be
bimodal but okay so first of all the
truth is that X is really the course and
Y is the effect so I think X is the day
of the year and Y is the temperature and
so one reasoning would be that sort of
it could be the other way around but
somewhat this model would look very very
weird because you have very like small
support for these Y values and then
suddenly it becomes bimodal as you said
correctly and maybe there are some
examples like this but some of the other
models seems to be much more much more
natural to us okay anyhow so when you
say you can reconstruct the structure
does this mean assuming only four nodes
no no no this is channel this is you
would also be able to rule out another
hidden cause for two of these I've been
working on this but it's not included in
this slide so in in this slide we assume
everything is observed so the reasoning
is so the the precise statement is if
the distribution is generated by this
model with a certain graph it cannot be
generated by a different model with a
different graph and now if you have
hidden variable that's the intuition you
will not find you will not be able to
find any model that fits the data so you
do a model check and in this way you
could at least so this is the first step
you could say well something is fishy
here maybe it's the there's a hidden
variable or maybe there's a model miss
specification but at least you can say
well I I don't know for sure and then
the next step is try to fit a model
including a hidden variable and this is
a bit more difficult
you say that email be able to say no at
the moment you will be able to say I do
not know yeah okay so this is sort of a
try to and include this as a benchmark
data set and all I want to show here is
that I'm this is not a prove that causal
inference works but I think the results
are sort of promising so what we are
what we should be looking at is this
additive noise this is the red curve and
what you see here the y-axis is the
number of correct decisions and thanks
for the question
so because in some situations you don't
want to make a decision because you
think maybe there's a confounder or none
of those models fits very well so that's
why we included the the x-axis as well
so what does it mean so here 100% means
you always have to decide on all data
sets and if you go further to the left
that means you can make less decisions
and as you see this is also encouraging
I would say it's not a proof or anything
but it's encouraging that the the
examples where you are pretty certain
about are also more correct which would
indicate that if you are lucky and date
if the nature really generates data in
this way you have to be lucky because
it's a model assumption but if you are
lucky then your your decisions seem to
be more often correct that incorrect
different methods yeah I mean yes I
think we all skip this we can discuss
them later yes so we use things like so
we are also happy I mean we publish this
data set and we are happy to receive
comments on this what we usually did is
this is I'm not saying this is perfect
but this is the best we could come up
with before I think six people
collecting data sets and we always chose
the one where all of us six people
agreed before seeing the data so one one
found the data set is sent around the
variables and then if all of us agreed
on which is the course then we put this
in these are variables like very
different kinds so okay you have seen
day of the year temperature there's
something like availability of clean
drinking water and baby mortality rate
there's some financial things some
physical things like sunspots
so they try to include which if these
are only two variables then which one is
likely to cause the other but ruling out
confirmed yes is almost impossible yes I
agree I mean so there we hope that the
confounding is not not too strong but at
least then you would say well at least
this partial knowledge about the ground
truth which is the course in which the
effect at least this is correct and
maybe as a side note so I mean again
this my very if you ask different people
working in Consulate II but what I
believe in is not that you use causality
as sort of a black box you look at some
observational data and you get a ground
truth with 6,000 nodes of biological
let's say genes and you get the correct
graph so what I believe what causality
could do is that it hints it gives hints
to the experimentalist so it the the
biologist analyzes some data or I mean
we discussed this earlier you have some
advertisements system or something where
you observe some data and then maybe the
causal inference can tell you well this
looks like an interesting variable maybe
you should try to intervene on this one
because in biology for example you
cannot I mean it's it's very expensive
you cannot do all the experiments and if
you have this point of view then maybe
it's also not that bad if there's a
confounding factor because the what
Metis is really the direction of the of
the H in this case because then you
would say well if you really try and in
these cases and try to intervene on the
note that sort of the causal inference
person to suggest it then you will see a
response and your experiments as a side
norm and like did you reason about the
number of readings associated with each
sort of relationship you guess because
you could sort of categorize saying okay
yeah this is a very deterministic
relationship I do I believe that there
is no other hidden involved in this
relationship
in other they might be a lot of a number
of other hidden we don't make a
distinction so usually when we know that
there's this strong confounding and a
lot of hidden variables so even if the
chocolate has some effect on your
thinking ability this will be a tiny
effect to the huge effect of the
confounder so this is when we when we
say the confounding is very too much
they don't include this so but it would
be interesting to have categorized
examples like this okay so now I mean
what you can do it in in simulations
this is easy theoretically to analyze
this much harder so we are working on
this right now and it's the question
about model miss specification so what
happens if you have an additive noise
model and now you are perturbing this
with a confounder so in simulations it's
not surprising so what you see is at the
beginning you still find the correct
direction and if the confounding is too
strong you say I do not know because
it's a model miss specification and
analyzing this I think is pretty
important from a statistical point of
view but I also found it very difficult
like equation velocity and you also have
some of them included very very clear
maybe not maybe we should yeah so I
think in in general no we didn't make a
distinction so I think the general rule
was this very small confounding included
but we should I think it is a good
comment okay so this is sort of this is
sort of the first idea that says well if
you make structural assumptions then you
are able to gain in terms of
identifiability and now comes the second
one this is the more classic one but I
think it should not be missing it should
at least be mentioned this is something
that we call now independence based
methods some other people call it
constraint based methods so this is the
work by for example the PC algorithm
which stands for Peter spiders and clog
lima those are the people who invented
them and the idea is as follows so you
may be familiar with the usual graphical
model stuff so if you look at a
graphical model
so it sort of doesn't you see that it's
a slightly different flavor because you
don't need the language of structural
equation models here so if you just look
at the graphical model what it does it
and it encodes some in dependencies
right so here for example you see that
the distribution is Markov with respect
to this graph this is what we usually
assume and this implies that the
variables X 1 and X 2 are independent
and you have conditional independencies
for example X 1 and X 4 are
conditionally independent given X 3 but
you also have some funny ones so for
example X 1 and X 2 are dependent if we
condition on X 4 right so now what you
do is the Markov condition sort of gives
you one direction it tells you if you're
familiar with this separation so if you
have a D separation in the graph you
have an independent statement in the in
the distribution and now the trick is
very easy all what you need to do is you
need to assume that it also holds in the
other direction so and this is what we
call faithfulness so you assume that as
soon as you have an independent
statement you have the corresponding D
separation statement in the graph and if
you make these two assumptions so they
are really the reverse of each other
then you have a one-to-one
correspondence between D separation
statements which are just graphical
statements of the graph and conditional
independencies in the drawn distribution
and now what you what do you do you just
you're given the Joint Distribution so
what you do is you choose your favorite
independence test check for all
conditional independencies and
independencies and then you scratch your
head a bit and look for the graph that
satisfies all of these conditional
independencies a couple of problems so
oneness and esta kate this year so this
is not unique and you can think of a
Markov chain for example x causing Y
causing Z you will not be able to
distinguish this from Z causing Y
causing X so this is one problem and
this is why I wrote like different
graphs here so this basically this is
called a mark of equivalence class but
the the principle works as follows so
here they said find all conditional
independencies and then we select the
graphs that correspond to these in
dependencies and you're all computer
scientists or at least familiar with
this of course this is not feasible so
you try to be smart about
and this is really I think in this
respect the PC algorithm is quite genius
because it sort of does this checking of
Independence tests in a very smart way
maybe a few comments so if you try this
on simulate the data it does not work
very well and so there are a couple of
reasons one is of course these are very
difficult problems condition
independence testing I mean here you
only have to test given one other
variable but so if you condition on five
variables in nonparametric independence
tests we have been working on this a bit
I think it's pretty much hopeless unless
you make a very strong regular
regularization and the other one is M
you also because you make all these
statements if you make a mistake at the
beginning you are more or less lost
there's also bit problematic multiple
graphs just and just the quality of the
difference between independence and
causation right well the different
graphs would imply different causal
models that all agree on the
independence model yes I agree but it
was a jump you from Independence to
causation there is no way partly because
it's in this sense for example I mean I
didn't write down the graphs but I think
so here for example this is what we call
a V structure so if you look at the
combination X 3 X 4 X 2 so this is a V
structure because both arrows are
pointing downwards and this is something
you can identify and the reason is quite
so forget about X 1 so assume that this
is not existence so then you find the
independence X 2 is independent of X 3
but if you condition on X 4 suddenly
they become dependent and this structure
is the only one that satisfies these
independent statements so in general I
would agree so there is no jump from
independencies in general from
independence these two causal statements
but if you really believe in
faithfulness which is another issue so
then in some
situations you can do this and I mean
this is very involved now but people
analyze now because if you want to do
something like consistency your uniform
consistency you have to say that you are
not even close to faithfulness and
people analyze this and said well okay
this is a pretty strong assumptions so
faithfulness is maybe a reasonable
assumption but what they call strong
faithfulness saying that we are not even
close to non faithfulness this becomes a
very strong assumption but this may be a
bit involved okay but this is sort of
another idea that you can do so you say
well we know that we have this
correspondence between the
independencies and the graph structure
and now be a bit sort of clever about
this and assume that this really want
one correspondence and then we can try
to exploit this and this is also what
people try to do okay so if you're happy
with this I would like to spend maybe
the last 10 minutes or so on something
that we recently looked at because I
thought so I find this I could imagine
that this is also interested interesting
for machine learning in general okay but
are there any more questions about this
part good so then we come to the third
idea so what is this and now I come back
to what I said before about this that is
sort of afiyah if we are given the
structural equation model you can sort
of generate new distribution speed tilde
by exchanging structural equations and
now we will turn this a bit around so
first of all we set a slightly different
problem it can be generalized but we are
interested we have a target variable so
you can think of a prediction task for
example so this target variable we call
Y and now we are interested and whether
we could somehow find the cause of
predictors of X 1 because of predictors
of Y in this case it's X 1 so now we
will do the following so we will
actually assume not like this is now
different than before we don't assume we
have a Adid observations from one
distribution but we assume that we
gather data from different environments
and now the idea is based on the
following observation and in a way it's
trivial but we will make use of this so
if you change some of these structure
but you leave this one as it is so you
do not exchange the second structural
equation the one of your target variable
then what happens is that this
conditional P of Y given the parents of
Y this is always the same okay so some
examples so this is something we have
seen before it's called a randomized
experiment so the randomized X 1 it's in
a way they are intervening on X 1 this
could be one of those environments and
if you look at this for a while you see
well this didn't change the conditional
distribution of Y given its parents it's
sort of also in a very trivial because
the conditional is specified by this
structure equation right
we cannot like have all sorts of
different environments so here we are
changing the structure equation for X 4
or you can also change like a couple of
structure equations the idea is even a
bit more general so you can also
introduce I don't know cycles or hidden
variables you can change quite a bit but
for now we assume that you do not change
this conditional here ok and now the the
overall ideas if you like
in a multi task setting maybe if you
observe data in different environments
we will actually use this variation of
the data for identifiability
so it's really a kind of different idea
than what we have seen before
ok so how does it work formally so I
will concentrate on the examples maybe
but just to make clear what the problem
here is so we have a target Y and some
covariance X and now we observe this in
different environments so we always have
like different distributions and let's
let's say e is the index for the
environment and then our assumption is
that we have this what we call an
invariant prediction so this this
conditional y given its causal parents
and so here we write this as we have a
linear model that holds in all
environments so in all environments we
have that Y is a linear function of X
plus some noise variable and here's you
see the noise variable doesn't have any
mix and the the coefficient doesn't have
any next so this really the same in all
environments just as a site so this
doesn't have to be a linear model you
can if you like you can use a kernel
machine here or you can use the deep
neural network whatever you like it all
what you need is that this model is
always this
in all environments okay and then um
maybe this is the the last difficult
concept so we sort of we look now at
different sorts of predictors okay so it
said S is now a subset of predictors and
we say that it's a good subset if we
really get some invariant prediction
okay and then the last concept that we
need us we look at all those sort of
candidate sets of predictors and we say
if there are some variables that appear
really in all of those good candidate
sets s these are sort of the interesting
variables so if you see that some of the
variables you really need to get
invariant prediction then you say well
this is a very good good variable okay
and so this is the thing that they're
after so this is I will show you now a
few examples at least this is all in the
population setting so we can discuss the
finite samples offline I think I won't
have time okay so this is an example I
don't know whether it's a good one so
here we have whether we eat chocolate or
not this is whether how good the weather
is and the target is whether we in a
good mood or not and then this may
depend or this may influence whether we
sleep well in the evening or not
so it's Cupid example I think so now the
target is what are the causal parents so
this is x1 and x2 okay so this is what
we are after this is sort of the good
set s star now if you have only one
environment so we if you have only like
observational data this is something
something I say the sentence let's see
whether it's understandable say then we
cannot say anything so then the problem
is we always get in variant prediction
so even if you now regress Y or nothing
on the empty set which seems like a
stupid thing to do but even if you do
this you get in variant prediction
because you have the same model in all
environments which is just one
environment right so then well if the
empty set is the good set then you don't
say anything but now it becomes better
so now I make sure you changing the
environment I'm also going to a happy
place where I can eat much more
chocolate
maybe it's cheaper or whatever so what
happens now is that if you want to find
a model that holds in
environments then you really have to
include x1 as a predictor because in
this so in this environment they eat
much more chocolate I'm also on average
I'm happier so this means if I try to
explain the mood by noting this sort of
doesn't work because it's not an
invariant model because here I will get
like different residuals then I would
get get here so this means in short so
you really need this variable x1 to get
invariant prediction and this means that
we call this in identifiable cause a
variable and now maybe you can already
get get a glimpse of this I'll skip this
M so this is what I would like to say
here if we see more environments this
always helps this is sort of nice so if
you get data from more and more
environments more variables become
identifiable this is the third one and
some other statements that are quite
easy to prove as for example this one if
you have only one environment you cannot
do anything so then you do not output
anything and this one the top one I am
now doing it the other way around so
this is the my favorite one so you can
prove that whenever you are outputting
something so all the variables in the
set they are correct and this is sort of
I think what is nice about this message
so in a way you are only outputting
something if you are pretty sure that
this is the causal variable so you are
guaranteed that this is a like a subset
of the true set and now what happens is
that the more environments you see so
the more variation you actually see in
your data the better this is for causal
identification okay and so I'll skip a
few slides now maybe as a side note this
is all population case but because we
are in statistics the finite sample case
sort of it's trivial because what we can
do is we just do this statistical
hypothesis test and then you get
statements of this form so that says if
you have finitely many data you still
are correct with a very large
probability okay this probably not
possible so I skip this simulations so
what we did I again maybe only the I
mentioned the data said that we looked
at so again we were wondering whether
there are some
data that you have some sort of ground
truth but it's not like this toy data
said that we collected before because in
a way you already know that sort of the
day of the years causing the temperature
not vice-versa so what we found is this
data set it's biological data said there
you have a lot of genes so it's a high
dimensional data set the number of
dimension is much larger than the number
of samples you have some observational
data and so now this is what we treat as
a different regime we also have
interventional data so we're the
biologists deleted some genes so they
always had targets and what's nice about
this you get some ground truth why
because it looks as follows so this is
the activity of gene of this number and
this is the activity of this this gene
and the blue dots very hard to see
probably from the back this is the
observational data so this is what the
genes usually do so there are some small
correlation that you cannot see here but
this is what's usually doing now if you
knock down or if you actually delete
this gene number five thousand nine and
fifty four what happens the activity
goes down this is good this tells you
the biologists did something reasonable
but at the same time you see that the
activity of the gene 4710 also dropped
down and this tells you that there's
really a causal relationship so you have
because of these experiments you have
some knowledge about the ground truth
and this is what we looked at so I found
I mean we spent quite a while on this it
whoops a lot of things to learn these
real data sets
I found the results quite promising I
think it's a difficult data set and sort
of our method proposed a lot of correct
hits actually okay but let me sum up the
time is over I think so what I tried to
show in the beginning these three ideas
for causal inference so the first one
this was this additive noise so we're
making structural assumptions to gain
identifiability the second one these are
the independent space methods so there
you're trying to connect the graphical
structure with the independent structure
and the third one that was way too quick
was this invariant prediction so there
you gain identifiability by observing
your your system
in very different environments and what
is this I would really like to stress
again what is nice is you can control
the family-wise error rate so in a way
even if the biologist tells you these
are two different data sets and they're
the same even then you will not make a
mistake so in a way this is a quite
robust method I think lots of open
questions so this is like the idea
number three was focusing on the target
variable we could extend this to graphs
combining these ideas would be nice I
think nonlinear models I mentioned this
this will be fine okay this is an
instrumental variable setting that I
think is nice domain adaptation I didn't
spend a lot of time on it and but I
think sort of maybe you fear that there
should be a relation because you're sort
of the course language gives you a way
to connect all these different
environments and we are looking at this
at the moment and then instrumental
variables is something that's very
relevant and practice it has sort of a
different a similar flavor because here
the instrument variable sort of tells
you in which regime I'm in in case you
have seen this model before okay for
this I would like to thank you for your
attention and how different do those
environments have to be yeah I think
it's very good point so this is what I
didn't talk about this but this is what
I am meant by finite sample guarantees
because now sort of one one answer would
be the method does not care because if
the environments are not very different
you're conservative so you are saying I
do not know anything so then you can say
well just take the data and look how
different the environments are right but
then what is sort of nice about this I
do not know answer is that there are two
reasons for this so one is maybe the
environments are too similar and the
second one is maybe we have only seen
too few data to actually realize that
there are differences but these
differences may be
small right so this is I mean for the
method that does not care it sort of
combines those two methods of like not
being sure the sources of not being sure
together but it would be nice to
understand this theoretically so saying
if the environments are that different
how many samples do I need to actually
use this this kind of difference and
this we don't know yet so we sort of
have this standard causality toolbox or
like you like the answer but there is
but an hour so the thing is so there are
some versions for example of the PC
algorithm and so I remember when I was a
PhD student I downloaded three of them
and all of them had different bugs that
were extremely hard to find so I mean
there's a like the group in stir in that
I'm working on now in ETH the statistics
group they have some senior scientists
and sort of they have permanent
positions and one of them said that it
would be nice to maintain this Kosala t2
books and therefore this is called PC
arc and so they are trying to include a
lot of C code but the interfaces and so
on this is all written in R but I think
this is so they are maintaining this
they're putting a lot of effort into it
so I think this stuff you're finding
there is correct and the name is
misleading so we also included a lot of
other masses so it's not only the PC
algorithm but the package started with
this so that's why it's called PC alch
so this is something that is becoming at
least in statistics more and more
popular because also you need a lot of
like graphical tools for these
separations and so on this is all
included
it seems that all this existing research
has a sort of classical statistical
flavor to it like worst-case events
abominate probability or whatever were
some assumptions but it's a lot sort of
also work in trying to post it as a
Bayesian problem so I have a certainty
on the causal structure and then doing
inference over possible causal
explanations because the models itself
okay so this exists
um I mean David hecka meant for example
it was like there's also a nice paper I
think by Daphne Koller that is phrased
being Bayesian about Bayesian networks
or something so I mean the this work
exists a bit a question is so it's a bit
bizarre I think so because what they did
this is what they're usually doing is
they're saying okay from an independence
based point of view we cannot
distinguish between certain graphs so
now they have a score based methods like
a likelihood or a posterior approach so
what they are sort of doing is they are
putting a parameter on the like a prior
on the parameters that leads to non
identifiability in these independence
classes that you cannot distinguish in
this mark of equivalence classes not
sure whether this makes sense so somehow
I think there was always this
frequentist way of thinking saying well
we cannot distinguish between these two
graphs so this omits set our priors such
that we cannot distinguish them any more
afterwards and so David mchavez was one
of the earlier people who has said I
think well this is stupid right we
should make use of the fact that if we
use reasonable priors in the parameter
space maybe we can use this to actually
identify which of the ones is the more
more likely graph but I guess people
were a bit cautious about this because
they said well there should this
knowledge come from but I I think you
can you can do this and a few people are
trying to do this Phi for me it's
already difficult enough in the
frequency space but I think one should
be doing this now regarding the first
person if I may add some is something
because so for me this is actually quite
interesting for me to see like also some
statistics people working on it a
man-machine - because as a statistician
your I totally agree so you have
family vice ever write that they're
quite happy about but then as in a
machine learning and it's a very good
VHC student who is now starting to work
on this um from a machine learning front
of you maybe this is not what you want
to do rights because maybe if you want
to do transfer learning you do not care
about only the cause of predictors but
maybe you want to get rid of the
predictors that you really don't want to
use so this is something sort of it may
be a different flavor of looking at the
problem and I think both views actually
very well sort of justified point where
he related causality with
semi-supervised learning that and talked
about the causal directional predictors
this is a very neat idea I think so
um so something that I didn't talk about
and that also leads to identify
abilities the following idea and it and
again it's an assumption but the
assumption says that think of two
variables that somewhat the mechanism
and they are now very vague about the
terms I'm using so you have to look at
concrete examples to make this
well-defined somewhat let's say X is the
course and Y is the mechanism there y is
the effect sorry so then you're saying
the mechanism so some this thing that
turns X into y the course into the
effect this sort should of should be
independent of the course so somehow
this should be this mechanism should be
independent of like the distribution of
the course and the reasoning is that
sort of if you get like smoke a lung
cancer from smoking some of this and
internal mechanism this should be the
same in all different people right so
why should it depend on the distribution
of smokers or something so we have seen
like if you like these edit of noise
models maybe it's a debatable but they
may be an instance of this why what is
the the course is just the distribution
of X and the mechanism is sort of you
can think of this as being the noise and
we assume that the noise is independent
statistically independent by from the
input ok and now so this is sort of
losses so to speak you have the
furniture class together with the noise
is the
yes and yes so in a way we fix the
function class it's a bit vague no but
you can also if you like you can also
consider the function and you could say
and there are some methods based on this
if the function sort of coincides a lot
so let's say the the areas where the
function has sort of a low derivative or
something
if this corresponds by chance with the
like the input of having like a low
density region or something then you say
well this is a bit funny real as this
come from so this is an assumption right
so yes they're saying the mechanism
should be independent of the input and
now so what is it the relation to see
mr. Voyles learning I think it's pretty
neat this means the following
so in C me service learning we are
interested in the conditional Y given X
right so we are trying to predict Y from
X and C me she was running we have a lot
of unlabeled data meaning only data from
X that we are sort of using for
improving our the estimation of the
conditioner well now if you believe the
assumption I stated before means the
following so if X is the course and we
said that sort of the cost should be
independent of the mechanism this means
that no matter how much you learn about
the course about the distribution of the
input this will not tell you anything
about the mechanism because we assume
that they are independent whatever this
means right so and so then this means
that see my similar nning can never work
if you're trying to do it in the causal
direction it can only work if you're
trying to do this in the antequera like
anti causal direction which in in
practice I think happens a lot so if you
think about I don't know image
classification or something you want to
know whether there's a lion in the
picture or not then usually what you
have are pictures this is the X and you
have the fact the the label is whether
there's a line or not and the label here
is the cause of the picture right so
they often you do this in the under
causal direction and so this is
something that we looked at so we looked
at all the service about simi service
learning and we checked the cases where
it was working and we didn't find a
single setting MercyMe well some of us
learning was working in the causal
direction so I mean I'm not sure whether
this proves anything but I think that
this sort of way of thinking can help
also an other area
years of machine learning it's sort of a
it gives you some some other insight
that is in a way orthogonal to what
people are used to think this is why I
found this neat great sorry for acting
overtime thanks a lot and afterwards we
will go to the to the public evening so
if you want to join just drop me an
email
thanks give us again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>