<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Power and Reliability in Extreme Scale Computing | Coder Coacher - Coaching Coders</title><meta content="Power and Reliability in Extreme Scale Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Power and Reliability in Extreme Scale Computing</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BvXUAc7yqjc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay it's my pleasure to welcome Amin
and sorry here from the University of
Illinois I mean did his PhD at the
University of Michigan and has been on a
postdoc position at Illinois he comes
very highly recommended and has a great
stream of accomplishments in
architecture and so I'm really looking
forward to hearing your job talk which
you told me has been practiced many
times so it'll be perfectly smooth all
right softly puts it later with a
pressure on me okay ah good morning
everyone and welcome to this talk and
thank you so much for an introduction oh
I went to University of Illinois after
winning national science foundation
computing innovation fellow and started
working with Joseph Torrell us and after
finishing my PhD and today I'm going to
give an overview of what I've done the
last few years on a topic that I think
is a central computer architecture as we
get to the end of Moore's Law the title
of this talk is optimizing a power
efficiency and reliability in extreme
scale computing and well here I'm going
to start by looking at the significance
of power and energy efficiency as you
know improving energy efficiency has
many implications and here I'm going to
start by looking at a few of these
implications the first thing is there
are many devices that are backed up
rated and they are becoming more and
more popular such as pdas laptops and
medical devices and basically enhancing
the energy efficiency can prolong the
battery life of these devices for more
true put oriented system such as data
centers a reese's cost is a more
pressing issue and this cost includes
the cost of cooling electricity thermal
packaging and enhancing their energy
efficiency of in such a domain can save
millions of dollars for typical data
center per year are there other end we
have the device lifetime
hi power consumption leads to our
hottest spots on the chip and since
we're our failures are highly dependent
on the chips temperature enhancing the
power efficiency can prolong the life
time of all the computing devices and
basically if we focus on power and
energy efficiency we can enhance all
these different aspects at the same time
now we are looking at a trade-off
between energy and reliability I have a
plot that shows the energy per operation
on the y-axis and have supply voltage
and x axis and this is for a typical
CMOS process we start with the dynamic
energy and as you can see by while
increasing the supply voltage we have
this quadratic increase in the energy
consumption for add the next is the
leakage energy and as you can see for
the small effort at low voltages we have
this exponential increase in the leakage
if you put these two together we're
going to get the green line that shows
the total energy of the operation and
there is an optimal point that is at the
very low voltages now if we consider the
cost of reliability this is going to
change a little since that ultra low
voltages will have an excessive amount
of voltage noise you need to deal with
the timing failures that are arising in
that region and preparing these faults
but defining this reliability
reliability meaning that you can
accomplish their operation that you're
supposed to do so if I went out though
the reliability is going to go way up as
well you hunched through oxide or
whatever so where what kind of beautiful
yeah this is mostly for a up to like one
vote for example 2.2 volt that is the
reasonable voltage range yeah you're
right if you go to like 5 volt probably
you have your we're out failures will be
so like a dominant that you need to have
some other reliability mechanism at that
point that's true liability back
here is just fit flip student boys
mostly timing timing faults that meaning
that the circuit cannot make the timing
constraint that you put on it and it can
cause a bit flipping out but that you
have good to understand sub lula and i
understand that that is a you design the
circuit for that supply board because
clearly if you if you reduce the supply
voltage is the same circuit you wouldn't
lose more energy that's so so big as you
reduce the blonde you thin out the oxide
that's true yeah sorry I don't agree
with your answer so if I'm having timing
violations I can just slow down slow
down my clock your slope you know when
you're moving you're moving your voltage
you're slowing your block any this is
assuming their alpha power model for
clock clock is scaling at basically
you're not gonna slow down your circuit
by like a factor of a thousand eggs in
order to allow it to operate its
assuming that like your semi linear
relation between the voltage and
frequency how the circuit will act yeah
you're right absolutely if you are
willing to reduce the clock frequency by
like thousand X basically you can let
the safety door correct results yeah is
where you're going to solve exact i'm
assuming loses its availability causes
for underlying operating that gets
detected early not not or not to cause
mischief because if i don't know i
become became active as an airbag on me
that may be expensive when the next
lawsuit that's true
demented ICB else
okay so if we put this a reliability
cost we add this reliability cuz the
green curve that we had we're going to
have this red curve here that basically
there is a there is a substantial shift
in the minimum operated in the optimal
operational voltage of the operation
toward the right side but still one can
decide to operate that they're
reasonably lower voltages that at that
point you don't need to go just let you
have this sorry thank you take away what
put your units that let you add energy
and reliability what's the unique what
is the units of reliability cost it's
basically in terms of this is a
schematic plot showing the trend so
there is no exact number on it but in
the cost is basically the cost of
detecting the fault recovering or
repeating the operation or rollback
basically any mechanism that allow you
to repeat the calculation and get the
correct result so the units are energy
yeah definitely this oh that's the
answer the question so you're assuming
that there's there's an energy costs to
be table I've exact which any penman and
you can do a redundant computation but
you're doing it with a checkpoint based
architecture something like that problem
is yes or no yeah I guess yes
so we either can operate that the region
like I don't know what one wall that you
wouldn't observe any failures due to the
process variation or you can choose to
go for these lower voltages and try to
operate at the optimal voltage value and
basically save some energy out there at
that point and you need to have this
aggressive energy efficient techniques
that can trade off reliability for
energy efficiency and still can tolerate
some of the errors that are happening in
this region and the rest of this talk i
will look at the ways that we can
achieve this low voltage operation by
efficiently tolerating some of the
errors that are happening in this region
i oh god sorry are you making a
distinction between errors that occur
vs. errors that have an impact or not
these are the quite errors or false you
mean that false that you can detect and
you need an can isn't correct is
actually manifested architectural stay
eventually yet you need to correct it
for example had a bit flip in an
uninitialized portion of the cash that's
that's not a that's not an error under
your definition or it is an error you're
talking about architectural masking and
like Mark architectural masking and
different up with different type of
phenomenon is in that category here by
failure we mean that something that can
potentially corrupt the architectural
state of their of the program that I
have one more question notably absent
when you shifted from power to energy is
any discussion of performance and we
will discuss their four months later but
at this point the thing that we are
trying to do is basically assuming
linear scaling of the performance with
voltage how this circuit will react
basically yeah but that's a very good
good question that energy and power
basically there's a motion of
performance there as well and you need
to look at your presupposing your letter
G point is your desired operating point
and I'm not sure that it is
we are targeting like new x-ray shows
domain and for those type of systems
basically the main criteria is energy i
think at or at least we are assuming
here is like energy is a very big factor
like if you care about the battery life
and like cost of calculation like
electricity bill like some of those kind
of map today energy but you're right
people use it because they care all the
energy don't stop for us which is why
you're useless yeah i guess as i go
through the talk andrew present some of
the schemes you will see there is a
variation between the skins of a prison
some of them more care about it like
getting a constant true put off out of
the system some of them are willing to
sacrifice some of the performance that
you can get but this is the general
trend i want to just to discuss that how
how the behavior of the system is as we
go for the lower supply voltages so now
looking at this low voltage operation
vdd is known to be the best level for
energy efficiency that can gain a very
big reduction in dynamic power and
steady power and as I mentioned we want
to have this near threshold voltage
operation that means we want to reduce
the supply voltage to a value that's a
bit higher than the threshold voltage
which maps to around 500 millivolts for
the current technologies and their main
advantage I will get is that we can get
a very big reduction the power and
energy but the main drawback is the
speed of this the chip can go
substantially lower like a 10x is that
something that's very typical and
another thing is that we will have a
substantial increase in the gate delay
variation that's a big obstacle in the
way of this low voltage operation and
lastly everything
okay so you make a bold statement that
the eruption is the best lever for
energy efficiency I have discussed this
with many people and actually this isn't
I wouldn't be comfortable saying best I
would prefer that's true i would say one
of the best
okay it's definitely one of the best I'm
not sure whether it is the best one well
I would argue that specialization gives
you a much bigger they're interested see
the DVD guy that's true but for general
purpose computation like to see the
techniques you can do a very limited
you're right if you have like a
completely application specific
processor it probably gain gains like
North several orders of magnitude better
initiations that's true angry makes you
understand so you say 4x on energy
permutation you do it ten times slower
and can you save for X on my energy so
the 40 X is good Mercury Venus all right
hey you do near threshold and not sub
threshold because we are not going to
discuss of treasure because because in
general i guess the sub-threshold
circuits you expect a very very large
raised in the fall straight very very
large and you need a completely new
techniques at the circuit level to deal
with in general the architecture
techniques can deal with failure rates
that are that are reasonably high but
not super high like not one every out of
ten instructions will go wrong something
that's going to go wrong for
subthreshold you generally need to deal
with failures at the very very
aggressive manner and also the
performance that you get out of the
circuit is very low in general mostly
sub-threshold applications that I've
seen are for like medical devices that
like you put a chip before like a
pacemaker in the heart that performance
is pretty much not a relevant issue so
and this near tradeshow we're still
hoping to get a decent performance out
of the system
so as I mentioned the last bullet get a
variation is one of the obstacles that
we have here and since it's an important
issue i'm going to provide a little give
explain a little bit more on what is the
parameter variation and how it's going
to affect the chip this is defined as
the deviation of device parameters from
their nominal values and 12 these
parameters or threshold voltage and
effective channel length which are more
important for the modeling purposes and
here i have two plots and the one on the
left hand side shows the static power
versus threshold voltage and when you
have a nominal chip basically there is a
nominal threshold voltage and that maps
to a certain static power as you add to
the parametric parametric variation the
thing that happens is that you will have
some devices that have lower vth and
some of them have a higher we th and the
ones that have a lower BTW ish since
this curve is exponential while when you
are going for the lower vth values the
ones that are at the end and at the
lower end of this curve they will have a
exponentially hired leakage power so the
thing that happens that the net effect
of all the devices that you have on a
chip will be a substantial increase in
the leakage power of the system and on
the right hand side you should not
purchase and this is the variation I
this is static power of a particular
device of one device but inside the chip
when you add to the parametric variation
some of the devices will end up having a
lower vth some of them higher we teach
and the ones that have do every th the
static power is much higher therefore
the net effect that you will get across
the chip is basically a substantial
increase in their steady paw on the
right hand side I have a plot that shows
the distribution of path delays for a
pipeline stage and on the x axis we have
the delay and since the path that has
the worst timing characteristic will
determine the click clock frequency that
a town on for example is their delay is
their corresponding frequency of the
chip in this case as you add to the
party
variation this plot becomes more flat
and the tail will shift to the right
side and some of the past will have a
much worse timing characteristic
therefore the clock frequency of the
chip needs to be reduced substantially
as a result both power consumption and
the delay become pressing issues while
we are targeting low voltage operation
alright here i have the roadmap of this
talk our objective is to enable low
voltage operation of high-performance
micro processors with comprehensive
low-cost solutions and i have working
for these processors here threshold
regime actually that's that's the
objective of this talk I will try all
right you can write you can see how it
goes so I work in several different
areas but given the limited time that we
have in the rest of this talk all
present three solutions that have
proposed to tackle this reliability and
energy efficiency in some of the main
components of a high-performance
microprocessor we're going to start by
looking at network-on-chip our present
tango let's the work under submission
it's a collaborative research with Intel
Labs on there DARPA and your grants and
the objective is to dynamically do the
voltage adaptation based on the errors
that we observe in the network after
that we are gonna look at cache
hierarchy i'm going to present
archipelago which is the worker
published in hpc 2011 it's a highly
flexible cash architecture that
tolerates the SRAM failures at near
threshold domain and at the end i will
present in chrome answer which is a
solution for processor pipeline it's
published in esco 2010 and the extended
version was published in 9 30 micro 2010
and the objective here is to protect the
processor pipeline with energy efficient
solutions and at the end of the talk i
will present some of the potential
future research directions and also give
a brief overview of my other research
accomplishments so we're going to start
with networking chair what is the impact
of variation on NFC
network contributes especially
vulnerable to variation this is because
this routers and links connecting very
distant very distant parts of the chip
and due to the systematic variation they
will exhibit very different speed and
power characteristics across the chip
therefore you need conservative voltage
governments to tolerate the process
variation and it has shown in the prior
work that and network on chip can
consume up to forty percent of the chip
hour therefore since their power is
strongly proportional to the supply
voltage there is a great opportunity
here to save some energy by reducing
some of these carbons that has been
added for tolerating process variation
here i have here to demonstrate the
impact of variation on the routers i
have a plot that shows the probability
of error versus supply voltage for a 64
router to the mesh with one voltage
regulator / router and in this plot as
you can see there are 64 caves and each
of these curves show the failure rate of
a single stage of a single router and
described would be by having a voltage
regulator for our office versus our
culture yeah we have one regulator pair
router inside the chip
it is a mesh network for example that
you have many routers so the coverage
area is 64 voltage regulators day that's
a very interesting question I have
multiple answers to that one of them is
that in order to do the limited study of
how much can we potentially get we did a
study with 64 routers per chip we can
change the scheme so that you can
combine some of the routers and put like
four or eight of them in the same
voltage regulator another way to
implement it that there are el Dios that
there are like low drop of voltage
regulators and you can implement this
thing in a hierarchical fashion put like
each I don't know four by four a sub
mesh inside one of their voltage
regulators and use el Dios close to each
of these things since the variation in
the voltage of each of them was very
small basically the thing that L do is
doing is that it can vary the voltage by
a very small amount like hundred
millivolts for example and given the
systematic variation the ones that are
close to each other should kind of
similar behavior so you can set the
voltage of the whole like a sub mesh
like a 16 16 router sub mesh with a
regulator with the on-chip voltage
regulator and then tune each of them
using an L do which is very very high
efficiency loca that was a great
question so as you can see we as we go
from high voltages to the lower voltage
of the thing that happens that the
failure it is extremely extremely low
and as we go higher we basically go to
this steep curve and gradually we get to
a failure rate that is almost one and
which meaning that all the time you're
going to fail we're going to have
failure in your computation and this
curve is very steep that shows a process
variation has a major impact on their
routers and one thing that I want you
guys to look at is that at the error
rate around 10 to the minus 18 which is
a relatively lower rate that allows a
fault-free operation for a long time you
can see there is a very large variation
of the voltages across the chip of
my from almost 530 millivolt all the way
up to 750 million volt and this means
that if you can have any scheme that can
adapt the voltage of each router based
on what is the minimum vdd which at
which it can comfortably operate you can
save a substantial amount of energy in
the network our estimation shows 30 to
40 person can be easily achieved I think
this chart is showing systematic process
variation for across the chip was the
setup but you know arguing or is that
incorrect I go on because but now you're
arguing that you want to have her router
or much more local control of these any
this chart to me says we ought to be in
our parts if we do already but no no
these are network is on in one chip so
all the routers are visiting once you
how can you bin ich that's how I read it
but here earlier was talking about
systematic process very HTS what hits
his network on a chip inside wants you
so if you manufacture one chip inside of
that chip if you measure the voltage at
which that each of these routers can
have this error rate that's what you're
going to end up having like for snow
these curves is a single route inside
the chip and grand that entire routers
assumed to be at some process value but
different routers have different
variations of the problem we actually
use that variant which is towards they
will
university of illinois ins that does
process variation modeling at very
detailed manner and basically we did the
synthesis and we got the net list of our
different routers and basically
calculated logic effort of each of those
paths and kind of like feed them to
variant to get this result here so it
includes the impact of variation on each
of these singer routers all the curves
are identically shaped over there yeah
all of them pretty much have the same
shape meaning that as you increase the
as you decrease the voltage the failure
rate moves in a similar manner like
increase in similar manner and are ugly
so what's the main idea behind our
approach basically we are trying to
achieve high energy efficiency by
removing their vide is that a margin
that is added for variation where all
while keeping the frequency constant so
here's the kind of answer to one of the
questions that log ask that what's going
to happen to the frequency and the
performance here our objective is to
keep the frequency of the chip constant
and try to remove any margin that is
added for a variation without purposes
and we reduced well the way that we are
going to do this is that we are trying
to reduce the VDD of each of the router
to the minimum level that it can
comfortably tolerate and we start by
having a high voltage for each of these
routers as you saw in this plot
something around 800 is the safe voltage
and all of them can offer it correctly
and allows the fault three operation and
we periodically decrease the VDD and we
monitored errors that are happening
inside the system and if needed we're
going to increase the voltages and we
rely on some inexpensive error detection
since we want to have only a few bits
added to each packet we're going to use
CRC and since we want to do the error
checking as infrequently as possible
we're going to do end-to-end basic once
we do it encoding at the source node and
we do the error detection at their
destination
this is more of a protocol questions
this is something that we've been
wrestling with Scotts Scott I'm sure has
a lot of thoughts of this too so if
you're going if you're not going to run
the network at a guaranteed reliable
State yeah then you're going to start
flipping bits in your packets that's and
you're going to put your router in all
sorts of very very scary condition
that's right so you can you can route
stuff randomly you can change the
destination though how do you you know
how do you guarantee that your duck at a
deadlock your your your network I've say
that any protocol guarantees on top of
that if you're not going to drain
packets that have been corrupted here
are at each dough that's a great
question I will describe the scheme and
I will try to answer your question there
that's a very very great question okay
as a result the thing that tango is
trying to do is dynamically changing
their supply voltage of each router base
and the errors that observing inside the
network and that this allows us to adapt
to the workload phases temperature and
also they wear out so here I'm going to
describe how we handle the errors inside
tanga we want continuously monitor the
errors and when an error occurs the
destination node drops the fleet and
waits for a retransmission from the
source node at the source node we have
watched your time and for each of the
fleets and when the watchdog timer
reached a certain value this source node
sends a signal to our reliability
management unit and ask for a vdd
increase given the deterministic routing
the reliability management unit unit
knows what subset of routers it need to
increase the voltage on because it's
like a XY routing and if there's a
failure it knows that what's up to the
route is this packet supposed to go
through so it's going to increase the
voltage on that path how does the source
term though that this little drop source
node is has a watchdog timer that if it
doesn't get them
if you don't get a knack after a certain
amount of time you know that something
was wrong and then you tell your
reliability management unit that
something was wrong do something for me
here's me and here's what was the
support here's what is the destination
that this pack is supposed to go through
I shall have examples of this thing in
the next slide I don't need example I
was jailed Harless yeah just you're too
you're assuming that that the source
that is going to buffer everything until
it receives a knack and what if the app
gets corrupted and gets us a world
that's a very good question the same
thing happens basically if the axe get
corrupted the source node still didn't
get the axe so it's going to have a time
on so it knows that the routers that
this axe supposed to go through has some
problem as well but how does it know
whether or not to but so now it's going
to retransmit something that's already
been correctly received no no but you
joined voltage increase and then
retransmit but yeah so I think with doug
is getting it now is that the
destination got the message in and so it
believes it's moving forward but it's
going to later on get another copy of
that message because it's at got dropped
that is true I guess it has a
destination node we can either use the
reliability management you need to tell
the destination know that okay this is a
duplicate message because that one knows
that what message is going to get to the
destination node as a duplicate because
that's the thing that that's the request
that is that from source now surviving
the revival of edges so we're all the
network right is a distributed unit yet
inside
this is embedded in a hierarchical
fashion all there all the source nodes
are connected to that unit
so you're going to get another copy of
the message and so whenever you get a
message you have to talk to the
reliability management you see if you've
already received this message no no
realize the management unit can send you
like an interrupt that says that okay if
like because first it increases the
voltage on a particular path and then
the packets will go through so it knows
that which node will eventually I don't
care about the voltage increase because
I could just assume that it does are
drawing the first time it doesn't write
the second time yeah that's pretty much
with that yeah so because you know other
things can corrupt that we're packin see
you can't assume that it's always
correct you just yeah he's the voltage
and your probability of adair is lower
so now so whenever I receipts are from a
destination whenever I receive a message
I have no idea whether I've already
received this message or not because
this could be a retransmission yeah but
they realize its management he knows
that you're going to receive this packet
again right because originally when this
packet got transferred and then there is
a time order the source node source node
knew that that this packet needs to be
transferred again so when it sends that
packet to the reliability management
unit and says that if a problem happened
in this packet the reliability
management Union knows that this is
going to be retransmitted and it already
knows the destination for that so so
which is there is some network between
the reliability management units
the source and destination all those
you're getting things had and is that
also just as likely to be buggy or do
you have something to you know and that
operates at the higher voltage always a
network is always operating at maximum
voltage or 800 million volts so that you
don't observe Everson
so I receive a packet crc checks out I
receive another packet
and how when I received that packet how
does the network the rod the management
told me that that's a I mean I could set
a bit in the header that said this was a
retransmit but I vote I mean you want an
answer his answer there can be even
shirt yeah one of the ones and I was
proposing was that reliability
management unit basically knows that
this packet is definitely going to get
retransmitted so it can tell the
destination know that this there will be
a duplicate of this message so you can
just like just send the neck and drop it
or as you said you can have a bit what
I'm uncomfortable with I don't care
about the reliability because I can also
set up date in the header this if this
is a retrans yeah that means I get just
their state but but when i received this
duplicate pack it what do i do with it
this is wrong and send an act to
distortion but how do I know if I've
already consumed it or not 'if the crc
ching so sir here's a scenario i'm sort
of hard for this i'm not sure this works
so i received packet a yeah and what's
that I think he's okay so okay keep
rolling if you yeah alright i receive i
received pakka day i I'm going to
consume it i'm not going to buffer it
waiting for an Akha by our j Mithra III
consume it now and i sent an act back
the ACT gets dropped or rounded
somewhere else somebody else receives a
nap god knows what happens make it
disappear yeah i'm making his appear all
right and actually the crc will catch
that yeah alright so now i send packet b
which is the same as packet a and i have
some state that says that this is a
retransmit and now i received packet be
do i consumed i could be no
but what if what if packet a was dropped
and i got packet be like a packet a was
sent to the wrong place and it's at a
time down then it says pakka P&amp;amp;I get
packed be is ok now great i can knows
what edit this nation you know that for
example you see that you have a number
for like a packet number or something
and you know that the other you consumed
it or not cry and when you get the
duplicate you know whether this number
matches that one or not so you're so
you're saying that i need to keep a log
i need to come up with some unique ID
for every packet i could sue in it
instead of people at a time a log of all
of those packets and then do a cab
against them to make sure that the
package that arrived isn't sitting in
those yet not for like the law wouldn't
be that long because that that would be
the latency of basically doing this
retransmission fit and changing abusive
network how can you bout the lazy
congestion is actually like the chapter
in their paper like how we deal with the
congestion and I had you about how you
do you see we're going like I don't I
know where you're going I know how to
solve it but I don't know whether you
want me to take you there yes okay so
all you need in the destination is a
piece of information for each source
which says what is the packet ID that I
have successfully gotten and I'm going
to make a rule that says once I pack it
you're guaranteeing it or delivery we're
assuming in order okay right and so what
you do is when a packet arrives if it is
the next one you increment that value
you consume it you send the aggregate
yeah yeah okay and then if it is either
too large or too small compared to that
you if it's too large you drop it if
it's too small you drop it and back in
right and I think you're fine is that
what you do found to be interesting
we assume that the mechanism can be used
to handle this reason about this problem
okay rather than puts a water dog timer
is a router and having an ultra reliable
network to say I got a bad network could
contribute to the water up timer in the
reliability main unit and then you send
message that things are okay and
arriving start jacking up the voltage
what are we too many messages that's an
interesting question so you're saying
that put their watch the cameras in the
reliability management unit and send
basically acknowledge the things that
are wrong or the right only the inland
and with a note says things are all
right don't do anything if a note
doesn't say anything right jack up the
voltage then it can be as unreliable as
the rest of it you may have many more
package I don't know sir that's a
different way of designing it I think
that makes sense I don't know what what
are the trade-offs here basically if you
put the watchdog timer it's an area
liability management unit well my guess
is that you need probably a lot of a lot
more communication with the reliability
management you need to realize what's
going on with different packets but
that's an alternative we were trying to
keep the reliability management you need
Network very rich like low bandit so you
don't need so much communication on that
one where the gold doctor yeah I'll I'll
talk a little about like what is the
mechanism that we're using basically to
change the voltage so here i have an
example of what's happening to the
supply voltage of a particular router
over time and as I mentioned we started
a relatively high voltage around 800
million volt and we gradually reduce the
voltage at the beginning of each epoch
basically an epoch is the time period
during which we first decrease the
voltage and we monitor the network to
see what their errors happening or not
if it's happening we're going to
increase the voltage and we start with a
relatively high voltage tuning a step
and as we gradually get to the lower
voltage values we are going to reduce
this tuning step as you can see for
example the first step is around like 60
millivolts or 70 millivolts and
gradually goes down to around 10
millivolt which is kind
getting to their physical limitations of
the voltage regulator and for example
the thing that's happening in impact for
is when we are going down to around 610
20 millivolt we are going to observe
some errors in the network we're going
to raise the voltage and we continue the
operation we're going to decrease and in
the epoch 6 you will see that some other
errors happening and basically the thing
that's happening is that you gradually
converge to the value that that router
can comfortably at operate at you will
see errors every now and then but the
overhead of fixing those errors are very
very marginal compared to the time we
know that we have here so if I
if I am within a nanosecond of an epic
boundary and I get a request to or
whatever your smallest amount is and I
get a request to increase the voltage
you are in fact going you get an error
you're going to increase it decrease it
almost immediately mean it okay so do
you I still don't understand how you can
guarantee they don't deadlock given that
you're allowing corrupted messages to
flow through the through the mesh I'm
sorry to keep harping al-watan oh that's
blue I actually kind of enjoyed under
station so I don't mind so happens so so
if i flip a bit and now it gets routed
to the role in place and that's
happening arbitrarily in the network now
i can end up with a i can end up with a
circular dependence of resources and
deadlines that's not going to happen I
didn't show you the example of how it's
working and then we're going to discuss
this particular example yeah so here I
have a very very simple example I think
that you guys already have a very good
vision of how it's working basically
it's a four by four mesh the darker
color means that that router has been
affected more by process variation and
has the worst timing characteristic and
the lighter one means that it has a
better timing characteristics so we
imagine that the ones that are lighter
we hope that they end up with the lower
voltage values and the one other darker
with the higher law higher voltage
values we set everything to 800 million
volt we want to transfer a packet from
core 1 12 core 14 it goes through the
base based on deterministic routing goes
through X and then goes through the Y
and it gets delivered without any errors
after a few epochs the thing that's
happening that voltage of all the
routers will go down to around 700 may
volt and if we want to transfer the same
packet the thing that happens is that
router 10 yeah we're going to have a
fault then it's going to go to the next
one have a fault and then deliver the
thing that's happened is that I'd router
44 we're going to drop that packet raise
the voltage of all the routers on that
particular path and basically do a
retransmission
and in the next and the next epoch
basically I have another example of
another path that how the voltage
changes happen in this path now going
back to the question that dog has if
what's going to happen if you have
deadlox so how can you have a deadlock
b.c the lock means that you have it this
problem starts at the particular node
right are you assuming store forward ah
yeah
unless assume that this is this problem
stirs at the particular node like lists
let's figure out this particular example
that we want to transfer this packet
over this path at some point this
problem starts to happening right stored
for each router so you're buffering the
whole message before you set any data
for that message of the next router on
Anton ship network no no no so you do
work all right yeah it's one more ok
so okay ok so now so I have a soldier's
to the point you're making I have a
problem and router 1 comma 3 and it
sends a message down and I have probably
some other router that sent a message up
and now the buffers are full and they're
both waiting for boxers to clear but
they're depending on each other ok so I
think where Doug is heading was that the
message you want a message that where I
actually corrupt the destination address
correct yeah yeah that's ok i guess it
did not one scenario that i can imagine
is that the route the packet is going
through the forwarding paths and at some
point you will have a failure and
basically gets to a loop here for
example something like that yeah that's
this is scenario try to describe my ass
classic classic yeah Harry and cute
level yeah ok so the thing that's
happening is that the voltage of this
rocker was wrong so it's doing something
incorrectly right no the voltage the
voltage is only going to affect the
error rate yeah okay and you feel about
you up the voltage you're going to
change your going to improve the error
rate but I've already incurred the error
and now I've got these I've got a sink
with deadlock where the buffers are full
that the best just can't better than our
collage then flush the buffers okay so
only won once the day incorrect CRC is
detected at a destination correct uh no
no no and I wanted source me ok so the
source of it oh yeah you didn't get an
act yeah which means you're going to up
the power yeah and you're going to flush
the entire network no no not that I'm in
them just the ones in her town just
their office but but i may let them do
it yeah because i could have sent two
routers i could instead of message off
that path the message left the path yeah
but either doesn't message yet they
leave it to the destination or it does
you say that it's not going to guard
here here's the scenario oh alright I'm
routing from the top part of the bottle
pourer yeah
right and yes this sounds unlikely but
we're running a large scale so now i
have an error where I'm now going to
route that message down this way no all
right so the big gets love it is a long
message so it's going to fill up think
you actually want to have the flip when
it's coming down to go over well well
actually let's just say go down find my
example and now I flipped another bid
now I transcended from here to here
right it's our two errors of the message
and now it's routing through and
actually trying to route through itself
oh okay sure I mean you can do it you
can construct a more likely example with
two messages right and and and no and o
ICS you just now have but it's all off
it's all I have yep I see it but I kind
of see what you're seeing the one answer
that I have is that forget those those
nodes are completely wedged but they're
not sending or receiving did not get a
timeout this guy x actually doesn't get
it ash he flushes the buffers along that
half of the message to off that path
everyone is that their likelihood of a
message getting into such a such a
scenario that basically two faults are
happening such that you higher
probability already said that so don't
don't never never argue that the
likelihood of the devil oxygen is low
all right just remember we're Microsoft
we've run it lore video our detail so
any coordinate kids can happen that's
true tically okay I guess the only
answer that with having that scenario is
that you probably need to flush all the
routers inside your network that's
that's the only way that's going to fix
it at there at the beginning and out one
of them is you can tell us why the
message that was going down through from
our 23 r33 can't make that turn before
i'm going to die on me about how Doug's
stuck thing is eventually going to get
flush I think you might have an answer
for either one of those yeah I think one
is that foot
of the XY routing which it cannot go
through that particular path it cannot
go probably it's very it's doesn't it's
actually possible I mean that in if the
fault is in a very very particular
manner that as dog is saying that for a
larger scale system it might happen you
it is possible so i can see that
happening yeah that will go from y 2x
routing in your error yeah it's not my
two weeks routing it is 2x2 I routing it
comes here excellent next one yourself
now I think by saying the are 23 r33
flowing message yep cannot possibly go
to our 32 is that true or is it not true
this message going here it can if its
destination gets flipped at our 33 I
don't think Ken he's guy believed you
have a router that does all exes before
wise yeah but so if you had a message
that was moving on why you would never
look at it and send it to X because that
path doesn't see one for Senator that I
can get a gene that can happen is that
this one says send a message to here and
basically you're going to send it here
then this one says send it here and
basically in a loop you have this fault
patterns that are like complementary
this one says look it's very very
unlikely but I can see that happening
let me give you your second hand at horn
let's take Doug's scenario Doug scenario
is you have a tail the dog chasing tail
one message are too 3333 22 to you
frankly that's okay tell me what happens
in that Network when I go forward now
what messages are delivered in that
region giving the other messages other
messages in your network are trying to
be delivered in that region are they you
can
they can be delivered that message is
going to flow so the other other things
are not going to get it's stuck buffers
are full no fluid can be transmitted but
why the buffer zone them because they're
filled up with the message and they
can't empty allocated until the message
makes forward progress that message
can't because the head the head of the
message is waiting for a buffer refill
them to be transmitted but what if all
of them try to go through the same path
and where scholars going is that
somebody is eventually going to try to
transmit and they're going to type out
come on then I get anymore going to
watch that kind of you're gonna push
them right that's the very interesting
answer yeah I can imagine lock stop
stuff in which case it stopped stuff and
your system will eventually recover or
it doesn't stop stuff and if it doesn't
stop stuff doing your hair yeah
thank you so much or I guess I describe
this particular example so the thing
that's happening is that eventually
basically the system will converge to a
state that is that the voltage of all
the routers are getting close to their
optimal values and the main reason for
this is that the routers that have a
worse timing characteristic basically
will end up being at the intersection of
a larger number of paths and those paths
that are intersecting cause this routers
voltage to go high up faster in a faster
rate compared to the other ones
definitely for the very very small
networks this might be not a correct
statement but we realize that for
example for a 16 and 16 note amesh and
upward all the simulations will end up
having a similar will end up with the
similar behavior so in summary we show
the dynamic route oriented approach in
order to save energy in network-on-chip
it adapts to temperature workout phases
and we're out and we try to achieve this
in the presence of process variation by
keeping the frequency unchanged reducing
the voltage of the each router to the
minimum value that they can operate at
comfortably and we efficiently tolerated
some of the occasional errors that are
happening in the network applying
tangled who is 64 node mesh we were able
to reduce the energy consumption of
network-on-chip way around a and
twenty-eight percent and this came with
less than five percent overhead for a
wide variety of benchmark suites so what
I'm talking about flushing the network
down yeah
so when I flush a path I'm going to kill
a bunch of messages that may be using
routers love that may be sitting along
that path and right from ya can and so
then they're going to death they're
going to nap and so i'ma let you going
to flush that path so if I've got a
highly congested network why doesn't the
flush create a cascade of flushes gonna
end up with live walk it is possible i
can see that happening that if you flush
one path i think it it might make more
sense to say that they're going to flush
the whole interval this epochs are
pretty long so you still have to say
problem in that you've warned you killed
a bunch of messages and all those
messages are going to try it out and
they're good i be no no but when you
flush you're going to reset their time
of reach of them basically so how can i
processen so the first detected watchdog
failure yeah is going to be assumed to
be the only problem you can detect at
that point I bet you flush everybody you
penalize that pan and then you restart
and even if there were other problems in
the system you're going to detect them
with some few native and that's a chance
that's what this okay yeah I guess I
didn't follow that I still don't see no
I've killed a bunch of messages I don't
know if they've been delivered or not so
they're buffered at the source yes and I
have to wait to get it back yes and I've
killed a bunch of naxx maybe of acts so
I'm going to find out and then when I
time out something's gone wrong in the
network and that timeout could be a
deadlock so I have to flush yes so every
time based on your mechanism every time
you
you flush the network you're going to
kill some good messages if there's any
traffic in the network and then you're
going to cause some time out then you're
going to flush again the is that last
sentence where you're wrong okay so i am
not exactly sure how he does it but i
believe he the first timer goes off for
the first message that is found to be a
problem first time out and he will then
somehow wave a magic wand and during the
night drain the net well just dump
everything the network and then tell
every timer that the messages you had in
flight are not going to be considered to
be bugging in and of themselves they're
going to be bugging because of the path
i had and so retransmitted and don't
peen a lot and reset the counters and
don't penalize them for having been
caught up in this I think it's subtle
but I think it's doable yeah I wouldn't
want to write that code assuming you
have the mechanisms microcode probably
bear a lot of you first so wild dogs
thinking I i buy it I'm just trying to
think of their corner cases there's
worried about the amount of source
buffering that we have to do and I'm and
you have to puffer and you have to have
the detection 164 nose to see ya the
best actually buffering and the source
known as a very important thing actually
I mentioned we have some collaborators
from Intel lab and buffering at the
source node is the main thing that they
mentioned about this work that might be
a limiting factor we try to do it throw
a study for the buffering and one thing
that we realized that around six
buffering 64 fleets at the source node
is in office for a 64 node mesh network
to have a relatively like to have a low
performance overhead and make the system
kind of working and not stuck a lot but
that's a that's a that's a big drawback
basically
okay okay sorry the source buffer you
need is 64 flits yeah that's was what we
try the not sound that's an empirical
study it's completely what about how
many messages is that it it's we assume
I guess seeks Sixth Fleet Air message
yeah
then when you when you run out of
buffering space you just you just stall
the process of generating so you said
that you're the voltage / router tends
to get close to the optimal yeah how do
you know that or have quantified the
reservation our status static analysis
pacing visually a static analysis as a
showing like they're one of the earlier
thoughts that we have a have an idea of
what should be the voltage of each of
this router is going to end up to and
basically at the end of the day would do
a comparison and we see that these
voltages that we get across the system
are close to the ones that we got
through the static analysis you exactly
envelope so for how many cycles is a
router to router roughly just just the
link itself or there oh my cell donor to
gets tit for a quality cycle proud that
the link is that's probably you can as
soon as one sec but your closet you're
crossing clock debates aren't you you
know as if we keep the clock constant
that's the main of jig a scandal
originates yeah so we already crossing
voltage domain be
devising cell 3 and you need voltage
shifters but it's not as bad as crossing
the frequency domain voltage so but is
it that I mean there's a free in terms
of Todd it is not expensive all right so
let's say four cycles which sounds a
little optimistic speed but four cycles
and you've got a 64 their network so
your average message transmitted time
will be how many cycles anions for
outside of 12 c 3 or 2 1.5
going to state it's gonna be like
yeah yeah it'll be your 8 + 8 10 so it's
40 cycles for the first data packet to
get through and how many cycles doesn't
take to get one flit through this in
your size to what Robert the flit is how
many cycles on the network if your flit
was tiny 18 1068 is a sign Mullin
winning one of 20 bits and how wide
release is 64 yes those two 22 cycles
all right so so that's the noise and an
average message is 6 flits that's 12
cycles so you're looking at let's say 50
50 cycles transmit a message on average
across the network and then it's 50
cycles to return back that's a hundred
cycles round trip how long does it take
you to flush the network
that's I actually don't have it okay so
that yeah because we because we've
changed to flush policy so 100 cycles
for message if I've got six messages
source buffer that's six hundred cycles
yeah well I guess you could pipeline
those but what is your injection rate
into the network how often does a
processor said I don't remember okay we
don't have a lot of experiment
benchmarks of my imagination that way
what I imagine that there's different
injection rates for different vendors
sort of little slots yeah and see what
given the given the size of the window
you know how add the number of messages
in flight how many you need to buffer at
the source before you started throttling
this is going to be a lot of stuff
little t matter of catalytic cetera yeah
okay well we didn't vote for it okay so
in the second part of the talk i will
discuss some of the work that we did on
the cash and the main reason that i
present the tango about 10-15 minutes
left in your talk all right travel this
would you mind jumping ahead to the the
necromancerr you like North um I just
alright so since I have limited time i
guess i will present a necromancer which
is the work that is done in order to
protect the processor pipeline so
assuming that i have presented the cache
work as well we've presented some works
that can protect network-on-chip in cash
and in order to maintain an acceptable
level of yield for the prop for the
whole process since the processor
pipeline also consumes a substantial
area of the chip we want to have a
mechanism that basically protect this
part as well however this is more
challenging since the processor pipeline
is inherently more irregular for example
in a network on chip all the routers
look the same on the cache most pretty
much everything is SRAM so is easier to
basically have a redundancy based
techniques to repair that is the
structures here we are trying to not
rely on redundancy based approach and
yeah use an alternative to make their
det-cord do something useful and
basically enhance the system throughput
in the next few slides i'm going to
describe how we're trying to exploit a
functional a dead call to achieve a
better system throughput so given that
that dead core contains a fault we
cannot trust that core to execute the
program even for a short period of time
and then the question becomes that how
can we exploit such a core the approach
that we're taking is to use that core in
order to accelerate another core we have
this debt core that contains a hard
fault in it and we add a little core to
the system that we called anime torque
or any metal core is basically an older
generation core running the same I saw
with substantially less resources and
then we sorry chipper get rich
so animator core is running the main
program and we let the dead core also to
run the same program and we abstract the
execution information on the det cord
and extract some useful information from
it that we call hints and we send this
hints the anime torque or in order to
enhance their performance of the
animator core here in order to see how
much speed up we can potentially get I
have a study that shows the IPC of
different alpha course normalized to the
most simple one from left to right the
to order and to issue in order for
issuing order to issue out of order and
six issue other world and we want to use
the first tree as different order cb6
so it's it's for issue out of order it
can dispatch six instructions peak but
it will never sustain more than four
we've never sustainable in our
experience we actually use six issue so
basically sorry if it's not EV 6 e vc
yeah it's similar to a v6 we try to keep
it six issue yeah you're right might not
dispatch six so the thing is happening
is that from left to right basically the
complexity of this course increasing and
it needs more resources we're going to
use the first three as different
alternatives for our animator core and
want to see that how much performance we
can get by providing perfect hints here
perfect games means that we have it
we're going to have perfect perfect
branch prediction and we're going to net
we're we're going to have no l1 cache
misses so this yellow bar is showing
that how much performance is going to
get proof having that perfect hints for
the animator go as you can see the thing
that's interesting here is there if you
can provide this is correct hints for
the animator core the performance of a
dwell issue out of order can actually
exceeds the original performance of a 62
out of order that means that there is a
lot of opportunity for that acceleration
obviously providing perfect hints is not
possible what we are trying to do our
best to get close to this chart to this
plot so here is there and it'll be more
detailed as structure of necromancer on
the left hand side we have the dead core
and on the red right hand side we have
the enemy at work or most of the hints
go to a cue in order to get transferred
between these two cores and the modules
that we added to both this course are
highlighted in this in this figure so
we're going to start with describing a
dead core and we gradually go through
the different structures here that core
basically runs the same program and
provides the hint for the animator cord
you can think about it as an extender
run ahead engine is then six issue out
of order and we have instruction cache
genes that are basically PC of committed
instructions we have data caches that
are addressed
committed loads in store and we have
branch prediction hints that our branch
prediction updates of the animator core
and one modification that we make here
is we make the l1 cache of the animator
code to have only read access to the l2
cache since we don't want to we want to
preserve the memory estate we don't want
to write the deity line back to the
shared memory so I'm building a
billowing of designing a processor we're
going to have a bunch of pores on it so
the idea here is that we'll have to put
in all this hint gathering logic and
abilities to drop dirty lines and and
then push those out to the small for am
I doing this from the day I light up the
designer doing it the first thing fails
and this is for the manufacturing faults
so basically you will know at the
manufacturing time that something is
wrong you need to somehow fix it yeah so
I have a slide I will jump to slides the
head I think that would be useful that's
how we're going to design the system
okay if they're dead coke another cannot
read what it is written can you run a
hedge how's it far enough to do the
different type of failures that are kept
at it can happen inside there inside
their written grill only has realized
the cat
nobody has local local l1 cache and
which is yeah l2 is only yeah I missing
sorry okay so here I have it have a
picture that is basically sure that how
we are going to design the system this
is model after son rock with 16 cores
each cluster has four cores and this is
how we're gonna sow how's our vision
basically that we share that animate or
core across several aggressive course
and at the manufacturing time we know
that whether any of them has a failure
or not depending on that we're going to
like hardwired like burn some fuse or
something that's going to copper one of
these within with the animator core
inside the group alright
so animated for is as I mentioned an
older generation or in same I saw here
we as soon as a to issue out of order
and since it has the precise estate
basically with allow it to handle the
exceptions we treat the cash ins to warm
up there as the prefetching information
to warm up to local caches and we rely
on some fuzzy hint disabling approach
based on a continuous monitoring of the
hint effectiveness to realize that when
do we need to disable a particular type
of hint to save on their energy and also
reduce the contention and resources of
the animate worker and we copy their
architectural the PCN architected
registers whenever we realize that their
state of the animator the app the det
cord is basically so far away from the
correct state and we want to do every
synchronization because sometimes it
starts executing in the wrong path and
goes on and on and we see the hints that
you're getting at that point is no
longer useful now we do every single to
in terms since most of our
communications are unidirectional we're
going to use a single queue for
transferring the hints and l2 warm up is
provided free that means that there is
no need for a hardware to put to do the
communication between the two cores a
lot of work or your slips DB prosecutors
SS RTE and you know supportive
simultaneous scores and all of us are
you it is your big advance here and
clearly the the coupling for yield ISM
is a novel step all right are you adding
other hints and mechanisms that do
better Thank You Meyer work in a big
quiz or acceleration are you basically
taking the best practices from that
prior work and applying it to this
topology and create a great question so
the biggest advantage that we get in
terms of selling points definitely that
we are using it for yield and hot pot
the second thing is that since I would
say that the way that we apply it is
different from the prior work but there
are some few things that are
completely different and that's mainly
this decision to disable a particular
type of hints or do there is
synchronization and these are like the
fine-grained decisions that we make that
are not like the case for the prior work
and basically the way that we designed
the hint distribution and gathering is
based on the fact that we know that we
need to disable this since once in a
while so here for the sake of time I
will skip over cash ins and I'll go over
the life cycle of a single branch
prediction hints in our system the thing
that's happening is whenever there is a
branch prediction update in the death
Corps we send the signal to the hint
gathering Union and we say that okay
generate a new hint it's going to look
into the queue if Q is full it's going
to install meaning that the enemy the
dead code is already too far away far
ahead of the animator when it does not
need to generate a new hints otherwise
it's gonna get that pc NX PC and add two
more fields to end one is type of the
hint and the other one is H tag h tags
it is a feel that we use in order to in
order to let the animator core know that
when to use that particular type of hint
when is the right time to use it and we
inject this hint inside the queue it
gradually goes through the queue and
gets to the head of the queue at the
animator course I'd we have a buffer
that basically buffers this hints and
aloe and basically all the water acts or
random access to the different hints
inside of this buffer because we have
the cash hints as well and whenever the
H tag is less than the some empirical
value we know that at this point we can
use this particular type of hand and if
the hint disabling unit is allowing that
hint to be applied locally we get the
hint and we send it to the fetch unit of
the animator corps in their face unit we
make a small modification I don't know
whether small or not the original
predictor of animator core is a simple
by model we're going to add another
bimodal predictor we call an incremental
predictor and this thing this predict
this
the particular particular basically will
keep track of the hints that are coming
at a higher level we apply terminate
apply a tournament predictor that
decides for a particular given PC
whether their original branch predictor
of the animator course should take over
or the necromancer predictor should take
over and we're going to use this
tournament predictor to decide whether
for a given at a given point in time
whether we should disable the branch
prediction in sauna and this as the
slider described how we apply a
necromancer to a larger CMP system in
summary we're going to use a necromancer
to enhance throughput of the system by
exploiting death course and we leverage
the set of micro architectural
techniques to provide intrinsically
robust hands fine and coarse grained in
disabling online monitoring of the hint
effectiveness and also dynamic estately
synchronization between the two cores
applying in chrome answer to a fork or
cmp on average we get eighty eight
percent of the original performance of a
life core and this comes with a modest
area and performance overhead of five
and nine percent formance of the
necromancer for with the hits compared
to the original paper at issue for exact
compared to the CC
is it really that's really just a pair
something's Joseph here yeah all right
yep it means that it shares the net man
that that's a boy talking about a
pairing of two exact the sharing is not
not I think she is giving exam so i
guess that i will go over some of the
potential future works and some of the
concluding remarks if we have time and i
have a lot of global quite good so for a
fork or sea of you in the necromancers
they are let's assume with the door help
to or anything like that so each core
with its all wanted all that is twenty
five percent of that cluster cruising
area and now i'm adding a new core plus
all of the you know attracting logic and
buffering and so to add five point three
percent of area means that they hit plus
the Duke or ads 5.3 person area to that
which means that the core is a sixth the
size of the other one so the big is
roughly one-fifth this is including the
l2's UCL tools are already there so if
you remove the l2's it's going to be a
bit more area overhead I see including
out you mentioned that excluding l 2 i'm
saying this is this five persons
including of those but the area
comparison of a the thing that you're
interested in is that what's the area of
this one compared to this one is around
the fifth yeah what is it so what are
the cache sizes of the two portions this
one is 64 kilobytes for like their and
data 64 for instruction and this one is
around 4 kilo
so it's very trying usually smaller yep
yep so I'll go over the future work with
quickly some of the things that I can
basically work on on the same area of
reliability and low-power design is one
is their usage of control theory for
power management of processor pipeline
caches and network-on-chip that I'm
actually currently working on this would
work some way we have some collaboration
with intel labs and so I have some other
students that working on this at U of I
and another area is we want to exploit a
process and temperature for reducing the
refresh energy of dynamic memories since
diner memories are becoming more common
on for on-chip also like power 7 and on
the process will have it they see usage
of temperature and process variation can
substantial decrease the number of
refreshes that you need to do another
thing is using more reconfigurable
architecture for near threshold voltages
and I have some ideas to extend some of
the prior works that I've done for this
domain another thing is that I'm excited
about is to study their newer technology
nodes and see that what are the
trade-offs between the energy and
reliability for example for PCM carbon
nanotubes and spin torque transfer
memories another thing that I'm excited
about is to see if we look at the
approximate computing what are the
things that basis reliability can
contribute to this for example some of
the things is like understanding the
fault propagation and containment
measuring accuracy of result what is an
acceptable level of accuracy limiting
the fault rate devising a fault back
McCann is fallback mechanism or even
making more applications suitable for
this domain I think these are all
interesting challenges in its domain
here i have a list of publications
during the course of my graduate and
postgraduate studies i work general and
three areas i work on reliability and
fault tolerance low-power design and
also single thread performance and
throughput in addition to what I've
present and I also work on some other
areas like transient false recovery
we're out centering
and without centric scheduling online
testing refresh energy reduction for
dynamic memories energy efficient
accelerators and also
application-specific processors so in
conclusion as you know we have more
transistors these days than what we can
power on therefore in order to scale the
performance in an energy constrained
environment we need to improve the
computational efficiency and these needs
to rethink the computer architecture for
energy efficiency from ground up as we
saw in this talk reliability and energy
efficiency are tradable and we can take
advantage of techniques that can
tolerate process variation for this
purpose however conventional solution
reliability solutions are too expensive
for this mainstream mainstream
high-performance micro processors there
is a need for new proposals that can
trade reliability and power and energy
efficiency by providing runtime
adaptability and high degree of
reconfigure ability thank you so much
for your time I'll be happy to take any
further question I got very very
interesting feedbacks for the first work
and I really appreciate it thank you
very much the interest of time I think
you're gonna be meeting with most of us
anyway for ask your questions giddy they
have a question I pop out for a prop
thank you for the talk all right it's
all right interactive yeah I'll central
authority and you are on for 130 yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>