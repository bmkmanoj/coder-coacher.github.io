<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python+Machine Learning tutorial - Data munging for predictive modeling with pandas and scikit-learn | Coder Coacher - Coaching Coders</title><meta content="Python+Machine Learning tutorial - Data munging for predictive modeling with pandas and scikit-learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python+Machine Learning tutorial - Data munging for predictive modeling with pandas and scikit-learn</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Fpox_IzbVuA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um so now we'll focus on machine
learning itself so machine learning is a
way to write programs that except to
analyze data and autotunes their
internal parameters to make good
predictions automatically based on
historical data so typically you you try
to have a program that finds the
structure of the data to find the
relationship between input variable and
output variable so that when you have
new data you pass it as an input to that
program and will predict the most likely
output for that specific piece of data
so for instance you can do time series
forecasting for the price of an asset or
the temperature if you're doing weather
forecasting are spam or not spam in
emails and so on so it's going to be
more intuitive if we do some plots so
let's execute matplotlib and plot this
function which is already it's written
as a as a external Python file that you
can find in the Indies if older c'mon so
this function basically shows a data set
in two dimensions so the first dimension
is on x-axis and the second dimension is
on the y-axis and those are the two
input variables for our model and the
goal of this model is to separate blue
dots or yeah gray gray blueish dots from
orange dots and you can see that there
is a natural organization in that two
dimensional space and so we can find
build a model that will draw a line a
boundary between those two areas and
we'll try to maximize the margin between
those two areas so in that case it's
using a stochastic gradient descent a
linear model so it's train with the
stochastic gradient descent optimization
routine to fit a linear model and a
linear model is just something that
finds a hyperplane in into that space of
two dimensions so it's a straight line
2d i won't detail the mathematics any
more than that so we just give you some
names like this and if you can have a
look at the reference documentation
scikit-learn website if you want to have
more details on the mathematics of the
models and in the documentation we also
include references to the the main
papers in the literature to to get more
details so that does it make sense so
for instance here the two variables
could be the age and size of people and
you and half of the people could be men
and off of the other people could be
women and you could try to find a
boundary based on the age and size
information it's probably stupid because
in real life men and women will have
almost the same size and matching edges
so the two groups will overlap a lot in
this case there is a clean separation
this is really very rarely the case in
real life otherwise we don't need to do
machine on so let's know so that if you
are interested in the in the source code
for for this you can execute this
statement and you see figure here is a
is a is a folder that is also a pac-10
package because there is a minute file
in it and and we import this function
for from there and this is actually a
this function is defined in in that
module so it's just not very complicated
to do that plot you can see that there
there is the function itself the psychic
lamp classifier here that is fitted on
the data and then we do a bunch of
matplotlib magic to to generate the
figure itself we will see in detail how
to train models there is a second kind
of machine learning algorithm that we
will tackle during this session is
called regression so the first one was a
classification problem because we add
two categories of samples and we wanted
to find a boundary between those two
categories so it's classifying blue or
orange in that case we will do
regression problem where we have one
input variable on the x side and one
output variable on the y axis and in
this case we we try to find a linear
relationship between the output and the
input so you can see that the the output
is a continuous very variable because
there are many different possible values
of Y there is not just two or a finite
number it's a floating point values so
basically regression is when you do
machine learning and the target variable
that you're trying to predict is a
floating point number it's a continuous
variable classification is the same but
you are trying to assign finite number
of classes to the output so discrete
variables so integers or our label
string labels alright so to do machine
learning in Python you can use the
cyclotron project so it's an open source
project you can download it and it works
on most platforms so if you go on
scikit-learn org you have the full
documentation and if you import
scikit-learn you can play with examples
data sets that are shipped into the
library directly which is very nice too
quickly explore the behavior of models
on sample data so this is mostly toy
data because it's small so that you can
download it quickly but it's already
useful to get some intuitions so in the
data set package of a scare are there is
a function called load digits and that
returns an object that has several
attributes data target and images and
many more so data so let me execute that
so data is an umpire array and you can
see that it's a two dimensional an
umpire ray if I do shape here I see I
see the shape of the array so it has
more than normal
22 thousand rows and sixty four columns
so we call the rose the samples and the
columns the features I'll give more
details on what it means but the input
data of most scikit-learn models will be
expected to have a two-dimensional shape
with samples and features and we also
have a target variable that is stored in
a separate array which is a
one-dimensional array and you can see in
that case those are not floating point
values but those are integers I can
check the d-type and in integer 54
integer so those integer are basically
the labels for each of the rows in my in
my data set so why why do you think
there are 64 values in those rows you
have an intuition on why 65 yeah a 64 is
eight by eight and those are actually
pictures represented as rows in an
empire array and they are just a pixel
values gray level pixel values of eight
by eight so we can check that by
accessing via the image is attribute
which is which has the same data but
shaped in a different manner so if I
check the shape you can see that I have
the same number of rows but then I have
two dimensions after that it's a three
dimensional array and I have eight by
eight so if i take the first row of that
three-dimensional matrix I get an a by
eight array and on this two-dimensional
array I can do in show with him up
equals VLT that's CM that gray and I can
see I can visualize my metrics we also
do interpolation equals nearest because
otherwise slightly misleading so you can
see the actual data those arrays are
actual images and there are gray level
images with pixel values so you can see
that I've just eight by eight pixels so
64 in total and those are gray gray
level values between 0 &amp;amp; 1 I think one
is white and 0 it's black ok so the
digits that data attribute is the same
but reshape so the first the first world
will be concatenated with the second row
and so on on a single 64-bit a vector
the 64 value items vector ok so now we
have the data we can train a model so to
train a model in scikit-learn you you
import you read the documentation and
you see the list of modules and you find
the models that you are interested in
and so in that case we are looking at
the SVC class from the SVM package so
the SVC class stands for support vector
classifier so it's a support vector
machine for classification and
regression so discreet target variable
and it takes two input parameters which
we call hyper parameters those are
mathematical constraints that will
affect the way the machine learning
algorithm will learn from the data so
those by used here I'm fixing them
arbitrarily but in practice it's very
important to try on the data which value
works best and we need to automate this
using what we call the parameter sweep
or grid search
when we want to do that in real life for
now to start we will just fix those two
values so I created those the CLF object
which is classifier so by default it's
untrained and if you print it you see
all the default parameter values above
all that model so some of them are like
mathematical attributes that constrain
the machine learning algorithm like go F
and degree and gamma and others are more
like technical attributes like whether
we print stuff on the output the cache
size the in-memory cache size of the
object so you you have to read the
documentation on the website to
understand what they mean so once we
have our instance object we can call the
fit method under data so in this case I
will just take all the element of the of
the data set except the last one columns
dash minus one yeah column the dash one
means all the element except the last
one and I do that both for the input so
all but the last row and the output all
that last the last variable and this
will train internally the model so it's
it's exactly the same object but it has
been mutated internally to fit its
internal parameter to compress the data
and find the structure the structure of
the data once why I have this I can call
predict on the last row of my data set
and it outputs a single integer which is
the integer matching the class of this
data set so if we can have a look at the
last image of the data set and use em
show again and you can see that the the
last data point that we used for the
prediction is this and it predicted
eight so it looks like an eight so
apparently you did a good job in that
case so we can check by
printing the the target value target the
value of the target variable for the
last element it's actually innate so
it's a good projection all right so you
have questions on this let's move on so
you can close this notebook and go to
the next one we will explain in a bit
more detail how the data works in
psychic lamp so a view as you've seen
we've we've dealt with an umpire race
for the input variable that has two
dimension the first dimension is called
the sample the second dimension is the
features so the samples are the
observations are the instances in your
data set the features are the attributes
of each instance so you can also
visualize this as a database table the
sequel database table are an Excel
spreadsheet for instance you have
columns and rows in general the
convention is to put on the columns
attributes of observations and
individual record as rows are samples so
this is the the traditional way and this
is the representation that scikit-learn
expects for all the algorithm so we can
learn another data set which is a data
set that was built by adding like
biologists moving around in the nature
and looking for flowers like this one so
the species of this flower is an average
setosa but there are like very close
species versicolor and an virginica and
so the biologist wants to classify those
flowers and to do the to do so they will
observe numerical attributes of those
flowers to be able to build a classifier
so in your opinion what kind of
attributes can we collect on those
pictures to try and classic
their color yeah yeah and if you are a
statistician you know that it's
interesting to measure the fatal lens
the petal with the settlements and the
circle which so I'm not a biologist
myself so I don't know which one is
disabled and which one is the film but
if you are bad at this you would do and
so in that case we have this data set
which is a classical data set for
statistics and so it's loaded it's
available as a CSV file that we can load
from scikit-learn and it has four
attributes so four columns sepal length
in centimeters settle with that our
lands better with and we have three
target variables which are the
identifiers for the fish species so
again we can import this data set from
the data set folder and scikit-learn and
see this object here I race it's a weird
kind of object that looks like a Python
dictionary so it has different keys or
attributes so we have the data array the
names of the features the target
variable and the names of the target so
let's have a look at the data array we
can load the shape and print the first
element so we have 150 flowers we have
the four attributes for each individual
flower and so in the first flower in my
data set I have a little lengths i think
no sepa sepal length Sipowicz petal
length little bit so apparently the
staple are bigger than the flowers so
this is the input data and we also have
a target variable and you can it's a
good sanity check when you are dealing
with data like this and you want to do a
machine learning to check that the
target variable you have as many
observations in the target variable as
you have in the input data because
otherwise you have a mismatch in admits
that it's not
surely the right labels and we can have
a look at the target variable arrays and
you can see that i have 150 elements and
they have three possible values 0 1 of 2
and to understand the meaning of those
three values you can have a look at this
target names attributes and 0 stands for
setosa one stands for versicolor and 24
virginica so in scikit-learn for
classification we also will often use
integer encoding for all the different
possible classes it's because it's more
efficient from a memory memory point of
view than repeating the string values
over and over again so we use numerical
encoding like integers so let's plot two
dimensions of this data set so here I'm
selecting the first two dimensions of
settlements and sepal width on those two
axes and i'm using matplotlib with to do
a scatterplot with different colors for
the three possible species and you can
see already that some some the setters
are species for instance just by having
a look at the suppose it's quite easy to
separate them from the other two so
let's try to play with this cell by
changing the values of the indexes so
you can see that those are the indexes
of the of the columns of the data frame
of the data array so let's try to use
other attributes and look for a better
separation between the three to the
three classes and do that on your
computer and see if you can find better
ways
actually all the
hopefully the video is still fine okay
thanks all right so do you have any
suggestion 1230 actually this is the one
that I printed and it's actually a good
suggestion because you can see that it
would be easy to draw a line here to
separate cities our asses versicolor
drawing another line here would work
quite well so it means that if we select
a subset of the features the right
subset we can actually have a good model
so we what we did there is just manual
feature selection in real life we will
not do that we will give all the
features through the algorithm and most
of the time the algorithm should be able
to pick up and wait the feature related
which one another to find the good ones
the yet so the question is can we you
plot a third dimension it's possible
with matplotlib to do 3d plots but it's
a different syntax I don't remember it
but most of the time anyway 3d plots I
they are you have like matching thats
everywhere is not really visible so but
you could if you if you want and anyway
in that case it's for dimensional data
so it's limited and in real life
problems you would have hundreds or
thousands of dimensions anyway so there
is no matter magical so another question
okay so the question is can you explain
this function for matter so basically
this is a trick yeah no I cannot explain
this yeah it's galvez matplotlib stuff I
wouldn't have done it myself this way so
I think if you want to you can call
several times plz scatter with different
subset of the data and assigned explicit
color to each of them and put a label
and call the legend it would have been
easier to understand but a lot and
slightly longer to type okay but you can
if you if you want to know how to do my
plot hips visualization the best way to
do it is to go to my blog website there
is a gallery you look at the plots that
looks like the one that you're
interested in and you adapt the the
source code to feature your data alright
so in scikit-learn there are many other
available data sets so if you import the
data sets package you have functions
that starts with underscore fetch or
make so load underscore is for loading
data that is shipped within the source
code of scikit-learn so those are very
small data sets other more realistic
data set can be downloaded from public
URLs on on the web so you can use fetch
data and it will download the original
data and convert it into an empire way
for you and save it a local copy in a
temporary folder and psychic lon and
finally you have also make which is
useful for generating synthetic data
with statistical assumptions that you
may cover area to just try to break your
machine learning algorithm using a
statistical statistically generated data
so let's try to have a look at the kind
of data set
that you can load I forgot to import I
think yeah I have forgot to execute this
okay so if i do a load underscore and I
eat the tab key ipython will will
autocomplete so you can see a bunch of
standard data sets like Boston for
regression of housing prices in Boston
in the 70s there you have the digit
digits data set diabetes and so on if if
you want to know where those data come
from I just want to do that know how to
get rid of that sir so if you want to
know where those data come from and
actually when you fetch data it will
store them in a local folder and to know
where this folder is you can call get
data home from the data set package and
you will see the past your local folder
on your machine on your hard drive yes
so you need tab completion so you do
data sets you can tabs in complete and
you type the beginning of something and
you tap tap the tab key again
so this will work on my machine because
I have the home environment variable set
up in sabash man but what you can do is
copy and paste this guy and do a deer or
LS and you should see the temporary
files that are downloaded by psychic
lamb when you fetch the asset so don't
fetch any data set now because it's
gonna be too slow but in the future if
you use the fetch comments it will
download them all here okay okay so
let's play again we're with the digits
data set actually actually we can we can
move on a bit and keep the end of this
and move on to the next one because
early so actually we can break now and
do the next one each year microsoft
research helps hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>