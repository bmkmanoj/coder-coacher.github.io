<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Locality-Sensitive Hashing and Beyond | Coder Coacher - Coaching Coders</title><meta content="Locality-Sensitive Hashing and Beyond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Locality-Sensitive Hashing and Beyond</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/t_8SpFV0l7A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's a great pleasure to introduce
today's speaker Illya reason Steen is a
graduate student at MIT we work in on
the supervision of IBM dick so he has
done a lot of work on on the nearest
neighbor search and today he will tell
us about some of his recent work on this
topic thanks for the introduction
so I'll talk today about two recent
papers of ours and these papers are
joined with Alex and Ani from Columbia
forensic who as Costa said as my advisor
at MIT theis laarhoven who is a graduate
student at Eindhoven and Ludwig Schmidt
who is another graduate student at MIT
so this is this is my talk outline
essentially I'll talk about these two
results eventually but both of them have
pretty large common prefix which I'll
start with so I'll start with defining
the problem that we'll be solving and
the problem is actually very easy to
formulate it's called near neighbors
search it has different names but the
basic idea is the following so you have
a data set which is n points in R Z and
you have a certain distance threshold R
so what you want is you want to
pre-process your data set so that given
a query find any data point within
distance R from your query so the
parameters that we will care about will
be mostly space that your data structure
occupies and query time another
parameter that we would naturally care
about this pre-processing time but
usually if you can bring space down to
something reasonable then pre-processing
time usually can be made relatively fast
as well so let's not worry about it for
now so at least in some scenarios there
are great data structures for this
problem so a model case would be
something like if all of your points are
on the plane and the distance is
Euclidean then what you can do is you
can build Voronoi diagram and then given
a query just perform a point location in
this word on a diagram and that would
give you the nearest neighbor and so
like using more or less textbook
algorithms and data
you can achieve linear space and
logarithmic query time for this case
so unfortunately approaches like these
are completely infeasible for the high
dimensions so the problem is if you
would do something based on Voronoi
diagrams and even more generally
whatever data structures we know for the
high dimensional case they require space
that is exponential in your dimension
and that that's definitely not that
great so at the same time I would argue
that all the fun in some sense happens
in high dimensions so many applications
that are interesting very definitely not
on the plane so it would be nice to do
something about them and what we will do
is well as good ferret issues will
change the problem that we are solving
and so instead of exact nearest
neighbors we will be happy with
approximate nearest neighbors and the
formal definition is like this so in
addition to the data set and the
distance threshold we have approximation
factor C which is some real number
larger than 1 and now we have the
following question so suppose that I
give you the query with the promise that
they will be at least one data point
within distance R so then I want you to
return any data point within distance C
R from the query so basically there are
two balls and I know that there is at
least one data point within the small
ball but I would be happy with any data
point within the light bulb so is the
definition clear good so yeah so near
neighbor search or lake similarity
search or whatever you call it it has
quite a few applications so the most
obvious applications are similarity
search for all sorts of different data
images audio video text biological data
and so on but there are a couple of non
like cup cup couple of applications of
different sorts so let me just briefly
mention them so there is a recent one
for crypt analysis so namely turns out
that nearest neighbor search can be
applied in practice for solving shortest
vector problem and lattices and it can
give you pretty good speed ups and
another application is for optimization
so nearest neighbor search was used to
speed up different methods for
optimization such as coordinate descent
stochastic gradient descent so these two
are relatively recent results and
actually in this talk will be mostly
looking at the specific case of nearest
neighbors search not for the whole talk
we will consider general case as well
but important special case is when all
of your points and queries lie on the
unit sphere and Rd so for certain
reasons it will be convenient to look at
this case and it's actually relevant for
both theoretical and practical reasons
so in theory it turns out that we can
reduce general case to the spherical
case and I'll show this reduction well
at least I will outline this reduction
later in the talk and then practice
Euclidean distance on the sphere
corresponds to cosine similarity which
is widely used by itself but also even
if you don't want consigns similarity
you want like genuine Euclidean distance
sometimes you can pretend that your
dataset lines on the sphere and you
wouldn't lose might by doing that yeah
so even more specifically a special case
that is good to have in mind would be
spherical random case so what's the set
up here so my dataset is not only points
on the sphere but they're random points
on the sphere just chosen uniformly at
random n times and query i generate
queries as follows i take a random data
point and plant query within 45 degrees
say from a data point at random and what
it looks like is something on this
picture so basically if I have a query
then I'll have a near neighbor within 45
degrees just because I generated it like
this but all the other data points will
be tightly concentrate at around 90
degrees yet because if you sample n
points on the sphere they will be
pairwise almost orthogonal unless you
have like too many of them and just keep
keep this case in mind it would be nice
to illustrate our algorithms on this
case and in in certain sense it will be
the core case as we'll see later and
again in practice what often happens is
that actually
of angles around 90 degrees it's not
uncommon to see in practice so it's it's
also it's also relevant for practice
somehow this case so any questions about
it so that's it no n is much larger than
G but let's say it's not like
exponential in G let's say it's
sub-exponential in G so - - square root
G something like this so of course if
you have lots of points then you will
not have that good concentration but
yeah okay so that's it for the problem
definition now let me introduce
locality-sensitive caching so if you
have any questions maybe you should ask
them No okay so what is locality
sensitive caching so this is the
technique introduced by indigent motwani
in 98 and this is the way to solve near
neighbor problem in high dimensions and
the basic idea is pretty intuitive so
what you want is you want space
partition of Rd such that closer point
like it will be random partition so that
close close pairs of points collide more
often than far ones so something in
spirit of this partition inter andum
disks so the formal definition here is
the following so I require my random
partition to have the following two
properties so if my if my points are
closed then they should collide with
non-trivial probability under this
random partition say with probability at
least p1 and if they are far then they
should collide not too often with
probability less than p2 and this R and
C are distance thresholds are exactly
from the definition of approximate
nearest neighbor I cared about like
close pairs and far pairs and these are
exactly the distance thresholds we care
about and useful way of thinking about
it is if you have some random random
space partition it would most likely
have some dependence on the probability
of collision on the distance and then
this inequalities just tell you
something about two specific two
specific points on this plot so actually
now let me demonstrate one example of a
Lesage so not to think of it abstractly
but just have some concrete example in
mind and it's actually very useful
family
it's used it's very useful for both
theory and practice and it was
introduced by Jerry car in 2002 and it's
actually heavily inspired by a certain
approximation algorithms by governments
and Williamson for those who understand
and they actually like hashing family
hashing family looks looks very simple
so it works only for the sphere and if
I'm on the sphere what I can do is the
following
I can sample a random unit vector
uniformly let's call it R and then I
crash my point into sign of the dot
product of my point and R so basically
another way of saying it is we take a
sphere and cut it into equal pieces by
random hyperplane and it's very easy to
compute exact probability of collision
for two points so if the angle between
my two points is alpha then probability
of collision is just 1 minus alpha over
P why well because if you have two
points then in order for your hyperplane
to cut this two points you need this
hyperplane to pass through this like
angle between P and Q and probability of
that is alpha over PI and with the
remaining probability they actually
collide so that's why we have this
expression it's like exact formula and
on the plot it would look something like
this
so remember that we would care about
random keys right with 45 degrees so if
points I'd like 45 degrees then
probability of the equation is 3/4 and
490 degrees is 1/2 so that's like
typical case we would think about okay
so we have this nice simple family in
mind and let's now see how to use it to
solve it like to do similarity search in
high dimensions so the first idea you
would think about it to just take our
hashing family and just crash my points
using hashing family so basically
compute crashes and then just I don't
know look up points with the same curve
but of course it wouldn't work for
instance for the hyperplane that
wouldn't work because you have only two
values of care so your points in the
best case they would they would be split
just like they would be split evenly and
then each budget you have n over two
points right so we would need to
enumerate n over two points so that's
too much
national extension of this ID instead of
one cash function used K independent
cash functions from my family and query
time yeah and space but space even let's
not worry too much about responsibly
near query time right if you just use
one cash function even sub linear credit
and we won't get of course I would want
us as good as possible so let's see what
we get if we use K hash functions
simultaneously instead of one so for one
hash function probability of collision
is as I said before it's just a straight
line right
but when we'll start increasing K it
actually goes down this is more or less
obvious I mean probability just gets
raised to the power K right just because
everything is independent but what's
crucial here is that for four points
this probability goes down much faster
than for closed pairs right and that's
exactly the crucial thing here and
that's actually it so what we do is we
choose K appropriately I'll see in a
second how to choose it actually and
then we have our points using tuples of
cash functions simultaneously and then
just to numerate like given query we
enumerate all the points in the same
bucket so that's the whole reduction so
let's see what parameters we need to
choose and what we get in the end so
turns out that the optimal choice of K
is such that this point for four pairs
becomes smaller than 1 over number of
points all like of order 1 of the number
of points why because in this case for a
query if you look at its beam then the
number of outliers in this beam namely
far points that we don't really care
about this like constant on average just
by linear linearity of expectation it's
like M times 1 over N this is 1 right so
the query time would be constant
actually in this case well proportional
to the dimension but I think of
dimension is something small for the
sake of this talk so we will numerate
constant like constant number of points
on average and are we done no because
actually those we need to care about
probability
probabilities for clothes pairs so we
would want to find at least one clothes
pair one clothes point with decent
probability and if we just do this whole
thing once then the probability that it
would collide it's freak waters 2k so
it's exactly this point and if you do
the math so if you take K from here and
put it here we would get that
probability of success would be
something like 1 over m 2.42 or
something like this and in order to
boost probability of success to say 99%
we would need to repeat this whole thing
and 2.4 to 2 times so in the end we have
like L hash table hash tables no no I'm
happy with any point within this 45
degree range let's say but for the
random case it would be exact nearest
neighbor yeah but in general not
necessarily question so the overall
scheme is like this we have K times L
hyperplanes
so in each hash table we have care of
them and we have L hash tables overall
and the overall space is something like
m2 1.42 and paratime m 2.42 so that's
exactly the query time that would be
typical for this talk sort of kind of
young some polynomial in n so are there
any questions about it like if that's
unclear it's better to spend some time
you're doing worst case right I mean you
can't do pre-processing and get like
maybe some amortized this is I mean this
is pilot query yeah yeah yeah so it's
like worst case over queries so for HP
each query will succeed with decent
probability but I'm wondering like why
is that important like if I'm allowed to
do okay processing then of these points
and maintain they call so what do
propose a new amortized thing like maybe
former ties double would like queries so
you want to be average over queries yeah
so turns out that we don't we don't know
any better in this sense well in
practice maybe sort of button in
I'm sorry something like spitting
something like square root of n here
yeah and you're saying that's what I
should expect in me yeah okay Roxas and
practice succeeding only for like good
queries it kind of makes sense but in
theory we don't really know any better
so we don't know we don't know how to be
like average over queries in some sense
okay good so this is this is a pretty
simple argument that actually appears in
the same paper that introduced Alsatian
Indic Madhwani and they actually like
for general case i I showed you complete
numbers but in general case what you can
show is that you can always choose
number of tables and number of hash
functions per table so to get space that
is m2 1 plus row and query time m2 Rho
where Rho is the gap between
probabilities of collision for like
closed pairs and for for pairs and the
proof is exactly the same as just
instead of concrete numbers we get this
formula yeah okay so that's it for the
definition of Alice H now let me show
you the optimal construction for LS h
for a sphere so can we do better than
hyper planes so a complete question you
could ask for this specific random
instance where I planned queries within
45 degree can I beat these bounds which
has sort of square root of M and like n
to 1.42
space can we do better or not turns out
that we can and that we observed in our
previous paper and like used it in one
of the papers I'm going to talk about
and actually you can do much better so
you can get for the query time you can
improve more than quadratically actually
so you can get M 2.19 query time and
space and to 1.18 and this is actually
optimal so this new bound is optimal
unlike the old one I'll say in a second
how it works but let me just say for now
that for spherical case we understand
the best bounds exactly
and of course of course
I again tell you the numbers for the
random for the random 45-degree case but
of course it works for general case on
the sphere it's just like the foremost
will be slightly more complicated so
that's why I'm not showing it here but
let me actually show the construction
the construction is actually pretty
simple and clean and again as the
hyperplane this is also inspired by
certain approximation algorithms this
time is by result of Karger motwani
ensued on who used somewhat similar
space partition to round is the piece
for coloring and let me go at Voronoi
LSH I'll explain in a second why would
call it like this and the construction
is actually fairly simple so we want to
hash our points on the sphere so let me
for this let me sample let me choose
some parameter T how to choose it it's a
separate question not entirely trivial
but let me sample standard deviation of
gaussians
so each GI is a dimensional iid and 0 1
vector and then hash of my point would
be index of the Gaussian whose dot
product with my point is the maximum
possible so pictorially what happens is
something like this so I sample bunch of
gaussians they don't have to lie on the
sphere but I mean their length will be
approximately equal so let's think of
them as uniform vector from the sphere
it's not going to matter for this
discussion so a basically sample bunch
of random points on the sphere and then
my sphere gets partitioned according to
with which Gauss and it correlates the
best so something along these lines
and that's exactly might like that's
exactly my space partition is the
construction clear and let me just
observe that if I sample only two
gaussians it's exactly the hyperplane
LSH why because if I sample two
gaussians then my partition would be
just hyperplane that is in the middle of
these two vectors so it's exactly
equivalent so this is a national
generalization instead of just two
regions we might have more than two
regions and we'll see you it will be
beneficial for us
okay let's actually compare it with
hyperplanes so as I said one hyperplane
is exactly the same as Voronoi u.s. age
with two gaussians right just because
it's exactly the same and turns out that
the right way of comparing it is to
compare K hyperplanes so remember that
we like we basically cache with respect
to K independent hyper planes and
Voronoi LSH with 2 2 k gaussians why is
that well because actually it turns out
that it's good to compare we'll see in a
second why but for now let me just tell
that in both cases we have 2 2 k regions
so at least it we can meaningfully
compare these things so for one
hyperplane and photo gaussians there is
no difference but then when we'll start
increasing these things things start
getting interesting so even even for the
two hyperplanes and for gaussians so
this point is exactly the same on both
of this plot and that has to do with the
fact that in both cases there are two 2k
regions but in this but for these points
we start seeing difference and it
actually turns out that for Voronoi LSH
we get slightly better slightly higher
probability of collision for like small
distances so it starts kicking in right
and then when we increase the gap
actually widens so when we increase to
like 6 hyper planes versus 64,000 the
gap is actually pretty non-trivial
and you can do the formal analysis and
show that if your number of gaussians
grows the gap between hyperplane LSH and
Voronoi realization creases and the
exponent that we get it approaches that
value point 18 that I promised you the
problem that I'll cover in the second
half of the talk and yes that's a
problem
so what Sergey said is that 4k
hyperplanes we can essentially like
decide where our point lines in like K
iterations roughly speaking and for
gaussians we would need to do 2 to K
iterations yeah so
it comes at the cost in the improved
improved exponent but actually let me
just tell that for for the sake of
theory doesn't matter we like we'll
choose parameters that it wouldn't
matter much yeah so okay so that's
actually eat so this is the optimal LSH
construction for a sphere and now let me
tell you how to use it to get the
state-of-the-art algorithm for so that
is computing dark product you're taking
this as a unit operation oh yeah yeah so
think of dimension has being Logan
something like this okay yeah so it's
like proportional to log n what I'm
mostly worried in this talk is factors
like m to epsilon so if something is sub
polynomial this constant famille for the
sake of this talk in practice set of
cost matters and I'll talk about it yeah
so you're talking about this uniform
random it's case and then you also in
the comment about general chaos and you
said that this wouldn't work there is
well but it's only optimal in the the
optimality proof is in the uniform
random case so optimality proof is for
the general case but when your distance
thresholds that you care about roughly
corresponds to the random case so namely
square root 2 vs square root 2 over your
approximation factor so so square root 2
is the typical distance between two
random points on the sphere
so our optimality like in a way in a way
it shows that like you can think of our
optimality proof is that it's optimal
for the random case if you want yes and
it immediately implies that except it's
optimal for like arbitrary case if your
distance thresholds correspond to random
case but whether this construction is
optimal or not for two arbitrary
distance thresholds that we come like
that we can't yet prove although i
conjecture that this is the case so
that's the exact state of things and you
still draw the gaussian yes
even if the data is somehow skill even
if the data even if the date is somehow
skewed but so optimality for LSH so
actually like I'll cover in the second
how to do better for the case when your
data is skewed but you would need to do
something else we'll see but yeah it's
it's a very good point it's like exactly
what I'm going to talk about so now let
me tell you about our about my fit like
our first result and that appeared in
this year's talk so you can ask like
okay so now I talked about this fear now
I'm going to slightly switch gears and
talk about the whole RG it's like my
general case so let's talk about it so
you can ask what what are the best
bounce on LSH you can get and it turns
out that for Euclidean distance and I'm
not going to talk much about it but
still like it's very interesting to see
what happens for hamming distance and
Manhattan distance actually we know
exact boundary engage so for Euclidean
distance the right bound is 1 over C
square and where C is my approximation
factor remember and for Hamming distance
is 1 over C so in particular photo
approximation what we get is we get
query time something like m2 1/4 and
square root n for Hamming distance
respectively and that was established in
the sequence of works over quite some
time actually so we know exactly best
bounds for LSH for l2 and the one yeah
and just let me briefly let me briefly
recall that 1/2 here means that we get
space onto three-halves and query time
square root n so can we do better than
Alice age so yes we can and that's
exactly the main point of what I'm going
to talk about and how can we do better
than Alice age so the basic idea is to
do again space partitions but the
crucial the crucial idea is to do space
partitions that depend on your data so
remember that my definition of Alice a
should it
was actually pretty strong so I required
these two conditions for every pair of
points so for every P and Q I want that
if distance is small and live distance
is larger than something else right but
actually we don't need it so for the
reduction what we need is we need to
make sure that these conditions hold if
one of my points is a data point and
that that gives us the following
possibility so maybe we can look our at
our data set before building the cash
family and just like cook some nice cash
family that works nicely for this data
set right and that's exactly what we do
but interestingly enough not only it
works for like good data sets it
actually gives improvement for every
data set so you can say that every data
set has some structure to exploit and
formally speaking and now let me tell
you our result so basically we get
optimal data dependence space partitions
optimal after proper formalization it's
a little bit subtle but again let's not
worry about it for now and when she
basically what we get is we get almost
quadratic improvement
so for Euclidean distance we get say for
to approximation one over seven instead
of one over four and for hemming for
hamming distance we get one one-third
instead of one half and let me say again
that these bounds are optimal for data
dependent LSH if you do if you if you
formalize it properly so what's the main
idea so basically our our algorithm
consists of two steps so first I will
show you how to handle random data sets
random in the sense as I described and
actually for the random data set the
spoiler is that Voronoi LSH works well
and it gives better bounds and this step
is completely data independent so if you
know that your data set is random so you
just use vernalization apply the
standard thing so the second part which
is more interesting and that's the main
point is how to take any worst-case data
set that may not necessarily look as
random and then I'll show how to
partition it in part that for the sake
of our algorithm essentially random and
that
that step is data dependent and it would
exactly address your question about like
skewed data but okay let's first let's
just look at the random case so actually
yeah so what exactly it means is that we
have a sphere and double points and
queries are random and we use the fact
that distances are concentrated around
like if you have if you have sphere of
radius R then distances are concentrated
around square root two times R and for
analysis gives you basically the right
gives you the right exponent so it gives
you exactly the improved bound 1 over 2t
square minus 1 which eventually we want
to get for every data set but if your
data set doesn't look random then
actually Voronoi LSH is suboptimal and
the good example is if your points are
for example cluster then line lie in the
small region of your sphere then
actually Voronezh edge doesn't work that
great and it gives it doesn't give good
results and we need we need to do
something about it and that's exactly
where the second part comes how to
reduce from general case to randomly
looking case so if something doesn't
look like random let's make it look
random forcefully so basically we need
to remove structure so what do I mean by
structure here is basically low radius
clusters that contain lots of points
I'll say in a second what does it mean
to be low radius but again like at least
conceptually it should be pretty clear
so if we have any low radius dense
clusters we just take them away
something like this and I'll show how to
how to work with them on the next slide
of course we need to do something about
them because what if our near neighbor
is one of these points right but for now
let's not worry about it let's say that
we just removed everything and the
crucial thing is that the remainder
pretty much looks like a random set so
we know that we have no dense areas
anymore and that kind of spread so we
can apply for anomalous age and recurs
so after we recurs
so by recurse I mean we apply for
internalization for each region we do
the same and then
Aster's can appear again because the
definition was relative right so since
it has now way fewer points we again
potentially can have dense clusters
which we take away and begin apply for
annihilation recurs so now yeah
so before I will tell you what to do
with the clusters let me tell you how we
process queries so for queries we
actually do the following
so we first query every single cluster
they will not be many of them will
choose parameters such that there will
be relatively small amount of them so we
can afford to query every single one of
them and forward analysis will like hate
one part where our point lies for
example this one and recursively created
the corresponding part so that's the
whole thing so three it remains to tell
what we actually need to do for the
clusters
I didn't tell and actually that's pretty
crucial so for clusters we observe the
following thing actually now it's time
to tell what exactly it means to be low
radius so by low radius I mean something
that is slightly smaller than half of
the sphere so basically I declare
cluster to be low radius if it has
radius if it is a spherical cap of
radius square root 2 minus epsilon times
R so it's slightly non evilly but
slightly smaller than the half of the
sphere and crucial thing is that we can
actually enclose such a cap into
somewhat smaller ball by a factor of 1
minus epsilon square and that's great
why because we can recurse with reduced
radius and as I will explain in a second
we may we may we make actually progress
by doing that so let me state the
algorithm again like the overall
algorithm so basically for clusters
we'll use the radius and after several
reductions the problem assumption sir
essentially becomes trivial for certain
reasons and for the random reminder
warren LSH works well so that's exactly
conceptually how we handle different
cases and at some level it can be seen
as a decision
to you what we get so we start with the
root then we take out dense clusters
then we have random remainder which we
partition using vernalization then we
recursively do the same thing for
everything and when we query we can go
potentially to several sub trees so as I
said we query all the clusters and one
part and again it continues branching
and the parameters we get other
following so you can show that we can
that my tree occupies near linear space
and query time can be bounded by some
sub polynomial function and that's great
and of course as before one three would
not be enough because it would give you
an opinion or merely small probability
of success so we need many of them to
succeed with probability 99% so that's
actually it so is it any questions about
it yeah so one one one line summary is
that we observed that for another sage
works great for random data sets and
then if something doesn't look random
enough just like make it random okay so
now let me tell you a little bit about
our second result and actually the
second result is how to make
vernalization practical and that has to
do with your gaze question and that's
how nips paper is for another safe
practical know why because convergence
to the optimal exponent is on the one
hand very slow so we need lots of
gaussians to make it asymptotically good
and at the same time evaluation time is
roughly dimension x number of gaussians
so and that's bad even say for example
64 gaussians is already pretty much
impractical and that wouldn't bring us
even close to the optimal exponent so
can we do anything about it it would be
nice to do something because actually
hyperplane is a it's used quite a
bit in various forms in practice and
would be nice to like use theory to get
some practical improvements can we do
something yes that that's exactly the
point of the second part
so let's make let's make our
vernalization practical step-by-step so
the first step is to make our set of
vectors a little bit more structured so
vernalization samples bunch of random
vectors and that might not necessarily
be that great because that brings us
like that makes the evaluation time slow
let's make it like less random less
arbitrary in some sense and that
actually there was a very nice paper by
Tarasov and tanaka who showed who
proposed such a scheme they didn't
analyze it but at least they proposed a
possible improvement to vernalization
what they proposed this instead of
random vectors use plus/minus basis
vectors so basically you if you want to
hash your point on the sphere you
perform a random rotation and then after
you do random rotation you find the
closest plus/minus basis vector so for
example for the case of two we partition
everything in four parts and four for
general high dimensions we will have
cross polytope so for dimension D we'll
have two G part yeah so in this paper we
actually for the first time analyze this
scheme rigorously and show that it gives
almost the same quality as Voronoi LSH
with two G gaussians again which was 2d
gaussians because it's the same number
of parts so essentially we show that by
moving from gaussians to this structured
set of vectors if we do random rotation
first then it gives almost the same
result and anyway you can think of it as
like blessing of dimensionality actually
so exponent improves as dimension grows
because number of gaussians effectively
grows right and it's still not that
great because random rotations are
expensive so applying random rotation
stage takes d square time and storing it
also take d square wheels so it seems
that we didn't do that much progress but
in fact we did so just like wait for the
next slide and the way we did progress
is that at least the second step finding
the close at plus minus basis factor now
it's cheap you can do it in like one
scan over your coordinates it takes G
time instead of G square so that's
exactly the
so second AG is to use pseudo-random
rotations so as I said the bottleneck is
to store and apply random rotations and
that would be expensive so instead we
use pseudo-random rotations and they
were introduced in the in a paper by I
alone and Giselle and since well since
then they were used in like many other
places in both theory and like applied
papers and so on it's like very
beautiful idea if you want to like
basically if you want to learn one idea
from this talk I want you like - sorry
no no it's it's like you'll see it's
it's really nice so we want to do
something that would serve roughly as a
random rotation but without doing the
whole random rotation so what we do is
the following thing so instead of random
rotation let's do Hadamard transform so
what is the Hadamard transform it's a
certain orthogonal map that preserves
out two norms so it's certain rotation
and it has two properties in one like
one vague property is that it mix as
well I might say in a second what it
actually means but what's more crucial
is that it's fast so we can compute it
in time D log G so what is the Hadamard
transform it's a recursively defined
matrix that is the following like zero
Hadamard transform is just 1 and then it
gets replicated four times and then one
copy will flip all the sines so it's
basically plus minus 1 matrix with
pairwise orthogonal rows and columns so
this is nice but of course it's a
deterministic map and we want to inject
some randomness so the crucial idea in
Nylon and chisel paper was to flip signs
at random before applying Harmar
transform so basically for every
coordinate we toss a coin and before
like safe heads we change the signs for
tails but don't do anything and then we
apply Hadamard transform and for the
application that was enough but for our
application it's not quite enough why
because for example suppose that we
started with a1 sparse vector so it's a
vector with only one non-zero XA first
basis vector then if i flip the sign it
could be plus minus 4
basis reactor and even after I apply
Hadamard transform it can be one of the
two vectors and that's not good enough
for us but what is good enough for us is
to repeat this code think a couple of
times and then it works so with the
caveat that we don't know how to prove
that it works but it works empirically
well and I conjecture that it actually
works in theory well it's just I don't
know how to prove it and that's actually
pretty much it
so the overall hashing scheme is to
perform two or three rounds of flip
signs atom are and then find the closest
vector from plus/minus basis vectors
which essentially boils down to like
finding maximum chords in it or
something like this and the valuation
time becomes d log D instead of D Square
and that's exactly where we save a lot
and again this is the statement I don't
yet know how to prove but empirically it
seems that it's pretty much equivalent
to the cross polytope a Lesage with
truly random rotations which are
rigorously equivalent to Voronoi Lesage
with two T gaussians so is it clear yeah
so you have this transformation is to
three rounds and another one and then so
you start you have a query point you run
into this and then what you get you get
the close the closest vector then is
already in a way that you would be able
to get to a set of points and one of
them is close so the good good way of
thinking about it we just use it as
analyzation plug it into the reduction
that I described so then for like every
hash table you would have several of
these things so you think of it as a
hash so it takes a point and gives you a
number from 1 to G so that's the hair
right it's like closer or like true from
1 to 2 G in this case so just which
basis vector is the closest and then you
do the same as I described before so you
just do several of these caches and just
compute these several crashes for the
query and look up the corresponding bin
and retrieve all the data points from
there so now you have the union of many
many bins rather the intersection
because you want to collide on over all
the over key hash functions and then you
do Union so then the overall thing you
repeat many times to boot probability of
success to like ninety-nine out of
everyone in this Union you just when you
try all the point try all of them and
just find the close the closet yeah how
many how many points are in that Union
so we set parameters such that in one
hash table you would have one like oh
like five or like constant number of far
points and everything else is close so
we're happy with those and for the Union
you just get that essentially a number
of bad points is roughly the same as
number of hash tables so it's some like
m to some power how many how in Hastings
so let me actually let me actually go
back to the slide where I show it for
the hyperplanes and for this LSH the
same no we computed so we compute it for
every hash table we compute three times
K atom are transformed so essentially we
in total for it where you can keep like
three times K times L transpose yeah you
need right you need to recompute you
can't I mean if you if you if you will
need to do it that would be very nice
that would save a lot but
yeah
so this slide so basically so now think
of we use our harem like I I'm not sure
how to call it cross poly tope cash
instead of hyperplane hair so in so we
have our point using K quarters K
independent copies of our hash functions
from our family so this is the parameter
K and we also have parameter L which is
how many tables we have so in total we
have K times L hash functions and L hash
tables and for the hyperplanes we have n
2.42
tables for the cross polytope we would
have had M 2.18 so some small polynomial
relatively and K is something like
logarithm of M okay so it's exactly the
same reduction just instead of
hyperplanes use cross Polito and it
works much better
okay let's go back have ten minutes
right
okay so now let me turn to the actually
quite a big issue memory consumption so
if you look at like there are lots of
papers that do nearest neighbor search
in high dimensions say in practice and
many of them use LSH as a baseline and
in many papers you'd see statements like
this LSH is terrible because it consumes
lots of memory let's try to figure out
if it's true or not
so we actually can do can do math and
compute exactly how many tables we would
need for example for hyperplane Alice
age for that random instance that I told
you with 45 degrees so if you have
million points and your queries are
within 45 degrees at random then you
would need to achieve probability point
9 you would need 725 tables so that's
terrible actually it's like million
points come on it's not like it's it's
nothing right now now we would more like
care about like billions of points if
not more right so some 125 times to
replicate the whole thing that that's
not what we want right but turns out
that there is a very nice solution for
this it's called multi Pro Bowlers agent
that was introduced in a very nice vldb
2007 paper and basically the idea is
like this I'm not going to tell you in
the food you take but the AG is in each
table we can create more than one more
than one beam so of course the best beam
to query is which collides with all our
hashes right but intuitively wood also
might look might want to look at the
beans that almost collide so let's say
they collide on all copies except 1 or
something like this so they have a very
nice heuristic way how to do it and
actually there are theory papers that
kind of analyze this something similar
to this multi problemas age but it's
vandalised something way less practical
but in practice you would want to use
multi probability and one of the
contributions of this paper is we have
similar scheme for cross poly tope it's
a little bit more tricky the main the
main source of trickiness is that for
hyperplanes you have only two regions so
we just need to decide whether we switch
or not in each thing here we need to
decide by how much we speak roughly
speaking but it can be done
so we did quite a few experiments let me
just show one of the experiments it's
it's on the shift like it's on the data
set of shift features for I think it's
called imagenet or some I'm not sure so
it's basically a certain data set of
images from from that data set they were
sift features computed and the
parameters are we have million points in
dimension 128 right so the linear scan
takes 38 milliseconds it's actually not
that great data set because linear scan
is already pretty fast but nevertheless
even like compared to 38 milliseconds
this whole thing improves things quite a
bit so with hyperplane and multi probe
you can improve by a factor of two and
cross polytope improves even further
so this is not the biggest gap between
hyperplane and cross polytope I can show
you but even here it will already works
better and in practice you would not
just want to like take your data set and
apply say cross polytope let's say so in
fact turns out that for this specific
data set you can look at it stare at it
a little bit cluster it a little bit
recenter and then actually improve both
like improve results for both hyperplane
nova station cross pollito polarization
here the gap is a little bit larger
actually so cross polytope benefits a
little bit more from it and we have
other experimental results if you if you
are interested in like applying
something like this for your
applications like read our paper so this
is just some concrete numbers oh so yeah
it's it's a great question so we require
to use the same amount of memory the
data set itself so it's roughly double
the size of the data set so it's
actually pretty reasonable so you
wouldn't use like 725 tables or anything
like this if you use more memory
actually both hyperplane and cross
bolotoff benefit from it
yeah so okay so that's pretty much it
but I actually lots of open questions
here some of them are hard some of them
is some of them are meaningful some of
them are not so much but what I showed
in this talk so like essentially I
showed you two results optimal data
dependent caching for the whole l2 which
is a theory result and practical and
optimal LSH for the spherical case which
is more applied and I'd say the main
open question which I really like and I
have no idea how to approach it is to
have practical version of our worst case
two random reduction that would
essentially make the first bullet point
practical and if it is possible to do I
don't know what would be very nice yeah
so all this is a again finding one point
that's close enough to find all the
clothes you want to just within certain
distance threshold recover everything
and approximation however is reasonable
so I'm willing to accept some you know
notional approximation but I want to
find all of the appropriate approach
approximately find all the nearest
neighbors good so here the analysis
shows that for every point from this set
that you care about we would find its
probability 99% so it means that on
average will recover 99% of all the
points of all the closed points again
this said that they are rating over
doing some brute-force search is a
subset that's already pretty much all
the neighbors pretty much so it is 99
percent of them so you can just
essentially repeat this many times and
boost this to whatever you want - of
course if you if you do something like
this your running time would depend on
how large your date like how wide this
set is but right you can't do better on
the size of the cell so so you would get
something like m2r or plus the size of
the search time
any more questions
yeah yeah maybe again this is true maybe
this is the same as my previous question
and maybe not but still trivial so if I
want to find a this the single closest
point we like absolutely closest
absolutely okay probably India then I
understand that 99% those ones are
nearby
yeah but you don't necessarily find this
closest point because you would you
would like basically again like how the
analysis goes you say that in your beam
there are essentially no four points so
it means that you would very quickly
find some close point but not
necessarily the closest so actually
finding exactly closest neighbors in
theory it's very hard precisely like I
wouldn't expect I wouldn't expect it to
be possible to do in like high
dimensions and strongly sub linear time
unless your space is huge but what you
can do often in practice is to say that
look my data set aside that I have a
query I have a like exactly closest
neighbor no approximation and then there
are not that many points which are not
much like not much further than that and
that that often happens actually for
instance in in this shift oh yeah I am I
might I should have told you that here
we look at the like this product like
here the times are of probability 0.9
for finding exact nearest neighbors of
course no approximation because it has
this property that there are not that
many approximately closest points
everything else is concentrated much
further so in this case that that's
actually your best bet and that you can
always like you can often see in the in
the in practice but other than that not
really unless you want bet dependence on
the dimension so you can for instance
show that if you can do polynomial
pre-processing and strongly sub linear
query time you can do sod better than 2
to n it's very easy to fro so there are
there are lower bounds they are not that
convincing because this assumption that
sat you can't do better than 2 2 and
it's like really strong but we don't
know how to do it at least so I wouldn't
expect it to be possible at least easy
little insane</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>