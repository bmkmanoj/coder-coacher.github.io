<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A polynomial bound for Green's arithmetic triangle removal lemma in vector spaces | Coder Coacher - Coaching Coders</title><meta content="A polynomial bound for Green's arithmetic triangle removal lemma in vector spaces - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A polynomial bound for Green's arithmetic triangle removal lemma in vector spaces</b></h2><h5 class="post__date">2016-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/i1z2aEegvYM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome everyone today it's a
pleasure to have lotsa lovas with us he
is a grad student at MIT and he's a
visiting student and Stanford where he
is advised by Jacob Fox and he'll be an
internet microsoft research New England
this summer like he was last summer as
well he does work an extremal
combinatorics and today he'll tell us
about greens arithmetic triangle removal
Emma which builds on lots of exciting
recent work on the cap set problem let's
thank you for inviting me to give the
talk so as you mentioned I'm going to
talk about some recent breakthroughs by
croute lipocalin Bergen guys white and
others and how that can be used to give
a polynomial bound on on an arithmetic
triangle removal emma of been green and
this is joint work with my advisor jacob
fox so doesn't work so so an
introduction so many of you might be
familiar with Roth's theorem which says
that if we have a subset of the integers
from 1 to N and we have no three-term
arithmetic progression then the size of
the density of a goes to the maximum
possible density of a goes to 0 as n
goes to infinity and Roth's original
proof gave the bound of the big o n
divided by log log n using a free a
analytic methods and the best-known
bound is due to best no bounds were
developed later by Sanders in bloom like
give a bound of n over log n times some
log log n factors upper yeah sorry I
upper bound and in terms of a lower
bound a construction of Baron gives n
divided by this e to the root log n so
these two are very far apart now later
cemeteries famous theorem extended this
to caterham arithmetic progression
is using the using a regularity method
so he developed a regular the well he
developed the precursors of the
regularity lemma which was a very
important work with many important
applications so one important
application was the triangle removal
Emma for graphs what is the triangle
removal Emma so roughly it says that if
we have a small number of triangles in a
graph then we can remove a small number
of edges to delete all triangles so to
be precise for any epsilon there exists
a delta such that if a graph on n
vertices has at most Delta and cube
triangles then it has at most epsilon
sorry that should say then we can delete
epsilon and squared edges to remove all
triangles and this was proved by russia
and cemetery using the regularity lemma
the regularity method and this is a very
important use of the regularity method
it has many many important applications
in extremal graph theory in additive
number theory theoretical computer
science and discrete geometry and this
similarly has a very large gap in the
upper and lower bounds as we will see so
right so these these two and and this is
also very closely related to Roth's
theorem now so here we have this these
terrible or these terrible gap between
upper and lower bounds sorry yeah so
it's basically a tower type upper bound
and add something like a so Delta 1 over
epsilon to the log 1 over epsilon lower
bound but so instead of looking this is
kind of looking at it in n we can look
at it in FP
to the end so in kind of vector spaces
over finite fields so let's brings us to
the cap set problem so many of you are
probably familiar with the game of set
where we have 81 cards so each card has
one two or three elements on it they can
have three different kinds of shapes
three different kinds of colors and
three different kinds of fillings and
you put 12 cards on the table and
someone has to find three cards so that
for each of the four properties they're
either completely different or all the
same in that property so for example
here they're completely different in
each of the four properties you can also
have some properties they're completely
different some properties they're the
same and the question is how many cards
can you have on the table without having
a set and more generally how large so
instead of let's say instead of three
properties we have four sorry four
properties we have n properties how
large can a set X in f3 to the N be
without a cap set which is kind of the
generalization of this set property and
this is equivalent to being a three-term
arithmetic progression in the case of f3
it's actually equivalent in f3 too many
different things which for different
fields is not equivalent but on should
mention f3 here we just mean the let's
say the integers modulo 3 and in general
f p where p is a prime will refer to the
integers modulo P which forms the field
and Michelle improved a bound of Big O 3
to the n divided by n and this was later
in a very long paper extended by Bateman
and cats to 3 to the n over n to the 1
plus epsilon for an explicit small
epsilon and this problem is kind of
closely related to many famous problems
combinatorics and computer science
including it's related to the matrix
multiplication problem so for matrices
so the question if you're given two
matrices how how how how much time do
you need to multiply them there's an
obvious algorithm on the order of n
cubed it's suspected that it would be n
to the two plus some small 2 plus little
01 but that's not known I an explicit
number between 2 and 3 is known and this
so kind of lower bounds here could imply
sorry so this problem implies a kind of
could this there gives a method for kind
of improving the matrix multiplication
bounds it also has implications for the
sunflower conjecture and it can all it's
also related to assume is for roth serum
i mean it's kind of a special case of
Roth's theorem in the case of F 3 to the
N now in terms of a lower bound here you
can easily find a lower bound of 2 to
the end by just fixing two of the three
possibilities for each property and
taking only those you can get something
better by taking a small example over N
equals 4 or 5 and then taking kind of a
product construction and that gives a
larger constant a constant greater than
2 to the N but until recently in terms
of an upper bound nothing better was
known and so there's a recent
breakthrough there's a recent
breakthrough result bike route 11 park
so they worked in Z 4 to the n and they
showed that there you can actually get
an exponentially so you can actually get
an upper bound that's kind of
exponentially smaller and with an
explicit see that's about point nine six
and this is this paper is a bit more
than a month old now this their method
was then later extended by ellenberg and
independently guys fight to prove that
in F P to the N if your three AP free
then you get an upper bound then you
basically have so if P to the N total
elements and you can get sort of a much
smaller upper bound again exponentially
smaller for explicit CP so in particular
this gives a better bound on the cap set
problem and blade later blazey act
church cone groans and independently
alone and independently nesslun showed
that this proof extends to the
multicolored some free problem so what
is that the multicolored some pre
problem so if we assume that we have now
three different sets x y and z and we
have the property so their index and
three elements from so an element from
each of the sets sums to 0 if and only
if they're indexed by the same number
then we have the same bound on M as in
this theorem and this is a
generalization because if we take X to
be a why to be a and Z to be minus 2a
then if we have an AP free set then we
obtain three sets that have this
property and climb Berg prove that the
bounds that you get here is for F so the
bound in the multicolored some free
problem over f2 is sharp by the exponent
is sharp by a construction of foon
Kleinberg okay so the proof of the cap
set problem is actually rather short and
nice so I'd like to present an overview
of that proof
and it used it it's a very nice
algebraic proof so it uses the
polynomial method so fix p and let so
fix the prime p and then we look at so
we look at polynomial so let's let em ND
be the set of monomials of degree at
most D in m variables so total degree at
most D such that its degree is less than
P and H now the point of that being less
than P and H is because in terms of
functions you don't gain any more
functions by having a higher degree in
FP so so now let's fix the polynomial Q
of degree D so this is kind of the this
lemma so what I'm presenting on this
slide basically appears in the fruit
left pocket all those formulated a bit
differently for P equals 2 and then it
was extended by Ellenberg and guy so I
and their proof so fix the polynomial Q
of degree D and then we define this
matrix mq so its rows and columns are
indexed by FP to the N and it has and it
has entries p basically the entry x
corresponding to the vector X and the
vector Y is P X plus y sorry yes Q's p
sorry so then the key lemma in the that
kind of makes all this possible is that
the rank of this matrix m q is at most
two x MN d over 2 now why is this the
key why do I say is the key lemma so
it's not difficult to show that the rank
is at most mnd but m ND half is in
general much smaller than m ND so kind
of to illustrate this if the so if n is
large compared to d then kind of the
distribution of the degrees of monomials
and n variables
it's kind of a very concentrated
distribution and so if we take d to be
about two thirds which is what we're
later going to do and d have to be about
13 then this is kind of a huge decrease
and how do we prove this lemma so we
could rewrite q X plus y so this is the
sum of many monomials of degree at most
D so each such monomials it's either the
X term or the y term has degree at most
D half so then so this isn't a unique
decomposition but one way to do it is to
collect all the terms where the X
monomial has degree at most D F and then
in the remainder terms the why of
monomial must have degree at most D half
and so we can write the sum in this way
and now this is actually a decomp now
for this matrix this is in fact a
decomposition of this matrix into two MN
d half of matrices of rank one right
this when it's indexed by x and y this
is a matrix of rank one are there any
questions about this or anything so far
okay so so now how do we use this so
I'll present the proof for the
multicolored some free problem so take x
y and z as in the problem so again what
that means is x i plus y plus Z I equals
0 but any other triple is sums to non
zero and then the claim is that we have
the bound on em of so for any T
basically we have this bound with that
most two times the monomials up to
degree t plus this is the monomials with
degree above 2t plus 1
and how do we prove this so we can take
a polynomial Q of degree at most so
let's assume to t is bigger than is
large enough so if we take a polynomial
Q that has degree at most 2 T plus 1 and
vanishes on the complement of Z and is
non zero on this many points of Z why is
that so the compliment has p to the n
minus M points so this is the dimension
of the vector space of all such
polynomials is MN to t plus 1 the
compliment of z has p to the n minus M
points and you can show that this can be
done so we can so the dimension of the
space of polynomials that vanish on the
complement of Z is MN to T plus 1 minus
P to the N minus m and then one can show
that then there's a there exists a
polynomial whose support has at least
the dimension of the Azalea size of the
dimension of that space and then so we
have this matrix Q with the sums and
let's look at the sub matrix index by X
cross Y so if you're so we kind of have
a bijection between x and y so if we're
off the diagonal then that means that we
have X I and YJ for I and J not equal
and then we know that we're not in Z so
then this mq off the diagonal must be
zero if we take a diagonal element then
we basically obtain the polynomial at Z
corresponding to that at zi so what this
means is that we have we know that we
have this number of nonzero elements in
the diagonal so the rank of this matrix
mq is at least of this number on the
other hand we saw previously that it's
rank is at most two MNT and so this
gives us the inequality MN to t plus 1
minus this number is at most
this and if we rearrange it we obtained
the inequality given here and then if we
choose t to be roughly one-third times
so P minus 1 times n is the total
possible max degree so if we choose it
to be about one-third then this P to the
N minus I meant to t plus 1 is kind of
the monomials here which is basically
the same as the monomials here and we
obtain the bound 3 x MN p 9 s 1 times n
over 3 and then if you just care about
some bound you can use Bernstein's
inequality if you want the exact number
and which they do you can you use
Kramer's theorem to show that that gives
an exponential decrease in the upper
bound are there any questions about this
gaku bow yeah so so these monomials
their degree at most p minus 1 in each
variable so it's basically x12 the i 1 x
2 to the i 2 xn to the I n and each of
these goes from 0 to P minus 1 so we
kind of so what's the distribution of
their sums it's basically a number from
0 to P minus 1 times n and if we take T
to be P minus 1 times n over 3 i'll
leave off the integer part then here the
two MN then here we have two x MN t and
this p to the n minus MN to t plus 1
will be the same so then this gives
three times MN p minus 1 times n over 3
so another way to think of this is we
have P to the end total monomials if we
take a random one then we basically have
n random variables that are each 0 to P
minus 1 what's the probability that they
sum to at most p minus 1 times n over 3
their expectations p minus 1 times n
over 2 and so we get an exponential
decrease and we can calculate the exact
exponent using Cramer's theorem okay so
now let me move on to the arithmetic
removal Emma so again if we have F P to
the N will say three elements form a
triangle if they sum to zero and green
proved an arithmetic triangle remove the
lemma which says which so it's basically
the earth metic analog of the triangle
removal Emma so for any positive epsilon
and prime P there exists a delta greater
than 0 such that if we have three sets
that have a small number of triangles so
here the total possible number of
triangles would be F P to the N because
x and y you can choose anything and then
Z the choice of Z is fixed and so if we
have a small number of triangles compare
Tom buddy we can have total then we can
delete a small number of elements and
remove all triangles and green the proof
that green gave works for general finite
abelian groups so he develops a free
analytic arithmetic regularity lemma and
uses that so later crawl Sarah and vaina
gave proof which holds for general
groups which basically uses the triangle
removal emma for graphs so I believe I
can briefly sketch the proof so we take
three sets of so suppose we have these
sets x y&amp;amp;z we take three sets of
vertices each corresponding to the
points and FP to the N so now if we have
a point in each of them we connect u and
v if their difference is in X we connect
to V and W if their differences in Y and
we connect w and you if their
differences in z now if we have a
triangle so i'm presenting it for
abelian groups but this works for
general groups so if we have a triangle
then that corresponds to exactly big end
triangles in the graph where begin is
the size of that P to the N and vice
versa if we have a triangle in the graph
then that must come from a Triple X plus
y plus Z and so since we had Delta and
square triples we have at most Delta and
cube triangles which means that we can
remove epsilon n squared edges to get
rid of all triangles and now if we have
a point in X we remove it if at least so
it has n edges corresponding to it and
we remove it if at least a third of the
edges corresponding to it are removed
and so then any triangle x plus y plus Z
at least one of the three corresponds to
n triangles here and at least one of the
three points must have been removed one
of the three edges must have been
removed and over three times now so the
removal emma has implications to
property testing or it's very closely
related to property testing so this was
initiated by rubinfeld in sudan and
subsequently buy gold ray Goldwasser and
Ron
for combinatorial objects so the general
idea is we look at functions f from some
domain to some range are and we have a
property of these functions so which is
basically a subset of these functions
and now we say that a function f is
epsilon far from satisfying this
property if there's no other function f
prime that can be obtained by from f by
changing the value on at most an epsilon
fraction of the points and a typical
property testing problem is to quickly
decide whether a function satisfies this
property or is epsilon far from
satisfying this property with good
probability so if it's kind of between
the two then it's the algorithm wouldn't
be very good but if it's either
satisfies it or it's far from it then
the algorithm works with a very good
probability and we think of D as being a
huge set and we make queries for certain
elements of d what is the value of f of
d and we want to determine again if we
have a function that we know either
satisfies it or as epsilon far from
satisfying it we want to determine
whether the function is in p with the
with only using a small number of random
queries so for example we can look at a
triangle free so d is a domain a
property yeah
so for example let's say so let's say
the domain is kind of three destroyed
copies of F P to the N or in other words
we have a triple of functions f1 f2 f3
which we think which we can think of as
the characteristic set characteristic
functions of sets and the property is is
this triangle free so if X plus y plus Z
we cannot have each of these be equal to
1 now using the arithmetic removal Emma
we know that if a function is epsilon
far from being triangle free then we in
fact have a large density of triangles
so if we start querying random points
random X Y and then if they're both 1
according to the function X plus y then
based on how the Delta from the lemma we
can we can say that if we have a certain
number of queries and with high
probability if it has a triangle we will
see it and the point is that because of
the because the bound on the density
depends on epsilon but it does not
depend on n here we only need the number
of queries we need depends only on
epsilon and not on n so the so if we
have a good bound for the removal emma
then that implies a good upper bound for
the removal emma then that implies a
good bound on the number of queries in
terms of we need in terms of epsilon so
so one question is can we test it with
the polynomial in 1 over epsilon queries
so I promised to talk about the bounce
so I'll talk about the bounds of the
removal Emma so let t be this tower
function so that means we take T 1 to be
too and TN so TN is a tower of tues of
height n so basically TN plus 1 is
always to to the TN and this function
grows like
enormously so even if we just take t5
then we have a tower of two's of high
five so that's already two to the sixth
out 65,000 which is I think way more
than the number of atoms in the universe
now greens proof of his removal emma
gives a bound so it gives an upper bound
on 1 over delta which is tower in
polynomial of 1 over epsilon so this is
obviously very huge which means that
Delta is actually very very small so you
would need to take a very very large n
in order to actually see the
implications and so we have a similar
kind of bound for the graph theoretic
triangle removal mo which means that the
crawl Sarah and vana proof gives the
same kind of bound now Jacob Fox
improved the bounds in the graph
triangle removal Emma instead of T of a
polynomial in 1 over epsilon we have T
of log 1 over epsilon so that's in some
sense a huge improvement who are still
enormous and so using crawl Sarah and
vanos proof we have the same kind of
bound for the arithmetic triangle
removal Emma now haha me such Devon
tiziana gave a kind of free analytic
proof so it doesn't go through drafts
for F 2 to the N and but that gives a
similar kind of bound with this log
Tower of log 1 over epsilon now if in
the case of f 2 to the N butta butta
charya NZ gave a lower bound of
approximately 1 over epsilon to the 4.8
47 and foo and Kleinberg gave a lower
bound 1 over epsilon to the 13.2 39 this
is an explicit number minus little 01
and but until recently nothing better
was known even for the specific case of
FP to the end so using the kroot let the
ellenberg eyesight results with my
advisor we can prove a polynomial bound
on this for greens trying to remove a
lemma if we fix p so for P equals 2 we
get a number C 2 1 + 1 / 5 30 minus log
2 3 and this is this is the same number
that I had on the previous Kate previous
page yeah yes well so the way this is an
upper bound on one over Delta so to
lower bound the Delta and there's was a
lower it was basically a construction
which gave gives yeah yeah Susan so this
is a general so this improves this
results and in general for P we get an
explicit bound and so through personal
communication with a climber expire and
so when they're apparently currently
writing up a proof that shows that this
is tight for a general p okay so how
does this proof go so first we assume
that we have these three sets x y&amp;amp;z that
are disjoint and we again to note n as
the size of f 2 to the N and each
element is in exactly the same number of
triangles so some Delta N and then we
want to show that in fact x y&amp;amp;z must be
small so let's take a random sub space
of dimension log 1 over 5 Delta so
basically the size of this subspace will
be approximately 1 over 5 Delta and then
take the intersection of each of these
sets with this subspace
and now we call the triangle good I'm
sorry that should be X prime Y prime Z
Prime if it is the unique triangle once
we intersect with this subspace for each
of these three points and now we further
so we define X tilled y to z tilled as
subsets of X prime Y prime Z Prime the
set of points that are in good triangles
and so for an explicit see something
like 1 over 100 or one over a thousand
we show that the expectation of the size
of X prime is at least the constant
times the expectation of the size of so
the expectation of the size of X stills
at least constant times the expectation
of the size of X Prime and further it's
not difficult to show that the expected
density of X prime is up to small errors
about the same as the density of X so
how do we show this so so that so we
want to show this claim so first I want
to show I want to argue that if we have
a triangle x1 y1 z1 and we know that
that is contained in w then we have a
good probability that it's three
probabilities reef it's that it is good
so it's the only one that each of them
is contained in so let's look at x1 so
x1 is in t triangles and because I said
that XY and z are disjoint these t
triangles correspond to different 2 t
different 2 dimensional sub spaces i'm
sorry i forgot to mention that i'm going
to concentrate on the f2 case which is
slightly simpler than the general FP
case so the probability that one of the
other ones condition on this being in
the subset is approximately due to the D
minus n there are some lower order terms
in the fraction here
and now we have so we know that this one
is in our subset we want to say that
none of these are in the subset so the
bound the probability of that so we have
t minus one of them and each of them
it's approximately two to the D minus n
so using a Union bound we have
approximately delta x 2 to the N times 2
to the D minus n and since dealt since
two to the D is roughly 1 over 5 Delta
this gives us an upper bound of one
fifth okay so what we have here is that
if we have a triangle contained in X
prime Y prime Z prime then with
probability three-fifths it's good so
now if we just have that x1 is in this X
prime what's the probability that is
good so we have again it's contained in
t possible triangles for each of these
there's approximately again 22 the d-
and probability that it's contained in
our subset and if it is contained then
it with probability three-fifths it's
good and now these t events here are
disjoint because each of those events is
basically saying this one triangle is
good this one triangle is contained in
the subset and none of the others are so
these two events are disjoint so we can
get a lower bound by t times this number
and again this gives an explicit
constant C tilled so as these seas come
in is Fergus is the fact that two to the
D is not exactly 1 over 5 Delta it's in
a range and similar it comes in from
smaller order terms but it's an explicit
constant ok so we saw that this
expectation is at least a constant times
the expectation of X prime use density
and expectations at least the density of
X so that means that we can take a sub
space of dimension D where the set of
points in X that
in good triangles is at least a constant
times so the density of points and go
triangles at least the constant times
the density of points in X now if we
only take the good triangles then this
is actually a multicolor a sort of
multicolored some free sets so the union
of these forms s'okay if the three sets
are indexed by which triangle there and
that gives exactly a case of this
multicolored some free sets so that
means that we have a bound on the size
of X tilled of size 1 minus c2 times d
by the argument of Ellenberg and ice
white and then if we divide by 2 to the
D then we have that this X tilled has
density at most 2 to the minus c2 times
D plus a low order term which is
basically Delta so 2 to the D was
basically one over Delta so this is
basically Delta to some exponent plus an
error term and here by little 01 I mean
that as Delta goes to 0 this error term
goes to 0 and so this implies the same
bound for X up to a constant factor now
so here we said this is if each of the
elements is in the same number of
triangles so this same proof goes
through if we have if they are roughly
the same number of triangles so at least
Delta 1 in and out look at most Delta 2
and we get an upper bound delta 2 to
this exponent times their ratio and here
this is Delta 2 and delta 1 are not
going to be a constant factor part
they're going to be a log Delta a factor
apart are there any questions about this
and now for the proof we show the
following claim so suppose we take
epsilon n disjoint triangles then if we
take kind of the union of the X size 4x
why I saw r YZ guys for Z then just
among those sets we must have at least
Delta and square triangles so why does
this prove the theorem basically the
number of disjoint triangles we can take
in a triple XYZ estimates the number of
points we need to remove by a factor of
three so in one direction if we have
these district triangles we must remove
one from each of them on the other hand
if we if we just start greedily removing
triangles and so then we get a set of
disjoint triangles and if we really
remove each edge in the triangles then
if we can't get any more if we can't
extend the set of disjoint triangles
than if we remove the edges in all of
them then we what remains is triangle
free so so we just need to show this
claim so now we want to kind of equalize
the degrees so first if we have a point
whose degree is too high then we delete
it so if we if at some point we have
density delta prime if we have a point
so on average since we have epsilon
endpoints we would have delta prime over
epsilon degree so if it's higher by a
square log factor then we delete it and
then we update delta prime and we keep
doing this and because so because of
this log squared factor we can show that
we do not we delete at most a fraction
of the points so as the this hat yeah
basically due to the sum of the
some of the an infinite series because
of this square here ends up being finite
and so when this process ends so we've
gotten rid of the points of high degree
and we've kept at least a constant
fraction of the epsilon endpoints so
then after that we delete points with
degree less than delta prime over 6
epsilon and since we have at most
epsilon n point we have epsilon end
points in each set originally we end up
deleting at most delta prime over to
fraction of the triangles so we keep at
least a delta prime over to fraction of
the triangles which implies that since
we have an upper bound on the degree of
any point we have that we have at least
epsilon over poly log 1 over Delta
points remaining so now we can apply the
previous lemma with Delta 2 and delta 1
and we get so I've moved the poly log to
the right side here but we basically get
this to the power of C times their ratio
so this gives some log factors which
I've moved here and then it gives that
epsilon is at most this delta prime of
our epsilon x to the c2 times some
positive times some poly log terms and
then if you rearrange this this gives
epsilon is at most so this gives a
epsilon to the 1 plus c 2 is at most
delta x poly log delta
delta prime which is at most delta and
then we can reverse this to obtain the
polynomial bound on Delta in terms of
epsilon there any questions okay so
there are still many further questions
in this area so I'll just list a few so
we generalized the cap set problem by
saying that three points form a
three-term arithmetic progression but
you could also generalize it to pee so
for f 3 this was equivalent to saying
that there's no a fine line contained in
our set so an open question is if we
have a subset in FP to the end where p
is bigger than 3 how large is set can we
have that contains no a fine lines is it
sub is it can you get an exponentially
better bound or is it p to the undivided
by just sub exponential another question
is can any of these techniques help with
Roth's theorem and another open question
is can any of these techniques help with
the graph theoretic triangle removal
Emma so for the graph theoretic triangle
removal Emma there is no polynomial
lower bound by a constructive Behrend
but still the lower bound is roughly 1
over epsilon to the log 1 over epsilon
and the other and the upper bound is
this tower but due to foxes can proof so
there's a huge gap there and thank you
for your attention
any questions I have a question about
your question too so because of the
route one you cannot hope to have such a
big improvement for us here at night
they're in fp2 the end you get an
improvement which is putting on your ear
size of yeah you do not have a
polynomial thud so it doesn't need
naivety maybe means that those
techniques and that's not the case well
I mean yeah maybe maybe the adjectives
question is your successful now that
might be worth looking after you say a
few words but very construction noticing
the construction yeah Titan can you see
that constructions on how right so this
proof I mean for episo for f2 this proof
basically shows and choose n and over
three times a constant and their
construction i think is enter than over
three times e to the minus i'll just
write c root n and they basically take X
as a subset of the sets of size and over
three actually this is probably too low
for anyone to see so basically okay so
they take X as a subset of n over n over
3y also so wait so these are coordinates
of weight and n over 3 n over 3 + z2 the
coordinates with weight and over and
divided by 2 n over 3 and now this
doesn't work because
well clearly it doesn't work but then
well I don't want to go into this proof
but they but they kind of take a right
take random subsets a certain i'll just
say they take random subset certain way
to kind of weed out the they construct
they basically construct a lower bound
for the multicolored some free problem
and they kind of weed out bad sets in a
way such that what they're left with is
is a is a good case of the multicolored
some free problem and they can show that
with that in expectation they keep
enough of these points I don't know more
of this proof off the top of my head</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>