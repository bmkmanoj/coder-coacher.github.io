<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NSF Interdisciplinary Workshop on Statistical NLP and Software Engineering - Session 4 | Coder Coacher - Coaching Coders</title><meta content="NSF Interdisciplinary Workshop on Statistical NLP and Software Engineering - Session 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>NSF Interdisciplinary Workshop on Statistical NLP and Software Engineering - Session 4</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kSkiMZKXl5A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
en tiendas from Iowa State University
and the group has been doing a lot of
interesting work with large software
repositories and how to make them
accessible on the internet and you can
go there and get all kinds of data and
he's going to tell you all about it all
right oh then no way okay good okay good
morning everyone my name is T Nguyen and
I'm from Iowa State University um today
I would like to talk to you about the
block code repository and mining
infrastructure that would be what I was
there University so they do the John
work with several people at Iowa State
University on when is my former student
currently postdoc with me and Mike
O'Leary destra John is an expert in
programming languages and the rest are
the student and me and Andrew debt and i
would like to thnkx NSF for the support
for this work and i hope that throughout
the discussion we can get more user of
our infrastructure and make using in
community so let me first start with the
open sore posit ori so here is the
several repository out there that's
supporting hosting open sore repository
and this is the big wave of knowledge
okay people can learn a lot about the
sub wear division process what thing
have been done good not good and we can
learn from the pats and support the
futures and i really don't need to to
convince you that we need to my software
in arts three yesterday tao given an
excellent top on the survey of the area
and mining subway repository and we and
then this morning char konso mentioned
again that several interesting research
result can be can be get from mining the
processor so we can learn from the past
see what have been doing well something
that we are not so well we find empty
patent and then guided the current
activity in software development
and guarding the future development
however one key problem that we identify
in mining subway repository is the high
end barrier to entry in Bigg good mining
so any interesting scientific discovery
usually having to part the first marker
you need to mine the raw data and then
I'll now analyze the data to to make the
scientific discovery and the mining park
is actually a lot of high cost at
beginning for everybody to do any good
scientific discovery and now I'll take
one simple example in subway engineering
and mining software repository let
people interested in see how many bug
fixes at the noon check okay so this is
interesting question because that's
supporting the automatic program repair
tools and the recommendation to nip this
see that the Loon checks are very open
in bug fixing you can recommend it in
the future for for debugging a program
however do this that not an easy task so
first you have to identify the host
repository that maintaining on the open
source project those hoes also providing
the API so you need to write the code
that conform to the API of the host to
mind the project made of data and then
you're ready to write a coded for its
project you look at to have the repo or
not if yes then accept the repo
traversing mining the revision make sure
the revision is a bug fixing revision
and then download on the source file for
its source file you need to traverse the
code mining the source code to see if
any noon check for bug fixings and
finally output those number or the list
of noon check-in in the program so this
involving a lot of external libraries
some expert on mining look at different
library different hosts and different
revision control system I'll take one
seamless small piece of this process is
mining the source code here for his
situation
so we wrote a program where in Java with
a hundred by over a hundred forty live
code and that program access to the host
repository and the host for example sort
froze our github we need to use library
to accept those sorts of github so SPN
get those repo return the data in JSON
so the program have to use the JSON
library to process the data and then we
you have the file you need to pause it
using eclipse charity to traverse the
tree and getting on the noon check okay
and because we want to have the program
that's scaled to the last coast corpus
we need the you polarization a framework
for example Hadoop so we need to write I
place it many operation code in the
program and so if you look at the whole
big picture that require a lot of
expertise in different little details
before you can actually do something
interesting before you can actually do
any analysis on it so that's where we we
think about a solution for that is the
domain Pacific language that's
supporting the last keokuk mining and
this is how where our expertise
complement to each other where my
collaborator at Iowa State read estrogen
is a expert in programming languages and
I am working on big big coat mining so
we want to provide the domain Pacific
language that can easy my the last game
code copies and how gone in my is to
having a simple language that high on
the details up you know external library
handling the repository mining except to
the ability repo and get the data and
also hiding on the data paralyzation so
the user don't need to know where to
exit writing I place it curve for
paralyzation at all and more importantly
we want the program that can scale 2
million project and running efficiently
will then get resolved within minutes or
hours
rather than days or weeks or months and
here is the architecture up the bar and
fresh structure that we have at Iowa
State so first currently we having
supporting the downloading data to to
hosting website which is github and
source forth and we'd wrap it get it and
transform the data and catching the data
into and start it in now cluster and
when the user put the query on the
website the query will be sent to our
server our server will take and come
hide their query in written in wha which
is the language that we support the
mining and compile it into the Hadoop
program and deployed it in the cluster
execute it and return the results to the
web page as we as we speaking in the we
are preparing for the release of the the
next version the boil which is running
on Eclipse IDE so actually if you go to
the blog website you will see or even
link let's say Eclipse boy for Eclipse
IDE and we will announce officially in
the next couple day so you can write
your program the board program inside
clips with on the syntax highlighting
and you can send a query from eclipse to
our cluster get result back and
displayed it and process the data so
that's a that's more more user friendly
than the web page that we have in in the
past two years so this is something that
we are working on and we want to explore
the ID support for that like debugging
testing later on in our future work now
let me zoom in the programming because
I've been talking about how easy to
write its program so this is the program
that traversing the code and look for
the noon check ok so the program is
basically a declarative up input and the
output and here the output is the
applicator and to to visit the program
we can use the visitor design pattern
and if we see any statement
if statement we look inside the
expression condition expression and if
it's an equal not equal in the right
hand size the new value we can send one
to the applicator okay and then the
acura TL will stun them up and I'll put
the result so the program is small no
external library involved and you don't
need to write details of paralyzation
it's automatically converted into Hadoop
version come in Java and running our
cluster and and more importantly the
most important thing is can scale to
large data so this is how our recent
data as of September 15 we have almost a
million project in github in short short
with 23 million revision 146 million
unique foul with 71 billion ast node so
and then when we run the program it's
mon program i show you earlier on the 69
porn a million file with 18 billion st
not the result come back within 20
minutes so it's very fast and then you
can use you can analyze that result and
then make other analysis and finding in
your mining research so yeah I able to
support that sitting in which Lord can
you have a little really in this room
started asking phrase that's a good
question so but we have that we have the
funding support to improve our cluster
and and we are in the process of budding
and upgrading more computer in that he
supported Microsoft or not actually
depth that is something that's also
interested to our group because when we
found the research moving on to
something infrastructure that's support
community that's a lot of engineering
initiative that we need to make their to
make it robust and usable
that's our intention yes we we discussed
that the months ago and that we own
agree in the team that will make it
available cuz I think the idea would be
my command Microsoft probably would be
at least willing to go for Gordon next
yeah I should think so but I mean we
have this edger HD inside stuff for you
can host Hadoop clusters and Azure cloud
okay yeah that's a really interesting
question but I can't say yes right now I
would love that yeah so yeah all right
so if you want their hair is the program
work so basically the the cluster el
agua server instantiate each web program
that i wrote earlier for each project
okay instantiate it running on it and
every time it's see a noon check it will
send one to aggregator the cow irrigator
and the aggregator just you know because
of the summer aggregator is some on the
resolving now put the counting upon the
noon check this very simple concept and
on the automatic paralyzation where's
handle by the compiler and we providing
several alligator for some mean tough
bottom set collection you can check the
programming guy on our website and for
own building function and aggregator as
supporting and then the the blast
custard will translate the query into
Hadoop MapReduce coat and run in the
cluster because we're supporting mining
so we need to have the domain Pacific
types and this is where my expertise
come to the picture where we have to
write different Monday bowdoin domain
pacific time for handling different kind
of artifact for mining somewhere
repository for example here you can see
curve repo is a type mining sub or visit
type get fixing snapshot is something
predefined the user-defined function for
domain pacific up mining we have to rise
several based I've and tie for
supporting the mining software
repository for example the
Jack a mandible Godfrey pearl it's
called repellent multiple revision is
code revision hair multiple file each
found him SD root and then having on the
abstraction factory that you can
traverse to over the program we cannot
provide own the domain pacific task we
know that so therefore we support the
user-defined function so you can define
your own function either in what
language such look like this you declare
the function the return in the body or
you can write the user-defined function
in java and we've integrated into the
compiling process in ha do and then you
can call the function for example is
fixing snapshot I explained earlier this
is one up the domain pacific function
that check the five type look at the
extension of the file and see if it
matched with southern name now here's
the simple fixing revision that many of
us here using in mining in sub
engineering mining software repository
area so we look at the commits lock and
see if the clement lock containing the
work like fix error bugs are issues and
we can take we consider it's the fixing
revision of course it's the very shallow
but you can do more interesting with
your own prone define function for how
how the revision should be a fixing
revision um the most interesting part in
war is inborn language to the visitor
okay so you can visit the code and do
interesting collection in code there is
in fact is very simple you're going to
run some stamen before after you see
some type some no time okay and we also
can support the white card types or list
type for example if you see t2 t3 t4
then some statement will be executed or
some default behavior for the visitor as
well and the default reversal in wha is
a depth-first traversal so you can see
before a before be before see
so on and at the same time we also
support the custom traversal so if you
don't want the visitor to traverse the
cheer and upsetting note you can say
stop don't traverse it or you can elicit
Lee traverse certain notes so you can
define any traversal that you want for
your program to collecting to collect
the interesting feature from from
different cup statement in source code
on here is an error may the data program
that and so accept the metadata for
example look at revision look at the
size of the code 40 this program simple
program looking at on the Java adept can
be accessed your svn and see how many
file that chain / revision so that's
concern rate and so everything is tech
the function lang and then return it to
the result so those are the mining for a
different kind of thing but for our
workshop I'll prepare a few example that
I will run life to show you that how we
can do it for NSC example so the first
one is the Engram model so this is a
prepare a field list so this is the
website by the way and you can ask for
the user and we will almost Graham user
for for everyone within the day or so
and you can access and run your program
here's I prepare a few program that show
in for the NL community so this is the
program that compute and Graham in the
many project that we have okay so the
idea of that is very simple you can look
at the code here first get the snack
shop and Java program visit the snapshot
for each of the steps that we have and
for each of the note we traverse the
first phase is to
and passed the program CMEs denote and
posit pushed on the stack so on up the
bar program the first phase is the mare
face mapping face will will unpause it
so you see here pushed on a token if you
see the follow we push for open and
recursively cone in the initialization
and recursively conan condition and
update and push on the token there and
we do the same thing for other variable
expression etc and in the second part of
the program which is the reduced space
the applicator will pop they took the
net we pushed in the first phase push it
up and forming the string in this case
this program I hard coded for the Engram
with n equal five and then we will
counting an output aggregator where I
put the top 10 most popular and Graham
in in the coppice okay so that's very
simple program let me initiate it and
see change into the motor we can run
okay and I will run it on the small data
set so that we can see the result so any
guess what is the most and 5 gram most
popular one and I equals 0 okay we use
it a lot in their paper right to speak
does it run okay so ah let me see if the
result I think it done okay so this is
running this morn data set so you can
see the result I can show you is all
right away so you can see that the
output right there or you can download
it still waiting I hope the server don't
die on me okay here we go so you're
almost correct
so it's equal zero semicolon semicolon
plus plus and then you know semicolon
semicolon plus plus I closing or the for
loop here right so that's in fact
Charlie's right we speculate that this
one is the most popular one and we use
it many times in now paper and
naturalness in source code we claim that
source code repetitive and this is the
evidence right evident that this this
for loop occurs quite a lot and if you
look at that there's also and this for
loop and so interesting this empty for
lookin so very popular but not as
popular as the for with i equals 0 and
so okay so you can do you can mine up
sequin then you can do own analysis on
that with your own program later and we
have you with the first part that mining
the data from the source code all right
I also prepare other tests that do other
thing all right so this is the program
that we look at the method call so
instead of on the sequin you know open
parenthesis for I you not interesting
that you might interest it in the
sequined admitted call so many of their
research in self engineering try to look
at the user pattern I think we talked
about it the user pattern in chart top
that look in net API use it Adam so we
want to see what the path and the sequin
pattern that that program looked like
for example the sequin like Phi input
stream and then Annunciation and read
and glow so this is one API use a
pattern example and in and here the
program in doing that I just need to
traverse the snapshot look for the
mythical and chaining them collecting
them and and then when you got the
result you can do you know frequent
mining on that and let me show you the
result
actually this one known so very fast
this the program runs about two minutes
or so on but that the output very big so
you see to point 87 megabyte so here is
the list of the methods for its mythical
in east of the method body and remember
that here I do not feel I do not keep
the interested api for example if you
like to look at jelly cake you can add
into the code that the barcode i show
you earlier to filter keep only the API
here on the method call may be submitted
conan within the project so for example
this code you can see this is more like
compiler code and they have in a lot of
translate sustainable clear set fire
description in though thing but if you
having the you write your own code that
filter the project pacific and keep on
leading api you will get an interesting
call get five example get fayette name
here it's down to bottom here and then
when you have in the list of mythic on
you can do frequent itemset mining to
find out the frequent up method call on
the so you will get you get the API use
it patterns from the raw data that we
provide any questions over the other
tests that i want to show you is
actually mining the control flow so we
look at the here's a is a little bit
longer but we look at the control flow
you know what type of control statement
in the program that most popular okay
and any guess
so let's view the output oh okay still
waiting this is the castle okay here so
the here is not actually truly control
yet we just look in the SD type but you
can do more furthermore more finer grain
analysis to get the real flow but you
can see here is the statement expression
get a method call in an exit to variable
because quite often I keep only the
sequin of tree element but you can
extend further if you want or hear you
have a method declaration inside you
have a block and then inside the block
you have an expression so it's also very
popular so you can do other thing on top
of this to do except to more interesting
result from a raw data but we provide
the abstraction level that you can get
over the sim that overcome the simple
challenges that hinder the several
mining mining tests and currently we
have more than 300 user and you can go
to this website asking for user and and
you can test and running on on many
other query involving not just the code
at the day more than I give you is about
code but here we also having the
metadata that you can my on for example
what is ten most use programming
languages our project management so how
many project were created or cell
created and how many project using
certain revision on version control
system or was it the most if I most
you'll Ison in in those data set and how
many project you more than one licen and
many more other interesting and so
please visit the website to help you
hear more
mission and one thing is that the Dubois
infrastructure have been selected to be
the data set for the mining challenge
competition at sub engine conference on
mining sub we're positive 2016 and the
deadline is fairly 19 and that's what we
talked in the previous session on the
competition and so ms our conference
have been storing the data over the year
yesterday tile listing on the on the
link to the previous MSR from 2006 right
and up to up to this year so we have
some other ongoing work so clearly the
the title supporting board for eclipse
ide we are getting there however there's
through several other challenging
research as well as the engineering
effort for example building debugging
you know how to debug for the big data
program some program can run you know
hours so how do we know that when we
stop the program and see it's not
correct are testing them other efforts
involving optimization when you have
multiple people connecting and and you
know get the curie are very similar data
so how can we change them together and
reuse other query and you use a
component not a curry of other people
then have been have been have been posed
on on on the website or have been sent
to the server and so facilitate the
results sharing collaboration is also
challenging the several software
maintenance and program analysis can be
done when the program was loaded in ID
as well so research on program analysis
the booking testing ID support like code
completion for example for bois program
ones are very useful and finally i would
like to to say that we have been used
the blood to get the raw data and and
and process some raw data that can be
benefit for nao se community for example
the work that we publish the 13 chart
mention about it so we have the unmute a
shin on the code so the Engram model
keep only the lexical but we want to
keep on so early annotations semantic
annotation like what are the data type
already the token type like few except
method call variables and on those thing
we have the data set that can give you
you can you can mind a code and the
sequin of semantics and notation
together with the code as well we also
having the API use a pattern that we
might using the graph based statistical
language model which we published this
year Dixie so we're having on the data
that an api user that i show you the
fine new retry but we have it in a graph
model the graph data structure and and
other to think that also interesting the
data set that is a use it for language
migration we seen in the previous talk
and we were able to mine the data that
having to implementation in java and c
shop and mapping them and we have a bell
now about 80k eighty thousand pairs of
methods in java and the corresponding
version implementation and see shop that
you can try out for your you're my grave
stitute for your translation algorithm
or model when last we have in the corpus
that doin the ghost synthesis so given
the tax we want description of the past
for example create file with to the file
right another foul and then we
synthesize a coat and put out the api
use it for for that task and the corpus
is from the text and the code and now
and we might it from the stack overflow
and get that care of coat and text cross
funding lee for the task so i'm happy to
share with others in
in an audience and then and also open
for collaboration in and we can talk
more in the next few days and so that's
the let's conclude my talk and a happy
detect question easy web interface the
only way to access to data or do you
have a like a large double archive of
this metadata that we can download and
have to analyze all time oh the the
website or the Eclipse IDE that you can
download the data from chi minh download
you have a plug-in but i understand they
still talks to your server but i wonder
if you have like a big archive top of
the meta data that we can download the
net I know and then analyze of offline
so currently if you want to get the data
you have to go put the query and go to
our server we don't put the raw data
because several other legal issue that
we put the raw data and we cannot
distribute it something that we are not
own so we can only download the
qualities out clears up yeah thank you
yes though oh like your infrastructure
is very useful for serving as a building
block for like curiosity driven
scientific discovery and the sensing
exploring this data set for inspiring
you're new to or two ideas right I seem
that I mean quite some of the the work
that you have done will actually based
on the POA infrastructure I'm wondering
like how easy this and i say i want to
give some useful tour I being used by
others and then because seen that I have
to go in and create a curie I guess some
drizzle and have the integrated back to
my tool maybe your your IDE integration
provides inconvenience but still I see
like I mean let's say pick any of the
papers say how fast someone could
prototype the the two in your paper from
the POF on the scratch
I don't know oh yeah so I agree that
enemy and the mining scientific
discovery usually you have the gap data
the raw data and news analysis on it and
actually the mining park is where we
deploy I feel that it most useful
because you you don't need to do a lot
of you know engineering to get the data
but the real data analysis you might you
might have to write what your your
expertise do too for example the neuron
Network language model you know and we
are thinking about putting them into as
a boy infrastructure as well for machine
learning and that's that's one thing
that we want in de crédit into eclipse
but we can only provide is something
very very splendid like wake cartoons or
Mallette library that have been very
properly used but for your Pacific needs
used that's why we we go to the IDE
route because you can still use your own
project and own infrastructure that
usually you mind with Java or other
language that you use and and can still
use that but again I want to emphasize
that the first park is a more harder the
barrier that we want to overcome in my
customers or studies I realized that
very often we we find some salami sore
code we write another code which is in a
baggie so the metrics we get our are
just cloud and so in this case of course
I can have a Berg in my bro screams so I
can check this how how can I ensure that
there is not very in the droid Buster
trade set the compiler in there can be
many many many problems that could be
completely confronting form for the
metrics i get so what's your
this yeah so you're right so the buck
can happen in even in their processing
to sew and also in your the block query
as well as in our compiler for example
but we try to minimize that's errors in
the in their converting into how to
version of the query and improving
further on the debugging for the poor
task so so you can do the debugging on
your euro a query and you know for sure
that ok I'll run in a small data set and
its work fight and so but or work in
correctly so you know that you can
localize the area's actually and now
compiler for example and we can you can
point out to us and you and we go and
fixing so mom in our infrastructure
frame is Kim you for surfer gets up and
source the community could could could
go over the power infrastructure itself
and did a confidence and that it works
right yeah to be three threes and I'm
curious about um when do you want a
domain-specific language for data
analysis in this case specific to
software data and when you want to kind
of more general language that you would
use across domains so what do you think
was specific to the software domain that
you really exploit it but you were
building this to me a specific language
so your question you mean which park
would be general I for other other than
data analysis so the language actually
starting from the sea so pro compiler
which is dealing with a lock and Yahoo
and we dash and his student using that
and and with our help in identifying the
mining software repository types and we
identify in and adding more domain
pacific time and language into that so i
get the core pocket still you can
Taylor for other so but for for example
we are dealing with not just the source
code and not a meta data book reports
and so we are in the price of extending
the data type that for dealing with
issue tracking comments and polls and
other thing that related to to subway
development how I we expect to use both
so if I wanted to do some new code
analysis and I was like you know doing
the cycle of debugging and figuring out
what my tool even did and how it worked
would I write one analysis up front that
kind of cold down all the data I cared
about and then go and do that offline
and iterate on my own code or do i
integrate on my code and call out to
your servers every time I run my code
that kind of which is the sort of
standard either way it should be good we
super bowl of the way so you can write
your program in de credit every time you
need to process the data you call it now
get the data back download there and if
it's small enough like top plane you can
process or if you get like on the Engram
then you need to download and store it
in in your loco and then process it
later but if you just want curia to see
top 10 then you can you can ride the
program time for me to download gigabyte
output or whatever it doesn't project so
we support both of the parent air and
this is one yes so one of the more
interesting things that we should start
to think about as a community especially
where are these extremely large data
sets is that for like for ms our
conference before one of the things that
the data sets they live on so a data set
that's created it lives on for a very
long time but when you start approaching
an area where its data sets of the
service what happens is that the data
sets change over time and when the data
sets change over time that means that
the reproducibility of your queries
change over time and that has some
fairly significant implications for what
we're talking about you know as we move
this so I like Bo I think that it's a
great achievement but when you look at
boa or the or
they get hard it means that we as an or
we as a community of research need to be
prepared to address the fact that well I
can't go on just clone but I can't just
go and clone github archive the same way
and that means that we have to be a bit
perhaps more accepting of results that
can't be reproduced in the same way or
find better ways to actually validate
some of the results that commodities
because the existing methods they it
doesn't seem like they'll work yeah III
think project is touched a very very
good point in this case is that data
change all the time so get up data
yesterday Chang he asked me you know if
the data can is synchronize the github
data and the answer is no way we can do
they synchronize and daily basic even
even the data that they haven't github
downloading directly from their website
expects a lot of time vectra menace
amount of time and actually yesterday I
discussed with I Bremen he very kindly
that point me to the contact person and
github that you know something that they
they willing to share for example that
must easier than sending requests and
downloading used on our resources just
downloading download and if we can they
have already facility to update the data
we just provide a query on top of their
data that would be would be the ideal
case yeah change reproducibility problem
though by date yeah well but in what it
means that we need to be very careful
about that because one of the things is
like if you add in additional
repositories that have data that might
have been in that time box yeah so so
that's where you start to need to be
very very careful about it so it's fine
as long as we can we can make some
guarantees but do we want to make those
sorts of guarantees because those
guarantees say that oh if there's
existing large-scale projects that then
move to example github then we do we not
include those we only include the data
that happens after that point so github
archive actually does this because
github archive it only includes project
information the date that they start
committing events because it's only
about so that you can time slice but
with the way that these other sort of
art
growing our archives of the service work
you don't have that same ability so
maybe I line I guess I guess and so one
of the things I 10 that you guys do is
that you have just you know individual
data sets and so right so we have
different traps chopped liver to say
that this is the data stuff that I ran
over yeah yeah actually I try to find it
to create I mean like to create a data
set so let's say that I published oh you
created the SF state exam yet idea so
these are the repositories that had
scanned at this point in time that would
be really good anything yeah so right
now we providing certain snaps online
2015 September and december two thousand
fifteen or so that's why I tried to find
out the button that you can select ok I
want to reproduce in the data set it's
actually the result of that I run
earlier there is always a little
differently my running the larger
dataset so maybe let's just take one or
two more questions and draw the session
so this will be quick so we already
mentioned ipython notebooks or Jupiter
which is the new thing do you are you
thinking of doing an integration with
this different languages or these are
the notebook functionalities like
ipython notebook where you could have
integrated code and text into one kind
of HTML page that runs on a server so I
think that you might be able yeah that's
a good idea I mean we should look in
that into that yeah that's a good
suggestion picture one more question if
you have solved a snapshot problem but
basically just be playing the combats
one kit I mean theoretically should be
with time travel right I just like
you're playing around they place a
comment in there yeah that's that's
something that I think we should look
into to get the incremental data of
dating that way thank you everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>