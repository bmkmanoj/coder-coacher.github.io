<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Concurrency Testing Using Schedule Bounding: an Empirical Study | Coder Coacher - Coaching Coders</title><meta content="Concurrency Testing Using Schedule Bounding: an Empirical Study - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Concurrency Testing Using Schedule Bounding: an Empirical Study</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KeMVDvY8wMM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so let's get started it's my great
pleasure to welcome once again professor
Alistair Donaldson to Microsoft Research
he has been visiting us off and on for
several years now in fact just before he
joined Imperial College London he spent
two months with us working on problems
related to verification of GPU kernels
and since then he has done many many
more pieces of work on that topic and
and and other topics at Imperial with
the students so today is going to tell
us about some some new work mmm
unrelated to GPU verification I think
yeah Thank You chefs for the great
introduction I'm really pleased to hear
I'd been promoted to professor which has
I'm a cord lecturer which is kind of
like a system professor I've heard that
in the u.s. lecture is really not
particularly our prestigious is that
true so they're talking about changing
imperial to talk about to use the u.s.
names which maybe is a good thing but we
have this thing called reader which is
like associate professor I think that's
such a cool thing and so I would love to
be reader sometime so I hope they make
the changes after I become reader so I
can be reading ok so the work i'm going
to present is joint work with my PhD
student paul thompson and also my
postdoc adam bets but the work is really
has really been led by paul sao paulo
spent a huge amount of time on this
study so it's a it's an empirical study
about systematic concurrency testing
methods based on schedule bounding I'll
explain shortly what those methods are
many of them were developed here at
Microsoft Research so the background is
that in Paul's PhD he's interested in
looking at advanced techniques for doing
systematic concurrency testing looking
at new algorithms and heuristics for bug
finding and doing this quite practical
work requires significant empirical
evaluation to make sense of whether the
techniques are working on up so Paul
spent a huge amount of time building up
a set of benchmarks and this benchmark
gathering is very very time consuming so
it involves huge amount of time spent on
messing around with make files getting
things to build on certain versions of
Linux trying to then remodel parts of
applications
they're amenable to the testing method
under consideration really a huge amount
of work is involved in this so we're the
idea that we would like to I guess get
some more money's worth our effort or
Paul should get more money's worth for
his effort so before starting to really
look at brand new techniques and
evaluating them why not take the
existing techniques that we have read
about and that we have been inspired by
and try and do a very objective
evaluation of those techniques on the
concurrent bet concurrency benchmarks
that are open source the people are
using in prior work and in related work
on concurrency testing and I think in
the end it's led to a pretty interesting
study we had a paper this year the
principles and practice of parallel
programming conference which poll
presented and I was delighted that Paul
won the best student paper award for
this but it's work so the study is
completely reproducible if you go to if
you if you search for the study online
you'll find our web page there's a
virtual machine where you can get all of
the benchmarks all of the tools in you
there are scripts actually rerun the
experiments so we hope that this could
be useful to researchers in future in in
evaluating their methods so the
motivation for systematic concurrency
testing is that as we all know
concurrency bugs are horrible because
they a concurrency bug may manifest
non-deterministic Lee rarely it may be
hard to reproduce the key thing is that
these bugs are dependent on the schedule
of threads and by a concurrency bug I
specifically mean a bug that may or may
not manifest according to the way
threads are scheduled so a burglar would
always occur would not to my men beer
concurrency bug even if it's in a
concurrent program I would say a bug is
a concurrency bug if whether or not it
manifests depends upon the interleaving
of threads and in our study we consider
crashes and assertion failures to be
bugs and we don't consider data races to
be bugs I'll come back to that point
later on so we're talking about a
concurrent program that runs until
either it crashes say with the
segmentation fault or some assertion
fails and the assertions are either
they're in our benchmarks or we've added
these assertions because the benchmarks
perhaps contained output checking code
which we've then replaced with
assertions systematic concurrency
testing is a pretty simple idea in
principle the ideas you have a
concurrent program and a fixed input to
that program so one test input and
furthermore the concurrent program is
assumed to be deterministic so the
program should not exhibit ran
zation the program should not be doing
things like reading from the network and
getting data values that are not inside
the program so it should be a closed
program there are methods for coping
with non determinism by modeling and
systematically exploring non determinism
we didn't look at that in this work so
in this case we're talking about a
deterministic concurrent program with
the exception of the thread scheduler
which of course is non-deterministic so
having this fixed input program then the
OS scheduler would usually be
responsible for scheduling the threads
of this program and a systematic
concurrency testing tool or SCT tool
sits in between the OS scheduler and the
program takes control of the scheduler
and determines the order in which
threads are scheduled and this means
that it's possible to repeatedly execute
the program controlling the schedules
that are explored and to potentially
enumerate thread schedules and if the
program is guaranteed to terminate for
any thread schedule then in theory it's
possible to enumerate all of the
schedules of the program on this input
of course in practice for significantly
sized programs this is not feasible
there will be a vast space of schedules
so and while every shedule would be
considered in the limit and practice the
idea here is to try to find bugs in the
program through the systematic method so
there are a number of tools that have
implemented systematic concurrency
testing I would say the two best-known
tools are verra soft and chess so verra
stop was developed by patrice go far
when he was at Bell Labs he's now at now
at MSR and the chest tool was developed
by by colleagues at MSR here and and I
guess some of their interns and yeah I
think that both of these tools have had
quite some impact in finding bugs in
real work on current programs so the
basic idea of SCT then is if we consider
the space of schedules as a tree so from
some initial state a thread runs makes
an instruction makes another instruction
and then there are we get to a point
where there are several options of which
thread could be scheduled next so here
t1 t2 or t3 could be scheduled so the
systematic concurrency tester makes a
decision about which third to schedule
in this case t 1 and then t2 or t3 could
be scheduled so t1 is now blocked the
tool considers t2 and then this leads to
termination so this is a terminal
schedule and then these dotted circles
are onyx
Laura J jewels these are scheduled
prefixes which demand more exploration
ok so this terminal schedule we can
refer to as t1 t1 t1 t2 because we have
a fixed input and the only non
determinism comes from the scheduler the
sequence of thread IDs precisely
characterizes the state's reached during
this schedule and then we have these
unexplored shedule prefixes so t1 t1 t1
t3 which is where we get down to the
bottom left where we get to this note
here ok and then t1 t1 t2 t1 t1 t3 these
can be explored in future executions so
then we might look at this execution
next which would then give rise to you
know we would have now to terminal
schedules explored and then a bunch more
unexplored schedules so the really good
thing about systematic concurrency
testing is that is relatively easy to
apply it to real programs what you have
to do is essentially make a concurrency
unit test for your program that that may
not be trivial and in our study we
actually devoted quite some attention to
discussing the challenges associated
with doing that but if you can get this
concurrency unit test you then can run
SCT fully automatically there's no need
for any sort of static analysis or
invariance or anything like that you
just run and if you do find a bug you
can then reproduce that bug to your
heart's content in order to debug the
problem you don't get any false alarms
because you're really executing the
program and if it's possible to execute
the program in it all schedules up to
some bound and we will talk I will talk
later in the presentation by schedule
bounds then you can get a bounded
guarantee of the program's correctness
on this test input that bounder
guarantee may be useful the problem is
that the concurrency bugs may still be
very hard to find because the schedule
space is so vast episode this is all
making sense of her yep okay so there
are a couple of standard optimizations
which you can do first is reducing the
scheduling points to visible operations
this was something that the very fast
the very soft techniques re not the
separation logic very fast tool did from
the start so you said you're only at
operations that could be visible to
other threads shared memory accesses
lock unlock operations etc the
observation being that invisible
operations cannot influence other
threads until a visible operation occurs
the chest tool shed yours only at
synchronization operations so rather
than changing
every read and write you showed your
only at thread create thread join lock
and unlock and if you guarantee
detecting data races and flagging them
up as bugs then you're guaranteed not to
miss any bugs if you employ this sort of
reduction so both of these are forms of
partial order reduction and there is a
method called dynamic partial order
reduction from Flanagan and go far in
pople 2005 which reduces search based on
happiness before relations and based on
detecting conflicts during execution so
these are all appealing reduction
methods because they're sound in the
sense that they don't miss any bugs okay
however if we're willing to potentially
miss bugs then we can do something more
drastic and but potentially much simpler
and more useful which is scheduled
binding so the idea is as follows there
is a hypothesis the realistic
concurrency bugs don't require too many
context switches to manifest so of
course we could sit down together right
now and we could write a concurrent
program that will only crash if 17
threads interleave in one particular
order right but no programs like that
actually exist okay so I could I could
be convinced that there may be programs
that require say six or seven into
leavings in some strange order but but
this seems to be rare most concurrency
bugs appear to be expose about using
only a small number of context switches
so this motivates the idea of
restricting search to only schedules
that that do a certain bounded number of
context switches this can drastically
reduce the schedule space and if this
hypothesis about concurrency bugs is
true it can hope it can still be useful
in finding bugs and potentially can
provide a bounded guarantee so it may be
feasible to explore all schedules that
evolve up to say three context switches
and if you can prove there are no bugs
up to this depth that gives some
confidence in the correctness of the
concurrency test and you might even
argue that knowing that any bug would
require more than three context switches
gives you some feeling for the low
probability of such a bug occurring and
practice so the idea of schedule biting
and there are two key methods preemption
binding and delay binding which I'll
come to in a minute it's as follows so
we would explore in the space of all
schedules potentially all schedules
involving zero preemptions this may be a
very small set a superset will be all
schedules and bubbling up to one
preemption or up to two preemptions and
in the limit if we carried on explore
changes with more and more preemptions
then we would in theory explore the
whole space so premature binding to my
knowledge was first proposed by moose
avati and career in pldi 2007 and delay
binding was proposed more recently by
Michael anxious kadir and von amerika
marriage in poeple 2011 so I'll talk a
little bit about preemption binding and
delay binding and then I'll get on to
the empirical study itself so in this
diagram I'm illustrating the difference
between a context switch that is forced
versus unforced so in red a schedule has
used 0 preemptions and in yellow one
preemption so if you look here thread
one execute and then thread one thread 2
and thread three are enabled ok so if
thread 1 continues to execute there's
been no preemption so there's there's
not been an unforced context which
however if control switches to thread 2
or thread 3 then there has been one
preemption so the schedule has cost one
preemption on the other hand if thread
one was blocked at this point then it
would cost no preemptions to switch to
either thread 2 or thread 3 because
there's no choice it's not possible to
continue execution of thread 1 so these
are unforced context switches and if we
look at this slightly more complicated
example we can see for instance here
this this red path is a schedule with 0
preemptions this yellow path of this
yellow path I share just with one
preemption and this this path that ends
up blue there are two premises any
questions regarding this ok delay
binding is I guess slightly less less
obvious than premature biting let me try
and explain it so the idea of delay
binding I'll give you the idea version
then I will try to give you some
intuition for for Wyatt wire can be
useful so the idea is to fix a
deterministic scheduler for example a
round-robin non-preemptive scheduler but
it can be any deterministic scheduler if
you run a fixed input deterministic test
case with such a scheduler then there
will be one schedule right ok so the
idea of delay binding is that during
systematic testing we use this scheduler
but we can deviate from the shedule by
skipping over a thread at the cost of
one delay and in the study as in prior
work on chess we consider delay bound
with respect to our non unknown priam
through round-robin schedule ok so let
me try and illustrate further how delay
bound it works and then I'll talk
briefly about the intuition so suppose
we have four threads initially only
thread one is enabled and we're using
this round robin Tregenna so thread one
execute until it becomes blocked so even
if threads two three four become enabled
thread one carries on executing if at
some point thread bomb becomes blocked
then we go to thread too ok if threads
to and thread three become blocked we go
to for a threat for in through our bun
become plot we go to two so this is the
round-robin scheduling but to illustrate
delay bending at this point suppose
we've got the situation where thread 1
it thread 2 is executing and it's
enabled then at a cost of no delays it
can carry on executing that's the
default thing the scheduler would do at
the cost of one delay we can skip to the
next thread so we do an unforced
preemption to thread for note we don't
it doesn't cost anything to skip over
third 3 because 3 is disabled ok and at
the cost of two delays we would go to
thread 1 and skip it and go into thread
sorry we would go to put 3 and skip it
and go on to thread 1 ok so this
illustrates 0 expending 0 delays one
delay in two days and here is that the
shedule tree so suppose we've got to
this point having expended no delays we
can carry on having spent no delays by
continuing with red too or we can switch
to thread for cost of one delay or
thread one at a cost of two delays so we
now may consider any schedule that uses
up to D delays for some small number d
so let me try to give you the intuition
for why why delay banding can be
effective in practice I have to confess
that I don't have a terribly strong
intuition our study confirms that it
does work better than preemption biting
my interest is not all that strong for y
so maybe she has can comment as well but
my intuition is that with preemption
binding can be useful because it
dramatically reduces the schedule space
but does consider the possibility where
at some non forced point a thread yields
to another threat the problem is with
Prem combining the thread yields to any
other thread and then you get this
explosion of possible threats
but it doesn't seem necessary to
consider all possible threads that could
be ordered to the key thing is to stop
this thread executing and let someone
else have a go and with delay bounding
we are deterministic Lee picking who
gets to go but not completely
deterministically because we have this
costly non-determinism so if we want to
deviate quite far from the edge of this
cost something okay so we're not going
to by default consider if we use a small
delay band we're not going to consider
these rather costly sources of
non-determinism we're going to consider
the cheapest which is just to stay with
the current thread or something slightly
more expensive which is to skip on by
one thread or two threads yeah tom she
was a deal with huge numbers and friends
so it this goes to the state space
explosion if I have a preemption where
have a hundred enabled threads right
then then I have many many choices about
right next and if most of those threads
are symmetric it really doesn't matter
but but the preemption bounding I sort
of treat them as as if they're all
unique with delay bounding I don't
suffer that problem so I think I can
deal with huge numbers and threats or
just be some threads that don't get
problems but I mean absolutely the idea
of the large number of threads leading
to a schedule explosion and not going
away of the day banning to me that
completely makes sense I guess the thing
you're adding there is saying that if
these threads are kind of roughly the
same there's nothing especially
interesting about all of them then it
really would not be particularly
effective to to consider all these
possibilities if thread for can do
something bad to thread one it may not
matter that thread two major beings
right you're going to get to Fred for
eventually the key thing is getting away
for thread one at a certain point and
letting thread for do its bad thing no
okay yeah this also can be a function of
the density of the details
this dependence relations various parts
like in message passing certain white
card choices make the difference that
might be another marketer okay the
examples where the difference is very
very sparse and they keep increasing the
coupling so you think that if there's
sparse dependency then delay binding is
likely to work well you know what come
in we go to the term that mob also okay
so we can apply these methods
iteratively so the idea is rather than
necessarily saying we're doing delay
bounded search with the delay of three
we can actually just say we've got an
hour or we've got a hundred thousand
schedules and we're going to try all
changes with the delay of 0 and then if
we manage to finish all of those there
will only be one of them so we will then
we will try all share this with the
delay one there'll be many more of those
all shejust with the delay of two up to
two then all with the deliver up to
three until we either exhaustively
explore or we run a time or we reach
some some agreed number of schedules so
I iterative ipd for iterative iterative
preemption binding and I DB for
iterative delay banding so the claims of
prior work are as follows a low schedule
band is effective at finding bugs then
shared rubber burning provides benefit
over more naive systematic techniques
like an just a straightforward depth for
a search of all schedules and that that
delay bounding is better than preemption
binding in the sense that it's faster at
finding bugs because you can you have a
smallish edge of space to search and you
can find these bugs in that smaller
space so that what we felt was that
prior work is mainly from Microsoft and
it uses a lot of non-public benchmarks
so the benchmarks sound very interesting
but they're not publicly available so
but by and large they're not publicly
available so it's hard to independently
validate this research and then there
are all these papers about concurrency
analysis which use they have these names
of benchmarks you see cropping up again
and again so what we thought would be
good would be to try to get all of these
open source benchmarks and implement
re-implement the algorithms for delay
banding of preemption burning and do a
study to a assess how effective these
techniques are with respect to each
other and with respect to not naive
systematic search and be actually assess
the benchmarks themselves so these
benchmarks that people are using are
they any good or you know
I'll come back to that so the
experimental method oh okay so what we
did was we took this talk of maple which
is from researchers at the University of
Michigan and at Intel this was published
at oops the 2012 and the actual paper
about maple is to do with coverage
guided hunting for concurrency bugs and
that's not really at all the focus of
our study but maple was a suitable
choice for us because it was
independently implemented so it's not
something that we implemented it
supports systematic concurrency testing
and it's open source so what we did was
we took maple and we added support for
delay binding and we studied when i say
we i really mean paul thompson he
studied the source code of chess quite
carefully and tried to make sure things
were implemented similarly to chess and
then we got hold of all of the buggy
concurrency benchmarks that we could
find from existing papers that are
amenable to systematic concurrency
testing and our people we go to some
lengths to explain which benchmarks we
had to exclude for instance there were
quite a lot two benchmarks that involve
gooeys and that's very difficult to
apply systematic concurrency testing
tube without doing quite some
significant work Tom C programs yeah
this is C++ programs yeah c++ benchmarks
ok and the maple two works using the pin
instrumentation framework so these are
compile these programs and then we use
buying your instrumentation to to do
systematic testing final maple was to
use coverage techniques to try and guide
which schedules a slope yes yes so maple
does um maple is non-systematic so it
controls the scheduler but it uses
heuristics to try and find us find a
shade of is likely to find bugs okay and
that in our paper because we could we
did actually compare with maple I'm not
going to present that data though
because that's not that wasn't our aim
at all in the study him okay so we found
these 52 buggy benchmarks I mean there
are many more benchmarks and we whittled
them dead 2 52 which were amenable to
systematic concurrency testing with
modest effort and these are all public
code bases and we've we've amalgamated
them with the permission of the various
authors and we call this SCT
systematic concurrency testing
benchmarks and this is at now publicly
available benchmark suite and what we
did was we looked at three techniques
initially iterative preemption binding
iterative delay binding and naive
depth-first search and we applied every
technique to every benchmark and we gave
every technique 10,000 schedules in
which to try to to find a bug we use
schedules rather than time because we
believe is a future-proof metric so we
running this kind of experiment takes
serious time and we had to do it on a
cluster and furthermore we had to do an
Acosta where many benchmarks were
running on the same node using multi
core capabilities and then timing
becomes difficult ok but number of
schedules doesn't become difficult and
if people want to compare with our
techniques in future they can compare
basically a number of schedules even if
they don't use anything like the same
hardware we're using did you have a
question what are you consider approach
as well the concurrency fuzzing a
project because it's not systematic so
we wanted here to look at systematic
methods stateless motor checking so
there's there's also there's also a kind
of combinatorial explosion of how many
things you compare so we there are like
very since we'd like to look at like PCT
for example but in yeah in this study we
uh you know you know it's like you get
closer and closer tracks you're trying
to submit a paper and eventually you
think right for this study we're going
to have to just rain things in a bit and
see what happens yeah yeah absolutely I
mean we're not finished with this study
we have a paper on the study but we'd
like to we'd like to do more santosh has
worked as open source too yes right so
you can actually have a completely
dependent on daesh impressive
so some countries I don't think it does
the systematic no doesn't do so so
everything the busy schedule you know
you can use it in an artist man yeah
absolutely honor to do list of things
especially given I'm going to talk a bit
about random scheduler in a minute and
we're very keen to try PCT given the
results we have with the random
scheduler all right so quick word and
eight races over half of the benchmarks
we found to contain data races by using
a dynamic data race detector these data
races are found almost instantly by any
of these methods so if we treated data
races as bugs we really wouldn't be able
to distinguish between the bug finding
ability these methods because they would
all just find a bug okay and we had a
look at some of the benchmarks and in
some cases it's kind of clear that the
developers would regard these races as
benign in other cases is things like
incrementing a counter and histogram
which really should be done with a
relaxed atomic in C++ 11 in the end what
we decided to do as has been done in
prior work was run dynamic race
detection upfront and find a load of
data races and then look at all the
instructions that participated in some
day to race promote every one of those
instructions to be a visible operation
so treat every one of this as a single
single and then and then ignore the fact
that there may be further data races and
do systematic concurrency testing
scheduling it sing cops and known racy
instructions but every time those
instructions are executed whether or not
it's a racy scenario we consider the
missing cops so this this essentially
explores sequentially consistent
outcomes of those data races and is
oblivious to future date races so we
argue that this is a it's what has been
done in some prior work and be it's it's
not biased towards any of the particular
techniques we're evaluating okay so here
are the names of the benchmarks I won't
spend too long on this slide but you may
you may be familiar with some of these
names so a get and PB zip I think of
very famous in papers about concurrency
testing then we have this rather large
set of examples which come from the tack
ass software verification competition
maybe say say we're just taking the
benchmarks we can find it and the people
are using in their papers and people are
boasting about so here for example you
can see that there are many variants of
a
the dining philosophers problem so you
know you may or may not think that
that's good in a benchmark suite but
these are the benchmarks that we could
find small so yet they are you guys know
the big or a small idea I mean I'm not
intimately familiar with the source code
of these things but i think that PB zip
is moderate sized none of these are
massive programs or some of them come
from massive applications but they're
sometimes i think it can be really
misleading in these papers to say we try
this on SQL Lite which is two hundred
thousand lines of code when actually the
test is running on a hundred of those
lines so you know then there are some
some benchmarks from from chess which
paul ported into to linux so that these
work stealing q benchmarks which we
thought were very interesting which were
available there are some pie set
benchmarks some rad bench benchmarks and
some of the splash benchmarks and in
each of these suites there are i think
in all cases there are more benchmarks
than just these but there were always
examples that it would have been really
quite difficult to make a mean or
systematic concurrency testing there was
only so much effort we could put in
inter inter curating this benchmark
suite for the study ok so the top level
Venn diagram of results is as follows so
we have 52 benchmarks seven of them none
of our techniques could find bugs in so
preemption binding delay binding or a
naive search 33 of them we could find
bugs in and in all of those cases the
bugs could be found by one of the
bounding methods ok within 10,000
schedules so there's never a case we're
doing this naive depth-first search was
better in terms of bug finding on these
benchmarks and we can see here that the
labeling is significantly better than
preemption burning in that in that there
are seven benchmarks where delay binding
was capable of finding the bug and
pressure bedding was not there alone all
of these things have those we know they
all have bugs so i'll come back to the
bugs we couldn't find we know
anecdotally they have bugs like people
say there's a bug in this and explain
the bug in english right so it could be
there it could be that they're wrong and
you know yet but i will convert that ok
so what this does is confirms to claims
of prior work so first of all for this
benchmark suite supports the claim that
but shade your binding is effective of
finding bugs and furthermore it supports
the claim that delay bounding is
superior to the preemption binding for
finding bugs okay but one of the
reviewers of our Peapod paper suggested
that it might be interesting just to try
a completely random scheduler so a
scheduler which every scheduling point
randomly selects a thread it doesn't use
any PCT type stuff it just randomly sex
a threat so we read that and to be
honest we thought kind of grown you know
I guess we should try that but it's
going to be some more experiment to run
but we did it okay this is the result
all right so what we found was that the
random scheduler the completely naive
random scheduler was able to find all
except one of the bugs that could be
found by the systematic methods and
furthermore it could find an extra bug
and furthermore which you can't see here
it find these bugs significantly faster
in most cases in delay binding of course
that could be due to luck in terms of
the random schedules so let me explain
again about the random showed you that
we're picking on the fly we're making
random choices we're not recording those
choices so we could be doing the same
schedule in this set of 10,000 schedule
so we would just do 10,000 random
schedules and see how far we get okay so
we found this very surprising and I
guess if I'm completely bluntly honest
it it suggests one of two things either
suggests or a mixture of these two
things either suggests the SCT bench the
set of benchmarks which people are using
and we gathered is not representative of
real world bugs or it suggests that
actually these bounding methods don't
provide benefit over just a naive random
approach for raw bug finding so I'll
talk a little bit about how good this
benchmark set is in a minute but but one
thing I in defense of preemption binding
a delay binding is first of all it's a I
think it's definitely true that if you
if you find a bug using a delay bound of
one it's likely that the counter example
you get is going to be much more
palatable than the counter example you
get from some crazy random schedule okay
the second thing is that because these
results support the hypothesis that
bounding is useful that adds weight to
these sequential ization methods so
context binding and certain forms of
delay bending admit sequential is a
shins where you can take a concurrent
program and you can rewrite it in to us
quencher program such that the
sequential program exhibits all of the
behaviors of the concurrent program up
to some band and then you can use static
verification methods from the sequential
land to proof not for just one input but
for all inputs that this concurrent
program is correct up to disband now how
useful is such a claim well it's
potentially quite useful if it seems
really to be true that you can find bugs
in small bands so to these results I
guess add weights to that use of these
bandar methods even if they potentially
detract from the use just the bug
finding which compares pure random to
know the guy really does seem to be a
difference but that's on substantially
large real applications right like
sequel server because that technique is
not systematic it's able to run under a
large program yes that's just because
you can run a large programs and
prevents us from running even small
programs oh no no no but what I'm saying
is that you know I think I think these
benchmarks these benchmarks perhaps our
when it down in such a way that you know
your schedule space is not that he
selected very intently gee that's not a
needle in a haystack it doesn't seem
like a needle in a haystack your friends
well so yeah in a second I just want to
give one piece of intuition that I have
for for this scenario so it seems to me
that if a bug can be exposed with just
one in preemption then to me they seem
to be too extreme scenarios one scenario
is that the bug will occur if this
person happens sometime right doesn't
matter when it just has to happen in
that case it seems that loads and loads
and loads of schedules will expose the
bug and then random will find a bug and
on the other extreme that's the case
where some preemption has to happen but
a really key point and if it doesn't
happen to that key point you won't get
the bug and then that's as you say Tom
like searching for a needle in a
haystack and we wouldn't expect random
to do well random is finding the votes
in fewer guests yes I did right so
systematic is penalizing you right right
she's doing worse than so the only
intuition I have their this this came
from Hannah chocolate kinga's College
London who saw Paul talking about as
where can she I'll try and I'll trying
to
what Paul told me she said to him which
is that if you imagine the tree of
schedules then then shallow terminal
shadows are likely to be favored by
random right if you have a bug that can
crush the program very quickly then it
will have a shallow terminal schedule
and there may be a very high probability
of there there'll be a number of these
shallow terminal schedules you will have
a high probability of hitting one of
those does that make sense except that
we're doing so when you something i
haven't mentioned in this top but is it
in the paper is that we're doing idb and
IP be with you have to pick a scheduling
method underneath that and we're using
depth-first search underneath ipb your
ID be that's that's mainly because in
maple that was a pretty core engineering
decision and was hard to reverse so
maple is running you know you're running
on your recording of schedule and then
you're trying to schedule variant and
there are many ways to enumerate the
schedule variants and a simple way is
depth-first enumeration of schedule
variants with a shared amount of one say
okay and that will not necessarily favor
these shallow bugs it does that make
sense uh no you're misunderstanding what
you mean by by a shot I guess if you
imagine the space of all shadows as a
tree then I'm shallow would mean
terminal schedules which are not very
deep in the tree it means few
instructions executed will stop yeah
number of instructions executed
because if you imagine the random
scheduler every time it which is the
same cop it makes a decision and if
there are loads and loads of shallow
terminal schedules to em with a crash
then if it makes Randall decisions it's
quite likely to get to one of those
rather than if there's a very very very
deep crash yeah in terms of not
rescheduling decision without in terms
of never answers they're counting
schedules okay I wonder if you might be
able to talk about it after because i
would like to is something i haven't
thought about that motion would like to
think about morn yeah what's happening
here is our idb is doing something
systematically Wow right it's going off
on some piece of the search space where
the solution isn't right you know
whereas apparently the space of
solutions is quite dense and you'll find
one quickly if you few guys running it
does it does seem that way we can we can
look at the big table so this is the big
table I mean we're not be looking at now
but this this table has all the data in
it right and so you know it we're
included this in the paper because there
are only so many observations we could
make ourselves and fit in and this this
has really got a lot of information so
if you're interested we could offline
maybe pour over the table and look at
some of the results we're very curious
to understand this we were very
surprised by this writing we're very
grateful to the review if you're
watching for suggesting we do this and
and unfortunately we didn't have a
chance to have our comments about the
random shatila peer-reviewed because we
put these in the final version of the
paper Ganesh yeah actually she's una
vela an experiment when I was visiting
Intel for a year we took Murphy and the
use pavel antipov basically used
different seats okay we put hitbox
reviewing offense why we didn't recorded
the trace that was the problem okay you
didn't report it rest yeah because it
was start a book and then the other
thing is we also did a count of how many
variants of the same error cosmodrome
cares each other
ten thousand a thousand times in the
state space so they have no need to hit
that serve as the gold cares populated
all over the state space so random is
not package when you say random I
presume yourself to the random you put
you could reconstruct yes if you sooner
I saw somebody having this behaves yeah
right but I think we should focus now on
the benchmarks and their limitations
because let's not get too carried away
because some of these benchmarks are not
good so so but but the main findings
then I think I've covered these so
edgebanding similar body was similar in
terms of bug fighting ability to random
search many bugs can be found it with a
small bound blade by the bees permission
binding but why I want to talk about now
is that a significant number of the
benchmarks may be regarded as trivial in
some sense I think this is quite
important because if benchmarks are
trivial researchers should not boast
about finding bugs in them right they
should become a minimum baseline if your
toe can't find these bugs you're too liz
is not a tool and you need to boast
about finding you know better bugs in
these bugs so I hope that I study can
potentially set a clear baseline for at
least systematic concurrency testing
tennis maybe concurrency testing
techniques in general so if you have
these big table of benchmarks if you
have all of these benchmarks which i'm
not going to elute it by name here but
yeah well let's see so trivial benchmark
so here's a property bug was found with
the labor and 0-14 benchmarks right so
what this means is the single the single
schedule with the day about a zero find
the bug these should be a struggle right
with it but let me emphasize again we
just wanted to take the benchmarks
people are using and study them right
okay the numbers here there is overlap
between the numbers this is a
partitioning of the benchmarks there
were sixty mint marks where Tom you
you're correct to say that the edge of
space is not vast so there were fewer
than 10,000 terminal schedules overall
forget bounding I mean and what what
this means is that all of the methods we
studied would eventually find the bug
because they would all exhaust the
search space right so then it might be
that the labor and we'll get there
faster or premature my never get there
faster maybe maybe not but every time he
would get there we found that in
nineteen cases that more than fifty
percent of the random schedules we tried
exposed to bug now of course that could
be could be luck but I think it suggests
that there are loads and loads of
schedules that explosive bug in these
benchmarks
okay and therefore the more of those
nine of them every random stretcher we
tried work was buggy okay and then and
then in the amenities nine right and
then in these nine I i would have to
check the exact number by think four or
five of them every schedule we tried
with every technique header bug so going
back to my at the beginning my talk we
would not actually call those
concurrency bugs because they are not
scheduled dependent right so we include
that we included in the study because
their claim to be concurrency bugs by
well sir no this is the schedule no
that's a little bit better because it
depends on which scheduler you chose I
mean it's a little weird because that's
like that he fall schedule that's the
thing that would happen pretty much on
an ordinary computer and and I'm sure
isn't I I'm sure it's not the case
because we didn't find otherwise would
find this for all 14 and we didn't find
this ah I'm saying there's another
moment we don't have a column for which
is every jewel was buggy and we didn't
find that for 14 definitely not right so
anecdotes I mean on the benchmarks we
were doing chess which were the ones i
worked on forgot net from library okay I
mean for small first of all like things
like hash table parallel hashtag loving
you I mean you have millions of
schedules right i mean you would funny i
mean just small programs we were just
and this is with all the bounding and
everything and huge numbers so that some
first of all just that I mean 10,000 I'm
asking where they all buggy so all of
those benchmarks are buggy right whoop
you found buck squalid on note on the
same I'm just saying this or anything
that's bizarre site exists that Nicholas
is small whatever let me emphasize
though I'm not saying that so what we're
saying is that there were 16 cases where
you had the small search space right for
everything else there were 52 benchmarks
so for the the remaining benchmarks we
don't know how big the search base was
but it was more than 10,000 and you know
that is not going to be it's probably
not gonna be ten thousand and two it's
probably going to be big you know
oh yeah because I mean with just
definitely we have an experience of
these Austin for preemption bound of two
or three and i know we have examples you
know you can make up whatever number you
want but but I mean for the real things
you know we would just we just we can
keep exploring for very I'm relatively
small to complex complex kids okay so he
says these these were things you know
with volatiles and with interlocked
operations and the code was small but
schedule specialist yep so yeah I mean I
think it's like pretty clear from these
Rudy I mean I'm focusing here on the
ones we say at reveal okay then there
are the rest of them some of which come
from chess and which are harder and I'll
come back in a minute to some of those
but it sounds anecdotally that you prop
your your reading of these results is
probably that when I said that you know
we were one of two conclusions right
either these benchmarks are not
realistic or these methods are not
providing benefits so it sounds like you
probably thinking the first which I
would like to think because I find these
bounty methods fascinating i like to
study the more the media was a message
in a way bit of a disappointment when we
found this random result cuz i kind of
thought what we doing then if we can
just find these bugs randomly so you
know I I think the key thing is the
difference between bugs that just you
know programs that are just very likely
to crash and then you probably don't you
know you could argue that maybe don't
need systematic testing maybe the
program is so likely to crash the genre
purchases they may be right testing
awesome people became very in demand
tubidy scheduler I mean it's all a
matter of searching you search a
scheduler space yep how do you wanna
search that space via fencing of your
BFS it could be from a ballistic search
and probabilistic search is random
investing and then you can ask the
question how do I want to go mr. right I
think it might be useful to qualify the
benchmark season qualitative approach
which would say this isn't clearly a
synthetic benchmark created by hand to
insert a bug into a well known but small
concurrent program and try to find I
mean versus you know a benchmark that
comes from the water which is you know
typically what we were going after yeah
Jess I mean clearly everybody creates
synthetic benchmarks we create them
basically as functional tests to say is
the tool just working it's as if I'm a
bug or not and and to also do unit
testing which is an interest anything
for for exponential yet search space but
they have really tiny programs and a
small search space so that the tool can
we can we can test the two over hospital
right so what we wanted to do in this
study like I said we'll just take what
peg well there is an evaluator and do it
quantitatively we didn't wanted we
wanted to be very objective here but I
think given some of the findings I think
maybe uh you know some more reading of
the code could be in order so I'll talk
briefly about the buds not found and so
so there were three bugs which were not
find in a rather trivial way these bugs
could be exposed if you reduce the
number of threads so you saw in the list
I think this is dining philosophers
examples where this two three four five
six seven I think it's the case I
remember exactly by think is that in
dining force of a seven you couldn't
find a bug but it's basically the same
bug is in dining philosophers too so i
don't really broke out those is
particularly interesting unfound bugs
except there may be delayed binding
should find them if the event is meant
to be good with large Perkins maybe okay
then there's this rad bench bug number
one so this is a JavaScript interpreter
and that this is a we have a scenario
where thread one destroys a hash table
thread to must access / table and Paul
believes that this should be exposed
with only one delay but there are
upwards of 14,000 changeling points in
he looked
execution and he looked at the number of
Chechen points in that terminal
execution and there were you know
upwards of 14 thousand of them so you
can see that there would be a very large
edge of space it was able yeah noted he
was a fun right and the the ravaged bug
too we know requires at least three
delays or preemptions because i believe
we were able to search this one
exhaustively we need to double-check the
table okay but we didn't find any we
were able to search it exhaustively up
to this band of three sorry what what i
mean so no no no no sorry that's not
true no we didn't fight this bug and
what we did was we explored 0 1 and 2
and we got into three when we reach the
10,000 limit so we know that we would
need at least three preemptions to find
this bug are def metric that you define
the PCB paper no does it sounds like
you're actually not going after
something similar here no figuring out
how many scheduling points and how many
preemptions so we have this
classification evocative paid on a
similar it except it's priority lowering
point it's not reactions okay if oils
can do a similar thing and it's a
possible use that metric to assess
education we didn't find the bug work
and you will use it when you find the
book or can use as a to give you a lower
bound on work actually measure this
because it states what is the minimum
number that you need and can either of
you detention facility sort of qualified
over all possible okay oh no we didn't
we didn't look at that I I didn't
remember that for my reading of the
paper so I wanted to go back and study
babe all right and then there's a
variation benchmark which we which we
classify as miscellaneous this comes
from dmitriev you cough and this is
posted to the chess forum this is a
stack a lock free stack there are three
threads and and the poster of this of
this benchmark claims that this bug
could only be exposed with at least five
preemptions and they're 114 chicken
points right so this if if he is correct
and we know we couldn't find this bug so
it's I can't say you know we validated
this week he's explained the bug in
English right so his
claim is that this bug cannot be man
cannot be exposed with a small number of
preemptions so that is obviously there
will exist kind of counter examples to
this claim of but you may still regard 5
as a small bound but this is a
not-so-small bound so would be entering
to see whether where the PCT could find
this and we didn't find this with random
and this sounds like the sort of thing
we wouldn't find with random right
because it requires five preemptions
random is not going to you know the
chart the chances of just naive random
in searching those permissions at the
right place is very small something
which I won't talk about neighbor which
we the bottle is all the strength always
10,000 soldiers so I mean what does vana
mater suggested to Paul that people was
what do you need to take that
benchmarking just run it for ten weeks
or something so let's see this hard to
match a real program needs diagram is
stupid or like a clearly contrasting or
right to meet that man I mean I would be
interested public terrible to love cats
are making out okay you really hard to
believe that did the authors says that
he crafted this to have to be half of
this fine print from not lower bound or
something like san jose by accident i
could I haven't personally look at it I
could click the link now we clip its
goes back wall I go to that chest on
black stock go ahead mrs. that's that
was aired it okay gosh I can't believe
that still
in what sense in that you would think
his mobile number we haven't looked at
that flow in a lot of them before and I
see I mean people just think the forum
not the actual bug
okay i don't think i'm gonna go find
this during the thought but less let's
look at afterwards right so the tener
get a better systemic concurrency
testing full stop the main problem we
had during the study was environment
modeling so GUI applications
applications would use processes or
threads and use the network and what
Paul tried to do was to take those
applications and then extract from them
isolated concurrency tests so try to
take out something that involves a few
threads in a scenario and use that for
scenario testing but you found often
there will be global variables and this
will be very hard to do so would not be
looking at these more real world things
it would be very difficult to extract
these test cases so I think for people
to really use systemic concurrency
testing they need to be willing to put
some effort into right tinkling currency
unit tests and trying to you know make
these tests some standalone so that you
know so that you can systematically
explore the schedules then another issue
was that many of the bugs we found were
related to memory safety but what we had
to do was actually knowing from reports
on these bugs that there's a problem
with memory safety we had to then add an
assertion at the point where we know the
problem is and then see whether we could
find that assertion but we don't have
inside maple is good dynamic Val grind
style memory analysis to try to find his
bug still game crash right with a sick
or something often not I mean if there's
enough for everyone it often doesn't
crash soon you and and whether that
happens maybe non-deterministic so that
there's a kind of challenge of
engineering challenge of trying to do
you know if you want to do if you want
to do race detection maybe to do full
exploration of all schedules are rising
from date races you need very fast on
the fly race detection if you would find
his memory areas you need very fast on
the fly memory analysis and and getting
all those things integrated into a tool
and then having the flexibility to be
able to explore these sharing strategies
it's quite some engineering challenge
but not not really a research and yep
the star the chess project the there are
two challenges with this kind of testing
one of them is of course creating the
isolated unit test and the second one is
this come
torial explosion and I think that at the
start of a project it was not clear to
me that they are two very distinct
challenges and I think I found you know
talking to engineers and testers in the
company that oftentimes the reason for
doing random testing then they think
that they are doing random testing
because of one reason but it's really
because the other you mean because they
can't isolate these because you're not
isolated so these are the reason why the
testing is condensed interesting is not
principle is because you can't isolate
isolate least s even what you call
random scheduling failure you are able
to live a truly random scheduler only
once you have gotten full control over
all right now sir is living its choices
yeah it's good funny you are not going
to go into the industry what people
refer to this random scheduling just
means that they're just running the test
again and again and they are you Bertram
yeah things right and that's absolutely
no but we did you and so you should do
this I mean what we are doing is already
way more principled what I'm saying is
way more principled than what what is
happening yeah in an actual product
setting but harder to apply but now is a
smooth transition right so you can have
the perfect eyes like this then you run
around scheduler then you have guests in
the real scenario where you can maybe
you're not catching every single piece
of synchronization you have still have
some idea that whatever work value the
isolated case is probably also going to
be better if you only have partially
print so I mean the same scheduling
decisions you can make in eqs way the
case for random scheduling have a good
chance of working once you apply them to
the real even if you don't catch all the
same
but but is it how do you apply them to
how do you just apply them into some
synchronization you just can you can
restart the program by the time you
wanna restart and you have no guarantee
that you want to meet some some
synchronization or maybe sometimes you
guess incorrectly the thread is blocked
and it's really just slow so those are
like Inquisition approximations that you
have to do in the difficult so that's
what we doin cause like right so but
marvel of those guesses are incorrect is
still running a principal scheduler okay
yeah makes sense right so to conclude
with a slide on future work so what we'd
like to do I mean Paul has a bunch of
ideas related to extensions of systemic
currency testing and heuristics but
terms of this study we would like to
look at more techniques so yes the the
PCT technique i think is primary on the
list but then it will be interesting to
compare with non-systematic techniques
as well we would like to now we have
this framework for running these tests
we would like to really extend the
benchmark suite significantly we didn't
look here at partial order reduction the
reason is that it's not trivial to
combine partial order reduction soundly
with these binding techniques in soundly
in the sense that i would say that p 0 r
is combined signing with the lay-by name
for instance if you don't lose anything
additional to what you're already using
with the labor ending by applying p 0 r
and my understanding that there is no
other case if you naively combine them
and that it's an open problem which
people are working I think Madame Musa
body had a paper with Catherine Coons
and Catherine McKinley oopsla last year
on precisely this topic so that's
something we would like to look at and
then there's this issue of weak memory
in systemic concurrency testing where I
understand there's been some preliminary
work but we would like to explore that
further as well okay so thank you for
listening and thanks for the questions
and I be directed to answer any more
questions
so there would be other possibilities
for virender I mean instead of just
doing complete random scheduling you can
do you know random delay random
perturbation from a fixed schedule soch
as you told me that you could you said
something a bit like that to me that you
even tried delay binding with a
different schedule with a randomized but
the terminus the clear and amjad I said
you feel see to the beginning iron you
have a now a deterministic scheduler but
it's like a randomly chosen
deterministic scheduler yeah yeah you
can do that I was thinking just as a
bias in your in your random schedule or
great we would say you know bias towards
round robin okay no that would quick you
a little closer to the you know the
delay bounding ideal yellow with that
work better than you know unless run
given the delay bounding does seem to
improve yeah yes an interesting idea and
I think it varies a little bit PCT right
which is in favoring round-robin but is
prioritizing threads and randomly
changing priorities is to find the
randomization that gets us quickest to
the shallowest bars this
characterization of that and the thing
that we found you on a randomized is
priorities and priority lowering points
so we give all rights in initial run
priority and then the schedule is the
deterministic basin at priority now
there are like random points in the
execution that we lower some priority of
some threads randomly that's it okay
you're questioning back he is that one
one famous comparison but we call the
space shuttle had to be scrubbing the
launch pad do you try to add in your
list we live in track we didn't try to
add that to I this I don't know whether
it's a C++ program with fixed input and
ya know we didn't connect I was
wondering whether you could take
advantage of the code structure some of
the initialization code may not be of
interest you want to get past it and
then turn on the surge and things like
that yep yep so start the search answer
yep see absolutely yeah you may want to
I mean run the application up to some
point I suppose and then take maybe take
a snapshot at that point and then do
systematic encourage anything from the
snapshot yeah so I don't have a question
without comment so when you're
introducing dilemma you mentioned that
the landing is with respect to a round
robin scheduler what I said in general
is respect to I deterministic scheduler
and break in this implementation like
chess we used android so the result of
doing delay bounding varies a lot
depending on what deterministic schedule
you used because as ken was saying the
the domestic scheduler is sort of like
the point around which your biasing the
search yeah so in other application so
I'm interested in testing systematic
testing of message passing applications
but those applications there is no such
notion of preemption on context which
processes running and they're
communicating here message passing so
there is no a priori reason to believe
that free in sherman among those
processes is particularly going to be
useful and we found that so we were
using the idea of deterministic schedule
right because that's a very general
concept that doesn't depend on what
you're running on a single bus single
single core on a multi-port it is
applicable even to a distributed system
and we found that the speed with which
bugs are uncovered depends significantly
on the particular deterministic
scheduler okay that we started with so
we experimented with the round-robin
scheduling a convention and there's
another one called run to completion ref
scheduler there was one more call so we
created a random giving scheduler okay
so is that a kid but that's not like
into our random check schedule there is
no no no did you try random scheduling
let me try we have not tried ranch to
try that but we should please try guess
what surprised me a bit was that in
prior work there was not a comparison
with this very naive random approach it
didn't surprise me because we didn't
think of doing either it was this review
who suggested we try it right so I guess
kind of my takeaway from this work is
definitely try all these easy things
right you know they good to try them but
don't try them because they mean you
can't probably favorite is that but yeah
I mean and I think with I think with
this benchmark set my reading of it is
that about half the benchmarks I think
are nonsense benchmarks I think about a
fifth of the benchmarks are really hard
we can't find the bugs and I think the
remaining are interesting benchmarks and
for those benchmarks I think we're
seeing you know the claims of prior
being supportive but it's random
approaches is doing well and you know
maybe there's bugs are not super hard to
find but the benchmarks are not super
simple either random so watch out said
and also what we knew that didn't really
notice them it's not really that much of
a difference between you know randomized
surgeon on advanced search if you think
of the random the number of random
choices as an input to like picking a
schedule right so you can just pick
those random choices before you start
and that would be similar to
you can you can deterministically
enumerate all random choices and then
you have a systematic right show your
system so so if you know that there were
hundreds cajon points you systematically
numerate yes and you know the design of
the crew grand scheduler is to pick few
random choices it's just when you
determine the complexity of a mess
algorithm you know it costs something to
pick around a bit so you weren't
scheduled and it uses as current choice
as possible and that same scheduler than
is actually good for systematic
exploration because you can actually go
through all the choices okay I'd like to
talk more about that achievement okay
more questions thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>