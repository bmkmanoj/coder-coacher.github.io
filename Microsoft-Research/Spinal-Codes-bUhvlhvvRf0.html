<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Spinal Codes | Coder Coacher - Coaching Coders</title><meta content="Spinal Codes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Spinal Codes</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bUhvlhvvRf0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning everyone so today I'll be
talking about some work that actually my
advisor my former PhD advisor how r
balakrishnan at MIT did on raid list
codes and they're called spinal codes so
just before I get started just before
get started just some housekeeping notes
so you may or may not be aware that we
have the website the schedule up on the
website and all the papers my work
yesterday the partial packet recovery
this paper this spinal codes work all
these papers can be downloaded from the
website so your homework is to go to the
website read download and laborious ly
understand all the details in the papers
and I really encourage you to do that
because there are more details than we
have time to really go into during
lecture so with that let's get started
with rate less codes okay so as many of
you know the wireless channel varies
very quickly and unpredictably when you
are mobile for many reasons this is a
trace that we took of walking speed
mobility and we're showing usnr this is
a trace that might lead booty guru and
myself took in some previous work that
we did showing usnr on the y-axis
sending packets to a stationary base
station as a client starts walking away
from that base station and you see that
we have this really rapid fading and the
channel is fading out in and out at time
scales of just tens of milliseconds even
less than that and these blades are
really large in magnitude more than 20
decibels of difference and they're
completely unpredictable when they're
actually happening so why does this
happen it turns out that we characterize
the rate at which the channel chain
changes by this quantity that we call
the wireless coherence time okay and to
help you understand the wireless
coherence time let's do this simple
thought experiment let's say that we
have a transmit antenna here on the left
hand side and we have a receiver antenna
two antenna attached to a little cart
moving at some velocity V to the right
okay and in this world we will have just
one wall and no other reflectors so we
have two paths to the receive antenna we
have that direct path of length R and we
have that reflected path bouncing off
the wall okay and when we're sending we
send a wireless signal at some carrier
frequency f propagating at the speed of
light which gives us some wavelengths by
that well known relation so what we see
when the card moves that velocity V to
the right is we see the superposition of
two sinusoidal waves that you see on the
right hands on the right hand side there
actually you can't see the one of the
other signs so I the stationary sign
here which is the solid curve the solid
black curve and that corresponds to the
that corresponds to one of the paths and
then I have another sine wave here
moving corresponds to the other path and
the signal that we receive at the
receive antenna is the sum of those two
sinusoids and when you have two
sinusoids moving past each other like
that you get this combination of
constructive right there and destructive
right there interference in that red
curve which is the sum of the two okay
and if you think about this world for a
minute you'll realize that when that
receive antenna goes a distance of a
quarter wavelength the the path
difference the difference between the
two paths will be a half a wavelength
and we'll go from that constructive from
a peak to a trough from the constructive
to the destructive
case right so this is what we mean when
we mean the coherence distance this is
the wireless channel fading in and out
so typical numbers to put some
real-world numbers to the to this
imaginary scenario typical numbers on
this if your will if you're sending at a
wireless LAN frequency of 2.4 gigahertz
typical right moving at walking speed
you'll have a coherence time that's
corresponding to the coherence distance
by your velocity of about 15
milliseconds so that means that the
channel will be going from from a
constructive to a destructive fade in
just 15 milliseconds okay so we knew
that we've we've known this for a long
time right this is basic RF theory basic
physics modeling and what's the typical
approach to adapting to these fades well
there's two methods two primary methods
that Wi-Fi protocols like a tour to 11
use to adapt to these fades the first
one is that the modulation layer
adapting the speed that we're modulating
right so here I'm showing you different
transmit constellations here's four
qualms where we're sending two bits per
each of those yellow symbols you see
here in this in this IQ plot right and
we know that when the signal gets
received it gets corrupted by noise and
interference and fading and so we have
this metric called the signal to noise
power ratio where we're measuring the
power the ratio of the two powers of the
signal power and the noise power and the
effect we see experimentally of the
signal-to-noise ratio is this spreading
of the received constellation points
here in red spreading out over the IQ
plane now the typical receiver will look
at these received red constellation
points and draw these decision bound
I've drawn in the dotted black lines and
they were the typical received receiver
will decide that this symbol was sent
when the received constellation point is
in the lower left-hand quadrant and so
on for all the other symbols so if we
have a very low signal-to-noise ratio as
you see here in the upper left-hand
corner the thing to do would be to lower
the rate of the modulation sending just
one bit per symbol binary phase shift
keying as you see in the upper right
this will reduce the amount of bit
errors that the receiver makes
conversely if we have a very high snr
will see the received points tightly
packed in the IQ space like this so that
means that we could go faster we could
go step on stepping up the modulation 24
qualms and sixteen qualms and still make
very few mistakes crossing very few
decision boundaries even at a higher
order modulation so this is what Wi-Fi
is doing or trying to do during these
very fast fades in the constructive in
the constructive parts of the
propagation when the wireless channel is
very strong it'll step up the modulation
in the in the destructive fading out
parts times it will step down the
modulation the next thing the Wi-Fi does
in addition to varying the modulation is
varying the coding rate ok so we have
the modulation this is a rough idea of
the physical layer of 8-11 Wi-Fi right
we have the modulation and the
demodulation here down at the bottom the
modulator is taking these channel bits
that are the output of a channel encoder
the message bits are coming in from
above going through the encoder and the
encoder is adding redundancy in those
message bits to give us some number n of
it's there is greater than the K message
bits that came into that encoder so as a
result we have some fixed code range are
that's the ratio of K &amp;amp; N and the
decoder is trying to guess or infer the
right channel message bits given those
receive channel bits in general lower
your code rate are the more parity bits
you're adding to the message the more
redundancy you add to the message and
the more mistakes you can tolerate down
here at the wireless layer but of course
the slower you go so it's a trade-off so
that's exactly what a tour 211 is doing
when it's varying the bit rates it's
varying both the modulation scheme and
the code rate that it uses and here's a
typical table you'll see in some product
literature from 802 11 Wi-Fi vendors so
we we have this we have this scheme
where we're transmitting fixed rate
codes and we're transmitting fixed rate
modulations so the rate of data going
across that wireless link or the maximum
rate of data is capped so here I'm
showing you some different modulation
and there and the throughput that each
achieve at different SNR ratios and you
probably see probably seen very similar
graphs to this before and you'll know
that bpsk is going to cap out at about
one minute per second but it will be
able to achieve that one megabit per
second at a lower signal-to-noise ratio
than higher-order modulations and and so
on so forth as we go up in modulation
rate to qpsk quam 16 and so on so these
bit rate adaptation protocols are aiming
to stay on the
on kind of the envelope of this curve as
signal-to-noise ratio increases by
changing the modulation scheme and the
way they do that the way they change the
modulation scheme is they have to get
some feedback from the receiver because
if you think about it the sender just
sends the bits over and the sender
doesn't know what happened right when
the bits are received and decoded the
receiver must give some kind of feedback
at least in terms of how I set this up
for the sender to know whether to turn
up the rater turn down the rate and
typically what in the past how this has
worked is the feedback is in the form of
the presence or the absence of an act
packet right so think back to marinello
yesterday and partial packet recovery
yesterday the sender is sending over
data frame if the receiver gets it he
acts if not just silence that's the
stock eater 211 you can vary this with
knacks and and so on but the
fundamentally is the presence it's a
1-bit feedback essentially lots and lots
of bit rate adaptation schemes as we've
talked about in the summer school in
other lectures have been based on this
based on this method but notice that we
have latency right so what's the latency
what's the time between when the sender
transmits and when he gets the feedback
it's the entire frame time plus the
turnaround time plus the time to
transmit and receive the ACK and only
now only probably milliseconds tens and
hundreds of milliseconds later does the
sender have an idea of what happened on
the channel right and the wireless
channel is so quickly varying that this
this information may be well out of date
depending on whether the mobility is
walking or stationary depending on
whether people are walking nearby many
many factors going on here the other way
that we have that before the
introduction of reckless codes we had of
assessing the right rate to use was by
measuring and other speakers have
alluded to this as well Coon was talking
about this yesterday measuring the
signal-to-noise ratio using the preamble
part of the packet I did some work on
this with with mightily in two thousand
and Hari in 2009 we were using the hints
from the from the physical layer to
essentially estimate the bit error rate
but again we have this delay so it gives
you slightly better higher-quality
information on the feedback but again we
have this delay between the sending of
the data and the receiving of the
feedback to tell you how to adapt next
time around so any questions I can take
so far about rated adaptation before we
go into rateliff
yeah so the question was those were
modulations does this apply to air
correcting codes yes so this is a
typical error correcting code you're
adding redundancy by adding parity bits
and but you're still adding the key
point here is you're still adding
redundancy at this fixed code rate right
so you still have to play the same game
of adapting that code rate in response
to the channel conditions I say so the
question was how does how does this get
started initially when the client sends
the first packet to the AP for example
and the the answer is that typically
these bum these bid rate adaptation
algorithms will start at the basic the
very lowest rate and then quickly
quickly ramp up to a high rate so start
you do the sensible thing basically you
start to conservatively okay so rate
this codes and this is the beautiful
idea that is behind the sitcom paper in
2012 that's linked from the website
respite for spinal codes read those
codes the idea is at first glance
something that sounds like a really bad
idea okay the sender should now transmit
information at a rate higher than the
channel can sustain okay so we're not
going to try and adapt to the channel
now we're going to send impossibly fast
and the receiver is not going to be able
to understand what we're set the the
whole content of what we're sending
right that sounds completely broken
right so we have the wireless channel
now varying in time fading in and out
and say this curve here is the rate that
the receiver the wireless channel could
sustained okay what rate lyst codes are
doing is sending at some higher rate
that is above the best rate that the
channel could possibly sustain so we
take this wireless channel we look at it
we say okay we're not going to be closer
to the AP than about one meter that will
give us a max rate that the channel can
sustain right because the SNR can't get
possibly can't possibly get better than
if we're one meter away but we know it's
going to fluctuate like crazy under that
rate and we say okay we're going to send
at this orange right here and completely
completely disregard what at the center
what's happening on the wireless channel
okay and now our difficult task but one
that they managed to solve with the
spinal codes is that the receiver is
going to extract information from this
from this very fast dense transmission
at the rate that the channel can sustain
at that very instant okay the receiver
just is able to achieve that varying
capacity and throw out this adaptation
loop entirely
okay so that's final codes we're going
to talk now about how the sender works
so we're going to have to arrange for
some kind of encoder algorithm to
operate at the sender to send this very
dense constellation okay so it's not
going to be a bpsk or a qpsk or a 16
quam constellation like you saw before
it's going to be something very dense we
don't know what it is yet okay that's
very high rate and we're going to pick
this we're going to pick the bits the
constellation points that we send in a
clever way so that the decoder which has
a very difficult task I think you'll
agree the decoder has as simple a task
as possible and then we'll see how well
it works okay so here's the encoder
right I'm going to take a message with
Big M bits i'm going to divide my
message i'll divide my message up into
chunks of K bits each and then I'm going
to have the sender and the receiver
start by agreeing and this is kind of
built into the system as it ships okay
before it ships the sender and receiver
have all agreed on some hash function a
random hash function okay and at an
initial state that's going to be fed
into this circuit or or algorithm that
I'm showing you here s not so we feed a
random number in to the circuit on the
left hand side here and we have the same
hash function operating on each of the
blocks of data and the result is we
compute this series of spines I said
I've shown you here in blue s1 s2 s3
where the spine is the hash of the
message bits and the seed and then the
next spine is the hash of the output of
the first with the next group of message
so notice the information flow here
where's where's the information flowing
let's think about that for a second ok
so these first K bits of information
those flow into all the spines because
they flow into the first spy indirectly
the first lines output flows into the
second and so on so the state of one
spine you can see depends on the message
bits in that stage and all the message
bits prior to that stage so now it'll be
clear that only the very last spine at
the end here will have information about
the entire message those trailing bits
in the message only come into the hash
function of that very last by Ness and
over k that should be there should be a
big m / k instead ok so now we have our
spine values and you can think of these
you can think of this hash function as
taking bits and a real number in and
giving a real number out ok a continuous
number just a real-valued number so now
we need to take these real numbers and
somehow send constellation points ok so
how do we go and do that so we attach to
it each spine a random number generator
and this random number generator it's a
pseudo-random number generator and all
it does is take that spine value s and
use that as a seed into the
pseudo-random number generator standard
standard computational element to
generate a sequence of c bit numbers
these X's here
and the way we send these seebut numbers
is we make passes horizontally across
each of the spines okay so the first set
of data that we send bits now so we can
send them on the air is the x 11 x 21 x
31 and then we go back to the start of
the message make another pass but go
back to the start of message make a
third pass okay so now we're striping
across the message sending information
about all the bits by the time we've at
we finish the first pass yes so so far
I've just told you about how to generate
bits encode spining coded bits out of
the message bit so I haven't said
anything about constellation points
which is what I'll tell you about next
ok any questions so far up up to this
point
so k is ok it's just the block length
here do you mean a given message in a
sense so yes you can think you can think
of the hash function as a lookup table
that's true but there are expert as
we'll see in a second there
exponentially many property many
possibilities about what will come out
of the hash function so it's not like
it's that easy yeah there's a question
over here yeah
yes so the question is the question is
are we the sender is transmitting at a
rate above the channel what the channel
can sustain does that mean we're
transmitting above the Shannon capacity
yes it is trying to transmit above the
shank Jen capacity the channel okay so
okay so you're asking you're asking
isn't it true that the wrist the sender
and the receiver must make errors now
all Shannon says is that the receipt the
communication can take place at a rate
actually i'll get to what shannon says
in a few slides but all shan't answer
your question all shannon says is the
communication can take place at that
maximum rate it does Shannon doesn't say
that oh if the sender sends such and
such constellation this can't happen
doesn't say that so we're not violating
the channel capacity the probability of
error opt error is bounded away from 0
ok ok ok ok gotcha ok so I think this is
yeah yeah and also this is this result
is probably asymptotic with respect to
the block length and we are not
approaching this so I don't think we've
a violation of this here
okay all right so we've gotten to the
point where we're generating bits out of
these out of this message now let's talk
about how we map that to actual
constellation points to send right so
two simple ways that they propose to do
this the first one is a uniform mapping
where we have just a very dense
constellation that we're sending and
we're basically taking the sea bit
numbers we're taking each see bit number
and mapping the first one to the
in-phase x axis of the constellation
diagram and the next C bit number to the
y axis the Q the Q dimension of the
constellation diagram right uniformly it
turns out that we can also do you can
also think of other mappings that are
more efficient so you might be wondering
at this point what's the best
constellation shape all right so let's
think a little bit about that okay so
let's start with a square a square
constellation and remember that the
power I send when i send any symbol in
this constellation the distance of each
symbol from the origin 0 here in the
middle is going to determine the power
that i'll be sending so if I draw a
circle I'm tracing points that have the
same power constellation points possible
constellation points that have the same
power so now let me do a kind of another
thought experiment where I take points
that are outside this circle and I move
them in to be inside the circle so I'm
not changing the number of points that
I'm sending here when I go from A to B
right and we saw earlier that the number
of points you send essentially
determines the rate at which you send so
in case be here I'm sending at the same
rate but I have moved point points
outside the circle to inside the circle
so I've reduced the power on average
that I'm transmitting
in be here this is called a shaping gain
in the literature and furthermore the
spacing between my points hasn't changed
why we went from A to B so the
probability of making an error has
stayed the same so i can send at the
same rate here but i've reduced the
power so now let me equalize my power in
my circular constellation with case a
here in kc so now i'm just adding points
here around to equalize the power in the
two cases so now i can increase
throughput right so you can play these
games with the shapes of constellations
to increase the throughput they chose
not to in spinal codes they choose they
choose usually to use a square uniform
constellation just for simplicity but
it's just the observation I'm making
here is that there's actually room for
improvement in the in the way they've
designed spinal codes so and as you'll
see later they get results that are very
close to Shannon capacity so it's kind
of interesting that there's still room
for improvement ok
alright so what we do now is why don't
we take a five-minute break because
we're going to have an hour and a half
slot so when we come back we'll talk
about the decoder and then the
performance alright let's get started
again yeah question yeah yep okay so the
question is how many bits on the encoder
side how many bits are used for the
constellation right so we're taking one
of the X which is C bits and mapping it
to the I axis and the other X the next X
in the past mapping it to the q axis so
to see bits per constellation point yeah
yes yeah so the question was does the
decoder know about the sea yes so all
these parameters K the block size the
message length see the decoder knows
about yeah all right so now let's talk
about the decoder and will be begin by
talking about maximum likelihood
decoding alright so the way the decoder
works is we have this remember the end
coder now we have this hash function we
have our message bits and i'm going to
give you an example here the will will
stick with to simplify things so my
example is that the sender is
transmitting a 1 and a 0 and that's the
message ok and k equals 1 so my block
length now is one bit ok and i have an
initial seed here and on my sender is
taking my one hashing it to some
constellation point right taking the 0
hashing that to another constellation
point and those are my transmitted
symbols ok and now let's talk about how
the decoder works so here now the
decoder i have received symbols which
I'll denote for you by X's here ok let's
just for simplicity again let's assume
that the noise is very low so
transmitted symbols are roughly equal to
my received symbols so now what the
decoder does is instead of inverting the
hash function which should actually some
people during the break ask me about
that's a hard problem instead of
inverting the hash function the decoder
is going to replay all the possibilities
of the message that was sent okay so
first case 00 right the decoder is going
to run those that candidate message
through the encoder again and come up
with a set of replayed symbols which all
indicate here by the O's for you here
there does that for all four
possibilities conceptually does that all
possible messages that are sent okay so
now we have four possibilities of
messages only one of which is correct so
how do we decide between these four
possible messages what we do is we
measure the total distance between the
received symbols and the replay symbols
so now here's my original picture you
notice that if my first bit is incorrect
that'll go into the first hash function
and give a result that has a large
distance from the received symbol but
the observation is that now that
information that incorrect information
will flow into the next hash function
and even if I guessed the next bit
correctly i still have bad information
coming on this link right here to the
next hash function and that hash
function is just going to blindly
compute another replayed symbol but
because i have bad information here on
this link that replayed symbol will be a
large distance away so this is the key
to the decoder working so well in spinal
odes one incorrect one incorrect guess
during my replay message causes the
distance to increase at the first
incorrect symbol or bit that I guessed
okay if i guess correctly now over here
as you'd expect everything will match up
and my symbols will be on top of each
other i'll have a very small distance ok
and i'm taking for the distance I'm
taking the the total distance summed
across all stages in one of the passes
for my received symbols so here I guess
incorrectly both times and of course
there's no correlation between the bits
coming in and and the receive bits so I
have a large distance and here I guessed
my first bit correctly second bit
incorrectly so my distance spikes up at
the first bit sorry the second bit
because information only flows from left
to right ok so the takeaway message is
the distance is going to is going to
jump up increase at the first incorrect
symbol that I replay when I replay so
now to decide between the four possible
messages I some the distance and I take
the minimum distance and in this case
with no noise I've successfully decoded
the right message ok can I answer any
questions about how the decoder works
okay well it will get so the question
was that does it go through all the
possibilities and not going through all
the exponential possibilities is how to
make this tractable yes so stay tuned
yeah right so they're doing Euclidean
distance here because we're in
constellation point space so we can
compute a reasonable Euclidean distance
they can generalize these codes to work
with just bits if you don't have access
to the physical layer of your of your
device but that's not what we're talking
about here okay so now we're sending and
receiving one pass and trying all the
possibilities okay but what if the
channel is really bad and there's lots
of noise corrupting our signal what will
happen is we'll need to send more
information we'll need to lower the rate
okay in order to be able to decode so
the way that does this remember the way
the encoder does this is by making
multiple passes over the same message
blocks right the sending pass one has to
pass three and so on so now that the
decoder let's think about the the
sensible thing to do now at the decoder
side at the decoder now our picture
looks like this slightly different the
first pass i've drawn in red here i have
replayed symbols and received symbols
now in my second pass i'll have another
set of symbols here in black and they
belong on the same IQ plane because
they're they're having to do with the
same data the same output of the same
hash block so for every pass I get a new
set of replayed and received symbols and
what I'm going to do is I'm going to
take the average distance I'm going to
sum up all the distances involved both
across stages
spines and down across passes so i'll
take the some distance some euclidean
distance now of all this and everything
i said before about went distance
increases when you have an incorrect
guess applies here okay but the beauty
of this is that as the number of
increases increase increases I'm more
immune to noise because if I guessed
correctly say here I guessed correctly
but if noise threw me off that's going
to throw off the decoding okay that's
going to throw off the metrics and make
this correct decode less likely to be
picked so the more passes I make the
more I get to average out any bursts of
noise that throw any one of these passes
off the correct point okay so i get this
averaging out of interference and a
noise okay so let me give you a few
questions just to kind of formalize this
this is exactly the same formulation as
i've shown you graphically and you can
show that this is actually a maximum
likelihood decoder what I've described
graphically so you have 2 to the n
possible messages that could have
possibly been sent and you can show
theoretically that the maximum of the ML
decoder minimizes the probability of
error this is picking the best most
likely point to have been set it's
choosing the message m prime that
minimizes the vector distance between
the vector of all the received
constellation points Y and the vector of
constellation points that would have
been sent that's the replayed as the
replay constellation points that would
have been sent if M prime were the
message and here we have just the
minimization over all possible messages
of the the Euclidean norm of the
of the two so in more detail we're
taking remember X is the teeth
constellation point sent in the elf pass
and we're summing over all TNL for the
Euclidean distance between the replayed
and the received consolation points why
okay so now in order to limit our search
let's talk about how to think about this
this decoder in a tree structure the
observation is here for our guesses we
have two guesses that share the same
initial bit so let's merge these two
guesses into some identical stage now in
our in our in our thinking and we do the
same for the two stages on the right
that have any of the initial bits of one
now let's take now let's take the the
two halves of the tree and we'll to have
so that we saw on the previous slide and
now let's arrange this into a tree with
s not spine values 0 at the root okay
and the upper branch of our tree is
going to correspond to going and
guessing a 0 for the first bit and we're
going to hang the the the graph on the
previous slide off that portion of the
tree the lower branch of the tree is
going to correspond to guessing a 1 for
the first bit will do the same for the
graph on the previous slide okay so now
what we have is a tree that has n over K
levels one level / spine and every
branch of this tree corresponds to
guessing either a 0 or a 1 for that
particular message block
and now remember what we wanted to do
was we wanted to find out the some
distance between the replayed symbols
and the received symbols so what we can
do now in our tree structure is label
each of the branches of the tree with
that distance the Euclidean distance
between what was received and what was
guest so now a message corresponds to a
path through the tree from the root down
to some leaf okay so that's a 1 and a 1
message and the metric associated with
that message corresponds to summing the
values of the spines over that path okay
so I'm just rearranging the structure
into a nice tree structure in
anticipation of the next step which is
the real the real spinal codes decoder
okay now if we make multiple passes
we're going to label our branches with
the sum as you might guess the sum over
all the passes so that a sum over a path
from root to leaf computes exactly the
ml metric that you saw before yes yes
that's right so these the 01 messages
here are you can think of those as the
blocks but I'm just simplifying things
yeah okay but as some of you have
realized this tree how many leaves does
it have has 2 to the N leaves
exponential number of leaves right if n
is 12,000 bits right for a 5900 killed
by a bite message right then we have way
too many leaves to compare so what are
we going to do about this
alright so observe here think about the
ml message call that M star and then
think about some other message m prime
and that ant message m prime differs
only in the i'th bit so because of this
prefix structure that we saw before only
symbols after that that errored bit will
disagree so what that means is the
earlier the error that aired messaged
the the earlier the bit error that that
aired message has the larger the cost of
that incorrect message m prime and the
larger the cost the less likely we are
to erroneously decode m prime instead of
M star
so what they observe is they can show
that these runners up the m primes
differ only in the trailing bits of the
message so you're more likely to make
errors in the last few bits of the
message so if you think about the tree
and you think about the best 100 leaves
in the tree it's likely that they'll
have a common ancestor with the ml
message m star in a small number of
steps so the strategy is as you decode
as you go through the tree only keep a
limited number of ancestors let's see
how they do that so what they do is they
explore the tree but they don't go down
to every single leaf ok instead they
limit their search to a beam search
meaning that at every level of the tree
they have this beam factor B where they
say ok we're not going to explore more
than B ancestors in the tree and then
for each ants a fridge member of the
beam they look ahead down to a depth D
and compute the scores for every one of
those children and they call this the
bubble right so they're looking down
depth D in the bubble and they compute
the scores which are just the just the
metrics on the edges that we talked
about before ok so the algorithm
step-by-step works like this they have a
here they have a beam of two they're
going to look ahead now and expand the
bubbles down one step here in the second
step be
they're going to score each of the
children here so here I'm showing you a
lighter color for a better score these
white nodes are the best and then
they're going to take these the set of
eight nodes here which are the survivors
here and sort them and discard all but
two bubbles for the two to enforce the
beam constraint ok so this bubble here
has a red and a pink so it was the worst
this bubble here up top has a red in the
pink so it was tied for worst so they
pop these two bubbles and they maintain
these two bubbles as the survivors for
the next step ok so the rather than
going down all the way to every single
leaf they just limit their search to a
beam
okay so no so the question was is there
a possibility that Yuri explore you
backtrack and you go Andrea explore this
popped bubble at some later time the
answer is no surprisingly and it's
surprising because you worry that the
right answer might be here in this
popped bubble if that happens we've made
a decoding error but they've the the
argument on the previous slide was that
the right the right answer always shares
a common ancestor is likely to share a
common ancestor with all the best
scoring nodes no matter what so it's
unlikely if you have a beam that's big
enough that you you lose the correct
answer in the decode but it's it's
possible though yes yes that's right
right so when we talk about performance
keep an eye on how we pick B but yeah
question okay so the question was Burt
what what about burst errors in the
early part of the tree right so look at
how we're set look at the order in which
we're sending the data right so if we
have a burst error what's going to
happen is we're going to knock out just
one pass or a fraction of a pass right
so subsequent passes we'll get to
average out that burst error okay so we
picked we sent horizontally for for a
good reason any other questions about
the decoder
yeah yes yes yes right actually yeah so
the observation was this is very similar
this is exactly the sequential decoder
it also resembles the sphere decoder as
well the breadth first sphere decoder I
will say though in response to that that
the structure of the encoder the prefix
structure of the encoder is something
that I haven't seen before yeah sure
sure well right but there's this prefix
structure we can let let's talk further
later but this prefix structure of the
end cutter I I found unique all right so
do you cut a complexity so how now how
much computation do we save by limiting
ourselves to that beam okay so we're
going to have remember our message
length is n over K or look ahead is d so
we have NM of K minus D steps each step
is exploring be the beam nodes right
times how many nodes our bubble expands
to every time we look ahead which is 2
to the K times D okay I've k is the
block length I've only shown you k
equals 1 so be x to the D if k equals 1
and then we're in value 8 than a random
number generator L times for each of the
L passes that we make then we need to
make some number of comparisons in order
to select the best be candidates so
overall we're making a for the decoder
we're doing this many hashes we're doing
a number of hashes and now let's look at
this big o notation doing a number of
hashes that is linear in the message
length and over will n right and we're
doing a number of hashes that's
exponential but only exponential in
those fixed parameters K&amp;amp;D k is the
block length these look ahead right so
we're exponential only in those constant
really factors and same story for the
number of comparisons we do to choose
the best survivors in each beam okay so
another comparison point in terms of
decoders are the low-density
parity-check LDPC decoders those use an
algorithm called belief propagation
where some of you will be familiar with
and you might be a good question to ask
here is what's what's the complexity
comparison with these LDPC belief
propagation algorithms right many
similarities in that they operate in
iterations as well there they have the
advantage of being very parallelizable
as the spinal codes decoder does to some
extent difficult to give a head-to-head
comparison but in the end the
performance of spinal codes is quite
good as as we'll see in a second all
right so so far the only story I've told
you about in how to adjust the rate of
transmission right was to make more
passes ok but this has two problems so
we said that remember the information
flow in the encoder we said that in
order to transmit information about the
last bits we had to send the last symbol
so that means we have to transmit one
full pass so we're going to max out at K
bits per symbol I may be off by a factor
of two there but i'm going to max out is
some factor of K bits per symbol so if
we want to speed up that max
rate that we can send that we would want
to increase K but that's a non-starter
because we saw before on the previous
slide the decoding cost is exponential
in K when I keep that low so what to do
the other problem is that we have this
problem of adjusting to the fine snr of
the channel okay we want a a finer
tuning knob then simply passes okay
because if we look at the rate versus
SNR curve we're going to see steps if we
don't have some other story of adjusting
the rate other than passes and we're not
going that we're going to sacrifice
capacity we're not going to get to the
maximum shadow and capacity of the
channel ok so the mechanism they have
for introducing a fire controlled rate
is this puncturing idea which is a gehen
a classic idea in communication systems
the idea is let's not send all the
spines say we have 32 spines here let's
not send all the spine information let's
just have let's skip a few in between
and send every eighth for example so now
we introduce sub passes in the first sub
pass send every eighth spine so we
quickly get to the end of the message at
a high rate and now the receiver can
attempt to decode before this full pass
concludes right so we can possibly go at
a very very high rate in fact if you
think about it in the extreme right you
would just send the last spine
constellation point right and if a
channel were good enough that would have
all the information of the bits in it so
interesting the decoder algorithm you
can basically make trivial changes to
the decoder algorithm you can introduce
a zero score to those missing symbols
and you'll score the the sub passes
correctly right so now we've increased
the max rate if we do a puncturing of a
28 times K bits per symbol so great and
then on some subsequent sub passes we go
and fill in right uniformly until we've
reached a point where we send all the
spines and then we start a new pass a
fresh and this by the way is kind of
bringing up a point about when V when
does the receiver attempt to decode
right so now let's let's pause for a
second the physical layer and think
about framing and the link layer and all
that when should the receiver attempt to
decode right at this point when we send
the first sub pass it can attempt to
decode and it can send a message back to
the to the transmitter but the that will
require the transmitter to pause and and
wait for that feedback and interrupt its
it's for datalink transmission right
that is wasting channel capacity right
so actually here is is a great kind of
connection between the talks in the
summer school so such ins full-duplex
work right would allow the receiver then
to start would allow the sender to not
pause and the receiver to send that
feedback on the reverse link full duplex
while the sender's going and then the
sender could just stop right but the for
such an before the the full duplex work
was really proven and and matured the
authors of this paper of spinal codes
had a Mac protocol for a half duplex
send a receiver pair to decide when
those poor the optimal pause points for
waiting for feedback that was published
in moby calm of a couple years ago
alright so more about the link layer
they have the standard synchronization
techniques to maintain synchronization
sequence numbers for the packets the
other thing that we see in terms of
performance and we'll see this in a
second is that a shorter message length
this is actually converse to most of the
standard codes that you'll read about in
the literature a shorter message length
and is actually more efficient right
this kind of makes sense if you think
about the way the decoder works because
longer you have for a message the more
opportunity you have to lose the correct
message in the decoder in a popped
bubble right so what they end up doing
is dividing one of one long link layer
frame into shorter code blocks and then
coding over those shorter message
messages all right so how well does it
work so somebody during the break asked
me about the hardware implementation and
admirably they've they've gone the extra
step to implement this they did an FPGA
implementation I believe this is the
block diagram of the FPGA implementation
which I'll let you look over in the
paper for the most part but at a very
high level what they have is they have
in the decoder they're going to have
worker blocks that are exploring K bits
at a time and scoring each of those
candidates and delivering them to a
selection unit which is selecting which
of the bubbles right to keep a witch of
the bubbles to pop and discard and then
they copy that to a back tracker units
the back tracker unit is essentially
remembering the path that the decoder
took through the decoding tree and then
remembering that so that when they get
to the leaf that they think is the right
leaf they can backtrack and decode the
message they store that there
and the rest of the details I'll let you
read about okay so performance so an
aside about Shannon capacity and some of
this will be review for some of you but
let me let me say it anyway so here's
Shannon and this was at Bell Labs just
post World War two and the question that
they were thinking about back then was
what's the fastest that they can
communicate bits without error over a
link with some SNR and before Shannon
came along we thought that the fastest
way to do that was to send slower and
slower what Shannon said was no up to
some rate we can add coding we can do
the right coding and make the chance of
error arbitrarily small so this is the
Shannon rates see that we talked about
ok so the codes that they were designing
back in those days we back in in those
days we weren't very good at at yet at
designing codes so they were very far
away from capacity okay and so we didn't
actually realize the significance of
this result until we had codes that
actually reached capacity so kind of a
funny circumstance where it took time
for it took time for this theoretical
results be appreciated basically okay so
in the experiments they did they
simulated a wireless channel with with
two cases both with Gaussian noise just
constant additive white Gaussian noise
and with this special type of fading
called Rayleigh fading where the channel
is doing what I showed in the beginning
its fading in and out because of
reflectors nearby okay and they
implemented this on that vertex five
FPGA on real 10 and 20 megahertz
bandwidth channels
and they looking they're looking at this
quantity called the gap to capacity the
gap to Shannon capacity okay so here's
the picture that you should keep in mind
right the rate here bits I send per
symbol use is here on the y-axis and my
snr is here on the x-axis my Shannon
bound here is up here in red and the gap
to capacity of some scheme some code
right is going to be the difference at
some rate of how much noise a capacity
achieving code could tolerate versus how
much noise I tolerate okay so if I want
to go at six bits per symbol my codes
gap to capacity is to decibels if I can
tolerate if I need an snr of 20 decibels
whereas a Shannon achieving code would
need an snr of 18 decibels it's my gap
to capacity okay so and these questions
the answer now came up before how well
there are they doing against other codes
we want to know how well are they doing
against these two codes called rapture
and Strider so Raptor codes are codes
that are based on X whoring random bits
of data together in a clever way and as
all say about this their rate lists
they're not rated as we sure as we saw
as i showed you before strider is
actually worked by by suction that
appeared in sigcomm a few a few years
ago and it was another type of rate less
code and then they're also evaluating
against these rate rated codes like LDPC
this is significant because LDPC were
one of the first rated codes to actually
achieve very close to that shannon
capacity back in the 90s all right so
these are some of these are basically
the world-leading code
and then how should we choose these
various parameters that I talked about
bits per trunk beam width number of
output bits per symbol C okay so this is
kind of the the Marquis result they have
here with the students is rate this is
the bits that we're communicating per
per channel use per symbol we send as a
function of SNR and here's the familiar
Shannon bound here in red okay so we're
going from SNR of minus 5 decibels noise
is 5 decibels stronger than our signal
all the way up to an snr 35 decibels our
signal is stronger by 35 decibels than
the noise okay so just for to put this
in perspective typical wireless LANs are
going to operate around 40 decibel max
max 35 or 40 decibels all the way down
to maybe a men of 5 disables okay so
they're actually exceeding the region
that we typically operate in wireless
LANs okay and they're in the spinal
codes are in the blue curve here very
close to Shannon bound right so what
this means is that decoder is is
choosing the wrong bubble sufficiently
rarely for them to be achieving this
this high rate so that's that is the
significance and surprising result of
spinal codes okay the dotted green curve
here is is choosing a longer a slightly
longer message length so that blue curve
was a 256 byte a bit message the green
curve is 1024 bits and then down here in
the cyan is the envelope of what rapture
codes do ok so the Raptor codes are
going to be ok the rapture codes are a
rate less code and they're going to be
maxing out at some rate here and then
down here
in purple here you can't it's it's
hidden it's hidden behind some of the
curves but down here in purple is that
envelope of the LDPC curves okay so each
of the LDPC curves is going to be like
the rated curve curve I showed you at
the very beginning of the talk it's
going to be going down from infinite
from zero up to some rate and then
across when it maxes out at its max rate
okay so they've taken the envelope of
how a bit rate adaptation algorithm
would do with a set of rated LDPC Kura
codes and this is significant because
this is what the latest versions of n a
total of an N and a total of an AC they
both use those LDPC codes a top with a
rate adaptation algorithm a top match
okay so that's the rate the best that
they'll be doing and spinal is is out
doing them in addition they'll have to
be selecting the best rate right to stay
on that envelope and that doesn't always
happen right so another reason why this
rate this approach be it spinal or even
Raptor would be outperforming a rated
approach okay
the other the second reason that a
rapist code in general is out doing a
rated code is this hedging property and
let me see if I can explain this in the
five minutes I have left so we said that
we were operating at some constant
signal to noise ratio right so the power
on average of the noise is some fraction
of power of average on the data but at
any particular symbol time that we send
there's going to be some amount of noise
some deterministic amount of noise
impacting the channel and it might be
higher or lower than that average SNR
power this is just an average so what
that means is you have to be kind of
risk averse either if you're using a
rated code if you want to keep not
making mistakes if you think of the
noise as a Gaussian you have to pick a
point and and send it a rate such that
your decision is is way to the right of
the LCM so that you won't make mistakes
most of the time you have to be a bit
risk-averse so what they did to
demonstrate this was they took the same
code spinal codes and they used fixed
rates okay it didn't adapt the number of
passes and they plot the rate they
achieve as a function of SNR and then
they took the spinal codes rayless the
full spinal codes and they're they're
well above each of their fixed rated
codes right so they're they're
essentially weightless codes get to
hedge their bets on what happened
actually happened in the wireless
channel after the decode and that's the
advantage and you see the same effect
when you have an unknown fading rally
channel okay so now we're we're looking
at rally channels rally fading channels
with an average snr with what you see on
the x-axis and they get a bit of a
degradation
in terms of performance but they're
still well above spreader
okay so how about how we choose the
chunk length right so that was the caper
ammeter now so we said that the decoder
is going to make be x to the k
operations / k bits and remember the
decoder is trying within every chunk the
decoders expanding the tree the decode
tree within every chunk so the bigger k
is the harder the decoder has to work
right so what they did was a basically a
trade-off analysis where they're trading
off performance here the fraction of
capacity a fraction of Shannon capacity
that they achieve on the y-axis versus
varying k on the y on the x-axis it
turns out they're not too sensitive so
this graph starts down here at point six
of Shannon goes up two point nine so
they're not too sensitive is the first
thing to note about choice of K and that
you can you can basically pick a
sensible value based on this trade-off
okay the last thing I'll talk about is
sending those short messages so again
looking at gap to capacity on the y-axis
SNR on the X if we have a longer message
then we have more opportunities to
proven that correct path so you can see
here when the message length is 64
they're achieving a very small gap to
capacity close to 0 at low SNR up here
at the higher sm bars you see that gap
to capacity of luxury fluctuating so we
can see that puncturing that inability
to really finely adjust the the rate is
is hurting us okay so that's all for
today about spinal codes I think the
takeaway points I'd like you to think
about with spinal codes is you know just
the surprise I was very surprised when I
I saw that this schemes could come so
close to Shannon capacity using such a
simple encoder that really surprised me
and actually they've they've given my
research group the source code spinal
codes and my students have actually run
spinal codes and verified these results
so I believe they're little bit for sure
eliminating the need to run a bit rate
adaptation algorithm and simplifying the
design for better performance so I think
time for lunch now
any questions you want to take off yeah
maybe yeah so few questions it's time
for a few questions before go to lunch
yeah yeah the hash function was
technically universal loss of passion
okay so question was is this from a
universal class of hash functions that
would be what you want theoretically and
at first they used I think they tried to
to go with what Universal classical
class of hash functions it turns out
that it didn't matter in the end so much
so they and the hash function itself was
what was limping the performance it was
taking up most of the computation in the
decoder so what they ended up doing was
they simplified the hash function
function to something resembling I I
don't remember exactly this is from
personal communication with them it was
something like a bit a few X ores and
maybe an ad or two very simple and they
and performance still remained high so
it didn't it doesn't need to be truly
random surprising huh good yes so
tomorrow standing the spine code
actually have make you on innovations
here when I say it's a coding structure
okay now there is a decoding structure
so how do you see this performance gain
is from this coding structure or you
could instruct okay so the question is
like where's the innovation is it in the
encoder and in the decoder so I think
this is no means they have the two
innovations but yeah come here lease
which way you've contributed much I use
their pop limits I see so which does the
encoder or the decoder country bit more
to a performance that's a good question
so I think the key structure is getting
back to what we're talking about before
that I think the key part that's giving
them the good performance is that prefix
structure in the decoder that causes the
metric to jump up the minute they make
an incorrect gas
and then immediately that metric jumps
up close to the error point so that the
bubble or the the backtracking decoder
the bubble decoder can choose the right
can realize this and choose the right
path so there was a comment here that
the decoder is is the sequential decoder
so fair enough so it's okay but fair
enough but i think the the observation
the structure of the encoder i believe
is is novel but I I don't know the
literature quite well enough to say that
for one hundred percent sure that has
that has been some literature on three
codes with exactly this prefix structure
as well so the nobility appears to be if
if I'm a hazard a guess I haven't read
the spinal cord paper so I don't know
but I am familiar with year with the
previous literature on three codes and
sequential decoding having contributed
to some effort myself to that effort
myself so the novelty appears to be on
the exploration it appears that you are
essentially exploring if you explore for
example if you have a tree code you can
show that with three codes you can
achieve capacity provided you to maximum
like your decoding convolutional code is
essentially a trickle longer the longer
the constraint length essentially your
size is much larger you will be able to
get close to capacity the problem is
decoding complexity then they came up
with this suboptimal breath for instead
of bread first let's do depth and then
let's backtrack that's the sequential
decoding algorithm right and it turns
out that they actually had analysis on
sequential decoding algorithm which
essentially said that when your rate is
larger than capacity you will not be
able to do the decoding your exploration
will essentially take you too long the
expected number of stages before you get
to a leaf explained expected number of
operations before you get to a leaf
turns out to grow exponentially rather
than
polynomial so there is a cut off and the
cutoff is actually very unimaginative
Lee called the cutoff rate so it turns
out that the complexity here is the
decoding of course if you explode
everything then you'll be able to get to
capacity what they seem to have
optimized over here and I think that's
probably the reason why we've been able
to get a little closer to capacity
perhaps then cut off late is in choosing
the right p you're you're doing first
but you have enough for breath as well
why are the choices would be that's
that's its guess that I'm huzzah it
sounds like it's fair to say that then
you could say the contribution is is
then the combination of using a tree
encoder with a sent a sequential decoder
and displaying it marginally right able
to get beyond the right so in that
context one other comment that I wanted
to make was the right comparison is
perhaps not the shannon capacity but the
cut off flip because then you know that
if you do one exploration depth first
then you know cutoff rate is going to
essentially be the limiting anything
about that your acceleration would be
will be going back and forth too often
scrubs fish so that's why you don't get
close to capacity you're a little away
how far away you can essentially gauge
it with respect to another parameter or
the cutter
yeah what about the frequency selective
channel yeah yeah so the question was
what about the frequency selective
channels to do they need an equalizer
before that so there was no story i I
just I just talked about constellation
points right so yes the if they're in a
frequency selective channel you do need
an equalizer right now ofdm is dominant
which is basically meaning that you'll
get many constellation points on many
sub caste you can map those to the OFDM
sub carriers which is I believe what
they do in their implementation I would
assume that this would work even for
great ordered constellations and all
that is that correct ah I believe it
would work I seem to remember talking to
them and they had some problems with
gray coding but that is just from memory
it could be inaccurate so since we're
they're using a random hash function yes
it should work but there's all I see or
you're thinking about using stock
hardware right so other than that
there's not much reason to to vary the
constellation mapping function it
doesn't seem to be critical indeed yeah
okay I guess we're done with questions
thanks look guy lives in the next movie
for lunch
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>