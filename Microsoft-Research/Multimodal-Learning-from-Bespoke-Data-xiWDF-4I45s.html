<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multimodal Learning from Bespoke Data | Coder Coacher - Coaching Coders</title><meta content="Multimodal Learning from Bespoke Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multimodal Learning from Bespoke Data</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xiWDF-4I45s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay we'll get started it's an honor
today to have Ken forbus with us he's
the malt water p murphy professor of
computer science and professor of
education at Northwestern University Ken
received his PhD work in his
undergraduate work at MIT he's been my
mind one of the core people over the
decades working on challenges of
representation and reasoning with
qualitative knowledge as well as doing
analogical reasoning and learning
spatial reasoning more recently for my
watching his work over the years sketch
understanding natural language
understanding he's been a long term
advocate and leader in this realm of
cognitive architectures which is an
architecture for reasoning about the
world more generally and he's been
working in number a number of areas in
number of application areas he's a
fellow of the triple AI of the ACM and
of the cognitive science society and
he's received a number of awards for his
work over the years and it's always a
pleasure to have him visit us again
thank you Eric for that lovely
introduction so I'm going to talk to you
today about some of the work we're doing
involving learning from what we call
bespoke data custom data and and why
bespoke data well as you all know big
data is a big thing these days and my
heavens it's wonderful right speech
recognition is finally getting seriously
good in many ways simultaneous
translation and skype all sorts of great
things is capable of but it's not what
you want for everything so for example
if you're teaching a child understands
stories and read then you're actually
interacting with that child but if it
takes that child a million times of its
being exposed to a word to learn it you
got a problem there's evidence that
actually human children can learn new
nouns in a single exposure so there's
something we're doing is very different
than then what today's machine learning
systems can do
if you're teaching a child to do a
jigsaw puzzle or you're teaching an
assistant a new task you don't want to
have to teach someone how to fill out a
form a million times you want to have
data that's actually tuned towards your
estimation of their capabilities and to
be able to work with them effectively in
the same kinds of range of examples the
human collaborators take now why do I
care well one of the things we're trying
to do is actually achieve human-level AI
now if you think about today's AI
systems they're like drag racers they're
very efficient very fast okay but like a
drag racer it does one thing it's
carefully crafted to do one thing it has
to be carefully maintained and baby the
nurse back to health every time it falls
down by carefully trained experts who
know its internals in the way that we
don't know each other's internals if we
did cognitive science would be trivial
ok so now think instead about dogs you
can't ask a dog to do all sorts of cool
things and they don't require a whole
bunch of maintenance you feed them you
give them some affection they don't blue
screen on you a lot right there's all
sorts of great things and and what if
our AI systems were is robust and
trainable and task well as mammals one
way I describe this sometimes is is what
we're trying to build is a sixth-grade
idiot savant from Mars okay very smart
but clearly not from around here enough
where we can communicate with it and
work with it or not trying to make
something that passes for Hume and
that's not the point now the way we're
doing this is a companion cognitive
architecture and what we've done is we
formulate the goal in a way that I think
is actually pretty productive we're
trying to build software social
organisms things that work with people
using natural modalities and for less
that means natural language and
sketching those are sweet spots because
natural language gives you expressive
power sketching gives you spatial
aspects learn an adaptive extended
periods of time so things that can learn
for weeks and months or years without
having human beings maintaining them and
in fact they should be maintaining
themselves most of the time we shouldn't
have to know their internal
representations
just as we don't know the internal
representations of our assistants our
associates or our children now I social
organisms well first of all it's going
to make them very useful but also I
think that's actually essential to
making things that are as smart as we
are Mike Tomasello has made this
argument very plainly in several books
now and I think there's a lot of
convincing evidence for it also vygotsky
has argued that much of a knowledge of
learning by interactions with people and
we actually call companions sometimes
the first Vygotsky in cognitive
architecture that's a goal not in
achievement okay we're trying for that
and there's many things we understand
that we've never directly experienced so
yes you need to ground things in sensory
information but you know none of us live
through the Revolutionary War none of us
have seen molecules directly except with
the aid of a whole bunch of very
carefully crafted equipment we see the
results of play technics but that's
inference as opposed to actually
watching plate tectonics in action given
the time scale it happens and same thing
with most examples of evolution so you
have to have conceptual knowledge as
well as physical knowledge now if you
think about cognitive architecture
there's been a lot of cognitive
architectures and some people when they
hear new cognitive architecture say oh
my lord why one more well if you think
about it Newell actually broke things
down by time scale of tasks so
biological bandwidth neuro modeling
cognitive and thinking about skills
rational band more about problem solving
and reasoning social band and in fact
there's a developmental band above that
and if you look at most cognitive
architectures they focused here so for
instance two of the leading
architectures akhtar and sore have as
their signature phenomena skill learning
if you look at akhtar where they're
going is down here they've worked a lot
to model fMRI data for example and
wheres or has been going is here and so
for instance they've had sore agents
that run for many hours in training
exercises a real tour de force and all
sorts of stuff and they're actually had
it even further up companions we care
about these to dance just as when you're
modeling a complex phenomena you mater
the multiple layers and ultimately those
models
together you need to explore each layer
and you can't say I'm going to solve
this one before I solve that one because
we still be doing quantum mechanics we
wouldn't have chemistry and wouldn't
have meteorology okay and so you have to
do things in parallel and talk to each
other so for us we're starting here we
assume these folks are handling skill
learning really well and they are but
we're more interested in how you do
reasoning and learning at these broader
timescales so that's the big idea of
companions and now I'm going to tell you
about the hypotheses a little bit more
detail I'm going to walk you through
three kinds of examples one is learning
games both strategy games and a very
simple tic-tac-toe thing and I'll show
you a video of that learning spatial
concepts and learning plausible
reasoning and one thing you'll see is it
in all these cases it doesn't take much
data to actually learn these things you
can get a surprising amount if you know
a lot and if you use an illogical
learning as you're learning mechanism so
by analogy I mean getting a structure
mapping theory the ideas analogy and
similarity involves structured
relational representations so you have
entities with relations connecting them
up and you're comparing one description
the base against another description the
target that gives rise to a bunch of
correspondences saying what goes with
what and then if there's things in the
base that don't have anything
corresponding on the target you can
project those as candidate inferences to
do a kind of pattern completion of these
symbolic descriptions now it turns out
there's a substantial amount of
psychological evidence supporting this
model in phenomenon ranging from
high-level medium level vision auditory
learning learning mental models textbook
problem solving and conceptual change
and the evidence is both from purely
psychological things with humans and
laboratories like reaction time studies
and protocol analyses but then also
computational modeling so the story is
actually pretty interesting so we have
these three hypotheses about human
cognition and we think that to build a
eyes we need to actually take these
seriously the first is analogy is a
central mechanism of reasoning and
learning a lot of things we think
his rule based learning actually are
probably applying something that's a
high level structure that still has some
concrete stuff attached to it by analogy
did we get in his article why we're so
smart is a very good introduction to
this idea the common sense is basically
an illogical and reasoning and learning
from experience so you start out by
within domain analogy is and they
provide robustness and rapid predictions
when I go to start up a new car and if
it's the same kind of car I've had
before then I do the same thing and that
works I don't have to have a rule formal
I saying oh it's a lock i have to turn
the key in the lock or it's one of those
fancy fobs i just keep in my pocket and
i don't have to touch it I just do it
okay and now so even one example you can
do an analogy to learn some other stuff
but we do generalize and that
generalization process gets us to first
principles reasoning but the emerged
slowly as generalizations from examples
now by slowly I don't mean millions I
mean dozens okay so it's very different
in terms of time scale I think much more
likely what humans get and qualitative
representations of central Lewis
qualitative representations provide a
bridge between perception and cognition
they provide an appropriate level of
understanding for communication and
action and generalization so those are
like the three big hypotheses were
exploring now there's these models we
have an illogical processing I'm not
going to go into gory detail about each
one of them because each one of them is
an hour talk but I'm going to give you a
picture by how they all fit together and
that will show you how they get used in
these subsequent things so here very
roughly is what we think happens you
have the structure mapping engine which
matches to descriptions and these are
either examples or generalizations and
the candidate inferences give you things
like predictions or explanations or
possible principles to apply in dealing
with some new situation now where do you
get these things you get them from your
memory of course and we have a memory
model called Mac vac many are called few
are chosen that's designed to scale up
the human sized memories because the
first stage is a cheap filter using flat
inversion
of the structured representations with a
special kind of vector design so that
its predicts what SME will produce in
terms of overall match quality it's an
inaccurate prediction of course that's
why you have to the second stage that
actually uses SME over several examples
in parallel and that's where you get
your stuff and now the generalizations
happen because you have basically
another model which uses SME and Matt
faq to sit there and given instances of
a concept like models for a word we've
done this with spatial prepositions of
contact for example in English and Dutch
you basically build up models
analogically by combining these things
using match in keeping the stuff that's
in common and deprecating the stuff that
isn't in common by lowering its
probability of being there so what
generalizations for us are probabilistic
we have a frequency information about
every statement in them they're
partially abstracted there can be very
concrete things in them still and they
don't have logical variables we don't
need to go to variables because we can
just use structure mapping to do the
matching now sometimes we do introduce
variables in fact there's a couple of
cases where we've gone through for
various reasons and turn these things
into hardcore in a real live logical
rules or probabilistic rules but that's
not what we have to do by default to
make knowledge useful and that's the
essence of how the companion
architecture turns over sort of high
love lessons yes do is you take a bunch
of similar examples and abstract out the
commonalities how do you know what
similar examples ours sorry so there's
two answers that question first we we
assume the thing that there are
generalization context that actually are
sort of the analogical models you're
building for a problem like how do I
interpret on or how to interpret in
another example if I'm doing language
processing and doing word sense
disambiguation for this word in the
sense what are the ways in which that's
been used okay so that's that's
categorizing things in terms of
relevance to a problem then to get the
things that you compare against you use
the same retrieval model to retrieve
from the pool of
you've been building up so if two of
them become are sufficiently similar you
store the assimilated version of those
things back is the generalization and
the same retrieval model is used again
so you basically it's it's just yeah
this question like so what is similarity
like this similarity like Oh general
like what is the similarity metric okay
does a similarity meditation for
difference just computers no no it's
computed by a semi it's defined in
advance same similarity metric for where
it since disambiguation matching visual
stimuli story understanding
counterterrorism moral reasoning and
sanely robust so it's this it's the
structural evaluation score that SME
computes for two descriptions so it
really it really is that general ok so
here's some experiments that we've done
early on so learning physics from
costume and analogies you start with
linear kinematics the model of that and
companion is a balloon rotational
kinematics electricity and thermal
problems now Matt back retrieve the
correct thing only forty percent of the
time if you know the psychology of this
that's actually hi that's high because
cross demean analogies are relatively
infrequent well the Beast didn't know
much so it's going to be able to find
stuff more easily and like humans if you
actually give it the precedent it
actually was able to transfer the
information eighty-seven percent of the
time of it had that precedent we've also
done this for learning games at 60 games
which were generated by an external
contractor experiment run by an external
contractor I think this is still the
largest experiment in cross to me and
analogies that anyone has ever done so
John layers group at Michigan developed
a sort of basic game which the
contractor then went and went and did
went crazy over you're trying to take
this character and make it to the exit
by building a bridge to escape Tom
Henrik smart lab diddle version of a
rogue mini version of rogue and that was
little version of mummy maze here
variant of that and what happened was
you
learn a game a game that you've never
seen before and the companion we
basically learn HD and strategies by
experimentation now it knew enough about
games that if it couldn't master the
game in 10 trials it quit because it
never would okay and these games are
easily the complexity of the kind of
Atari games that it takes the thing in
nature 38 days to actually master so I
must not not so impressed by those
results now given one of those learn
games as a base you would then try to
learn some new game sorry and you'd
measure how much faster you learn so
positive regret is good I would have
learned fifty percent faster if I had
the game almost sixty percent of the
time and there is some negative transfer
so fifteen percent of the time in fact
I'd learned twenty-five percent more
slowly than if I didn't have the analogy
and so we're kind of excited about that
now modalities we want natural
interaction that's still a really hard
problem that's still a very much an open
research problem and we made some
progress but well let me show you where
we are so where natural language
approach is kind of different we're
focusing on deep understanding all the
way down to things you can do reasoning
with and we're willing to simplify
syntax just like human cultures do with
children we're quite happy to simplify
syntax because we want to go all the way
to reasoning and decision making we use
James Allen sparser research sites
knowledge-based contents we've been
building our own lexicon replace complex
we use this course representation theory
for for doing semantic interpretation as
the representations for it and we have a
query based abductive semantic
interpreter we use the idea of narrative
function trying to figure out what a
statements telling you in the context
that you're working in for example in
solving moral decision problems the
system is looking for a set of choices
that has to make and decide among it's
looking for the utilitarian cost of each
decision and it's also looking to see if
any sacred values are involved and it
will basically decide in a human-like
way what to do based on those factors
interesting thing about that
those factors are the same for any moral
decision problem and normally abduction
grows that's been issue with the number
of sentences in this system it actually
grows with the number of questions
you're asking as opposed to the number
of sentences and so by basically
formulating it as a top-down problem you
can do pretty well so here's an example
of some of the things you can express
because of a dam on the river 20 species
of fish will be extinct on the left is
the predicate calculus with credit gets
drawn from the psych knowledge base if
you're going to implement DRT psych
micro theory structure is your friend
okay because it's all about context so
these boxes each of these boxes is a
micro theory in Psych with statements
relating the micro theories to each
other so this is the DRT version of that
same hunk of predicate calculus okay so
take some work but it can be done now
what do you do with this well one of the
test beds we use is the strategy game
freeciv it's an open source version of
civilization to its cool because it's
got spatial concepts you've got terrain
of different types you have to design
transportation networks and figure out
where to place your cities you've got a
complex economy you can go bankrupt you
have guns and butter trade-off you have
investment versus immediate spending is
all sorts of complex stuff in here so
it's a wonderful rich domain now one of
the things you do when you think about
this game qualitative reasoning a lot of
it in dynamics grew up around
engineering when engineering you have a
blueprint you know all the parts in
advance and that's a really simple world
in this world it's not that way in this
world your reasoning about things that
don't exist yet your reasoning things
about the reason about things that can
get destroyed so solely object level
representations are impractical when
you've got limited attention and storage
and processing time if I have to build
an explicit qualitative model for every
tile in the game I'm host okay that's
not going to scale and that you have to
plan for things
that don't exist yet so you have to
build models of the dynamics for things
that don't exist so we've introduced the
kind of type level qualitative model
these as predicates and collections as
arguments so for instance queue prop +
normally says the first quantity
shouldn't stay in front of the screen
the first quantity is causally
determined in part by the second
quantity the type local version says a
quantity of this type and a quantity of
this type apply to objects of these
types with this relationship between
them has AQ poppet we knit so in other
words this cache is out to for all X for
all y if x and y are gaseous objects and
x is the same thing as Y then the
pressure of X is qualitatively
proportional to the temperature of why
okay so that's how that's the
translation to instance level but you do
the reason of the type level as much as
possible now if you're dealing with
generic statements like you're reading a
simplified version of the free serve
manual this is a true blessing so here's
a little bit of a manual and here's the
translation after the whole abductive
reasoning processes happen on the first
sentence it's detected these I've
written this and frame like syntax just
for easy visual processing normally
there's a lot more parentheses and
they're all independent statements so
type level process that's age of generic
production process and that kind of
event is actually tied to the language
in the site kb we didn't introduce that
there's some event it refers to a
production from the sentence that's a
discourse variable it's done by
something that's a free civ city the
outputs created is something that's the
type of amount of food and there's a
positive influence of the amount of food
of the rate of production now you take
the second sentence and remember this
because you'll see it again here as the
population of the city increases the
food production of the city increases
and so this is actually a classic
qualitative proportionality introduction
and so it's seining it's as positive
qualitative proportionality and the
constrained quantity is the rate of food
production and the thing that's
governing it is a city
population and finally a citizen
consumes food from the city you're
introducing another process notice it's
a destruction event another kind of
thing from the ontology and now you're
filling out it's dynamic consequences
these qualitative models are very
powerful so one of the things about
strategies in games like this is they
involve trade-offs analysis of the
qualitative model can identify
trade-offs and let your reason about
those now even a little bit of advice
improves performance ah because these
are all things that are happening
continuously while the process is active
so in other words it's it's a mechanism
by which behavior is generated as
opposed to the description of the
behavior itself we have other ways of
describing the behavior itself okay so
turns out these typos level
representations are very useful for
advice so irrigating a place increases
food production absolutely true in the
game adding university in the city
increases in science output now you take
a half dozen pieces of advice like that
and you basically say okay how well do I
do it producing science this is averaged
over 10 different games and by 97 turns
it's still experimental conditions still
doing better on population not hugely
better because that's not what it's
optimizing for its optimizing for
science output here you're getting to
the point where you can build libraries
and other units right before that you
couldn't build them and so these two
look pretty much the same on science
output but now the advice kicks in and
you start getting more science produced
for the cities okay so that's one
modality and it's also a little bit of
the game learning now what about the
other modality sketching is a very
natural way for people to communicate
knowledge this is a picture of a
geologic formation and if you're an
instructor in Geoscience what you want
to see when students mark it up is
something like this that's the fault the
main fault these are the directions of
movement these are called marker beds
and these indicate the displacement of
the market bits okay and we've actually
built something with our sketch
understanding system that handles things
like this
it's domain-independent which is very
important for us because we want this to
scale in an experiment at university of
wisconsin-madison a geo science grad
student made 15 worksheets and showed
significant gains pre-post using those
worksheets and they're into a geo
science class we've has similar results
with a unit on the heart for fifth grade
biology and we're about to go back into
the classrooms in an engineering design
course for learning engineering graphics
and we have independent evidence from
laboratory studies that you could
actually do assessment by looking at
what order people draw things in a
sketch and what they included don't
include whether annotating diagrams this
is a massive effort involving a lot of
people he's a well-known structural
geoscientist bridget is the grad student
who actually cranked out the worksheets
she's really good at this maria and jeff
ER to the cog sketch developers that's a
whole talk there but I'm not to go there
but instead pivot to other roles of
sketching so sketching is also a tool
for thinking completely he says more is
about the ordering that's okay so if I
have princes of photosynthesis diagram
then a student who doesn't know for the
synthesis will start with those visually
salient parts a student who understands
photosynthesis will start from the input
and go through the causal chain to the
output very simple to catch and if you
got did your link you can know but where
do people do things in and this has
happened a couple of domains now
basically if a student understands the
domain well for instance Geoscience
there's certain things they'll pick up
versus certain surface features that
literally are irrelevant so it's pretty
easy to it's it's it's not rocket
science to tell the difference it's not
a subtle signal okay so this is a sketch
by a painter Shona Trescott she was on
an Arctic expedition you'll see some
little stick figures here you'll see
some notes about the background noticing
to reisen disappearing figures
and so in the Arctic you can't really
paint okay and even if you're in the
more genial climate artists often do
this this is the painting that resulted
so she's perfectly capable of doing all
sorts of very fine subtle visual work
but to just think it out she first did a
sketch and this is pretty common
sketching is an age of thinking and so
what you really like is systems that can
sketch with us and sketch for themselves
in human-like ways so here's the
long-term vision you want to software
that understands sketches people do now
what does that mean so here's a ramp and
a block on your gravity and you can
infer the block will slide to the right
and down perfectly sensible now that
requires fluent natural interaction and
human-like visual and spatial reasoning
and conceptual reasoning about the
contents of the sketch and you might be
domain general if you look at the sketch
recognition literature right now every
sub problem in every domain is a
separate system you have to train the
recognizers in you you have to build new
software that doesn't scale that's one
of the reasons why we've done some
engineering workarounds for segmentation
and conceptual labeling we actually
don't do recognition on the whole okay
because it turns out you don't need to
when people are talking to each other
during sketching that's how a lot of the
labeling happens it doesn't require
recognition recognition is the best of
catalyst you look at the speech
recognition research its focus solely on
that topic and we're focused on the rest
so even if recognition were perfect and
there's reasons to suspect that it can
never be perfect at the level you need
it to be especially for education
purposes you still have to do what we're
doing let me show you what that involves
now the thing I'm about to show you is
actually something you can do yourself
if you download cog sketch and fire up
the design coach and ask it a question
about the behavior and what I'm going to
show you is basically a rational
reconstruction of the reasoning in the
truth maintenance system that the system
is actually doing so
here's our ramp so we have the ramp on
the block and clog sketch recognizes
visually that they're touching directly
which causes it to extract an edge
representing that surface contact it
computes the surface normal of that
because that's very relevant in terms of
how forces transmitted it's in quadrant
three here that's a qualitative way of
describing angle and now you think about
gravity and we're now using some of John
Wetzel's qualitative mechanics work you
have a force applied to the object in
the downward direction and that is the
only force on it we're assuming the
friction doesn't matter here and so now
you have a translational constraint from
this edge saying you can't move in the
downward direction and that plus a
little bit of other reasoning says well
the translational motion will be in
quadrant four IE to the right and down
and that's how it infers that stuff now
it turns out there's two parallel
literature's one in artificial
intelligence about qualitative reasoning
and one in cognitive psychology that
they call categorical coordinate and
they're the same thing and they're
interestingly complementary literature's
so the other thing is the structure
mapping processes are using visual
reasoning so let me show you some some
work from Andrew lovett's thesis so
geometric analogies a is to be as C is
to one of those if you download clog
sketch you'll see all of Evans problems
in a sketch that you can play with an
experiment with Andrews model has the
lovely feature that of course it gets
them all right this is a very easy task
it turns out and it makes reaction time
predictions that are borne out in human
behavioral experiments in fact there's a
later paper where by adding working
memory constraints and two strategies
which is the model that actually you can
play with in cog sketch the correlation
coefficient is point nine something it's
insanely good it really is a simple task
a much harder task as Ravens progressive
matrices which is commonly used as a
test to flew with intelligence in humans
Andrews model is better than most adult
Americans and again it makes reaction
time predictions and what's hard for
people is hard for the model and vice
versa and finally there's an oddity task
that DeHaan and spell key used to look
at cross-cultural differences in
geometric process
between Americans of Monday ruku Andrews
model again same representations for all
these things all automatically computed
from PowerPoint stimulii copy and pasted
into cog sketch Andrews model again
solves most of the problems was hard for
it is hard for people and vice versa and
you can do ablation studies on the
models obviously not the people and get
some insights as far as what's happening
across these different cultures okay now
at the risk of putting everybody asleep
i'm going to switch to a briefly to a
video and this is showing you a
companion learning tic-tac-toe the
demonstration of flexible multimodal
instruction in a cognitive companions
architecture and we're going to teach
our computer to play tic-tac-toe through
a combination of natural language and
sketch interaction so we and i say i'm
going to teach you to play tic-tac-toe
and this provides some expectations
about how to interpret future statements
you'll see the preso along though it's
coming down create a new sketch and i
start by classifying the game and I say
tic-tac-toe is a marking game as opposed
to a piece moving game or a construction
game and this tells us to expect some
kinds of marks as well as a board now
we're going to draw the tic-tac-toe
board and it doesn't know what this is
and it's not recognizing the ink or
anything but the board is going to be
the background and we're going to
explain what this is in natural language
so i type tic-tac-toe is played on a
three-by-three grid of squares that
contains a lot of information it tells
us that there's a spatial configuration
its Cartesian coordinates and the
maximum extent of any dimension is 3 and
on that basis it's able to label the
glyph we just made as the board now we
go on to classify the game two more and
tell it that tic tac toe is a two-player
game which means that it can expect us
to introduce some player rolls so X is a
player it understands that to mean that
X is a game player not an athlete or a
musician and we can now draw an X and
doesn't recognize the X but rather it
understands in context that this must be
the X and so you can see that it labels
it with its own little X below it now I
can proceed to make an O and I can do
this before I've introduced the player
it doesn't recognize it and I entered 0
as a player and we can see that it
understands and labels the 0 the next
thing we do is describe the actions in
this game and I tell it that X and O
take turns marking empty squares that
contains a lot of information about turn
taking in the kinds of marks it makes in
the precondition for making marks on
empty squares the next thing we enter is
a description of the goal and we're only
describing one goal here we're saying
that the first player to mark three
squares in a row wins it only
understands row to mean horizontal row
not a column or diagonal but through
demonstration we will teach it the other
wind conditions such as vertical columns
or diagonals and then X goes first is
highly ambiguous and context-sensitive
but it tells it who starts the game and
at this point there's enough information
in this representation to play a legal
game so I tell it to start a game and
what it does is that takes those marks
I've made and puts them in a catalog a
separate layer creates a new layer for
the game state and made its move it made
an X in the middle left square and I
respond with an O in the center square
it's playing a blind random game it
doesn't understand strategy yet so I'm
going to be a little cruel here and I'm
going to demonstrate winning by a
diagonal so I mark three in a row on the
diagonal and in order to help teach it a
new rule I select the winning
configuration our way of doing deictic
references sketches
doesn't know that it's lost yet so I
tell it I win it takes the winning
configuration and creates a new rule for
winning on the diagonal we can inspect
this rule if we look in the command
transcript window in this case the rule
is specific to the particular diagonal
so it will require another trial to
learn the opposite diagonal but if we
demonstrate winning on a vertical column
it will generalize this to all columns
analogously to the rule it already has
four rows so we only need a total of
three trials to learn the complete rules
of tic-tac-toe through a combination of
instruction and demonstration this
concludes our demonstration of flexible
multimode was not a win in this special
variant of no vertical wing tak tak toh
what would happen at this point you
generalize say oh I've generalized in an
inappropriate leading to back off and
create a special yes ok except now
except this version won't actually if it
learns a bad rule its host this version
no right yeah no we're we're focusing I
mean for that project our focus was
entirely on what you have to do in terms
of multimodal interaction to bootstrap
these things ok ok so what was going on
to the hood there you've got your
language coming in translate to a
general logical form predict your
calculus interpretation it's interpreted
in terms of communication events and
instructional events so it understands
how to interpret them and you know in
some cases we have to fall back on
general heuristics do interpretation in
this case no because there's enough
knowledge about the context I'm being
taught a game ok and that's an
incredible driver the digital ink gets
to the symbolic representations also and
interface gestures like they just added
a glyph and things like that also get
turned into the same communication
stream and that's what leads to building
game rules
so the interpretation process as
described earlier this turns out to be a
lot of fun because the psychic knowledge
base has a boatload of stuff in it in an
earlier system we talked about the
Jordan River enters the Dead Sea and I
thought there was an entering the
container event where the container was
the musical group the Grateful Dead okay
so you get all sorts of amazing things
if you have something that knows about
the world and it's sitting there
basically interpreting these things to
where it gets down to a point saying
well X plays the role of something in a
game and there's a whole set of
instructional events if you look at the
intelligent tutoring literature this is
a spin on some of the kinds of
instructional events you see there but
games specific like for instance wind
conditions is not something general but
defining configurations that is pretty
general entity introduction and action
journal ation are pretty general the
game classification is from earlier Rick
on GDL is basically nothing that we
really added ourselves and communication
events there's both the stuff from
language and the stuff from interpreting
what's happening in the sketch world now
where we're going next with this we've
done hex upon that's kind of trivial the
only difference is it's a piece moving
game tomm actually can now describe all
the regular moves of chests to the
system by language and sketching okay
can't do castling capturing up us all
and pawn promotion and all those things
are things that you really want to do by
language you really don't want to do
this by demonstration right how do you
say you can't castle if the thing hasn't
moved that doesn't work so well and we'd
like to then expand to discussing
strategy as well as rules for play now
spatial concepts one place we've done
this is is freeciv where we looked at
geo spatial concepts we can map a fee
civ map into sketch and sketch on
top of the map and so here matt has
basically said drawn a circle and said
this is a straight and what causes it to
do is take the encoding of that region
and shove it into a generalization
context for the word straight as using
that to build up a model of the concept
and then you can then draw circles
elsewhere and ask it to basically
classify those things and it's basically
saying if I do an illogical retrieval
over my encoding of this across the
whole set of generalization context for
the concepts then which is the strongest
match from and that's its way of doing
classification and this is this is okay
I mean you know 60 examples temper
concept interval cross-validation about
seventy six percent accuracy I'm not
excited by that but we look at similar
tasks in in this world it's not out of
band I think we should be able to do
much better but one thing I worry about
is overfitting so it's always dangerous
to just use your own data so there's
this lovely corpus by some folks at
Berlin and brown and what they did was
they took 250 concepts like snowman ice
cream cone giraffe grapes bunny rabbits
airplanes etc and got people on
Mechanical Turk to make a tea sketches /
concepts that's 20,000 sketches now they
claim in their paper that this exhaust
everyday objects I don't believe it and
it includes things like hand grenades
and flying saucers neither of which are
everyday objects for me and especially
hope not together ever become everyday
objects for me okay but it's still a
great corpus it's very tough so here's
the experiment what they did was they
use pixel level image encoding
properties and machine learning
classifiers and reusing cog sketch
encodings because it's all ink we just
suck it in plus an illogical learning
now we start with ten concepts y10 both
run time because we have not engineered
this in the way we need to do to scale
up for large batch experiments but also
another problem as you're going to see
in loving detail in a minute okay
because some of these things we just
can't do yet and we know why and we have
partial solutions so encode with
sketch Eightfold cross-validation and
those numbers of concepts typically when
we do
analogical generalization within 12
examples we've got as good as we're
going to get the only example before
this that was different with
counterterrorism there we needed about
30 to figure out who the perpetrators
were in some events reliably so this was
how they broke it up so we encoded it
their way so here's the results for
three versions of our system these are
two extremes in sage this is the knob
where you say I want everything to
assimilate I set my assimilation
threshold so low it always merges
everything into one model ok this is
turn it the other way has to be a
perfect match before you'll assimilate
it and so it's a way of doing both
prototypes and simpler is the same model
a line is like these two it's using sage
but it also has near-misses
automatically derived near misses if you
get something retrieved from a
neighboring concept that's mutually
exclusive that's obviously a near miss
and so it actually constructs small
hypotheses about what those differences
are that in some cases significantly
help in terms of telling them apart now
hear the differences turn out to be not
so much and if you look at the pizza
level stuff it's also about fifty six
percent but overall 250 categories okay
so for the categories we can do at the
moment hey we do about the same as what
they do but there's two things that
disturb me about this first thing that
disturbs me is you know why so why so
low and why so slow the second thing
that disturbs me is humans are up there
by a turkey experiment that they did on
their data set that's a lot of headroom
okay that really bothers me because
usually we get human-like performance in
a small number of examples so what's
going on well if you look at it close
here's at least one thing that's going
on it could be others so if you take a
turtle this is from their corpus here's
a visualization of the cog sketch
analysis of it so it breaks things up
into edges you can see the Junction's
between the edges it also constructs
edge cycles connected sets of edges and
these things often are cool because they
segment stuff into
objects and these ads connected objects
or sort of higher-level descriptions
that group those things okay now bad
move instead of just picking one level
the one that's most informative we threw
them all in okay so that's a lot of
facts so that's 464 facts for this
turtle now in other experiments it looks
like for an illogical matching if you
can't do between 10 and 100 relations /
description you're not in the game
you're not going to stand stories you're
not going to do moral decision-making
you're not going to do visual processing
you're not going to solve textbook
problems but this is a lot and now when
you start doing more textures it gets
worse okay so a lot of texture in this
turtle right a lot of regions in this
turtle and so what do you only things
happening so here's a here's a principal
trying to extract out of this the part
of the goal of encoding processes should
be to construct concise informative
representations and we've always worried
about informativeness but we've never
really worried about conciseness and
with perceptual processing that's a
mistake so we think of this is going to
be an internal metric on the cognitive
architecture you start thinking in terms
of for balance and assertions and trying
to extract away when you have that there
is some evidence people do this but it's
very weak evidence I would not take it
to the bank way of separating is
actually figuring ground or so that it
may be a layer representation oh yeah no
we're assuming layered so Cox at Jack
she has three actually four different
layers of representation so can do
groups shapes their groups objects and
edges and there's a fourth thing in
terms of surfaces and it'll actually
dynamically move from representation to
representation couldn't do Ravens
without it for instance or the oddity
task but but we have to add more and
this is a clear you know if your think
of this envision terms
this is a texture problem and so we're
looking from the vision literature a
planar Ising model to handle these
regular repeated structures you
basically take a whole bunch of them
that ourselves they're similar in some
way and turn those into the description
that's one big chunk of stuff and say
and it's got a whole bunch of things
that are about the size and about this
eccentric and all that so you basically
represented it now that works for some
of the turtles works nicely for this one
and this one oh my god that was that was
a nasty little turtle before right
because look at all these different
textures in there works for this one
this is all one unit now and it's got
some extra properties of it talking
about the visual properties of the
things inside so the any of these icing
models as you say i'm going to make i'm
going to put basically little little
control points of these various spots
and say can i get rid of these are these
things efficiently alike that i can
merge them together into one unit and
then you do some energy minimization of
that well we didn't we didn't invent the
technique we just got it off the shelf
okay um physical white section right
what i see models are for texture yeah
and we're treating us like actually for
hair for example whether or not yeah i
don't know like strands of hair my peer
review yeah and you know people people
put in what what the sketch recognition
community calls adornment or decoration
right and yeah people do that so you
really have to learn how to handle that
now that works sometimes does not work
all the time so here it didn't actually
figure out that you should group these
things that way here it turn the entire
turtle into one big blob okay so we're
still we're still trying to figure out
what's going on here now i'll close with
with one more example and this is
learning to do in prints on structures
this is not a problem we picked out this
is a machine learning community problem
and so the idea was well you got the
Semantic Web which is growing by leaps
and bounds you've got the whole
knowledge graph thing in Google and I'm
sure Microsoft has an equivalent thing
I'm sorry sorry Tori ok yeah its
lightness so can you do traditional
logical inference sort of but the data
is incomplete and noisy statistics well
their structures not feature vectors so
if you're a dedicated machine learning
person you'll say we can fix that we can
vectorize those suckers ok so you make a
distributional vector space you take the
relations you mush those into vectors
and you crank around you make a tensor
network and you do stuff ok now i think
it's sort of sad i think feature vectors
are not as expressive inherently as
relational representations and if you
have the structure to begin with it's
really a mistake to not exploit it so
you get good accuracy but it requires a
lot of examples and it has to train over
all the relations at the same time and
it lacks interpretability you get this
number that this is the best one you
can't say why let me show you a better
way so you build cases for analogy by
pathfinding and the inside here which is
not unique to us is if you have a
relation between two entities they're
likely being directly related to other
in a bunch of other ways and as usual
with these sorts of algorithms you put
in limits on branching and search for
tractability so parent of one person
another you also generate negative
examples by corrupting that triple into
something else and that's the same
technique that these folks use and what
we do is we basically search through the
database and grab a bunch of relations
tying those two things together and
those become a case and we only grabbed
10 positive cases and permute those to
get 10 negative cases 10 remember that
number it'll come up again then for
template learning we do analogical
generalization and so we basically take
two cases and we match them to find out
what corresponds and then we basically
use sage to construct templates each if
after this match if these are much
together
the knees only appear in one so they're
going to have probability point five
these appear in both they're going to
probably one point o if you keep going
and see the things that aren't in common
are going to get smaller and smaller
probabilities now the things that are in
common stay high okay now you can do
better this the usual way we do these
things is just say hey we use SME and we
let that be that now what what Jen
figured out was okay there's cases where
some properties are really for a task
more important than others and you'd
like to learn what those are and you'd
like to bias the match so think about
the numbers computed by a structure
mapping computation and I'll convolve
that with task-specific importance
weights and so he computes those by
extending logistic regression to work on
structured data so think about an
analogy between vectors in a structured
case dot product becomes structure
mapping vector addition becomes
structure addition let's age is already
doing with the alignment step and then
you train with gradient descent etc so
if I've got the traditional way of doing
it with input vectors each vector
position tells you what goes with what
so that's trivial and then you get your
dot product with structure mapping these
are expressions now and I have to
compute what things go with each other
and then I get my stuff that then I can
train by gradient descent how well does
it do well so there's two data sets the
people have used for this they have a
word net data set and a freebase data
set looking at 11 to 13 relations
respectively large tests that a whole
bunch of training data and the other
models use typically 10,000 training
examples and train on all relations at
once we use 10 and we can train for each
relation independently you had a new
relation we don't have to retrain on the
others and how does it come out that's
the scoreboard that's our system
and you notice the top systems in both
cases are not the same and so we're
right up there we're not the best on
either corpus but we're right up there
with three orders of magnitude less
training data I think it's pretty cool
I'm excited about this and you know it's
we've done this plenty of times before
on data sets that we found or generated
from our reasoning systems but to be
able to do it on data sets the machine
learning community is done and get the
same performance we're very happy about
that not at the moment and we're trying
to figure out why that is yeah the
regular face an interesting randomly
chosen now the other thing it gives you
as explanations so you can say basically
by sort of evidential explanations
what's in the matching and this
deliberately chose to not be interpreted
either Lee so if you had a natural
language generation system to test your
knowledge you'd have a hypothesis like
saying well this person is Tongan and
you ask the human yes or no and they're
going to say oh right unless they have a
lot of world knowledge in which case why
are you learning this knowledge but you
can say well as parents Tongan a person
of the same country as him as ethnicity
is also Tongan and then you can look
that explanation and now have more
context you have something some
knowledge about the reason why the
system believes that and we think that's
also very valuable and so by sticking
with structure you learn fast your ad
you get explanations which are not
inconsiderable advantages so just to
wrap up for some purposes bespoke data
is better than big data if I'm trying to
Train software assistance and I'm
looking at you Cortana you want to say
something to it once like never show me
Fox News in my notification stream ever
again I can't say that I can say that it
won't listen oh you want human-like
learning of tasks
because you want assistance to be at
least as good as we are at learning
about the tasks that we're training them
for and rich knowledge supports learning
from bespoke data so we didn't use
analogy in learning tech tech though we
actually just talked it through and
because the system knew about games more
broadly it was and knew about sketching
and knew how to connect those things and
knew about instruction it was able to
interpret that information in a way that
mattered and if you're doing learning
structure mapping can support learning
with a small number of examples and
better representations like sticking
with structure when you have it and drop
your data needs by three orders of
magnitude in the case we just saw and
that's we think that's true more broadly
and a little joke here just like orange
is the new black structure mapping is
the new dot product ok so I'll end by by
thanking all the people who really made
all this happen thank you questions yeah
so you mentioned earlier in your talk
you mentioned the word understanding yes
do you have a definition for that what
does understand that the system
constructs representations that enable
it to do the kinds of things that we
would expect people to do given that
same material in the same context way to
measure if a system understand is by
having human or by by measuring its
performance in some other way I mean so
here's an example from another study
we've done so you take little kids and
you give them a forced-choice task where
you have things that are are the same in
one dimension versus another and turns
out you can pick the task so that four
year olds can't do it there a chance
eight year olds can do it okay and then
by by cleverly reordering the stimuli
you can get four year olds to learn it
okay without feedback
okay and this is something did we get in
there and her student Laura kotovsky did
quite a while ago now here's the cool
thing if you're going to simulate that
you need to do two things you need to
figure out how to do the forced choice
task in this case it's are these things
similar enough and you know which of
these is more similar and you need a
signal that says I'm not doing it right
yet and so how do you do that when a
fourth choice task if my encoding of the
situation's is not sufficient to make it
clear which one to pick I know
internally without someone telling me
that this is not a good encoding and so
the system actually cast about a bit to
figure out what a better encoding is and
then snap you know it gets it okay and
so internal signals like that we think
are critically important and so we're
kind of on the lookout for those now
right which is why on the the encoding
thing and conciseness we're now thinking
that should be a big signal if you're
getting too much stuff you're clearly
encoding it wrong and you have to think
about this thing differently and rather
you could advice from people on how to
do it or you have to search your own
space of alternatives open question but
I think those internal signals are also
crucial one thing that happened in
learning reader is you could have the
good news there's good news and bad news
if you want to text surprise then you
could you could give it some new example
and if it's been building up an
analogical model of something and it's
got a bunch of examples in it and
nothing's retrieved that's a surprise
right because if you've really seen all
the space then you should get something
now it turns out the bad news is there's
two reasons for that to have happened
you really are seeing a different part
of the space or you have an error in
your natural language understanding
system and then you get more things with
that same error telling those two
situations apart is really hard okay so
that's that's the downside of it hey ice
hard
maybe can share some tuitions that you
showed nearly ER about the approach that
you can play of your career high metal
fixtures and results coming out of some
of the larger scale of data or folks and
all those things we talk but like the
Atari game learning but like Eve mind
and other groups they've been doing that
kind of thing we're thinking is upping
along these lines asleep as a general
community challenge problem in larger
spaces like a couple of gated games
people look at general connectionist is
the old named for its pile okay models
that promise to learn from from data and
potentially having supercharging those
someday with these kinds of abstractions
we're starting to the abstractions using
a little bit of learning on top your
thoughts on methodology and approach
will simulator bury these more human
using author kinds of approaches even
when you it's just really big boxes but
arrows between them well okay so so
think about what we're doing we're
making the bet that you can safely
packaged off a lot of the perceptual
processing and hand JM that so for
instance our clock sketch for better or
worse is our model of 2d geometric
processing and vision that's it doesn't
do everything envisioned by any means
think about texture color shape from
shading just a raft of problems it
doesn't try to address but it's a sweet
spot for interacting with people so you
know gerbils learn in slow ways as do
people there if you think about how long
rehabilitation takes after an accident
physical therapy takes forever right
because our motor learning systems
aren't
that fast and you probably don't want
him to be that fast they'd over adapt in
bad ways but the stuff that's the humans
can do that's pretty darn amazing is
couple the stuff that lets you interact
with the world in reasonable ways with
all this amazingly complex conceptual
knowledge and so I think if I were to
make a bet I have made this bet you know
we're going to have human level a eyes
helping us figure out how brains work
rather than understanding about how
brains work be central and making human
level a eyes there's an analogy that
can't afford and Pat Hayes did with AI
and artificial flight and they pointed
out the first flying machines did not
flap their wings and did not have
feathers there are plenty of attempts to
make those at that time but the ones
that didn't and succeeded basically said
we're going to understand the principles
of flight so the principles of
intelligence now to understand those
principles I still take a lot of
guidance from humans because humans are
the best example we've got but I'm also
willing to look at other animals okay
and so eventually in fact now materials
have gotten better and different the
people are trying to build flying
machines that are like birds but that's
over 100 years later I think I what will
happen with AI to you know eventually we
are going to have one story it goes all
the way down and says you know these
dopamine receptors look in all the way
up to you know your knowledge about
intention you know Mel sauce like me
hypothesis where think of it as an
analogy between yourself and other
people right I can say that hypothesis
without talking about dopamine and I can
have a completely good scientific model
conceptual reasoning without knowing
about neurons and so that's why I'm
betting the way I do and glad other
people making other pets because I could
be wrong and ultimately the whole
story's got it all hooked together and
so you really have to have all these
things going on
but so you guys are gonna do some game
learning then we talked a lot about it
but okay actually your opinions oh yeah
I think games are great so we did the
sketch games as a DARPA seedling no more
money for that alas but I do think it's
the kind of platform where you can
imagine really open it in conversations
let's talk about it offline okay
everybody out there so but like you know
if you think about the way people are
playing go or chess it's not very
human-like right think back to the old
chase and Simon results so one of the
reasons why we're doing this is we'd
love to make a companions executable
that has the sketch game capability
built in as well as a whole bunch of
exceptional reasoning built in so that
people could then experiment and I'll
bet you that you could make a really
cool like human like chess player they
could really explain what it's doing in
terms like those you would see in a
chess book for example okay and even if
it wasn't the best chess player hey
would still be kind of cool thanks very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>