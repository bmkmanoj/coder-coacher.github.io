<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introducing Project Naiad and Differential Dataflow | Coder Coacher - Coaching Coders</title><meta content="Introducing Project Naiad and Differential Dataflow - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introducing Project Naiad and Differential Dataflow</b></h2><h5 class="post__date">2012-11-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nRp3EQy6ccw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so we were with Frank McSherry
he is a senior researcher from MSR in
Silicon Valley or Mountain View I guess
I think they're one in the same right
absolutely
so you were working on a really
interesting project called Naiad which
is tangentially related to Dryad and
something we covered pretty extensively
on channel 9 with one of your co
scientists right Michael is art exactly
and that was a fun conversation it's all
about data paralyzation right yep what
is not so 9 is sort of where we went
after doing the work with Dryad and
drive linked drive link took Dryad data
parallel computation tried to make it a
bit more accessible for people who are
familiar with using C sharp visual
studio they wanted to write programs
that look like link as opposed to it's
horrible scripts for firing up poop jobs
and stuff like that so we gave them all
the good features of a integrated
development environment type safety all
sorts of good stuff NIDA sort of the
next step we got a lot of people asking
us for features and dried and dry at
linked sort of system level structural
features like I want to be able to get
early results out of my computation or I
want to be able to do interative
computation which is something that
Dryad didn't support it only does
directed acyclic data flow graphs okay
and so we started thinking about how to
how to go about doing this and actually
initially we started taking Dryad and
trying to hammer on dry it and bang it
into a shape that would allow some of
these features but fundamentally things
didn't really seem to be working out as
little annoying and difficult so we
thought that and we thought for a little
bit I thought you know what how could we
actually do this if we had to start over
from scratch and after a bunch of
thinking we came up with some neat bits
of of math related to how you do data
parallel computation that turned out to
be much more general than just getting
early results out or just doing loops
this neat system that has all these
nifty composable properties you actually
do them all at once which is unlike
anything that we're aware of from the
research community before and
interesting so so can you like at a high
level can you kind of explain sort of
the math behind it we don't have to get
into the equations and weird symbols
that you guys like to do that's already
in the paper people can read that
absolutely they want to geek out on the
math so so the general gist of this sort
of thing is to Beckwith with try and
write Lincoln and lots of big data
systems you have these collections of
data which you can imagine
a terabyte or so of data that you tell
the computer to pick up and process in a
certain way computer picks up the
terabytes of data and if you're running
something like a group buy or a joint or
something like that it fires a key
function across all the records figures
that where to send all of thee of all
the records and it goes and processes
all of these things and that was good
enough for that that type of work what
people have realized and they certainly
realize this a while ago twenty years
ago the database community was already
doing incremental computations so they
would say it's it's silly to pick up the
terabyte again to do a join if only five
records have changed let's but the new
five records and see what they're
supposed to match in that joint and just
produce the new records out on the other
end same thing with loops it turns out
people have also realized in
approximately twenty years ago that if
you're gonna do a loop with a big data
commutation you don't really need to do
the whole computation each time you
should just figure out what's changed at
the end of the loop push those changes
into the beginning of the loop again and
push them through again as if someone
had changed your data your input from an
sergeant a source so these ideas were
already out there and we could have just
made nayad be a clone of these ideas
just implemented that people probably
would have been pretty happy it would
have done the cool things that people
had asked for but there's this neat
twist with Nyad that is what makes it
actually new interesting research which
is that rather than having these
collections which in an incremental
setting sort of vary from the first
collection to the second collection to
the third collection this linear total
order there's always the most recent one
and it's always just a diff of the
previous one these collections can
actually change in multiple independent
dimensions at once and this allows
really cool stuff to happen particularly
you can incrementally update the input
to an iterative computation so you have
these two dimensions one of them being
the sort of the epoch or batch of input
data that your computation is taking the
other dimension is the iteration around
the loop inside the computation and as
far as we're aware most of the previous
techniques for doing this type of
computation essentially have to restart
the loop each time new data shows up
there's there's not really a lot you can
use from the previous time through words
we can actually push changes through
iterative computations there's one of
the examples we might get to out of
doing this and getting results back in
millisecond time scales Wow having done
big iterative graph analyses on
social networks and stuff that's amazing
so why don't we like see what this looks
like there's a lot of people watching
this of course our developers they used
to system that length they think those
query operators and stuff yeah let's see
what you've done so it's very very
similar to link we've definitely tried
to make it be as accessible as possible
and in some cases we have succeeded in
others there's some things we couldn't
really hide but the general idea is is
that you have a a Naiad dll that you you
link in and you just fire up using nayad
and you have access to the nayad
namespace which has a few useful things
in it to get you started one of them and
this the I should say this this program
is a one of several programs in an
example download that we have it's
actually driven by a main method in a
different different file I'm just not
going to show you that for the moment so
some things you know we could have an
execute rather than a main that sort of
thing but the idea is that you have
initially everything starts off of this
Nyad configuration file this tells night
how you want it to run you want to round
on remote machines you want to run it
locally
we're gonna run it locally here you spin
up a nayad controller and this is the
the thing responsible for making sure
all the work gets done in and I had
computation typically you just have one
of these per computation if you're using
something like the reactive framework
this might be analogous to a scheduler
okay so you spin up one of these folks
and it controls all of your your
resources and make sure stuff happens
then and this little bit here is the the
program it's going to be really simple
you ask the controller to make a new
input for you this is a lot like asking
for a new I observe able something like
that this is a new place that you can
put input data that you're going to base
your computation off of you're going to
attach a few things to this and build
your data flow from this from this route
so we've tried to mimic a lot of the the
length patterns in this case we're going
to take this text input it's meant to
accept strings and and subject strings
to further computation we're going to
take this and we're going to hit it with
a select many function a lot like in
links this is something that you hit
each record that it sees with it with a
function and it produces a list of
output records this case we're just
gonna split lines of text into multiple
individual words okay
we then throw a count at it this is a
slight deviation from link so count and
link if all y'all are familiar with that
is sort of a terminal operation returns
an integer to your program it interrupts
the data flow all right so you can't
really continue further data flow off of
that the count that we've done here is
sort of a dataflow friendly counted it
produces records that are still inside a
collection okay so it's like doing group
by and then account essentially for each
of the groups it tells you how many
things had that that particular key and
that's exactly what we want to do in
this this is a word counting I should
have said example we're gonna take all
of these words we're gonna count them up
when we go say how many times have you
seen a particular particular string all
we're gonna do when we do the count is
stick together the key which is the word
we had in mind with the count which is
the number so nothing too surprising and
then we subscribe to it in the same way
that you would subscribe and the
reactive framework so it says something
that's gonna get called every time a new
record gets produced from the count and
we've scrolled down a call back here
that's going to print out all of the
stuff that happens to get I hand it back
to us and that's the nighted program so
there's a little bit more program of
course there's a c-sharp side of we have
to get the data from the console and
that's not related to dyad well it's not
specific you know you certainly you want
to have it because otherwise you haven't
actually given that any input yet so
that's no good but but it's not at all
complicated says you can just see it
here it just starts putting lines of
text off of the off of the console and
every time you give it a line it stuffs
it right into the to the input of the
computation ideally exactly as would
happen with with the reactive framework
with that and I observe her in i
observables mm-hmm so the pattern we
hope is gonna be familiar to a lot of
people the thing that we're imagining is
going to be different is that the
performance you're gonna see out of this
the scalability the ability to
distribute across clusters and do
exciting things like these iterative
computations will be different but just
to give a sense let me let me run this
and this is the console dot write line
that we that we started there it just
wants me to enter some lines of text and
I will say things like hello world and
press ENTER and we get some output back
and this is the the thing that's
different about a Naiad
as if compared to the reactive framework
the I observables is it is going to show
us records so hello : one is one of the
records we might expect to see that's
the kind of the word let's go show us
these records with a number after them
so that they're not just records that
we're getting an output there's actually
what they are telling us the change in
the frequency of that record so this is
saying that previously hello world had
some frequency in the data to zero in
fact it didn't exist in the data set now
it's here and it's got a one higher
frequency than it used to have so
there's there's one copy of that record
now your data set but I took something
like hello this makes it a little
weirder uh you see a negative one so
what's happened here is that the record
hello : one has disappeared from the
data set now there's no longer a record
we want to show in the output instead
there's hello : - which is not the new
correct account for for all of these
things in the reactor framework this is
a little complicated so you can't really
take records back right show someone
something and you've shown it to them
now and you have a sequence that's meant
to grow indefinitely an important part
of Knight is ability to correct previous
outputs that you've you've transmitted
and that's it's part of both the input
to thee to the system so if I had
changed the program a little I could
take back inputs I could do a
subtraction of world for example and
undo that it would tell me you know
world : one has just gone away but all
of the internal aspects of niya it's
really important to have these pluses
and minuses the ability to undo an
aggregation particular in this case when
the correct answer one has changed to -
you want to be able to to take that back
sure so you know this is not not too
mysterious
this app is meant to be incredibly
simple so it's not good not mind-bending
you can play with it indefinitely but it
gets boring pretty quick the main dish I
add is not really doing much work no no
no behind the scenes it's just hanging
out and it's put these terms into an
index that have a account next to them
in each time you throw a new word at it
it looks them up in the index bumps
their account sure and does the
appropriate outgoing logic okay so
pretty simple in the intent is to try to
convince people that this is actually an
easy system to get started with at least
sure um I have another example
look I can throw up all right which is a
slightly more complicated one let me
change the size here to 150 so this is a
connected components computation
connecting components is a graph
algorithm I guess I have a time it's a
decomposition of a graph like a social
network into these disjoint of the
vertices in two disjoint regions that
everyone within a region can reach
anyone else mhm and you know it's
interesting if you have a social network
what are the connected components you
know who actually interacts with whom
and that sort of thing so connected
components could be groups of friends
ever
absolutely well there's a particular
mathematical definition that is not only
groups or friends but all the people
that they can reach all the people that
they interact with add out to an
arbitrary limit and actually that
arbitrary limits the complicated parts
so in a standard data perla setting if
you just said go one hop out or
something like that they're fine with
that you can write that on something
like dry out dry link just one step is
good but in this connected components
computation you have to go out an
unbounded number of steps you don't only
know how big it's gonna be you could put
a hundred in there but you'd be wasting
your time if 100 was too big and you'd
be wrong if 100 was too small sure so
you'd like to be able to do this
iterative computation they were talking
about before you start doing your work
and and if things change if the ability
you know the set of people you can reach
it grows a little bit you do a little
bit more work based on the new people
you can reach and you don't redo the
work based on people you already know
you can reach got it so we're gonna do
this particular example which is going
to be a much bigger scale we're gonna
throw together a big random graph and I
want to say it's on the order of 10
million nodes or so we're gonna fire in
here we're gonna put a bunch of edges
and about 5 million a little bit more
this is the right number to make it an
interesting computation turns out
otherwise it's too easy
and then like before we set up a
controller the same way we would before
we prep an input which is going to have
these pairs of integers that's all we're
gonna describe our graph each edge is
going to be a source and a destination
integer get a stopwatch ready but then
we fire up our connected components
computation this is an extension method
that all I'll possibly sneak us up to in
just a moment but it goes off in it it
returns a bunch of inter pares that
describe for you
vertex in the graph what is the label of
the component they belong to and
everyone in the same component should
have the same label and then we just we
do a little bit of counting here so that
we can report how many components are
there of each size essentially what
we're what we're looking at as these
results come out they'll tell us how
long it took to remember that when we we
actually start running it because it
might be otherwise might be a little
mysterious to see what's actually it's
actually going on but with the exception
of this extension method which which
I'll get to that's basically the program
you know we push in the data down here
so edges on next and you can sort of see
this is hinting a little bit the
distributed nature so if if the
controller's process ID is equal to zero
put in the data otherwise don't put in
anything this is already looking forward
a little bit to running this on multiple
computers and if you're on a multiple
computers each you don't each process to
put in the entire data set again you
just want one person to put it in
they'll take care of distributing it out
for you so all of these these programs
the same program runs on your laptop as
runs on the cluster it's meant to be
very nice that way you're not going to
get too confused about wow distributing
stuff across compute clusters is hard
it's supposed to be easy if we've done
our job right the program was going to
continue after we stick all the data in
and allow us to start doing updates to
it so there's this weird iterative
computation we're gonna start changing
some of the inputs a little bit and I
just I you know will look at the answers
and see what how they've changed but the
timing is gonna be the most interesting
thing so how long it takes actually do
an update to this this big computations
going to be gonna be interesting let me
just say a little bit about what
actually happened in this connected
components computation this isn't the
one I showed you the one I showed you is
connected components fast but this is a
lot easier to understand now what's
going on here they're just basically is
that we're gonna take a bunch of labels
at each vertex we're gonna push them
through the graph everyone's going to
keep track of the smallest label they've
ever seen and the idea is that everyone
within some connected component things
reachable from everyone else is
eventually going to take the label of
the smallest node present in that
component a so they'll all stabilize at
the same value but to do that so we need
to start with some labels and we need to
run this iterative algorithm
propagates this this value alone and
that's this this propagate min feller
here that is actually pretty easy it's a
little you know we can you can sort of
posit maybe and go through this in your
brain or check out the download yes
the codes in there but it turns out it's
all just link stuff with a new fancy fix
point method but otherwise doom joins
can cats means nothing nothing too too
weird so I'm just gonna fire it up okay
it no I'm not gonna sorry I need to tell
it to start connecting opponents instead
sorry now I will fire it up so it's
gonna go off and it's warning me that
it's gonna compute these connecting
components on a big graph I made it
pretty big so that it takes a little
while and we don't just get the answer
immediately it should be just a few
seconds I hope I'll save screwed this up
the number of nodes are looking as 10
million or so that's you know pretty
modest for a laptop you can definitely
get bigger than that the night is
constrained by the amount of memory
essentially it likes to keep stuff
around in memory so they can quickly get
back to things sure when when things
have changed yeah it's gonna be fine
Yeah right there we go so so what it's
printed out for us is for each of the
sizes of components here 2 3 4 5 how
many of those components did we find in
the graph and the labels were chosen in
such a way that there's a really big
exciting one at the end so there's a lot
of folks that are of me a medium size up
to 600 or so and there's one one fellow
who is about two million or so vertices
so how you know a lot of computation
needed to go on to actually identify all
of those people and only those people
but the fun thing now is each time I hit
enter it's gonna take an input an edge
in the original graph and rewire it to
some random new edge basically and it's
gonna tell me how long it took in this
case it took 36 milliseconds to push
that entire computation through that
data flow graph through the iterative
you know propagate until terminated
thing and it telling me what's happened
here so one of the components of size 5
during into one of size six and one of
the components of size 346 turned into
345 hmm and you know you can do this as
much as you like all of these times are
the let's say tens of milliseconds tops
and they're absolutely updating
correctly all of the information about
so ever get these components let's talk
about that yeah so I mean it's
impressive that it's so fast all right
but I mean in some sense you have this
giant graph that you've created rice and
I think what we should do is talk a
little bit more about these iterative
computations or they're sort of because
when you press ENTER a random thing
happened but there wasn't a whole
compute again that's absolutely just a
little right right so what what night is
good at doing is keeping track of
essentially what are the dependencies in
the computation so when I take an edge
out for example tonight is very aware
how that edge because we've broken
everything declaratively how that edge
interacts with the computation it
remembers what got propagated along that
edge and it knows once that edge is gone
I need to undo that propagation I need
to tell people at the other edge that's
right other end of that edge you don't
have this label anymore I changed my
mind
or at least the frequency of that label
has gone down by one someone else might
have told you it's okay to have this
label but not this edge anymore and
those consequences will will carry on
the ripple forward that vertex might say
oh I shouldn't have told the
Skai downstream that I had that label
and tonight we'll sort this out but
it'll stay totally away from the part of
the graph that hasn't changed
so if nothing's going on in one big huge
hunk of the graph and most most cases
nothing's going on
no computation will occur it doesn't
even bother to check it to see if
nothing has changed it just knows it's
not because this is all all push we've
been told what needs to be worked on
someone has said this is the particular
edge that needs to be changed so we just
do that work and we avoid producing any
spurious reconsider the whole graph or
anything like that so this this in
principle could be taking milliseconds
you know arbitrarily large scales you
have you put a terabyte of memory no
computer and run this thing on it will
still take milliseconds so now when you
think about connected graphs of
information it could be that at some
point something changes that should be
reflected across the whole graph so call
up the God change surfa something
massive
how is Nyad understand it it's it it
understands that and it will do all that
work a problem so it you know if you
have two big parts of the graph and
there's one edge that goes between the
two of them the president said that it
is really important for the computation
if you take that edge and take it out
put it back in and take it out put it
back in
you're gonna annoy now a dance could do
a lot of work and it's that particular
update isn't going to take milliseconds
right sure it's actually going to go and
not only compute the right answer for
you which is good it's gonna leave the
system in a state that you can still do
these incremental updates afterwards so
it's gonna need to update all of its
breadcrumbs as well Wow
so the the size that I chose for the the
number of edges is actually the right
size to leave the graph in a sort of
very tenuously connected state if you
put a lot of edges and everyone's just
really well connected and it doesn't
matter if you add a room of edges that
everyone's still connected and if you
don't put enough edges in no one's
connected so it doesn't really matter if
you there's no carrion effects from
removing these edges so this particular
edge density is the one that actually
produces interesting results you know of
course I haven't flattened the entire
screen with with updated output but you
can certainly do that and I would
actually figure out the right answer for
you you just might be a little
discipline so you know and I think what
we should do now as I come back come
down just a little bit from and this is
very understandable theory by the way so
I think everybody out there's you know
clearly should be clear on this
especially if the big dummy over here is
so what I'll say is this let's move into
a world where or a situation where we
talk about let's apply this to a
real-world problem sure and that will
also help us understand where it's
really useful excellent what look it's
almost as if we had coordinated on this
so the the final if I know a demo we
think that I have for you is I can see
this here a Twitter trace that we have
and we've actually built up a slightly
more interesting more complicated
program off of a Twitter trace in terms
of dickin and we're gonna feeble Twitter
data at this computation in real time
when we try to watch some real-time
analytics sorry in fact it's actually
gonna be 5x real-time so just to show
you that it's actually capable we're
gonna let's see if I can do this
properly
yeah so what it does do it takes it to
make the demo actually interactive we
load up the trace beforehand we don't
actually have Twitter on the line for
handing us data so that does take a
little bit of time and I have to admit
the the demo was written by someone
other than me it's written by Derrick
Maria coworker and it's very possible
that I'm not gonna drive it brilliantly
that's that's fun I think the it'll show
it's good to actually have a concrete
that's the great thing about UI right
it definitely ends up looking pretty
when you when you see it so let me close
this and restart it actually there's two
applications that are going on now one
of them is the actual computation which
is this Nyad program that we've written
that goes and thinks about things like
tweet histograms and there we go so let
me tell you what what all these things
are so the bar at the bottom is a
timeline of about two hours of tweets
that we've we've loaded up fits nicely
on my laptop we have a little slider
across it which is a 30-minute time
period we're gonna start scrubbing the
slider around and watching statistics
change okay what we're showing up here
is the most popular tweets sorry
hashtags mmm in all the tweets so
essentially it's done account basically
by you know the same way that we did
with the word counts then we've kept the
top ten or so I think there's they're
running over there a little bit on the
right hand side we have a graph and
these are the large strongly connected
components in the the graph defined by
mentions and Twitter so if you tweet at
someone we're gonna put an edge in the
graph from you to them and this is
actually the strongly connected
component structure so it's going to be
directed edges going between different
groups here and there's a few fun
stories to tell if I can find them but
I'll let me just start it playing what
you see in the lower left-hand corner is
a little green and yellow light when
it's green diodes doing no work and it's
just hanging out waiting for the next
second to tick and it's ticking along
these are counting seconds as doing five
seconds of Twitter time for every one
second of real time okay just to give
you a sense for the fact that it can
move around and in fact it's updating
all of these counts
call it in real-time second-by-second
updating the strongly connected
component structure which if I were to
show you the code would cause a
substantial amount of anxiety it's a
doubly nested fixed point computation
that has all sorts of basically our tech
demo in terms of algorithms about as
clever as we could get great em and this
I want to say it pauses real soon and
there's a little story about the Giants
but no the Yankees sorry the Yankees of
media the Cardinals here we are so if I
can find them in here there we go so we
have the Yankees and we have also the
cards and apparently around this time in
in the the tweet trace they had a in a
significant game and so there's a whole
bunch of people talking about the
Yankees talking at the Cardinals people
so presumably the Yankees did something
relatively impressive that the Cardinals
people needed to hear about sure but so
this is just an example application I
mean it's very pretty and the prettiness
has nothing to do with Nyad sure but the
ability to throw up sort of real-time
analytics see computations and you had
we just attach this to a Twitter stream
mhm
you'd be seeing what's going on right
now which is potentially really exciting
we have a sort of historical trace we're
not actually social scientists we don't
really know what to look for either were
systems designers and stuff but you can
imagine that lots of people out there
all sorts of people watching at home
will be able to take these tools and do
really cool things with them well
absolutely I mean one of the things we
hear a lot about these days you know
we're in the land of buzzwords is big
data right I mean one of the things you
want to understand about all this Aimee
ambient data is another way look guys
just so much data everywhere so many
edges as you would so many connections
this is the kind of technology that
could be used to help us understand what
the hell's really going on right right
so definitely being able to take these
big piles of data and work with them
interactively right if you have to wait
an hour to get the answers back to your
question yeah it's gonna take you a
while to actually find the right
question right you're gonna have to
iterate for a while whereas if you can
just scrub a little slider around or
tweak the questions you want to ask and
interactively get these sorts of answers
back I think you know we hope that this
is going to lead people to understand
their big piles of data a lot better
do more clever things with them better
services now I mean if you think about
if you look at Facebook which is sort of
the world's biggest marketing experiment
certainly something like this could be
exceptionally useful they're probably
using Hadoop or something like that -
right I know they actually are doing
machine learning actually I understand
what's going on this could be useful in
machine learning
absolutely so you would imagine I mean a
lot of these iterative algorithms for
example this is how machine learning
works is you take you know some property
is some some model that you try to learn
about your data and you iteratively
improve it you sort of go around these
loops and keep trying to improve it
until things stop changing and this is
absolutely critically important for all
sorts of you know Facebook certainly
lots of advertising that they try to do
for example they want to figure out what
are hot trending topics and who wants to
see what based on you know the current
conversations that are going on within
Facebook in principle very exciting the
stuff they've they've told us about the
research community is that they're using
hive and doing the sort of batch jobs
and you know they they do what a lot of
people do which is they try to reduce
the data down a bunch first and then the
best jobs are a little bit more more
efficient because they've distilled
things down to aggregates first but this
makes a really complicated workflow you
got to go re aggregate everything again
and it's it's a bit of a of a nuisance
end ideally you just have one solution
now I'm that's that's the hope at least
we're still of course engineering stuff
but so but the thing that's also
interesting here I mean is that in some
sense this is this is mathematical this
is based on math so the particular
language in tool set you happen to
implement it in is sort of orthogonal
it's this could be implemented in the
tools that they actually want to use
it's the math that matters absolutely I
mean you'd like something that has an
interface that's rich enough to capture
these things so for example you know the
reactor framework is nice because it has
things like on next and subscribe yes
these are things that we'd want to have
and it's less clear that I mean hive has
sort of you know you can update tables I
think you can update tables there's some
things even that you know that in sequel
that you can't do very easily and a lot
of these languages but you really like
to be able to have this sort of
programmatic access to the computation
the whole dataflow C sharp is a nice for
for that you could totally do another
languages I was just saying from the
standpoint of people who are running
you know server infrastructure that
doesn't support dotnet oh no yeah they
don't get they don't either stack or
anything someone we need to actually go
and take the and I had smarts and make
that work on there sure so your night
wasn't beautiful look Lincoln C sharp is
amazing it's an excellent well very
simple to use the the next question I'd
ask I mean I'm sort of wondering if
you've used Rx in your implementation so
we haven't which doesn't just say that
we couldn't we just happen to a lot of
the funny Matthew bits like the multiple
dimensions stuff are very deeply
involved in the scheduler and a lot of
the resource management that goes on and
if we were to use something like I
really need to rip it open and get our
fingers in there and it's a little
tricky when you're doing prototype code
that you're not sure is really gonna
work out in the first place
hmm trying to work around you know well
engineered and sort of bolted down bits
of code can be a little absolutely it's
nice to be able just open it up and say
it's not working let me put my finger on
this and we'll see what happens so great
so who's using this like you guys I
would imagine you have some scientists
already using this to blast through data
sets so that's actually where we're
going next with it so we've gotten it to
a point we just had a code release two
days ago and our goal now is to start to
find people who actually want to use
this in enforce we've been using it for
a lot of little sort of toy projects
like this little tech demos mm-hmm a lot
of people have come to us and said I
really want something like this and
we're trying to make this link now where
we go back to them and say okay we've
got it what what did we do wrong or does
it just work perfectly for you and and
we can iterate at this point on it but
it's a little one of the big differences
I guess between Nyad and drive link is
that the selling point for nayad is this
interactivity so a lot of the scientists
were very familiar with the research
scientist interactivity is fine but it's
not really what they want to produce
some charts for their papers man being
able to press and go on a MapReduce job
or on a dryad and try the link job it's
good enough for them whereas what's
great about night is interactivity for
people who are actually trying to
develop services things that need to
stay alive and interactive this is sort
of the sweet spot for nayad and we're
not sitting the same building as these
people necessarily so all the
researchers we know we love
them but for paper writing and IA may or
may not be the the single best tool is
it's not it's it's selling selling point
that's great so I mean I really kind of
want to talk about that real briefly as
we close here
I mean we've you know in some sense
computation as a service is something
that we don't talk enough about in terms
of cloud computing right sure we it's
it's as a scientist he doesn't want to
you think about people who don't want to
manage their own IT infrastructure I
guarantee you scientists and people who
need to get numbers out of giant amount
of data or are among those and so you
want to send up a request to do some
sort of computation and then be able to
as you mentioned interact with it when
you start getting some of the results
back without having to recalculate the
whole thing again so that you kind of
can help help know where it's going and
see what trajectory you're on
so you mentioned this would be useful
for a service context so you could
imagine this being what I'm talking
about
absolutely yeah no there's a lot of
things I mean we have a lot of stuff in
front of us in terms of what might be
due next we with nayad you know we tried
to build the system the thing that could
actually make it go assuming you could
actually tell it what to do yes and
that's sort of where we are now and
there's a lot of stuff you could totally
imagine building on top of this you know
moving this out to something like as
you're setting up so that multiple
people can actually use the same sub
computations know instead of having
everyone do their own Twitter graph
analysis everyone just taps into the
exact same feed off of the results of
the computation and then they can do
their own thing with it but we've saved
a tremendous amount of work by only
having one instance of this iterative
computation going on awesome and so
it'll take a fair amount of RAM oh yeah
yeah no it's you know that's one of the
things that we're working on engineering
but the main limitation of night at the
moment is that the bigger and bigger
data sets you get the the more memory
you're going to need which is why you
want the more computers it turns out
getting more computers makes it go
faster in some parts of the computation
but these low latency aspects you don't
you don't really need a thousand
computers to make this this millisecond
response time this is because the change
was so small hmm
the memory is definitely exciting and
we'll see how that goes and how that
really ties our hands we're definitely
not targeting at the moment
terabyte plus datasets but but generally
smaller more reduced datasets Eve you've
analyzed your your web corpus you've
actually extracted the hyperlinks you've
you've thrown away the the leaves that
point to nothing in particular and now
you've gotten yourself down to a few
gigs or so of enough data no such thing
but I mean once you once we get we
solved the problem once think of like
cloud ram like endless amounts of memory
I'd be nice that would be a great thing
because this is like one giant state
machine yeah right some super complex
you know brilliant work you guys have
done but in essence that's what it is
right
it's basically keeping a whole bunch of
different indexed relations up in memory
so that when you think that you want to
change one of them you can leap
immediately to that at that point and if
stuffs not in memory that's gonna be
slow steps in memory it's delightful
it's a little weird that you know if
stuffs in memory but it's on in a
different data center or something like
that it's not that's not great so
there's still some limits by absolutely
if someone had a big data center they
said this is going to be the compute
engine for this sort of thing and we'll
share all those resources between
everyone who wants to use it definitely
seems a lot better than forcing everyone
to go out and get you know their own
terabyte of memory or something like
totally well some maybe it'll be like a
cloud Numa kind of thing I mean the
future is very exciting in this sort of
way hey so there's a lot of smart people
on channel 9 you know I'm talking to you
people out there download this they can
see the link right under the player
start playing around with this and give
some feedback and I'm sure Frank would
be happy to engage absolutely no we're
very excited to see what people want to
do with this what sorts of things we've
screwed up we've certainly made some
compromises and we'd love to know that
was a total error or or it's actually
it's really easy to fix and we'll just
fix it then right now I can have what
you want
cool well hey thank you oh no of course
happy to and great work man thanks
Cheers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>