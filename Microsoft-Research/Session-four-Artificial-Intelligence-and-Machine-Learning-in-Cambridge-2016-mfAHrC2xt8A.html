<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Session four - Artificial Intelligence and Machine Learning in Cambridge 2016 | Coder Coacher - Coaching Coders</title><meta content="Session four - Artificial Intelligence and Machine Learning in Cambridge 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Session four - Artificial Intelligence and Machine Learning in Cambridge 2016</b></h2><h5 class="post__date">2016-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mfAHrC2xt8A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Alex Matthews from the University of
Cambridge talking about variational
inference for Gaussian processes okay
can you hear me at the back yes good hi
I'm Alex and Sebastian said I'm going to
talk about what we think is quite a
general framework for 4gp inference so a
bit about me first so i'm actually home
groaners undergraduates i studied
physics over in the Cavendish laboratory
and actually to start with I was mostly
interested in quantum physics but in my
fourth year I took David mkhize
information theory course and that
really kind of changed my path and and
so instead of doing a PhD in quantum
physics I went to study and well went to
work at a start-up which was spun off
from the University of Oxford and while
I was there I I encountered the work of
zubin Guerra Marnie and I became very
interested in it so I came back to
Cambridge to do a PhD with zubin which
has been absolutely awesome and and I'm
just just finishing that PhD okay so I'd
like to talk about my co-authors they
are James hensman ruat c/o Philippine a
Richard Turner and my supervisors you've
been guerra money it would be difficult
in the available time to do justice to
all the excellent work that's been done
on this topic but I wanted just to
mention a few people named Lee McCulloch
stood CS Edwards Nelson and Matthias
Seager okay so what can you do with the
Gaussian process well the GP area is
really kind of a thriving area of
research and that's partly because you
can do so many different things with
them so you can do spatial statistics
there you model the log rate of some
disease using a Gaussian process you can
stack them and make a deep GP and tan
gave us a very nice talk about that this
morning
you can do Bayesian optimization where
your model for the unknown function is a
Gaussian process and for instance in
sorry in this paper they were using
Bayesian optimization to learn a
controller for a robot and that made the
cover of nature you can use it inside an
automated statistician and led by zubin
there's a lot of work going on in that
direction in our laboratory and of
course Carl gave an excellent talk which
is really quite related to what I'm
going to talk about this morning a where
he talked about state space models and a
variational approach to that problem ok
so you get these these challenges that
crop up in in GP and prints all over the
place and there's two particular ones
i'm going to talk about because i think
they're the tricky ones so in general GP
regression requires a worst-case order n
cubed computation where n is the number
of data points the second problem is
that if your likelihood isn't a Gaussian
likelihood then the posterior inference
isn't analytically tractable and you
need to do an approximation ok so we've
been thinking about this problem for
quite a long time now and we now have
these free papers on the topic and of
course I'm not gonna be able to describe
everything that's in those in this time
but we also have this this software
package GP flow which is a Gaussian
process library built on tensorflow and
that really implements the work in those
three papers so what I have got time to
do is talk a little bit about the
general approach that features it has
and maybe illustrate it with some simple
examples so the features that I draw
your attention to before I get into the
detail are that we face the order n
cubed problem using an adducing point
method which turns the computation into
order n m squared where m is the number
of inducing points the approximation we
use has the characteristic that it gets
closer to the posterior as M increases
and we'll see in what sense I mean that
as we go on we can deal with non
conjugacy in the same framework and it's
all integrated and we can go
even more scalable than a 10 m squared
using stochastic gradient methods and
were able to achieve all of these first
four things in a way that will converge
so the glue that holds this framework
together is variation inference a
variational inference we have an
intractable distribution P tilde which
is approximated with the distribution Q
in some variational family big Q we
measure the similarity between those two
distributions using the KL divergence
and that has some lots of useful
properties their most important of which
is that it's positive and it's 0 if and
only if Q equals P tilde and what we're
going to do is going to choose Q hat
which is the the Q distribution that
minimizes the KL divergence okay so
let's let's take a simple example I've
got some data here you can see that
there is some pattern in the data but
there's also some noise okay and because
we're being Bayesian we're going to put
a prior on the function that map's the
inputs to the outputs and so how do I
actually generate from this prior while
I'm going to need some notation I've got
big X which is the set of all possible
input points and I'm going to partition
this X into three sets I should say that
at the moment I'm assuming X finite just
for the simplicity of exposition and
I'll talk about the more difficult
infinite case in a minute the partition
that we have is into three sets Z which
are the inducing inputs d which are the
data inputs and star which is everything
else in the index set and instead of
sampling in a big block that FX we're
going to break it up and we're going to
sample it sequentially using the chain
rule probability so let's illustrate
that so here I've got three possible
samples from the latent function values
at the inducing point inputs I'm going
to choose one and then conditional on
that I'm going to to draw samples for
the latent function corresponding to the
data inputs and there's three possible
draws
and we're going to choose one and then
this has gets a bit busy this plot but
i'm going to sample everything else the
starred set conditional on the data
points and the inducing points and you
can see by now that in many places it's
really quite well specified the function
but further away from the points that
i've already sorry that i've already
seen we have some uncertainty still and
if we take a sample then now we've
really kind of pinned down this function
ok so back to the data with the data is
associated to likelihood and that fact
arises and it's a simple application of
Bayes theorem to get the posterior and
what's the posterior look like well here
we here we have a posterior ok ok in
this simple cases it's analytically
tractable to compute the posterior but
for larger data sets we're going to need
to approximate and to do that we're
going to use a cue distribution but his
this form and what I want you to notice
is that it looks like the prior except
we've replaced p of f of z with q of FZ
and we're going to vary q of F of Z so
substituting that expression for Q into
the KL divergence gives us a long
expression I'm sorry for all the
equations but what I want you to notice
is that the blue terms will cancel and
the red terms will cancel and that's
what leads you to a simple expression
these first two terms are going to be
tractable but this third term is this
log of the marginal likelihood that in
general will be intractable but it's
only a constant so it's not going to
affect optimization okay so if I
optimizing my queue of F of Z in the
Gaussian family I get a pretty terrible
answer the green is the ground truth and
the blue is the approximation so what
can I do well actually it turns out that
i'm allowed to optimize the inducing
input said as well and that they are
variational parameters in the sense that
they will reduce KL divergence and
they're protected from overfitting
actually this was a sticking point in
the literature there was some debate
initially as to whether you could do
this and it would be variation
but but the way to see it in the way
I've shown you is that effectively the
zed is only really a label ok so the
underlying set big X stays the same even
if you swap your definition of Zed and
if I do that then I get a much better
approximation the blue and the green
line up much better and you'll notice
that the inducing inputs have moved
towards the data but this won't be the
case in general i'll show you some
examples where the inducing points don't
move towards the data ok so i should
talk about this tricky infinite index
set case that i mentioned just because i
don't want to mislead you that this is
all super simple I'm going to sketch it
for the benefit of the people who have
some background in the necessary
mathematics namely measure theory but
don't worry too much if you don't
understand this so we're going to have
three measures p p hat and Q and they
correspond to the prior posterior
approximating distribution we're going
to assume a dominating relation between
them now the first problem we have of
the infinite index set is that the
normal Bayes theorem won't work for you
so you need a generalization of the
Bayes theorem which is what you have
here the second problem you have is that
the KL divergence that you would
normally use won't work in this case and
there we need a different definition of
the KL divergence okay so I've we've
done that laborious work for you and
we've followed it through and we found
that it all comes out ok you get a very
similar answer to what you would would
if she'd done it with a finite index set
but I wouldn't assume that that was
always going to be the case it works ok
here though so you can also do
classification in this framework so what
I've got here is the orange points
correspond to one class the blue points
to another the black points correspond
to inducing inputs and the black lines
correspond to decision boundaries and
going left to right we're increasing the
number of inducing input points and the
top row is what we're suggesting and the
bottom row is an established method
which is the generalized fitzy
approximation
and what we would claim is that for
smaller number of inducing points you
get a more sensible answer so looking
for or 16 but you can see that actually
they both do okay in this 64 case also
in the 64 case for our method look at
where the inducing points are there near
the decision boundary so in this case
they're not just moving towards the data
points okay the other thing that that is
really nice about this framework is that
we can easily do mini-batch inference by
taking a mini backs approximation of
this sum over data points term and if we
do that we can go after really very
large data sets so here I have six
million training data points and the
task is to predict whether or not a
North American Airlines going to be late
based on some reasonable characteristics
of the flight the red line shows what
you get from a linear classifier which
will run very quickly for you and the
blue line or the blue lines show what
happens in our case and the take home
from this with the time available is is
that takes about 15 minutes for the
stochastic gradient to get to a point
where you're outperforming the fast
linear classifier I would argue that's
not too long to wait okay so we can also
do the famous em nice digit
classification data set and for the
removal of suspense we're not state of
the art of this and we're not claiming
that so you can see that the sort of
various deep learning methods come in
very low on em this effectively it's
it's kind of salt for deep learning and
we come we're sort of somewhere in the
middle we outperform some some
reasonable methods um but our aim wasn't
to get state of the result arts on this
source excuse me our aim wasn't to get
state of the art results on this our aim
really is kind of to show that you can
get sensible results using the scaling
approximation because of course 60,000
cubed is rather large and we can do
things like double the number of
inducing points and we still get about
the same result so we think this is a
pretty accurate portrayal of what a GP
will do on here also interesting from a
scientific perspective is what sort of
inducing points you get so
the inducing inputs in this case lie in
the image space so they're inducing
images and here we have a selection of
them but if we zoom in a bit to look at
some of the particular inducing points
so we initialize using k-means which
will put the inducing points near the
data input clusters but again in this
case we find that the inducing points
move away from the input clusters to the
decision boundaries so these three
digits started in the middle of the
sixes regime and in this case it became
65 hybrid in this case is 64 hybrid and
in this case are more extreme six and
you can see the difference here if that
helps okay so that's a whistle-stop tour
of this framework for approximate
calcium process inference I'd like to
just now you've seen it reiterate
features that it has we take the order n
cubed complexity to order n m squared
using an inducing point method the
approximation gets closer the posterior
as M increases and in fact not all
methods have that property the non
conjugacy is variation as well so that's
part of the same framework and we can
easily scale it using stochastic
gradient methods and we can do all of
this in a way that's convergent if
you're interested in what you've heard I
would suggest having a look into these
papers in some more detail and also our
code is publicly available as GP flow a
Gaussian process library built on
tensorflow thank you very much that's
that's all I have and I'd be happy to
take your questions
ah save it so can we now do deep GPS
intensive flow as well um it's on the
way yes it's not it's not in the public
version but we've been playing around
with it yeah using bizarre endurable
somehow or like that they has on me even
look you said about the movie away from
the games okay so I what I wouldn't jump
to is a kind of an interpreter pretation
in terms of what you see in deep
learning papers I think I think these
are different I think since they're
traveling in the classification case
towards the decision boundary you might
think of them as being a bit more like
support vectors as in the case of a
support vector machine but actually we
have much fewer inducing points than you
would have support vectors in this case
so we're quite fast at test time Brett
shoes were on the youngest how did they
speed up for vector issues what was the
method that was used to to get that move
it over social photographer head we did
repeat it though I guess maybe work in
the dual representation yeah but i think
it was off the top of my head tens of
thousands of support vectors and we had
about 500 inducing points so
parallelizable is a method I mean
obviously can compute the individual
quantities in the mini batch in parallel
but then the updates of the mini after a
mini batch would be still sequential
always has some intrinsic feature and
your message that you can exploit to oh
yeah safely parallel computation it's a
great question the answer is very
paralyzer ball so we've we've talked
about doing this sum over data terms
using a mini batch but of course you
could distribute that sum over many
different workers and that's one of the
reasons that we're pushing in the kind
of the the 10 to flow direction is that
we can exploit some of the computational
power better
and tends to play we'll do some of that
for us under the hood of course as with
all of these things you have the
synchronization issue that you have to
to update the gradients to be exact in
any case at every iteration and people
go down the asynchronous route of course
if there are no more further questions
then let's sync Alex again so now we
have the startup talks the first one is
fair in Susa originally from University
of Cambridge but now was a startup
called magic pony technologies and he'll
be talking about generative models for
image processing
yeah does it work now yes do I have to
switch this too thanks so hi my name is
Franz I'm at this company called magic
pony technology i'm not going to explain
why this is our name now but i'm going
to show you a little demo video which
explains what we are working on and what
what our company does hopefully i can
show this in so what we care about is
using the latest machine learning
techniques to enhance the quality of
visual data so what you see here on the
left is a compressed video stream in
this case of somebody playing a video
game and this is what you would normally
see online if you have limited bandwidth
or limited availability to of high
quality data and under on the right of
this line you see what we get when we
apply our real time machine learning
optimized quality enhancer so I'm going
to talk about this just a little bit
that this is essentially a convolutional
neural net of some sort but the key
point is that this runs in real time on
various devices including mobile devices
so now you can or we can put this
technology on your mobile device into
your mobile lab and enhance the quality
of the visual experience people have
with particular data so this is showing
an enhancement where you increase the
quality of compressed content but you
can imagine this working in other kind
of scenarios you know making
computational photography kind of
applications or making it high dynamic
range or of scaling it to 4k so
generally we we care a lot about low
level vision
which is the kind of computer vision
where you don't care about high-level
concepts so much like in classification
but you care about things like pixels
and generating content that looks good
and so if this works the method that you
just show on the video is actually
produced by this this method that we are
just about to publish in cvpr and and
this is the the main graph so basically
shows that these are all the other
methods that are known for machine
learning based video super resolution so
when you take low resolution video and
turn it into into high res and this is
ours is both faster and more accurate
than than all the others basically so
this is the kind of plot that people
like to have I guess but I'm not going
to talk about this version I'm going to
talk about where we are going next
particularly in machine learning
research and this is a very high level
overview of the three things that
determine the success of any machine
learning application is roughly the data
the quantity and quality of it the model
that you have bayesian people tend to
care a lot about this or spend most of
their time here and then a third one
which is the objective function which
encapsulates also the way you train this
model or you wait you fit your model to
the data and what I want to argue is
that for most of the low-level vision
applications that we work on actually
that the first two things are relatively
solved the models that we have
convolutional neural nets are MRFs are
roughly a good model for the kinds of
things we want to do with them the data
is plenty and high quality so there is
no problem with that but we really have
a problem in understanding what the
objective function should be you know
what does it mean for content to be high
quality and this is an example which is
work bye bye bye Lucas who's sitting
somewhere in the audience this is the
original high resolution content this is
a picture of Obama of course and and
then here we have the low resolution
content upscale using the
this sort of baseline bicubic
interpolation so you can see that
obviously a lot of the details are are
missing and if you use the feed forward
approach that I showed you before then
you get a lot less blurry results so
this is I don't know if you can see it
on the projector but it's supposed to be
so I'm supposed to look nicer but it's
also missing some of the some of the
features and some of the uncertainties
and some of the textures particularly
from the image and if you use a
different way and this is because this
this method was trained to minimize mean
square error on a pixel level which is
what most machine learning people end up
doing and it discourages the network to
learn complicated complicated patterns
because it might incur a large mean
squared error versus an algorithm like
this which is based on something called
neural transfer approaches will produce
much nicer results perceptually but it
might incur a lot higher mean squared
error so if you just minimize the mean
squared error this solution which
actually looks much better would be
penalized and you can see that it's not
perfect because it also introduces some
artifacts and extras but on the whole
the I for example looks much more
convincing on this one dirham on this
one so generally speaking we are
motivated to find new ways of training
these image enhancement low-level
low-level vision models and I'm going to
talk about generative models and
particularly within this perspective of
perceptual quality and producing
nice-looking samples so what are
generative models roughly their models
that you can sample data points from
sample data from that subs thats that
has in some sense similar qualities to
the observed data they have a wide range
of applications I think in this audience
I don't have to list them all some
people use them for unsupervised feature
learning and semi-supervised learning
but we are more interested in the
applications in denoising
super-resolution things recently there's
a lot of renewed interest in using
neural networks
as generative models so we've seen
examples of variation Lhota encoders
mentioned today there's also generative
adversarial networks which I'm going to
talk about and then genitive moment
matching networks which there was work
from CBL from zubin and colleagues on
this also recently and I would argue
that most of this success is driven by
the deep convolutional neural networks
ability to represent visual information
information you know pretty much much
better than anything else that we know
today so just a little bit of notation
I'm going to use these all the time not
particularly consistently X is typically
means an image or a data point Q is a
generative model so this is a
probability distribution P is the real
data distribution he just means
expectation KR just means car back
library divergence i noticed that now
making these curly is the new transfer
my switch to that and then Shannon's
entropy is just H the key question about
generative models particularly
interesting when you use neural networks
where you know none of the likelihoods
or the normalization constants are
typically available is how do you
evaluate and train them and the key
point that I always emphasize is that
the way you train these new tease these
genetic models should reflect the way
you want to use them and in our
applications we can assume that we want
to actually show these samples that the
model generates to a person and so our
objective function could look something
like this we sample data from these
genetic model and then we show it a
person and the person evaluates how much
they like this sample and gives it a
score s so this is called an average
perceptual quality but if you only had
this as an objective function then it
the model would basically just always
produce the nicest-looking example and
it wouldn't have any diversity so you
might think about adding something like
the Shannons entropy of Q which then
encourages diversity
the sandals so that's what I'm going to
be working with for this scenario when
when we we care about producing samples
that people like so how do we think
about s s is called in the literature no
preference or blind perceptual quality
score to model this week rucha Lee need
to understand human perception because
it describes how human perceives errors
that we make the assumption is that the
human perceptual system learns natural
image statistics so we are going to just
assume that this s is well modeled as
the log of the real data distribution so
the more likely an image is to be
sampled in the real world the more a
person will like it so if you put this
back here then suddenly what we end up
with with is the scale divergence which
is the objective function we want to
minimize and notice that this is
different from this KL divergence which
is what people people typically use when
they fit a model using maximum
likelihood and I think this is also not
surprising to anyone that these sort of
known differences between the different
directions of KL divergence if this gray
thing is your real data and you want to
fit a Gaussian model to it this is what
you get if you minimize this KL
divergence so maximum likelihood it
tends to overestimate the probability
mass returns to produce samples that
have no that would never occur in nature
whereas this KL divergence is very
conservative and it will only sample
data from maybe one of the mode so it
underestimates the variance but this
means that it would never sample
anything that a person would find
unrealistic so this more conservative KL
divergence would be good for us to
minimize it's also very tricky to
minimize this because in this case we
don't have any analytic handle on P we
only have samples from B and all and
possibly only samples from Q and one way
to do this minimization is this recent
method called generative adversarial
networks and
bye and good fellow and others so this
has two components we have a generator
network which is typically a neural
network that defines skewed the
generative process by first drawing some
random noisy and then squashing it
through a neural network to produce an
image and then it also has a
discriminator network d which takes an
image as an input and outputs a
probability and it's supposed to tell
apart real samples from the real data
distribution from fay examples generated
by our generative model and then the
algorithm is iterative ella in a high
level in the d step which which is what
i call this we fix the generative model
and we try to optimize discriminator to
do its job well and in the juice that we
fix the D we fix the discriminator and
then try to adjust the generative model
in such a way that the discriminator has
a harder time classifying between fakes
and reels now it turns out there are
multiple different variants of this
algorithm so this is the originally what
they derived from theory and then they
introduced the second objective function
which is what they eventually end up
using you can also come up with a
another alternative step the point of
then I'm not aware of anyone actually
talking about this basically you can
show that if you use algorithm one then
you end up minimizing the Jensen Shannon
divergence between Q and P if you use
algorithm to then we don't really know
what this minimizes or if it minimizes
anything but it does work in practice
and it turns out if you use algorithm
three then you you end up minimizing k
RQ p which is exactly what you wanted to
minimize so we can use generative
adversarial networks to to minimize the
clear divergence and it does produce
pretty nice sample so these are some
among the characters and flowers faces
even more faces the problem with them is
that they are really hard to train so
neural networks were you know notorious
for having so many hyper
meters to optimize now imagine that you
have two different neural networks that
you have to optimize in tandem so so
they're not exactly easy to get working
the summary the pros are they have nice
theory in the sense that you can get
them to minimize scale or John sunshine
and shown on divergence it exploits
convolution on your own networks which i
think is a key to their success it
produces nice samples empirically and it
only needs to sample from Q to do this
algorithm you don't actually need to
evaluate anything about Q only being
able to sample from the process is
enough the cons are they are very
unstable it minimizes a lower bound so
that's not good we want to minimize an
upper bound or maximize a lower bound
but minimizing a lower bound is it is
not good noticing there is no single
underlying objective function that the
two steps minimize their would mean for
example an algorithm like yam in this
objective function for in this algorithm
in once that you try to maximize
something and in the other step you try
to minimize the same thing so it's a
little bit of a disconnect and I'm going
to talk about this property later so
what we seek an algorithm that retains
the nice properties but is more stable
or has better properties otherwise and
this is what i call the DG GM that the
noise or guided generative models and
this is work in progress so not very
conclusive yet so again our goal is to
minimize this KL divergence and let's
review in this light what the generative
adversarial networks do they directly
estimate this log probability ratio from
data and this is not new there is
already literature on this in domain
adaptation you also want to estimate
these kind of probability ratios
directly from data so I'm just putting
some references in here it's called
direct importance estimation or direct
probably ratio estimation and training a
classifier to classify between fake and
real images is one of the ways you could
go about doing this but there are other
ways you can use kernel methods you can
use
sort of other ways to to estimate this
this logarithm the problem with the
classifier approach is that although
convolutional neural nets classify
images very well they are known to not
represent the derivatives of the
decision function very well so this is
an example that many of you might have
seen this is an image that the commnet
says is a panda you add a little bit of
noise to it and then you get an image
which to you still looks like a panda
but suddenly thinks it's a given and to
me this says that basically the neural
network learns a function that's not
very stable outside where it actually
sees any data points so if you want to
use the commnet to estimate gradients it
maybe not the best idea and this might
explain I think some of the unstable
unstable behavior that we see in get
generative adversarial networks so what
is the algorithm that I'm proposing
instead we still want to minimize this
and the first idea is that we we don't
actually want to estimate these log
probability ratios we only care about
their gradients to be able to do
optimization but how can we obtain a
direct estimate but first of all i'm
going to rewrite this in this form so
you have the lock you the derivative of
lock you and the derivative of log P X
this is called score in statistics and
also in machine learning and score
matching the second idea that this core
these log probabilities the
differentiated log probabilities can be
estimated directly using a denoising
auto encoder this is a known property of
of denoising and the key idea is that if
you consider the Bayes optimal denoising
function what is the ultimate best
de-noising that you can do on images it
will turn out that the the the optimal
solution will be this form if you try to
do noise an image X the output is X plus
a constant times exactly this gradient
that you care about so the key idea here
is that we can instead of training a
classifier to classify between p.m.
q we can train denoising auto encoders
that will give us estimate of these
gradients that we are after so this is
the dgge em algorithm in summary we
start out by training a denoising go to
encoder or any kind of d noise or
function it can be a gaussian process i
don't care at this point as long as it
works well it can even be a heuristic
algorithm as long as it's trained on
examples from the real data distribution
and this denoising function will
implicitly capture something about the
data generating distribution then we
repeat the following steps onto your
convergence in the D step which is not
discrimination stuff anymore it's now
denoising step we train a least square
the noise around cutie so we have our
current generative model cutie and we
trained a denoising rotary encoder to
denoise example sample from our
generative process this is what I call
the noise cutie this is a function and
then in the G step I update my genetic
model parameters using these estimated
gradients and this is what this
algorithm will end up looking like which
i think is pretty nice to look at so
geez out of theta is one of my samples
that are generated from the model
essentially the noise are that strained
on P will try to make this sample more
look more like samples from P and this
denoise are trained on Q will try to
make this sample look more like you and
because of the negative sign what this
term ends up doing it will try to push
the samples apart from each other so
this term is trying to increase the
entropy of my generative model whereas
this term is trying to increase the
perceptual quality of them so this is an
example of such a method in training
here at my general generative model is a
multi-layer perceptron Z which is the
input to this multi-layer perception is
a 2d Gaussian the D noises that I use in
the algorithm are also multi-layer
perceptrons this is very simple because
it's just a two-dimensional
set the optimal for the optimization i
used the atom algorithm which is a
variant of stochastic gradient descent
so you can see if i can wait another
cycle is that the algorithm starts out
by producing a lot of samples in the
middle and the first thing it does is it
blows up the the entropy if you look for
the next cycle to start it'll blow up
the entropy of the generative model
until the variance roughly reaches the
variance of my data and after that point
it will start focusing on you know
getting the getting the details of the
distribution right so this seems to work
at least on this toy example here's
another example these are patches of
grass image because i live in england
and this is also the training process so
as the training convergence converges
these fake samples look more and more
like grass texture this is again
work-in-progress it's not really using
the the full algorithm that i just
showed you but it's just to give you an
idea that the noises can also capture
some of these statistical properties we
care about another example with this
candy texture again it doesn't really it
gets the idea that there should be
colorful things but it doesn't actually
converge to really nice results I hope
that eventually it will so if I still
have time for a summary the key
takeaways would be that training a
generative model should reflect how we
want to use it if your goal is to then
use the representations in a supervised
learning setting then some high or
objective function should capture that
if you want to use a generative models
to sample from them and show it to
people look on nice my samples are then
this is what your objective function
should consider and so here we care
about a perceptual quality of the
samples therefore we we kind of minimize
this more conservative KL QP rather than
doing something like maximum like good
estimation generative adversarial
networks I think produce nice samples
partly because of this connection
as opposed to something like variational
auto encoder which tries to do maximum
likelihood so the question that I asked
is can we come up with a more stable up
algorithm for minimizing k RQ p the
answer is I don't know yet because this
thing isn't particularly stable at the
moment but the two ideas that I used is
instead of estimating the log
probability ratios as the gun thing does
I am going to estimate the gradients
directly to reduce some of the noise and
the second idea is that for to do that i
can use de-noising go to encoders or in
fact any kind of denoising algorithm
trained on these on these distributions
and I think that the actual algorithm is
quite takes quite a nice form in the
sense that you know you basically use D
noises to nudge the data points towards
high probability regions and away from
regions where you already generated a
lot of samples that's it so we have time
for questions I actually have a question
so I mean it's absolutely exciting to
see these samples and it was a very nice
talk and I just don't understand these
methods you can just you can sample from
them but how would you use them for
example in the compression video that
you showed in the beginning how could
you leverage these models for the actual
applications because essentially it sort
of ties your hands you can only sample
from them so the answer is that I'm not
necessarily going to use these the
generative models themselves but what I
am using is the idea of how to train a
generative model and that that idea can
transfer over to how do you train
something else like you know a model
that does denoising again which would be
an interesting thing because i have
denoising the algorithm itself so that's
the general idea is that yes these
genetic models cannot provide me with
with with good probability probability
estimates
but I can hopefully use the same ideas
if I can come up with a generative model
training procedure that produces nice
samples then i can probably also come up
with other techniques that solve those
other problems using the same objective
functions are the same ideas bridge the
log p of data approximation to the the
perceptual quality seems a little bit
problematic because even in a typical
set of images that we observe in the
everyday environment that log p is going
to vary a lot but i don't think that the
sexual quality of microsoft is
necessarily more naturalistic than the
mountains that the meant to be a
negative see very often some very sloppy
varies a lot for those two systems and
and even if you do choose a lot of p
still not clear to me that you don't
want some ivan parameter like she
trade-off between log p and the ND and
sort of the correspondence to caries or
maybe you could think about a monotonic
transformation of lumpy indeed so could
you just explore a little bit about the
sanity of that and now yeah so i haven't
doing that you don't yes so i think that
the motivation that that this is some
this does something like a perceptual
quality metric i think is relatively
weak i think it's a nice story when you
go through it but obviously it breaks
down because of exactly these kind of
reasons also why do you use this Jen on
entropy i just happen to use a Shannon
entropy because I know that then my math
works out so in that sense it's not
particularly strong but that said if you
actually look at the properties of KL QP
it still makes sense to minimize that as
opposed to K up uq because the thing
that you absolutely want to avoid is
producing visual content that looks very
very unrealistic so and this is what the
k RQ p is 0 avoiding property will will
basically say so yeah i think that's a
question of saying you are right it's a
it's a week motivation but actually you
know you can motivate it by just looking
at look this is
the maximum likely it would do it would
tend to produce samples that are
unrealistic if there is model
misspecification we don't want that if
you minimize scare the other way around
we might be able to avoid that and
that's the that's basically the I think
maybe a stronger motivation than than
the perceptual to be so if you don't
want to produce images that are
unrealistic then I guess a limiting case
of overfitting there is just to produce
exactly images that you've seen before
yeah you know you might not be able to
produce Obama's face but you just put in
somebody else's face there yeah I think
that's a very good point so they don't
do you think that if you massively over
fit this somehow that could actually yes
and I think I think that the the
community doesn't have a good answer for
how do you characterize overfitting in
generative models yes so yes these
models because particular if you use
very very large neural networks they
could just memorize these things in
there in there in there parameters so
one thing that people say in response to
that pays look we only only made one
pass of the sarcastic gradient algorithm
and there is no way that you know the
algorithm only saw each image just once
there's no way it could have memorized
it so that's like a cheap way of trying
to argue against it I think that there
is similar to coming up in an objective
function to how to train these methods
there's also very interesting research
direction in how do you characterize
overfitting and there's some recent work
from from Arthur Breton on three sample
tests which i think is a overfitting a
little bit like the generalization
cross-validation but for genetic models
and not supervisor it's all that problem
for us out of me that'll be pretty okay
let's sync again for a wonderful talk
the final session of this watch at the
five o'clock of the final session of
this workshop is again from sada or I
think by now a well-established company
cambridge-based called audio analytic
and the speaker is Sasha a crystal ah
bitch and we will hear talk about sound
recognition yes I'm such a cursory VP of
Technology at odeon ality thank you very
much for inviting us to this machine
learning and artificial intelligence in
Cambridge we've seen is the end of the
day we've seen lots of very very
interesting talks and I is about applied
mathematics this dog is going to be a
lot more on the applied side under the
mathematics I'd I hope you're going to
find in the entertaining and hopefully
interesting as well
so just a quick quick outline of the
talk we're just going to tell you a bit
of the about the context of what we're
doing which is a automatic environmental
cell recognition we're going to focus on
one particular aspect of it in his top
which is performance versus
computational cost we're going to tell
you a bit about the experiments we did
the kind of data sets were using the
evaluation metrics we've used for the
study and then results and conclusions I
have to point out this work was a
collaboration between audio analytic and
and queen mary university and university
of surrey so from the side of university
of sorry that was with mark blum leg was
here today with us and then Queen Mary
there was a sedan cetera who is doing
his PhD on this topic and then Adam
Stark who was a postdoc at queen mary at
any time so all you are a key part isn't
that weird that we're doing with this is
our is selling and we actually
commercializing automatic environmental
sound recognition for the smart home so
that means so you're familiar with
speech recognition and with music
recognition we recognize every other
sound and speech and music that's also
known as the audio even detection what
we take the labels such as a baby crying
or or smoke alarm going off in your home
when you're not at home or glass breaks
we also have other sounds such as
aggression car alarms gunshots keywords
and many other things so again this is
focused on the smart home the
application is you're not at home
something happens the system hears it
and it's able to send you an alert about
what happened when you were not here to
to hear it so how is that different from
a machine learning perspective or
classification perspective how is that
different from speech and music well in
the case of environmental sound the data
said that you're trying to separate
segregate recognize is not as bounded as
speech so if sounds were vegetable
that's what would happen to your system
you're throwing all sorts of shapes and
types and so on
the acoustic space or what what can
happen you know home is very very rich
you have some beeps you have some pie
linguistic speech such as people
shouting or babies crying you have
crushes knocks which are sons which were
so far pretty understudied you have
broadband noises such as your vacuum
cleaner your icon and so on so it's a
very very rich feature space that that
you're trying to analyze and recognize
automatically there is no language model
yet in the domain of the environmental
sounds I say yet because they are we
have ideas about calculating the
co-occurrence of certain sounds in
certain scenes but so far the data sets
that are available for that kind of
studies are not up to the task so in
theory it's possible but in practice no
one so far has a language model although
we can have some when we do testing for
example in homes we know that certain
sounds are going to happen some other
sons are not going to happen but we
don't have explicit models for that yet
at the fundamental level there is also
another interesting thing is that there
is a quasi infinite priors on false
positive so your system is going to get
exposed to a lot more other sounds than
the target sound that you're trying to
recognize so if you're making a carrot
recognizer then your system is going to
be exposed to two carrots vs lots of
bell peppers and all sorts of other
sounds so there we go that's that's a
very specific problem to this field
we're in speech you usually have the
sentence which is triggered the
recording of the sentence is being
figure either by your phone service you
leave a voicemail all by your keyword
like a hello Alex our hello katana or
whatever is going to trigger the
recording of speech so in our case you
get everything in the microphone all the
time then there are also problems of far
field capture and channel FX which are
similar to the things which are faced in
the domain of speech these require so on
the signal processing side they require
digital signal processing and
equalization capabilities but on the
machine learning side you also need a
model which has good generalization
capabilities so what is the question
that we're going to focus on in this
party
talk so for us industrially we are we
need to use the the algorithms which are
going to cost the less in terms of
computational power because we need our
systems to run on embedded devices and
so lots of people tell us well why don't
you just use a cloud-based system you
ship your audio to the club and so on
the answer to that is very simple is
that because people mind the privacy so
it's better to have a recognition system
running directly on your device on your
home camera or your home hub or whatever
the device is going to be than to beam
the whole audio of your house to a
server so for us is very very important
to be able to run on embedded device so
we need to evaluate algorithms on one of
classical axis which is the
classification error we need algorithms
to be good at classifying sounds but we
also need them to be good at classifying
sounds at a small computational cost so
when we evaluate algorithms we need to
have that second axis of computational
cost which extends from very small stuff
this is a hard we know processor to cry
if the algorithm is going to need a
creator run we just simply can't use it
so we have those two axes the best
algorithms are the ones which are doing
a good job of classifying sounds at a
low computational cost the worst
algorithms are the ones which are which
needed too big a computer to run and
don't deliver very well so we want to be
as close to the origin as possible and
that kind of graph now in terms of
comparing machine learning algorithms we
comparing the usual suspects which are
Gaussian mixture models which you're all
familiar with support vector machines
and deep feed-forward neural networks we
also give a shot at recursive neural
networks because we're dealing with time
series so using recursive neural nets
was also made sense in that in that
particular case so we did this
experiment on to target classes but we
used three datasets that I'm going to
tell you why so first dataset was baby
cried so it was a simple problem of baby
cry detection swear we recorded tensile
babies
we had to recording conditions hospital
and outdoors the those conditions were
spread evenly in both the training and
test set we had also a variety of
recording devices so a bit of channel
effect in the mix and we had so data
sets which are pretty big compared to
whatever was available state of the art
before this experiments we have about
200,000 data points for the training set
and 90,000 a tapas fortress test set
second experiment was smoke alarms where
we had ten different models of smoke
around for this particular experiment
that's a subset of our data set those
who are recording in three homes and two
homes were used for the training set one
home in the test set so they divide here
is more environmental divide those were
recorded through an array of consumer
electronics cameras so we had up to 10
channels depending on the places where
we recorded we had this bolo how all
these consumer electronics cameras and
we call it through all of them and they
can sound very different and so the
training set again was pretty big
compared to set of the art sets for
their experiments about again two
hundred thousand frames for training and
one hundred thousand frames from for
testing now just to wake you up a bit
what do you think the third set was any
idea anyone okay we call it the world
set which is a completely unconstrained
variety of sounds and devices and this
potentially covers thousands of classes
so very often people misunderstood a car
experiment telling us while you're only
dealing with two classes while we're not
only dealing with two classes we're
dealing with two classes against
thousands of other sound classes which
are being thrown at the system and so
the world set participates both into the
training and testing because we're
training rejection models as well on the
on the world set on the case in the case
of gocha's you're going to have what's
called a background model which is
trained on the world set and then a
foreground model which is trained on
your target class in the case of neural
networks you're going to have different
output depending on what you're in if
you're inputting one of the target
classes or one of the world samples
and for support vector machines you're
drawing the margin between the world and
your target class the century okay right
so I said this is a subset of our data
set so you can imagine the fun in in
recording data we actually have hundreds
of smoke alarms that we're dealing with
we bought every single smoke alarm
available in the UK and in the u.s.
there are interesting anecdotes about
that in terms of some smoke alarms have
an electronic component with a small
amount of radioactive material in them
so we had to find ways to ship
radioactive material from the u.s. to
the UK there we go so again in terms of
doing applied research this is the kind
of fun we're having doing products
really based on on machine learning
right so getting back to the evaluation
and these axis of computational cost and
classification error so to evaluate the
computational cost we simply counted the
number of operations involved into each
of these models so that's the gmm and
then support vector machines with virus
scan of kernels and then deep neural
networks with various I mean with either
sigmoid activation functions or
rectified linear or recursive neural
networks and this is simply the number
of additions multiplications and
nonlinearities that that you're going to
have into these models knowing that
models with less nonlinearities are more
desirable because non-linearity is
depending how you implement them can be
costly so either you implement them with
Tyler series in which case there would
be more than come for more than one
operation or you implement them with the
lookup table which can be approximated
can be counted as just one look up
operation but cost you a bit more in
terms of memories so in in terms of the
cost of nonlinearities we went the route
of considering one on arity is one look
at table lookup but generally speaking
you want the algorithm which is going to
have the least non-linearity as well
when it comes to some detection
performance metrics so your error rate
access the
is one thing which is important to
understand when it comes to detection
systems is the notion of trade-off
between the false positives and the Miss
detections so the way these systems
works is all of them compute some kind
of similarity scores so in the case of
GM ms that's your likelihood ratio
between the target model and the world
model in the case of SVM's you can use
the deviation from the margin as a
similarity measure and for the deep
neural networks and recursive neural
networks we had a single output which is
the class membership probability which
oscillator depending whether it's a
target son or not so you have I don't
know how to treat you later anyway you
have this kind of score the blue part is
in a smoke alarm you have a the scores
going down so it's similar to a smoke
alarm the red part of the scores is the
part where you're outside of your smoke
alarm and scores have a different
density so the graph on the right is the
red skies densities of scores which are
outside of your target labels and blue
line is the blue density is the density
of scores which are within your target
labels in that case smoke alarms so what
you can see here is that the overlap a
little bit so depending when you're
going to place your threshold which is
that green dashed line you're going to
either have a system which is more
conservative is going to reject more
sons in general so it's going to miss
more of your smoke alarm or if you put
push your threshold up in that case
you're going to let more sounds go
through so you're going to have a lot
more false positives but you're going to
miss less of your target sounds a lot
less of your target sounds so is that
notion of operation point which is the
choice of where you place the threshold
to to balance your your-your-your false
positives and miss detections and that's
application depending on said that's the
choice of the user where you place your
threshold now the goal is to compare the
modeling power of these various machine
learning methods not the wisdom of the
choice of the threshold so instead of
using final error rates in terms of
false detection on me
detection rates we use what's called a
detection error trade-off curve which
which is the results of browsing your
threshold across your score densities
and looking at every point every
compromise you can achieve i'm going to
show you dead curves in the next slide
for you to understand that the dead
curves can be summarized into one figure
which is called the equal rate which is
the location of the dead curve in the
plane of miss detections vs false am so
those are dead curves so you have the
x-axis is a false alarm probability the
y-axis is a Miss detection probability
and when you bro through bros your
threshold across all possible operation
points you end up with those those dead
curves and what you want is you want
your dead curve to be as close as
possible to the origin so and so the
equal rate is simply just to coordinate
across the across the diagonal so those
are the results for this experiment
across the baby cry class and the smoke
alarm class for the best model across a
range of models so we chose different
neural networks architecture different
number of layers different numbers of
units per layer and so on for support
vector machines different parameters of
pruning to end up with different amounts
of support vectors and so on so when
looking at the best model of each class
between GM's SVM and neural networks in
both cases the neural networks did a
better job than the support vector
machines and the energy mm's and the GMs
now in terms of what we really wanted to
know from this experiment which is so
that number of oppression versus
performance classification performance
we can see that the neural networks
essentially achieved the best
recognition results at the low lowest
lowest computational cost the lowest
number of operation the GM ms were still
doing quite a good job in classification
and at a low at a low number of
operations and then the support vector
machines were essentially all over the
place so conclusions so on this XP
dns deep neural networks appear to offer
the best compromise between
classification performance and
computational cost depth matters so the
best models were actually the ones which
which I mean you could have the same
number of parameters with wider layers
or deeper networks so the deeper ones
we're achieving at the same cost we're
achieving the best better results also
the nice thing is that the rectified
linear units were best which is good
news because they are very very simple
non-linearity so programmatically when
you implement them into an embedded
system they're very simple to implement
it really don't cost much gml provide
these are low-cost baseline so if your
GM system if your system is a GMM system
it's still a valid choice is doing a
good job maybe not the best job but
still a good job and SVM's were not
really competitive with the other
systems that's it thank you very much we
have time for questions I have actually
won yeah so I would imagine that if if
you don't want to recognize a sound
immediately but you delay the
recognition of a sound by one or two or
three seconds or something you can
accumulate evidence and make me make a
more robust recognition of the sound so
in your particular applications how
critical is a time delay between the
actual sound or event occurring and and
having to output a decision that this is
actually happening yeah so that's true
not true so for some songs is true like
if you have a detection of a smoke alarm
for example you want that smoke alarm to
have Sun it for about 10 seconds before
you send an alarm to someone because you
don't want to react to simple beeps for
example when the batteries offer
something you really want the event of a
smoke alarm to be defined as smoke alarm
sounding for a long time same story for
babies now for some events such as
gunshots if you have a single gunshot
you have to recognize gunshot glass
breaks same story you're not going to
have the tug trying to enter your home
heating the window several times they
hit it once it breaks there in and so
you want the latency of your system to
be very small because you have also
extra latency in the other
system that has to circulate through
whatever alerting system our customers
are going to have in place so we really
have to react as fast as possible but
within the use case or use case for the
smoke alarm would be it has to has
sounded for 10 seconds for the baby cry
baby has to have cried for some time and
the some time we define it as five
seconds people most people want to know
that when consider the baby has cried
after it has cried for five seconds so
yes I know it depends some sometimes you
can get the evidence sometimes not
really sometimes you really need to have
immediate recognition of your of here
audio even yes so you tried the iron
yeah the DNA is that come placement
confident rodeo or is it no it's a
simple formula network comparing the
company like a convent rodeo to the
islands or is there any service yes so
we are thinking of looking at other
architectures so call nets and also the
lstm neural networks but i mean when it
comes to the con nets it's it's a model
which is optimal for image and there's
not really any reason to think that
convolutional neural networks apply to
audio so there is still that kind of
fundamental question to think about like
are they the right models for the right
ties there is some hyper unconventional
at which because in image they're giving
those really state-of-the-art impressive
results are they going to be the best
tool for audio i mean we have to have a
look at that yeah yeah so how do you
think that you will have a better way of
dealing with the scalability of data set
collection so if you want to learn
gunshots you probably don't want to do
the same thing as you did with smoke
alarms and by polygons so we hired them
we did back never ever did we did higher
number of guns and went in different
places and shot the guns as well so we
do a very fun type of work which
consists in breaking windows and we
again did break your real windows and
hired a hole in in a in a former
military base where they used to do the
engine testing and it's all so it's a
hangar and it's all sound proofed and we
were breaking windows in it so
scalability I mean each different sound
class is going to have different fun
it's difficult to shoot guns inside a
home but you can shoot guns in places
which model at home so you can hire a
shooting range and but it in the way
that would mimic the acoustics over home
so yeah data collection is fun it's it's
costly but we good at it and we've had
the kind of unelected courage or being
crazy or something but to go and record
this sounds really we don't take things
from from sound effects database we do
record real sounds in real locations is
there a way to to optimize that I mean
no you have to record the sound someone
has to do it and we did it and we're
still doing it for new sound and so on
so I'm not sure what could be done for
for for for making that less costly to
be honest yes do it so how much of the
performance evening comes from different
machine learning algorithms and how much
from the feature pitch representation do
you play so yes in this in this
experiment we used the same feature set
for all machine learning algorithms so
so there was no the algorithms were
competing from themselves not not for
the feature set now it is true that
different sound classes are going to i'm
going to i mean mash it recognition of
different sound class is going to depend
on different feature sets you don't
necessarily want the same feature set
for something percussive such as a
gunshot than for something tonal such as
a baby crying very true the interaction
between the feature set and the machine
learning algorithm
is is not is one of these questions of
metal optimization of some hyper
something which is the match between the
machine learning algorithm and the
feature set in that case it was both
classes were tonal sounds you had the
babies and the things so using the same
feature set made sense in this
experiment but you is true that in our
case the future choice of feature set
makes a massive impact on classification
accuracy absolutely okay great let's
thank Sasha again for a great talk and
that concludes the workshop so thank you
all for coming it was a great pleasure
to host this event here and I'm I hope
you enjoyed all the talks most of the
talks will be available as video
recordings on research microsoft com but
i will also send around an email to all
the speakers and if you check the
workshop homepage maybe in a week or two
all the videos will be linked there so
thank you very much and i hope you
enjoyed it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>