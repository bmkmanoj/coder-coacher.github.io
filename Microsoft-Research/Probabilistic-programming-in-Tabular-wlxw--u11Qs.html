<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probabilistic programming in Tabular | Coder Coacher - Coaching Coders</title><meta content="Probabilistic programming in Tabular - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probabilistic programming in Tabular</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wlxw--u11Qs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm delighted to be here giving this
talk in front of Luca and all of you
tell you about a lot of the fun we've
been having learning about machine
learning and how ideas from functional
programming can have an impact now this
is I said when I agreed to give this
talk that I would really talk about my
current work i wasn't going to try and
review what we had done lookin I many
years ago and so is there a connection
between this and what I did with Luca
well so one I split this is a playful
thing and the Luca is always been
someone who I think it's been an adopter
of sort of mainstream technologies may
be ahead of academics or other
researchers like david dave' last night
reminds us about and lucas early
adoption of Max and he was also an early
adopter of microsoft word and wrote his
papers in word which i thought was
pretty cool when everyone else is using
latex so so this this work is like
taking probably tabular is taking these
ideas of probabilistic programming for
doing statistics and putting them into
excel so we're kind of going from the
the sort of mainstream product rather
than sort of going for developers
another connection is that I was so
cross with Luca for stealing spy we had
this spy language Martine and I and then
he had this system with with Andrew
Phillips spin Rosmah disgruntled about
this so my not really it's exactly it's
not spy it's but it's still called you
know spies starts to spy so anyway I
language
you stole the yes so we're still in his
name so here we heard yesterday about
fun the the the the the type theory in
his paper on understanding types and
data abstraction and so we stole it for
the name of our language I have fun
which is a probabilistic functional
language that I may be prepared it's got
nothing to do with Lucas fun but what we
felt we were still the name okay so what
is this about so what I'm going to do
first of all is give you a brief idea of
how probabilistic programming can
represent statistical inference and say
a bit about the policy programming
languages that currently exist and then
I'm going to explain how we took those
ideas and I've embedded them inside
excel with the attempt to take them to
the sort of large class of data
enthusiasts who are very comfortable
using Excel like a huge amounts of data
in Excel are there they're kind of not
professional developers but the people
are quite capable of writing scripts and
do quite sophisticated things in Excel
we like to give them the power to use
politic programming to learn from their
data directly within Excel okay so what
I'll do is describe a sort of little
probabilistic puzzle on how we learn
from data and so it's a it's a murder
mystery so we have there's a body in the
room think we've got some English
country house and there's been a murder
think Ludo and Alison barber in the
frame and for whatever reasons we think
it's actually most likely Bob done it
seventy percent chance and we think that
if if alice alice is preferred weapon
would be a pipe with ninety seven
percent chance and Bob's preferred
weapon would be a gun so let's see if we
can represent that as a bit of code so
this is the basic idea probably
programming that we write some code and
this is in a sort of well a couple of
boolean variables so we flip a coin to
indicate whether Alice done it or not so
that she has a thirty percent chance of
doing it and if this is false it's bob
has done it and and then has a gun being
used to know is it a gun or a pipe well
we look at whether it's Alice done it or
not
and if it was Alice then we click we
flip this coin because there's only a
few percent chance you'd use the gun
otherwise we flip the eighty percent
coin and so overall you would get this
distribution over the the two outcomes
of who has done it and and which weapon
they've used so you see the most likely
thing is a priori before we've observed
the scene it's most likely that bob has
done it using the gun now suppose we
actually get to the scene and we
discover a pipe so this is the point
where we conditioned the probability
distributions that we've defined with
with some evidence right so far we've
got two unobserved variables who has
done it and which weapon has been used
and that's what this chart is showing so
now suppose we found that the gun was
used so we conditioned that
whoops-a-daisy we condition that the
distribution and we can do that in code
by adding a constraint essentially an
assertion that the wif gun variable is
false and and so this essentially rules
out the runs of the program where we're
with gun is false and so we'd end up
with this sort of chart while we sort of
chopped out this probability
distribution here and we normalize these
two bars and now we see that the
situation is flipped around completely
now it's most likely that Alice has done
it using the pipe so the and I've been
talking about runs of the program so the
sort of naive understanding of these
these programs is you just run them many
times and you maybe rule out any runs of
the that violate the the constraints and
and then look at the results and that's
how you can sort of compute these
histograms also this chart and that
that's a nice thing and you can
formalize that and then there's also a
whole great panoply of inference
algorithms that can do a much better job
than just naively running the program's
many times and the idea of problem stick
programming is is really to let you
build these models by start by by by
sort of taking a programming language
say a celite language or a sale or
whatever language you like actually and
you need to add the ability to draw
random numbers
the ability to place constraints to
actually condition you know unobserved
video or well variables once they've
actually been observed and finally need
some way to indicate which distributions
are to be inferred and the the
alternative to doing this would be you
know which is what a machine learning
person would do would be perhaps right
these are rhythms from scratch and the
thing is that community has been busy
working on this for a good long time as
I say they have many algorithms I really
got to the point now where you can build
quite practical compilers that can take
a description in a language with these
sorts of features and compile them down
to efficient inference code so they
should be done for a while so so bugs
actually started out about 20 years ago
in Cambridge and it uses a kind of
sampling and it was it was a
probabilistic functional language
developed in that in a stats lab at the
University here and it's been a bunch of
others the inferred or next system which
we added this fun component to to drive
it from F sharp has been developed in
this lab but there's many others and if
you want to go look up the semantics you
know Gordon is one of the where's Gordon
Gordon was one of the pioneers and his
work with with claire jones and dexter
cozen mentioned yesterday was probably
the first person to do a nice formal
semantics of probabilistic languages
back in the 80s and Prakash has done a
lot of work and the semantics of the
various languages we developed over the
years have been based on on this but I'm
not only going to get into formal amson
annex today except Aleut if you're
interested in that we've got lots of
papers with theorems if you want to see
that um Kathleen Fisher and all i'm not
sure if he was a core she's a co-author
of yours looker but she did send her
regards could she couldn't make it to
this event but she's currently at DARPA
and she's heading up a program they're
called P panel that is basically sort of
seeing that we're at a point where high
level languages for machine learning are
a bit like we're high level language
book for any sort of programming were 50
years ago and the DARPA spending a lot
of money into sort of a researcher in
this area so it's rather a hot topic um
now what I'm going to talk about
specifically is languages for what are
called graphical models which is a
particular case of probabilistic
programming
and I want in this in this slide to give
you an idea that it's really a kind of
functional programming and you can think
about quite sophisticated sophisticated
machine learning model as a bit as
programming and the the thing and what
I'm going to give you two different
interpretations of this program but
first of all let me tell you the problem
so we've got the Master Chief up there
you know and and some years ago
microsoft launched halo and the problem
was you've got men and in particular the
online version of halo so the problem is
you've got many players in the online
version of halo and you want and you
want to match make them to have sort of
dueling parties and you want to match
make people who are roughly about the
same kind of level of difficulty that's
where most people get the most fun if
they're you know not being completely
hammered by someone who's much better
than them so I'm how do you do that I
mean so so intuitively you know players
have got a skill in the game and you'd
like a number and let's say it's a
number between nothing and 50 with me in
25 and you want to sort of figure out
how skillful people are so how do you do
that I mean it's not as if everyone has
played everyone else we don't have a
rank ordering we don't you know we've
got rather a sparse comparison of the
players in the tournament so let's see
so the way it works is that we we define
a sort of program this graphical model
that would generate the data that's
observed and then we can sort of run it
backwards to to infer those skills that
are not directly observed so what you do
observe is when players come together so
in this case they've got a simple
situation where the the observations you
what you have is of binary matches
between players and you can see where
the player which of the two players is
one so up here we we say so we've got it
we go to silver collection or a table of
players in a collection of matches and
these boxes this is that what it's
called plate rotation in the machine
learning community and it basically it's
basically like a map so the code that's
inside the box is to be replicated some
number of times for each of the players
and so what this is doing so it's a
graphical model and there's two kinds of
nodes and the circle nodes are random
variables and the
square nodes that link the the circle
nodes are what are called factors and in
functional terms they religious
computations that take the current
values or the yeah the current values in
this run of the random variables and
produce some other random variable so
what this is saying it's what I just did
what is what I just said that the skill
of each player is a is a random number
with mean 25 and the standard variance
100 which is the same as standard
deviation of 10 so it's rough a number
between nothing and 50 sort of comped
around 25 and then what this part of the
program is doing is picking up so it's
saying for every match we look up and
who are the two players playing so these
blue circles are actually inputs that
are not actually random variables
they're sort of their set by the data so
we pick up the indication of who is
playing in that particular match and
then these factors basically look up the
skills of each of the two players and
then we want to compare them but
actually it's too strong just to say
that the person who's going to win is
the one who is the highest skill instead
we want to add a bit of random noise
because something you know there's often
again there's this element of chance in
many games and also you know people you
know very day to day so what we do is we
have a performance which is like of each
of the two players which is like a noisy
copy of their skills and then we and
which is obtained by just doing a
Gaussian draw with the mean being the
skill and we had another sort of
standard deviation of 10 and then we
compare them and then that's that
produces the output to indicate who is
one so that's the that's the forward run
so imagine running this program giving
you know if you had a if you had the
fixtures of a tournament you run it
forward it we saw like a fantasy
tournament that would first of all
choose the skills at random and then
produce the outputs based on on these
formulas not at random I mean based on
the the skills that were chosen and so
what we actually want to do is run it
backwards because we don't know what the
skills are but we do know what the
outputs are so that's the inference step
where we do the observations a bit like
observing that the weapon was in fact
the wisdom that the pipe and we can
think about these these orange output
variables has been constrained
observed by the actual data in the in
the data set and then that's the point
where we would use an inference
algorithm to actually infer what these
random unobserved random variables are
so that in a nutshell is how you use
graphical models and they're pretty
amazing I think in that this is a
formalism that can define do a whole lot
of machine learning tasks so there's a
few other models and it could do things
like regression cluster analysis topic
modeling and so forth and I don't have
time to trying to explain what those
things are if you if you don't know but
but but I mean what attracted me to this
and was that the guys in the machine
learning group here had built this cool
compiler that had a really weird input
language but the really cool thing about
it was that it was really broad you know
the zillions of algorithms of models
that you can define using it I so what
we knew the last viewers are trying to
understand what they were doing and
first of all come up with us use their
sharp to define that the models and then
now in tabular we're defining these
models using inside Excel and so that's
a link to the info donate page and
credit to John when Tom minka who are
the leaders of that project so in this
talk I want to do these things they
describe tabular and I said already that
we're aiming at this sort of the data
enthusiasts I mean info doc net is
directly sort of aimed at machine
learning PhDs and and sort of any high
end and sort of sophisticated like data
scientists and we're trying to sort of
take probabilistic programming to the
masses which is you know challenging but
I think it can be a great payoff if we
could pull it off and there's some
guiding principles so the key one which
I'll explain in the next few slides is
that instead of thinking about the
probabilistic program is being something
that's sort of completely separate from
the data which is the case in all the
prior probabilistic languages that I
know of all the ones I put up we think
we say well actually you all you're kind
of machine learning without data the
data is going to have some sort of
relational description and look it
actually the relational inscription
corresponds really closely to the plate
notation that i showed you know I de
plate for all the players and at a plate
for each of the matches which other
corresponds to the table of the table of
players in the table of matches
as well hey I wonder if we could I
annotate the problem so the relational
schema of the data with probabilistic
expressions and use that as a way of
writing the programs and we think if we
could do that then this would fit really
nicely into Excel because these days
Excel actually has a relational data
model built into it so already has that
ready has a UI for defining the the
relational structure of new data so we
figured what if we added some a bit of
some little bit of functional
programming actually to that there was
to the Excel data schema and and and
then that like that led to tabular um I
said you always have to define the you
know what it is going to infer like
whodunit or what the skills are and
we've got two ways of doing that one is
by querying for latent variables the
sort of gray variables but the other one
is for missing value so sometimes you
don't haven't seen all the matches you
know there's you've seen half the
fixtures and which are observed and the
other half are unknown and often that's
what you want to predict because you
maybe want to make a bet or something so
the second way that we can do inference
is by filling in blanks and I'll show
you that in a moment and at the very end
I'll say a bit about auto suggestion
because I think at the mall I mean
tabulous I think well it isn't quite
usable by people who at this end of the
spectrum at the moment those are our
current users but to get down to this
spectrum we need some way to
auto-suggest models and say a bit about
some ideas for that okay so now let me
let me go through the same example but
now in the setting of tabular so we have
a we have a small data set here of
players we got Alice Bob and Cynthia and
we've got just a couple of matches so
this is alice has met Bob and Alice is
lost and Bob's met Cynthia and she's
lost and so the schema would look like
that and these are also the snapshots of
what we'd have in Excel so you could
have a schema like this and the so the
name each each row in the players table
has got a name it's a string and the
matches I've got a you know who is one
and then these links are just indicating
that this is a foreign key relationship
which is actually supported inside excel
so to say these are integers but they're
actually foreign keys into the players
table
so here's how we annotate that schema
with probabilistic expressions and in so
doing we get back to the graphical model
i showed you to start off with okay so
on the players tip so if you like the
the i should point out we've there's
been a shift by 90 degrees okay that the
the data tables if i go back here so
this might be confusing these data
tables are laid out like that with along
that the column names are along here and
when we get to i'm going wrong way when
we get to the the schema the column
names are laid out vertically okay so
moving back right so here we've got a
column name in the original table and so
these two columns in this sort of schema
correspond to the original schema and
the new stuff is over on the right and
the blue the color coding just
corresponds to the three kind of
annotations that we replace upon the the
columns in the original data set so over
here on players we just declare that the
name is an input meaning that we're not
going to try and predict it but we may
make use of it in fact we don't use it
in this case and then we have a grey
column labeled latent which is not
present in the original database but we
want to be able to predict it and we
have a it's a sort of part of our model
so we just say Gaussian to indicate how
the skills are defined and then down
here we got the matches table and we've
added a couple of new latent columns to
the matches table and which are the
performances I spoke about the noisy
copies of the skills of the two players
and we just say we say player one dat
skill is just dereferencing this foreign
key and getting the skill that we
defined adding some noise and then
finally we have an output per one
greater than / 2 and in the in the
original program i showed you remember
for Cluedo i explicitly had to put in
this observe that the you know the pipe
had been used and that's the typical
thing you have to do in most other
probably programming languages and a
nice feature of tabular you don't need
to do that you know simply having label
this as an output it's implicitly what
it means is that we were that the
probabilistic model or this graphical
model we get from this has these output
variables observed by the or constrained
to be equal to the actual data if
they're present
so we just do that implicitly there's no
need to add food put those extra
observes in ok so the different kinds of
clearing the first one is like waking
column which is like filling in those
lighting up those kind of gray great
columns so if you have that data and run
it what tabular is going to do is infer
some gaussians for the guys and
distributions for the performances but
most interesting for interesting that we
get their performances for the skills I
mean in this case you can see Alice's
lost to bob and bob was lots of Cynthia
so as expected bob is still roundabout
skill of 25 but Alice's got a lower
skill and and and simply has got a
higher skill and also that the the the
variances dropped significantly so
before we had a variance of 100 but we
now we've dropped rather which is
basically mean so much tighter Gaussian
you know it's it's much more likely to
be you know close to 22 and then the the
other cup mode of operation is clear by
missing value and where you know we want
to predict what's going to happen if
Cynthia in this case where Cynthia meets
meets alice so this is blank and so what
time will do will do is fill that in and
so Bernoulli distribution this is a is a
coin flip with eighty-five percent
chance of coming up true so because
simply has got a much higher skill in
the model and Alice it's much more
likely she's going to going to win that
so since that's all there is to tabular
you know you put your data in thought I
think that isn't a slide you start you
put your data in you have your skin of
the data you you you you have to have in
mind a model a probabilistic model of
your data so you know you such as based
on these hidden skills so you can
express those hidden skills by adding
licking columns and then you you write
your models for the latent output
columns which is really just those
little prolific expressions we define
and then tabular will step in and in
further layton columns and and also
parameters of the table and will protect
values and so um you so do you infer
them symbolically by so these missing
values these distributions these family
are they done by using reinforcement
learning or something so the relation
techniques
they done using analytical way they're
done they're done by statistical methods
so they mean either by doing by sampling
methods where you generate lots of
parameters and then observe the data or
alternatively disease what are called
message passing methods where you have
the graph and then you have an
approximate distributes over each of the
going back a little yeah the in these
message passing methods you it really
they really exploit the structure of the
graph here so that what you do is
essentially a constraint satisfaction
problem that you have a you have it so
the whole graph is defining a joint
distribution over all the random
variables and the joint distribution is
intractable to work with when you've got
lots of data because it's just so big so
instead what they do is the approximate
by saying well let's look at the
marginal distribution it's yes yeah
Beijing network is a special case of a
graphical model so you see ya you had to
you burn you have a you having a
disability bution each of the random
variables and then you have a series of
cycles where you try to hit a fixed
point by by sending messages around
based on the local structure of the of
the graph but I i I'd like the measure
theory I don't want to get into that in
the Senate's talked when we work with
and in fact that's not a contribution I
mean that in a sense that that existed
but we felt it had a poor user interface
and so we're trying to use language
techniques to make it better yep you get
such a reduction in variance with just
two matches that something I'm missing
here that's a good point it's um um it's
possibly a typo in the slide no hot you
know Gordon does it you know it probably
isn't updated i'm not sure but i did it
could be yeah yeah let's leave it at
that
apparently because it's a tiny example
that no one interesting thing is when
you look at what i was going on to next
is when you look at larger versions so
one of the first user is one of the
machine learning guys in the group who
was wanting to study is once a new
sports predictions using the underlying
system in fredonia and he's been a great
doctor of of tabular and so here a
rather more complicated model so here he
was looking at american college football
and there you start to get a hierarchy
because you you have the i mean you have
teams which sort of correspond to
players in the the original model i had
but teams are grouped into leagues and
leagues I mean the fact that you're in a
league gives you some information
because some of the league's are better
than others so as soon as you so once
you say you want to have a model that
can after it sees some teams in one
league artist once you have other teams
in the same league as you playing you'd
like that information to affect your own
skill so in his model each league has
got us what he called a skill modifier
and then each team has got an individual
skill and then the actual skill of the
team is the sum of the individual skill
and the modifier and then he has a match
model which was like the one I showed
you accept it also has a home advantage
and variable but then otherwise he
computes the performance of the two
teams and then and as add them together
and so then over here he's you know he's
able to actually do a nice job of making
predictions about the quality of the the
teams based on the based on this model
and in fact he's so he's got a so the
point is that I mean I should have a
baby example but he's got you can do
more sophisticated ones that actually I
was as good as as using infrared
directly okay so what I what I would
like to do and say a little bit more
about the language features and tell you
in the time remaining about I suppose
from a programming language point of
view the unique feature of tabular which
is this concert called of model
expressions
which I'm interested in feedback on
because I've had this hunch that it's
like a sort of module mechanism or a
kind of object mechanism so in
yesterday's talk maybe intrigued about
that again so I'd be very interested in
feedback on this so firstly I want to
give you an idea about a future have not
told you about which is parameters of
the model so so think about linear
regression so you've got a bunch of blue
points and you want to learn this this
red line and the parameters the red line
are the slope and intercept so it's
given by this wise ax plus B so we've
got the slope a and go to intercept with
you with the axis B and then we also
have some noise and the noise is
Gaussian distributed with a certain
variants and if we want to represent
that in tabular it looks like this well
what's the data we always start from the
data the data will use the XY pairs so
that's down here we've got the X Y pairs
X is an input and we want to produce the
output and well we transcribe the model
we've got an intermediate variable Z
that's ax plus B and then we add the
noise to the to Z to get the Y but
follow the a and the B weather outside
loops are so far let me bring up the
graphical model so so far I showed you
that in a true scale model everything is
inside the two loops the plates are like
loops of the data and all the random
variables are sort of associated with a
data point well it's often very useful
to have parameters that I outside the
loop and that's what this is
illustrating where the parameters a and
B of linear regression are outside the
loop and so we have so we had input
input layton and output are the key
words that say that you're inside the
loop but we also have a hyper and / am
and hyper in cystic it's just a
deterministic constant outside the loop
which in this case is this is zero an A
and B we declare as being params which
means that they're outside the loop and
they have particular models so they're
just they're just chosen randomly so
these are like the priors of the models
we don't know where a and B are so we
just choose them randomly using using a
Gaussian and this is somewhat arbitrary
so I don't particularly to defame the
choice of zero and one it would really
depend on the situation you're in and
but then given those priors you you
you have a plate that actually will
model the observed data and then that
will have the effect of of pushing back
to what a and B are most likely so so
that a standard least-squares algorithm
corresponds to this so to just by
writing this forward process we would
you know when you compile this to
inference code you get something that's
similar to the least squares algorithm
but the point is your stuff outside the
loop so now if we go to the tabular
language here's its grammar so a schema
is it's just a list of tables it's a
little tier the names the tables and big
tier the descriptors of the tables and
each table is going to series of columns
named see one of the CN they have some
types that generally speaking of scalars
or arrays or vectors I won't get into
them and also and that would be so if
you just at that that would just be you
know a particular way of writing a
relational schema but we have these
annotations and that's the tabular sort
of contribution and so far what I've
shown you is annotations that look like
these where so this hyper and pram which
are outside the loop and input latent
and output are sort of within the loop
within the map and these aims have
always just been simple probabilistic
expressions like gaussians or
conditional expressions or sums or
whatever but what is interesting is that
actually and the is where you you have
full model expressions where a model
expression is defining some material
that's to go outside the loop and some
material it's to go inside the loop and
this is what i mean by the module and
it's almost like an object where there's
a sort of constructor that sort of
outside the loop and then there's a sort
of instance that gets cold you know for
every entry within within the loop and
so a model expression m defines more
formally defines two things is the part
is outside the loop which is called the
prior and and it's a distribution over
some parameter w which in the linear
regression example was a and B and then
as a part that's inside a loop which is
called for the we call the sampling
distribution or something's called a
likelihood which is what's inside the
loop and it
says it sort of generates a data item
okay so then the the syntax we have 44 m
can either be an instance of one of
these model expressions a primitive
model expression rather I'm going to
give you a couple examples in the next
slide or it could be these a model index
model which is a way of sort of
assembling one model expression from
another okay so now let me I'm going to
talk about primitive models and then
talk about model expressions sorry index
models and how we can use them to
assemble these complicated models so so
here's two perimeter models so first of
all see Gaussian and this is a special
case actually for simplicity so there's
two parts to it so this is a bit this is
like a really simple version of linear
regression if you like so there's a
parameter a mean that's outside the loop
that's a particular drop of a particular
Gaussian and an inside loop given the W
Oh drat well the W is really the meat so
I there's a wmn missing there so given
the given the parameter we we do a draw
from it so it's so that the one the
result one Gaussian is being used as a
mean for another okay so that's that
simple and now a little more complicated
a quick refresher first on some basic
distributions and so we're going to want
to define some discrete distributions
okay and the the the so what's the
discrete distribution well the parameter
is by a vector V and it's what's called
a probability vector which is just a
vector of numbers that adds up to one
okay and you can think about a discrete
V a distribution it's a bit like an
n-sided dice that is going to come up on
face I with probability P I ok so maybe
so so for example you know an ordinary
dice would be we'd have a better of you
know six copies of 1 over 6 but but they
don't have to be they don't have to be
uniform in that way and so that
corresponds to the gin but then we
we so what will often happen is we don't
know what that vector V is we don't know
whether it's a fair dice or not so we're
uncertain about what the probability
vector is so deliciously distribution is
a distribution of distributions it's a
distribution that will return a
probability vector okay and so it's it's
specified like this and the idea is that
you you you you well what would you know
about dice well you know how many times
it has come up on each of the different
sides so the idea is that you you put
the counts in and a solution a
distribution with these counts is the
the distribution or your uncertainty
about what the probability vector is of
an end I given that you've seen see I
observations of each of the faces I so
in the case where these are all oh I've
missed out a plus one the-the-the so
minus one and what it's so actually when
you said that yeah there's no off by one
error here sorry about that the so we
can put if you poke in one everywhere
that's like a that's actually the
uniform distribution so that would give
you the fair dice or they're not seeing
any observations of the dice but after
you seen it come up a few times maybe
it's come up you know five you know ten
times and other numbers one each then
you're going to start together your
uncertainty over the distribution has
changed rather you don't think it's a
uniform distribution anymore and you
think it's you know going to be comped
around 10 okay I don't wanna go into all
the technicalities of it but I hope that
gives you an intuition of what the
distribution is so anyhow back back to
tabular and the primitive distribution
see discrete we would we we its prior
over its parameter is given by Jewish
lay where we put one we basically seen
know the ideas but we don't know any we
don't anything about it how fair it is
so we just put in one for each of the
counts which correspond something not
seeing any observations of any of the
faces and then the gin part that
actually produces rolls of the dice is
just a call from from discrete so those
are two models so let me move on to a
use of them to do costing
which is a very important you know
machine learning problem so it would be
like this that I at the end of the day
given this data you can i'd like to
actually have the machine infer that
this that the these guys here to do that
i like the machine to do the labeling
and say that these guys are one cluster
and those guys are in another cluster
and so the way we're going to do that
and and yeah the way we're going to do
that is this that we we have a model and
it has the it has the XY points okay and
this particular model is is observations
of eruptions of the Old Faithful geyser
from Yellowstone okay and the the so
there's a there's a setting time before
the you know but from the last eruption
to the next one which are on the the x
axis I think so no the waiting time is
on the y-axis so that's like 60 minutes
that's 180 minutes or whatever and the
durations here are how long that the
next eruption actually was and so you
know this is a sort of famous data set
from statistics and so we'd like to be
able to model it so the actual data you
have are they are these durations and
the times and they and we have a latent
variable that we don't know that is
going to indicate which of the two
clusters the the eruption is going to be
in and we write like that so you see
discreet so the thing we just so it's
like it's really we set N equals 2 so
it's more like a claim than ur than a
dice but we you know we don't a priori
know what the bias of the this coin is
but we set it up like that so that's
going to declare a parameter which is
the bias of the of this dice between the
two clusters and it's also going to
produce a sampling distribution that
would define it for every roll of the
table is going to define which of the
two clusters it's in and then here we we
model the durations and the times by
gaussians using the C Gaussian model I
showed you earlier but crucially we use
this new construct of
index models which is the unique thing
in tabular is that we index these these
two or three of these models so these
two models here by the the cluster and
variable so it might be simplest to put
up the the charge so what we get in the
end is this where the this line here say
discrete is declaring this variable so
this is the the bias of the coin that we
don't know initially so we just put this
this deliciously distribution on it
which essentially is a well anything up
we don't we don't know and then within
the loop the idea is that each cluster
variable is chosen by doing a draw sort
of rolling that dice and which produces
that and then the what the effect of
this model is indexed expression is is
that it it takes the model here and
remember the model has got some stuff
that's outside the loop and some stuff
floats inside the loop and what it does
is it just leaves the stuff outside the
loop alone those parameters are I'm
sorry other way around when it does is
it makes copies of the stuff that's
outside the loop so a Gaussian we just
have a single mean but because we've
indexed it by the custom variable which
takes values 0 or 1 we get an array of
means up you've got two different means
and corresponding to the two different
clusters and the same thing so that the
two different yeah the two different
clusters and we do that also for the the
times though in this case and and that's
what this notation is showing that so
it's a plate so it's saying there's two
copies of the the material within it so
it's essentially saying we've got two
copies of the mean variable and two
copies sorry off the duration available
in two copies of the time variable and
then what the indexed model does for the
sampling part is essentially leave it is
is is run it as before except we use
this expression here to choose which of
the what index the array of parameters
so that it's going to use so depending
on so for each data item depending on
whether the cluster is 0 1 we use either
of the two copies of the duration
mean to produce the actual duration
output variable that get there gets
observed so I hope that that makes some
sense I have another example but I'm yes
I've got another example of
classification but maybe I will so I'll
just show you it that this this is um
this is what's called a naive Bayes
model and and you can see it's using
exactly the same kind of syntax that
here we've got each row has got is an
individual and they're either male or
female and they've got Heights weights
and foot sizes and some of them are
labeled so males and females and some of
them are not labeled so the problem is
to predict these it's a classification
problem and we can week until use these
index models to define a nice vs.
classic naive Bayes model and it and it
gives them give some good results but
i'll skip through them let me let me
conclude actually doing slow too long
one on that but I guess I in this
audience I wanted to talk about those
index models and if that's made sense to
you I love to you know talk or flying
about you know how those you know
compared to two other constructs um okay
so tabular I've dived into details there
but but but sort of you know leaping
back up what we've done is take taken
these these all this technology of
machine learning inference algorithms
that previously was sort of surfaced in
just about developer-friendly
programming languages and we sort of put
them into Excel where we think a lot of
data of significant you know of the
significance is and where there's a lot
of users have got a lot of data that
they would like to actually you know
work with and we've done that by
exploiting the structure of the data to
write probabilistic programs and we have
the system its ongoing this is I mean
project it's available from this web
page you can actually download it we've
got some internal users I was really
excited user oven Redmond has been
running on some vast data sets with
tabular on a 96 gigabyte
machine so you know this there's
actually scale up to you know large data
sets and I think there's a lot of
exciting to be what work to be done on
this so if you're interested please
download it or talk to me we'd be very
interested in your feedback so thanks
your attention each year microsoft
research helps hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>