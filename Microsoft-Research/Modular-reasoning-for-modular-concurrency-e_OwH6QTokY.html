<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modular reasoning for modular concurrency | Coder Coacher - Coaching Coders</title><meta content="Modular reasoning for modular concurrency - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modular reasoning for modular concurrency</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e_OwH6QTokY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
Thanks yeah so this is this is joint
work with dart dryer and large brick at
all and it's sort of following up on
some work that appeared in poeple this
year okay so the the general area that
this talk takes place within is
concurrency verification and if you've
been sort of paying attention at all in
recent years you'll know that there's
been a massive explosion in this area
with sort of new logics appearing in
every conference you know constantly
right and it's fit so here I've drawn
sort of just the most obvious
descendants from separation logic right
and already you can see quite a bit of
bifurcation right so so but maybe you're
not following this development so
closely so you might wonder why you know
so many logics what what's sort of going
on here right so I mean obviously
there's there's a lot to say about these
things and I'm not going to sort of
spend time going through much of the
history but generally what's been
happening is with each of these new
logics we're able to reason about more
and more sophisticated kinds of of
concurrency for larger and larger
programs and the key to doing so I think
is that as you sort of go forward in
time these logics introduce new forms of
modular reasoning right i mean this this
is sort of the motherhood and apple pie
kind of statement right that what we
want is modularity along every dimension
possible this is what allows us to scale
up the reasoning technique so let me
just give you some examples right so
separation logic sort of introduced a
very important form of modularity namely
spatial modularity where you can reason
about stuff that's going over going on
over here in one part of the heap and
know that it's not going to cause
problems for something going on in
another location in the heat right so
you can reason in a heap modular way and
then as you proceed to introducing
concurrency of course you care about
things like thread modularity then more
recently with things like abstract
predicates you deal with things like
data abstraction right which is one of
the really basic forms of modularity and
programming right so my claim is that
sort of what's been happening with these
logics is that you're seeing increased
modularity and that's giving them
increased power to tackle sophisticated
concurrent paradigms okay that's the
general trend
okay so of course with this setup like
that you sort of have to expect that
what's going to come next is we're not
modular enough yet right so I I want to
sort of continue this line of work by
tackling two key forms of modularity
that arise in programming practice that
existing logics I can't really deal with
okay so so the first one and I think the
most important one is something that
we're calling granularity abstraction
which is a generalization of add in the
city abstraction okay i'm going to say
much more about each of these points a
bit later but just to give you a basic
idea in many concurrent programs you end
up using libraries of concurrent data
structures that interact in very
sophisticated fine-grained ways for
performance reasons right but as a
client of one of these data structures
you want to think of them in a much
coarser grain way okay so for example
you might think of a hash table that
you're interacting with that internally
tries to allow a great amount of
parallel ISM between threads right so
threads writing two different hash
entries don't have to synchronize at all
but as a client of this table you'd like
to imagine that every operation takes
place in a single atomic step right so
there's a difference in abstraction
there and that's the kind of modularity
we want to take advantage of now a lot
of logics have tried to prove that this
kind of abstraction holds with various
degrees of success but essentially no
logic actually then lets you use this
kind of abstraction to reason as a
client okay so what I'm going to I'm
going to show you a logic today that
actually allows you to both use this
logic to use this kind of abstraction to
verify an algorithm and then sort of act
as a client of that algorithm okay
another form of abstraction that I know
is near and dear to many of you is
functional abstraction or higher order
programming so again in practice you see
a lot of programs that that take in
functions as arguments produce functions
as results and do so in the context of
of concurrency right and again this is
like largely ignored in in sort of state
of the art logics okay so to make this
sort of emphasize this point about these
algorithms arising in practice right I
want to bring up a well-known concur
library java.util concurrent and.net has
sort of a similar although somewhat
smaller set of concurrency primitives
right so Java util concurrent is sort of
a dozen year long project headed mostly
by a guy named ugly and it provides a
number of fine-grained very high
performance data structures and
synchronous synchronizers for concurrent
programming ok so these these kinds of
things each and every one of these
generally represents a papers worth of
ingenuity right to get to perform well
in sort of multi-core setting so just to
pick an example right I already
mentioned hash tables the hash tables in
Java util concurrent allow clients to
read the hash table without acquiring
any lock so they can do so totally in
parallel they allow clients to write to
the hash table using fine-grained locks
so if two clients are writing two
different hash keys they don't have to
contend for a common lock and then sort
of most amazingly the hash table can
actually be resized while all the rest
of these operations are going on ok so
again these are these are the kinds of
things that are very important in
practice if you want to enable
parallelism by building on top of these
algorithms ok but as a client you don't
you shouldn't have to care about any of
this stuff you should be able to
abstract away and just imagine that if I
do an operation on the hash table at
some point it'll take effect atomically
ok clear enough feel free to ask
questions throughout by the way
excellent ok ok so that that was sort of
those the introduction so here's here's
the plan for the top so the logic I'm
telling you about today we've called
carousel and I'm going to start by
briefly introduction introducing sort of
our big picture for carousel which
brings together two kinds of reasoning
that you see already in the literature
so one horse style breezing which I
already showed you coming out of
separation logic and the other something
called refinement which is how we tackle
granularity abstraction I'm going to
walk you through at a high level a
simple example data structure and show
how we unify these two modes of
reasoning to support
abstraction I've mentioned already then
I'll show you in a bit more detail how
we actually carry out proofs for a data
structure and then finally I'll talk
about sort of a major case study that
we've done using this logic okay so
here's some code yes okay so this is
about the simplest non trivial example
of one of these fine grain concurrent
data structures I've been talking about
okay this is known as treiber stack and
the idea here is so this is this is a
stack that's threadsafe it can be called
by concurrent clients okay it supports
push and pop operations but it doesn't
use a lot to protect the contents of the
stack it does something more clever okay
so here's here's the basic idea first of
all what is the representation of the
sack well we have a head pointer that
points to some mutable list okay but
it's in one of these so called atomic
refs and what atomic breasts are
basically are references that support a
special operation called comparing set
comparing set lets you safely resolve
races between threads that are trying to
to write to a given memory location okay
so what we're going to do basically if
we want to push something onto this
stack is follow a style of optimistic
concurrency so ignore the back off here
for a second if we're trying to push
will allocate a new node hey with our
data in it but we don't know exactly
what the tail pointer is to start then
we start a sort of optimistic loop to
try to insert the node so every time
around the loop we take a snapshot of
the current state okay this is where the
optimism comes in we haven't told any
other thread that we're trying to
perform an operation on the stack we
haven't acquired any locks we just
taking a snapshot which might be stale
okay nevertheless we use that snapshot
to set up our tail pointer and then we
try to install our node okay and here's
the key bit we only install the node if
our snapshot is still valid right so we
check whether our optimistic assumption
actually held up or whether some other
thread interfere
along the way okay and then in practice
you often use things like like back off
to make sure that you're not getting
sort of too much contention over central
memory locations but I'll sort of gloss
over that point okay so so let me just
walk through what it actually looks like
to run this bit of code right so here's
here's a graphical representation of an
example stack and suppose now we're
trying to push a note on so we allocate
a new node with some value and we take a
snapshot of the current head right and
set up our node to be added but maybe
behind our back some other thread
installs another node right this can
certainly happen because again we have
we haven't signaled at all that we're
trying to do a push okay so some other
thread might install a node and now if
we updated the head pointer we'd be in
trouble we would have dropped their note
on the floor right but that's okay
because this compare and set operation
whoops sorry this compare and set
operation sort of make sure that our
snapshot was still valid right sorry I
just realized that I didn't actually
spell up the semantics here so what
happens is we're saying we want to
update the head to n if the current
value is atomically cur okay that's
that's the checking of the snapshot okay
so the point is when we try to do the
compare and set here it's going to fail
because our snapshot is not valid and so
then we'll loop again will you know get
a fresh snapshot and maybe this time
we'll get lucky and we'll win the race
update head ok all right so that was a
fair amount of detail and it may not be
clear exactly why one would ever want to
program an algorithm this way so I'm I
don't have time to sort of get into the
nitty-gritty of the motivation for this
kind of algorithm but I'll say that
essentially the main reason to do this
for something like a stack is cache
performance but basically by avoiding a
lot we can communicate between threads
using fewer cache lines causing fewer
cache misses for other algorithms like
hash tables you also get a parallel ISM
benefit but that's hard to do for sex ok
and ok so popping basically works in the
same fashion you see the same kind of
optimistic retry loop taking us
snapshot and so on okay any questions at
this point okay so now if we want to
start to talk about verifying this kind
of algorithm right so just your I'm
asking you to take my word for it this
is something that's good to do in
practice but the question is how do you
think about this as a programmer so we
need to specify the algorithm first of
all and you might be tempted to give it
a spec like this in a traditional whore
logic fashion okay so let me walk you
through this right so suppose we have
some stack predicate that's an abstract
predicate representing the value of the
SAT right and we say okay well if the
stack currently has the value X's and we
push X onto it then the stack now has
ex-cons down 2 x's right fair enough and
then similarly if we try to pop an
element off well maybe the stack was
empty maybe not and we'll get a return
value reflecting that and learn
something learn something about the
value of the stack had and also
potentially update its value by removing
an element the problem is this doesn't
work at all and the concurrent setting
okay because these predicates are not
stable under interference by other
threads so basically the moment that you
pushed an element onto a stack nothing
stops some other thread from then
completely changing the value of the
stack but here we're serving know that
value is X constanta X's right okay so
some some concurrent separation logics
deal with this kind of thing by treating
these assertions as sort of private
assertions meaning that no other thread
could possibly be interacting with this
this stack and that's fine that sound
but defeat the purpose of using this
concurrent sack in the first place right
so my claim is this spec just just won't
do ok clear enough so let's try again so
here's a rather different kind of spec
that really throws away quite a lot of
information but actually works ok in the
concurrent setting so pick any predicate
p and think of this p as being something
that's satisfied by each element you put
onto the stack ok so the idea is you can
push something onto the stack x if you
know p of x
now you've given up rights to P of X so
P of X might be some resource that's
associated with the element for example
if you're in separation logic predicates
have sort of associated resources with
them so you're transferring p into the
stack and then later or transferring P
of X into the sack later if you pop and
you get some element back then you also
learn p of x about that Holliman it
could be as simple as that absolutely
yes and that's and that sound even if
you're doing it concurrently right
because all threads have sort of agreed
to follow this protocol that they'll
only push elements that are bigger than
three okay but then this resource bit is
well maybe this element is actually a
pointer and it's a pointer to some
memory whose access is sort of guarded
by P and so you're saying i currently
have ownership over this bit of memory
now I'm going to put it into the stack
and when some other thread gets that
element back out they have received
ownership of that region of memory right
so these are kind of stories that we can
tell on top of the data structure as a
client right that are saying like we're
setting up some protocol between the
threads that are using this data
structure passing bits of memory back
and forth or whatever this predicate is
that yeah okay um but there's an obvious
problem with this spec it's satisfied by
more than just stacks right this is also
satisfied by queues bags many other
collections right so we've lost way too
much information about this tag okay but
on the other hand we've captured some
particular way that you might use the
stack as a client you know passing NP
getting out p with elements okay so also
unsatisfactory but somewhat better
here's a rather different way we might
specify a stack okay as a piece of code
right so here is the trivial way to make
a stack or any sequential data structure
threadsafe you just wrap every operation
with essentially a lock right so I think
synchronized keyword in Java here okay
so basically we're just using the
sequential implementation and forcing
mutual exclusion between the operations
okay so this is a specification that
loses no information I claim about the
operation of the stack works in a
concurrent setting right and that gives
us something reasonable to work with as
a client right so now when we think
about push we get a nice simple atomic
operation instead of that retry loop
that we saw before yes indeed um so yeah
yeah that's true so if you're going to
do this kind of thing you probably want
a sort of specification language that
you know actually has some atomic
keyword for instance you know that maybe
that avoids deadlock although there are
some problems with the higher order case
I sort of I don't want to get into that
too much at the moment yeah right i
think so i would say that for most of
the data structures of interest in
something like Java util concurrent
you're not getting that kind of free
entrance II right yeah but it's a fair
point okay so so so we're going to take
something like a mixture of the
approaches that I've shown you in
carousel okay so first of all this last
specification I showed you is the key to
doing granularity abstraction right so
we have some implementation of a data
structure that we're threads are
interacting at a very fine grain doing
things like comparin set and then we
have a simple coarse-grained spec and
the idea is that we want to say that no
client that's plug that's linked against
the fine-grained implementation can tell
the difference from the coarse grain
spec that will be our
recognize criterion for something like a
stack okay so so in other words you run
with the fast thing and your reason with
the easy thing then you can connect that
kind of granularity abstraction with a
kind of whore style spec that I
mentioned earlier okay so as a client of
something like the stack right maybe you
don't actually care that it's a stack
maybe you're layering some protocol like
the bag protocol that I showed you and
that's really what you want to reason
with okay but carousel supports this
kind of decomposition of the proof
because the bags back isn't the only
spec that you might imagine associating
with this stack right it it loses a
great deal of information yet you don't
want to reprove things from the treiber
stack every time right in general this
could be very complicated algorithm
right so in carousel what we do is we
sort of cut this down the middle using
granularity abstraction let you prove
some sort of canonical specification for
the data structure in terms of other
code and then further abstract that to
say whore style specs as as you wish
right without having to sort of reproof
things from scratch sure so so four
stacks yes yeah yeah so so stacks stacks
up you know support a nice simple
implementation so I use it as an example
here they're not so useful in the in a
concurrent setting so a better example
is something like a hash table and so
there are you know many parallel
algorithms that use hash tables
internally to control some kind of
protocol between threads so your
knowledge there might be for example
that all the rights to a certain key are
sort of monotonically increasing over
time or something like that right so you
might
you that's the abstract right so you
have a canonical a simple atomic version
of the hash table and then as a user of
that hash table right you have to
somehow your you have threads that are
interacting through it according to some
higher level protocol something like
values are monotonically increasing or
you know things along that line I mean I
could get yet does that does that sort
of make sense yeah
is there sorry a semantic difference so
I mean yes also so I'm not quite oh I
see what you're saying um yes so so this
this semantically is a contextual
refinement sort of like I alluded to
earlier where you say no client can tell
the difference right whereas this is
just standard horrible satisfaction that
doesn't explicitly talk about about the
clients you know it's not a refinery
property yes um I don't see how oh oh oh
I see yes you could turn the horse horse
satisfaction into a refinement yes yeah
so specification statements basically
internalize pre and post conditions and
into a language construct if you did
that then you could work completely with
refinement and that's that's a perfectly
reasonable way to go if you're willing
to sort of change the language right
yeah any other questions at this point
okay so so again the point is just that
different clients of a data structure
that are using it can currently impose
different kinds of protocols on their
use but we don't want to have to reprove
the fine-grained version ever for each
different protocol okay that's that's
the point so we're using granularity
abstraction to factor this proof into
two steps okay that was granularity
abstraction the other kind of
abstraction I mentioned was functional
abstraction all right so suppose we
extend the stock i showed you earlier
with an iteration construct okay which
just takes in some function and applies
it to every element of the stack okay
that sounds simple enough if you read
this as sequential code it probably
looks like what you're used to okay but
in fact this is quite subtle because as
you're walking through this list the
stack itself could be changing under
foot right there are no locks here so
nothing prevents other threads from
mucking about what
the actual contents of the stack yes
indeed well so notice that first of all
okay so I take your point um but the
question is as a client again of of this
method what connection if any do you
have between the elements that you see
and the current value of the stack the
current elements of the stack so in
general none right but exactly right but
of course so again the story is we need
so we need some specification that
basically captures that idea but then
that allows clients that are better
behaved right so in general if if there
are arbitrary threads out there they
could be mutating the stack in some
unknown white but that's not how we
program right we know what the other
threads are we wrote them they're
following some protocol and interacting
in a certain way and so we want it we
want something that we can use to get
more predictive power about what
elements we'll see hey um yeah I'm not
however the type was important because
uh so there was actually another source
of mutation already as a bit subtle but
when we push the elements on right we
allocate this node and we sat there and
kept mutating its tail until we finally
put it in yes but we need to be able to
actually prove that in the logic right
yes so you've set me up very nicely
actually ah ok so I'm going to skip over
this because we kind of already talked
through it and show you the the atomic
specification for the stack and notice
that in fact I change the type of the
list here to be immutable
so when you're programming against this
spec you get exactly sort of what we had
in mind that look you read the head
which is an immutable value and then you
loop over it and that's all you know and
there's a priori no connection between
the elements you see and the current
value of the head okay but again this is
just sort of the canonical atomic spec
that works for any concurrent client but
in general we're going to impose some
protocol that say our threads are better
behaved than that did you know they're
not going to pop arbitrarily there's
some logic to what they're doing with
this stack okay fair enough okay so that
that was the overall picture of how we
want to deal with higher order
abstraction and granularity abstraction
to sort of modularize reasoning in a way
that hasn't been done before okay so the
algorithm so did yes yes okay so so it
is true you know this was not a very
interesting example of higher-order
programming so I have two things to say
about that one the the case study I'll
show later is a more interesting example
and two existing logics just can't don't
scale to higher order code at all even
even simple cases like this so there is
a lot of work just in building a
semantic model that could support even
the simple form of higher order okay but
the claim is that higher order
programming is really important in
practice we need to be able to handle it
right okay so so that that was sort of
the high level picture of what we're
trying to yes yes
yes indeed yeah so absolutely so I mean
so I'll again although it was a somewhat
trivial case this kind of iterator thing
is used all over the place in
object-oriented programming and Java
util concurrent has hash tables with
iterators like this and you know it's
absolutely essential to to programming
up sophisticated algorithms like so the
work I did with Claudio for instance on
join patterns actually uses this kind of
iterator um but again I'll get to at the
end here this other algorithm that's
sort of more interesting right and is
used in practice okay any more questions
before I go on ok so for the next part
of the talk I want to delve a bit into
the detail of how we actually prove
these refinements that I was talking
about actually prove a granularity
abstraction ok so the way we do this is
by introducing what we call protocols ok
very general term protocols are meant
just to capture some sort of the
expectations about a bit of state that's
hidden but shared between some number of
threads ok for example the
representation of the tribe or stack
that threads are interacting with and to
model these protocols we started with a
recent sort of the state of the art
logical relation work from I cfp 10
which showed in the sequential setting
how to use a kind of transition system
to explain the evolution of hidden state
ok the it turns out this is a very
natural fit for concurrency but to scale
it up you need to add a few sort of
features to the basic transition system
model ok and i'm not i'm not going to be
able to cover all of these today of
course but just to give you a flavor
basically the extensions align with
certain kinds of locality or like the
modular reasoning that I was talking
about before so for today I'm going to
focus just on thread thread locality
aspects and i'll explain sort of what
what these terms mean as we go along so
first of all what what does a protocol
look like so here's
like the simplest imaginable example for
concurrency so if you have a lock right
you can think of that as basically a bit
with two states that can cycle arbiter
right between locked and unlocked so
what we do is we write down a protocol
as a state transition systems where the
state's themselves are sort of abstract
names that we can we can pick as we like
but they're interpreted as assertions
about the heap under under the hood so
in the unlock state for example we say
that the lock itself is false that's how
we're representing on lock and that in
addition there's some resource that's
protected by the lock right that sort of
owned by the lock on the other hand in
the lockstate the lock is true and the
resource has disappeared okay so the
idea is when you take this transition
you get control over this resource are
you being some thread that actually
acquires the log and then similarly when
you unlock you have to have Reese Attis
fide this invariant are and you give
back up ownership to the to the lock
okay so if you've seen concurrent
separation logic this should be pretty
familiar stuff okay so but there's a
slight problem at least if we look at
this abstract level in that the way we
read this protocol is basically you know
I might know that the current heap
satisfies the unlock state my
environment is allowed to take any
transition it likes at any time and so
if you have a cyclic transition system
like this you really don't know much at
all about it's not constraining
interference much at all right so if I
think it's in the unlock state it could
be in the in the locked state and vice
versa but in reality what we want is to
say that only the thread for example
that acquires the lock is allowed to
actually then release the lock some
other thread in its environment can't
release the lock on its behalf that's at
least an example protocol for a lock
yeah you could do something more
complicated okay and and this protocol
you know system will sort of support
that kind of thing but just as a simple
motivating example suppose we wanted to
encourage this so way we do this is is
via sort of abstract permissions that
show up in the protocol so which we call
tokens
and tokens represent a way for threads
to take on roles in a protocol by taking
certain transitions ok so this token you
can think of as representing the lock
and again there's a kind of conservation
property where the thread that takes
this step gets ownership of this token
and then only a thread that owns this
token can take the step from lock to
unlock so let me let me walk through
that in a bit more detail all right so
the idea is if you want to go from the
unlock to lock state you start by not
owning any tokens and the protocol has
the token and then in the end the
protocol doesn't claim the token so sort
of by conservation you must have the
token now and then vice versa if you own
the token and want to unlock that's fine
because here are the protocols claiming
it you have to give it up but you had it
at the beginning so everything's kosher
but if some other thread who doesn't own
the token tries to move you from the
lock to unlock state it won't be able to
satisfy this equation right the protocol
says you have to give me the token the
other thread doesn't have the token so
it can't make this the move yeah yes so
so the idea is that ok so it's true for
this particular example that if you sort
of cook up the right kind of are you can
you've already ruled out this kind of
move but you know for more sophisticated
locking protocols I could hand some
other thread the lock for example and it
could unlock on my behalf and you want
to be able to model to model that and
you know using these protocols I mean
but the sort of more general answer is
it's useful to be able to talk about
what threads are allowed to do
completely at this abstract level of the
state-transition system without
reference to the underlying resources in
their interpretation ok so you set up a
protocol using these tokens that tells
you how things are allowed to evolve in
general and then in any given state you
can see sort of what the state of the
heap is under the
uh yes yes in general mmhmm yeah there's
sort of a whore logic where tokens work
as any other kind of resource yes yes
absolutely yep yeah yeah so I mean yeah
you could do something more complicated
but generally we say that there's some
set of tokens and each token is sort of
unique and can be owned by only one
thread at a time okay any other
questions yeah uh yes I mean you mean
formally or I mean so spiritually yes
yes yeah yeah and in the long run that's
that's where we want to go I mean we've
modeled it well let me take that off
wine but we should talk more about it
after yeah okay all right so that's
that's a just a relatively simple
extension to the basic transition system
model and again the point is all threads
share a central protocol but threads
have different views on it by virtue of
the tokens that they own okay clear
enough okay so then the other extension
that we need to handle thread locality
is something that we call specification
resources so to motivate this I want to
give you another example algorithm that
builds on top of the stacks I showed you
earlier okay yep
sorry no I think you're reading too much
into this picture and you know I guess
so the point I was trying to make in the
beginning is just that this progression
of logics has been sort of introducing
new axes of modularity and I'm just
trying to take the next step yeah
nothing more than that okay okay so so
I'm going to so I want to take one more
look at stax here and think I've bit
more abstractly about what's going on
right so say say we have a stack like so
and then we have four concurrent threads
that all try to do operations on it at
once okay like like so now what the
atomic specification that i showed you
guarantees is that these operations will
take effect in some sequence okay it's
non deterministic interleaving but each
one atomically modifies the value of the
set right that's that sort of the basic
view of other concurrent stack but it
turns out that we can make very clever
observation about stacks in particular
that allows us to paralyze access to
stats right by observing when we have
this kind of concurrency so this is an
algorithm known as elimination stacks
and the idea is very simple you can
notice that certain operations on stacks
like pushes and pops cancel each other
out right if I do this if I choose a
sequencing where first I push three and
then I pop the stack stays the same and
all that's happened really is that I
passed this value 3 from the pushing
thread to the popping thread okay
so this is a very interesting
observation because it allows us to do
two things to this tag number one it
means that we can service these
operations from these other threads
without having to modify this single
point of contention the single head
pointer and also it means that we can
communicate operations between threads
in parallel right I mean that's sort of
another way of saying the same thing
right so let me show you what this looks
like concretely so the Elimination stack
has sort of two parts to its
representation some underlying stack
like the Treiber stack and then a side
channel four threads to discover each
other and try to cancel their operations
out okay so then the idea is if you're a
thread that's trying to do a push you
now have two choices you could attempt
to do the push on the on the real stack
or you could advertise your intent to
push and hope that some other thread is
trying to pop at the same time so
suppose we choose the latter we make our
offer and then we wait for a moment to
see whether some popper happens to come
along likewise the popping thread now
can notice that there's an offer in play
to push this value and accept the offer
okay so at this point we've transferred
the value 3 from the pusher to the
popper without ever modifying the
underlying stock right and so if you
imagine now you actually have some cable
of channels you can actually support
parallel eliminations of pushes against
pops and again eliminate this contention
on the central stack resource okay and
stocks are just one simple example of
this kind of phenomenon of threads
helping each other out to do operations
on some underlying data structure
indeed so you know you have to carefully
design this in a sort of heuristic style
to try to figure out how active this
data structure is and they do sort of an
exponential back-off in terms of use
using the table right but semantically I
mean the way this works is you make an
offer you wait around for a limited
amount of time but there's no guarantee
that some opposite thread is going to
come along so after a while you give up
and try to modify the underlying stack
right oh sorry yeah no no I'm not saying
that at all um no I well so this is what
so I did you know there are many
strategies you could use right so my
point is just that in in practice with
this algorithm they use a kind of
heuristic strategy that's based on
exponential an exponential distribution
right so essentially you start with the
first element and if there's somebody
already there then you sort of assume oh
there's there's a lot of activity on
this and you sort of double each time
how far into the table you're willing to
look right some of its it's a bit
complicated right but you can choose any
strategy you like okay okay so as it
turns out right so this kind of
algorithm again is something I mean
there's there there are many algorithms
out there and practice that use this
kind of helping between threads but to
actually verify these algorithms is
quite difficult if you're trying to do
thread modular reasoning okay and the
reason is generally if you want to show
that say the tribe or version of push
satisfies its specification the atomic
version of push you have a sort of
one-to-one correspondence right my push
implementation corresponds to one
invocation of the push specification the
problem here is that when the popper
accepts an offer from the pusher
semantically it's doing both the push in
the pop at the same time those two have
to be sequenced adjacently for this to
make any sense and that breaks the kind
of one-to-one correspondence that we had
generally ok so to deal with this we
introduced specification code as another
kind of resource that you get
uh in the logic and I'll show you what
that looks like as we go along here but
basically this is going to allow a
thread to give up access to its spec
code to some other thread that can do an
operation on its behalf and then later
get that code back ok ok so to actually
walk you through this I'm going to
simplify a bit further let's forget
stacks and just think about an even
simpler case of flags ok because if you
negate a flag twice it's the same as
doing nothing so 22 knots can sort of
discover each other and cancel cancel
themselves out ok so the specification
is very simple we have a boolean value
we have two operations you can read the
current value and you can make a tit for
enough of all of these happen atomically
so that's what we're trying to implement
now I'm not going to show you the
implementation straight off it's a bit
complicated I'm going to start by
showing you a protocol that explains
sort of the essential idea so the
implementation in addition to having a
flag has a side channel just like with
the Elimination stack the side channel
takes on three values either it's empty
so the value is zero some thread is
trying to do a negation so there's an
offer on the table or some thread has
acknowledged the existing offer ok and
then the original thread sort of cancels
this out so there's a use of tokens here
which basically tells you that only the
thread that made the original author and
thereby gained the token is allowed to
sort of clear out the channel from to
back to zero ok why is that important
well there's another bit to the protocol
and this is the specification resources
so suppose I'm executing one of these
negations and I have some corresponding
specification thread let's say it's IE
is j and its trying to perform the
corresponding flip operation when i make
this move to the offered state the
protocol demands semantically that i
give up the right to my specification i
should share it ok now some other thread
is allowed to take this step right there
there's there are no tokens involved in
going from here to here but if that if
some other thread takes that
it's obligated to execute the flip
operation right leaving just a unit
behind and then only I who originally
made the offer I'm able to regain
control of this specification resource
so in other words i share it temporarily
but then i'm the only thread that can
get it back later on okay so now let's
walk through this again but actually
looking at the code all right so so this
is a you know I've tried to simplify
this example as much as possible is it
looks a bit strange but bear with me
okay so basically here's how here's how
the algorithm works in detail first we
try to do an honest flip of the flag
right so we guess hey maybe the flag is
true let's slip it to false okay notice
we started here with ownership of one
instance of the flipping spec right that
corresponds to our flipping
implementation here okay so if that
comparing set succeeds everything is
fine right we just execute our
specification to match up that action
right we flip the flag on the
implementation side and so we run the
spec to keep things in sync no problem
okay same thing we can try an honest
flip going in the other direction if we
succeed no problem but if neither of
these two things succeed then we try to
use the elusive like uh that would work
too it's yeah either either way is fine
yeah uh yes yeah yeah I mean this is
right yes yeah okay so another so if we
fail to sort of do an honest flip then
we can try using the side channel to
eliminate some other flip that's out
there okay so you can actually do these
things in basically an arbitrary order
so here we're going to choose to guess
that there's currently an offer on the
table and we're going to accept that
offer right by causing the channel from
one to two
so this will succeed only if the
protocol is currently in the offered
state right in which case there's this
resource here the other guys
specification the thread that made the
author right notice we have our
specification at threadid I and there's
their specification at thread I DJ so if
this succeeds then we gain access to
their specification at which point we
can execute both flips back to that and
that's perfect because the effect of
executing two flips is nothing no change
and indeed we haven't changed the flag
at all here we're about to return unit
without having touched the
implementation flag right so we're
maintaining sort of synchronization
between what's happening in the
implementation and the spec but we don't
have this one-to-one relationship
between implementation flips and spec
flips okay yes yeah yeah so in fact this
started happening quite early in a sort
of that initial diagram I showed of the
family tree of separation logics this is
kind of old news in a sense of coming up
with new notions of resource that
generalize beyond just the concrete keep
right so indeed I am introducing a
particular new notion of resource which
is specification resources but it's sort
of all par for the course for for
separation logics these days so what can
I say okay so I think for the sake of
time I'm going to skip the rest of this
example I think I've showed sort of the
key bit of actually accepting accepting
an offer to flip I want to move on
okay so the last thing I wanted to talk
about then right I've shown you in some
detail right some very simple examples
but you know all this stuff is only
interesting if you can actually scale it
up to deal with some real interesting
examples okay so so we've actually
applied our logic to a pretty
sophisticated algorithm called flat
combining and so I'm going to start by
first explaining the specification for
this algorithm and then I'll give you
the key idea for it okay and and flat
combining is a higher order algorithm in
particular so the idea is basically a
lot like STM we take in some some
sequential piece of code f okay and you
should think think of this as
representing basically an object with
some method you could generalize it to
multiple methods it doesn't really
matter that does some you know
sequential update to some internal state
what we want out of this algorithm is
something that behaves sequentially just
like that original function but that
guarantees mutual exclusion between in
vacations right so this is basically the
same kind of thing that STM is is meant
to do okay so if we wanted to give that
specification more formally we can write
down a piece of code and use refinement
right so what is the specification well
we can introduce a global lock that
guarantees mutual exclusion and so the
wrapped version of the operation just
acquires the lock does the sequential
operation and releases it just like
you'd expect right so this is this is a
specification for Adam icity right or
for something like STM yes
so when user sync app is not equal to
sync it's a yes yes and so this actually
captures so I don't want to get too much
into this but there there's a
distinction between so-called strong
atomicity and weak atomicity and it
exactly comes down to this this kind of
generativity so strong an atom icity is
very difficult to implement efficiently
but weak atomicity is much easier so but
in practice the point is you know you
you have some library like against a a
hash table you've got a good
implementation of you apply this wrapper
once and now you have a new I mean it's
kind of like a functor now you have a
new synchronized hash table right clear
enough ok ok so what's the idea for flat
combining right so so we're going to
attack this problem in a way that's
rather different from how STM
implementations do it basically we're
going to keep around a kind of global
lock but it's going to play a very
different role from the one I just
showed you so here the lock is going to
mediate cooperation between threads kind
of like with the elimination stack the
idea is is basically that if a data
structure is hard to parallel eyes then
give up on parallelization and focus on
cache performance okay that's that's it
it's surprising actually how well this
works in practice for things like stacks
or priority queues or you know where
there's a great tends to be a great deal
of contention over a you know a fixed
location in memory so what we're going
to do is essentially have one thread
become the combiner that does a bunch of
work on behalf of the other threads that
are trying to operate on the data
structure and this is good for cache
performance because that combining
thread pulls a bunch of the data
structure into its local cache and
performs a bunch of operations in
sequence with everything in cash right
as we'll see it also this algorithm also
sort of cuts down on the synchronization
between threads that's necessary so ok
so I'm just going to give you a very
high level picture of the algorithm
basically the representation has two
parts we have this combiner lock that I
mentioned before and then we have a
subscribe a list of subscribing threads
so basically the idea is when when a
thread wants to use this algorithm the
first time it does so it'll add itself
to the list of subscribers excuse me the
stack of subscribers what does that mean
well it adds a subscriber record which
at all times takes on one of two
different values either a request or
response okay so the idea is a thread is
advertising that it's trying to perform
an operation on this data structure with
argument X and then later the combiner
can respond with some answer why and
what we're going to try to do again is
have one thread execute many operations
on behalf of others with minimal
synchronization okay now one key thing
is that this subscriber stack can change
concurrently while while the combiner is
actually doing its work right so we're
not using this lock to protect access to
the stack in fact we're using a lock
free stock right so this is all changing
underfoot we're just using this lot to
make sure only one thread is acting as
the combiner at a time so so so so
generally you don't want to pop elements
so okay pushing on to the subscriber
stack is going to take a compare and set
right so you want to pay that
synchronization cost only once upfront
and then you keep your record around you
reuse that same record for as long as
you're interested in the data structure
once once for subscriber that's right
then the point is exactly and and the
reason this is so important is once you
have your subscriber record in the stack
then you can just do a normal right no
no compare and set to make a request
because this is your record no other
thread is going to make requests on your
record right and then the combiner will
at some point perhaps notice that this
request is there and sir
visit okay okay but of course some
threat needs to become the combiner so
wait so the general algorithm is you
know after you've added yourself to the
stack and you made your request you can
you sort of check the combiner lock see
whether some thread is currently
combining if it's not you try to become
the combiner and otherwise you wait
around for the combiner to do the
operation on your behalf okay visit busy
wait indeed and as you're doing so you
need to keep an eye on the combiner lock
because there's there's a race condition
here right that you might have appeared
just as the combiner was finishing right
makes sense okay so so again it's a bit
weird if you're not familiar with these
kinds of algorithms but the whole point
of this is we're trying to minimize
cache misses sort of across-the-board
minimize use of operations like comparin
set okay all right so I'm I'm running
out of time so I'm just going to very
briefly you know sort of show you how we
capture the idea of this algorithms of
protocol so we have one global protocol
similar to the one I showed you before
for governing the combiner lock then we
have a subscriber record protocol that
governs each subscriber record
independently right so we have one copy
of this protocol per record the bottom
state means the record hasn't been
created yet basically so when you first
create the record you do so by making a
request and you say I have some spec
thread J that's trying to perform that
request and just like before with
elimination stacks you have to give up
your give up the right to that
specification then the combiner can
actually move to the execution state and
gain access to your spec resource okay
so I've annotated these edges with the
tokens that are required to actually
make make the move okay so so the point
it and and i should say this this l
annotation is represents the thread that
actually owns this subscriber record so
only that thread is allowed to make
those moves okay so this is the thread
trying to do an operation is allowed to
request it the combiner is allowed to
perform it and then the threat trying to
do the operation is able to acknowledge
it and take back
ownership of the Specht just like in the
Elimination flag example I said before
okay so that at a high level that's
that's how we can deal with the flat
combining protocol but then you know I
sort of mentioned in passing that we're
using Treiber stack internally in this
data structure so I just want to now
take a step back and say how modular how
the modularity of carousel sort of
enabled us to factor this proof right so
I showed you before how Treiber stack
refined some atomic spec which we can
abstract using something like a bag and
that's exactly what we do here we don't
actually care about the order of the
subscriber records we just want to know
that when we iterate over them there's
something that's true about each of
those records namely that it's
participating in the protocol I just
showed you okay and then in actually
proving this satisfaction we use a
couple of other data structures right so
we use we need to reason about the lock
that's present in this atomic stack we
do so using horse style reasoning and we
need to reason about lists right that
we're using for iteration which again we
do so using horse style breathing so we
use horse style reasoning improving this
satisfaction and then we use all of this
stuff in actually proving the refinement
at the end of the day okay so even if
you don't understand all these details
the point I'm trying to make here is
that we can freely mix these two modes
of reasoning refinement and satisfaction
and build up a modular proof that
matches the modular data structure that
we developed okay I'm going to skip that
little footnote for the sake of time
okay so we've covered sort of the three
aspects of the talk so now let me just
make a couple concluding remarks so flat
combining is not the only interesting
algorithm we studied we've looked at a
number of common lock-free data
structures that are actively used in
practice and and done fully formal
proofs of these data structures and here
just sort of I'm sorry no no no so yes
but I mean yeah so it sink
comparison to eat you know each of these
papers contains a proof of correctness
that's not formal at all right and so
what we've done is actually on paper
done a complete proof in a program logic
yep okay so that's just to give you an
indication of where we are with carousel
and where we'd like to go well we want
to keep sort of this forward march of
modularity that I showed you at the
beginning we want to scale this up
further to reason about more interesting
higher-order concurrency programming so
if you're familiar with for example John
rupees concurrent ml that's very close
to a monadic interface to concurrent
programming and right now essentially no
logic is able to scale up to two
actually reason about this style of
programming right so now that we can
tackle higher-order we have some hope of
doing this but there are a lot of
problems to address before we can go all
the way also so I focused only on safety
properties we'd like to tackle liveness
as well and there are some actually very
interesting problems in the model theory
to make this work and then also of
course you know I'm sort of surprised I
didn't get this question so the memory
model that we're using here is
sequentially consistent and that's an
increasingly sort of questionable thing
to do but essentially all program logics
at the moment you know work at that
level although you know there are some
sort of forthcoming proposals for doing
better so we'd also like to reason about
relaxed memory with art with our logic
eventually okay so just to sum up so
parasol is a logic that supports two
modes two new modes of modularity
granularity abstraction and higher-order
functions and it does this by bringing
together refinement and horse style
reasoning thank you very much
excuse me Oh concurrent and refined
separation logic oh the a is for ant
yeah yes yeah they're related um so yeah
you can think it's fair to think of it
that way yeah I mean but formally
speaking they are two different
properties so one would need to prove
that they're equivalent which they are
in certain languages but not all yes
uh yes I'm optimistic I mean I think so
what okay but more seriously on so one
thing that is actually really surprising
to me about the framework that we've
been using of logical relations is how
orthogonal the different components are
so like scaling to higher order we
really didn't have to say anything
specific about the interaction between
that and concurrency it just kind of
fell out so my hope is that we'll be
able to change the memory model without
having to change you know much of the
sort of general supporting framework of
logical relations but but it's really
early days I mean I haven't really
started looking at that in detail that
it's certainly possible to do things
that way so there's already so on in
this year's poeple there is a paper
about library abstraction for weak
memory for c11 in particular and
basically they were advocating exactly
this kind of refinement as a way of
specifying weak memory data structures
okay but whether in the long run that's
sort of a sustainable way to go is
unclear right there I mean there are
some problems with specifying code by
writing more code right so another
approach would be to increase the power
of your specification language right so
that you can say you can talk about Adam
icity directly but without having to
actually write code that that just is
that you think is obviously atomic right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>