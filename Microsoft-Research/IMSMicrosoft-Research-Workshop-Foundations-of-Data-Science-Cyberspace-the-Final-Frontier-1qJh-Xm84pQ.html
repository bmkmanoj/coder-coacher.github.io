<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IMS-Microsoft Research Workshop: Foundations of Data Science - Cyberspace, the Final Frontier | Coder Coacher - Coaching Coders</title><meta content="IMS-Microsoft Research Workshop: Foundations of Data Science - Cyberspace, the Final Frontier - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IMS-Microsoft Research Workshop: Foundations of Data Science - Cyberspace, the Final Frontier</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1qJh-Xm84pQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay I think we should get started this
is the second session uh where we're
going to talk about data science
challenges in earth and space science
we're split here we have one talk for
lunch and then three talks after lunch I
think I get five minutes to say
something and I'll try not to use it all
but I do want to thank IMS and Microsoft
for hosting this event and for
supporting it my name is Amy Braverman
I'm from the jet propulsion laboratory
in pasadena in case you probably do know
about us if you follow the Mars rovers
and Mars landings and things like that
asteroid crashing things into asteroids
that's us um we are a lab of Caltech or
basically a national lab we're funded by
NASA and operated by Caltech so there's
a very tight relationship between JPL
and Caltech we have many many many data
science problems everybody at JPL does
data science whether they know it or not
cuz everybody's has data that's all
we're about as data collecting it
analyzing it and turning it into data
products that the larger worldwide
scientific community can use for their
science so what I had hoped to
accomplish here was to let you know that
we are very actively involved in data
science and acquaint you with some of
the problems that we have because
there's there's a lot of room for help
we do have a NASA supports a fairly
vigorous machine learning community less
less so on the statistical theory and
statistical methods front but we're
trying to change that so I've invited
two colleagues from Statistics to join
us today they're both working with me on
various projects that's y any and
Mattias I've invited George to George
offski to talk to us about the larger
landscape of data science in the NASA
and space science community George is no
stranger to any of us that have been
sort of roaming around the data science
world or its precursor the massive
dataset world he was involved for a long
time in helping to formalize that he's a
professor of astronomy at Caltech the
director for the center of data-driven
discovery and has many other credentials
that I'll let you read about on the
world wide web so thank you George wow
thank you Amy good morning everybody
first let me apologize for the bogus
title and abstract in the program those
were placeholders that were never
replaced and if that abstract reads like
 that's because that's what it
is I give that talk to but you're
different audience so what I'd like to
talk to you about are some actual
specific challenges that we're dealing
with now or anticipate dealing with so
as Amy mentioned we have this brand-new
Research Center at Caltech called Center
for data-driven discovery and it's in a
close partnership with the Center for
data science and technology at JPL our
goal is to obviously provide the
expertise in various aspects of data
intensive computational science to
research groups Caltech and JPL but in
my mind the core intellectual function
of our Center is what I call methodology
transfer trying to reuse solutions
developed for one domain it somewhere
else because I believe that there is an
underlying Universal methodology for
data science that's emerging now and
it's silly to try to invent the wheels
that make Muslims square so for example
one of our first projects we were doing
is we're using some of the data
analytics tools that are based on
machine learning and statistics that we
developed to analyze data from sky
surveys and now applying them in
neurobiology and we're already have some
interesting results with our colleagues
for example we probably have better
diagnosis for autism than anybody before
it's just a start we're doubling into
cancer research geophysics all kinds of
stuff okay so what I would like to tell
you about are some examples of
challenges where I think we could use
help of statisticians and machine
learning experts first I'll tell you
some about those that were working on
now there
drawn from astronomy because that's my
feel that's what I not talk about and
they had to deal with what's I think it
really a more general problem or
challenge or opportunity of extracting
knowledge from massive data streams not
just data sets the data stream amenia to
process it as you go along and that is
driven both by necessity otherwise
you'll never catch up but also by
opportunity because by being able to
react quickly to interesting things that
happen in time domain you have new
opportunities for discovery and also
talked about several broader challenges
I think are coming up in front of us and
by necessity this will be very
superficial coverage but I'm happy to
talk to anybody later in about details
with any of this so here is one slide
summary of what's an emerging vibrant
field in a strong meets time domain and
it's enabled by growth of technology
that we now can make movies of the sky
sky surveys were major source of data in
astronomy for a long time they ushered
us into terascale era 20 years ago we're
now well into petascale here are going
to exascale and instead of taking
panoramic images of the sky we now take
them again and again and again and
compare them in real time to what was
there before looking for things to
change move like her killer asteroids
for example or things that go bang you
tonight variable sources exploding
sources and so on there's a lot of
exciting science that goes from solar
system all the way to cosmology and
these surveys are by far the biggest
source of data in astronomy today it's
not the big telescopes like the CAC or
your Hubble it is these little
telescopes with a lot of programming
detectors to generate rules and rules of
data which then lead to follow up with
other machinery later on and this is
just the for taste of things to come
because things that have been planned
now are much bigger surveys part
synoptic survey telescope in Chile and
optical Square Kilometre Array in South
Africa Australia in radio and so
and so that will increase all of our
challenges in going from data to
knowledge by several orders of magnitude
in the scope of next ten years or so so
we better learn now an example of what
this is is a small relatively small
survey that we run at Caltech it's
called Catalina real-time transient
survey basically we monitor 500 million
objects in the sky and how they change
their brightness now in most cases these
surveys discover things but to get any
science you have to follow them up with
additional observations to understand
what's going on and there is simply not
enough resources to follow them up all
and this is where I classification and
automated follow-up are crucial problems
for the science so just to give you an
example basically all of them look the
same so something looks like a star it's
brighter tonight then used to be before
or maybe wasn't there before and they
all look the same that's all you know
about them at first but they can be
vastly different physical phenomena from
relative cosmic neighborhood all the way
to the other end of the universe and
from something is like solar flare is
all the way to biggest relativistic
explosions and some of them a lot more
interesting than others but you don't
know upfront which ones you got and so
this is why rapid classification is
crucial in order to select the most
interesting ones and then there is an
even bigger machine learning problem how
to decide to deploy your resources in
actually following them up well the
reason why this is hard is as I said
it's not your grandma's specification
problem that unlike textbook examples
the data are very sparse incomplete
heterogeneous are always different you
cannot use the standard back you know
feature vector approaches by and large
although there is a trick that I'll tell
you about and in fact most of the
information you have about that spot in
the sky is not from the survey it's from
what was no
about that spot before and distributed
archives that are unified in virtual
observatory framework that you have to
pull all that prior information because
that's what tells you what might be
going on and then you have to combine
that dynamically with the new data as
they're coming in so because we're
talking about huge data rates humans are
completely overwhelmed by this you want
to do it also repeatably reliably so
this is why we have to do this in an
ultimate fashion and then there is this
other problem of actually being able to
follow it now okay so we tried a whole
bunch of different things this has been
going on for about 10 years they're like
three big research groups involved in
this and you'll recognize some of the
buzzwords here I will not go into any of
this but there is one thing that I think
might be useful in other contexts
because basically what you're dealing
with here is a time series of
measurements of some sort in our case
it's the flux from celestial source
which we call a light curve but it's
just the time series and the problem as
I indicated already is that they're
always different they're different
numbers of data points cadence is
different signal to noise is different
you know all that kind of stuff and so
you cannot use the data themselves as an
input into the machine learning
machinery so the trick we come up with
is that instead of using data themselves
we evaluate a whole bunch of statistical
descriptors of the time series which to
first order do not depend on how many
data points and so on and that gives us
a homogeneous set of data or feature
vectors that now have every slot filled
and those we can feed into the standard
machinery and do the classification so
this is actually real data about quasars
in some observable parameter space but
things are usually not nearly this nice
this is nice separation there I mean by
large things look much much worse and so
once we have feature vectors in there
about 72 of those features we have to
decide which ones are you going to use
for which problem we have for example in
Bayesian networks there is a super
exponential expansion of computational
complexity as you increase the number of
dimensions and so there is a high
premium on figuring out which you give
you most information so we apply variety
of feature selection tools in here is an
example of two light curves of two very
different kinds of stars and you'll be
hard-pressed to tell that there is any
difference between them but when you
select the top three features you get
nice and clean separation parameter
space between those two very different
kinds of objects so we're doing a lot of
work on this because different features
work in different kinds of
classification problem so the machine
learning answers that istic answer
depends on what is the question that
you're asking right are you separating
supernovas from something else so it
does kind of start Afghan star and so
there is a lot of work that needs to be
done that then after we do all these
classifications we have a problem of
making the final decision what I called
meta classification that different
classifiers will tell us different
things because a priori we don't know
which is the case that we should be
using to optimize a choice and so there
is the question of combining an
intelligent adaptive way output of
multiple classifiers so they have one
optimal decision which can change in
time as new data come in and again we're
experimenting with how bunch of
different methods on how to do this so
once that's done we get into the real
hard problem so we have surveys that
provide these transient events then in
the first box event classification is
where first magic happens that in jest
the data from data streams archives and
so on and that goes into a bigger box
that even more magic has to happen where
you have to decide which of your sparse
follow up resources you can use to get
further data and there are multiple cost
functions and currencies at work here
first how much of a given resource you
have when
because Earth turns around there is
weather and so on you have a finite
applications of telescope time do you
want to use it all up now oh wait or
something better what's scientifically
more interesting how do you quantify
that and which things will give you best
discrimination between classes and so on
and so essentially we're trying to build
a research ecosystem of robotic
telescopes and computers that will
perform reliably in robustly and without
human intervention and the idea is that
indeed a stronger sleep nice nice little
night wakes up has an espresso and looks
what the robots have discovered last
night the next step use costs of course
to have them write papers but okay so
let me tell you but I'm another cool new
result which is not from time critical
but it is time the main astronomy by
doing archival research on databases of
these time series of 500 million of them
so we now know that most big galaxies
have big black holes in their cores and
sometimes they're active by accreting
material releasing huge amounts of
energy those are quasars we also know
that galaxies evolve largely through
merging hierarchical assembly when
galaxies merge of entry the black hole
set to merge when that happens the
amount of energy that's supposedly
released in those events is
approximately 10 billion supernova
explosions at once mostly in
gravitational waves and that's what
people building the LIGO and other
detector is hoping to see and so be a
really important thing to actually pin
this down the problem is that once these
two black holes are close enough they're
approaching the merger they cannot be
resolved by any current or anticipated
instrument yes Tommy you're way too
close and so we need something else so
the we use time domain as a probe
looking for periodic variability that
will correspond to the orbital motion
but the problem is that's proposed to
what's inherently correlated noise of
accretion and so we played with various
ways and we have very good ways now
actually
detecting such signals superposed on
intrinsically variable signal we did
this on 300,000 quasars that we cover
and roundabouts countered candidates
this is by far the best but this is
deluxe version right mostly they don't
look nearly that nice right and tells
can a big deal at first time we had such
an evidence for black holes heading
towards a big collision I thought this
was a good illustration of how you have
big data you must have big data in order
to find rare things and you need good
tools to be able to do that as well okay
so now let me switch the gears and talk
about a little more general stuff and if
if you think about it the role of
science in general is to reduce extreme
complexity and messy phenomenology of
the world into what we call laws of
nature simple rules that will describe
why something happens and the problem is
that all simple stuffs been done we're
no longer talking about apples falling
from trees we're not talking about star
formation and biology and ecosystems and
so on and in fact most of those things
probably a priori don't have analytical
expressions so on the other hand there
is also the question of how complicated
are these new laws of nature with trying
to discover by exploring amazing new
datasets that we have and there is no
reason why should they be simple enough
for not for us to understand so there
probably are real important laws of
nature that you just can't wrap your
mind around at once and so we will need
some artificial intelligence help in
doing that so a simple example this is
the curse of dimensionality so suppose
that you you are so lucky that you can
express your problem as a as a set of
feature vectors in some highly
dimensional data set this is just a
simple projection of some of the real
data from our survey incidentally it's
something else we're doing developing
new ways of visualizing highly
data is in virtual reality as a platform
and we can't wait to get our hot little
hands and hollow lands somebody has one
relating around it yeah and so the
question is well how do you actually
explore such high dimensionality
datasets because most algorithms scale
very badly with increase of
dimensionality some power of dimension
especially if you want to do montecarlo
or anything like that well here is what
I think is the generic problem with this
business so you have some highly
dimensional spaces multi dimensional
hypercube which I can only illustrate in
3d and there is a cloud of data in it
very messy and most of the data do not
contain useful information their noise
of some sort right but somewhere in some
corner of some particular projection of
that space there is something
interesting going on correlation or
cluster or something and something
that's different from noise and the
question is how you discover such things
so there is a 20 dimensional projection
of a thousand dimensional data space and
in it there is some multivariate
correlation going on well just about
anything we have available to us now
just simply does not scale well please
so the question is how do you develop
generic tools to approach this problem
because you don't a priori know what's
the form of your noise it means not be
Gaussian it's never girls or poissonian
it can be very complicated you don't
know what you're looking for you just
know it's different from noise so there
is some sort of change of some entropy
like function that describes this
phenomenon that you are supposed to
discover and that I think is common to
just about every area of big data
science so how to do this other than
just simple brute force which basically
doesn't work
another big question I think Jeff jewel
talked about this in more detail is
quantifying model uncertainty by model I
mean two different things really there
is data model that you know means like I
applied neural network on something and
it produces classification a B and C and
so that has certain assumptions built in
or it could be a numerical simulation of
sampling a climate say and then there
are many different sources of
uncertainty in all those that depend not
just on the data but in accuracy of your
computing but also on the choices of
algorithms choices of the workflows and
so on and so forth and this is obviously
crucial for just about any field that
relies on large data sets in numerical
simulations in aspires to predict
anything and climate change being an
obvious example and you probably know
that most of the climate deniers always
talking about all but you know these
guys don't know it's gonna be two
degrees of five degrees right and so
it'd be really good if there is a clean
reliable way to say this is what we
think uncertainty band is and of course
this appears again in design and
anything else you may think about so
this is one aspect of quantifying of
uncertainty that I think will come back
again and again I think this is prime
territory for statisticians to work
together with machine learning in
experts in domain experts to really put
data science and solid basis because so
far most of it is very much ad-hoc
people kind of do something sensible or
run multiple models the highest and
lowest or whatever but having really
well-established methodology in same
sense that you know how we deploy
statistics and simple lab experiments
would be very good now so we will
basically think we're moving towards
synergistic discovery what lik leader
was talking about 50 years ago
collaborative human-computer discovery
and he didn't have technology to do it
but now we do and so we use all these
machine learning tools already
more or less cottage industry fashion
but some of the problems I already
alluded to are clearly in this domain of
human minds needing assistance from
silicon based minds in discovering
interesting stuff so simple example that
is the set of techniques that discover
functional forms to describe the data
symbolic regression is usually used and
there is particular implementation of it
from hot lipman and company Cornell
essentially what you have is toolbox to
which you feed a library of mathematical
functions you choose all usual
arithmetic maybe trig make yeah whatever
and the program discovers what are the
mathematical expressions that best
describe your data without knowing
anything about what the data are all
about so in most cases we're thinking
machine learning as doing classification
finding correlations clusters and sown
in this case it's finding functional
forms that describes acting which may or
may not have any meaning whatsoever now
we tested it to rediscover some of the
known things in astronomy and it did
pretty good then we actually give it a
new problem to do and it was a game
classification problem so by finding a
function of some subset of parameters
and then multiplying it with Heaviside
function so on to the lefties like one
and to the right it's tagged to it
cannot it can work as a binary
classifier and it did as well as any of
the other tools we did on the first go
yeah very high accuracy so again here is
an example of two totally different
kinds of variable stars you look at the
data what's going on and the program
fine right kind of functions some some
parts of which we understood in some
parts which were new to us which
actually performed as well as anything
else we tried before and we tested it
with the independent day
time you can see there is a very clean
separation of two types in particular
parameter space program didn't know
about this now here is the question not
only do we have uncertainties associated
with the output it can stems from the
data themselves in number of points
measurement whatever here the question
is how reliable is the functional choice
that program is making or found for a
bunch of local Maxima you want right how
do you know that that's the right
expression to be used which you can then
try to interpret and what is the
uncertainty associated with that
expression in terms of its parameters or
functional and so on so this is I think
even more complicated approach to
quantifying uncertainty and I haven't
seen anything whatsoever that would
actually help us in this regard finally
we are trying to do the synergistic
collaborative computer-human learning by
trying to harvest human pattern
recognition skills and sometimes this is
done through crowdsourcing you have
people look at pictures we have
fantastic pattern recognition system in
our head and you can click on something
or describe something very quickly and
so the question is how do you harvest
this turned us into semantics
descriptors which you can then feed into
scalable algorithms because you can do
this for relatively modest amount of
data but it absolutely does not scale to
the even current never mind for coming
data streams because human time and
attention are by far the most precious
commodity we have so that to them has a
whole another slew of uncertainties
because humans make mistakes and how do
you actually assess how well is your
machine learning system learning on the
basis of this human input and what's the
real
ability of the other decisions it will
start making okay so here is the very
quick summary of some of these in terms
of time domain astronomy or rather any
kind of research that deals with real
time processing of massive complex data
streams and requires instant or a rapid
reaction and follow-up measurements
their challenges involving
classification of interesting events
that occur in what's usually very messy
incomplete noisy environment and you
want to optimize this you don't have any
false alarms and don't miss anything
important and so on it could be English
floating in the sky could be your
environmental sensors could be security
applications and then how do you also
optimize the decision-making of which of
your sparse resources are you going to
deploy when so our colleagues at JPL are
worrying about this in the context of
spacecraft networks say rovers and
orbiters around Mars to decide on their
own which things to follow up because
they are severely bandwidth limited and
they cannot even send all the data just
like a few percent possible then there
is a generic problem of the pattern
discovery in high dimensionality data
spaces which are populated by noisy data
of some sort and there is a whole set of
these quantity quantifications of
uncertainties in models themselves in
the emerging arena of machine assisted
discovery of functional forms not just
descriptions of the data and now in
symbiosis between human and computer in
doing this collaborative learning how do
you then approach that so those are all
I think very big problems and I think
every one of them is relevant for broad
range of data science and it's going to
be only more so thank
well thank you George um I think we have
a few minutes for questions if they're
already yes I just I have a quick
comment so I enjoyed your eye I like the
point that you are feeding some many
statistics instead of the entire data
too so that has a precedent in bayesian
statistics it's called the proximate
based computation it actually started in
population genetics so you might find it
useful to look at the literature and
that this is exactly what stuff yes
you're right I mean I've cut many
corners in the stock we did play
extensively with Bayesian methods at
first precisely for these reasons right
but yes yes you talk about the problem
of noisy data and I'm wondering whether
it's possible to leverage the prior
knowledge about our literature to help
solving this problem I'm thinking you
some literature articles might say at
this spot on its location in the sky we
have observed a soothing phenomena and
if we catch mining and by the lab which
literally there are bits relative that
information that might guide us to
select data points this is actually
three different things you're talking
about absolutely using archival data
which includes literature in
classification process is part of that
automated classification but that's not
noise what I was talking about is actual
noisy random data that you don't care
about a top of which there is something
interesting happening right and you
don't know a priori which data points
are good which are not in that's it
ignore sky it's this parameter space
we're talking about there are many there
are points that you tried the phone died
and you're not sure which part might be
more important than that if some
locations on engineering research
literature about back then we can assume
the the points that are close to that
location
might be more useful I don't know it
makes sense because I don't know much
about that I see what you're saying but
short answer is no and you shouldn't
think about pastrami at all this you're
talking about generic parameter spaces
and finally the question of
incorporating expertise is to me part of
this business of collaborative human
computer learning because when an
astronomer or student looks at the
picture and says oh this is supernova be
yeah they're using some domain expertise
they have unfortunately most of the
crowd source citizen size project
basically use people's click through
slaves and we're trying to do something
little more smart where you can actually
harvest whatever expertise somebody has
and you don't know how much expertise
they have until you can quantify it in
some good way because some people be
really good at making the right choice
and some people be awful and they all
think they're good right so there's
that's yet another problem any other
questions let's pop your love be a
psychologist you talk about the power of
a human visual system for pattern
recognition there are also some real
biases in what it is going as she and so
like linear patterns are way easier to
detect than some that have some honor
happen if you thought at all about data
transformations that you might use to
facilitate things people
just something yeah I mean they're the
usual thing is well how do you know this
doesn't say a chance alignment of
something or other and so yes absolutely
you know there'll be patterns which may
be spurious that we just due to chance
that people pick up patterns that are
not easily processed by the possible no
I I was always pop up what visual
analytics really means and yeah i think
we this is why this has to be a
collaborative human-machine discovery
which computer should keep you and
straight and narrow right there yeah
you're pointing to a whole another can
of worms because of course we see a lot
of stuff that we actually don't see
there are very few pixels in your eye
and your brain interpolates a lot what
you think you're seeing like the famous
go rely in the elevator experiment right
all right yeah so is there a possibility
putting some of the data on making it
online available i mean i like some of
this kind of streaming astronomy data
like there's a lack of kind of really
big interesting big ones that are
available they are all online already
and in fact we are strong believers in
what i call open data philosophy that we
make all of our data available to the
whole world immediately warts and all
and we add data products the top of that
and this is because we can only do maybe
one percent of all science that's
enabled by the data so it's
irresponsible not to let the rest of the
world got it and so all of our data are
available online i can talk to me if
you're interested alright thank you
George I'd like to make one follow-up
comment which is just that for some of
the problems that you mentioned there
are some statistical formalisms out
there like the model discrepancy problem
how you quantify the inadequacy of your
either your machine learning model or
your statistical model or even your
climate model but the challenge i think
is to figure out how to turn that into
an actual algorithm that can actually be
applied to actual data when those data
are massive
I didn't mean to imply that there is no
work understand already I know that
there is a huge amount of literature and
effort in all of these arenas but
whatever is out there in professional
statistics journals has not trickled
down to the proletariat like myself that
actually tried something really so
that's that's a that's a very fine point
at which to go to launch on so I'd like
to say thank you George and please come
back at 130 for the other three speakers
in the session thank you each year
microsoft research helps hundreds of
influential speakers from around the
world including leading scientists
renowned experts in technology book
authors and leading academics and makes
videos of these lectures freely
available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>