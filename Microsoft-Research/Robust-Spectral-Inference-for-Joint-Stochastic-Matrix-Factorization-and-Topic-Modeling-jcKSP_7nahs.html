<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust Spectral Inference for Joint Stochastic Matrix Factorization and Topic Modeling | Coder Coacher - Coaching Coders</title><meta content="Robust Spectral Inference for Joint Stochastic Matrix Factorization and Topic Modeling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Robust Spectral Inference for Joint Stochastic Matrix Factorization and Topic Modeling</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jcKSP_7nahs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay walk welcome to this lecture we
think monk high flying all the way from
a canal to come here for this week so he
would pop up to us about our topic
modeling and this is Ruby that are just
paper he will present upcoming nips so
we get a preview of his work he's a
extremely productive researcher and our
intern and we appreciate that he spent a
lot more with us ok thank you bye thanks
for the introduction so today I will
present robust spectral inference for
join stochastic matrix factorization and
also for topping modeling this is a
joint work with David bindle and David
meme no professors in Crennel university
before going into the outline I will
briefly talk about what's the difference
between several learning models so in
machine learning in order to learn the
model parameter we have roughly two
different method one is likely based
training we first chooses a purple
likelihood estimator of course there are
a lot of different estimators like
pseudo likelihood or maximum likelihood
map likelihood and then we form a likely
to function in terms of the model
parameters and then find the best
parameter usually via optimization there
are another class of method called
method of moments so which is first
relate of population moments to the
model parameters and then estimate
population moments via sample and it
usually involves solving multiple
different equations so if i compare this
two different approach to learn
parameters the solving likely you'd
usually uses optimization which
requires multiple iteration that makes
the algorithm usually slow whereas
method of moments we are solving
equations closed-form equations and
speed is relatively fast and in terms of
the estimation quality unless the likely
function the design is strictly convex
it's not always obtain more whereas the
method of moments it's statistically
consistent what I mean by statistically
consistent is if we get more and more
sample it always converges to the right
estimator but whenever there is a
mismatch between our models and the real
data likely based method has intrinsic
power to manage model mismatch but it's
unclear in the method of moments so one
of the biggest focus of this talk is how
to handle model mismatch in a specific
problem related to topping model and the
method that I'm interested in this paper
is using both matrix algebra and
probabilistic inference together in the
same framework then so I will briefly
explain the latent Deerslayer location
so we have two hard parameters for a
multinomial distribution and for each
document and end position we first
sample the topic from the topic
distribution of that document and based
on that topic b sample the word and this
is how the topping model explained the
document generation if i propose the
corresponding view in terms of matrix
factorization it looks like this so we
have a word distribution for each topic
and topic distribution for each document
and
and because of this is a distribution
this matrix is a column stochastic which
means the sum of every column entries
are equal to one so let's say we have
currently only one document in the
right-hand side and this document the
first document it is the first topic is
the biggest topic which affects a lot
then what we are expected to observe in
terms of the word level is the
duplication of these words first word
and fourth and fifth word and if the
second document contains the second
topics and forth topics a lot and then
the expected document looks like the sum
of these these these words and these
these these words and but in reality in
reality always the model is not
coinciding with the real data so we are
observing these noise in here now I'm
explaining the event space for join
stochastic matrix factorization so in
comparing to the previous lda model now
we have a pair of topics and pair of
warriors here so these pair of topics
are sampled from topic topic
distribution and then based on those
pair of topic we are sampling the words
and words are of course related to the
word topic distribution in here so the
corresponding matrix vectorization view
look like this so now we have a topic
topic matrix the sum of all entry will
be equal to 1 so this is a joint
stochastic matrix and we have more
tapping matrix here and transpose here
what be
observe is here this is a word word
co-occurrence matrix or just empty so in
reality what we observe is containing
some degree of nuances like this so the
goal in this paper is to decompose what
we observed into this fashion okay so i
will explain why we are using
second-order method rather than the
first order method so the benefit which
is proven in aurora at all 2012
unfortunately the first order statistics
which is simple word occurrence in each
document is far from the ideal stuff but
fortunately the second order word word
co-occurrence matrix converges well to
the ideal word word co-occurrence this
is proved mathematically so the goal is
to decompose the newest observation of
our co-occurrence matrix into those be a
B transpose and what is called inference
here is to recover the word topping
matrix B and in order to do that we are
going to utilize something called
separuh bility assumption what I mean by
that is the typical non-negative matrix
factorization the goal is to minimize
the difference between C and B a B
transpose usually in terms of frobenius
norm but here just merely doing that
will not give us a great result and
usually it produces unrecognizable
topics it's because there's no identify
abilities guarantee in some sense this
is a probability assumption which I'm
going to explain in the next slide will
guarantee there was identify ability to
this problem so again just minimizing
see NBA
the difference between CNB a B transpose
is not the goal of this paper so what is
separable ax t is a probability
assumption each topic a has a specific
anchor word as K which is exclusive to
that topic what I mean by exclusive is
described in here so when that topic is
given we have a positive probability to
observe that anchor world whereas given
different topic there is no prob ability
to observe that specific word this
implies not every document about topic a
must contain that anchor word but every
document which contains that anchor word
will tells a list something about that
topic then what i mean by anchor word
defined in the previous slide correspond
to this red blocks here so this red
block is entirely dedicated to the
second topic and this intellij decade to
the first topic because there is no
probability of seeing this word fifth
word given another topic so we have
three anchors in this picture valid this
assumption is valid is something valid a
lot oh so in real data it's of course
not always valid but without this
assumption as i said the decomposition
doesn't have an identifiability so
usually all these on supervised learning
we would like to identify the mixture
different mixture topping mixture for
example in this problem that those
mixture are not sufficiently separated
to each other without this assumption
these will group some different
publix it why oh no this actually
separate different topics more
distinctively the Viper set assumption
doesn't know then it's possible that you
see the same ankle what gives several
different topics and those all those you
are different hobbies value me gustas oh
it's different cause topic is not
because ur top it is an hidden variable
so if we have this assumption what we
would like to do in the inference is to
create topic which satisfies this
assumption as much as possible as we can
and because these anchors are exclusive
to one topic it actually tries to learn
topic as separated as possible topic is
now we can observe yeah so if I reorder
these and push all these red blocks into
the above and reorder then it means B
matrix contains diagonal matrix in here
which is called D in this notation so
that basically this decomposition will
be re-written into this block diagonal
matrix form and there's already one
interesting correspondence so if i see
this first blog after the renumeration
then the da d transpose will correspond
to certain matrix of the co-occurrence
matrix so this is those types an
extension of the original topic model
the two branches oh is it at the same
type of model of the different women
it's not actually exactly same topping
model Kozma we no longer have the prior
for the word topic matrix part but it
actually subsumed some degree of those
all LGA different models so you will see
I want to open miss something so can't
there be to anchor words in the same
column to anchor wars in the same column
in here yeah I know there is no way to
get there was two same anchors in the
same column cause and covert assumption
means if inserting rows there is only
one activated cell yeah and the number
of those rows is also hyper parameter
what I mean condition says if you have
to be the only non-white thing in your
road doesn't say anything about your
column so I don't see why you can't have
two red ones you could have a the last
row could be identical to the next to
last row oh so again this is not we
can't observe this is not we can observe
be so we what we can observe is only see
then as an user input for example in
this picture user oh I'd like to learn
topping model with topic 3 number of
topic 3 then we are going to decompose
this matrix assuming there is three
certain blocks in a ok yeah so it's just
another requirement for anchor word uh
it's not another requirement that's how
the inference goes in terms of the
directionality now you will see the
details go ahead so put it another way
is that me even if you have like to pack
a world with thing ok just ignore what
just for me really just using one uncle
word that's enough ah no cuz if you'd
like to separate the data set into 10
different mixture you need 10 and coors
basically that's so because you are
assuming you have a diagonal which
it's a top yep which means for each
column the only I have one as well yeah
apartment is a is possible that for some
topical Muppets or some topics you have
more than one and good boy that is not
allowed in this model yep so every topic
must have only one anchor word so in
2014 there is an extension theoretical
extension so every topic has multiple
different n koers I haven't seen any
real implementation of that paper but
there's an extension yeah so if I have
an anchor word this is the one of the
main inference how to find the C bar IJ
is a row normalized version of word word
co-occurrence matrix so now it's
conditional probability given observing
one word what's the probability of
observing another word and because of
this equation in the end these relation
will behold which means so some row it
rose in the row normalized co-occurrence
matrix will be convex combination of
certain rows corresponding to anchor
words and these the coefficient the sum
of this coefficient will be equal to 1
and these corresponding to the our
topping matrix that we've seen in the
previous slide so assuming we somehow
know the anchor word the rest of the
inference is just to figure out these
coefficient so assume we somehow learn
those anchor word and then to solve the
previous equation is just to solve non
negative list clear its simplex
constraint there will be many different
method all right all use exponentiate a
great gradient and there is also active
mÃ©thode and this is easily
parallelizable for each word that's one
of the biggest benefit and then the
inference becomes very simple Bayes rule
so this will be one entry in that B
matrix I k entry and once we know these
convex coefficient based on these
methode the all these entry could be
rewritten based on Bayes rule so finding
anchors really matter so at the
beginning a rattle in 2012 they try to
solve a lot of LP basically pick one row
in the co-occurrence matrix and see
whether it could be reconstruct it could
be reconstructed by all the other rows
which is pretty exhaustive methode and
it empirically doesn't work at all and
then people developed or use QR with
rope avoiding which is very famous
method in that matrix algebra so pick
one extreme point in those row
normalized co-occurrence matrix as
initial anchor and projects every other
point rows down to the orthogonal
complement of that vector and choose the
farthest point and repeat this process
again and again until we find K and
curse of course these K anchors will
never recover the rest of the rows
perfectly and what we are doing is just
approximately find best k rose and this
is a greedy met though so it's not even
the best but something we could do in a
manageable fashion so benefits of this
encourage algorithm is as you could
imagine the every inference process is
deterministic so there is no random
initialization goes or yeah there is no
funky behavior at all
and so once we construct this noisy
co-occurrence matrix based on the real
data and then we no longer play with
these documents at all these are the
deal mists of Statistics then we need
for the inference and then we buy
produced an anchor word which is
exclusively dedicated to each topic
which might bring some interpretability
so what's the problem then this always
happen in machine learning so then real
data never follows the model so in
reality this co-occurrences with
railroads makes sparse row so if there
is a very rare Wars happens one or two
times through the document the
co-occurrence with that words is
extremely rare so which makes sparse row
but in terms of the matrix algebra those
rows look like very strange point or
eccentric point of that Corcoran space
secure with row pivoting algorithm
prefers to select those rows so anchors
are selected as two rare worries and the
co-occurrence with those anchors become
noisy statistics and even worse the
co-occurrence exists between those
anchors is usually diagonally dominant
which means so one anchored correspond
to one topic if this becomes diagonally
dominant we cannot capture any
interaction between topics this is a
serious problem so as a result oh there
was previous work they usually manually
crowd use manually crafted document
frequency cutoff which means oh they
just set some threshold like if in order
for this word to be an anchor word it
must happen a list in five different
document ten different document
sometimes 100 different document and
they measure the held out like clean
again while in order to measure the hell
del likelihood we need to finish the
inference process entirely and then oh
it doesn't look like it and said change
the threshold and measure how they'll
likely again it's extremely painful
process and even with the document
frequency cutoff and the lure in them if
the number of topics is very small they
learn garbage topics and cannot capture
interaction between topics at all
because of this problem and even if K is
high the topic quality is poor so this
is discovered recently caused the
original two papers they all used a
synthetic data set which follows the
model pretty well but in reality in the
real data oh the topic quality is very
poor and comparing to probabilistic
inference like hip sampling the entire
inference quality is far inferior so i
will show you some sample topic ah
sample n koers with the original
algorithm which is called greedy here so
this is New York Times corpus precision
which is popular in the scene and as you
could see no one of the anchors are yeah
understandable and so we had a toy
experiment in previously MLP paper we
just compressed down this co-occurrence
matrix using either PCA or T stochastic
neighborhood neighboring embedding and
we realized it gives us much better and
cord and even increasing the held out
like cleaved but still we cannot explain
well why this work that explaining those
is one of the purpose of this new paper
so I will show some visual example of
those anchored
so this is a 2d or 3d projection of
small Yelp academy corpus so these is
illustrating the current word
co-occurrence base and the anchors
corresponding to the vert vertices of
the convex hull because as you might
remember the goal after getting anchored
the goal is to learn all those
coefficient to express all these words
by the convex combination of the anchors
and interestingly these n koers
corresponding to certain topic of the
anchor of the Yelp and we can also do 3d
projection while it looks messy there is
homeless and enchilada here so what
leave down in the new paper coming for
naps is we study extensively what's the
structure mathematical structure of
co-occurrence matrix e so while i
skipped and tally this has a lot of
probabilistic and statistics structure
you could see in the paper but would i'd
like to articulate in this presentation
is the geometric structure of the c so c
must be low rank and at the same time EE
non-negative the w non-negative matrix
is a category of matrix which is entry
wise non negative and positive
semi-definite so and by definition see
must be join stochastic which means the
sum of all entries is equal to one and
the lower rank is document is believed
to be generated based on small number of
topics so see you need to satisfy this
for different structure at the same time
on top of these probabilistic and
statistics structure so of obviously the
sea must to most satisfy a lot of
different condition at the same time and
this is a proof very rough proof YC must
be the positive semi-definite
I will skip it so seems to be this this
is and in order to make C satisfy all
those condition we perform alternating
projection so projecting see down to the
low rank positive semi-definite space
first and then projecting down to the
cone of normalized matrices then cone of
the non-negative matrices and repeat
this process again and again so the
first projection is easily achievable by
the truncated eigenvalue decomposition
so rather than doing the full AG in
value decomposition which is clearly
painful if the vocabulary size is high
we just use for example by using power
method just get the first biggest k
eigenvalue and reconstruct see based on
that i invent and the projection
orthogonal projection to the cone of
normalized matrix is given by this
briefly the intuition is the sum must be
equal to 1 and the measure compute the
difference between the current sum and
the ideal some and get the average and
to revise average and do basically
penalize a reward based on that and the
non-negative matrix cone it's true it's
pretty easy so I will the algorithm
alternate these three projection and
again and again until conversions
however this alternating projection
algorithm no longer guarantees the
convergence to the global optimum it's
because what where is the first two
cones normalized matrices and non-
matrices are convex cone but the
positive semi-definite with low rank is
no longer convex whereas simple positive
semi-definite is a convex cone
however ap instead enjoys local linear
conversions so there is a mathematical
proof in my paper but roughly speaking
only about the intuition so the set of
rank K matrices still forms up not bad
shape which is a smooth manifold and the
intersection with the convex cone and
smooth manifold is still smooth manifold
in almost everywhere sense and so so
long as the our estimator is not too far
from the convergence point it is
guaranteed to be converged and this is
one of the theory by Adrian Lewis 2009
paper so the rectified and chord
algorithm that be proposed in the paper
is consists of five to five different
procedures first construct the noisy but
unbiased estimator word word
co-occurrence and then rectify that by
using alternating projection so that C
will satisfy all the structure of the
idoc and then find the anchors in the
rectified co-occurrence recover the word
topic matrix via probabilistic inference
with Bayes rule and recover topic
topping matrix a base down the block
matrix diagonal decomposition you see in
the earlier slide so it contains pretty
simple procedures and this all of these
procedures are deterministic and that's
of course clear benefit of this method
so
I will excel the result so how the
rectified co-occurrence looks like this
is a two-dimensional visualization of
the co-occurrence space this is from the
original algorithm oh by the way the
data set is nips data set and the goal
is to fight find the five different
anchors of this data set so each dot
correspond to word and this is a word
word co-occurrence space and if I you
run the original greedy QR Withrow
pivoting it choose five different eggs
vertices like this and whereas if I
rectify the space like this and then if
I choose five different anchors on the
rectified space it corresponds these
five to these five points and if I map
these five points into the original
space it looks like this so as you could
see the coverage is much higher and it
can explain the rest of the world as a
convex combination better so this is a
typical topic result so this the day
Isetta snips and again I have five
different topics so the original
algorithm a varietal if I run the code
based on his algorithm and their
algorithm this is the result topic so
everything looks like newer on layer
hidden and basically each topic agrees
with the Uni Graham distribution of the
corpus and in general in the word of the
topic modeling if the topic in firms
does not work well topic simply mimics
the Uni Graham distribution that's
typical behavior and if I run the gibbs
sampling
it gives pretty good topic one is about
the neuron cell topic or the control
theory reinforcement learning or object
recognition or speech recognition neural
network stuff regualtion approximation
step and if we run the code in a
logarithm it's pretty similar to the
probabilistic lda even with this small
number of topic so if you actually
increase the number of topics
drastically like 200 then still in there
is a chance to cover all these vertices
eventually at the end so the topic in
firms quality becomes better but if the
number of topics is not that large
enough there is a large amount of
chances that the original algorithm
fails because of selecting too eccentric
words railroads which is not
statistically stable and this illustrate
the topic topic interaction which is the
second part of the inference so this is
coming from the original Aurora at all
method so what they did is while I
haven't described after finding the word
topping matrix which is B they
multiplies the pseudo inverse of the be
to the left hand side and the right hand
side of the co-occurrence matrices of
course right inside the transpose of the
pseudo inverse matrix and then it gives
this result of course this is wrong
cause some entries are even negative and
there's no way for the probability
becomes negative and some entry is even
beyond one the sum is close to one
that's that's because it's algebraic
properly satisfy that and this is
another method that we are using in our
paper by multiplying those diagonal sub
matrix to the left hand side
the right hand side of course that is so
simple met though then there is no way
that the original author didn't try that
method but actually if we try the
original that multiplying diagonal
matrix method to recover the topic topic
interaction without rectification it
will look like this so it's entirely
diagonally dominant because of the
reasons that I explained before again n
koers is likely to be selected as very
rare roars because of that the
co-occurrence between anchor and anchor
are extremely rare and statistically
it's not a good statistics at all and it
makes this diagonal matrices which
cannot capture the topic interaction at
all whereas this is the result from a
rectified and chord algorithm it
captures the topic interaction pretty
reasonably if you actually match one
topic with the previous topic in here
that we learned this fail use our turn
these values turn out to be reasonable
and this plot dros the overall quality
yet just one big image so we are not
only testing our method in the document
collection which are nips and New York
Times we can also do this model for the
movie and some data in the movie data
the word correspond to each movie and
document correspond to the collection of
movies that each user observed and in
the song data the song each song
correspond to word and the playlist and
it's broadcast station plays frequently
will be the document and then I we
currently have six different measure the
recovery indicates
how well those rows corresponding to n
koers reconstruct the other rest of the
rose and the approximation error is the
Frobenius norm difference between the
original co-occurrence matrices and the
decomposed vectorized matrices so
basically the approximation error is the
traditional measure to a traditional
metric to measure how successful the
matrix decomposition czar and the
dominancy is how those topic topic
matrix is diagonally dominant and
specificity is how much each topic is
specified from the uni gram distribution
and the similarity is how well each
topic is separated to each other and
clearance is a well each topic coincide
with the document so while this graph
may looks a little bit messy the the
thing that we have to focus is AP and
Gibbs and the original baseline methode
so if you see the original baseline
method for example in approximation the
approximation is pretty I but the AP or
our alternating projection method or the
gibbs sampling method the errors are
pretty low and those behavior agrees or
across different data set and also in
the recovery arrow as you already seen
the word word co-occurrence figure after
the alternating projection the recovery
rate is becomes far better and if you
see the another measure the AP and gapes
follows pretty similar trajectory it's
comparable to each other which means we
finally achieve the probabilistic
imperative result to the probabilistic
inference if you see the original
baseline method they are all far from
there was probably stick inference or AP
method right criterion to judge which
which topic yeah yeah that that's one of
the question which pops up always in
cutting modeling so this is unsupervised
clustering that there's no clear way to
judge which is better so people used to
introduce all these different metrics
like that I suggested and also do the
human validation at the same time yeah
it will rape ugly yep so basically you
see this it's not that hard and some
people some researchers designed the
matrix this is up some another metrics
based on this result how well how much
each word frequently occurs across
different topics like that but those are
usually subsumed in the metrics that I
illustrated in the plots so in other
task oh you know I saw that may be used
for st yeah perplexity perplexity izle
hell del like we're measuring yeah we re
actually did that while i didn't include
in this paper yeah so it because the
reconstruction a recovery error
drastically goes down the hell they'll
likely increased allowed so the
conclusion
so we study the various mathematical
nature's of the co current statistics
and this might be exciting cause as you
as all of you know the word embedding
stuff they are all based on the word
co-occurrence while they are not coming
from this topic modeling assumptions but
of assuming there are some clusters of
words in the word code in the natural
language we there might be exciting
mathematical structures which is
desirable for a certain embedding of
course that will be becoming different
based on which task we are tackling and
we develop a principled way to rectify
the loose no easy co-occurrence rather
than exploring the documents the cut off
again and again exhaustively and based
on this method we can learn the quality
topics even if K is very small you
already seen k is equal to 5 example and
another example which is in our paper is
in the movie data k is equal to 15 and
if you run the original incurred
algorithm I think Pulp Fiction appears
across every every 15 topics as a top
top movie and the second it's I forgot
the name of that movie but those two
movies always toddlers across every
different topic while yeah what we've
learned has exciting cluster like lord
of the ring cluster and Star Wars
cluster and Walt Disney cluster and if
you actually run the gibbs sampling
method they gave pretty comparable
result and then we correctly learn the
topic interaction in a stable and
efficient manner and as I said we
achieve the comparable result so these
talk are based on these two papers which
I published
here and it's gonna coming soon in this
year and while I haven't prepared the
slide we are doing several exciting
extension in multiple different fashion
so as one of you might one of you might
already realize so the topic modeling
contains two different inferences one is
a word topic inference which is of
course the main inference so how which
topic is represented by the distribution
of words that part is included in this
algorithm but the secondary inference
which is what's the portion of topics
for each document that part is entirely
missing in this algorithm so that is
currently ongoing experiment and
interestingly the all the original
authors in Princeton like San llevara
wrong go and and on Kron we try and all
of the authors in Cornell are
collaborating each other all together
for those and another exciting with
extension will be like anchor topic in
for author topic inference so one so
rather than viewing all those thing as a
hidden variable let's say each author of
the document sometimes document has a
footprint of the authors for example all
these papers have a collection of author
and the another assumption which as one
more layer in the generative story is oh
this author has are interested in la
blah blah topics so author has a topic
distribution and based on that the
observed words are decided basically
some authors are interested more in the
topics for me I'm interested more in
probabilistic method
and then based on that the word that i'm
using frequently will be decided for
example like map estimation or bayesian
those crew is another ongoing extension
and an entirely different field like
privacy issue or so the basically the
co-occurrence matrix c is large if the
vocabulary size is just 10,000 it's
going to be 10,000 by 10,000 matrix and
save that in the memory is painful but
usually the natural language vocabulary
is 100,000 so how to store and do the
rectification step without explicitly
storing those all entries is exciting
question and even more I mean how to
store that efficiently and do the
rectification without violating the
privacy and those are all exciting
extension for and the future work yeah
so again wrapping up the presentation so
this is a mÃ©thode combat this is a new
inference combining the probabilistic
method and the spectral method and so
after forming the co-occurrence matrix
which is a second order moment matrix we
do we find the anchors on that matrix
and then based on that anchor verse all
the inference process is deterministic
and transparent and the exciting part I
think the you can take to your home and
for the future work so it is pretty
susceptible to the sample so if the
sample is not enough the estimation
result is pretty bad and also the model
mismatch
there's no intrinsic ability to handle
the model mismatch however if those are
solved the result from the method of the
moment it's easy to compute efficiently
and if we plug that result in to the
original probabilistic inference for
example get sampling needs a burning
process we need we no one knows how many
iteration we need to run at the
beginning to get a good result but if we
actually plug this result to as an
initial value to the gibbs sampling it
shows amazing result which even never
appeared in 100,000 of iteration which
is very exciting so usually all these
like lead fortune in the likely based
method there are no never convex and
there are a lot of modality inside up to
there and of course we cannot find the
good initialization point cause the
parameter parameter space is high
dimensional and this is a really good
way to give good initialization so
combining those two methods seems highly
promising and other people's work there
are several other variation rather than
doing the second moment matrix some
people use third order tensor which is
word word word co-occurrence and those
tensor decomposition stuff is also
another way to do the topic inference
while they need to assume each topic is
independent to each other but yeah
that's another direction in which is
done by anand kumar yep so this is the
end of the talk
the Christians pretty short yeah I
finished in 30 minutes good yeah thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>