<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Neighborhood Sampling for Estimating Local Properties on a Graph Stream | Coder Coacher - Coaching Coders</title><meta content="BDA - Neighborhood Sampling for Estimating Local Properties on a Graph Stream - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - Neighborhood Sampling for Estimating Local Properties on a Graph Stream</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nWVQcld8wzc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so I'm going to talk about giant work
with pollen from Iowa State and cannot
penguin son and kun lumu from IBM reset
so you know why are we talking about
graph streams that the motivation for
many of these streaming models i think
is monitoring monitoring in this case a
very large graph that's changing
dynamically for example if you think of
entities in a network say IP addresses
and the communications between them
that's a potentially massive graph with
lots of edges coming in as time
progresses every time there's a
connection there is an edge right so the
point about this is an event in this
system is an edge of perhaps a hyper
edge so we want to continuously maintain
some property of this large evolving
graph in this pointing in this case I'm
going to talk about local property and
what I mean by local is that each within
the one neighborhood of a vertex ok
let's say count the number of sub graphs
within the serta specific sub grass in
the one neighborhood so I'm going to
focus on basically small machines in big
data and in this case basically all the
algorithms that we talked about can be
deployed in a single machine reasonable
resources it also is one path to data
just like the classic data stream model
and the good thing about this model is
when it works then it's good for a lot
of cases it's good for parallel
computing it's good for discretion in
data and so on and so forth so in this
case this is the model and we've also
have some follow-up work on how to use a
multi-core machine effectively for
example you can process really large
graphs like 106
seven gigabytes with 10 billion edges in
about a thousand seconds on a 1200
machine all right so the focus of this
talk is going to be on triangle counting
okay so the problem is very simple you
have a pointer here yes oh okay Thank
mouth oh thanks oh it's okay oh thanks
okay so if you take this graph you know
we want to count the number of triangles
one and two and three a very simple
problem okay and why do we why are we
concerned with triangle counting and
first of all it's a very basic
structural property of a graph you know
most of the times when people want to do
a structural analysis of a graph in
social networks and so on number
triangle seems to come up all right in
particular there is a metric called
transitivity coefficient which is used a
lot in social network analysis and
that's defined to be exactly the number
of triangles times 3 divided by number
of connected to flex meaning how often
are two PR three people who are
connected to a single person via a
single person also form a free click now
how friends are friends are they
connected to each other so and then
there's also a property called the
clustering coefficient which is also
very related and in general there have
been many applications for example
betcha 88 all talked about web spam
detection they showed that a larger than
usual number of triangle
the indication of a web span a web graph
okay so if you want to go to higher
order clicks in biological networks as a
graph summarization technique that's
called as network motifs or graph lights
what it essentially is is a vector
consisting of the number of occurrences
of different types of sub graphs in a
large graph how many wedges how many
triangles how many clicks of size 4 and
so on and that's a vector that's used to
describe the graph that's called a
structural summary all these are local
properties in our sense okay so sorry
okay here's our contributions I'll give
you a new sampling algorithm for graph
streams i'm going to describe it in a
moment and we show it can be applied to
counting number of triangles it can also
be used to sample a random triangle in a
graph these two are closely related
problems and then we show extensions to
higher order clicks k 4 + KF i apologize
about this you know i made powerpoint in
one machine and moved it to another
machine and something happened she was
like and then we also can go to directed
graphs and we show some experimental
results so one thing i want to say is i
think this is a fairly flexible
technique a sampling technique and so
before going to the technique I want to
briefly tell about firewood there's a
lot out there on sampling on counting
triangles in offline and online models
started off with this the streaming
literature started off with this paper
in 2003 which reduce the problem to a
for computing frequency moments and
appropriately defined streams then
there's a lot of follow-up work 2005
johari and goatse really tall 2006 both
of they were sampling based and then
their sketch-based estimators more
recently goo ha on mcgregor genital
Manjunath at all all these a sketch
based algorithms meaning that they work
under insertions and deletions
whereas we were only under insertions of
edges is also a more recent sampling
based algorithm by seshadri at all okay
so these are all different from what we
described but you exact details i'm
going to we can talk offline so in the
non streaming batch context there is
also work by the MapReduce model
especially how to ensure a caucus soon
vassal which ski and so on so a graph
model simple graph let's say they were n
vertices and M ages we want to estimate
tao of G which is defined to be number
of triangles in G ok and the arrival
model is that edges can come in an
arbitrary order hey and adversely can
decide how they just arrived there is
also a different model called incident
stream model by the way this is called
adjacency stream model what we consider
in the incident stream model it's a
little easier because it's assumed that
all the edges because all the edges
incident at a vertex arrive together in
some way it's like the incidence graph
presented so he digests and see me sorry
an adjacency list of a graph presented
one list at a time so but what we talk
about is arbitrary order so there is a
close connection between sampling and
counting as follows ok suppose somebody
gave me a procedure a witch did the
following on graph G so if the procedure
succeeds it may not succeed sometimes if
it succeeds then it's going to return a
uniformly chosen uniformly randomly
chosen triangle from the set of all
triangles in the graph ok sometimes the
procedure might fail I did say I don't
know I do not have an answer suppose you
had a procedure like this you can use a
procedure to count the number of
triangles in the graph in the following
way so with this procedure the
probability that it succeeds
in returning our triangle does not
matter which one the probabilities it
succeeds izzy is proportional to the
number of triangles in the graph okay
and so here is a parameter the
probability of this success of this
algorithm and if you can estimate the
parameter then that's we are done we
counted number of triangles in the graph
and the estimation of a parameter can be
done in the very straightforward manner
just repeat this algorithm many times
and use the fraction of number of number
of times it's succeeded fraction of time
it succeeded use that to estimate the
number of triangles ok so there's if you
can do counting so if you can do
sampling in uniform where you can do
counting all right so the only thing is
the accuracy of the estimate depends
upon the probability that a fails ok if
it fails too often then it's not going
to Ile have you need a lot of
repetitions to get a good estimate so
I'm going to go through a couple of
sampling algorithms sampling a random
triangle from the graph ok perhaps the
most basic one simplest one is to just
select a triple at random uvw hey from
the set of all in choose three possible
triples and then when I'm observing the
stream I only look for this particular
triple if it happens to be a triangle
great i succeeded otherwise i didn't
succeed so if this algorithm one
succeeds it's clearly going to return a
uniform uniform random sample from the
set of our triangles but it's not going
to succeed unless the graph is really
dense in number of triangles the number
of triangles must be close to intuos3
right so there is a better sampling
algorithm again this was used in a paper
by Bureau little 2006 and what it said
was first let's at least fix one edge in
the graph ok let's sample a random edge
in the graph you be and then you sample
a random third vertex W from the set of
everything but you and me
now I wait to see if u and V and W form
a triangle hey so this is certainly
going to succeed more often in algorithm
one so given in this background i'm
going to describe the neighborhood of a
sampling idea so in neighborhood
sampling the first step is to just
choose a random edge a sub 1 in the
graph uniformly choose them from the set
of all ages okay and then we say that
two edges are adjacent if the share
vertex so what we do is once we choose
our sub one from all the edges that come
after us of one and they're adjacent to
a sub one I choose another random edge
let's call it a sub 2 okay so also to is
chosen from a sub stream that's adjacent
to a sub 1 and comes after our sub 1
alright so that's it once I have our sub
1 and R sub 2 I have two edges they
define a potential triangle and a wait
for the third edge in the triangle to
come and complete it if it completed a
triangle then good am lucky otherwise I
fail procedure fails okay so one thing
to notice that this procedure can be
done in a streaming fashion we just a
constant number of words in a
straightforward manner because choosing
a random a giass of one can be done in
reservoir sampling and once you choose
our sub 1 then you have a we serve our
sampling on the sub stream to get our
sub 2 and so everything is constant
number of words okay so now the question
is two questions one is how likely is
this to succeed and there's one more
question which is does it really return
a uniform uniform random triangle
uniformly chosen random triangle so in
fact this algorithm is not going to
return all triangles with the same
probability okay some triangles are
going to get higher likelihood of
getting
some of them lower so that's a sampling
bias that will have to overcome somehow
so to give an example here is a graph
with Levin edges and the edges are
numbered according to the order in which
they arrived in the stream so let's say
this one came first then followed by
this followed by this e 4 e 5 e 6 and so
on okay let's so given our sampling
algorithm we focus on this triangle e1
e2 e3 what is the likelihood that this
triangle is chosen by our algorithm so
for that to happen we need two events to
happen one is that when in our random
sample a sub 1 choosing uniformly from
the entire stream we need to have e sub
1 hey if he said one was not there then
a sub 1 then we won't get this trying so
we do need all 1 equal to e sub 1 and if
you look at what's adjacent to e sub 1
it's e to and III among these two if I
choose III then I'm out of luck because
e 2 will have already gone by and they
won't be able to complete the triangle
so I have to choose e2 so probability
that r2 equals e to given r1 equals c1
is actually pretty good it's just one
half because even has only two edges
coming afterwards and neighbor into it
so the entire probability of choosing e1
e2 e3 is 1/10 times one half so it sent
one over 20 okay now let's take a
different triangle efore ephi v6 so this
triangle over here so likelihood that we
get this is first of all we're good to
choose a sub for in our first level
random sample in our sub 10 sub 1 should
be equal to e sub 4 and
among all the edges adjacent the e sub
for we need to choose a sub 5 okay now
he suppose X popular edge in that there
are seven other edges incident to it all
this all these 1234567 edges are all
incident to ease up for and they all
come after you sub for okay among this
group of seven edges we need to be lucky
and we need to choose e supply and then
we are too so the probability of
selecting this triangle is actually
going to be 1 / 70 this is 1/20 ok so
now algorithm that's going to give
arbitrarily different probabilities for
different triangles is not of much use
in general for counting problems so but
we thought let's do this anyway and then
what came out useful was that you can
actually track the bias that this
algorithm is going to give to the
triangle that we finally chose so in
particular let's define cioffi to be for
an edgy number of edges that were
adjacent to e and that followed e you
find that to be cioffi okay once you
have that let's look at what happens to
c of e 1 this case this edgy one
according to this definition c of e 1 is
just 2 there were two edges that came in
and if you look at before cioffi for
happens to be excuse me it's not 287 so
that's a mistake that should be seven
okay so and you know you know if you
look at the probability of choosing
triangle e1 e2 e3 it is exactly equal to
1 / 10 x 1 / c of e 1 and the
probability of choosing this triangle is
equal to 1 / 10 x 1 divided by C of E 4
okay so the bad thing is this the
strangles are coming out non-uniform but
the good thing is we can't track if you
can't keep track of what's the bias that
we are going to give against this
triangle so in general it's easy to see
that for a triangle T where E is the
first stage in the stream the
probability of choosing it is a constant
number across all triangles 1 over m
times 1 over C of E which is specific to
this edgy wait so from there the West
actually becomes straightforward so if
you were interested in sampling a
triangle uniformly at random and you had
a procedure which was non uniform you
can just use rejection sampling where
you reject the sampling you eject the
sample that you got with probability
proportional to the bias that you give
for this track a very very popular
technique and the nice thing here is we
know the bias we computed it an online
fashion just do that so for sampling
uniformly at random we can just do
addiction sampling for counting
triangles we can do something even
better we can actually use the
neighborhood sampling is described and
computer online the bias for this
triangle that we got and you can
incorporate the bias directly into the
estimator okay so are in fact the entire
will Gotham is actually very simple for
counting triangles in a graph so at the
first level you have our one which is a
random edge in the in the entire stream
and then let y sub 1 be their edges that
arrived after r1 and adjacent to our one
hour to is randomly chosen edge for me 1
and C 1 is the size of e1 okay now we
wait for this triangle to be completed I
want to know to define a potential
triangle it's completed then i return c
1 times mc1 is in some sense the bias
against this triangle
that time Sam I returned that and if
this triangle was not completed I return
0 hey that's how I estimate a very
simple now you can it's easy to prove
the properties of the estimator actually
then if X is the return of the algorithm
then needs the expected value of this X
is equal to the number of triangles in G
very simple proof and then you can just
run a whole bunch of independent
estimators and take the mean of all of
them and you're going to get a good
approximation because the key thing is
how many estimators do you need that
happens to be number of edges times the
maximum degree divided by number of
triangles okay now you might be thinking
this is crazy we want to estimate number
of triangles but number of times you
have to repeat it is proportional to is
inversely proportional number of
triangles well there's actually not much
you can do about this in some sense
because in the worst case these
algorithms have all got to take a lot of
estimators because to determine if a
graph is triangle free or not an
extremum model itself in the worst case
takes order n square space okay so but
this is better than previous algorithms
because there was it used to be either
maximum degree square for one of the
algorithms johari and good see our it
was a number of vertices x number of
edges for the algorithm of burry allah
tala ok so it's better than it quite
significantly improves upon the space
taken by the previous algorithms right
so the space complexity is clear because
it's basically equal to oops excuse me
equal to number of estimators that you
take and every estimator is constant for
the time question is if you run our
estimators in parallel does it take
order our time / update that would be
pretty bad so we have some methods here
where we use bulk processing you do a
whole bunch
Edge's at one time w edges at one time
and because of the random sampling
nature of this whole algorithm this w
edges can be processed much faster than
processing them one by one so in fact if
you choose w equals order or far then
the entire batch of w edges can be
processed in w time so order 1 / edge no
matter how many estimators you have okay
all right so I guess you can say about
this notation here it really was a sub 1
a sub 2 something happened to it so the
same thing can be extended for four
clicks very natural manner now you can
the algorithm that you can make up is
pretty much what I have over here is
sample a random age and then from the
neighborhood of this edge they say that
we sampled a sample another random my
charts are sub 2 i am from the
neighborhood of our sub 1 and r sub 2 I
sample another wedge a sub 3 and finally
what I have is a potential for click
which a way to be completed okay now
this isn't quite work because you need
there some special cases which this does
not take care of which we can there is a
ways to incorporate that also and
overall this very simple natural
algorithm actually gives the current
best bounds for counting the number of
four clicks in a graph okay so in some
ways it reminds me of sticky sampling in
the case of stream processing to find
frequent elements you randomly choose an
element and then count the number of
reoccurrence of this here in graphs we
choose a reg randomly and then only now
stick to the neighborhood of this edge
and that way we're going to get a little
better space bombs alright the
extensions I think are very interesting
are you know transitive code
transitivity coefficient is trivial
because most of the work is actually
counting number of triangles you can
extend this to sliding windows bigger
search based on random sampling and
random sampling on sliding windows is
well studied problem you can just use
those techniques here nice thing is you
can convert this into you can manipulate
this lot of ways now it's three cycles
that's pretty easy because you know
after you slick at a sub 1 you make sure
that our sub 2 that you select has a
potential of forming a directed cycle
with us of one okay so you only look at
things that are either coming into the
tail of a sub 1 or going out from the
edge of head of a sub sub 1 so you can
do directed recycles also and I don't
know if any previous work on direct it
recycles in a streaming model but so you
can use this one and then you can have
other constraints for example you might
have a temporal constraint which says
how many instances where you had a
connected to be followed by in time be
talking to see and followed by in time
see talking to a so it's not a graph
anymore it's something else where there
is time involved you can even count the
number of such patterns because of the
sequential nature of the sampling that
we have okay so just a few words about
experiments I'm almost done here so we
implemented this on a bunch of graphs
here is an example or could graph with
100 more than hundred million ages and
they're pretty high large max degree and
we found it he just with 1,000
estimators we got a relative error of
five percent on a single machine it took
50 seconds out of 50 seconds 33 seconds
was for i 0 so it was really very very
practical if use a million estimators
meaning a few mil megabyte of memory
then you get the error down to the
little arrow down to 105 percent and it
takes still a very reasonable amount of
time so I have a couple of graphs you
know basically if you multiply the
number of estimators by 8,000 the time
goes up by a factor of 10 okay according
to theory the time should be the same
but I think this is because of the
caching effects with 1000 estimators it
was much
faster and so all right so I'm so going
to skip all this so to conclude what I
presented is a general sampling method
for estimating graph patterns which lie
within the one neighborhood of a vertex
ok so it can work for small size clicks
you can throw in a bunch of special
cases and it still seems to handle it
and can take care of it like temporal
constraint H directions and so on and
then I think of this as something like
sticky sampling for graph streams so
hopefully to have more applications for
such questions the technique is very
simple sample within the neighborhood of
currently chosen ages I and what you
have is a biased sample but you can
maintain the bias online and incorporate
the bias into your final estimator then
we have fast implementations so in
particular we have a multi-core
implementation which uses a bunch of the
techniques from Carnegie Mellon like
silk especially the the cache oblivious
algorithms and for example a graph of 10
billion edges the synthetic graph which
has takes 17 167 gigabytes can be
processed in a thousand seconds on a 12
cold machine ok ok so that's all I had
to say if you want the paper then please
send me a email or you can visit my
webpage I have a link to that thang</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>