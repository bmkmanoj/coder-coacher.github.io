<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UW - MSR Machine Learning workshop 2015 - Session 3 | Coder Coacher - Coaching Coders</title><meta content="UW - MSR Machine Learning workshop 2015 - Session 3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>UW - MSR Machine Learning workshop 2015 - Session 3</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kieclpPtQuE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
he
intercession of our of our workshop the
first speaker is Brendan McMahon from
google and he'll be telling us about
delay tolerant algorithms for
asynchronous distributed online learning
Thanks hi yeah this is a joint work with
my colleague Matt Streeter when he was
at Google so I'll start out with a very
short summary here so if you'll remember
one thing from this talk this is
probably the thing to remember and that
is if you're going to run asynchronous
online gradient descent with massive
parallelism you should choose your kind
of / coordinate learning rates very
carefully because that parallel ISM is
really going to change the story here
when it comes to learning rates if you
remember two things you should remember
the first thing and you should also
remember that we have a cool algorithm
that addresses this problem in a
principled and and provably efficient
way okay so what's the class of
motivating problems here a problem I've
worked on a fair bit in the past is ad
click through rate prediction and I
think this is a kind of canonical
example of a web-scale prediction
problem so the you might have actually a
pretty simple model here so you can
think of something like a large sparse
logistic regression but with a billion
dimensional feature space right but for
any particular example you're going to
only have ten or maybe a hundred some
features that are present features that
are that are non zero so that can arise
if you have for example bag-of-words
type representations or indicator
variables on pairs of words things like
that so some of those features are very
common and some are very rare right
you're only going to see them in one in
a million training examples but the
president of that presence of that word
of that feature could still be very
important so this is exactly the kind of
data set or the kind of learning problem
or adaptive gradient or a degrad methods
have been shown to be extremely
effective so we're going to kind of
build on that same setting but we're
interested in then kind of extending the
algorithms to explicitly take into
account the fact that if you're going to
run on these truly massive data sets you
probably need a massive amount of
parallelism as well and then obviously
here you know any query that's done on
Google producers potential training data
for this problem so clearly
it's a large-scale problem okay so
here's the slightly longer summary
focusing on this adaptive revision
algorithm that I'm going to be talking
about so this is a gradient descent
algorithm we kind of stated and analyze
it in the online convex optimization
framework but optimizing say a logistic
regression model naturally falls into
that into that framework it's designed
for asynchronous lock free updates in
the spirit of the hog wild algorithm in
particular we're focusing on the case
where we have very large update delays
due to massive parallelism it's going to
adapt to the gradients on a per
coordinate basics basis like a standard
at a grad algorithm but the key
difference is we're not going to just do
that kind of adaptation but we're also
going to adapt to the the actual
sequence of delays that occur on a per
coordinate basis right so this is this
is again a different flavor of data
dependent adaptation so we're going to
prove some some strong data dependent
regret guarantees for the algorithm and
we show empirically that you can get an
order of magnitude kind of additional
parallelism without any cost in in
accuracy using these methods so you can
massively scale up the number of
machines are using to optimize this
problem and it doesn't cost you anything
ok so I'm going introduce the the
setting first and talk a little bit
about the the model for parallelism that
we're working in and then sit been
actually probably a decent chunk of time
on some intuition in theory for an
algorithm we can't actually run and then
I'll introduce the actual algorithm that
we can run and there's a fair number of
details that I'm just going to gloss
over in getting from the hypothetical
algorithm to the actual algorithm and
then hopefully we'll have time to talk
about experimental results which I think
are quite quite nice so here's the
picture and we're going to focus on a
single parameter a single coordinate in
this billion dimensional space there's
an what we're going to call the updater
which is a parameter server type
representation specific to that
coordinate and it kind of serves and
serves that model coefficient to
whatever readers need it so the reader
is the thing that's kind of actually
just computing gradient so it reads a
training example compute some features
then it goes to the relevant updaters
and requests those coordinates those
coefficients for those coordinates once
it's read them it can compute the
gradient and then it sends the gradient
back to the updater in order to take say
a gradient descent step of course the
trick here and you can kind of see in
this diagram is that in the time between
when that weed occurred and when the
update is actually applied a bunch of
updates may have actually come in so
when you go and kind of apply the update
g3 here the blue update it was based on
reading x1 but the coefficient has
actually been updated since then so this
is kind of a delayed and stale gradient
which is exactly what makes this this
challenging but we need to deal with
this problem if you want to avoid
locking if we wanted to run things kind
of as fast as our hardware will will
allow in a purely asynchronous way so
what does the online gradient update
look like here well I'm going to make a
couple of very important simplifying
assumptions we can use kind of a
standard reduction that lets us assume
we're working with linear loss functions
and then we're going to assume kind of
an unconstrained model space and those
two things combined mean that we can
essentially really do our analysis on a
per coordinate basis and that's
important for two reasons one is it
simplifies the notation and the analysis
is a fair bit but because we're not
assuming that those different
coefficients are stored even on the same
machine things kind of can get out of
sync in in different ways on different
coordinates so just reasoning about kind
of that combinatorial explosions of
different inner leavings of operations
would be extremely challenging unless we
can you know if we couldn't make this
kind of per coordinate assumption but it
turns out that the thing that helps with
the analysis even in the non parallel
case actually gives us a much more
powerful result in the parallel case
because we don't have to make
assumptions that that everything is
happening in the same order across all
of the different dimensions the next
kind of little notational trick is that
we're going to index things by the order
of the updates and when you do that then
you can your gradient descent update
looks exactly like the standard gradient
to send update write XT plus one equals
xt minus the learning rate a 2t times
the gradient GT the key difference here
is GT now is not the gradient of the
loss function at X T it's the gradient
of the loss function computed
some stale point right so you actually
saw that loss function a while ago with
some possibly stale and inconsistent
coefficient vector you made a prediction
and now at a later date you're actually
getting to apply the update so it turns
out the key thing is going to be how we
choose those / coordinate learning rates
so here's the intuition and this is this
is actually I think kind of high on that
list of things to remember because you
don't have to really think too much
about the math to for this to make sense
say we have you know m equals a thousand
machines processing data right and now
remember think about this ad CTR click
the rate prediction problem right some
of those features are very rare so
consider a feature that only occurs on a
pea fraction of the training examples
say a word that occurs on one in a
million examples well then the effective
delays that any given coordinate are
going to experience aren't that equal to
M you know that ten thousand machines
you've got working on this the effective
delay is only going to be P times M
right because what we care about is how
many updates come in in between the kind
of read of that coefficient value and
when that gradient comes back and if if
you've got a really rare word it's very
unlikely that any other updates come in
in between and so the effective delays
are much smaller on the other hand if
you have kind of a common word or a
common feature then the the effective
delays are going to be much larger why
are larger delay is a problem what kind
of standard analysis shows that that
effectively larger delays mean that you
have more danger of online gradient
descent kind of overshooting that's
really why these delays hurt you you're
just not reacting quickly enough and the
way to kind of get around that prevent
that overshooting is to use a lower
learning rate so this feels similar to
the to the standard adaptive gradient
observation that common features should
have learning rates that decrease more
quickly but that effect is actually
amplified in a kind of additional way in
the parallel setting and so the question
is how to take advantage of that all
right so the standard a degrad
algorithms use a learning rate that's
like 1 over the square root of the sum
of the past gradients the learning rates
we're going to study look very similar
but they have this additional term show
one in red so in addition to kind of the
current gradient squared we're going to
have a term that looks like the current
gradient x this GS forward that's
actually a sum of gradients it's the sum
of the gradients that are going to be
applied after this update but based on
reads that happened before this update
so you can think of this as the sum of
all the gradients that are currently in
flight and we've got some theory that
shows that that's kind of exactly the
right way to quantify the impact of
these delays right if you if you knew
this quantity this is the type of
learning rate you would want to use now
of course this is a hypothetical
learning rate schedule because there's
no efficient way to know the sum of
gradients that are currently in flight
right if you can do that you could just
kind of apply them immediately you'd
clearly be cheating so we can't actually
use this learning rate but if we could
it would actually work out nicely but
but let me back up no sorry let me just
yet to give a quick example of what that
forward some of gradients actually is
right we can go back to this to this
slide here if we're looking at the
update for the for the red gradient g2
what's the the forward gradient some for
this well it's the sum of gradients g3
and g4 because those are both computed
after reading the initial coefficient
value x1 but they're not going to be
applied until later ok so where did this
this learning rate formula actually come
from and a little bit of history here
this is actually a very similar analysis
technique to the one Matt and I used
when we were additionally deriving our
version of the of the a de grat
algorithm and I think the phrase in the
paper we used was was bound optimization
the idea is that we're going to prove a
regret bound which is kind of like a
convergence guarantee that's very
general it's parameterised by the actual
gradients you observe so it's a very
data dependent bound and it's also
parameterised by essentially an
arbitrary sequence of learning rates an
arbitrary non increasing sequence in
particular and once you have that bound
that's a much easier mathematical object
to work with than the actual performance
of gradient descent but if you're bound
is tight it can still be extremely
useful so the trick then is to kind of
take that bound and find a setting of
the learn
rates that minimize the bound it's a
function of the actual data you see the
actual gradients so we do exactly that
same thing here except we use a more
general regret bound that actually takes
into account this this parallelism and
when you prove that regret bound that's
where this these forward gradient sums
come from so now we could go about
optimizing this bound and if your work
with these kind of things before it
should be pretty clear that this is the
type of learning rate you want to use
your kind of trading off two terms in
the bound and so you get this kind of
square root of that sum of gradients
term but now we still have this
significant practical problem that are
learning rate schedule unlike in the
standard at a grad case depends on a
quantity that we don't know right so
we're really not at all done yet but
this provides some intuition that with a
smart learning rate with smart control
of these per coordinate learning rates
you can do something much much better
what's what's the solution well we can't
use these forward gradient sums those
kind of seemed clearly hard to track
it's much easier to compute a backward
gradient some so this is the the sum of
gradients that were applied while this
current update was outstanding and I'll
show you in a minute exactly how we can
we can do a little bookkeeping to to
compute that once we have that there's
kind of two main steps in the analysis
one is to prove that we can kind of plug
these backward gradient sums into the
learning rate and that that works just
as well but there's a caveat with that
initial thing we can plug in these
backward gradient sums but we still need
some backward gradient sums for some
future updates that haven't been applied
yet so it's still hypothetical author
and we still can't do it so the next
trick is to say well okay we don't know
what the right learning rate to use is
now but again using it turns out exactly
the same bookkeeping we can go back and
revise previous learning rates that we
chose and essentially use a smaller
learning right so the idea is initially
you take an aggressive gradient descent
step but then you see a bunch of delayed
gradients going in that same direction
you say oh wait a minute I probably if I
was going too fast with that initial
step because i didn't know that all
these delayed updates are coming in so
you actually scale back that previous
step that's the adaptive revision
step and that's kind of the core of this
algorithm before we get the name so I'm
not going to going to talk through the
pseudocode in any detail here there's
the kind of the bookkeeping trick that's
called out here there's the learning
rate which now depends on the product of
the current gradient in this backward
gradient some and then there's an
adaptive revision step so we're kind of
maintaining Lee this invariant that this
sum of gradients all have the same
learning rate a too old and so then we
can just subtract that off and multiply
in the new learning rate so that's a
very efficient step right it's not
there's no dependence on the number of
of gradients in that some we can kind of
do it all in one in one operation so
this really isn't much harder to
implement than a standard at a grad
algorithm there is one cost to running
this algorithm that you don't have by
running standard kind of asynchronous at
a grad and that is in addition to
maintaining the coefficient value like
x4 here in the first line Z is like the
the term that controls the learning rate
so the sum of past gradient squared
something like that we also keep track
of the sum of all of the gradients that
have been applied so far and then what
we do is when a reader requests a
coefficient value we send out the
coefficient value but we also send that
gradient some and the reader is going to
do absolutely nothing with that it's
going to just compute the gradient and
it's going to send that gradient some
back right so we're basically using the
network as storage here but that's kind
of a nice trip from a fault tolerance
perspective because we don't have to
worry about cleaning up caches when the
reader dies or something that's it's
kind of nice and clean conceptually and
in practice if your network is fast
enough it's it's pretty good so then
what's been happening on the updater
well before the sum of gradients 1
through 3 have been applied well this
update was outstanding gradients g4
through g9 came men and so by the time
this update from from the reader shown
here comes back the updater restoring
the sum of g 1 through g9 we subtract
off the sum of g 1 through g3 and what's
left is exactly the sum of the gradients
that were applied while this update was
outstanding out was was in flight so
it's a pretty easy trick but it's also
it's also quite nice and it turns out it
gives us exactly the piece we need for
the two the two other tricks in the
algorithm
I'm not going to focus on the regret
bound two much here because I haven't
even told you exactly what regret means
the key thing here is that this is
basically the kind of bound you would
hope to get right so we haven't had to
make any stochastic assumptions on the
data it reduces to the standard a degrad
bound in the case that you happen to not
be running in any parallel ism and you
don't have to tune your algorithm at all
to get that right like this algorithm
actually is a degrad if all of those
forward sums or backward sums happen to
be zero which is exactly what happens in
the case of no delays right so this is
kind of a strict generalization of the
standard thing that you would probably
want to do anyway on these kind of
problems and this also the bound we get
for the actual algorithm essentially
with a couple of minor caveats matches
the bound you would get if you could run
that hypothetical algorithm where you
could kind of know the sum of the
gradients that we're in flight all right
so so of course the real question is how
well does this actually work and a
little meta point here anytime you're
comparing gradient descent algorithms
you have to be very precise about
telling people how you're you're
optimizing the overall learning rate
scale factor right so I'm going to use
these / coordinate learning rates but
I'm going to pick some overall
multiplier that I multiply into all of
the learning rates across the
coordinates and if you've ever work with
gradient descent in practice you know
that doing that well is really important
here what I'm doing is the wrong thing
I'm going to optimize that the learning
rate for each of these problems but in
the zero to the zero delay case and then
I'm just going to use that same learning
rate as I ramp up the delays this this
is all in simulation so I can control
the exact delay pattern so the constant
delay pattern is exactly what you would
think you know the delays are always
exactly a thousand everything's in
lockstep the random delay pattern
actually doesn't satisfy one of the
technical assumptions we need to make
which is why it's interesting to look at
that as well things still work really
well so the black lines here are kind of
the standard asynchronous at a grad
algorithms both the dual averaging and
gradient descent versions and will you
see here i mean the y-axis is here in
terms of that the change in law gloss
are are a huge range but things are kind
of fine up to delays of like a thousand
and you know i've actually got a paper
from nips 2013 with john doochee and
there's a hog wild paper a lot of people
have observed that the standard
algorithm
if you have moderate amounts of
parallelism work just fine so our
experiments bear that out you know if
you're running on a single machine 64
threads you probably don't need this but
there's this real phase change as you
get up to say ten thousand or delays of
10,000 or 20,000 and you see the
standard algorithms are totally blowing
up right so like you know on the on the
URL data set you know we're like six
thousand percent worse or something like
that so something has kind of gone
horribly wrong there or as our algorithm
adaptive revision is paying a very minor
penalty for that extreme parallelism
okay so the obvious criticism to that is
well you weren't tuning your overall
learning rate scale factor based on the
amount of parallelism you should do that
right and in your right if you're not
going to do anything else you should you
should at least do that and so what this
plot shows it's the same experiments but
now kind of for each x value for each
amount of delay we tune the learning
rate separately and so the little the
bottom plots here show you how the
learning rate changes how the optimal
learning rate scale factor changes and
when you see again for asynchronous dual
averaging gradient descent kind of the
standard algorithms the learning rate
scale factor that you should use drops
by two orders of magnitude as your
delays get up to like 10,000 or 20,000
so you're kind of running with a whole
bunch more parallelism but then you're
giving back most of that speed up
because you're dramatically scaling down
the size of the steps are taking right
so it you know the algorithms aren't
blowing up now you're maybe two hundred
percent worse instead of six thousand
percent worse but you're really not
taking anything like full advantage of
the parallelism whereas for adaptive
revision we get a little bit worse and
you can see that you're not getting any
benefit from this additional scaling the
learning rates the overall learning rate
scale factor for adaptive revision stays
basically the same with only very minor
variation is you increase the delay so
what that's telling you is that we
really are taking the delay into account
appropriately in these these per
coordinate learning rate schedules
because additional manual tuning isn't
necessary and in particular like one
point of comparison here you can see
that with a delay of with a delay of
10,000 so think ten thousand machines
processing producing gradients
we still have better accuracy than the
standard algorithms with with the delay
of 1000 so that's that kind of order of
magnitude of additional parallel ISM you
can afford if you're using this this
algorithm so a few quick conclusions and
you know these this / coordinate
adaptation seems to matter a lot it
matters a lot even if you're not in the
parallel case for these large dimensions
sparse problems but in the parallel
setting it buys you even more and so you
need to take into account both the
gradients and the delays an adaptive
revision provides a very nice way to do
that so all in there thanks any
questions go ahead are their microphones
anywhere theoretical bound on the number
of the machines in parallel we can use
because I've seen like hike while has a
bound on the number of the parallel
processors but you mean like an upper
bound yes um I think you can you could
work backwards like there's there's a
modest additive term in our regret bound
in terms of the the worst-case
parallelism I skip it there so in this
bound in that additive term is is there
so it's it's those constant terms or or
on the inside you know you could work
out that certainly you know if you have
as many machines as you have training
examples or rounds that's going to be
too much but it's actually the upper
bound here is pretty pretty large right
I think if you get lucky with your with
the way things happen that you can use a
huge amount of parallels I mean the nice
thing to think about here is that the
Jesus does those forward gradient sums
those are signed sums right that's not
the sum of the magnitudes of those
gradients that's actually the sum of
those gradients so if you get lucky and
the gradients are canceling each other
out then you don't really pay any costs
for the parallelism so that kind of
tells you the over
really is the is the problem here if all
of those gradients are going in the same
direction and then the next gradient is
going in the opposite direction that's
where you kind of pay a big a big price
and is it over Lipschitz continuous
functions no we don't need any any I
mean all we need basically is to have
sub differentials but then what is that
elpida we are sorry we need a bound on
on the magnitude of the gradients as
well yeah thnkx oh so yes Lipschitz
Brendan did you overhear did you did you
you're showing it some fixed time the
log loss right that was what those great
right right so I didn't tell you tell
you what it was but this is actually
looking in the online setting so so
we're running this online this is phil
was kind of a continuous setting and
making a prediction at each time step
and then after making one pass through
the data where outputting kind of the
average log loss I see so it was over
the run I see so it's sort of over one
past you have a sense of what the what
the alert the different learning curves
for a degrad and adapt provisions look
like in other words if you actually plot
the sort of more traditional learning
curves as a function of time yeah we
didn't we I haven't looked at that
explicitly you know we were really
focused on kind of this one pass online
setting where I think the kind of
continuous validation is the is the
natural metric but I think if by just
comparing points on that curve you can
get a pretty good idea that you're
getting significantly more speed up but
we didn't we didn't compute kind of
numbers and in the standard standard
speed up numbers ok let's thank the
speaker again
okay so the second speaker of the
session is lee Hong Li from Microsoft
Research and he's going to tell us about
offline evaluation in contextual bandits
okay thanks steel good afternoon
everyone so I'm going to talk about how
to do offline valuation in contextual
bandits so what is contextual bandits
it's a it's a mission learning model to
to capture a class of interaction
problems that is everywhere in nowadays
for example users interact with search
engine the operating system or mobos
again with big companies and then in
this kind of problems there is a loop
that involves the decision making on the
left hand side and the use of right hand
side and this nature makes a problem
more difficult and more challenges
beyond those in arising in prediction
pure prediction problems can arise so
when a user comes to the company by
visiting a website or using the software
or ginga mobile phone the company sees
some of contextual information about
user please pass what is query submitted
the time of day etc and then we're going
to take actions in the form of showing
an ad or a new particle or a software
component to the user and in return
we're going to collect some numerical
reward from the user either in the form
of clicks revenue or adoption etc so in
this loop the distinction between
supervisors from supervised learning is
that usually when you compare to machine
learning algorithms you try to make your
pick shin as accurate as possible so
when you so when you try to predict the
what happens we showed an apt to a user
edge how to make your the cliq
prediction to be as accurate as possible
by looking at block loss but in this
kind of problem you're actually trying
to
maximize the amount of rewards in this
loop and these two problems are
fundamentally different and I will happy
to talk about the the difference offline
in more details so let me give you a
more concrete example of the kind of
interaction problems I called between a
focus plan today so this is a module
with being the search engine so when you
as a user you type a query on on the
search engine and we type too fast you
may have a typo so for instance machine
learning becomes machine leaning a table
that it may all the time modern computer
while more than search engines were
smart enough to realize this is a
mistake and then he can correct the typo
so so now he's showing being as showing
results for machine learning instead of
machine learning so what the module
speller does it Ben is to detect and
correct such titles in the query
submitted by users and when there's a
typo potential table he may generate a
set of or subset of candidate queries to
be sent to the back end so that later
modules in being can retrieve the result
and send the the right results to the
user and here we usually want to
maximize the some quick metrics that is
well defined that measures how happy the
user is with the final search results so
the user don't see the what the speller
does what is what I see is the actual
the final search result pages so here's
a mood hissar most details about how bin
speller operates is around by realm
interaction between bin and user when a
user comes with the query then we see a
query like Shanghai pass and then we
will send this original query or walk
worry to bin the speller and then spell
it may look at this query and realizes
that this could be a tie both so G may
be missing from the first word and
Shanghai is a bigger city in China you
may also think that the original curve
could be right so Q to the second
candidate is the identical to the wall
query
or maybe change the second word to from
past to past so they has so speller has
all these candidates and then he can
send a subset of these candidates to
later modules a bin so that other teams
are being will retrieve the result do a
ranking merging to produce the final
search results pages and then we observe
whether user likes it by looking at
whether your subjects on some of the
articles some of the URLs or whether
user consume some of the contents on the
page so in this this is a very this
natural problem captured by contextual
bandits so in the terminology of
contextual bandits the rockery that is
sent to the input of speller the
decision maker or the learner is called
a context this is the information that
is going to make decision on based on
and the the subset that spell output is
called actions this is the what the
spell of dust and the action affects the
final reward because affects the search
result page and affects the user clicks
so the final reward which is a number is
the number that we try to maximize over
time and then and then we also called
something called a policy which is a
function that I'll just tell you how to
select actions based on the context so
these are all the terminology we're
going to need in this talk and then
fundamental in this kind of contextual
bandit problem like Finn speller is to
do policy evaluation meaning that if you
even given a policy that map's context
to actions how do I know whether this
policy is a good or not in other words
can I know the average reward the policy
collects by running on live users and
this is the important question for
example when people decided ship a new
model then we need to know whether this
is a good one before shipping it and it
can also be used as a key step in the
more general optimization problem so in
terms of evaluation with the two ways
two general approaches to do evaluation
the first one is is all an experiment
mean
that even some idea or you have some
model new model you can just run it on
live users let it go and see what how
well he does and this is essentially
what people do in everyday science
practice it's very reliable but it's
expensive you need to wait a long time
you need to invest a lot of resources to
deploy the model on we'll use on real
systems to see how well it works also it
could be risky right you know the reason
we do evaluation is that we're not sure
about the quality of the policy so it
could be risky another approach is
called offline evaluation which is the
which is my focus is taught today
instead of running the policy online now
we we try to use historical log data to
predict whether the policy is good or
not so this is this is tripping thousand
words there is they already but this is
challenging for a reason I'm going to
explain the next slide so before I'll
explain the challenge later in this
slide let me formalize a problem a
little bit so here we focus on
stochastic contextual bandits well
meaning you don't have to care about in
the detail the mathematical details here
what it means is by stochastic is that
the contacts are stochastic or the iid
the jar ID from some unknown
distribution and then once you have a
context you select an action the reward
depends on there at the context and
action in the stochastic way another ID
way okay so it's not ever serial it
stochastic and the quantity which i'd
estimate is the value of pi with v or pi
which is which essentially is just the
average reward you're going to collect
by running pie on live users for example
in recommender system if you care about
click-through rates then the valuer
policy could be the average
click-through rate if you run it online
okay and then we used if you do offline
valuation based on data that here's the
challenge the data in this kind of
interaction problems is usually in of
the form of contacts 20 the action that
you took in a past when you collected
the data
and the corresponding reward are some a
that depends on action you're not going
to see other actions that are not
selected in the past for that context
right so this is so in other words you
don't have the corresponding reverse
signal or sub a prime when when you try
when you new policies so that's a
different action a prime that is
different from the one that you have in
data so this is called counterfactual
kind of factual what is a kind of
factual question you try to answer in
this in offline valuation so how can we
solve this kind of problem here I'm
going to describe a advice of my
evaluator for this problem so the key
here is the collection of randomized
data so so now you before you do
evaluation you collect a set of
exploration data or randomized data in
the following way when the user comes to
your website would assert the search
engine you observed rock query and then
instead of following the production
model to show the same outcome again and
again you will select one of these
possible actions with some probability
so this is the multinomial distribution
P sub a that you define so this is a
dispersion that you can define on is
known and then you just pick this
randomly sample a from this distribution
and observe the corresponding reward so
that's it very simple and once you have
this exploration data you can use the
the inverse propensity score estimator
to get an unbiased estimate of the value
of the value of the policy that you try
to estimate so what is the estimator is
simply as an average over the data set
that you have the exploration data set
is the average of the reward x an
indicator function which is one when
your new policy selects the same action
as the one in in the rent in the lock
for that context divided by the
probability of a so this is the number
you disability you design
there's also known as propensity score
in the statistic literature that's why
the estimators call inverse propensity
scoring so so since you since the days
and rock is selected from probability P
sub a so the indicator function is 1
with probability P so now to divide that
indicator function by piece of eight and
you're canceling the selection bias and
that's why you can end up with the
unbiased estimator using this important
sampling idea and then this is related
the ideas related to causal inference
covariation shift and offline a positive
reinforcement learning as some of you
might realize so back to the beans pair
example this is the the pipeline of bin
speller we are trying to optimize the
yellow box spello that text in this
input the query and output a subset so
for randomized data collection we just
need to randomize the actions so we
select a random subset for any query and
then observe what user whether you like
the results and we collected about 50
million search impressions and here's
how it works so on the left hand side
you have the position specific click
through rate estimate of the top a
position on the search result page so
one for instance one means that the the
city are the cydia is the y-axis a city
are and the the orange one the orange
part of is the offline estimate using
inverse propensity score estimates and
the the blue one is the actual online
numbers that we we measured by running a
real online experiment so we are
comparing these two and see whether it
close and indeed they're very close
across all positions and on the left
hand side we have the the Bailey
click-through rates and again you can
see that the online ground truth and the
offline estimates based on important
sampling are very close to each other
and this plot showed you that we can
also have some kind of have a way to
quantify uncertainty in our
so the sort of x axis here is the key
parameter in speller the y-axis is a key
metric that people care about a pin and
I can reveal here but the higher the
better so by varying this so we were
interested in how the key parameter can
affect the other key metric if you do
online experiment at very time you can
pick a point you can measure how well
the metric is right by using the offline
evaluation you just need one data set
and all of a sudden you can plot the
whole curve by the whole curve of
predicting the online metric by varying
the key parameter so the curve there is
the prediction of the inverse propensity
score estimates and then the the yellow
region is the confidence region so the
wider do this the wider the more calm
the more uncertainty there is in the
estimates and the star there it's the
production baseline and so you can see
that it's very accurate and indeed the
ground you really lies within the
communist region and then I just wanted
to briefly mention that if you can do
evaluation you can also use that as a
subroutine for optimization so briefly
we chained a model to predict whether
the user is going like a correction
candidate and then we can use a fraction
of exploration data to to tune some
parameter in speller right and use the
importance diverse propensity score to
estimate how I should set the parameter
to maximize the metrics that I am
interested in and did it we used the
deed offline experiments and we ran it
online and it showed a statistically
significant gain 30 stomach win example
that show how the new model improves
upon the ones so so all it seems good it
seems to be working well solve
interesting problems now we ask the
question of whether we can find
something that that is probably optimal
for this kind of offline valuation
problem so we know that one works but
it's a better one where is the boundary
limitation of the hardness of this kind
of problem and so this is a some of the
recent work in this work would take a
minimax approach so here we simplify the
problem a little bit this is this is
just a first step towards solving the
the whole problem here we look at the
single context bandit so we have a set
of actions and then the policy which I
the estimated value for its a stochastic
policy and so we just try to estimate
the weighted mean so r is the reward and
then when you have a data set t of size
n you can have an estimate that gives
you a number that tries to estimate v of
pie ok we're interested in looking at
the mean square error of any algorithm
of an algorithm estimated a and then we
try to see look at the worst-case
performance of this estimator within a
set of problems constrained by some hot
conditions like the variance asperity
the mean is pounded and then we're
interesting looking at the optimal
estimator that solve this problem and we
call that the minimax optimal solution
well the the map maybe a little bit
complicated here but at the end what it
says that is that we want the best
performing worst-case estimator so
that's all it says and then we have a
lower bound for this that capitalizes
the inherent difficulty of this kind of
problem it also gives you a guide of how
to define estimators I don't have time
to go into the details of this lower
bound which looks a little bit
complicated what matter is that it
consists of two parts the first part
involves the mean square error
contributed by not in sampling some
action in your data set even if an
action is not in your ex in your data
set that you just don't have the reward
signal for that action and you have to
suffer some amount of mean square error
because of that so that's the first part
and the second part rated the v1 is
related to variance so when your reward
has a high variance you then you you
expect that the problem is harder you
need
our samples to drive the mean square
error down to some pre specified
thresholds wrong right so this is what a
second part says and then we look at a
few representative estimators and choppy
and see whether it's it matches the
lower bound for instance the universe
propensity score that works well is
unbiased but other hand it's not it
turns out to be asymptotically
suboptimal although it's very popular in
reality and then there's another
regression estimator it's slightly
biased I haven't described it yet it's
very simple I can display offline it's
slightly biased and the interesting
thing is that it isn't illegal is
optimal and with finite time or finite
sample and we can show that this optimal
up to work a factor a multiplicative
factor however for a very small sample
size when n is very small then we don't
have a good solution a way to
characterize how it works so so we have
some answers to this question but it's
not the pictures it's not complete yet
and then the analysis we have in this
work mas can also predict what could
happen in practice so this is a plot
using data from being comparing
different estimators I won't have time
to describe it but what I try to say is
that this plot is consistent with the
theory we develop in this in this
analysis and then these are and work on
single contacts contextual bandits also
have implications to general context of
an item markov decision processes okay
so to conclude so today I talked about
how to do offline policy evaluation in
contextual bandits and then through a
case study of been the show that one of
the estimators work well and then the
next part I show I show some recent
efforts of trying to get the minimax
optimal estimator of solving this kind
of problem and we have some surprising
findings in the future is with
interesting to extend it to large action
problems
ranking when you have many actions then
how can you reduce the variance in this
estimator also how can we get in minimax
optimal estimator for general contextual
bandits and finally it will be awesome
you can do offline policy optimization
in reliable way or more general reliable
way ok thanks any questions yes can we
get a microphone up there thank you um
so my question is regarding basically
the probability of action and you assume
at every point the probability of action
is known and so two faults I can see if
the party of the action changes at the
Bandit learns you can have maybe a
temporal integration but what if you
cannot know what the party of action is
at a particular time that's a good
question so so here are where we collect
a randomized data the audio exploration
data you design the exploration
probabilities the piece of ace right so
usually you know the probability but in
some cases where you don't know there's
still possible to do this kind of thing
the obvious solution was 3 for X
solution would be to estimate a
probability from the data and is not
ideal but it sometimes it works pretty
well yeah so we have a paper in the past
that showed how you can do this in a
more in a safe way question so here so
it's a one advantage of doing it's in
like the speller kind of setting is that
you have very a very small set of
actions yeah how do you think this would
scale if you're like trying to actual
search like you have a lot more action
set because you have a lot more results
right that's a good question so that's
one of the open questions I'm really
interested in solving next steps and so
in general when you have large actions
and the piece of edgewood has to be
small right in this case when you do 1
over P correction in the inverse
propensity score estimation the varian
is high in other words you need to do a
lot of exploration to cover the data so
you have too many things to learn at the
same time so that's the source of
challenge in this problem I think that
the general way don't have any structure
the problem is hard we have lower bound
this as the last part shows but in
particular problem specific problems
like ranking I think it's possible to
take advantage of structures in problems
like you have ranking then it's possible
that you can take advantage of the
structure in this ranking problem to
reduce the effective size of data so in
wisdom earlier this year I we have a
paper that try to solve this problem and
I think it's a long useful direction yes
yeah thanks ok let's thank the speaker
again Thanks ok so now we have a couple
of spotlight talks ok so the the first
spotlight will be given by when we low
and it's entitled finite population
inference for causal parameters thank
you so I'm here to talk about my poster
it's joint work with my advisor Thomas
Richardson so imagine we have data from
this randomized experiment so one of
sample there will be a V testing
scenario where we have a randomized
experiment z there are two values one if
an individual's assigned to a treatment
group 0 if he or she is not we observe a
binary final response why so in this
case for example out of 165 subjects or
individuals assigned to the Z equals to
one group 90 of them observed we
observed mighty of them having a success
so we could carry out a test for the
null hypothesis of whether z and y are
independent so Fisher's exact test would
be one way to go about doing that and we
get a really small p-value of three
times 10 to power minus 22 now what
happens when individuals don't do as
they're told none of us actually do
sometimes we do so we have a variable X
which is actually the trick
chosen by the individual right this
takes place after they have been
assigned to a particular group so X is
no longer randomized and as you can see
from the Bajan network there are
personal preferences for example H let
me lead to a spurious relationship
between X and Y so now what are we going
to look at well can we still test
whether x and y are independent should
we test where the x and y are
independent so in a finite population
which is the contactor by looking at the
set of individuals and our study it's
fixed the assignment of the individuals
in our study to either treatment Z
equals to 1 or ctrl Z equal to 0 is the
only source of randomness that we have
and what we do is we propose a
likelihood based approach to test
whether X is independent of Y for a
select group of individuals so if you'd
like to find out more about who this
lovely individuals are please visit our
poster thank you okay and the second and
last spotlight for this session I will
be given by a Sofia chalak and the title
is a general framework for meta-analysis
of high dimensional network inference hi
everyone I'm Sofia chetak from the
University of Washington computer
science department and this is a joint
work with my advisor sanely in this work
we are introducing any machine learning
framework which aims to integrate
multiple data sets into a single
statistical model those lettuces have
different sets of variables so what
exactly are we trying to solve as you
may know genes interact with each other
and many diseases many diseases such as
cancer are caused by mutations and those
mutations lead to perturbations to begin
network so one of the fundamental goals
of cancer bioinformatics is to
understand how those jeans are
interacting
each other and how they drive the
biological or disease processes and the
limitation of a standard network
inference algorithm is that we had
thousands of genes which means they may
interact with each other and we have
millions of variables in network and we
usually have a hundred of samples so
patients which is usually to fear to get
the underlying network structure and
also it's becoming more common that we
have multiple studies and multiple data
sets from those studies so a natural
solution is to increase the sample size
by combining those data sets however
different data sets usually do not have
exactly the same sets of variables which
makes it impossible to combine those
data sets directly into a single model
and to resolve this challenge we propose
the combined module network estimation
framework and we assume that the
variables are organized into modules and
those modules are help conditional
dependencies so they are interacting
with interacting with which it each
other and those conditional dependencies
are conserved across the data sets many
real-world networks especially the gene
expression networks are known to be
modular and even if the jeans are not
exactly conserved across the data sets
the biological processes the modules are
should be concerned based our assumption
and so combined leverages this concern
modularity of multiple data sets which
makes it possible to combine these data
sets into a modular system model and
this also important thing is that it's
very hard to learn a network of
thousands of variables by using this
modularity assumption we can reduce the
dimensionality a lot which are results
in an efficient
framework in a coordinate descent
procedure iteratively update three sets
of parameters first are the latent
variables each corresponding to a module
and the second is we are obtaining the
assignments of variables to modules
using a mean by minimizing the Euclidean
distance and try to procedure and the
third set of variables is the
conditional dependency structure for
this setup we can use a regular like
rough learning algorithm like graphical
also you may know and in our experiments
on simulated data and ovarian cancer
gene expression data we show that the
network learned learned by our combined
model can fit unseen data better in a
cross-validation setting and also the
discovered modules are have much more
significant overlap with the non
functional gene categories and we also
use this Cancer Genome Atlas Project for
ovarian cancer data and we show that the
combined latent variables are much more
significantly associated with important
histopathological phenotypes and we also
show that the subtype the patient groups
learn by combined latent variables
better reveal the genomic abnormalities
of the patients if you want to learn
more please come see my poster in the
inning thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>