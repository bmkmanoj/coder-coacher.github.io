<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Session 1D: Environmental Applications | Coder Coacher - Coaching Coders</title><meta content="Session 1D: Environmental Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Session 1D: Environmental Applications</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JuEUIAjEkxA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so I'm hi thanks for coming the
afternoon and you guys managed to avoid
the beautiful day and come into this
room I'm very grateful I think I'm going
to talk about ecological networks and
our is moot creating our ecological
network research are actually porting
our research from basically notebook
computers and clusters onto the azure
cloud basically a tourney make the
application a cloud service and write
what you see here is one of our neck
networks being simulated the size of the
sphere these are plants at the bottom
these are herbivores you have some
carnivores up above the the nose ring on
those nodes are conveniently indicate
cannibalism the size of the spheres
represent the abundance of the organisms
as it's changing during the simulation
the size of these cones let's see
connecting the nodes represent the
amount of food are the feeding rates
viam if the rate at which the note at
the fat end of the cone is eating the
species at the skinny end of the cone
and what you see is a high dimensional
nonlinear dynamics basically despite not
having any DNA or government to
orchestrate all these interactions
ecosystems persists over time species
don't go extinct and a lot of our
workers have been focused on why that is
so why don't they just have huge
population outbursts and crashes and
stuff that you would expect from a
random system they aren't occurring in
ecological systems so we figured that
out and now we can have these
simulations in computers and do a lot of
science and that's what I'm going to
talk about so this is a collaboration
from a bunch of wonderful folks that are
based at the San Francisco State
University and our peace lab
Pacific eco informatics and
computational ecology lab now let's see
yo me and Paul Yoona right over there
who have worked a lot on this Paula's
moved the our applications into unto the
azure cloud with NSF support and a lot
of work done by rich williams at
Microsoft Research he's basically the
generator of the network 3d application
and that we put into the cloud so um
here's an ecological Network this is
something I put together this is actual
data that i put together as a graduate
student we changed the color of the
nodes to represent whether they're at
the base of the food web in a lake that
is LG zooplankton than fishes at the top
of the web we change the color of the
links to represent the type of trophic
interaction whether it's herb every or
karn every and I'm going the the way we
represent the diversity of these
networks is just the number of nodes
which is the numbers of species we
designate that as s and the number of
links is just L and the important
parameter that we use to to measure
connect ins or complexity is connected
sand that is the fraction of all
possible links that are realized which
is the number of links divided by the
square of the number of species it's a
fraction between a minimal and maximum
web eNOS usually it's around fifteen
percent in food webs somewhere between
five and twenty five percent and what we
did who it really is a real important
base of our simulations is this model
called the niche model it takes the
number of species in a food web and the
amount of connectin sin that food web
and builds food webs according to three
simple rules and a bunch of papers that
we've been publishing and it continues
to be get you the niche model is
continuing to get a lot of use based on
the idea that it builds food webs with
ecological networks that look very much
like those in nature and this is the way
it works these are the first these are
the three rules this is the algorithm
first of all you take the s
the number of species s and you give
each species a uniformly random number
between zero and one basically you place
it on what we call the niche axis they
are located in niche space second rule
is each one of those species gets a
range if it gets a big range it's
probably going to be a generalist if it
gets a small range it's going to be a
specialist it actually comes from its
the R sub I it's a comes from a baited
function parameterize such that its mean
is two x connect ins but then whenever
number you pull out of that distribution
you multiply it times the niche value
which is the number of 18 0 and 1 it
averages 2.5 so it gets a bunch of
different ranges but they all average to
connect ins and that RC so that makes
your web that you get out of this model
have the same amount of web enos as the
parameter you put in it last rule is all
you have to do is place this range on
the axis by choosing a center somewhere
between the niche value of the owner of
that range and half the size of the
range so it doesn't fall off that's it
those are the three rules and it does it
does a wonderful job of making food webs
that look like those in nature I won't
go into all the evidence but I want to
cite one of the more impressive feats
that it did that is it successfully are
predicted just based on the number of
species and amount of connectives the
overall structure of these food webs
that are have a half billion years old
there we work with a bunch of wonderful
paleobiologist to characterize these
food webs from the Burgess Shale and the
chinking shale and the Burgess Shale
wonderfully preserved very famous
formation with these dr. Seuss
characters like a invertebrate with 5
i's this one was caught is called
hallucinogenic wonderful critters they
figured out based on mouth parts and a
lot of the same sort of stuff we use now
to figure out who is who who ate who
back then and so what I'm saying
is that we understand network the
architecture of these networks pretty
well and that it's really robust there's
something about DNA and proteins and the
things that all life has been made of is
that these network structures don't
change that much there there's a lot of
variability but there's a strong central
tendency so um I generates a realistic
network architectures which you could
imagine is very useful for simulations
it provides a benchmark so we know when
a food web looks weird or when it looks
kind of normal and it forms a
scaffolding for network dynamics namely
this is a niche niche model food web and
the structure came from the model the
architecture model but now I'm going to
go on to dynamics how did how is it run
dynamics and it's just an ordinary
differential equation that models the
change in a species biomass is equal to
if it's a plant we have different sorts
of growth functions that plants can grow
but everybody loses biomass by the
metabolic rate of the organism times it
smile biomass because it takes energy to
stay alive then plus everything it gains
from eating minus everything it loses
from being eaten that's it there there's
there's there's some interesting
nonlinearities mostly coming from this
functional response that calculates the
rate of consumption based on the
abundance of the prey but otherwise it's
a straightforward o de system that gets
simulated as you see and one of the
things we've done is with these dynamics
is we've because network structures are
highly variable a very key parameter is
body size let's see and so what we can
do is generate in this case tens of
thousands of different networks and
let's see parameterize with different
food webs different body sizes and
empirically reasonable large things
eating small things that we
in nature all you do is stuff like that
we can remove boom a species from these
webs and the effect of a species on a
web or what we call interaction strength
is the difference in the abundance of
that of these nodes when that species
were its present minus the abundance
when it's absent that's how you know
what difference it makes if it's in the
web you take it out and you measure the
difference and what happens is that you
take a look at this web actually a lot
changes but I'm what we did is we
measured about a quarter of a million
interaction strengths inside the
computer just generating these webs
taking every species out one by one and
what we found out and it's missing well
there's an algorithm that we could use
to protect predict that interaction
strength just from our in silico
experiments and the prediction is it
depends on the abundance of the organism
and its body size and the abundance of
the target organism that you're that
you're measuring the effect on between
those three numbers we should be see
these dotted lines those are our
predictions either when the remove
species was very abundant or when it was
rare and it here's the data from Eric
Barlow's experiment when he was a grad
student in yellow I'm sorry in blue and
red dots matching least two different
conditions and you could see that the
regressions of the data really closely
overlapped the prediction namely we were
able to base it predict the effects of
species removal on a field experiment
that's pretty cool a lot of the
ecologist thought that was important
that makes what I'm going to talk about
like oh well they aren't just plain and
quit with equations they might even be
talking about ecology another example is
where we applied these this model we
call it the L metric trophic network
model Ella metric having to do with body
size trophic having to do with feeding
network obvious to this lake constance
food web we have about twenty four
species 104 links about eighteen percent
can
accidents there's fishes on the top
algae at the bottom we have a lot of
bacteria here's what happened is they
looked at the abundance of all these 24
organisms over like 20 years every two
weeks Germans are so good that way man
they were out there they were measuring
it and this is the year average
abundance represented in the size of the
Sears a lot of bacteria a lot of these
daphnia not so much fish some very wrote
rare ciliates and rotifers and the
average feeding between them over a year
but it didn't stay the same over the
year in the winter there's not much
green biomass there's lots of bacteria
but then when it becomes spring and
warms up the plants are very happy but
then the carnivores and herbivores are
even happier than the plants they get
really abundant they knocked down the
plants in this really famous clear water
phase a really impressive dramatic event
in Lake ecology we're often the water
you can just see it it's like wow the
water is nice and clear for know several
weeks to three weeks but then the less
edible LG become more abundant and you
know the dynamics of species just change
pretty much every year and this is what
we do did was each one of the phases of
these periods we just reduce to an
average biomass during that phase and so
here are those same data this is the
observations this is what our tuned
model outputs to it and it pretty much
gets a similarity of point 8 to gets it
pretty well in for the these very rare
species they're rotifers and ciliates
set like five species of each of them
about or four species and five species
they're pretty rare we over estimate the
model overestimates some of them
underestimates the other but by and
large the model does really good and
that opens up I think ecological
forecasting something like weather
models are numb you have some basic
equations maybe you have some real-time
data to feed it you keep on tuning the
model as you
use it to actually predict what will
happen in the future this sets you gives
a sort of a platform to go on with that
we can also look at the effects of
humans and food webs that's a pretty fun
food empirical food web that Jen done
put together up in the Alaskan ogre
archipelago of the sanuk islands that's
something you could look at the effects
of humans and food webs but more
importantly what we want to do is look
at well not more importantly differently
we want to look at the effect of fishing
so instead of those that o de that
represents with biological processes we
have one that represents economic
processes supply and demand and we can
plug those into the food webs and see
what happens here we're speaking three
different species the classic model used
for fishing is logistic growth just a
single species that grows up to a
carrying capacity fishing managers know
that's not true but they don't have very
many a good alternatives I think ours is
a good our model is a good alternative
and what you have here is what with a
logistic model you have the predictions
in these straight lines of the effect of
percent removed on the abundance that's
left in the system and what we find in
our network model if that abundance
decreases a lot quicker than the little
autistic model in fact if you do what
look a maximum sustainable yield
recommends you'd basically drive these
species extinct and indeed that's what
we've seen many times over in fisheries
history but um let's see but let's see
they don't there's a lot of statistical
modeling very little mechanistic model
I'm going to just go on to UM the how
many minutes do I have now
cool great so what I want to do is like
I've tried hopefully motivated you like
hey these are some neat accomplishments
one of the things that's computationally
intention intensive and what we
generated created was this a food web 3d
application that does some neat things
what people really like is that it
visualizes networks and you go to any
food web meeting almost everybody uses
it because it's the best tool for that
it does a bunch of the calculations so
people don't have to figure out what
let's see program how do you calculate
trophic level mean chain length a bunch
of pretty major things difficult things
and I'm just showing some it's got
actually quite a bit of let's see
possibilities it does a lot of neat
things but what we've done I'm just
going to kill it now is what we've done
is we've mostly Paul by the way has
changed this in using the Microsoft
Azure into a application as a service
where here's the basic architecture
where we store the time series in the
market the azure SQL you can even get to
it either through a browser or the
applicant client application that I
talked about and you could spawn a whole
bunch of simulations my student just
told me that her computer turned off
after five days and we lost five days of
work and we have different ways of
clusters but basically it's a pain in
the butt to run these things hopefully
this is going to make it much easier
where you guess you when you need your
data you just query the database so one
of the things I'm going to talk a little
bit more about is this world of balance
and we not only use it for research but
we use it for let's see a game and I
think it will first of all I want to
just make the point that was fairly easy
because it was already using Microsoft
foundation classes and stuff so
move it over to Azure pretty quickly
luckily NSF gave us one of those cloud
computing supplements that gave us the
resources to do it and um let's see and
then it becomes very accessible from a
different a bunch of different applicant
other applications one thing that want
to note is that because there's
additional parallel capabilities within
Azure you can take what is a large web
this 156 species web which really can
take hours to simulate and it on a pc
like what are we having 80,000 seconds
or something but because the azure thing
Azure gives you ability to paralyze a
lot of the different activities of that
program we could do it a lot quickly and
of course you can this isn't even
including the fact that you can get a
whole bunch of food webs going being
simulated at once so one of the ways you
can simulate a lot of a food webs is
having okay making a game out of this
service and you get your plates world of
balance after World of Warcraft and what
you can do is you plant plants in the
Serengeti food web and you can buy
elephants and or Robbie and all these
different organisms and then you've got
to manage it because it can overpopulate
and under populate you get you get gold
to buy organisms um let's see I'm going
to actually that's right I have to wait
oh yeah let me just see if this works if
I can get that you to clip is this going
to work
this now that's changing
I'm not being able to see everything but
you're going to get the picture okay
okay so this this is just a video of the
gameplay where the person's stocking the
organisms and one of the things that's
going on is of course we have this model
running the dynamics and so people can
learn notions of ecological
interdependence well and the balance of
nature in a very realistic manner i
think as real as realistic as us
ecologists have been able to achieve i
think at this point i mean let's say
they keep a score of how many species
they're able to maintain there's
endangered species in within these food
webs leopards might be the most famous
one and so you can get a score and
points based on your ability to manage
this ecosystem and it's a it's a
massively multiplayer game so you can
spend money and if you got too many
hippos dump them on the neighbors and
then the mid that that player has to be
like oh no I just had 100 hippos
invading they have to spend money to
deal with those hippos and things like
that yeah especially Berkeley and so and
so you get an idea there's the game
there's the game so I just wanted to
tell you I think what the future of this
basically cloud-enabled exploration of
complex second logical networks is what
I've been talking about more games and a
research simulations you've pretty i'll
probably all heard of the folded game
and the wonderful thing that it was they
actually figured out some really
important research i think what we're
setting this up to do is so the time
series of gameplay is saved and we can
mine or access those data to see who
maintain the most species and if you
we're going to add we're going to make
fisheries versions and basically we have
a bunch of people searching parameter
space for us and running these
simulations and we can search it and
mind those data for a scientific it
discoveries we're also going to
integrate our models with other data
there's databases on fisheries landings
which is you know how much fish was
bought for how much money in Vancouver
and all up and down the clothes so we
can find parameters by linking to other
data and we're also going to employ
matlab res simulations and then if
there's some real time data tuck telling
us how much life there is coming into
the lake well we can integrate that then
I think the most exciting thing is the
sloop the solutions that we might obtain
by doing this that is we will find out
more about the behavior stability and
robustness of ecological networks think
we're going to increase understanding
and management of human natural networks
and I think the more most fun is the
social networks and public appreciation
of ecosystems not only economic and
ecological in Turpan interdependence but
I think people probably are I think it
will increase the appreciation that hey
I'm the heart of an ecosystem humans
have impacts what are those what are
what is that relationship so in other
words ones place in ecosystems so thanks
a lot for that hopefully I've left some
time for some questions you have we have
time for questions good and you but by
the way that species that I got a what
took out the parasite basically this guy
got really big did not aight these
plants down to low amount and that
caused the extinction of about half of
the web so you know this is like that's
the parasites can be important lesson
that came from there so i have a
question in terms of how do we so each
of those blobs is a species so how many
species can you handle in one simulation
I know visualizing vast large graphs as
its challenge computationally but but
how big can you get we've haven't really
hit the limit yet we haven't I mean
because there's some right now commonly
we're dealing with tens of species
somewhere between 30 and 70 species
seems to be our sweet spot
for the longest time last 20 years two
species five species were what
ecologists worked on um there isn't
anything doesn't seem there's we haven't
found anything we've done hundreds of
species we haven't seen any qualitative
difference there hasn't been any benefit
worth the cost of going there but
computationally I don't think there's
any fixed limit the hardware can deal
with anything it's just it takes a lot
more time I was thinking of the you know
when yet to bacterial levels you have
many many many many just as I learned
today in my own body that's like a hotel
of a lot of species of bacteria that I
would think the scalability would be an
issue another question I have is can you
take a model like this and incorporate
evolutionary properties so new species
emerge to take advantage of
opportunities as a college estando
microbial point um that that Lake
Constance is pretty much in microbial
food level it's bacteria in its Russo
we've got I think we do a pretty good
job with microbial dynamics and
fortunately and say you know what is a
species of bacteria no one's really
saying no one really knows the answer
and we can functionally group them into
ever gets to deal with the fact that
there might be 10,000 different species
in there now as far as the evolution we
cook we call it the web Aleutian it will
be televised with these graphics we have
I let's see we have a a postdoc I have a
postdoc Rosalyn that basically you start
with a few species let a little hopeful
monster with a little bit of different
parameters emerge see if it lives or
dies and that we've been working on
years it's tricky this stuff is really
tricky and you know we've gotten our
results and hopefully we should have a
paper by the end of the year on that
subject we can and we are anybody else
about parallelism people use o it well
it depends which we write exactly so
usually what the parallel ISM results
everywhere from sneakernet to no
parallels ISM what happens is i think
the way it's paralyzing Paul you're
correct me if I'm wrong what because
there's so many food webs we give each
CPU a food web and it really doesn't
have to communicate very communicate
with anything else but then additionally
within the program there's a lot of work
that just calculating trophic level that
looks at the network and looks at the
amount of energy going through all the
different links that's computationally
intensive and it seems like one cpu our
son gets that gets does that and so
within even a single food web there's
some parallel ISM that goes on so it's I
think I liked your term pleasingly
parallel is that replacing
embarrassingly parallel yeah I like the
semantics yeah good good good reworking
of the word so I think this is mostly a
pleasingly parallel system that doesn't
really need much sophisticated a
discussion between it would be possible
who is your colleague on the stuff I i
would think that within a food web you
could do something that you have a large
system of ordinary differential
equations and you're doing probably some
rung a kutta method or I don't know how
it's solving the 0 DS but there's a
little bit of concurrency that you can
run across a systems and you would do
that well I would say that you could do
it like no well I was thinking within us
within a server where you have a
multi-course server yeah do it at that
level yeah I think well because each
node really only has to track its
immediate predators and prey that would
be the level of information exchange
that I guess one cpu or one core could
attend to so I if but you know this is
dynamic simulations so it's fractions of
time steps that you want to keep track
that might not be worth it but
theoretically yes each core could be a
species hey questions yeah oh thank you
very much thank you heck he's qua so Tom
and he's going to be talking about cloud
computing as cyber infrastructure for
mass customization and collaboration I
confess it when I created I was got
stuck with a job of putting together the
program and dated this session calling
it environmental applications and I
stretched the meaning of environment in
a number of ways but this I would
definitely wanted to make sure this
paper was presented thank you for the
introduction I'm very honored to be here
today and i plan to peace and ease talk
about cloud computing as a cyber
infrastructure for mass customization
and collaborations my talk will be to
third on a specific application related
to forecasting and then the one rest
would be devoted to how this can be
expanding to more general applications
in the trend of mass customization and
collaboration so i would spend some time
on renewable energy forecasting which by
itself is an important topic then I hope
you send the solutions that we develop
call the forecast as a service framework
and then as i said earlier will devote
some time on how this can be expanding
to more general applications
so the renewable energy as we know is
uncertain and is fluctuating and this
diagram shows the gist of some of the
issues involved so this diagram shows
the solar energy that we receive on
earth in general the solar energy
received on the outer side of the
atmosphere is pretty predictable Four
Seasons orbiting around the Sun and
rotate but because of the atmosphere the
energy has to travel through the actual
amount of energy receive is not
predictable and it's not easy to predict
so the blue line is based on what is
called a persistent model that is based
on an AO clock measurement so if things
persist this would be the line that we
will follow the power that will get in
terms of water per meter square by a 05
some cloud come by and left and we have
an increase in energy now this is the
new path so you can see that throughout
the day these fluctuations is not easily
predictable things although they are
methods to improve and over different
time horizons we might have different
forecasts for different uses so the
accurate forecasting is quite important
for effective utilization of solar and
wind power one way to deal with this
kind of problem is to provide backup so
we'll just assault all these
fluctuations by another system of
generations or we are were using this so
you would have some impact on the
economics as well as the deployment a
problem of solar energy and wind power
forecasting related to cloud is the
diversity in data and the volume of data
as many users have alert you so we
obtain data from different sources
federal agencies is one major source
such as the OE different energy lab we
have national databases that provide
historical data
that some people use for statistical
analysis private organizations some
environmental group also put out datos
University nonprofit organizations
international institutions and there are
companies that sell data for profit and
then of course the person can have their
own measurements the data types are very
very satellite images provide by Noor
geostationary satellite images sensor
data of different kind there are
computer model data human expertise and
this product data from different vendors
that change raw natural energy and
billion solar into electricity and they
come in diverse data formats in addition
there is a diversity in approaches in
forecasting there are physical models
that actually use the physics fluid flow
and so forth there are statistical
models based on time series data the
heuristic models such as the moldos
people used to interpret images of
satellite images about what how much
cloud is there and how much moisture is
in a most fear hybrid models that
combine different combination of this
and there are different ways to deal
with the uncertainty so one major source
of uncertainty is the more or less
chaotic theory of the atmosphere and was
very weather model so if you have a
slightly different initial conditions
and the results could be quite different
let's say for physical models in
addition there are diverse needs that is
different people need different type of
informations in different details
Engineers need detail for design
operating its detail for tomorrow's
operations order next week so that they
can plan ahead they are planning
decisions how much we should build for
let's say a wind farm and people who
invest money into this need to know is
there enough energy in these locations
to justify investment so we try we are
developing a FAs framework forecast as a
service to support on-demand delivery of
renewable energy forecasts different
types different levels of detail and for
different prices it is I hope that if
this kind of information is available we
will enhance widespread and more
effective use of renewable energies and
this would be good for the environment
it would be good for energy independence
so framework look more or less like this
and more little more detail later on
basically we have three frameworks
inside the FAS framework one is to
handle external data so let's say this
framework is for wind power we need
whether informations we need your
graphic information for particular
locations for example mountains tend to
provide more turbulence and ocean 10 to
4 y more moderating effect and same with
the lake and we have different types of
measurement data and we have equipment
from different vendors that change wind
power into electricity then we have
internal data that system already have
done some work and we can retrieve
though if i'll go into outside help and
this forecast generator framework try to
address the needs from different users
so there are wind power users there are
in power producers they saw the cell
wind power power companies that have in
wheels and potential users or individual
users we are trying to develop this with
the service oriented architecture
concepts so there are three frameworks
within this so each has a consumer of
in service within its own purpose so
there's the EDC f SOA architecture and
then an internal data retrieval
architecture and then a forecast
generation architecture implemented
different stages of forecasting
activities and they orchestrated by this
FAS controller so in a sure the general
process is something like this that the
user request is handled by web role
which will check a metadata repository
for any information that it has your
class is put into a queue and execute by
worker role that invoked an instance of
an faas controller which is a WCF
service this would control the workflow
first you will see if you need to get
data from external sources if it does
you would go ahead doing it and deposit
the data in the blob storage then the
internal retrieval framework would take
data from that storage and change it
into more standardized structure format
and then this generation framework will
do the work using the more standardized
structured format in the azure table so
a lot of these are implemented using the
WCF service and the way to deal with the
diversity of data is a two-step process
so we get the data from different
sources in different forms and put it in
blob first and then we have procedures
to change that into more structured data
and put it in table and the standardized
data is what we use for our analysis and
forecasting and so forth so any updates
are sent to the metadata repository so
when a user make requests we immediately
know whether we can meet that request or
not and the FAS controller monitors and
controls the workflow
to do the work we're trying to use the
frequency domain approach a time domain
data is first change into a frequency
domain and we find that this frequency
domain approach is less vulnerable to
noises and outliers have provide more
structural representation or information
and more structure in representation for
automated machine processing in our
procedures so the SOA concept allow us
to design different layers of services
so basically based on the user requests
we define the problem and have different
task that needs to be done this may be
view s activities in workflow and this
uses different composite services and
then this composite service may be built
from different more fundamental services
that might be shared by other more
sophisticated composite services and
fundamental services interface with the
databases so this is generally what we
are doing and the next section of the
presentation will be on how we
generalize this framework into more
general applications so one thing we
look at is the mass customization switch
is a new relatively new trend in
business the basic concept is to
increase the variety of products or
service to meet customer needs with our
large increase in production course so
to some extent you try to reap the
benefit of mass productions but it's not
producing one or two things effectively
so we have talked about the explosion of
data and so forth so this chain form
data and to information into knowledge
or three steps can benefit form cloud
computing
using somewhat of a mask customized
service framework so the concept in mass
customization is quite similar to SOA in
many aspects so in service-oriented
architectures we pay attention when
during design on service when I do lity
component we talk about modularity
service autonomy corresponding activity
will be component dependence service
reusability the component reusability
service composability component
configurability to have more complicated
components coming from simple
interchangeable parts loose coupling in
both cases important thing standardized
contracts in SOA is also important to
have standardized interface in mass
customization service metadata and
discoverability the component metadata
is also important and is important to be
able to be discovered so the diagram
that I just shown a few minutes earlier
this FAS framework can be make more
generic in a sense that you can be used
as a framework for mass customizations
so the external data framework collect
data from different sources and internal
data source and then forecasting
framework will be changed into a mass
customization framework which to some
extent is what the forecasting frame
will try to do in essence so there are
different ways that this could be useful
so already being implemented industry is
mass customized products I think to be
more important in coming years our masks
customized service delivery service from
this data information knowledge chain
and another important area is mass
customized learning so right now a lot
of the custom
make education program have not been
implemented because of course of
different reasons we are more or less
still use the mass production model in
education so it's very possible that the
mass customization provide by cloud
computing through the internet can help
in this sense and other trainees
collaborations is a new trend driven by
funding sources and some natural reasons
sharing our resources and expertise
division of labor separation of concerns
and collaboration is enabled by advances
in technologies in communication
collaborations and Kyle computers we
play an important part so this is
another diagram that I show a few
minutes ago that has been generalized to
provide a framework in collaborations so
let's say we need to solve a
multidisciplinary problem so problem
definitions then split up into different
steps data collection analytics
generation alternative final solutions
then the next layer is different
composite services and we can have
compulsive surface bill for
multidisciplinary domains and the next
level is more basic fundamental services
in different domains and in the platform
and then we can assess two databases in
different disciplines so the idea is to
expand the SOA that is usually applied
to an enterprise into more or less a
community such as engineering community
SOA that different disciplines in
engineering can cooperate a healthcare
discipline can also use this kind of
stuff the important thing of course is
standardization in need to be
standardized to interface with one
another
and some other SOA concepts like this
cover bility reusability and
composability so my conclusion is that
the custom specified renewable energy
forecast can be delivered by the FAS
framework that was presenter Leah are
using the azure platform and this
framework can be viewed as a framework
for more general mass customization in
the cloud and a framework for
collaboration in the cloud that's all
thank
so let me ask one I was intrigued by the
your online education example for mass
customization or for higher education do
you have an idea of an experiment that
could be tried you said that you applied
that there are many things that
prevented that from happening in the
past but this framework can make it
happen so can you explain that a little
bit more okay we are only enough to
remember the internet bubble about 10
years ago at that time there are
different companies they were formed to
do what is called eLearning if you
remember so they provide different
platforms and they / y monitoring things
in the sense that they try to provide
education on the internet 10 years later
a lot of this company folded they no
longer exists part of the reason is they
more or less just do the technology part
did not do the content customization in
other words I mean if you are learning
human resources then you'll learn the
same thing regardless of your ability
regardless of your background regardless
of how fast you're absorbing and so
forth personality stuff so I believe
that mass customizations could help this
kind of learning by taking into account
of individual need individual ability so
is anybody trying this some some are
because of this internet bubble thing
and all the people has been having you
know sphere about is the same thing or
is it not so but in fundamentally
there's some difference
so what about your framework that you
outline makes it easier to customize
such material compared to previous
frameworks okay the framework we
developed is based on this architecture
and go back to that
it's based on these architectures so we
already have a lot of these components
in place taking external data from
external sources have a more or less a
workflow workout with the service abuse
the observers stuff and can meet
different needs take into account of
requests from them to have a more custom
tailor output to each individual's
furthermore we have these layers of
services that basically we know how to
build services that can have attend to
individual discipline and across
disciplines to your composite services
we have basically the fundamental work
done already so all we need to do is to
have new content using this framework
then we can apply to other discipline
for different applications
is based on the requests that we set up
this service and this stuff this whole
process can be used basically to meet
varied user requests so in our case is
forecast requests in the education needs
it could be they want to learn something
and then they provide more data from
other sources and we take into account
of that and come up with an output so it
is that structure that program that we
have so let me ask so you've done this
with the forecasting for energy usage
which looks like a great example are you
going to attempt to apply it to a
different pic another area and apply it
or you gonna how how do you make this
work in other places to make this work I
need partner for mother fuels so we know
energy we know this kind of programming
we need expertise from other fields to
cooperate and develop those services
that is sufficiently applied so we need
partners basically so maybe you'll find
some here all right any other questions
okay thank you very much
last presentation is free 0 bars on from
a company called collaboratory oh and
this is a startup company looking at
Green Green civil engineering basically
in buildings they have been working with
our partners in Europe and I think it's
pretty interesting application okay I am
very happy and proud to be here so thank
you to the organizers I see Dennis here
because probably I am the very first not
in for information presenting at cloud
future I'm an architect I a master in
our master degree in architecture and
I'm here to to bring out testimony from
the building industry that in the next
decade this industry will require a lot
of cloud computing tools and computing
in general i am presenting here a work
we are doing in green prefab that is the
name of the startup that in the
incubator collaboratory us as preparing
in the last couple of years and we are
working also with with other partner and
the other contribution i would like to
bring is that this startup is starting
from the reserve the results filled in
in Europe specifically in the divinity
project that was a great chance for for
this data and so first of all I'm
talking about an emerging market in the
world so when I when I say green
buildings I think to an emerging market
a green building is a building that has
a received a certification that prove
proves that it is agreeable those are
the certification in Italy so we are
talking about a mess about certification
there are a lot there are not yet common
standards about certification but Sam's
are coming on a global scale the world
market is expressing good numbers but
especially it is in this tremendous days
it has a growing rate annual that is
very encouraging so we think that in the
last in the next 20 years this sector
will be will be growing a lot with
different kind of buildings with
different size of buildings around
around the world so the solution that
this platform I've found it is proposing
is quite simple is a technology transfer
of material technologies and processes
from industrial sectors to to the
building industry now this becomes very
feasible if the actor the stakeholders
in the building industry using share a
digital master model of the 3d building
to produce in this case green buildings
and basing the production of the
building on prefabricated components so
part of the buildings that are
controlled in terms of quality into the
factories with this structure and model
in mind we started to producing
different prototypes this is the damn
that is online you can go and and play
with it since it is a 3d database of a
catalogue a 3d catalog of components
prefabricated components that architects
use like a legal system within their
card system they cut software and with
those they produce the the green
buildings that we
we have also in the website available
for the community so three main concepts
I have to share with you prefabrication
is the main pillar of this system it
means we are controlling in the factory
the quality of the single part at the
same time we are very fast into the
construction site in order to deliver to
deliver a high quality building we are
talking about high performance is
building especially towards energy
saving in this in this case but that
process of contraction construction is
controlled by what I call digital a
digital master model so the building is
virtualized into a 3d model where each
single part is connected to a database
that is talking about it it is he's
talking about this physical behavior who
is producing this particular component
how long does it take a set of
information around a single audit within
the 3d model and other sets of
information for the whole model itself
we are based on a interoperable format
called IFC that is now creating a
community called open beam beam stays
for building information model so it's a
an open structure of data delivered by a
worldwide consortium that is called
international use for interoperability
now known as building smart
community-based also here at Lawrence
Berkeley National Laboratory he started
here those those digital models control
the construction mod he deals with all
the life cycle of the product building
so starting from conception and design
going through engineering and tastic
executive out with production into the
factory realization on site on
construction site and use of the
building
that in the life cycle the longest one
and and also the life cycle ends with
the dismantling of the building during
all these phases different people
different companies with different
competencies uses a lot of different
software's that not always are able to
communicate each other and a lot of
problem this platter is not available at
this time in the world except for very
very restricted players they are using
this kind of model for single building
single prototypes of high green
buildings there is that this model can
be open and democratized to to let
everybody join this kind of production
in the building industry so the phases
sharp this called call it database a
part of concept of engineering
production operation where where is
cloud computing useful for those kind of
players surely here during the concept
and engineering where we have a lot of
different engineering analysis to make
into the building particularly we have
nine in this platform form the means you
have immersive rendering immersive
environment where you check the 3d model
directly inside the model in the virtual
model you have structural calculations
piping pal and pumping plant plantings
or electrical energy performances you
have to check heating and ventilation
lighting simulation or the production
realization in terms of a program of
works that you will have to follow and
computation of course so we had the
chance to enter the VNC project two
years ago the project will we land
the last day of this month and we
started to do to develop the first cloud
computing tool for this kind of platform
we started with the easiest one that was
based on a software that had already
parallel computing itself it was about
rendering visualization we asked our
communities that is based on an Italian
community so far of twenty thousand
users architects most of them and
engineers how they are outsourcing the
services and we find out that rendering
was outsourced for thirty percent but
also video and virtual reality if we can
together more of sixty percent of this
kind of visualization outsourcing from
our users so they could be interested on
using a web interface we're outsourcing
the this kind of work also they are they
would like to use this system to
accelerate project completion of course
but also for a lack of skill within
within Vera they stood so they are the
the community of architect and engineer
are already available to use 3d models
but they are using just for rendering
right now not in an integrated platform
light edge so the architects until the
website i found 3d being the component
the import in there cad software we have
a specific path for google sketchup and
then they upload the model inside the
website and from there they can do
different things with the model and
apply different tests in the model we
develop a dedicated plugging in this
case in order to deal with the lack
surrender that is the
rendering engine we selected for this
first implementation it is an
open-source GPL physically-based render
means it's simulate the the algorithm
simulates exactly how the lights move
into the into the rooms around the
building with no reduction of
computation and also is dealing with one
of the major cuts its complied with the
major cuts around at the end the vape
reduction is a single picture a PNG
picture so we started to to manage how
to deal with architects and engineers
that don't want at all to set complex
data or virtual machines int'l into the
cloud they needs us to have a very very
simple and easy interface to to connect
to the cloud and we define a very very
simple and basic system in order to
expose a service toward the cloud at the
end we understood that we needed more
services for different kind of clouds
that was also available into the
research project when you see at the end
we developed different infrastructure
for different tools and fraud different
resources exactly we had microsoft
windows azure available and other linux
based computing resources that are
related to barcelona supercomputer and
confesses infrastructure controlling
also opennebula resources a small amount
but an out for early for early adopters
adopters into our community so
technically the description description
this game is that we have a web service
within our you be the hobby that is the
name of them of the service that he is
waiting for the
request user submits a job in this case
that is a file that the set of files and
some parameters that they have to
control and we have an a synchronous
invoke that caused the the service and
post delivering idea of the job to be
submitted a through ID this service gets
from my sequel DB settings and from
storage the files that are needed to
submit that on windows edge during the
day is a crucian the service the service
cell updates data in our server in our
DB for use on feedback at the end we are
the result of this PNG picture this is
the architecture specifically within
windows azure irma basically is master
controlling different difference leads
what is important in this case for our
exploration and I would look I would
remark to invite more people like me
coming from application site because the
biggest problem was not in this case
technological but was to let our basic
user to use this kind of technology so
so they they interface map in the
interface at the end the graphic user
interface must be very very simplified
even more than then these that that is
still that is still the prototype we are
going to deliver at the end of the month
so just to show you in one minute what
the architects engineers to inside here
they enter on on a dashboard where they
have the rendering and they need this
this is also very simplified because we
need to complete it with information but
they need to upload files butta title
and select render quality low or high
they don't want to do to set up and
anything more and at the end they they
provide let me see
I am uploading the files the system need
hello and hi let's let's say hi in this
case and Lance this is going to upload
the files and it will start the job it
will take about 10 minutes 50 minutes to
reach a picture like that what yeah okay
this is the status center in an embossed
owned by by Gary by the way yeah we are
talking about pictures like that with
different quality because sometimes our
key to changing and it's just too for an
internal checks sometimes they need they
needed to show it to clients in fact be
behind that the let let call it business
model is is dealing with the different
infrastructure so if you are using this
for rendering tools and you have to show
to a client you will apply for a high
quality and you will be interested also
to to pay this service if we are using
for very short off for research purposes
you can you can use that there's
infrastructure for for free we are
talking about very small amounts of data
in this case but there will be a
community pretty loud to to use it and
also they will be able to pay a small
amount for the for this just for
rendering but what is more interested
interesting for us it's what is net is
happening next so the other the other
kind of civil engineering analysis that
we are going to implement inside this
general architecture this general
architecture is working for different
kind of civil engineering analysis and
so within the end of the project so in a
month we will be able to integrate
partially some of the other scenarios we
had the into the Vinci project so the
polytechnic university in valencia in
spain at a scenario dealing with the
structural computation called our key
travel is the software they are
they have already ported into the cloud
in the bath infrastructure so azure and
also compasses and it is providing
real-time upon 3d models simulation for
for earthquakes and for the static of
the building it will be the next
implementation and the the third
implementation is related to echo
efficiency analogy this is the most
complex activity we are following so far
E and it is producing it is produced in
collaboration with the royal danish
academy in copenhagen and we parted as
first in the world energy plus that is
the energy in gene that he used by the
most diffused energy efficiency analysis
software in the world it was produced
here in in berkeley by the department of
energy it was not ported into the clouds
so am I small group of four people
within the day with the approval of
Copenhagen that will test this kind of
infrastructure ported this energy plus
for the first time working in the cloud
so now we can we can make different kind
of simulation with this energy plus
especially they the first test will be
on different parameters for example the
different building type types that we
can have the different orientation that
the building can have on the land
compared to the sunlight the different
HVAC system means eating and ventilation
and I conditioning system we can add
different kinds or we can have a
different range in terms of kilowatt for
a square meter for eating cooling fans
and paps we can have more econ Valley
produce a lot of combination
that we are able to analyze and it can
give this system can give great robust
suggestion to our cadets before starting
the designing phase on how the building
has to be settling there in the terrain
which is the the plants that are most
suitable for for that specific situation
producing something like 1,000,000 as 1
million combination for a small villa so
it's very intensive in terms of
computing so this occasion the cloud
future is the the world inducement of
this kind of platform we are going to
deliver at the end of the venue project
show this suite of tools of application
working life for Phil digital simulation
render engine building energy simulation
tool from day first from the first day
after the project we intend this as a
commercial platform to that needs to be
implemented as I need scientists helping
us to construct this revolutionary
platform for building industry
everything is happening on a small
company that will that is co laboratorio
working as a private micro incubator for
these kind of ideas in architecture that
is now all moving into the green prefab
company that that has been founded in
trento that is in the north of Italy one
of the richest place in a place where we
have pilot cases connected with the
municipality standards and local pre
fabricators so municipalities are very
interested on this kind of platform also
the advisory board is growing in this in
this activity a let me mention professor
Vladimir Masonic here at the Lawrence
Berkeley National Laboratory because it
was one of the IFC
standard founder the interoperable
format we are using for the 3d model
also participating in the energy plus
foundation and those are also some that
I want to mention in this occasion some
establish relationships all around the
world especially bilder smart will be
the main the main client for for cloud
computing that is this international
audience for interoperability in this
format so basically my contribution and
I will end here is interact more with
the application because there are great
domains great area like the building
industry controlling the thirty five
percent of the global economy that is
that needs this kind of system they they
will need this kind of system only if we
are able to create an integrated
platform that is going through the life
cycle of the building the whole life
cycle and we open those kind of
platforms that are already existing and
we need that everybody every architect
every company every engineer can join
this such a such a system thank you
so I was wondering if you could imagine
having a set of sweet of sensors or
something that you put in your building
endorsed sensors and they automatically
uploaded their results to your cloud and
that somehow got put into this and you
told me that I'd left the window open or
it's got my refrigerator was too
expensive I mean it was not the right of
frigerator or something or other and
maybe get a video cameras reproduce mean
you know I just see combining this idea
where the senses as a service in the
cloud could be reasonably interesting
yes of course I was talking and today
about this this phase that is
engineering so when you design the
building what you're talking stays here
so when the building has been built this
kind of building will have building
automation systems inside that that
basically our sensors they provide a
huge amount of data that can monitor in
real time what we are doing with that
thing and also they can suggest a better
use of that building bail upon those
data in terms of energy saving for
example they can monitor if an an area
out of office building is used or not
and so you can modulate the the energy
saving for that or the lighting so in
the next future building automation will
produce a huge amount of data that
naturally will be will be work at the
within cloud environments no way that
will happen that will happen very very
soon but it will be much more stronger
if those sensor has been planted here in
this phase not after the building has
been built so the most important things
in this sector is to create the whole
system next application into the hobby
will be in the
winget Amish yes one more
so your you've moved energy plus to the
cloud I'm a little confused as to the
source of the parallel ism is it the
case that you're running lots of
different simulation you're doing
parameter studies as so these are in
effect different simulations with
different parameter values or you're
looking at Monte Carlo simulations with
weather or what's what's the nature of
the parallelism well I can I cannot go
in detail because this presentation
comes before the presentation the
Lawrence Berkeley National Laboratory
and so I will not go in in in detail
about that but parting was pretty easy
was has been done also in a first in in
the comps infra infrastructure is
dealing with an amount are very limited
resources within within the research
project so far and so we are dealing
with a about 100 course where we are we
are implementing those kind of
parametric all tests within within the
building I cannot provide you in this
case more details about how we we have
done into the cloud but I can provide
you privately alright any other final
questions if not thank our speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>