<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>After Math: Following Mathematics into the Digital | Coder Coacher - Coaching Coders</title><meta content="After Math: Following Mathematics into the Digital - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>After Math: Following Mathematics into the Digital</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EuHmh0lw5FI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
oh thanks so much to everyone for coming
I'm really excited to have Stephanie
here um she I've had the pleasure of
getting to know her this year she's a
colleague of my wife's at the Harvard
Society of fellows and she's in my
opinion the most interesting of her
colleagues at the Society of fellows uh
uh uh my wife's colleagues at the
Harvard Society of colleagues she's most
interesting of her colleagues at the
Harvard said you fellas um and uh I
think she's like the perfect talk to tie
together this lab because she's talking
about the history of mathematics and
computation in places like Microsoft
Research sort of in the middle of the
20th century and we're really excited to
hear about it Thanks I'm really excited
to have a chance to be here to have an
opportunity to share some of my work
with this very vibrant and eclectic
community I've also had a really
wonderful day guys a really generous
hosts and it was a pleasure to have a
chance to talk to some of you and I'm
local so I would love to have a chance
to talk with more of you in the future
if that turns out to be possible so what
I'm going to talk about today and what
my history is interested in general are
mathematical proofs which I imagine
everyone in this room is in some way
shape or form familiar proofs of course
are supposed to demonstrate that
something in mathematics is true that an
object has some property or that it
behaves in a certain way or that it has
a particular kind of relationship to
something else and one of the sort of
canonical features of proof is that they
are supposed to demonstrate that that
thing is true not just for one example
or another example but always and
everywhere for this class of things so
very simple example and the internal
angles of a triangle you can prove that
this will be 180
rees not for this triangle or that one
or all the triangles we've seen but for
all possible triangles everywhere at
least everywhere in Euclidean geometry
and this is the sort of canonical way of
thinking and talking about what proofs
are and what kinds of claims are made in
them but it turns out that different
things of course count as a proof for
different communities working in
different times and places different
standards of demonstration serve to
convince some communities that something
is the case and not others so there have
been long parts of mathematics history
where diagrams were perfectly admissible
tools in the construction of proof and
there have been other times and places
were there with deep skepticism and
distrust of diagrams different
communities disagree about what axioms
to use about what rules of inference
should be permitted and so in spite of
this sort of central claim to time and
place lyst nest that proofs have there's
a lot of work to be done unpacking what
in fact counts as a proof for who um and
much recent history of mathematics has
been aimed at unpacking different
communities and cultures of proving to
find out what counts as a proof for them
and how they go about producing them but
in spite of the many variations among
different communities of provers for
much of mathematics history they have
something quite fundamental in common
which is that proofs are something that
people make but what I'm interested in
is this moment in the mid-1950s where
different communities and practitioners
in the United States and elsewhere asked
the question whether or not computers
might become theorem provers as well as
you might expect computers created some
very new and powerful and exciting
possibilities for the work of
mathematical proof but they've also been
quite controversial at some computer
proofs with which I assume many of you
are familiar including the
computer-assisted proof of the
four-color conjecture from 1976 are so
long that no person could actually read
them all the way through others have
them make use of computational
operations that are quite
combinatorially complex and not easily
apprehended as correct or understood
or easily followed by a human reader so
there has been some discussion among
mathematicians and philosophers
mathematics about whether or not these
kinds of computer demonstrations should
count as proofs in the first place some
people are willing to admit that
computers might show us that something
is true but that they may not be able to
show why something is true because they
don't identify the same kinds of central
insights or extrapolate able ideas or
important moments that human
mathematicians are interested in so
these kinds of questions have been on
the minds of some mathematicians and
some philosophers of math maybe some in
this room since probably the 1970s and
somewhat earlier these are not my
questions this is not the question that
motivates my work although of course
it's related the question that has
motivated my first project is a prior
question to this one what I wanted to
know was how these so-called computer
proofs were made and made possible in
the first place I wanted to know what
actually went into the design and
implementation of theorem proving
software in these communities in the
second half of the 20th century about
which people would later have these
debates about what counts as a proof and
what does not so I wanted to look at the
sort of earlier development work that
precedes those debates as a way of sort
of changing and shaping and intervening
in that from a historical perspective
that's up to that end my first project
is an early history of a field called
automated theorem proving not
surprisingly it sometimes also called
automated reasoning and automated
deduction those two names more commonly
in the recent decades and practitioners
of this field of course we're interested
in making computers prove theorems or
assisting human users in doing so but to
me it's a very interesting field because
the people working within it really
disagreed about what the automation of
proof should look like some people
thought that computers might become
totally autonomous and to
created contributors to mathematical
research other people thought they'd
never be more than servants or slaves to
take on the very menial tasks of
mathematical work here are some snippets
just from some of the people I've
studied very closely using often very
provocative anthropomorphic language to
describe what role they believe
computers might be able to play in the
future of mathematics so colleagues and
mentors at one end servants and slaves
at the other end and something like the
LOI graduate students in the middle
could make contributions but only with
excessive guidance and with limited
importance um and would interest me of
course was unpacking these different
visions and finding out where they came
from but my real question was how did
these visions get built right into the
theorem proving software that these
practitioners developed how did they
translate their vision of what kinds of
things computers were into programs that
contributed to the work of proof in
different ways on the ground and in
particular today that have focused
primarily on these three and I'll stay a
little bit about why in a moment but the
logic theory machine that was developed
at the RAND Corporation in santa monica
california between nineteen 55 and 57
the program p that was developed first
at IBM research labs and later at bell
research labs in the late 1950s and
finally the automated reasoning
assistant or the aura that was developed
at the Argonne National Laboratory just
outside of Chicago with some pieces from
the mid-1960s and some pieces that were
being developed into the 80s I chose
these three programs because as we'll
see they represented exemplars of very
different approaches to the project of
automation the people who built them
envisioned quite a different role for
the computer in the work of proof I also
picked them because these programs and
the people and the institutions where
they were developed different
relationships to the landscape of
military industrial academic
collaboration in the post-war period but
primarily I picked them because the
people who develop them I
fundamentally discreet about just about
everything they disagreed about the
character of human mathematical
faculties like intuition like
understanding like reasoning and they
disagreed about how much a computer
might be made to possess those things
and they disagreed about what computer
generated our computer assisted proofs
at could or should look like so the
logic theory machine was developed by
primarily by Ali Nuland Herbert Simon
with whom I imagine most of you are
familiar and they of course subscribe to
a belief that in a really fundamental
way the human mind and the computer were
of a kind and they sought to simulate
human mathematical and fear improving
practices in their computer program on
the program P was developed by a
chinese-american logician by the name of
how Wong who fundamentally disagreed
either that humans and machines were
like the same kind of thing or that the
right way to use a computer would be to
simulate what we can already do he was
instead interested in what new
possibilities the computer might create
because of being quite different from a
human mathematician and the automated
reasoning assisted why do you say that
that's surpassing ah supplementing that
right not necessarily surpassing that's
right that's right I should specify that
surpass was a word he often used and I
think and I think as a kind of PR term
he was under no illusion that human
insight and intervention and design
could be extricated from this process
but he was particularly interested in
the sort of speed and power of
combinatorial manipulation that we can't
keep up with in that particular regard
the team that worked at the Argonne
National Laboratory under the direction
of a character Larry wass just simply
didn't believe that in either of these
cases computers would become autonomous
contributors to theorem proving and they
thought instead they're real power would
be in developing collaborative software
that could assist and guide human you
in different and new ways towards the
work of proof and so these were the 3d
programs that I focused on the most
moving forward with this project i'll
probably study others i know some about
others and i'm happy to speak to the
sort of other variations on these themes
in the field of people are interested
and what i'm going to do with the rest
of my time today it is tell you quite a
lot about the logic through machine a
very tiny bit about the program p and a
little bit more about the automated
reasoning assistant just to give you a
sense of the the way the programs were
designed and the kind of work in them
that interests me and that I think
matter for the history of how proof is
being done I often say things like there
is no automation without invention and I
think that each of these exercises
involve the production of new formal
tools and also new material tools and
new practices and perspectives that were
added to the corpus of what it means to
do the work of proof and so I'll try to
give you a sense of what some of those
transformations and inventions that I
think are interesting look like yes of
course I'll talk about that so the first
two were both specifically designed for
your improvers for the theorems from
principia mathematica a text which I'll
talk about in just a moment and and the
aura had slightly different at a
slightly different problem base but i'll
i will talk about those things and so as
I said the logic theory machine was
developed at the RAND Corporation which
was of course count I just on the heels
of the Second World War by the Air Force
it's famous for being a hotbed of
systems engineering and game theory um
but I don't think that's actually where
the story of the logical theory machine
starts I think that story starts more
around the turn of the 20th century with
these two English mathematicians
Bertrand Russell and alfred north
whitehead and of course as I'm sure many
of you are familiar wrestling Whitehead
were working with a community of
mathematicians at the turn of the
century to develop what we now think of
as sort of modern propositional and
predicate logic and they believed quite
famously and also quite famously wrongly
that
three theorem from every branch of
mathematics could ultimately be proved
within the formal logical system that
they were developing at this time and
they spent about a decade drafting what
became their three canonical works these
volumes of Gibeah Mathematica in which
they lay out this logical system and
then proceeded to prove hundreds of
theorems within it very tediously
getting to the incredible results that
in fact 1 plus 1 is equal to 2 on page
298 and this this is of course
well-documented part of the history of
mathematics but there is something about
Principia that is much less remarked
upon by my colleagues in history of
mathematics in the history of logic and
that is that they weren't just
interested in crafting a formalism
Whitehead and Russell wat were also
interested in crafting a set of material
tools that they thought would enable the
heads and hands of mathematicians to do
the work of mathematics and proof in
this way and that of course was a very
particular symbolic notation system a
way of writing and reading on the page
and they said this about it the symbolic
form of the work has been forced upon us
by necessity without its help we should
have been unable to perform the
requisite reasoning it has been
developed as the result of actual
practice and is not an excrescence
introduced for the mere purpose of
exposition no symbol has been introduced
except on the ground of its practical
utility for the immediate purposes of
our reasoning they actually spend an
extraordinary amount of time justifying
and describing the value of the symbol
system that they worked out I of course
at bottom the symbol system was meant to
make highly abstract things like logical
relation and inference easy to see they
wanted to make patterns visible they
wanted to make structural comparison
easy and they wanted to make proofs easy
to follow and to construct and to read
line by line on the page theirs was
built on a previous one developed by
pano and of course this kind of written
symbol system is extremely familiar to
us this is how we do mathematics and
logic for the most part but it's worth
for marking that
is a particular spatiality to it there
is a particular materiality to it these
are paper proofs in book form and
logical expressions are here represented
in a way that accommodates visions the
primary faculty for doing mathematical
work we're supposed to be able to take
these things in as a whole and cognize
them in ways that wouldn't be possible
in natural language or other systems so
the question that interested me was what
happened to the way proofs were being
constructed what happened to the way
that logical propositions and
relationships were being represented in
this transformation from text to the
ranjani a computer at beginning in the
nineteen nineteen fifty-five I wanted to
know how it was that logic was being
done and with what tools and processes
in this transformation and so i'll tell
you that story now and i can't do that
without explaining a little bit about
these two men of course Newland Simon
Newell left a ph.d program in
mathematics at Princeton in 1950 to take
up a position at Rand yes sorry before
you leave Brussels a boy dead yeah it
sounded like a minute ago you were
showing us that page from capilla and
claiming that this does correspond to
what comprehensible mathematics looks
like uh I feel people where's the main
thing that jumps off your slide at me
yes is that math papers never look this
way and the notion that an entire
population of eventually every
practicing mathematician would come to
write their work up in this notation
like in the hindsight of hundred years
later seems like pauly and how was it
perceived at the time that's a great
question and actually so having studied
logic during college and graduate school
I went to print capilla expecting to
open it and find you know what I had
been studying in logic class and of
course it looks totally incomprehensible
at the at the time it was so closely
related to first piano and then
systems that it would have been familiar
to this particular community of
mathematicians the one that we are now
more familiar was Hilbert and Ackerman's
symbol system which they design because
they thought this one was terrible and
I'm not exactly sure why they thought
this was terrible but I really like this
point because all through the pages of
Principia Russell and Whitehead are
constantly calling on the obvious
intuitive natural primitiveness of
everything including what is an obvious
inference rule what is an obvious
conclusion this is obviously a totally
tractable symbol system for people to
work within and in reading it now I
think this isn't obvious this is it
natural this doesn't strike me as
primitive at all and that I think does
speak to the particularly contingent
character of these kinds of tools and
representation systems that are tied to
communities of people I guess what i
want to emphasize is that here they have
in mind the sort of seeing reading
familiar with algebra kind of
mathematical agent um and and this will
have more in common which what people
use and the symbol cysts are then simple
systems then the representational system
designed to enable the johnny act to do
this work so there are sort of bigger
and smaller instances i guess but that's
a great point thank you that's that okay
um I'll be to a new a leftist ph.d
program at Princeton in 1950 he was
deeply dissatisfied with mathematics he
said he could not imagine how people
could spend their lives doing that kind
of work those problems were not his
problems he wanted to do practical
exciting hands-on work that wasn't quite
so abstract in esoteric he was very
honest and vocal about that Simon of
course was a professor at Carnegie
technical institutes in business admin
and economics and he started consulting
for the RAND Corporation in 1952 and for
those who are somewhat unfamiliar when
the air force created the RAND
Corporation and allotted it significant
resources the sort of general mandate
they were given was to imagine and model
what future possible global warfare
might look like and to design strategies
for winning or optimizing performance oh
and this is a very tall amazing order
especially given the uncertainty
surrounding what new technologies might
be developed or who the global aggressor
might be or what have you so they
developed a lot of really amazing tools
for imagining the landscape of war and
entering approaching a study one of
which are one of the largest of which is
sort of unified under the rubric of
what's called systems analysis so in
this approach to research they would
parse the landscape of warfare into
systems which could be anything from the
Bolshevik government to an individual
you know weapon system on an airplane to
to the railway system and they would try
to find ways of modeling and empirically
studying and ultimately optimizing the
performance of different kinds of
systems that could consist of people or
machines or just machines it was a
really beautiful in a way of approaching
a very large and sprawling problem and
crucially for our story New Orleans
Simon came to believe that the right way
to study human minds and human reasoning
and also modern digital computers were
as systems as well they're quite famous
for saying that the mind and the
administrative organization and the
computer our species of the same genus
and that genus was information processor
they took symbolic information as input
and they manipulated it according to a
set of rules and that was how they would
solve problems make decisions I'm
formulate judgments and the logic theory
machine this theorem proving program for
pre khufia
thematic ax from the turn of the 20th
century was in fact their first attempt
to build a computer program that they
thought captured something about
rule-bound human reasoning behavior so
it was a complex information processing
system and they said about to try to
identify what it is that human
mathematicians do when they prove
logical theorems to reduce that to a set
of rules and translate it into this
computer program for those who may not
know the logic theory machine was
actually the only running program that
was presented at the 1956 Dartmouth
conference around which the phrase
artificial intelligence was coined it
was very instrumental for the character
of early AI research and it began as
this exercise in finding a rule bound
similarity between humans and machines
so to actualize this project the first
thing they needed was a model of what
exactly it is that people do when they
prove theorems and for that newland
simon turned so this man the Hungarian
mathematician George polla so in
addition to his mathematical research
she was also deeply interested in the
problem of mathematics pedagogy Allie
Newell took every course he taught while
in undergrad Stanford which is where
that influence came from and polyya
believed contrary to some idealization
or mythologize ations of mathematics
that what mathematicians do is not just
deduce consequences from axioms the way
they're sometimes presented in logical
texts like Frank API you thinks they do
all kinds of things they start from
something they believe to be true and
work backwards the experiment with cases
they look for counter examples they do
all kinds of things besides deduction
and pulley a thought moreover that these
sort of tricks or shortcuts or or actual
mathematical practices were not esoteric
tacit secrets of the trade he thought
they could be identified articulated
formalized as rules and most importantly
not
under mattress to give them some hope of
becoming contributing mathematical
researchers he wrote this which I love
certainly let us learn proving also let
us learn guessing so let us learn more
than the deduction of consequences let
us learn to guess so he of course called
these ways of guessing heuristics from
the greek word for to find and is as far
as i can tell the person who sort of
reintroduced that word to our sort of
modern way of talking about mathematics
which is interesting so this was exactly
what Newell and Simon wanted right this
was a set of rules that were supposed to
model and capture human theorem proving
behavior and so the logic theory machine
was a first attempt to automate polio
like heuristic since they're improving
there were two heuristics in particular
they weren't overly ambitious the first
one is that the logic theory machine did
not begin with the axioms of Principia
and move forward deducing consequences
in search of theorems this of course
leads to an exponential explosion of
data and is practically impossible now
still and certainly in the 50s so
instead the logic theory machine begins
with a theorem from pre capilla that it
wants to prove and then it generates a
set of subproblems that if true lead to
the desired conclusion in a single
permissible inference that this is like
inferential reverse engineering and then
it produces a set of subproblems that
lead to those subproblems in a single
step and so on in hopes of course that
the axioms will be generated themselves
as sub problems in which case you can
run the path backwards and have a
deductive proof of your desired theorem
of course there's no guarantee that you
will find a proof of the theorem this
way but this did not trouble you all and
Simon who suggested that in fact human
mathematicians never work with that kind
of guarantee either
so this was the model and in most of the
literature that addresses the logic
through machine at all in academic
historian of computing this is the end
of the story this was the first
heuristic theorem prover this was the
model this is what the logic theory
machine is um it was famous because of
early artificial intelligence research
and that's all true but I wanted to know
how they actually got this model to run
on the Johnny a computer and this turned
out to be an extrude an early difficult
task they had to take logical
propositions and inference rules and sub
problem shading and somehow put them
into the machinery of the Johnny act
which it turns out neither Newell North
Simon really knew very much about in
order to actually implement this model
they had to enlist a third much less
famous collaborator by the name of John
preferred shop he worked in the
numerical analysis department at Rand
which was later renamed the programming
Department he came from an engineering
and mathematics undergraduate background
and took that position after fighting in
the Air Force in the Second World War he
and I love this he actually describes
the numerical analysis department at
Rand as the sweatshop of the company
machines themselves were housed in the
basement at that time programming may be
quite unlike today was a really physical
enterprise they had to carry heavy boxes
of punch cards from one machine to the
next the machines produced a lot of heat
and noise and they would often work
without their shirts it's this whole
other world in which the models and the
simulations for other departments and
divisions at Rand were actually made to
run on machines and it's quite something
to learn about them so it was primarily
with Shaw's health working with Newell
and less with Simon that the logic
through machine is translated into a
running program and there were a lot of
obstacles to overcome that the one that
seemed to have held their attention the
longest and been the trickiest for them
is how to represent logical propositions
and the archives are amazing at Shaw
Newell's archive collections show them
experimenting on paper with different
kinds of structures to see what it would
be like to represent a logical
proposition
this way in memory and ultimately after
about a year of that kind of
experimentation they choose the linked
list information structure as the
representational system for logical
propositions we of course now call this
a data structure but they used
information for a lot of really
complicated reasons I can talk about it
data structures are of course I'm sure
everyone here is familiar just what they
sound like their ways of organizing
information and computer memory such as
to make them sort of controllable
meaningful and coatings of the various
things we would like to put into
computer memory and in a linked list
representation each element of a logical
proposition was represented by an
alphabet of symbols that encoded
different parts of the information about
it like what kind of element it was was
it a variable or an operator was it
negated what position did hold in the
proposition and of course the last bit
of memory in in this contiguous chunk
labeled Q of their stored the numerical
address in memory of the next element in
the proposition which of course could be
stored anywhere else in Johnny acts
memory that there happened to be memory
available um so here of course this is
maybe obvious the elements of the
logical preposition are my design not
stored together not concatenated and
health as a single object that could be
taken in by the I or even taken in by
the computer all at once as a single
hole in this structure the only way to
treat a logical proposition as a whole
is to traverse the address pointers
logical propositions become dynamic and
temporal in a way that's slightly
different in different behaviors and
properties than when they're written on
the page I mean and of course they chose
this representation system to address a
very particular problem of memory
management that matters now but it also
mattered in 1955 and especially mattered
for machines like Johnny active head I
think about 52 kilobytes the equivalent
they're often of memory so just to
emphasize what they were excited about
imagine if you have like a guest list
for a party and you want to keep track
of the people that can't
it's much more efficient to just cross a
name off of the list than it is to
rewrite the entire list all together on
a new sheet of paper or find a new sheet
of paper if you're at Bennifer and
that's the kind of trick that the linked
list represents if you want to for
example delete an element from a logical
proposition in the process of producing
these subproblems chains rather than
rewriting the entire expression together
finding the memory of creating a memory
you would simply rear out the address
pointer from the previous element to the
following element and add a new address
pointer from a list of existing
available current memory to the thing
you just deleted so that made two
operations out of what would have been a
large number of housekeeping operations
and it was this kind of trick in 1955
that made the difference between a
program that would run on the computer
and a program that remains just a model
and yet this kind of insight this kind
of idea this kind of tool construction
is some really underemphasized and under
studied by people like me who are
interested in the relevance of
introduction of computing two different
kinds of work I also think that linked
lists offer a different set of insights
and show traces of a different kind of
perspective about what logical
propositions are the different way of
constructing them and although it's at
this low level of representation it also
does introduce new properties and
behaviors to them it represents a
different perspective about what they
are it also given how prolific linked
lists have become today offered some
ready-made structural analogies with
other things that people use linked
lists to represent and a lack of that
we're linked lists are not a good tool
just sort of offers a new set of tools
for thinking about what logical
propositions are that I think is very
interesting yes so logical propositions
have a kind of nested structure defined
by brackets yes so what status an open
bracket and close bracket happened yeah
this is a great
and when I talk about the logic through
machine I just don't talk about it at
all so after they have their heuristic
model of how to do this backwards
chaining by subproblem construction the
next step in the development of the
logic theory machine is actually to
develop a new set of tools for thinking
about both proofs as trees and also
logical propositions as trees so in fact
if you could if you use a tree to
represent a logical proposition you
don't need parenthesis at all because
the you know the order of logical
operations is just built into the tree
structure so the late list operations or
the linked list structure was actually a
representation of a tree structure which
they worked out in some length in
between and those that piece of
information about what level in the tree
any given operator variable held where a
coded in the sort of eight letter
variables that they used in the lane
list but this is also this is also a
sort of housekeeping problem because
trees are more efficient as a
representation because you don't need to
parse the parentheses which we do quite
easily with sites but which add a lot of
computational operations for the machine
so they yes this might be a very
pedestrian question but doubt it seems
like you're sort of one level below
implying that a lot of sort of modern
data structures sort of could have
arisen or certainly were invented some
extent in the process of trying to make
sense of principia mathematica and
embody it in the particular physical
structures that were relevant at that
time do you think that that's a good
representation or are you just not
you're letting that feet just below the
surface because you're not completely
sure that that's the case I'm not
completely sure that that's the case so
so some things are certain one thing is
certain is that when I learned about
lake lists which was way before I ever
learned about the logic theory Sheen
they just seemed like the most obvious
totally intuitive
to me um and that somebody would have
had to come up with it seemed very
bizarre to me and when I discovered that
not only did they have to come up with
it but that they tried a whole bunch of
other things first did leave me with the
impression that the linked list is tied
to this automation effort that it has
been totally stripped of its origins and
found useful and relevant in uncountable
instances it can be implemented in
almost every language I think you know
that they have Pro lifted or
proliferated independently of this
origin context I think does it make this
origin context uninteresting but what I
would really like to do maybe sometime
in the future is do a history of data
structures and see where the different
ones come from if they were developed
relative to a certain particular problem
and it seems like the knowledge about
that question is mostly anecdotal where
it exists at all and and I do think that
it's not necessarily the case that
linked lists in the form they have would
have developed exactly the same in other
contexts I think there is something
particular about this moment but it's
really hard to make claims like that
well be a perfect place they came from
was Lisp right which also came out of
logic yeah absolutely so so um not only
did they create this late list data
structure they developed this language
the information processing language or
IPL they called it which is but by many
of lisp developers own admission the
predecessor they were sort of adopting
this processing tools from the IPL in
their development of Lisp um that's
something else I didn't mention is that
all of the inference rules in those
heuristics for working backwards were
translated into something like forty
four variations of eight list processing
operations which are quite different
processes for theorem proving than the
ones that we execute on the page I guess
but but I think list was sort of
following off on these on these
developments in a very real tangible way
but you probably know more about that
than I do Daisy about there
for my time I'm so funny you go and you
know you're being notebooks in the arc
AB his Island you try to reconstruct
these stories based on archival
materials and it only takes one person
who is there to say I really don't think
that that's what happened to make you
doubt your whole you know archival
experience but um so just to summarize
the kind of thing I think is very
interesting about the logic theory
machine as an example of doing this kind
of historical work on every element of
what proof is here gets reimagined and
retooled in some way so they asked this
question what is human reasoning they
read polla and they see his heuristics
Pelleas heuristics are actually really
quite sophisticated and from that they
take you know working backwards by
subproblem chaining and hold it up as a
good model of what human theorem proving
behavior is they think that's really
remarkable and of course a lot of early
AI in the 50s and 60s sought to do the
same in other disciplines what are the
heuristics of medical diagnosis what are
their heuristics of a psychological
questioning their sort of this move to
identify these heuristics following
along this platform and and that's
obviously not just a simple
representation in there they're sort of
making a claim about what human
faculties are that's new and then this
transformation from from a
representational system that
accommodates people and vision and
reading and paper to a representation
that accommodates Johnny ack is I think
significant there's a lot of recent work
in the history of mathematics about the
roles that the letter diagram played in
the emergence of deductive reasoning in
the Classical period people are also
interested in algebraic symbol systems
or the difference between my tits Ian
and Newtonian calculus representational
systems I would like to post a question
to the people who write about those
things what do we learn and know
differently when we develop these
computational we oriented representation
systems and I think that the the answer
to that question has to do with this
sort of dynamism and temporality and
sort of algorithmic quality that gets
introduced to mathematical objects in
those kinds of transformations um ok so
now I'm only going to say very little
bit about this wonderful character how
long who developed the program p i'm
happy to say a lot more about him if
anybody is interested he was one of the
most vocal critics of the logic theory
machine quite ungenerously it publicly
um and at bottom he think they made two
errors the first one was a yes for you
suspicious topics yeah and ignorant
about how successful that agenda was oh
right I also forget I always forget to
talk about this so ultimately the logic
theory she approved 38 of the first 52
sort of canonical theorems in in the
first volume of her cavia and one of
those proofs was all of those proofs but
one looked very much like the ones that
Russell and Whitehead publish in
principia which Newell and Simon took to
be great evidence of the fact that there
was a deep similarity between the
processes at stake in both of them one
of those proofs was a previously as
supposedly previously unknown one and
Bertrand Russell was still alive at this
time and he and Simon had this
delightful sort of six letter
correspondence in which he sends in the
proof did you know this proof and didn't
include it for some reason and Russell
said no that's more elegant than the
proof that we have produced Newell and
Simon also really celebrated this castle
also said with a great tragedy that we
wasted ten you live I'm working out all
the proofs for these theorems that are
now you know going to be proved by
computers are probably very short time
and so it was considered a great success
of course Newell and Simon are brilliant
and very aware of the limitations of
this
thought this is just a very small step
towards a comprehensive model of human
practice it's also a tiny step relative
to results but they thought this was
really the right path to move forward
with artificial intelligence research
and a lot of people working in the field
in the early decades sort of followed
after their their method dad's your
question yeah yes okay they gave it up
after this right what he did other
things yeah they did they definitely
gave up fear improving so the next the
next engine they built that was supposed
to simulate human behavior with a chess
machine and then after that they started
work on the gps the the general problem
solver which was their most ambitious
project supposed to enable the computer
to heuristic lee solve problems in all
kinds of domains which was never really
very successful but they they were
always so optimistic that this was the
right way so but this was their only
there only engagement with with
automated theorem proving all right so
one right long thought they got two
things really wrong he thought one of
the things they really got wrong who's
the idea that somehow mathematical
research is ruled out in some way that
all of the human mathematical faculties
of intuition and exploration are
rule-bound phenomena he wrote logicians
had worked with the fiction of man as a
persistent and unimaginative beast who
can only follow rules and then the
fiction foundation machine quite a few
clues newland salmon of wrongly
accepting the fiction and then wrongly
recognizing the computer as a
representation of man when all this time
it had been a representation of nothing
other than a fiction and the other thing
that wants thought they about rom was
then even if they were right even if it
was the case that machines would be able
to do the things that people do this was
the wrong way to use them anyway because
we can already do all of his things and
it would be more advantageous to orient
computer research towards what people
can't do so he wrote that the human
inability to command
cisely any great mass of details sets an
intrinsic limitation on the kind of
thing that is done in mathematics and
the manner in which it is done the
superiority of machines in this respect
indicates that machines while following
the broad outline of paths drawn up by
men might yield surprising results by
making many new turns which man is not
accustomed to taking we are in fact
faced with a challenge to devise methods
of buying originality with plotting now
that we are in possession of slaves
which are such persistent laughter's one
went on to develop a program called the
program P um where the logic fear
machine proved 32 theorems in something
like 10 minutes of computing time the
program Pete proved every serum in all
of bring capilla in under three minutes
it's a really powerful engine for its
time and it is based on a completely
different set of insights and ideas and
approaches to automation than the logic
theory machine in particular how long
wanted to use the computer to take some
of the abstract results of mathematical
logic and of proof theory and make them
into actionable tools for doing work in
mathematics or change their role and
status while say just a very little bit
about this but the program P was based
on her brands theorem which is one of
the fundamental results of proof theory
in it states that given any statement P
in the predicate calculus you can
construct an infinite series of
statements as well as two and so on in
the propositional calculus which is a
simpler branch of logic and the theorem
gives us that p is a theorem if and only
if for some number n the first end
statements in that series connected by
the or operator in a disjunctive Series
creates a tautology so either s1 or s1
or s2 or s1 or s2 x3 and so on is a
tautology meaning it's true under the
assignment of anything to its variables
this theorem was really important for
proof theory but what it was was a
reflex
chinon the relationship between these
two branches of logic it's a really
surprising result because the
propositional calculus is decidable and
the predicate calculus is not but of
course you can't get something for
nothing so this infinite series might be
an infinite series and there are some
cases where you can calculate an upper
bound for n but in lots of cases there's
no way to know if you'll ever find a
tautology or if a tautology is
impossible so this was a certain kind of
theorem it was a reflection about the
relationship of two branches of logic
not a toolkit for actually proving
statements in the predicate calculus but
Wang saw in the computer an opportunity
to build on her Brad's theorem to make a
set of tools for actually proving
theorems in the predicate calculus and
it turned out to be more efficient and
it's very powerful it actually turns out
that well there's a lot to say about
this but yes this this program at using
these insights from her brands theorem
successfully proved all of the theorems
from pretty capilla and was generally in
this community thought to be a more
interesting and more important program
even though it made no claim to reflect
what it is that people do so the insight
behind the program is that while you
can't have a computer check an infinite
series for you know disjunctive
tautologies and the prefixes either and
her long wondered whether or not there
was some structure in her friends serum
that you could exploit to turn this
infinite problem into a finite problem
and check whether or not the
construction of a tautology would be
impossible instead of am i right to
think that this is like the conceptual
origins of what ended up being the most
successful actual new theorems that were
approved in the sense that it was a
bunch of case checking and that's sort
of what most of the actually new
theorems
proves through automated you're
improving work yes so the answer is yes
but not this one the thing that has been
the most powerful is the resolution
principle which if I have time I'll see
a very little bit about and but Wong's
idea that this was the right way to put
computers to work doing a large number
of case checking I love his archives are
full of these empty tables because even
after you turn this into a finite
problem even for very simple cases you
can you can have to check three trillion
possible patterns for for forcing a
tautology so just to say very quickly
the very basic idea is that even though
the series are infinite and you can
reduce to a finite set of possible
structures that might make a proposition
false so this example is an implication
function and the only ways that an
implication function can be false is if
the left hand is true and the right hand
is false so even though these G's which
are just predicate functions which take
two variables and gives you back true or
false depending on whether or not those
two variables have some relationship
like that they're both on or the one is
the square of the other or whatever and
the predicate calculus is set up so that
you don't need to know what the g's are
in order to ask this kind of question
and so even though they're an infinite
number of substitutions for x y and z in
this example overall it can only be
false if the left hand is true and the
right hand is false because of the
definition of the implication function
so the first the computers first task is
to build a truth table of ways in which
a proposition could be made to fail to
be true and then to go through and check
if forcing one to fail to be true forces
another one to be true because of the
variable assignments that you've input
and if you can show that every possible
pattern for making the expression false
forces another one to be true then you
can show there will always be a
disjunctive tautology so that's the
basic idea of this proof which is a lot
less efficient than drizzle
yes which is a lot less efficient than
resolution yeah it was still far more
powerful than the logic theory Sheen and
and this this wrong thought was sort of
the right way the right way to do
automated proof he's a very interesting
character he had a lot of complicated
relationships with Marxism he was under
surveillance by the FBI for much of his
career let's say about him in his work
he actually won the first milestone
award from the american mathematical
society and automated they're improving
for this for this program but i have
very little time as well say a little
bit about the aura now to finish which
of course as I said before they thought
both of these approaches would
fundamentally and and it refute ibly be
BBM restricted in their possibilities
because computers just wouldn't be able
to be autonomous contributors to
mathematical knowledge they don't know
what mathematicians care about they can
only identify things that are
interesting that we tell them are
interesting mathematics is not just
about the the churning out of truth it's
about the identification of sort of
interesting and productive insights and
and the people working at argonne
thought the only way to include
computers in that process was to make
them collaborative with human users on
this project was undertaken in the
Applied Mathematics division which was a
lot like the numerical analysis
department at R and they got quite a lot
of freedom and decided it would be
really important as a general project to
study the limitations of automation
rather than just use the computer as a
handmaiden or a tool or service
instrument other divisions at the lab
and the person who headed up until very
recently their automated theorem proving
department with larry loss and he's also
very interesting character he was born
completely blind and developed a complex
braille system systems use me throughout
his life to do this higher mathematics
um and it was his experience as a
maximum
titian of moving forward with
mathematical research based on intuitive
leaps in the shower or or intuitive
instincts or ideas well on a walk he
just he was drawn from a lot of folklore
about what mathematical discoveries like
I think but he was very committed that
computers could never be made to do
these things on their own and he wrote
that proving theorems in mathematics and
logic is too complex a task for total
automation for it requires insight deep
thought and much knowledge and
experience and so the approach to
research at Argonne was to develop a
collaborative theorem prover as I said
that would be structured like this the
human user would be able to impart her
intuitions or insights about a
mathematical problem and the theorem
prover would then run with those
insights using these sort of powerful
combinatorial tools like the one that
one developed further and faster in two
different places than that human
collaborator might be able to go on
their own so there were sort of two
parts to this they took the work of
proof and partitioned it into two kinds
of work there's the intuitive work and
the inference work and that in itself I
think is really interesting is a sort of
division of labor implicit in this in
this automation project but i'll talk a
very little bit first about the computer
contribution to this side which is of
course the resolution principle and
which was introduced by this man Joan
Allen Robinson he started spending his
summers at Argonne in the early 1960s he
was actually a classicist who came over
from the u.s. from Oxford to study
philosophy at Baden yeah he's also just
some of these people are quite
remarkable and he came to believe as did
many people that computers and minds are
just quantitatively but also
qualitatively different and in his work
as a classicist he became convinced that
the entire history of logic all the way
back to the classics with dependent on
the sort of limitations of human
psychology everything people committed
to
as an inference principle as a basic
premise was was structured around what
human minds could do and he thought
computers offered us an opportunity to
rethink what logic is to rethink what
reasoning is to sort of move forward in
quite a different way I think it's worth
I'm quoting him at some length because
he's really interesting so he writes
traditionally a single step in deduction
has been required for pragmatic and
psychological reasons to be simple
enough broadly speaking to be
apprehended as correct by a human being
in a single intellectual act and this is
sort of going back to what Whitehead and
Russell took so for granted about the
naturalness of human psychology when the
AJ carrying out the application of an
inference principle is a modern machine
the traditional limitation on the
complexity of inference principles is no
longer very appropriate more powerful
principles involving perhaps a much
greater amount of combinatorial
information processing for single
application become a possibility so he
thought there's this way that logic has
been dependent on human psychology for
its entire history but in fact there's
nothing intrinsic about logic that makes
it so we have made it so and here with
the computer is an opportunity to
imagine other logics and other rules and
in 1965 he introduced the resolution
principle which is among the most
powerful of those rules and it's also a
bit involved but I'll describe it to set
a very general level and that means
arguing example I'm resolution allows
you to take two parent clauses and
resolve them into a child's clause the
parent clauses have to have a particular
property namely that one contains a
complimentary little literal of another
and so that means that one contains
something that is the negation of one
that appears in the other parent clause
although of course all these variables
here can themselves be replaced with
propositions in a nested way so this
makes it look simpler than it is and in
this kind of structure resolution
generally speaking can resolve to a
child closet has no instance of that
literal which was negated in the parent
clauses
that's simple enough to say um
resolution had built into it mechanisms
where the computer would check whether a
given pair of clauses could be
transformed so as to have this property
also it could do it for more than two
parent classes it was really a powerful
rule most of the work done an automated
theorem proving these days are based on
the resolution principle variations
thereof it's become a really powerful
tool which is quite remarkable in 2012 I
heard Robinson speak at the conference
the joint conference for automated
reasoning which is a parent organization
to the conference for automated
deduction which is sort of American
community and he said at that conference
he actually thinks that this moment in
1965 with the resolution principle might
have been a big mistake um because it
sort of took proof away from people at
black box did resolution proofs are
deeply involved in hard to follow it was
precisely this idea that mathematicians
didn't turn out to be so interested in
this because it didn't give them
anything to use it just showed them that
something was true and and it was very
remarkable to hear Robinson speak about
this moment as a sort of here is maybe
where automated for improving and
mathematics parted ways in a way that
trouble mmm and it's very interesting
yes seem like if you go back to the
quotation yes that's a logician writing
not a mathematician yes that's right
that's true and he is yes never he
actually makes no claim to being a
logician either he's still but when a
mathematician talks about a step that is
simple enough to be apprehended as
correct in a single intellectual act
typically you can it's a huge amount of
work to persuade the computer but that's
true oh of course yeah that's just an
enormous gap between this story and
anything the mathematicians actually do
yes exactly exactly that's right that's
right so on one end there's the sort of
myth from from logic that we are
persistent rule-following you know
applying inference rules and then on the
other end there's what actual
mathematicians do and then in the middle
these automation attempts that are
pulling from and making claims about
both ends of that in really interesting
ways I think that's I think that's right
um yeah I think that's right I think I
think Robinson would agree as well that
this is not a description of
mathematical practice but that it it's a
logical plan but but resolution
resolution provers today are often used
to sort of process really large
databases sometimes resolution provers
collaborate with each other instead of
with a person and do really astonishing
things and they don't produce the kinds
of insights or ideas that mathematicians
can take with them to the debt you know
to the paper and pencil practice that
they are engaged in although think
things are changing now it's not just I
pay for a principal practice it's the
inspiration for guessing yeah you know
that you want that that a non wrestle a
more conventional proof proof from the
book is one which gives you an insight
which allows you to guess another
theorem that's r and the resolution
proof does not in any way enhance your
ability to guess that's right that's
exactly impaired to something like
Mathematica which is an engine for
simulation which is extremely useful to
many of us well yes so experimental
mathematics helps you and things like
that that'll get help you to get or
seeing you know a proof from the book a
real proof like that sorry that's the
real reason gives you inspiration which
allows you to guess and a resolution
proof gives you an answer and no insight
on how to guess what the next great day
my god i turn to serve all did say I
have an intuition that gasps and there's
one step where I have seen tuitions that
this fall off from that and have no idea
how to prove that step no one resolution
but the but the but the point is for
something totally new when I take the
insight and I say might that weird thing
be true anyway we shouldn't we should
know
everything like one minute to wrap up
fresh again this page and said yes sure
who I was going to juxtapose resolution
to the syllogism of course this is a
sort of natural inference this is the
kind of inference resolution allows you
to do I my pen I can work this out but
it's not immediately obvious and so
resolution asks you to admit this kind
of step to your work of course the other
half of this collaboration was what
exactly is it that the humans contribute
to this collaboration and we know that
supposed to be intuitions but I like to
finish with this because this was in
fact the first thing I discovered in
doing this research where I thought well
that's really strange so the way that
human users input their intuition to the
computer of course in order to do that
intuition has to be translated into
computer input in some way and so of
course they devise something called the
waiting mechanism in which the user
could input waving templates to tell the
computer that certain kinds of
information were more important than
other kinds for certain kinds of
problems so could say that the the sum
of two songs is more important than the
product of two products or that shorter
clauses are preferable to larger clauses
and and the aura would preferentially
produce clauses whose ancestral clauses
had the highest concentration of this
quantify weighted information given as
these templates by the user so I thought
when I read into that loss was so
committed to this idea of intuition as
something uniquely human and totally an
automated all this Eureka moment in the
shower but in actually building this
interface into aura intuition becomes
this waiting mechanism they couldn't
help but translate it into the technical
vocabulary of the machine they were
working with and I think this is a
really interesting and surprising kind
of transformation and that took place
there either crush yes yeah i'll grow i
know i can ask her afterwards or off
with
so let's get these things in LJ at the
very end that so far the history of
software as I said to some of you today
that i had the pleasure of talking with
i'm at least in sort of academic his
history of computing circles has really
been treated at a very high level of
generalization implementations memory
management problems algorithm
development more work has been done on
the construction of human computer
interfaces but even that is often
relegated to the screen and not under
the hood and so i throw out my work i've
tried to emphasize that implementation
is a really significant custom illogical
factor for the history of computing and
its relevance in academic disciplines
like mathematics and that more people
should pay attention to those things
when they write the history of software
so i'm trying to do that as my
collaborator dan in the back we're gonna
write some history of software skills in
the last 1.5 minutes will you just say
what you're thinking about doing oh sure
dan meant to have your permission to
speak on our both our behalves it so dan
and i are a writing are working on in
the very early stages of a history of
windows and we're very interested in
windows because of its replacing the
very complicated economy of hardware and
software we're interested in it because
i mean it is a site where multiple
pressures and constraints that you can't
see when you study the history of one
program are brought to bear a very
complicated problem we're interested in
a sort of dance of backwards
compatibility and future proofing and it
won't be a history of microsoft because
we think windows involves a lot of
stakeholders third-party developers
hardware developers windows a sort of
unique and exceptional in being a node
where that kind of large-scale software
development pressures are in the sort of
crucible we want to bring this interest
in implementation and design and to bear
on a history that tracks those things so
maybe you'll be hearing more from us
things hopefully soon but thank you very
much to type something I asked my heart
yeah yeah so your last paragraph
reminded me a little bit how modern no
one first how modern chess programs work
so because they sort of into deciding
which moves to look at also use waiting
big wheels you know where's our survey I
would be surprised there was awesome
yeah I know so we're all sober because
sort of developed and worked out the
details for older lady rich originates
in transportation and I know it was
quite widely circulated a mother feeding
America generators at this time and I
don't know how much it made everything
yes we've kind of degenerated into a
half questions what I would just thanks
to me again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>