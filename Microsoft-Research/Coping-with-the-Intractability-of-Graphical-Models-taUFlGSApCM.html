<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Coping with the Intractability of Graphical Models | Coder Coacher - Coaching Coders</title><meta content="Coping with the Intractability of Graphical Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Coping with the Intractability of Graphical Models</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/taUFlGSApCM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello everybody welcome I'm extremely
pleased to welcome a rookie on one leg
of a multi like world so welcome to
Cambridge Justin received his
underground in Washington University in
st. Louis and then like all good people
I went and got a job as a research
assistant before starting his ph.d as in
anesthesiology department they did a
master's in p.a tomorrow was an
assistant professor at Rochester for two
years now I guess has been in Australia
at nick de so I'd like to I very much
enjoyed Justin's work and blog over the
years and I'd like to say rising mist
someone who produces papers which deal
with you know mathematically complex
like elegant ways but always with a
focus on actually solving some real
problems so I think it's a fantastic
approach I'm really looking forward to
hearing what he has to say welcome ok
thank you um so yes I understand the
culture here is to sort of interrupt and
ask questions and I really pleased it
any so this is a talk about graphic
assume with this audience everyone is
pretty familiar but if you're not a
graphical model is given a graph that
that encodes conditional independence
ease between a set of variables if you
have it's a graphical model is
essentially just a distribution written
as a product over in the graph so I'll
repeatedly use this notation of a
product over the clicks sometimes this
more exponential family kind of notation
so i will put of eight them here but
suffice it to say graphical models they
have lots of applications in vision NLP
folding etc so the problem that you
immediately run into if if you want to
use graphical models in most
applications is that if you some of the
most basic problems you'd like to solve
such as computing a marginal probability
or computing the most probability most
probable joint configuration or maximum
likelihood learning all these problems
in two very difficult complexity classes
Sharpie is sort of a worse than NP
complete you can even show that that
computing marginal inference to a given
guaranteed option factor is np-hard and
so this this this issue have this you
know this great a modeling family that
fits so naturally problems this is this
tension with the fact that computational
difficulties arise so easily is
something that's that's fascinated me
for four years now so I'm going to talk
today about two different sort of ideas
for cope with this this computational
difficulty the first is sort of maybe a
more practical technique and the second
is more theoretically be out there so um
I'll start by talking running with an
inference so just to define a little
patient um if you take if you have a
graph and the clicks are just the pair's
imagine that we take all of the
univariate marginal xand pairwise
marginals and concatenate them into a
long vector mu of theta so so please
please remember this notation of theta
this would be the output of doing
marginal inference diameters theta I'm
also use the notation the conditional
distribution over Y I'll write mu of
theta comma X to denote the the
conditioned margin once you've
conditioned on why so despite the fact
that it's Sharpie hard there are a wide
range of algorithms for approximate
inference of marginals and these these
off extremely well so I think it would
be it would be overly pessimistic to say
well you know computing Mars is sharp be
hard therefore I refuse to consider
patient that involves computing
marginals because just because the worst
case examples are sharp be hard doesn't
mean the instances that we actually see
in practice or that hard so these
algorithms um they often work really
well again we know that they can't be
accurate in general because it's it's
it's NP hard to compute limit marginals
but
it would be silly to ignore them so it's
a practical perspective we would like to
do learning somehow using these the
approximate inference algorithms so the
basic idea of what I call truncated
learning is first of all let's redefine
miu of theta x define that now to be the
output of some appropriate procedure so
the notation looks very harmless but but
what we mean here I murders theta and
some input vector X run your euro
products in 13 whatever generalized
belief propagation or whatever you like
output some set of marginals mu so let's
define an empirical risk well we just
sum over every element in the data set
and we have some loss function which
compares the true output Y to the
approximate marginals knew that you've
you've produced on that input so we just
sort of it just sort of measures how
well this particular output matched to
these approximate marginals EE hat
notation to denote an expectation with
respect to a data set so imagine that we
would like to fit a function like this
so we'd have a lot of advantages firstly
you can use an application dependent
loss function so depending on what you
do that the in which the matches the
marginals that that distance can can
really depend on the application
secondly this this will compensate for
defects in your approximate inference
method so arguably it does no good to
have wonderful parameters if those
parameters yield terrible marginals
under your particular approximate method
so that so this tries to fit the
approximate method to be to be accurate
rather than exact marginals this can
compensate for model error this will
degrade reasonably even if the does not
lie in the class that we're modeling it
with so really is to say this is just
this is just empirical risk minimization
this is essentially the most basic you
know principle and machine learning that
we're at test time we're going to use
this procedure and we have some some
what we care about a test time so let's
learn to exact
the parameters for for that measure so
how would you optimize a loss like this
um so what I think is probably the most
practical method is is not its again but
but it works and the idea is well if we
fix the number of message narrations you
can think of the loss as a function of
approximate marginal here at MU of theta
necks if the number of message passing
iterations is fixed this is really just
a big nonlinear function if you if you
if the messages computations are
happening in a fixed order it's just a
series of application of simple
functions and so at the end of the day
this is just a big big nonlinear
function and so in principle at least
through application of the chain rule
you can calculate the derivative of the
loss with respect to theta and so you
can admit are up to a local minimum so
so ideally we would live in a world in
which you can calculate these you can do
this this chain rule can be dramatically
that would be automatic differentiation
or or what's called back to apply these
sorts of ideas to neural networks at the
moment these are probably not quite good
enough speed as if you do it by hand so
you can derive out comes this is for a
tree related belief propagation these
are for me I'm very complicated but at
the it's it's just sort of back prop an
approximate inference procedure although
you do get some savings and Alexa T of
memory so as a simple exam on this the
the Stanford backgrounds database where
your and you'd like to compute a
segmentation of it into these semantic
classes such as sm building and so on so
here i'm using a set of very simple
features just um color position get
position Graham if gradients and some
edges of a four connected graph and as
an empirical risk i use the mean
marginal probability of each of the
conserved pairs so you want all of the
pairwise mark to be accurate essentially
so these are what the results look like
I'm here I compare truncated learning to
independent learning this would be just
a or essentially pixel by pixel logistic
regression so you can see it reduces the
the training and test errors from
twenty-nine percent to maybe nineteen
eighty-two percent test area um yes
sorry this is this is pixel i jist i'm
comparing to some recent papers i chose
these they happen to provide the results
of an independent classifier as well you
can see twenty two percent it's okay of
course if you really want to do well in
terms of the experiments the the action
is in getting really good features um
but but you could argue that the actual
improvement of the structured classifier
over the independent classifier as far
as far as i know the the biggest um but
step back if you look at these training
and test errors it's interesting to
think you know what do we have here do
we have a bias problem or a variance
problem is the problem that are are the
the predictor that we're fitting is not
powerful enough real funny or is the
problem that it's too powerful and so we
over fit yeah opinions so so my feeling
is that we predominantly have a bias
problem that is that the classifier is
not powerful enough because even even
error here is nineteen percent probably
too high so i think one thing we've
really learned conclusively in the last
few years is that architecture do you
have the ability to move along the
bias-variance trade-off so neural
network type architectures or ensembles
of trees and things like this that is
absolutely fundamental to get I you know
the optimal performance so if you're if
limits you to using just you know linear
features that are hand engineer pixel
graph the CF it might be doing more harm
than good so we need to be able to
make these sorts of bias variance trade
off so I'm a big fan of creating tools
and I and I felt that this this
truncated Lydia is something that it
could really be export and be used by
people who aren't you know CRF learning
experts I published a toolbox in 2011 of
it has a bunch of limitations in
particular the your limited licensing
issues in the amount of parallelism you
can exploit and these these types of
models it's it's important to kill a lot
of electrons if you want to get ideal
results and so you really want to use
high level of learning architectures so
I've been working more on a different
tool box to sort of get around these
limitations with with the idea of
creating something that would be usable
by someone who may not be that
interested in CRFs it is interesting to
design decisions here for example as a
learning I use limited memory bfgs with
a distributed gradient computation
there's lots of much fancier learning
algorithms recently but this still is
the one that seems yeah and the most
it's going to work when someone's using
it on a Havenstein um this is the the
code um I haven't really announced this
yet it's still an alpha but you're
you're welcome to to help me find both
like and I've seen linear speed ups up
to about 300 cores so this this is a as
far as I know it's the the largest
scales for the box for learning these
types of things I'd like to show a
couple of things that you can do with
this and I like to emphasize i reclaim
that these dibbles i'm about to show you
our smart models at all I'm just sort of
trying to demonstrate a flexible
framework so think of something called
discrim
to have restricted Boltzmann machine you
have a set of them a set of hidden units
that depend on the input they interact
with each other and then they interact
with the output node and you'd like to
maximize the the marginal probability of
the correct output given the input
marginalizing out these these hidden
nodes so these are typically learned
stochastic gradient descent on a
divergence type of loss but I thought it
would be interesting to see how it adorn
it learning on a data set like this so
on the m this database you get a test
error of 3% sort of okay in terms of
test errors when you haven't messed
around with the input features at all
but it's still interesting to see if you
visualize the connections from your the
input units to the to the four digits
you get these interesting looking
features I don't good interpretation of
this I mean if you look at look at the
connection connection
different output digits it looks like so
and here I'm showing on some randomly
test images the the active is it the
activation hidden the final marginal
probabilities you can see there's a
couple here that's this is
I think this mistaken why does it may be
forgivable so another type of model that
that's interesting to look at is I'm
sure this has been invented before but I
can't find a citation for it but the
idea is well typically in a CRF on a
grid rather than a line typically the
output nodes interact with each other
but what you could do if you'd like to
make the increase the the
representational capability have a set
of hidden you hidden no time to eat to
each output as you the number of nodes
there you can going to the the
representational capab model and make
it's trade-offs so I run this on the
horses database um and to make things
difficult I use no edge features what's
so and as we vary the number of hidden
units corresponding to each output it
the test arrow drops from 10 12 and a
half percent to nine percent and nine
percent is about what I achieved with
the best hand engineered features and a
tradition rev so you can visualize these
results here that the the 0 layer model
so independent by pixel logistic
regression so as we add hidden units
these are not necessarily going to match
the output is that this layer look
better you can see it sort of happens as
the model is more powerful you do indeed
get better looking marginals for these
output images although it's sort of it's
sort of a mystery to me what these what
these hidden units are doing or why
exactly helpful so um I think that still
really a lot of different cor CRF
architectures that can be explored you
can do things like explore multi-level
models you could have you known on local
connections non-local means types of
connections between variables and
there's sometimes a perception that you
know for every new model you need a new
bespoke learning algorithm and I think
it's true I think we have tools now that
so you can sort of concentrate on model
well rather than trying to just your
learning algorithm to work all right so
now I'm going to talk to talk about a
different issue heightening the
piecewise loss so this is um working the
structured learning framework which is
again with it with a graphical model
nope necessarily thinking
probabilistically so we have some energy
for of an input X in an output Y which
is the sum of functions evaluated on the
clicks graph so the input output the
cliq see for example on a grid you might
have one click for each univariate and
one for each pair so informally when
we're learning we would like to do is
given some data set we would like to
function f such that when you maximize
it on each input xk output which is
closed output YK somehow so there's
there's a very simple method for for
learning these types of models called
piecewise learning and what piecewise
learning does is essentially is for each
click see you train it completely
disconnected from the others you use no
inference and you try to make it such
that when you maximize that output YC
for the individual click you get
something close to the observed output
um so from a from a theoretical thing
perspective this a quite a crude
approximation to the real loss that
you'd like to learn but even so there's
there's a large numbers of examples of
people getting extremely strong results
using piecewise learning CRFs you might
even argue that most of the strongest
results on sierra use either the
piecewise loss or the pseudo likelihood
which is similar in some sense so so so
why is this happening why it is the
piecewise loss such strong results and I
think there's there's really there's no
magic to it the reality it's just that
piecewise it's it's so efficient and you
have a flexibility that when you choose
these factors FC the the the you you
have a lot of them in engineering good
features and in fitting a powerful local
classifier to and and the reality is
that you know doing a good job of
engineering features and engineering the
classifier for your problem that's
probably more than the particular
learning algorithm that you use so it so
if your see our algorithm sort of gets
in the way of stopping you from fitting
you're more powerful model it might more
harmony so so I'll come back to this but
but but to return to structured learning
when you're learning this this energy
function f typically you use a loss
which is a sum over all the data ignore
Delta for the moment and what this is is
it's the difference between the energy
evaluated on the correct output hey and
the maximum energy over
any output so you can see if you've done
a good job and maximized by YK then
these two terms are going to be equal
and you're going to have no loss on the
other hand if some wrong output beats YK
then your loss is going to be
proportional to how much it the energy
of the the ink but beats the energy of
the correct one um Delta here is is an
application dependent discrepant you can
add to sort of make this a bound on your
more application dependent loss for
example you might use the having loss so
if you can actually do this this
maximization in closed form you can do
learning on an objective like this but
in the case of how you typically can't
exactly max lusts and so what's
typically done is it's it's relaxed into
an upper bound so ignore epsilon for the
moment um and what you what you're
essentially doing is you're maximizing
over the space of marginals over the
output rather than the space of outputs
themselves so in fact if epsilon is 0
this maximization here is precisely a
linear programming relaxation of this
maximization but it turns out to be
beneficial to add a little bit of this
be smoothing to maintain French ability
so in this there's been some great
previous work where that this was
reformulated a little bit so this this
minimization with respect to um this is
a convex a problem and and you can
compute this objective for any F but the
problems that you have to do it's a
minimax problem so if you'd like to
evaluate your learning objective you
have to do inference with respect to
your your whole data set and repeatedly
solving this inner maximization
inconvenient but but you can take um
this inner maximization and you can
reformulate this into a joint
minimization or g function f and over a
set of lagrange multipliers lambda i
don't have time to describe the
particular form of this function a but
it's just it's just a simple function
and so these are completely
equivalent and and the benefit is that
here you can optimize f + lambda in
either order so it's this is a easier
optimization to solve so how these how
these algorithms work and this is
previous work is they work between
alternating taking gradient on the
energy so here the Energy's is assumed
to be linear and then for all K and
parallel update edges and this turns out
to coordinate a cent on these these
Lagrange multipliers it turns out to be
essentially message passing in front
I'll spare you the details um so this
this this well but something I noticed a
while ago was that if you take this this
this optimization and you look at
minimized the the energy function f for
a particular click c44 fixed messes that
can be reformulated exactly into a
logistic regression problem um but but
in the logistic regression problem
there's there's a set of bias term that
depend on the incoming message so you
get an algorithm like this in the first
step you do logistic regression with
these the biases by the current messages
and then and then you do message passing
well so interestingly if if if lambda is
set to 0 this almost it's almost
identical to optimizing likelihood so
you can think of this this algorithm as
alternating between solving a piecewise
type of loss and doing message passing
to tighten that
a single iteration it essentially is
peaceful right so on the this is the
detailed algorithm um it's it's actually
it's not very difficult to implement you
you have a set of biases that you
compute based on the discrepancies from
that you solve a logistic regression
problem for each click and then you you
update and do a message passing it's
important that they're both of these
algorithms message passing does not have
to be done to completion you can have a
very
convergence criteria or just pick a
single and then these are still
guaranteed so the advantages of this are
that first of all you need to do less
message passing typically message
passing is the bottleneck in these types
of learning algorithms and because
you're you're optimizing F all the way
rather than taking a step you don't need
to do as many iterations of message
passing um secondly since you're
optimizing a logistic loss you can use
for the factors any function class that
you can optimize a logistic loss over so
this being Microsoft I guess you would
obviously use an ensemble of trees but
the variety of things so here's an
example on the horses database and what
I'm showing is um pose or indifferent
univariate classifiers and the pears are
in the columns are indexing different
pairwise classifiers so as you see as
you as you move toward which is a good
layer perceptron or boosted trees and a
net
you
if you look at the the training curves
you can see the number of iterations of
alternating between do edge doing I
think this is iterations of message
passing and relearning you don't need
very many iterations before you've
saturated the test loss which is in the
black curve here and this this really is
quite efficient I think learning this
took maybe an hour or something on this
laptop to learn these models so it's a
much faster learning this you're
learning these nonlinear functions in
terms of test error you see you
basically the the same phenomena you get
four errors from using more powerful
functions so the advantages of this are
that it's efficient and you can you can
use a lot of classifiers um
unfortunately that the reality is that
if you're in you a linear set of
energies truncated learning does tend to
work better for example on this database
with the same features truncated
learning would give you air six or nine
percent training test error whereas this
gives you 12 and 17 percent and the
reason for that is simply that that
truncated learning is optimizing a loss
which is much more reflective of the
particular error measure that that we're
talking about here so all else being
equal you would prefer to use truncated
learning but it's not all equal
computation time matters and also you
know being able to you powerful function
class
alright so questions about that I'm
going to turn to a different topic now
so so previously I was talking about
methods to sort of make approximate
inference work well so in an alternative
to that you might be concerned with
these methods that Victor that you get
at the end it doesn't correspond to the
particular graphical model or the
parameters that you learned or indeed it
may not correspond to any graphical
model or parameters so in some cases um
you you really would like to have a
stronger pulse on what you're actually
doing and so I think an alternative is
to restrict the parameter so by far the
most commonly used example of tractable
parameters would be tree-structured
parameters so given any graph we could
say that theta is tractable if there
exists some tree such that theta is 0
whenever you're the tree so this is the
classic if you have a parameter set of
parameters like that you can do
inference exactly by message passing on
the tree you can also do approximate
inference bye-bye vmu so given some some
some intractable sigh you could try to
minimize the KL divergence attractable
theta 2 sigh and but what meeting field
inference does if richard lee factorized
and you can also do maximum likelihood
learning so it's it's it's a bit
surprising actually that you can do this
because if you look at this this set of
tractors it's sort of a combinatorial
object right because there's all there's
the set of all these possible spanning
trees but nevertheless you can find
exactly the maximum likelihood by
computing the mutual the impure
information of all pairs in the graph
and then finding the maximum spanning
tree on that on that fully connected
graph and that true and the maximum
likelihood distribution on that tree is
exactly the the maximum likelihood
solution is the child ooooo algorithm
from 1968 I believe it's really a nice
result so we want to talk about a
fundamentally different notion of what
it means for four parameters to be
tractable and that is that will say that
theta is tractable if a given Markov
chain Monte well Griffin can be
guaranteed to quickly converge to the
stationary distribution so this this
could be um we're not constraining the
structure of the graph at all your tree
which can be arbitrary but we want to
guarantee that this algorithm will be
efficient so everything that I'll talk
about here is using a unitive sampling
in univariate Gibbs sampling you just
repeatedly pick an index and then you
sample X I according to the conditional
distribution of X I given all of the
others I'm interesting like it
times community glauber got his name on
but um just so there's what
so I think it's an amazing algorithm in
principle you can you can say arbitrary
distributions but the problem is that
Mick varies
to get two samples from the dead of
interest
a uniform Isaac model on a grid
the strength of point 25 you can see
that if we if we take twinstar they look
nearly independent and put five all the
those are sort of in this
call negative 1 mode though by symmetry
of them is just as much probability
density in the all white kind of
because of because these modes are sort
of so far away sir it's the lever mix
between the two of them
threshold is known where we're mixing
time breaks down to we want if we're
going to use do
practice we have played this kind of
behavior so the result that we showed
for this is if you take a pairwise mark
of run so we have these parameters theta
IJ for all pairs we can guarantee fixing
will be fed if the following holds this
is a little bit technical but but what
do you do is you first each pair IJ in
the model you compute this this this
this function on i J and what would you
take the certain maximization over the
the configurations of two variables in
that particular factor so you do this
for each pair and you stick it into this
big big microchips are essentially what
this measures is how long are the
connections between X I and XJ then once
you have this matrix R if the if you can
take any sub multiplex no arm and if the
value of it is less than one Guerra
guarantee that the mix will be of order
n log n here tao of epsilon this is the
of markov chain steps t that we have to
make to guarantee that the the sum of
our coming from a distribution that is
at most distance epsilon from the target
so sampling will be will be n log n as
long as some sum norm a value R of theta
is not too big and again there's really
no magic here that this is this is still
just measuring in a precise way that
that the interaction tables are notice
in particular that the univariate
potentials can be our meeting will still
be fast right so the trees we can come
this function are we can compute a
matrix norm and if less than one mixing
is going to be fast some in the
experiments you'll see here we'll be
using the the spectra tuned arm so so
given this result it's easy to check if
a certain set of prime are tractable if
you give me theta I can compute this
matrix R i can compute norm and i can if
you're guaranteed to be fast mixing or
not the problem is if we want to use
these parameters to do approximate
inference or to do learning or something
we need more than we need to project
onto that set and you can see this can
be a bit difficult because it's it's
find implicitly in terms of this
function are so to visualize this please
imagine well so what we have is we have
some intractable or not known to be
tractable parameters sigh and we would
like to find the parameters in the
tractable set that I've got the closest
in terms of Euclidean distance so
imagine that's the the we'd like to find
point theta star so to do this this is
this is the optimization problem we'd
like to solve and it turns out this is
in fact a a convex problem and so we
solve it by taking the dual and then
solving the bounded limited memory GS so
the duel is is a bit complicated but
i'll just call your attention to two a
couple of things so we have this
function g which is a value as you have
these it decomposes into these two
minimization zovirax we can solve this
this first minimization via singular
value decomposition and we can solve the
second one form we can compute the the
objective efficiently and then we have
these simple bound constraints it but
convex optimization you can solve it
particularly difficult about it so this
gives a you have a very natural idea
here which is suppose that someone gives
you some parameters sigh which are not
tractable and you want to do inference
on those parameters so a very natural
idea would be to say great let's let's
projects I onto the tractable set let's
find the closest parameters theta and
let then we can just do sampling using
theta the prom is this doesn't tend to
work very well Google's help results but
the reason is that euclidean distance of
parameters is a very poor for the
similarity of the distributions that are
induced to milos / am so if we want this
to work better we would like to use some
sort of a probabilistic divergence
measure so assuming that we have this
this Euclidean project go well how we
would like to minimize these divergences
is just through projected gradient
descent so provided we can compute the
gradient of this divergence with respect
to parameters theta we can get at least
a local minimum of this function by just
taking repeatedly taking steps Andrey
projecting so the most natural
divergence you'd like to use would be
the the KL Dave from psy to theta so
this is really it's really the
divergence to use because in a certain
sense it tries to preserve the marginals
of psy fortunately computing this
divergence or its gradient it requires
computing expectations with respect to
sigh and that's that's the problem that
we started with so if it doesn't work to
have you know a subproblem in your inner
loop than the original problem that the
optimizing so you really just you can't
do this um what you can do is you can
take the divergence from theta I and
this again this is exactly the KL
divergence that that mean field use it
so you can think of this as a structured
mean field algorithm where your
distribution is a fast mixing
distribution rather than a
tree-structured distribution or
something like that so again assuming
that we can sample from theta which
which we will be able to to since we're
it to the tractable set you can you can
approximate the gradient of this this
divergence without local minimum so I'll
show some simple example using models so
there's a lot of curves here but the
ones I want you to look at a ROC curve
which is running sampling after we've
projected in the KL divergence and the
sort of I don't know what is it magenta
curve which is just doing give sample
the original parameters so this is this
this is a icing grid with a you know
medium strength interactions between the
variables and so you can see indeed
sample with the original parameters
provided you run it long enough of
course it's all going to give you very
accurate samples this is this is
measuring you mean univariate marginal
error however projecting and KL
divergence is the order to faster here
results um 10 node random rather than
grits it's a similar similar to the
story you can see here the results of
Euclidean projection which is which is a
quite a bit inferior although still
better than simple variational methods
there are however much better
variational methods which would
presumably are much much stronger so
rather than fixing the interaction
strengths and varying the amount of time
but you can do is you can you can fix
the amount of time available available
for sampling and you can vary the
interaction strengths so you can see
here for weak interactions several
parameters it works extremely well but
there's sort of this this phase
transition around here at which the
errors go to point 5 which is just
exactly chance but but you're projecting
into the divergence it you you decay
for these strong interactions selector
for random graphs although you can see
Gibbs sampling sort of hold on longer I
for one was extremely surprised to see
that the strong perform compared to a
simple variation on these types of all
right so I'm going to turn now to the
big learning fast mixing sets so here
I'm going to be working in with
exponential family notation so if you
take a graphical model like we had
before by defining a certain function T
of X you can you can this this is in
fact an exponential fat like T of X is a
it's a set of indicator for all the
configurations of all of the clicks so
it's more natural to state the results
for the general exponential family so
what we'd like to do is we would like to
find the minimum of the log likelihood
of some data set it's easy to show that
that the gradient of that likelihood
takes the form of the difference between
value of T under your current parameters
an expected value of T under the data
set so t-ball fixed and easy to compute
so the trouble with computing this like
loot in
first expectation so an algorithm that's
extremely widely used for trying head
learning would be would be something
like this so you have K iterations sorry
you have K iterations and each iteration
you're going to draw a set of n samples
using V iterations of Markov chain Monte
Carlo then using those samples you
approximate your likelihood gradient and
you take a small gradient step I mean
and then so what you would hope is that
you know as you repeat this you keep
getting good right grants and you
converge to the minimum of the the
negative log likelihood the problem is I
don't think that this can work in
general because if your parameters have
heard it into a part of parameter space
we're mixing time is exponential these
samples are not the current distribution
you'll have a bad grading estimate and
in your inner world so if we redefine
the problem to be something easier I
think however may prove something so
let's see if we restrict to the
parameter space where we're known to be
fast mixing and we measure against the
the strongest distribution in that
tractable set now maybe we can actually
hope to prove something maybe we can
hope that if we do projected gradient
descent like so we can actually
guarantee that will converge to
something close to that to that to that
distribution so intuitively on this this
seems like it should work if we have
missions Bulls would be run the the
Markov chains long enough you'd think
this would work but but proving it is
very involved for the reason that the
theta is it at all time all of these
random variables are completely coupled
across time and also I'd like to point
out this is not stochastic gradient
descent because since you're running
that these Markov chains for a finite
number of iteration your gradients are
not just noisy they're also biased so
it's very tricky to analyze convergence
rate of this algorithm so you can really
wonder you know how close is it going to
be how big do we need to make these
parameters km and V anyways it really
actually work so you can imagine from
the fact that I'm talking to you about
this that does work
and we can guarantee that if we say that
f of theta is the best solution in the
tractable set then with high probability
the difference between our solution on
the average and the optimal is bounded
by these three terms this first depends
essentially on the diameter of the set
this is sort of the Monte Carlo error
and the third depends on the mixing rate
of the chain or C and alpha or
parameters that determine how fast
ticular markov chain of interest is so
this has the right spirit in that if you
make km and you're going to tend to
decrease the the right hand side and so
you'll guarantee that your solution is
going to be close to optimal however
what they want is you want to do the
smallest amount of work such that
solution so what you really like to do
is to minimize so what a way to quantify
work would be the total number of markov
transitions that you apply in the entire
algorithm and in this case that would be
km and V so like to minimize km and be
such that you can guarantee that the
right hand side here will be the
simplest way to do that would be to set
three terms to the epsilon divided by
three if you do that you get these
parameters and that yields a total
amount of work up to logarithmic factors
cubic undivided by epsilon this next
sense intuitively because grint sort of
1 divided by K and Monte Carlo sampling
is one divided by this square root of M
but you can actually show that this
solution is some factor of the optimal
one and and if you'd like to you can
find the optimal one article search
these are all just small integers you
can find the best one so it's just a
there's an example of learning on an
Isaac model this is a small Isaac model
so we know exactly the we have a bound
on the mixing rate this is learning a
randomly chosen outputs so here's what
the results look like I'm on the
left-hand side you have the the sub
optimal parameters against a que
essentially time
get this rather simple convergence and
on the right a particular run the the
parameters as a function of time
measured against the maximum likelihood
parameters so this is for epsilon equals
1 and these are the parameters that come
out of that the finding those that
minimize the work if you make epsilon
smaller you can see that naturally the
pram jiggle around less and again for
epsilon 25 so this again I I don't
really claim a much of a practical
contribution from this because it's an
existing algorithm it's mostly a
theoretical result that you know though
it might be intractable to do you know
maximum likelihood learning oh just like
with trees maximum likelihood learning
inside of a tractable set does have it
has a fully pollen randomized
approximations game um quite
conservative so I mean look at so what's
so one here this is epsilon equals one
and you can see that the it's converging
to a value significantly tighter than
epsilon equals one so when your analysis
of an existing algorithm it's
now at fault right because you can see
that as we change epsilon by a factor of
two the actual type need change by about
a factor of two so there's reason to
think it's useful guidance on how to
select these parameters km and V this
justifies a lot sort of existing
procedures anything that the people who
are learning restricted Boltzmann
machines are actually they're very much
aware the need to regularize their
models to to encourage fast mixing
heuristic penalties such as the
Frobenius norm alright so I'll briefly
conclude um in terms of learning with
the province the lesson is sort of that
well using the right loss function
matters but you know computation time
also matters and having the flexibility
to use powerful functions also matters
so there's action in these two methods i
talked about between you know truncated
learning which fix exactly the right
loss function and these dekum this
decomposition method which is very
efficient but but optimizes the loser
bow so my dream would mean to have a
decomposition for learning a marginal
based loss like we talked about um this
gets extremely technical because of a
need to to bound lots of non convex
function but think it is possible in
terms of fast mixing sets you know
ideally adding another set to the list
of things that can be learned tractable
you'd have some sort of a unifying
theory of octuple inference and if
you're going to look at truck better the
action is going to be in parameter space
you can subject to some cryptographic
conjectures that if you're looking at
structural considerations is sort of the
only property that's going to to give
you efficient inference so anything else
you're going to have to look at the
parameter space the problem univariate
Gibbs sampling is not it's definitely
not this final answer for the ring that
you can show that there are
distributions on trees
we're message passing will be fast we're
univariate Gibbs sampling in fact takes
exponential time so you can in a Venn
diagram of distributions that are that
are shown to be tractable by these two
different bounds sometimes the future
work I'm looking at with some people who
are experts on sort of Bregman
divergences at minimizing other
divergences and I also think there's
there's there's work to be done on sort
of deeper integration in fern sand mcmc
the method I showed you it's just sort
of you do variational inference and you
call em cnc as a subroutine I think that
you can you can do something better than
that where you sort of you unify the out
the mental level so thanks that's all
that's all I've got
keep okay we have time for questions if
you're positing a second block so the
used to keep samba because it's a
natural starting point so for example
discrete models it's known that a result
of Leo in the mid-90s that you can
basically get it faster using this
metropol ice cream sampler so basically
small twist on the content of only four
discrete so do you think if you were
that he could potentially enlarger from
just to keep sampler eat all local Sam
plus it performs some functions of
larger outer space of possible samples
and would would that be a way to maybe
get them to improve the results oh hi
you the analytics conditional they give
some play and that's really what allowed
to do devices with us so yes so so
everyone heard the question so the
question was essentially the the
univariate gibbs sampling is not a great
sampling algorithm we should probably
look at better rhythms and I think
that's absolutely right as a strategic
issue in terms of the research I've done
in terms of looking at tractable
inference anytime myself trying to prove
a new mixing time-bound I would
deliberately not do that because it's
sort of as a whole there's a whole sub
community that works on these things the
issue is that that sub community their
priorities are sort of a bit different
from what your priorities would be in
terms of as a machine learning person
and the best analysis comes from these
varies you know things like you know
mixing on a grid with with us one per
interaction strength but these types of
models almost by definition they're not
useful for learning whether there's
there's no appears to be learned and so
there's there's a bit of a tension
between you know good bounds for very
specific situations and but more general
bounds like like like the one I was
looking at so so i don't i don't know of
the particular case that you're talking
about if there if there exists a better
bound I'm something I would I would I
have looked at actually it would be
using
swenson wang sampling which which seems
like a great thing to do because that
that is indeed fat on trees however it
is far as i can tell exist there's no
results that are amenable to the types
of things that are being used here
sometimes that's just because you know
their interest is sort of in the
physical properties of the system that
have mathematical interest rather than
what we're trying to do so but may be
bothered to prove the result so that
would be true for example for the mixing
timers or f's that I shown it's it's a
relatively straightforward application
of existing tool and sometimes very
harder to choose the bounce I'm flipping
the divergence it feels like it feels
like it induces a couple of start
wouldn't want to start from but we've
just does can you give some for starting
points for theta the this induces so
you're concerned that the divergence
might be non convex in theta and then so
much that just to explain the marginal
induced by sy right now you're doing the
most by theta and obviously if they're
all over the place or a wave or or zero
everywhere feels like there may be some
rules of thumb for where you stop yeah
it's right I mean it you really would
really like to use the other the other
KL divergence so something I'm looking
at more recently is the sort of a
natural thing would be to
might be doable sweets very strange
happens when you look at alpha
divergences you can indeed um optimize
alpha divergences other than the
particular reverse scale depths but you
get this strange thing that happens
which is that the the parameters that
you sample from you need this additional
condition so such that as you move alpha
from the reverse KL divergence closer to
the rect KL divergence the set um theta
that you can you have to sort of
shrinking so you see you the trade-off
between having a better divergence and
the set that you can optimize over I
mean you seem to get a very strange
result you would imagine that the set
would would shrink to zero at out you
know but that doesn't in fact you get a
an alpha somewhere in the middle i I
can't constructively show what the
number is but you you actually get an
empty set for a value of alpha somewhere
between zero and one so well it's it's
it's odd that's I'm not quite sure what
what the right answer is you also might
hope that you would get an eater
approximating family by going further in
the wrong direction
reverse KL divergence but that also
doesn't appear to work out with this
that I'm looking at with people who knew
divergences
anyone okay so ed and before we applaud
if you want to get to meet with Justin
what are you leaving tomorrow probably
around five o'clock these are all day
tomorrow the rest of today email Becky
frost I sent out an invite to MLP that
will go for dinner this evening probably
in a pub if you want to join email me
thank you again thank you each year
microsoft research helps hundreds of
influential speakers from around the
world including leading scientists
renowned experts in technology book
authors and leading academics and makes
videos of these lectures freely
available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>