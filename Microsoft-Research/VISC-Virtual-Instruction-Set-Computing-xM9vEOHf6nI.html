<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>VISC: Virtual Instruction Set Computing | Coder Coacher - Coaching Coders</title><meta content="VISC: Virtual Instruction Set Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>VISC: Virtual Instruction Set Computing</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xM9vEOHf6nI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so I think we're ready to start
it's a pleasure to introduce Vikram
advil who is visiting us is a is a
professor at University of Illinois at
urbana-champaign is currently visiting
EPFL on a sabbatical Vikram of course
needs no introduction right with
together with Chris lat Mary student he
developed the LLVM compiler
infrastructure which is now taking over
the world and it is by everybody in the
planet for loads of great things and
he's going to tell us about some of
those things I think thank you Thanks
hello yeah so it's great to be here it's
my first time in Cambridge um I was
telling Miguel that this is one of the
few places this an MSR Redmond is one of
the few are one of the few places where
there are so many programming language
software engineering kinds of people
that you could fill a whole academic
department with just these kinds of
people it just doesn't happen in any of
the universities that I visited anyway
so I'm going to talk about a broad sort
of project with multiple facets that we
call virtual instruction said computing
and its joint work with a number of
people but I just caught a couple of
them and my pointer works yeah so John
Criswell did the security work on secure
virtual architecture and he is now an
assistant professor at the University of
Rochester or continue to work on on
security from both programming language
and operating system points of view
chris lattner of course led the llvm
work and his has been an apple for many
years and swarup sahoo who got his PhD
in 2012 in is now oppose talk in my
group did the work on automated bog
diagnosis which I'll talk about in more
detail today so as I'm sure you all know
the way that static languages the
classical C C++ fortran these kinds of
languages have been compiled then the
compilation model for them has pretty
much stayed unchanged
since the earliest days of camp of
compilers and so for example so
essentially in all these cases a
front-end compiler generates either a
binary code so traditionally they would
generate binary code for each file which
would be linked and then executed more
modern compilers now add an optional
interprocedural optimization pass where
individual files might be exported in ir
form linkedin ir form so that you can do
cross modular optimization and then you
run it through a static code generator
and then that code is shipped and
executed but the common point here is
that in all cases what you're shipping
is native binary code and that's the
part that's remained unchanged for the
longest time since the earliest
compilers and even modern languages like
Ross do essentially this but an
alternative model that is actually quite
widely used today is what we call
virtual instruction set computing so the
term may be new but the concept is not
all new but the idea is that instead of
shipping native binary code you ship
some richer representation of software a
virtual instruction set of some kind and
then you translate that virtual
instruction set to the native code for
the target machine on the end user's
machine or or once you know what ends
your end users machine you're running
off and you can is generate code offline
you can generate code online using a
just-in-time compiler you can use
profile information from end-user
executions which are much more
representative than some generic profile
gathering to do a runtime optimization
in theory you could even use those
profiles to do optimization between runs
on the end user's machine I don't know
of any system that actually does that
but it seems perfectly possible and it's
a matter of engines of engineering and
the point is that in all these cases by
shipping a Richard representation you
can do sophisticated analysis and
transforms on that end user machine and
the point so the point here is that the
virtual I I see is what enables these
analyses and transforms and so the
informally we define version instruction
said computing as any case where the
software is a now I say that's used to
ship the software differs from the
target machine is a and I'm sure many of
you know important examples of this that
are widely used today yeah JVM dotnet
these are all examples of shipping a
rich social instruction set right in
fact this idea goes back to early lists
compilers which shipped a form of
bytecode IBM really took this to to an
extreme they had a machine instruction
set for their early mainframe series the
system 38 and later the as400 and others
that was so rich that it included
operating system primitives database
primitives things like that in the
virtual instruction set but that has
been they maintain compatibility for 50
years of hard work on that virtual
instruction said it's quite remarkable
it's up in frames or expensive machines
but much more popular systems today are
things like JVM in.net which broadly
focus on Portability and type safety but
scripting language is another good
example because there you are shipping
the source code itself that's a much
richer representation right so it's a
very good example another one that's
coming that's becoming more popular
today or GPU compute environment so for
example for CUDA the code that shipped
is typically PTX code and PT X's gives
you portability across different GPU in
a particular GPU family yes in any
language you can sep choke you can ship
source code but as a practical matter
it's very difficult to do that for
languages like c c++ photon and so on
because then you'd have to ship all the
header files and you'd have to ship all
of the make files in the build
environment that makes it possible to
actually link them together so language
like java and others define dynamic
class loading which gives you a way to
ship separate modules and link them as a
program runs but other languages
typically don't include that kind of
infrastructure so shopping source is not
straightforward
that problem of them late files in the
head about something equivalent to
that's my show up there yeah if you had
some another you could exactly yeah
absolutely okay so this is not a new
idea but the point I want to make is
that there are two at least two
important classes of software that don't
use virtual instruction sets and perhaps
they should so the first one is
high-performance software so things like
HPC applications for super computers but
many other contexts of media software
game engines financial software CAD
tools even web browsers database systems
many many libraries high-performance
libraries are essentially all written in
static language as in ship does need a
binary code an even more pervasive
example is system software so operating
systems hypervisors demons cryptographic
libraries things like that are
essentially all written in static
languages and ship does need a binary
code okay so what's wrong with this well
so there's nothing inherently wrong with
this it's just not as good as it could
be is this what we believe so first
modern hardware architectures are really
no longer a good match for this model
because of increasing diversity so for
example memory hierarchies have been
have been pretty diverse now for the
last probably 20 years or so what's
getting even worse now is vector
instruction sets vector instruction sets
with different vector lengths and
alignment requirements and instruction
features and things like that make it
very difficult to generate really good
quality vector code for all the
different possible variants of a
particular processor family another
example which is surprisingly common is
that on mobile phones and tablets the
application processor has a number of
accelerators on it but different mobile
phones let's say in a single family like
Android or Windows or windows mobile or
draw iOS have different configurations
of all those accelerators and so if you
want to write an app that's portable
across all this hardware it's very
difficult to use the accelerators for
application-specific core so the
accelerator is 10 too
used by apps only for generic things
that come with portable library
interfaces and otherwise they only use
the host CPU so for example an Android
they used J dalvik or art effectively a
Java variant to get portability across
different host CPUs and so portability
across heterogeneous processors SOC
which are basically all SOC these days
is a very difficult problem and so what
do you really would like to be able to
do is to adapt software using a compiler
analysis and transformations to the end
user to the specific end user device on
which you're running another problem is
the nature of modern software so modern
software increasingly uses of course
dynamic libraries that's been there for
quite a long time but also user loaded
software extensions and these kinds of
software extensions like in a web
browser or in a database system and many
others are effectively components that
get assembled into the executing
application only on the end user's
machine and so these are really barriers
to analysis and optimization there are
many other contexts in which this kind
of in which modern software would really
benefit from being able to do better
analysis and transformation on the end
user's machine I'm going to skip through
some of these motivations you are just
in the interest of time but the third
category are security challenges so I
just give a couple of examples here but
for example browsers these days are
almost universally add browser
extensions which run in the context of a
browser and have full access to browser
state and you really need to be able to
sandbox the browser extensions also
cloud computing is a very it's not
actually on the slide but in the cloud
computing environment anyone who wants
to run their applications in the cloud
has to trust the provider of the cloud
infrastructure and as well as all the
software layers underlying the
application in all these contexts you
are effectively running a untrusted
software in
for may be running on trusted software a
trusted context or trusted software an
untrusted context and if you could do
richer analysis and transformations on
the end user system you could do you
could have benefit from significantly
better security solutions and I will
give you a couple of examples of this so
the bottom line in all these cases is
that we would like to be able to do
richer analysis rich analyses and
transformations on the end user system
and a natural question that that you
might ask and you should ask is can we
not do this on machine code well JVM
you're already doing it i'm talking
about native languages where you don't
have a bunch of instruction set because
operating systems and browsers and
things like that are typically not
written see they written in C and C++
and there you don't have you basically
get native pollinated by McCoy this is
the legacy follow that if we will just
be working at a job if it was yes if you
could get rid of C and C++ and Fortran
and thrust so even more new languages
like rust which will be which has been
explicitly designed to write system code
is shipped as native binary code absa
like what these you know what the
virtual machines that are widely person
structures and widely distributed oh
very well then they do know well what
they don't do yes what they don't do is
support the two class of software that i
was talking about so HPC applications
are rarely ever written in managed
languages because not fast enough
exactly and hold 50 plus speed oh
security and speedier yeah absolutely
I'm actually also reliability so I'll
give you some examples of how automated
debugging can get much better if you
were shipping virtual instructions head
commercial absolutist traffic will he
give me the date yes you notice to do
reverse buddy buggy yes oh there's lots
of examples of machine code analysis
this this commercial tools there's
research tools there's actually some
very strong tools within Microsoft that
do this the
adjusting my translator so we can G ok
yeah yeah so you can lift it and in fact
the point here i want that i want to
make here is that any time you do these
kinds of analyses or transformations or
whatever your are constrained by some
some fixed budget of time and compute
resources and so the problem is that if
you start with native machine code all
of the tools today spend a large part of
that budget trying to lift the machine
code up to a higher level that gets
enough information to do better than
what you could just do with the machine
code itself so i'll call it quote
unquote a compiler ir whatever that
representation or information happens to
be and then you are only left with a
part of the budget to do the remaining
analysis and transformations whereas if
you start with a compiler i are in the
first place you can use all of your
budget to do those analyses and
transformations and that's a significant
difference it really does make a big
difference in practice but it's even
worse than this because in practice the
kind of information that you can extract
from the compiler ir is significantly
more limited than what you get with a
true compiler ir or something that came
from source code and that so there
there's a number of reasons so this the
instruction set semantics of modern
architecture is really so complex that
it's very difficult to reconstruct
information for x86 it's even difficult
to distinguish data embedded data from
codons from instructions in the code
segment so even constructing a control
flow graph precise reasonably precise
control flow graph is very difficult for
x86 and so for example commercial tools
like pin from intel don't even try to do
static analysis they basically are
designed to do dynamic techniques where
they constructed a control flow graph
dynamically by observing the behavior of
the program the memory model is
extremely low level for native machine
go and effectively all you know is there
a of earth and array of bytes and and
you have to distinguish data types and
code instructions and all of these
things it's very difficult to
reconstruct higher level types from that
so there's a number of reasons why this
compiler I are in quotes here is really
not at all
those two a compiler to a real compiler
gaya so you are quite limited in terms
of what you can do now in practice I
think it will always be the case that
some binary analysis will be essential
if you want to do anything for a whole
program but the point we I want to make
here is that we really need to try to
minimize this as much as possible and so
in a sense of the argument I am trying
to make is that to the extent possible
all future software really should ship
in a virtual instruction set form and it
am NOT we need a universal virtual
instruction set different systems for
example android vs windows vs linux
could define different virtual
instruction sets it's really different
you know the the environment of the set
of systems that use a common which
instruction set depends on many factors
but as long as you have a good
well-designed motion suction set for
analysis and transformations you can get
quite significant advantages and i'll
try to make a case that the security
benefits can be quite strong for doing
something like this the there are no
inherent performance penalties so one
common misconception is that you have to
use just-in-time compilers but you don't
in fact in our system we use static code
generation offline code generation that
is exactly the same quality in terms of
the techniques as before shipping the
code except it's at install time yeah
and so you actually have more
information on the end user system
because you know the exact system did I
compiling for so in that sense just in
time optimization should be an
opportunity not a cost ya well so if you
have a battery constraint device there
are other ways in which you could and
should do this for example you could if
you were downloading an app from the
Google Play Store for the corresponding
thing on windows or apple you could do
it on the server side you don't have to
do it on the end user device or you
could only you could def or doing it
until the device is plugged in or
something like that dementia is to do
with the invertibility of compilation
for debugging purposes and traceability
of profiling and that doing doing
population on the target device tends to
make it difficult to trace you have to
keep the information that allows you to
invert compilation so there you saying
it makes it mapping the maps or the all
the all the information the compiler
needs to trace back to the source code
in ER yeah reliable why is that I need
more difficult fundamentally you're just
all you're doing is if you think of the
compiler pipeline you're effectively
taking the with the last stage the very
back end and just doing it a little
later because the information the
residues from compilation need to be
stored on the device and if something
goes wrong like a crash dump yeah those
residues have to be sent back as part of
the crash dump and so the the cost of
sending the inversion back to the
original developer well is increased
because because you because the final
code generation the map from the
compiler I are down to the native code
happens only in the end user's machine
and not on the developers machine yes
and so you would have to send additional
map information back but in fact I am
going to spend some time on the
debugging benefits that you get from
being able to shape a virtual is a like
this and i think that they greatly
outweigh this kind of draw back the
third point I want to make which
hopefully will this actually I'm not
sure I really need to make any argument
so this but I think it's pretty clear at
this point that it is or it is
technically feasible and commercially
acceptable to be shipping virtual
instruction sets because it already used
quite pervasive lien's in many contexts
but just not all okay so as a quick
example I'm going to describe the LLVM
compiler system which we built in my
group which is an example of a virtual
instruction set it is not by any means
the only one by
oh and I am NOT going to expect you to
read through all the details on the
slide the main a couple of main points I
want to make here is that the llvm ir
which is the code that's shown on the
box on the right here is the compiler ir
and it but it is also a fully
self-contained virtual instruction set
in the sense that you can ship programs
in this representation and execute them
later on and moreover this
representation is rich enough to do so
quite sophisticated language independent
compiler techniques so it includes it's
a simple three address ir its
architecture neutral and language
neutral it includes the full explicit
control flow graph so you don't have to
reconstruct the graph effectively it it
just comes with the code moreover the
instructions always in static single
assignment form yes si is a relatively
widely used data flow representation
that enables a number of different it it
makes it possible to do a number of
different data flow analysis and
transformations much more efficiently
than traditional data flow techniques
and I said we have typed memory
registers this is really a week type
system because it's capable of
representing arbitrary C code for
example and even allows external
assembly but it does have some
information about types including things
like for example if you have an array
with an array reference the array
indexing operation is explicit in the
instruction set so you're calculating
the area in dixon and that allows you to
do ray dependence analysis in point
analysis much better than if you didn't
have that kind of information so that's
the llvm ir and the olivium compilation
model is quite close to the virtual
instruction set computing model that I
described earlier you can so front ends
can generate llvm which can be linked
and optimized and then a static code
generator can generate native machine
code just like a standard compiler does
but you can also ship llvm bit code and
then compile at install time or using
just in time code generation and you can
further optimize it later
moreover people have built experimental
managed runtimes for both JVM and.net by
translating the bytecode to ll be embed
code and then using the llvm for
code generation so all of these
capabilities already exists in the
olivium infrastructure and olivium has
seen pretty good success as makuha said
but in fact most of the uses of the
commercial user llvm like for example
all of apple's compilers now on iOS and
and Mac OS are based on llvm but most of
the commercial uses including apples are
effectively not risk in the sense that
they're all basically doing static ahead
of time techniques on the developers end
they use compile time in linked and
techniques so apple crane many others or
do this Google as a number of dynamic
tools for Fernando checking they do this
there are a few examples of risk systems
that use llvm talking about reserved
commercial systems so for example Mac OS
does it for graphics and cuda opencl and
render script ultimate for GPU compute
google uses it in an interesting context
for for browser extensions which I will
describe just briefly in a minute but
these are quite narrow contexts and I
think there are many other contexts in
which you could use an lvm for virtual
instruction set computing so I think
though the main point here is that it
does give you a reasonably good starting
point for building and exploring these
kinds of systems so in the rest of the
talk i'm just going to very briefly talk
about some of the security benefits of
wisc using two examples a commercial
example from google's portable Native
Client and a research project in my
group called secure virtual architecture
and then I'll spend a little bit more
time on automated debugging and how that
could benefit from by shipping a virtual
instruction set I will see how much time
i get clearly to spend on that and then
we have some our ongoing work is
focusing a lot on performance important
d but I really just have one slide on
that and then a discussion slide okay so
by the way peace please feel free to
stop and ask me questions if you have
any and this would be a good time
because I'm going to transition now to
talking about some very specific stuff
so let us quickly talk about the
security benefits here so the first one
here is the system from google called
portable Native Client which is used in
the Chrome browser to ship untrusted
browser extensions so typically such
extensions have been either written in a
type-safe language or they don't have
any don't have sufficient security
guarantees Google wanted to enable
people to write browser extensions in C
and C++ so to get the performance
benefits of writing code that we and
shipping native code but still to
enforce the security of the security
constraints of the browser and in order
to do that what they did is well they
first try to do this by shipping x86
code but and then using the segmenting
system in x86 but that was just not
portable it doesn't exist in in 64-bit
x86 architecture is it does not exist on
arm so then they ship they move to
shipping llvm bit code instead and they
basically define a restricted subset of
C and C++ that developers can use to
write browser extensions and then a
standard front and we'll ship llvm bit
code which is then translated within the
browser itself into native code and that
all of this is actually untrusted but
then a verifier running on the native
code checks a set of security invariants
that must be enforced on the browser
extensions and there are two advantages
here one of them compared with shipping
native code one of them is that the
static code generator is in their
control so they can enforce things like
alignment of instructions to prevent
jumps into arbitrary points in the code
and a number of other such invariants on
the generated code which makes the
both it makes the verifies job easier
and makes the performance of the
generated code better the second
advantage is they get portability so by
doing this they could relatively easily
port to the variance of x 86 and to our
last form of ella piensa correct exactly
so we call it red coat is the bytecode
because we shipped it from using a bite
level representation to a bit
granularity representation so
instructions it is no time limit that's
all it is yes yeah this thing ignores
the fact that it's not it is llvm we
just change the native code and we I
believe so I don't know if it takes it
upon so it suddenly takes advantage of
the restrictions that were imposed on
the C C++ front end but it doesn't toss
them so it has to verify that those
requirements for horizontal high-level
information and we come with you Olivia
okay no it doesn't need to it doesn't do
that but it earlier yes yes you know use
up your budget we constructing some
right I nice stuff right and in this
case they don't need to do that because
in a sense all they're doing in this
verifier is enforcing a relatively
limited security property which is
basically software fault isolation which
requires the so for example they do
require constraints on the on all the
loads and store loads and stores but
they can enforce that on the native
machine code itself so you're correct
this is not this is only taking limit
advantage of the virtual instruction set
here and it's not for the security it's
really more for the portability and
simply and it does speed up the code
generated by the for the extensions you
know the questions okay so that's that's
one commercial example of a system like
this the second one is a research
example which is a project or secure
virtual architecture which has been used
for a number of different kinds of
security solutions the goal of the SVA
project was to explore what kinds of
security benefits you'd get for
operating systems if you could write and
ship a whole OS in a virtual
instructions
now i'm not talking about operating
systems that are written using a safe
language like singularity or job OS or a
number of other systems like this that
have been written to take advantage of
safe programming languages i'm talking
about commodity systems like linux and
freebsd and others which are written in
c and c++ so uh in in traditional use
they do not get any of the benefits of a
virtual instructions and the way we did
this was to extend llvm with a few
operations in fact on the order of about
I think 30 or so operations that
effectively looked like an API so
they're implemented as a library and
they provide the kind of interactions or
privileged operations that an operating
system needs from hardware in order to
in order to work and so effectively the
virtual I say here is an extension of
llvm with these operations which we call
SVA OS so the op the kernel itself is
compiled into llvm bit code form and the
s vos provides the runtime operations at
the kernel needs s vos has implemented
as a library which is linked directly
into the kernel so these are not hyper
calls it's different from a hypervisor
model where you're doing calling up
higher privileged layer the library runs
at the same hardware privilege level as
the rest of the column and the
translation of this ll be a bit code can
happen at install time it can happen
while booting it can happen at runtime
using a JIT we've built both installed
time and just in time engines for
booting and running a colonel but in
practice for all our experiments we just
do it at install time because at least
so far we've not specifically made any
use of the just-in-time translator so
that's
a high level overview of SVA OS and I'm
sorry of SVA and just to give you an
idea of what the s vos API is like these
are a few examples of the kinds of
operations it provides there's actually
nothing terribly novel here this layer
is sort of like the hardware abstraction
layer in Windows or other operating
systems have also used a low level
abstraction layer of the hardware that
to which the operating system gets
ported and so we provide things like
function calls to register interrupts
and trap handlers to update page
mappings in the hardware page tables 2
i'm sorry in in the operating system
page table so the page tables themselves
are managed by the s vos library to do I
owe context switching is an important
one so for some of the security
solutions it's very in fact for all of
them pretty much you need to be able to
protect the same state of a thread and
not allow the OS to be able to scribble
on it if you want to make the OS
untrusted and so when we swap the state
of a thread we save it into protected
memory within the s vos library and only
returned an opaque handle to the
operating system so the West can say I
want to make it can request changes to a
particular bag of saved state using
other operations but it cannot directly
right into those into the fields of the
same state like the program counter of
the stack pointer or other things like
that for a single delivery for example
you cannot just arbitrarily allow the OS
to manipulate saved state of users of
user processes and I will give an
example of why this can be important but
so even so signal delivery for example
happens through an API function here and
similarly any changes that you want to
make to the same state of a process so
these are all effectively library
functions and in practice the way you'd
use this is
you would port a kernel to this API just
like porting to a new architecture
except that it happens to be a virtual
architecture so in fact in practice it's
a lot simpler and once you do this then
the kernel itself will have no assembly
coordinate all the assembly code is
actually encapsulated in the
implementation of the SPOs library
functions and moreover all the
interactions with the hardware happen in
terms of these operations that the
compiler understands so you have a
little abstraction of how the OS is
manipulating hardware state and program
state which didn't exist before and this
allows better analysis of Colonel
behavior and so this is how we ported I
say we but that sort of a royally this
is so John crisil did this work for his
PhD thesis and he ported both the Linux
kernel and the freebsd kernel to s vos
and just to give a sense of how what the
impact is on the trusted computing ways
for the Linux kernel most of the changes
happened to the architecture dependent
code in fact if you look at the lines
that were modified there were about 300
architecture independent lines modified
but those who are essentially all for a
particular say security policy you
weren't actually necessary for using SVA
just just plain old SVA this was for
memory safety in particular essentially
all the changes for us to use svi happen
in the architecture dependent's ID and
they amount to a little less than five
thousand lines of code and that compares
to a few hundred thousand lines of code
in the overall curl so the piece of code
that you would be trusting for a
particular security policy would be much
less than the whole in the original
konam so SVA so one question you should
you might want to ask is what is the
benefit of XP what advantages does it
give I think what's important about SVA
is
it gives you a unique combination that I
don't think any other system has that I
know of at least first it gives you rich
compiler capability similar to not quite
as strong as but quite close to what you
can get with the JVM or.net because you
have a full compiler I our
representation of the of the operating
system and better abstractions of the
hardware interactions in addition to
that you also get the ability to
supervise OS behavior like a hypervisor
nerves because the SPOs library even
though it's not a higher level of
privilege it's logically at the same
level as a hypervisor it's observing all
the manipulation so for example making
changes to page tables can be fully
interposed on by the s vos library and
in fact that's a crucial part of all the
security policies that we believe there
be when post and that combination of the
hypervisor like monitoring capabilities
and the compiler techniques is what's
unique here and you can use this sorry
song of kum'halla capabilities mean here
meaning and you can do much better more
precise analysis and transformations
then you could do just a native machine
code so if I was compiling freebsd for a
particular architecture compilers going
to do that so I'm gonna do you know
going to optimize the bell voting system
code anyway it's not optimized anyway
yes but what I'm saying is if you just
get if you generate translate freebsd
down to native code and ship that and
then give that to SVA now the compiler
techniques in SVA are more limited
because you're having to do any kind of
analysis or transformations on native
machine code rather than the compounder
virtual instruction set representation
and so for example i'll give you so
memory safety was one of the properties
we enforced and i'll come back to that
in a second but for memory safety we do
a number of fairly rich compiler analogy
we do point analysis we do a form of
escape analysis we construct the call
graph and
and these kinds of analyses are quite
impre sighs when if you try to apply
them to native machine code but can be
much more precise here and that
precision leads to significantly lower
overheads you did that analysis when you
are compiling your freebie as the
operating system in the first place yeah
do that I'm presumably people do of
course yeah we could do that on the
receiving end you can do it on the
receiving end but if you did it on the
sending end if you did it ahead of time
then it you'd have to either trust
somehow the binary code and preserves
all the security policies that were
enforced and the on the developers end
by the front end compiler or you have to
be able to add proofs that can be
certified on the receiving end so it's a
trust issue absolute if I build you a
free of this press the FreeBSD system
you install it you have to trust me I
bring a trust you or have to verify all
the checks yes exactly you're saying no
no if you run this and if I ship you the
llvm code then you can check locally
great why exactly yeah we can apply all
these techniques at the end user machine
yes it's officially a few of them and
the you know that provide purveyors
especially trustworthy we might think
the trust is less of an issue than we'd
say you know some random little app well
it's not so much trusting the developers
it's really interesting against
compromises against exploits it's all
the bonds that might happen that
exploits that might happen and so for
example if you go back and look at the
reported exploits a CV ease for windows
for windows 8 i think it was in the last
six months there are multiple screen
fools of them we basically we're looking
through that list and there's a quite
large number of them that can lead to a
full compromise off the operating system
were they to do with somebody messing
with the bits between Microsoft and the
link installed on your machine well
tackle in the middle which is what you
are describing no no so remember that
what we're trying to do here is to
prevent compromises off the operating
system from violating any of our
security process
if you or so at the end of the day
that's our goal now you're right that
you could either do that ahead of time
with ahead of time techniques but then
you'd have to verify them on the end
user's machine and they're the only
there the additional thing you don't
worry about as you say is some kind of
man in the middle attack on the bits as
they're being shipped yes but there's no
need so I think it's a chicken and egg
thing we like all secure ease you just
copy the screen full o course not how
many of the verge to do with
man-in-the-middle attacks right not but
remember that's a fundament that's like
a standard sort of chicken and egg
problem with security that if there are
easy ways to attack a system people are
not going to go to great lengths to find
some complicated way because today they
don't have to do that man in the middle
attack they can just go attack windows
directly if you made windows more secure
by doing ahead of time techniques now
you have to worry about more
sophisticated attacks because attackers
will be forced to do that and so really
what you're trying to do here is to
provide a security solution that's a
secure as possible that's really what it
is you know why this makes any progress
Oh makes any progress relative to
current system is right if so then find
one so if I want to be guaranteed what
Windows that I'm running is the windows
at Microsoft I can just check the
certificate right now I just this is a
secure it's securely signed if I want I
can turn on the TPM stuff that ensures
that i only boat signs images and I'm
done I don't need to worry about where
the best got compiled so this is also
you're right about that so so
effectively you could Microsoft's
compilers ahead of time compilers could
perform all the the compiler techniques
that we use for our security policies
and then you could ship that binary code
with a certificate and just check the
certificate on the end user's machine
that's absolutely true yes you could do
that yeah yeah and there you are
effectively using the certificate as a
way to to check bits and to preserve the
one small firm against attacks um yeah
much more plaintiff analyses yes 100
million times more expensive but knows
it yes problem is the Microsoft don't
actually do that oh I said I Microsoft
your code thus contained opportunist
wretch Alex and what his solutions going
to do is to catch those exploits on the
target machine um those words you've
look that's a very strong statement
millions of dollars a year on analysis
both static and dynamic to try and
prevent security vulnerabilities right
and in fact I think it's true that for
an organization with the kind of button
with deep pockets like Microsoft you
couldn't at an organization that also
controls the company Rowan compilers and
so on you could effectively build all of
this infrastructure within the
organization and just use that as a way
to enforce these security policies I
think this kind of model is much more
useful in systems where you don't want
where the organization doing the
operating system whether it's linux
freebsd or whatever doesn't want to be
also having to control the compilers
that enforce all of these properties
where you could instead rely on a
late-stage compiler that does just a
back-end compiler that does the security
checking for you so let me just give you
an example zuv the security policies
that we've been able to enforce with
this kind of system and I'm just going
to say a few words about them I'd be
more than happy to talk about the
security policies offline if you if
you're interested but the first one was
SVM which was the first system to
enforce full memory safety for a
complete commodity operating system and
we did this for the Linux for the Linux
kernel the second one is we call it
coffee it's the first system we now have
to provide full control flow integrity
for
played commodity operating system and
the motivation here or is that memory
safety while it's a pretty strong policy
has significantly is fairly high
overhead and many of the attacks on
memory safety bugs are effectively
control slow exploits and a controller
integrity policy can prevent most of
those at axle or all of the controller
exploits kinds of attacks it's mostly
lower a bounce checking that's right yes
and in particular Ravens checking in a
language that doesn't have any support
for it because now you don't know for a
pointer what object it points to and
where the object begins an end so you
have to keep a lot of additional
metadata in order to do that and looked
at me turn it up at one time the third
policy which is significant bishes is
something we call virtual ghost what
this does is it allows a system it
allows an application to not have to
trust the underlying operating system
and still preserve its own
confidentiality and integrity and so
even if you have a compromised off the
operating system on which that under
which the application is running the
applications code and data can neither
be read nor tampered with or any data
that it wants to protect can either
betrayed not tampered with by the OS
even if the OS is hostile or compromised
and there have been previous systems
that do this using hypervisors but they
have significantly higher overhead then
what virtual goes does so we use in only
compiler techniques to do the to enforce
the security invariants that you need
for this and there have also been
previous systems that use or more recent
systems like one called haven that use a
custom hardware in order to enforce
these kinds of policies virtual ghost
runs on commodity systems using only
compiler techniques so save anything
more about the details of these systems
just because of lack of time but if
anyone is interested I'd be more than
happy to
to talk to you more about this but the I
just want to leave you with some
examples of the kind of compiler
techniques that we use to enforce these
security policies just hopefully to
convince you that these are difficult or
or weaker to do one on native machine
code so there are a number of techniques
that we use for these security policies
that I think would be inherently much
less precise if you were doing them on
native machine code so this includes
things like call graph analysis point
analysis type inference escape analysis
identifying low-level US operations and
in enforcing a number of restrictions on
native code and then there are other
techniques that can be done effectively
on machine code but incur higher
overhead like software fault isolation
and control flow enforcement so control
integrity ok so that's all I'm going to
say about the security side of which
instruction sets here a bit about
Microsoft is spending millions of
dollars that somebody's pointed out and
still manage to ship operating systems
that have exploits in them yeah how can
we be sure that your software which
you're using to check for security is
safer than Microsoft you've got a much
smaller team you're not spending
millions of dollars you know of course
there's an exploit my how are you going
to be sure you're going to detect that
exploit yeah I think at the end of the
day you always have ways in which
vulnerabilities can be introduced the
one big benefit here is the trusted
computing base is far far smaller you're
not having to trust a complete OS you're
really only trusting the implementation
of this vos library and a few compiler
passes and the s vos library is on the
order of a few thousand lines of code
rather than a few hundreds of thousands
of lines of code and it does very simple
things so in fact it can be fully ver
the implementation of it can be fully
verified so today you people have built
special-purpose operating systems that
can be fully verified but it's very
difficult to
i trow fit that to existing commodity
systems the full verification where is
fully verifying the SPOs libraries is
relatively straightforward I think so
make sense so you effectively I think
the ability to enforce security policies
on while trusting only s vos is much
cheaper than having to do that for a
full OS QT close your eyes yes of course
all of those requirements still exist
very much yep the user the end user's
machine should have the flexibility to
enforce some other security policy other
than person who shipped the code is that
right so if Microsoft for example
compiled their operating system with
with in mind that they let somebody to
run it as fast as possible but you are
in particular place that would like to
run it you're willing to pay a high cost
for extreme security you should have the
flexibility to UM compile it and pay
forty percent overhead for so is that
kind of the benefit and how if that is
then how how would that comparative if I
just decided to be flexible enough to
ship one hundred different copies of
windows with very compiler guarantees oh
I'm not sure exactly what you're asking
but I think the point here is that if
you can if you can add a low-level layer
like as vos to the colonel so the
compiler issue setting are a bit of a
red herring here the the major benefits
really come from the abstraction layer
that s vos provides and the runtime
monitoring that s vos provides which
Microsoft themselves would have to add
to windows or the Linux developers would
have to add to Linux or whatever and
that layer is really a big part of many
of the secure so for example if you go
back to a virtual ghost right
we're an application can preserve
confidentiality and integrity without
trusting the operating system the
compiler techniques that system users
are actually really simple all it does
is the equivalent of software fault
isolation it basically just means to add
zech soars to loads and stores and add
some labels to branch targets and that's
something that can easily be done in a
compiler and then verified later but
what's crucial there was the SPOs layer
which adds a number of essential
monitoring steps for things like
manipulating page tables and that is
really where the big advantage comes
from now that advantage you're right now
what that allows end users to do is to
not have to trust the entire OS and so
if the OS is compromised they can still
enforce the security of your
applications yep everything this is that
you're saying if you take the C
programming language and you add a bunch
of restrictions to the C programming
language which increase its memory
safety and at the same time you add a
certain number of specific SVA OS
primitives which you assert are
sufficient to allow you to reimplement
an operating system in your new modified
version of C then the compiler only
features in this in the sense that it
allows you to compile this new variant
of see that you've invented and then
it's possible by example to take some
existing operating systems and
essentially port them into this new
world right and in this new world the
properties of this new modified C
language with your safety properties
provide you with a certain number of
safety property the safety properties
that you're interested in with regards
to memory safety so it's a reasonable
kind of way to restate what it's not
exactly because I so that's only sort of
half to we do not require memory safety
404s VA in general or for many of the
safety properties memory safety is one
policy that you can enforce using SVA
but you can take an operating system in
standard
at sea and ported to the s vos library
and then enforce much so other
properties like controller integrity or
what's called isolated execution which
enforces application confidentiality and
integrity without having to change the
programming language in any sense at all
so you are having you do have to apply
some compiler techniques to the
operating system itself but you don't
have to use any kind of simplified or
restricted programming language for that
if you take up and OS and see you can
apply software fault isolation and
control show integrity which are the two
things that virtual goes to also needed
for example and as long as it's been
ported to s vos you can get all of the
security properties with that we are
talking about here so make sense so
memory safety is not the is not a
requirement at all it's just one
possible policy we are not in fact using
memory safety and you can have acquired
powerful memory safety compromises and
still protect the application we should
talk more offline i can tell you some
more about what this does okay so i am
pretty much out of time here i wanted to
spend some time and automated debugging
but i think in the interest of time I
should probably wrap up right I think
that's what is the right thing to do so
let me just go to the I don't know what
slide that would be have lots of slides
automated debugging but let me just go
to
so okay I'm just going to give you one
brief slide on the performance and
portability work that we're doing and
then I will get to the end of the talk
so right now we're looking at three
kinds of performance and portability
benefits for virtual instruction sets
one is auto tuning and in particular
we're very interested in what kinds of
in how much better you could do in in
auto tuning and empirical optimization
of both corners and full applications if
you could do it on end-user machines but
do it with a rich code representation so
you could apply more sophisticated
optimizations then just need a machine
code but you could also do it for a
particular end user machine a second one
as I said for heterogeneous systems is
that today apps cannot use all the
accelerators on a mobile device without
sacrificing portability for application
specific code in particular and what
we'd like to be able to do is to enable
both using the accelerators and
preserving portability and the way we're
doing that is by extending the olivion
virtual instruction set with a couple of
abstractions of parallelism that can be
mapped down to all of the different
kinds of parallel hardware that exists
in a particular SOC and there is in fact
a wide range of different kinds of
paddle hardware that you have to be able
to target and the third or direction
we're looking at our news contexts in
which dynamic compilation could be could
give you significant benefits and so for
example vector code generation if you
could take advantage of information at
runtime could we do significant
significantly better automatic
vectorization in vector code generation
for application code so these are all
projects that are going on in our group
today and if anybody is interested any
of these I'd be very happy to talk I'd
love to talk more about that so I just
want to leave you with
um a few last thoughts about this I
think there are a few myths about
shipping code and virtual instruction
set form and I just want to see if we
can discuss them briefly so first a
common myth is that jet compilation is
in handling we inherently weaker and so
you get a performance penalty by
chipping virtual instruction sets but as
I was arguing earlier there is no
fundamental reason for this you can use
a static code generator and a
just-in-time compiler or runtime
optimization should really only be a an
opportunity to further improve
performance there should be no inherent
performance penalty here second doesn't
this arrival reverse engineering and it
is true that for that shipping of ocean
instruction set could make it easier to
reconstruct source code from the shipped
code but in fact machine code today can
be reverse engineer to reconstruct
source code and there are quite
sophisticated tools that help you do
this the real answer here is to use
proactive code obfuscation techniques to
try and obscure the behavior of the code
and in fact there are commercial tools
for languages like Java and C sharp that
do exactly this and that's the reason
why JVM and.net bytecode is routinely
used today for proprietary software a
third one is won't this kind of virtual
instruction set mod will be too
expensive for small-scale devices and at
least to a reasonable extent this is not
I think it's not really a problem for
example Android phones today already
shipped with an llvm back-end that's
used for render script and they do the
translation to llvm on the phone or
tablet the android phone or tablet and
so modern mobile devices are perfectly
powerful enough to do back end
compilation now very very small embedded
devices may not have the power or may
not be probably not the right place to
do this but in those cases you really
should be doing it on the server side
before you install the software
and the fourth one is will system
designers ever agree on a common virtual
is a but as I was saying earlier you
don't really need a universal virtual
instruction set to be able to get all
the benefits of it that I've been
talking about so oh I think that's
really all I have to say and I'm happy
to take more questions yeah and this is
one of the problems with I mean don't
already mention one of the big
serviceability problems are don't know
but one of the other one says that the
fact that if you do any kind of
different code generation for different
targets the test you know the test
matrix becomes extremely difficult as
certainly as a commercial software
vendor what you want actually you know
be in a position where you have to
support your customers who need stuff to
actually work you know and have the code
that's running on their machines is not
code that you're in a position to test
before they bought the software I mean
it's it's horrendous situation to be in
so I think this becomes a problem if the
if the backend code generator could
produce versions of software that have
bugs that were different from what the
backend code generators do during
testing effectively when you're testing
there's is it does the code generator
have bugs in it which only should also I
thought I was right yeah and separately
is does the code because of some
inherent you know difference in a
performance or whatever actually behave
incorrectly given that it is really so I
think this is a real concern and in fact
um you can test it for different
architectures ahead of time using
essentially the same execution
environment that would generate code on
the end user's machine and to the extent
that you can test on different target
hardware today you can test on different
target hardware with the versions of the
code generators that would be used later
so that would ameliorate at least the
problem to some extent
testing this kind of one statically
compiled generic flavor which everybody
uses and then if some particular piece
of hardware is sufficiently common that
the manufacturer of that piece of
hardware should work in collaboration
with the year software vendor to ensure
that the optimized version for that
back-end is also tested it stopped no no
sorry no I'm saying just that only that
so if you're shipping a particular let's
say Microsoft to say is shipping
PowerPoint and you want to test
PowerPoint for some set of architectures
some set of machines today you would
actually run on those machines in order
to test the actual binary code for those
machines yeah what's fun for me somebody
else's meet sorry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>