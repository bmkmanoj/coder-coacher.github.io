<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Blur-Kernel Estimation from Spectral Irregularities | Coder Coacher - Coaching Coders</title><meta content="Blur-Kernel Estimation from Spectral Irregularities - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Blur-Kernel Estimation from Spectral Irregularities</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vn2zwy6v87o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good afternoon everyone it's my
great pleasure to welcome renown fatale
who is visiting us this week from the
Hebrew University of Jerusalem roanoke
she got his PhD from Hebrew University
in 2006 and they needed postdoc at
Berkeley and he's been faculty there
since 2008 he's a longtime collaborator
of some of us here we had a co-author to
paper on edge preserving decompositions
in 2008 and he's also been a
collaborator of Johannes cup and he's
going to tell us today about blower
colonel estimation thank you Rick thank
you for inviting little MSL and give
this talk so i'll be talking about
deploying to work with amid goldstein
used to be a PhD student of mine he left
to the industry right in the middle okay
so as we all know in many practical
scenarios we hold the camera either on
our hand or it's on some moving vehicle
we can't fix ate it and the exposure
time is not zero so the camera sees the
motion integrated and the resulting
image is blurry which is something that
we don't like details are missing and so
on so the blurring is all about removing
this blur so the typical model that
people have been used is the following
so it's the blurry image B is the
convolution of the unblurred image sharp
image and some blur kernel that
basically tells how long the camera was
exposed a different offset that were
integrated plus some pixel independent
noise term at of X recently people have
started asking what's the scope of this
model is this an accurate model does
this M model
explains every blur that when we may see
so apparent Lancers no it is valid for
small rotations in the camera along the
x and y directions it's not valid when
those motion in the camera because if
there's motion the camera then we might
see parallax effects occlusions and also
it's not valid for the cases where we
have rotation of the camera along the
optical axis but I think that when
people hold their camera in their hand
these are the two most significant
angles that do go into account but then
again her to classic categories of image
deblurring the non blind deconvolution
we know the blur colonel we measured it
somehow some we have it somehow and all
we have to do and it's not a simple task
is to deke involve the blurry image with
the blur kernel and it's not a trivial
task simply because there is noise and
the model is not always perfectly
accurate a blind image deconvolution
says that we don't have the blur kernel
as well and we have to recover both a
bear criminal and the sharp image so now
we have this problem of undetermined
problem there are more unknowns than
equations this work and actually a few
others that i will mention kind of open
a different category where they do not
fall to either so we're estimating the
birkin all we don't have it nevertheless
we are not recovering the sharp image in
the process so it's not a part of the
algorithm so we get a blur Colonel out
of this new algorithm and now we need
some non blinded blurring in order to
produce the final image so image the
burrowing I think it will it's one of
the most researched problems in image
processing been tons of work so I think
the most dominant approaches the map
approach where the formation model of
the blurring image is transformed using
the Bayes law into the physical model x
and pyro model over the latent a sharp
image and the blur colonel people the
first attempts the people used gaussian
models for the images or their
derivatives later on people started
using the total variation a pyro at l1
more recently the max map k where
there's an integration across all the
possible images so people complete the
marginal distribution of course all the
possible images and the optimized for
the colonel in an independent manner
this requires major integrations I mean
integrations at very high dimensional
space dilip and Rob developed an
alternative normalized metric but
tackles one of the problems that were
originally in the map xk approach and
there was another acceleration of the
map k approach a different type of
methods make an explicit use of the fact
that there are sharp edges in the
original image and try to recover the
blur by the deviation of the edges that
we see in the input blurry image so Neil
has a work using the two color model to
explain edge sharp edges a jointly
recover an image that behaves like a
sharp image using some inverse diffusion
process and then try to see what's the
difference between this sharpened image
and the input image which is supposed to
be the blur colonel shirt I'll do
something
quite similar they detect edges and they
look at their profile and the userid on
transform to recover the profile itself
their works that try to employ hardware
into the process different ways of
playing with the shuttle in order to
generate some sharpness into the blur
colonel in the time integration the
coded aperture of an at levine that
allows better a spectral behavior and
also the recovery of the scale of the
blur and the acquisition of parameters
basically that generate the door colonel
motion the rotation another work of and
then there are new methods that try to
incorporate the more detailed model that
does not only account for translation a
type of blurring but also for the
rotation this is a relatively new works
they typically use the map formulation
to recover the three-dimensional blur
crevel once you add the additional
rotation so this work kind of goes into
a different path and the motivation I
think comes from the simple visual
observation on blurry images that blurry
image is in fact a superposition of
sharp image just moved around so the
image kind of resembles itself at the
various offsets because it's some same
signal just a translated and then the
question is how do we formalize that and
see what are the similarities inside the
image which would indicate the blur so
some theory there is a well-known power
law that people used to describe the
fall off of the power spectrum of
natural images here it is they were
supposed to decay something around like
the frequency to the power of two so if
we believe in this model we could do the
following take the following approach we
could take the laplacian filter for
example the five-point laplacian filter
and computer filter which is the square
root of that filter meaning that d is a
filter such that if I would convolve
deal with itself I would obtain a
laplacian filter they're actually many
such separations but there's only one
that is symmetric and since Ellie this
metric such a decomposition should exist
so d is a filter that if i would convert
it with itself i would taint the usual
five-point laplacian or any other type
of laplacian so let's see what we can do
is search filter so if i would convolve
the image with this filter and look at
the power spectrum of this signal so for
yes base will have the convolution
theorem that separates it into the image
times the differentiation filter squared
the differentiation filters filters
squared is actually the laplacian has
the same spectral response as the
laplacian and we know that laplacian
behave like omega squared every
differentiation adds multiplication in
foyer space by omega meaning that this
filter will actually whitens the
spectrum of the image okay so in case of
a blurry image we could just convolved
with this whitening filter and get this
identity missing one absolute value
bound here and there we know that those
two cancel one another so we are and
answer this one cancels this one so we
remain with the power spectrum of the
blur criminal which is like halfway
right we want to know the blur krennel
here is its power spectrum we are
missing the phase so we could use some
phase like a power method there's quite
a few of them so this is the general
direction that we would follow except
that there are some complications that
will have to deal with before so another
useful identity that we will have to use
is the whiner kinchen theorem that
relates the autocorrelation or the
Fourier transform of the autocorrelation
of a function with the absolute value
with a power spectrum of that signal
okay so autocorrelation is actually the
convolution of the signal with a
mirrored mill vision of itself which is
in force base point wise multiplication
between the signal and its conjugate so
we get the absolute value in Fourier
space it's very useful relation so this
relation basically introduces a spatial
counterpart to what we have seen before
right if we know that in four years base
were expecting to get a wide spectrum
for the image convolved without
whitening filter that would mean that
the autocorrelation of the image in
release in real space convolved with
that filter is supposed to give us the
Delta function and similarly if we're
looking at the blurry image which gave
the power spectrum of the blur colonel
now we're supposed to obtain in real
space the autocorrelation of the blur
colonel okay could see those a virtual
field anyway here we see the power
spectrum of the whitened image so it's I
convolved with gig this is the
autocorrelation in in space what we see
here is the
a gear so this is whitening the image
with the laplacian and this is the
autocorrelation that we get in space so
some people have the impression that
every differentiation filter is supposed
to whiten natural image but it's not the
case I think that you should be able to
agree that this is more constant than
this is it's not constant although we
expect it to be and we'll talk about
that too but it's more constant than
this I mean this is both these images
they seem to her with decrease as go to
their region so the was work by who at
all that actually whitened the spectrum
of the image using the laplacian rather
than square root of the laplacian and
they had very a severe biases in the
recovery of the blur criminal it's the
transfer for its power spectrum of some
image yes some natural image though
actually two of them will work with them
yeah we'll see them but I should have
shown the images by the way in one
dimensional space so if you assume
there's the blur is one-dimensional you
can basically do the same but the
differentiation can be done along that
particular direction so you could scan
all the directions and check in which
dimension the image seems to be most
blurry and take that direction and now
use a differentiation filter to deploy
the image and actually they so a
previous work by its hockey they
actually used the correct filter so it
was a 1 a first order differentiation
and this should have received the moko
act whitening although the business
issue of that phase recovery is harder
for 1d signals
signals it's not unique in 2d it's
suppose it's thought to be unique it has
to do with the decomposition of
multivariate polynomials and the some
open questions in algebra but the
assumption is that it's unique in 2d and
not unique in 1d I mean that could be
shown okay so as we have sinned there is
a but here this model predicts in
isotropic behavior in the power spectrum
of natural images right there's no
dependence on the angle but we know that
the edge content in the images do
influence the power spectrum of images
so those are the two images we've seen
earlier these are the power spectra but
of these images the non-white and ones
and we say that these functions are very
powerful being isotropic yes how
accurate is this exponent well to take
our images
calculate our spectrum
really so well maybe only large and
sample well I thought in man-made scenes
you would always expect to see these
vertical and horizontal ones so i would
expect some biases even in large and
samples but we don't have that privilege
anyway so what we did is came up with a
more refined power low and the idea was
to kind of look at the profile of these
functions at various orientations okay
and this is what we get on a log-log
scale and we see that we're actually
getting these linear functions they're
just simply shifted by constant meaning
that there is some multiplicative factor
that is different along each direction
so which explains why well sure why this
model was kind of view so so frequently
so there's some small deviation from
that so and we use this model of the
power spectrum of natural images to
model for recovery of the blur colonel
so we assume that there is some scalar C
that depends on the angle theta here
which we can recover by the coordinate
of the full year mode right it's the
inverse tangent okay so given that
observation we might need this tool so
let's look at it the foil slice theorem
gives us this relation it's the relation
between the transform failure of a
projection of a 2d signal into 1d line
with a line in Fourier space so what we
have here is we pick some direction and
we project the image just by integrating
along this direction this gives us a 1d
signal now we complete the transform for
year of this one this signal that was
integrated along this direction defined
by data and the identity tells us that
this
equals to slice just these values in
full airspace okay this can be derived
by thinking about the projection or the
integration step just as if we're
convolving the image with the function
that is delta function on one axis and a
constant function on another axis and
then we're looking on in one of the 1d
line info aspects because the others are
just constant so there's no content
there and this is what we get okay so
what we have here at the
parameterization of an eye line in a
Fourier space the way entation vector is
our Ted time this is a scalar okay so
Omega is two dimensional a coordinating
Fourier space and c is a one dimensional
coordinate in fully space so applying
this relation gives us the following
final validation so this is the window
kinchen a relation and now we have this
relation the Fourier sliced theorem so
at the end of the day we have a relation
between the transform for you of the
autocorrelation of a function after the
image is being projected to a generic
behavior info airspace and we're kind of
decoupling the sea of data variables
this way so let's just apply the same
relation over rather than the image the
blurry image and we get basically this
relation that if we take the image we
convolve it with this differential i
would first project it to 1d along some
angle we differentiate it in 1d we
complete the autocorrelation and then go
to various base then we are getting a
slice of the power spectrum info a
space-time some unknown constant but
those constants are the couple for
one another I'm working with different
projections orientations and in each one
I'm getting this unique number save data
that i do not know and i'll have to
recover so this have a real space
counterpart so here we're just
projecting the image whitening it
completing the autocorrelation and we
are expecting to see the projected
colonel it's autocorrelation time Sam
skies scalar ok so the algorithms how to
use these relations to recover the blood
colonel so i guess you can guess it by
their relations that we've seen so we
take the image we are projected white in
it complete the autocorrelation to
obtain these functions f of theta so for
each direction orientation we have f of
theta so f F theta gives us almost what
we need which is the autocorrelation of
the birkin alone on one direction or the
projected 10 a slice in four years space
but there are two numbers that we do not
know so there is the sea of data that we
don't know and we've discussed it and
there's also another quantity which is a
scalar in the dative scalar that we do
not know that results from the fact that
this differentiation a filter so it
loses the DC information of the signal
and that remains to be the case also in
the autocorrelation of a function so
there is this uncertainty between F of
theta and the two autocorrelation of
that function okay so to account for
these we have some pie or physical files
over the colonel one thing is that we
expect the burr Connor to be positive
volume that it has have a limited
support so it's not infinite and we can
bound it and we actually assume that the
user is giving us a bound and that it
has a constant Sam we can assume that
it's
21 so we show that one can reduce both
these unknowns and the constant sum to a
different unknown which describes the
extent of the support of the colonel in
each direction okay because if I know
the extent of the colonel and they know
that by that time the colonel should be
at zero I know to offset it and recover
m of theta okay and I know that it needs
to sum to 1 so i know to recover self
data so if i get f of theta and i have
these two unknowns Plus this assumption
given the extent of the colonel I'm
supposed to obtain I'm sorry both m of
data and SC of data so the algorithm
basically recovers the blur kernel and
its extent and its support these are the
three variables and it does that from
the f of taters that we compute so we
get the image we start projecting it at
different orientations I will summarize
that we differentiate in 1d we complete
the one-dimensional autocorrelation and
we get these f of theta functions for
everything now we're recovering so this
is done once okay and now we're covering
the blur kernel and its support
basically a iteratively using the
following iteration but this situation
is done only at the scale of the blur
color not at the dimension of the image
dimension of the image only affected the
computation of these one-dimensional
functions so we start with some initial
guests over s of taters and conservative
initial gas which is the point where it
shows minimal value the autocorrelation
or that we computed basically f of theta
and we estimate the colonel using the
assume support viable so we get f of
theta and as we said we can recover
self-designed m of data and obtain the
power spectrum of the colonel and now we
can use some phase retrieval algorithm
I've talked about a little about it to
recover k okay and then we given K we
re-estimate the extents of the support
of the colonel along each orientation by
computing the autocorrelation projecting
and computing build a coalition and
considering a point where the
autocorrelation has decayed enough and
we repeat this these steps iteratively
so here we can see an example so we
start with some initial guests over the
support of the colonel so this is D
theta the angle and this is the extent
of the credit that we start with and we
start these iteration step one step two
and we are covering both the colonel and
we are converging to the to support
which is a blue so this is an actual
synthetic example where we've blurred
the image with the kernel which we knew
so there's convergence both of the
support and the kernel itself and the
para que fazer table algorithms really
benefit a the most restricted prediction
that you have over the support of the
kernel that you're trying to recover so
every point where you can say it should
be zero it's a great help for the
vegetable algorithm to converge we have
someone strange
is draw a few sprawling all angles yes
right so the sum of the colonel the
two-dimensional colonel if you assume
that this is one you're basically
assuming that the free transform of the
colonel in 0 is 1 right so the
autocorrelation which is the square root
of 1 is 1 and also its project projected
versions so well basically when you
project you a colonel in 1d you maintain
its sam right so it's as what happens if
it's working model in some direction it
could be an actually to what you see
this one mode here and another here so
if you're longer direction oh so you
might have the app large body in the
colonel right large value here small and
another large value might be the case
there's no limitation here so yes we
folks over here one load another log I
mean there's no such a no you know
mortality yeah
so how do we cover phase we have the the
colonel so we can recover many kernels k
of fee so all of them have the correct
power spectrum so given the power
spectrum we can recover all these
kernels and they only differ by their
face content right so well there are
infinite solutions as we can see here is
the parameterization of these but but so
in one dimensional space one can show
that there is a finite number of
solutions when we include the this
constraint and as I said it's a thought
that in two-dimensional space and higher
there's only one unique solution but the
problem is that all the phase retrieval
algorithms out there there are not
guaranteed to converge to that right
solution and it percolates very jumpy
topology so they do converge to
different solutions so it's not the
matter of uniqueness that is the issue
but the ability to converge to the right
face that's typically well especially
when you have noise and you can't
measure you never know when you're there
or not so the typical phase recovery
that people use it's a go to inform the
70s fin up it does very simple iteration
so we start with some given power
spectrum it's f okay and what we do we
start with some initial gaze of the
function really space the kernel image
space we compute its Fourier transform
now we correct the constraints info air
which basically tells us to change its
magnitudes to the given money magnitude
but we maintain the same phase then we
go back to read space and now we apply
the spatial constraints which are the
positivity limited support we just start
setting values to 0 and clump them below
13
or maintain some integration and repeat
this process again this is a very old
strategy but it's still the state of the
art so let's summarize the algorithm so
I'm giving with a blurry image at first
I'm computing these f of theta functions
so I'm taking that image and we're
taking it projecting it by just summing
the pixels along one direction and then
I am whitening it so I'm differentiating
it in one dimension and completing the
autocorrelation and we see these 1d
function stacked here so this is the
polar coordinate and these are the
slices okay so each one of them is
multiplied by some arbitrary sylph data
that I need to recover and there's also
the missing additive em of data this is
why we see these discontinuities and so
on right and now comes the iterative
phase well we're trying to estimate mo
of the tendency of data but we're doing
it actually indirectly through the
software to the extent of the door
colonel that were updating and what
we're doing here is the following
iteration basically so this is the
iteration so we're assuming that we know
the extent of the colonel some time of
data and see if that is a subset of the
extent and given those we normalize each
of these a one-dimensional functions
right if we assume that we have s of
theta we know m of data and see of theta
so we could normalize them this is a
converged image so it looks much nicer
than this so this image differs from
this by just multiplying by a scalar and
some additive scale of each of the rows
here each line here corresponds to some
slice info airspace which we just copy
and this is our power a spectrum of this
is gives us just a power spectrum of the
blur colonel now we use a phaser table
in iterative festival to recover the
current
l from this kernel start estimating a
sub theta the compactness the size of
the colonel along each direction we go
back we renormalize and do this
repeatedly cainta we converge so one not
about the noise we assume that we in the
original blur model we have the
convolution + additive noise so the
noise is actually being reduced by this
projection operation because we have
independent nodes in each pixel and by
transferring from tlie a blur image into
these f of theta image functions we are
integrating along one direction of the
image we have n pics if we have n pixels
then while reducing the noise by 1 over
square root of n okay that is some so
now you have
is there any value to the colonel right
so in this process we never recover the
image okay well we can but I can use
some non blind deconvolution turtle damn
trip it doesn't help me yes I give it
well you could do that right and then
you can get a much more accurate
estimate of the power spectrum of the
image and then you could repeat the
whole process by modifying your
whitening with the beanie value you mean
silk theta that's really maybe it's not
exactly a power-law maybe it's something
you know it was slightly different but
you could actually measure what it was
and then you can go through your whole
process again right we'll never heard
that we never tried that what the
morning most immediate thing that I
think is just actually a recovering sea
of data that sounds like they made you
think I would have tried no we haven't
tried that yeah interesting but we were
actually in this mind setting where we
said okay no playing with the image okay
we want to go from this high dimensional
image this tiny colonel just work here
as much as we can and that's it so the
boundaries of the image is each
projection has a slightly different size
you know for a fact true where cyclic
remains right so that's is basically the
reason why we haven't kept on mentioning
the spatial counterparts I didn't really
need them I could have done everything
in free space but the problem is that
show the image is not cyclic so info i
would start seeing these biases and so
that's what made us complete the auto
correlations middle space well a lot of
very reasonable ways of dealing with
with the boundaries and so what you are
saying is that the autocorrelation i can
compute here is very why it is actually
now and as I'm rotating I have larger
one but the thing is that we need F of
theta just after the size of two times
the width of the blur colonel so we just
don't have that problem okay so
okay so input image output if we can dim
the light oh we don't do it because of
the recording guy okay able to see the
device person in the back does this I
say so I input image output image the
blink is supposed to blind you put image
output image comparison with other
methods and put our output is why do you
do that blinding where you ah I did not
prepare these slides and I forgot to
remove it I know it's very annoying it's
yeah my student here yeah here's
comparison again with other methods so I
think that in terms of quality we're
kind of meeting the state of the art
which is somewhere between Levine and
jointly input-output comparison
input-output
oh it's just a comparison of that in
books output
so the nice thing is that this power law
one over Omega squared is actually the
result of the presence of step edges in
the image I mean you could do this
exercise just take it Heaviside function
compute its Fourier transform say that
it's 1 over Omega and the square root of
that would be 1 over Omega square the
thing is that apparently also clutter
also give this spectrum and actually it
gives this spectrum from even smaller
chunks of the image so the methods so
that we have this advantage over methods
that search for well resolved edges and
do this local fitting and so on where
clutter actually reveals the for us I
mean it gives the the whitening the
property of 1 over Omega squared which
you can use for whitening and recap of
the blog and so very cluttered images of
deserts or foliage is actually playing
into our hands so output you use a small
official support I sleep specially
adapted
I think we played it that just a bit too
if the image is large enough we could
just run down go with them here here
here and recover these different blur
kernels but now you will need some
generalization of this model for the
entire image but at this point we
decided that ok why should we do it with
our algorithm I mean many algorithms can
be applied at different points in space
and then recover the third dimensional
block on or somehow but it's an
interesting direction I don't think I've
seen someone doing it which is so it's
kind of surprising yes but do they I
mean I told her I don't know the details
what a big was suggesting is applying a
translation violent algorithm in this
part in this part and this part just
sample them so you get different
two-dimensional broken nose now you have
to search for the three-dimensional
broker knows such that if you would kind
of like project it so it's not what
they've been doing they've been working
with a 3d translation exactly please
enter free yeah yeah this is what i'm
calling right so this is what i'm going
to the three-dimensional broker this has
another axis of rotation and then given
a few two-dimensional black rhinos could
you estimate the three-dimensional it's
an interesting question that neighboring
and josephs oregano
similar paper CPR on it that really do
that take these two knee estimates as a
bootstrap for finest I see some better
localize TD estimates will be able right
okay but that was in some global
optimization over the three-dimensional
colonel
okay some numbers so we compute so these
numbers are 41 megapixel image blurry
image so it takes us about two seconds
to compute all the f of theta I mean
project the image onto the different
orientations then 12 seconds we spent on
the face the table thing and the outer
iteration lots of iterations there we
are actually starting with 30 initial
guesses for the face of David's festival
is an issue and nevertheless this is
supposed to scale with the blur kernel
size and this is supposed to scale with
the image size so so it could be cases
where might be better worse and both of
them are really trivially to paralyze
because projecting the image on two
different directions can be done in
different CPUs and the phase of table
when you start from different initial
guess and just start iterating it's just
this genetic optimization algorithm
which can be done in multiple processors
we're on also the quantitative a
comparison so we used 8 images of a data
set of a th images for nature images for
man-made images it's not the images so
and we use 9 kernels from the data set
to live in a build we did not use her
images because her images are 256 x 256
pixels which is too small for our
algorithm to walk because our algorithm
kind of estimate statistics basically
right so the smallest images on which
we've been enough success support were
512 squared okay so her data set was
just too small for us and then we use
her metric which is actually asking what
is the deconvolved image that you get
from your kernel minus the tool sharp
image versus the deconvolved image that
you get from the to Colonel minus minus
the toy image and
we used dilip's hyper Laplace yawns yeah
and then we look at the function of the
images that is below epsilon if I
remember correctly so this is what we
obtain so those images you are not
familiar with we use them but we did use
the same blur blur patterns which I'm
not used and these are the result that
we get so we're in power with most of
the state of that and aspect of this
measure if you're not doing that great
here but okay a one failure case is so
what happens when we have a repetitive
pattern in the input image itself
intrinsically so this image is kind of
similar to itself in the short version
of it right so here is an blurred image
this is our result it's very rainy so I
don't have the sharp version of this
image right because I just found it on
the web but i did search for another
sharp image of the same bridge okay and
i computed the autocorrelation of that
function so that should have been a
delta function and look what we get we
get something that is different from
just because we have these periods
consistent period so that it's these
distances that if you move you get a pic
in the autocorrelation function by the
way if they were large enough in the
block amanda was smaller then we might
have a small region in which it was
sufficient for us to recover the blur
colonel but in this case it's relatively
small and this is a classic scenario
where we cannot expect our method to
work
vacation don't ask you then can you pick
up with that as well because no won't
happen to cope with this because each
has a little bit different angle right
but still at that particle I mean that
well i think that the autocorrelation
but also so this is supposed to give me
an autocorrelation along this direction
right if i move the image this direction
i'm supposed to see these additional
pics but i think that even if i move a
little bit in the orientation i would
still see so it's not like a metal
high-precision like the auto coil the
correlation that you see is substantial
I think it's not very fine lines that's
my guess and unless if the image is huge
but I don't think in practice that would
help much yeah so what assumption is it
about the image it says that it's widely
not a colonel
tough
he's just a second okay yes so if you
think about derivative images so you
have these pigs somewhere right Mellie
way to say that statistically if I saw a
pic which was basically an edge there is
no fixed distance that I would move and
on that particular fixed distance I
would see another another edge right so
we have edges in images but just two
edges that they're not always there's no
reason for two edges to be always at
this particular distance from one
another but here it happens yes I might
have horning the colonel right so but
but then again I don't know how to do it
apri okay because I don't know that was
there I mean I don't know if it was the
blood kind of it did it I don't know if
it's the real image is that say this
major ambiguity that I if you tell me
this direction is bad then maybe I could
do something but I could not do it might
be the result of the black hammer like a
weird like another half Delta Delta
Delta it could produce this yeah you
know not looking at all image for just
looking at six three stripes and it's
showing wire there were Colonel a
question miss Stoddard kind of thing
sorry so it's interesting that like that
the tree regions
because they're sort of the very similar
in your eyes there'll be nice yes but
they don't have I mean they might have
dominant scales but there have enough
variation around that yeah I know it's
kind of surprising you might think that
there's a very particular scale that
would play here right but if you look
here it's kind of different than here
and then you're integrating costs in
their image and then those here so
somehow it be yeah so let me summarize
so easily paralyzed the algorithm image
size effect only one component of the
image the projection step so the larger
the image is the lower the noise is but
I'm guessing that many other algorithms
would enjoy that and so we're not
repeatedly recovering the latent image
as the map algorithms do and therefore
were much faster than them and we are
not real angled on our ability to detect
the specific edges so for example a
multi-modal blur that Dilip mentioned
it's always how to say where was the
edge so we don't have that issue but
then this model and all the Fourier
transform that we've been using so much
requires are going to the model to be
like the convolution blur and also
cannot operate on small images just
because the statistics are not converge
and definitely not spatially varying
clever that's out of the question ok
questions
so let's go he ran it through your
algorithm one car right it's simpler
colonel that would be a more succinct
explanation to that image you have you
find some latent zakim bridge image that
didn't have a repeat of guards itself
that's what the middle adventures right
sort of its it basically hallucinates
repetitions this is a white and one yeah
the middle of interest is the result of
your cake yes right so what it does is
it elucidates repetitive structure here
in order to make these things sharper
right so or maybe not sharper but
somehow more more statistically white
yeah exactly so if I would differentiate
this image and look at the
autocorrelation that would be white yeah
so one so this image has much higher
total variation probably right if one
try to minimize that now it's a fight
between a variable kernel and the
natural image fire that would and
actually this are going for a couple it
has that right
is it a property of the signature of the
division algorithm like like with other
China think what other deeper you know
other colonel estimation algorithms
would this be a puzzle for them to
ovation finances coming was checks and
balance acquitted right like for example
the edge based techniques which I most
they won't suffer from it they as long
as it you know that found the top edges
of the pylon and focused on that would
get the right thing if it happened to
zoom in on the strings and his search
area was big enough that it actually
found this ring it would be very
convinced from that yeah but but of
course if you put in enough checks and
balances and saying it's just an
isolated thing on this image region you
know if you had some sort of a ransacked
like thing that maybe said sometimes
there is real repetition so we'll do
that in other words you know more you
you look at regions where there is a
single step better than nothing else
because one of the things that struck
you looking at your other examples it is
like our technique would have a harder
time with to look where the white tulip
is over the background of a very green
textured thing like this really soon as
local to color right here luckily the
sky and the pylon is very good yeah but
there where there was that faint little
echo white you against green textured
leaves our technique would would be
struggling
there's no free lunch so the best thing
you can do is hope that there's a glit
summer and that still isn't even that
easy your clothes usually saturated so
yeah all right that's right that's it
we blood some different intensities
ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>