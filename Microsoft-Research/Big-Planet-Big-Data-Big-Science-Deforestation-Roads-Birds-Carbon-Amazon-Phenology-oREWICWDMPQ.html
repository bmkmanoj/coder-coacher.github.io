<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Planet Big Data Big Science - Deforestation, Roads, Birds, Carbon &amp; Amazon Phenology | Coder Coacher - Coaching Coders</title><meta content="Big Planet Big Data Big Science - Deforestation, Roads, Birds, Carbon &amp; Amazon Phenology - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Planet Big Data Big Science - Deforestation, Roads, Birds, Carbon &amp; Amazon Phenology</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oREWICWDMPQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
I'll hand over to Drew so it's my great
pleasure to be introducing drew Purvis
he's the head of the computational
ecology and environmental science group
at microsoft research cambridge before
he joined microsoft research she was at
princeton and before that he got his PhD
from the University of York so she'll
hand you over to Drew okay hi everyone
can you hear me okay all right well
thanks for coming to the session I think
we've seen two great examples of
different stages of the process that we
have to go through to try to understand
and manage the planet better which is
the challenge that we're facing at the
moment so the talk we just heard showed
us how we need to gather a very rich set
of basic data about the environment
either now or in the past and there's
some you know it's great to see that
kind of really imaginative use of
unusual sources like literature and so
on so you can even go back you know
three and a half thousand years in China
which is crazy really amazing and then
young girls talk was a very good example
of a second challenge that we face in
the environmental sciences which is that
we have a lot of we have a lot of data
that is relevant to ecological processes
that govern the biosphere but we have
very little direct data on those
processes and so what yongle showed a
very good example of is how we need to
do a lot of modeling just to go from the
stuff we in the data like the satellite
observations to the things that we
really need like carbon fixation and
evapotranspiration what I wanted to
focus on today however was another part
of the process which is saying
ultimately managing their planet is
about the future the sustainability's
about the future and therefore we need
to be able to make a large set of
predictions about all different aspects
of the the earth system so for example
how agricultural systems or forests or
the oceans and so on might respond to
climate change in the future under a
business-as-usual scenario but
so use that then as a basis use those
predictions as a basis for coming up
with solutions so we can actually work
out what we can do to try to create a
sustainable future for all of us so the
focus of the lab that I head in
Cambridge and the Microsoft Research
Cambridge so it's called the
computational ecology and environmental
science group is to try to make those
kinds of predictions for various aspects
of the natural world I've got a few of
those predictions that I can show you
here I'm sorry young girl that some flex
climate didn't work for you today it was
view network issues but what we're
looking at here is a copy of fetch
climate of the fetch climate yourself is
running on as your as we heard it's one
of our prototype tools but the and then
we're just looking at that through this
web browser interface what we're looking
at there is some of our predictions for
the development of the road network in
the Amazon over the next 20 years and so
we've taken observation satellite
observations of roads at more than one
time period and we've used that to build
a kind of epidemiological like model for
the contagious spread of roads and
they're able to project that forward in
time we don't reviews a similar approach
for a deforestation so his deforestation
here and as young girl showed if you
hover over one of the points on the map
here you can see our predictions which
would be treated with some caution I'll
come back to that for for how the
deforestation would progress this
particular grid cell here's another
prediction because predictions don't
just aren't always just about going into
the future it can be prediction can be
about places that we don't have very
much information about and this is a
predicted map of the bird species
richness across the Amazon that's been
created by overlapping large numbers of
polygons describing the range of
different species you can see that we
predict that the bird species richness
is significantly higher just south of
the Amazon than is just north
and finally there's a prediction here of
the equilibrium carbon storage this
comes from terrestrial Cardinal model
that we've developed in my group
particular work of Matthew Smith and so
we're running that model to equilibrium
under the current climate and that gives
us then a map of again our estimate
which would be treated with some caution
of course of the potential carbon
storage across the Amazon
and here's another one of our
predictions that I wanted to mention too
because it also covers the Amazon which
is a postdoc Sylvia calderas predictions
for the Amazon ology of the Amazon the
leaf phonology so she built a
process-based model that describes how
trees allocate carbon to leaves in
response to variation in light levels
and soil moisture and so on and
temperature and then that enables us to
predict the leaf phonology of the Amazon
into the future the problem is with
those sorts of predictions however is
that you know the truth is it's actually
relatively easy to make a prediction I
mean you can make it verbally I could
just predict that like those seem do
plenty people at the moment they're
happy to predict the climate change
isn't going to happen or that if it does
it will be a good thing well it seems
have any qualms about that or I can if I
want I can code up a complex model for
example and make a prediction but but if
Humanity is going to actually make
important decisions about how we manage
the biosphere based on these sorts of
predictions then we need to be able to
trust those predictions we refer to this
is defensible modeling so we can imagine
as a kind of gold standard what would
happen if you had to actually stand up
in court and defend your model
predictions and so this is a picture
that I found somewhere called the
defense and it was someone defending a
model it came up on the on the web
somewhere and i've added that the idea
of a scientist defending their model now
of course this is much more than just an
idea of a gold standard it's actually
starting to happen more and more
scientists are have being called in by
government to justify their predictions
and in some cases scientists have
actually face criminal charges for
making misleading predictions and this
is a real challenge because to date in
our system modeling we've been very very
good at building interesting process
based models and we've learned a huge
amount about the world and doing that
but the truth is that a lot of the
modeling at the moment doesn't quite
reach this this gold standard of defense
ability this is I'm not saying it and
they're not defensible in a broader
sense as in people done anything wrong
exactly I mean in a more technical sense
can you actually go from your prediction
and all the way backwards
all of the steps all of the data or the
assumptions etc to and to justify your
predictions but I think the good news is
however that the philosophy for how to
do this kind of defensive role modeling
I would argue is now all in place so
what was having this picture here as we
start on the left with a couple of
familiar ideas of hypotheses and data
hypotheses in this case are likely to be
particular alternative models about how
some aspects of the world works encoded
in mathematics and software and you know
you've heard a lot about big data so the
idea is we have data relevant to our
hypotheses and were able to process that
those two things through a bayesian
inference step we might in the broadest
sense called Bayesian inference and what
comes out the other side is first and
foremost a reduced set of hypotheses
reduced set of models and what that
means is we've learned something about
the way what the world works we've been
able to reject some models and accept
others as as as good models and so we've
reduced our uncertainty and how the
world works in that way and for those
models that are still in our set we have
an estimate of the parameters of those
models so as you guys know all models
have these magic constants in alphas and
beaters and gammas and so on and were
able to actually estimate those from the
data not just the best values of those
but we always have a cloud of
uncertainty we have a distribution of
those parameters and with those two
things out our models and our parameters
were then able to make predictions with
uncertainty and that's really important
because if you talk to the decision
makers that need to use our models to
make decisions one of the first things
I'll always ask for is some estimate of
the degree of confidence that they can
attach to those predictions and this
pipeline this bayesian pipeline gives us
a very defensible way to put those kinds
of uncertainty bounds on the predictions
let's try to explain that a little bit
more because I think it's a concept but
um I mean they'll be plenty of people in
the room that know this already i go
through it very quickly but the way i
like to think of it is building through
different kinds of science so we can
imagine beginning with what i would call
forward modeling which is we have some
model structure some idea about how the
world works and then we have to supply
some parameters i'm going to presume
this structure is relatively calm
so it can't be analyzed in any pure
mathematical sense has to be simulated
and this gives us some output of some
sort and then we have and of course we
could stop there if you like but if we
but if you want to bring in the data we
then have the output in the data so what
do we do so we so the first thing we can
do is actually eyeball this and we can
buy I compare the output with the data
that's a perfectly rational thing to do
so on highly recommend doing it when you
first begin building your model and and
then by I we can decide whether we think
the model looks that model output that's
good against the data or not and it's a
natural step to start if it doesn't to
start playing around with the parameters
and all the model structure in order
that the output looks better and that's
what i would call tuning by I now this
is still you know probably to a large
extent is is the norm in a lot of areas
of our sister modeling and that's
understandable because when a model is
very very complicated you can only go
around this process so many times that's
something i'll come back to and of
course if the model is based on a sound
understanding of how the real world
actually works then then you have some
justification in making a few
modifications and then sticking with
that model and it can be very very
useful but if you are able to run your
model more times than that it's a very
natural thing to do to replace the human
eye with a computer if you provide a
measure of fit between the data and the
output then you can ask the computer to
to play around with those parameters or
the model structure many many times
that's what i would call optimization
and again that's a great thing to do but
there is a problem with optimization
which is what's your measure of fit what
does it mean for your model to actually
match reality you can't do it my hunch
anymore by I you can kind of do it by
eyeballing but the computer doesn't have
that kind of emotional reaction you need
to supply an explicit measure of it and
this is this is a real issue because
there are offer many different measures
of fit then you're going to get a
different different model out at the end
this but then what's what pet personally
what I find really exciting is is then
using a particular measure fit for the
likelihood and this then leads to
something called maximum likelihood
analysis and was there are several
things that are interesting about the
likelihood so one of them is based on
very sound nations of notions of
probability and because it's based on
probability very naturally then lend
itself to concepts like for instance if
I maximize the likelihood that i
actually have the most likely set of
parameters given my model in the data
it's a very slightly different
intellectual space than just saying the
best set of parameters to to optimize my
output but it also means that i can i
can explicitly put error bounds on
parameters because it's things that is
based on probability i can adjust
parameters until for instance my
likelihood is five percent of what it
was before kind of i'm going to splain
that in too much tea so that's kind of
the idea and that way i can put
defensible bounds on parameters and
finally often for a well-defined problem
i would argue actually all the time for
a well-defined problem the likelihood is
implied by your model in the data there
is no freedom and that's quite nice
because when you think about defending
the model you might have to explain
where your measure a fit came from
because it's just the likelihood it is
what it is and then find you then when
we go to the bayesian step the only
difference with bayesian is the idea
that we bring in some prior knowledge
and belief and that's and the bayesian
mindset is that that's okay to do that
if you really want to get your best
model of reality you don't throw away
everything you know about the world and
base it purely on your data like some
kind of robot you should actually be
confident about using your human brain
but it gives you a very explicit wave
including that that human knowledge as a
prior and the result of that is where as
before if you remember we get some
parameter distribution here for one in
red is the one that you just get from
the data if you do a maximum likelihood
like analysis and you can think of the
one in blue here is the one that you
believed beforehand
and what the Bayesian analysis does is
it compromises between those two in the
correct way if you have lots and lots of
data you'll converge on the data one if
you have no data you'll just go with
your prior belief and if you've got a if
you're kind of halfway in between your
lender is some compromise so if you
think of the Bayesian Alice has almost
been like a reasonable postdoc that does
the reasonable thing given what we know
and given the data so given this given
that that pipeline is is is all in place
philosophically why isn't it then that
every single one of the models that we
use in GCMs and agricultural
productivity predictions everything like
hydrology models etc aren't simply
pushed through this bayesian pipeline so
that all of our models in the
environment are defensible and there's a
very easy answer to that which is that
it can be really really hard sorry
that's a colleague of mine sorry nothing
i'm going to publicly ignore you but
then it can be really really hard to all
of these methods depend on running the
model many many times so that we can try
many many parameter combinations and
many models and as we heard earlier some
of the models we have at least in their
native state might take months to run
even for a single parameter set so it's
quite understandable then that we that
we have models that haven't been pursue
this pipeline however things are getting
a lot better all of the time so of
course we got more computer power than
we used to have but also there's been
there's a methodology called MCMC
sampling which is becoming more and more
widespread and is becoming more and more
efficient and what's exciting about that
so the problem is that the reason you
need so many runs is that for an
interesting model the size of the
parameter space is huge so so if you
were to do it naively you might have to
do trillions of model runs to get a
decent estimate of the parameters for
what this MCMC sampling is it's a very
clever way of exploring that multi
dimensional parameter space and so it
very very efficiently wherever you start
will find the hot spot the part of the
parameter space that's that's the most
likely and it will then explore that by
returning a set of samples from this
so-called joint posterior distribution
and the other interesting thing is if
you want to calculate the uncertainty on
your final model predictions
really easy because all you do is you
choose one of these samples that so
let's say you can do 30 runs where you
choose 30 of these samples at random and
do undo an ensemble of those 30 and just
calculate the distributions on whatever
comes out it's kind of magic really so
it's still nonetheless it's still the
case that a typical fills back but we
have our own torkoal fills back that
does MC MC ver a typical mcmc run though
for a decent sized model might still be
tens of thousands of iterations and so
you know if anyone fancies running a GC
and tens of thousands of times just to
parameterize it let me know but if we
can if I forgot that fast movie it well
and so still with at the moment we still
going to be stuck with this trade-off
between complex models that can't be
parameterised in this way but we can use
simpler models that cat that can be
parameterised through this sort of
defense for modeling pipeline and I'm
just just to demonstrate the phils back
the MCMC sampling so have this tool call
phil's about what you use in our
projects and we've put together little
web just this is just a web tutorial
type thing just just to get you familiar
with the concepts and so here you can
choose a model and read about the model
and look at the data and so on but more
importantly we can actually run the
sampler and what you'll see is this is
just running in the browser in this case
because it's a small data set and simple
model but the Edo started out here but
this thing is first of all finding the
hot spot in parameter space so we're
just looking at a particular plane
through the parameter space there but we
could we could look at a different plane
it's finding this hotspot and then once
it arrives there it kind of it never
settles down it never freezes it's a bit
like an electron it adopts a kind of
electron orbital which and its
proportions its time in proportion to
the posterior density so it's a kind of
is a piece of magic really that has
appeared it was actually invented by
someone called metropolis and someone
called Hastings a long time ago but it's
got is come to prominence recently
because it doesn't need a lot of
computing powers but what's appealing
about it is that it's an extremely
general-purpose method so for instance
in my group
we've used this to look at food webs in
ecological systems so that that's
networks of who eats what and we
specified a particular model of how we
think food webs work and then parameter
eyes that for lots of different species
and we've parameterised an
individual-based forest gap simulator at
the scale of the eastern US and use that
to simulate the by geography of us
forest and how that might respond to
climate change we've used it to simulate
individual plant growth so this is
little annual plants growing over 90
days we've used it to simulate the
response of bird species to land use
change at the scale of the whole of the
tropics we've used it to estimate the
number of remaining species of plants on
earth and in each region by actually
simulating the process of species
discovery as a function of the number of
taxonomy and the number of remaining
species to be discovered and the
efficiency of the taxonomy and one of
the biggest projects we've done to date
is we parameterize a new model of the
global terrestrial carbon cycle and in
every case we've thrown it through
exactly the same pipeline in exactly the
same tool and it's in every case we've
been able to parameterize these models
with uncertainty so that the terrestrial
carbon model is here and what it shows
is this would be happening at every
point in on the globe and carbon is
coming in here this is a this kind of an
old school model compared to a
state-of-the-art DG vm which are much
more complicated but of course the
appeal is we were able to parameterize
it against data in this way so the
carbon comes in that the carbon is fixed
through the MPP the net primary
productivity as a function of
temperature and preset that's allocated
to stems roots and leaves the plant
those things die it's a rate that
depends on the climate and that then
when they die they go down into the soil
and then the soil respiration that
eventually returns that carbon bats the
atmosphere in this fire as well the
important thing here is that then this
this stock and flow model though each of
the climate dependencies here has been
at
explicitly parameterised against data
and what these found show here is the
uncertainty on those functions the
yellow here being the data set and then
because of that we're able to make
predictions for things like the
equilibrium carbon storage across the
world but are able to attach a fully
defensible estimate of uncertainty on
those predictions and if we look for
example at the relative prediction here
is the uncertainty relative to the mean
we're able to pull out the fact that
that the uncertainty is much higher in
front of you look at the Planck carbon
is much higher at the grassland forest
boundary than it is in other places and
that's because the forest grassland band
is an area where lots of nonlinearities
in the model combine and so we're also
able to do things like discover the key
structural uncertainties in the model
and we're able to identify the fact that
of all of these different processes the
one that's the most uncertain at the
moment is whole plant mortality which
hasn't got much attention in the air
system modeling community to date and
finally we're able to make predictions
for the carbon storage of the globe out
in this case the to what 2200 we've got
the mean predictions here on the three
different structural uncertainties about
the way mortality works and for each of
those we've got the ninety-five percent
boundary on the predictions
so about this issue of complexity olney
done now but and that's the issue of
complexity then we have to one of the
things this encourages us to do is
explicitly recognized the idea of model
complexity and to illustrate that here
that these are four different pencil
sharpeners of differing levels of
complexity starting with a pen a pen
knife here one of the nice little
sharpeners one of the kind of desk based
ones and then this recalling heat we
have someone Robinson in Britain what
what do they call him in the States the
guy that does the Rube Goldberg Rube
Goldberg machine there we go for um for
sharpening pencils and so the idea here
is that we know that some statistical
theory it should be the fact that
there's an intermediate level of
complexity which is the best for making
predictions and we should be exploring
all models because we're going to be
dealing with them all in the long run
for understanding and summer when it
comes to predictions the statistical
theory is very clear saying that it
should be let a model of intermediate
complexity that makes the most accurate
predictions and so as we increase the
complexity if we put it through this
kind of pipeline I've talked about we
should always see a better fit to the
training data but the fit that to any
independent test status should go up and
then come back down and there should be
some optimal complexity here and today
we haven't really in our system modeling
set out to to seek that level of optimal
complexity but that's something that I
think we should encourage ourselves to
do the challenges however that for the
more complex models that we deal with we
actually can't put them on this chart
because we can't put them through this
pipeline at the moment but you know if I
science keeps transforming the way that
it seems to be maybe you know what will
obviously be able to deal with ever more
complex models and find this this sweet
spot of optimal complexity and of course
is our understanding increases and as we
get more and more data coming through
the model we'd expect to see that
optimal complexity move out through time
and then finally I do just want to
mention one thing which is if it's come
across in any way like I'm I'm critical
of highly complex models that are not
very highly constrained against data at
lot at least not explicitly just to
point out that we have built one of our
own so they are great fun to build their
very important and you know and I think
they do have a really important role
this is our model of global ecosystem
function we're actually simulating every
animal on earth and so how they all move
around and eat each other and live and
die and reproduce and so on it's called
the the mattingly model at the moment so
I'm just going to have a couple of
conclusions and then Kenji can let me
know whether there's time for me to give
a two minute demo distribution model
Honor I don't know because people might
be interested in seeing it but so I say
you know what we need to get to
obviously is the point when we need to
be able to collaboratively build not
predictive models of many different
aspects of the environment that many
different scales where those models and
their predictions are defensible and I
would say the philosophy is in place now
there are still some challenges though
how to deal with the idea of a
multi-component model so it's not just
one measure of complexity but you've got
different parts with different levels of
complexity and there is always this
question about long-term prediction so a
lot of issues like your your unknown
unknowns and so on come up there but
nonetheless by and large the
philosophies in place I would say the
technical pieces are mostly in place but
we could definitely do with better
systems for for instance specifying a
model you know in a more succinct way
rather than four instances fortran code
and to automate the parameter estimation
over large Suites of alternative models
and you know realistically this is still
at the moment very computationally
demanding but i'd argue perhaps that by
you know the single largest problem
holding us back from doing this kind of
work is probably sociological that we
need to really move to this this idea of
open modeling where we can lots of
different groups can propose alternative
models then we can compete them in a
kind of spirit of healthy competition
and then keep mutating the best ones and
so on and mixing them up to give new
integrated models
and of course we'd like to see feedbacks
on more feedback from the modeling to
the data gathering because in going
through this process now we can much
more explicitly identify the key
uncertainties about reality and then
that tells us what's the data that we
would like to get next and this is just
a few little tips that I would encourage
people I think and I try to remind
myself of these like for example when
you're doing modeling you know is it a
lot of pressure as an academic to come
up with a high profile PDF article that
will look good on your CV etc but if you
built a model you know how can we can
you pass it on as a model that can be
used by others as some kind of on demand
web service or at least as a nice you
know nicely commented piece of code
that's it open source or something we
can really try to use a heterogeneity of
data and prior knowledge to build the
best models think about the cost of
complexity try to understand the model
not just run it pass on the parameters
as well as the model structure and think
about this last question to which is you
know how might this model actually be
used by the people that are ultimately
charged with trying to create a
sustainable future for all of us and and
then Charlie but they have you think
I've got time for that little demo Kenji
let me know what time are we how long in
that case I was gonna we went through a
tutorial before we've got a couple of
other tools but I won't do those because
I want to make sure we've got some
discussions over Thanks okay thanks very
much true
like to invite the other two speakers up
on stage please and it's okay don't
worry about chess okay don't see so any
questions for Drew just while them young
rather than they were coming up say okay
oh yeah great
single microphones coming over to you
okay to understand in the middle Mayo
Clinic surgery
yeah okay to think for in wonderful
presentation and I very much can I say
that you know only with you right the
whole idea about you know uncertainty
modern is very important then I'm just
curious about don't have any plan or
having to try to map the uncertainty in
specially I mean can't you mad right
uncertainty to each picture in your
whole framework Thanks yeah so the end
that the maps I showed that come out of
our terrestrial carbon model that's
right you know so one thing you get is
you get your mean prediction but then
that's right you can look at the spread
over those predictions as an estimate of
the uncertainty and I think there are a
couple of real challenges though so so
one of them is that it's important we
don't get a sort of false confidence
about the uncertainty because if because
there's always a problem with the models
you didn't think of trying and the data
sets you ignored and so so you know you
can say subject to the idea that the
truth is one of these models of mine and
subject to the idea that this is the
best data then this is the uncertainty
and so you know as you know as people
talk about these unknown unknowns this
is one of the real challenges because
you know it predicts so if you look in
different areas our prediction in
machine learning for example most
predictions in machine learning is not
about predicting into the future
certainly a long way so it might be
predicting a witch ad you might click
give them what you searched for and
given the information that they have
about you it's not predicting what
you'll be doing in 40 years time but the
problem is these environmental sciences
challenges that we have large it's about
predicting quite a long way into the
future and a lot could change between
now and then that is not represented in
the data or and we hadn't even thought
of at the moment so that's one challenge
I think another is communicating
different kinds of uncertainty is really
hard you know because there are
different sorts you know you've got the
variability is different to uncertainty
as you know and so it's just it says
there's uncertainty in the variability
and so get allowing people to get a real
understanding of what it meant you know
of the uncertainty of your model in a
way that actually enables them to make a
better informed decision
that includes uncertainty is a real
challenge and you know in contrast to
many of the challenges that we face is
not really a challenge of maths or
computing it's a challenge of psychology
and visualization and philosophy or in a
bayesian philosophy if you guys have
anything that I add about uncertainty
that's very good questions so i think
the source of uncertainty is from
parameters and input data and data and
the bottle structure an ideal bjornsen
city is for the parameters right using
the MSMC then still there is another
source some solicited from the input
data and model structure but it is very
hard to qualify and also for the
uncertainty in the punctures it depends
on how to use mcmc third series of
different models and they produce
different estimates so yeah I learned
also disease very uncertain yeah yeah
that was my conclusion yeah let's see
III and I i believe it is pretty
important to for the decision-making
process oh yeah I sinking you know for
supplies the super super super eyes the
classification machine learning method
that the same post means a lot the
uncertainties of the sample that were
that were infected the models and also
for most innocent imagers actually the
different tunnels I mean that that is
different from the model modeling yeah
but for the classifications of remote
sensing imagers the type of that input
features that also means a lot to the
output yeah let's part ways so perhaps
what I said that the lies that the
philosophy was mostly in place maybe
maybe half in place or something like
that but yes yeah definitely I mean I
definitely agree with both of those
points as well it's really really hard
and it's you know if you just if you
don't bother to think about uncertainty
I suppose it's really easy its net and
the soon as you start to think about it
then kind of one thing leads to another
and you realize there's quite a lot to
it yeah
fuckin
who's the question came in so if you had
some about 10 years in on the sort of
e-science workshops and the world's you
know fast a lot so if you had a very
large amount of computing story so we
have the clouds today and I used to say
to my PhD students at the start of their
PhDs to think very freely so if you had
an infinite amount say of compute and
storage what would you do how this
proved what you do I was put us on the
spot i went first before so one VT a
sturdy now i think still there is a
steer challenge because the big data
does not locate in one place yes even if
you have first big stories but the
source of data could be scattered for
example now I'm i am working with the
nasa project and the modis land product
is located in the NASA Ames center in
the west coast and Emmas very products
is in the East Coast NASA the Carter
Center and they didn't allow to fedex
within their branch so they copied from
one place to the other place it took six
month within the NASA so big data does
not exist in single place that still so
even if we have big stories very steep
companies remains
that's a really difficult question
actually because I guess again we must
all feel like all we want more computing
power but then when you actually say
puts on the spot a bit they can chat was
quite an interesting question I mean I
think for me I suppose my I mean there
are many things you can do if we have
huge amounts of compute power one of the
things that I mean giving my talk is
that's predictable but I think it would
be really exciting to take you know the
key so each of the you know if you look
at the latest cmap 5 which is the
comparison exercise for pulp model
predictions and that is just being tied
up around now and I think there is
something like 60 models included in
that and each of those models has
several components in hydrology the
biosphere all the rest and so the number
of possible combinations of those is
huge really huge and then all of those
have lots of parameters and so you know
if I had an infinite amount of compute
maybe in principle I could if I could
get the data all together in one place
eventually then I could try every
possible combination of all of those
components with all the parameters and
somehow get the one or a few best grand
fantastic models maybe I mean it's
really so far removed I think from where
we are today but we the two challenges
being the broad compute the number of
runs of the huge but also at the moment
I don't get by and large we can't mix
and match components from different
models but but if we could do all of
that then in principle I would imagine
we could get to a real step change in
like the defense ability of our
predictions I suppose but I mean but
it's so far out in the future well I
don't want to sound pessimistic but my
understanding would be that that's
that's quite a long way out why just
described
yeah very very interesting questions
doctor I think the computer power become
the powerful and empower for that have a
lot but actually or question System
haven't been answered that we still need
more dates that more advanced algorithms
and more powerful hardware compute a
shins 22 to have to make not discover
make new discoveries but for me then
actually I just haven't used in the
cloud computing right now just like the
Jew you haven't you that the edge were
very much youryour relied on that can
help you to get a result real quickly
but for me I'm still doing that old
image processing on my computer yeah in
one single computer yea though I
hopefully yeah if it is my hope I
hopefully that will be the threshold to
learning how to use in the cloud
computing will be much easier though
that for people's want to play around
their ideas that will be much easier for
them yeah that's my hope
oh thank you what can you tell me or
what experiences do you three have on
interaction across models at different
scales of of a given situation for
instance from I'm not going to talk from
the Nano to the macro but like a plant
sized model to an ecosystem sized model
how how do you deal with the
interactions across these scales well
that's a big topic in environmental
sciences in general it's not necessarily
something you'd see lots of papers about
whatever but if you talk to people that
think about the modeling they'll often
bring up this idea that this get you
know the scaling problem really and I
think what one thing one thing I think
we can do about it is think explicitly
about these transitions so like liking
forest modeling you know you've got the
physical gas exchange processes at the
scale of the stomata then you've got
multiple needs within the canopy and
then forests and made of different trees
with different species which have
different leaf parameters and then
you've got environmental heterogeneity
at the landscape and somehow all of that
that's all within a one by one kilometer
grid cell which rich by by the standards
of most models is really fine so you're
right this is sort of you they'll always
be all all of these levels and one thing
I think that we should really bear in
mind though is that we do have data at
different scales and I think there is a
tendency for us as environmental
scientist to have a sort of scale of
choice so some people prefer to think of
them as a large scale maybe stock and
flow ecosystem model person and someone
more naturally thinks of themselves as a
leaf level eco physiologist but to get
what we should and then what we should
be doing is trying to parameterize these
models against all of the different
sources of data at different scales
simultaneously which has not really been
done very much so a really good example
is you know you can get a lot of data at
fluxx towers you know the eddy flux
towers you know younger you'd know all
about these and
so you can parameterize a lot of the
vegetation model based on those
short-term cycles that you can see but
some of the smaller terms that you can't
see there can grow through time so that
the uncertainty can become enormous and
let's say the carbon storage after 100
years but quite separately we have
forest inventory data sets where we
actually have forest stands of different
ages up to 100 years and plus when we
can invent you the total amount of
carbon and so if we combine those two
then lack then we can get the short-term
stuff on the flux towers but we can make
sure that the long-term predictions are
consistent with the corona sequences
that are coming from the inventory data
so I think there is a lot we can do to
help with it but fundamentally it's
intellectually an interesting problem as
well you guys want to comment on the
scaling I think the scaling issues with
it has long history in the pie
meteorology from 1960 1970s John
Monteith and 98 80s Graham farah and
paul jarvis in 1980s all these guys
thought about how to scale out from the
leaf to the canopy nut Grove lead to
canopy I think that study was done in
1980s and then I think with the
emergence of the room to sensing data in
1980s then people started to think how
to scale up from the kelpie to regional
to global scales like the sea running in
University of Montana and I think in
pionen diagnostic perspective I think
may work has been done but for the
prognostic perspective it is still very
challenging because to predict the
future from the leak to the global scale
it is very complex
so I think I just want to thank you for
the questions and thankful was the
speaker's here in the discussion as well
so I think have puppy waiting outside so
that thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>