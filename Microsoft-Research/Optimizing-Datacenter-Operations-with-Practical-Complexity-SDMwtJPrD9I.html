<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Optimizing Datacenter Operations with Practical Complexity | Coder Coacher - Coaching Coders</title><meta content="Optimizing Datacenter Operations with Practical Complexity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Optimizing Datacenter Operations with Practical Complexity</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SDMwtJPrD9I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello everyone it's our great pleasure
to have professor about Renly to come
and give another torque at Microsoft
Research Beltran is a let me see a bell
canada in now cheer in Computer
Engineering at the University of Toronto
he did a number of excellent works
including put network coding into you
you see engines for peer-to-peer video
delivery basically and you see is a
company in China pouch and also not only
write a large number papers he himself
write a lot of code in there his code is
of excellent quality and I have seen
some of that it's great to have a hexan
a'n't research scientist which come my
excellent research skills and coding
skills in there without further due
let's hear what Beltran has to talk
about an optimized data center operation
with practical complexity Thank You Jen
and it's always my pleasure to be here
and you know I visited Microsoft
Research quite a few times and today
what I'm gonna do is I'm going to talk
about some of my recent work jointly
done with my PhD student in his name is
Henry Hong Shi and he just graduated and
this going to be joining City University
of Hong Kong as an assistant professor
and the talk is going to be on
optimizing data center operations with
practical complexity so what I'm going
to talk about today is going a little
bit beyond optimality and this is our
recent effort of trying to do something
you know that is more than solving
problems to achieve optimal solutions so
the context of this particular talk and
our work is related to data centers and
of course these days there are lots of
these metadata center
for example this photo is from one of
the google data centers and each one of
these they're processing a lot of
requests they're doing a lot of work for
example we have you know from New York
Times some news related to the number of
web pages and the number of video views
and number of search queries they're all
in the order of billions every single
day so it's a lot of work it's done by
the data centers and what we care about
in research is related to performance so
we care about performance and related to
data centers there are two things we
care about the most one is related to
data center server utilization we want
to increase the utilization of servers
and the other is to reduce the
consumption of energy and in general to
you know reduce the operational costs
related to these data centers so if you
actually take a look at the related work
in the literature we saw a lot of papers
on optimizing costs and improving
performance of data centers and all of
these or most of these are related to
you know optimizing some of the you know
operations in the data centers to
achieve some objectives an optimization
theory is widely used in these papers
for example if we actually do a very
simple Google Scholar search related to
the keyword data center optimization we
can see an increasing trend of the the
number of papers related to data center
operation the optimization of the actual
cooperation of these data centers but we
have to watch out here we have to watch
out for something we have to watch out
for complexity so what we care about is
we care about performance but however as
we achieve additional performance we
actually move up in the actual
operational points so we are actually
try to achieve more performance by going
for more complexity in our algorithms
and the question that I have here today
is should we actually operate here where
we actually get the maximum performance
but you know a bit higher complex
or should we you know do a little bit of
trade off and try to reduce the
complexity probably by a wide margin and
without sacrificing much performance
margin so that's something that would
like to do today so here's an outline of
this particular talk so the first I'm
going to talk about within the single
data center so I'm going to talk about
the traditional problem of virtual
machine placement so I want to talk
about placing virtual machines on to
physical servers this is a traditional
problem and basically the problem is
that we have a large number of virtual
machines and we want to decide which
physical server is going to run these
virtual machines and this is in general
related to a problem in the category of
resource management related to the data
centers and the second part of the talk
probably the about one third of the time
at the end of this talk I'm going to
very briefly expand on that and try to
talk about multiple data centers with
users and trying to require a make
requests and these requests going to be
satisfied by multiple data centers and
the response will be fed back to the
users and this is typically related to
multiple geographically distributed data
centers and this is in the category of
workload management so two parts
resource management and workload
management so let's get started from
resource management first what we care
about is the traditional problem of
virtual machine placement and this is
the problem we have selected to show how
do we actually provide a better trader
of in terms of complexity versus
performance the traditional problem here
of making decisions related to vm
placement it typically has a direct
impact on the performance of
applications running on these data
centers for example if these
applications are web services then the
cpu frequency and utilization affects
the actual performance and the if these
are databases then for example the disk
i/o and the memory throughput will
affect the actual performance of these
applications so there are a lot of prior
work in this particular area and if we
actually do a for example a very simple
Google Scholar search for some of the
keywords related to vm placement and
optimality you can see a lot of papers
in this particular area but a lot of
them are using combinatorial
optimization so they're trying to solve
optimization problems to achieve optimal
solutions and the problem here is
complexity so we take a look at one of
these papers it summarizes the the
running time of different algorithms in
this particular area of vm placement and
different algorithms for about a
thousand VMs they actually try to run
for more than 15 almost achieving 30
minutes depending on the algorithm and
to us I don't think this is actually
satisfactory 4000 VMs you have to run
this algorithm for half an hour and this
is not satisfactory so the problem here
is complexity so we care about
complexity so what we wanted to do is
that we want to trade off a little bit
complexity but we don't want to you know
sacrifice too much performance but
obviously we cannot achieve optimality
so we wanted to use stable matching
theory a stable matching theory has been
developed back in 1960s by gail and
Shapley and this particular algorithm is
a very simple algorithm to achieve
stable matching it's so simple that if
we have nvm s and M servers and we
actually run the stable matching the
algorithm that actually achieve stable
matching which I'm going to talk about
briefly next with an example we actually
have all the order of n times m in terms
of the time complexity so this is a very
simple algorithm and the traditional
model of a stable matching is in the
college admissions kind of model and
this is about you know admitting
students to colleges and each college
can admit multiple students and each
student can only attend one college and
the input to the algorithm would be the
preferences so the college they have
ranking of students the student they
have ranking of colleges and the output
of the algorithm is what we call stable
matching I'm going to talk about next as
a solution concept of the actual output
so V emplacement is also a stable
matching problem in the sense that each
server can actually accommodate multiple
VMs and each VM is going to be placed at
one server so here's a toy example of
stable matching as a concept that's a
solution concept suppose on the left
hand side we have the virtual machines
and right hand side we have two servers
and each virtual machine has a ranking
of its preferences of servers and each
server has a ranking of its preferences
of the virtual machines so the college
admissions problem and the the seminole
Gail in Shapleigh paper back in the
1960s actually they just won the the
Nobel price of economics and using this
particular piece of work is to achieve
stable matching now what do we mean by
stable matching well let's consider one
match one possible matching which is to
match v 1 to s 1 V 2 to s 2 and V 3 2 s
3 this is one possible matching and we
claim that this matching is not stable
and the reason why this is not stable is
if we consider V 2 and s 3 will consider
V 2 and s 3 these are the two nodes that
if we actually try to think about v2
it's currently matched to s 2 but it
prefers s3 to s2 if we think about s3 it
currently matched to v3 but it prefers
v2 to v3 it's mapped to v3 but prefers
v2 so in general if we actually try to
establish a matching between V 2 and s 3
we actually have an unstable matching
because they each one of them they
prefer a different partner than its
current partner so this is this pair of
v2 2 s 3 in this particular example is
called a preference blocking pair and a
matching is only stable if there does
not exist the preference blocking pair
if we can find something like this it's
not a stable matching now the algorithm
is to try to achieve what it tries to
compute is a stable matching result of
of the of the matching of all of the
matching
and the algorithm is called deferred
acceptance this is proposed by Jo in
Shapleigh and here's an example of
running this algorithm so the other
algorithm is trying to say let's try to
let the virtual machines first proposed
to the servers and each one of these
virtual machines obviously in the first
iteration is going to propose to its
most preferred server so v1 proposing to
s 1 v2 proposing s1 and v3 proposing to
s3 and what we have here is we have this
first iteration and the server's is
going to choose their preferences so s1
currently receiving two proposals it's
going to choose v1 as compared to v2 so
s1 is going to reject the proposal from
v2 and accept be one and then v1 is
going to cross s1 out of its list and in
the next iteration is going to propose
to s3 now as three previously has
accepted the proposal from me three but
now it has actually two proposals here
and it's going to evaluate and try to
take a look at its preference list and
say you know v2 is better than v3 so
it's going to reject v3 accepts the
proposal from me too and now III is
going to across as three out of its list
and v3 is going to try to propose to
their to the next one on this list which
is as true and deferred acceptance is
guaranteed to be to terminate based on
the the seminal paper of gayland sharply
and if it is the simplest form of stable
matching which is one-to-one it is
proven that using deferred acceptance
unstable matching can always be found of
course there's not only one stable
matching it could be multiple of these
but one of these can always be found
using this deferred acceptance algorithm
and if we have the one too many college
admission this model is essentially the
same using deferred acceptance are a
stable matching can always be found so
in our situation we claim that classical
the classical model does not apply
what's the what's the problem in our
situation of r.e.m placement well our
problem here is that you know previously
when we talk about the
matching between VMs and servers we kind
of assume that the VMS they consume the
same amount of resources now in reality
VMs they actually require different
amount of resources in addition to that
the server's they actually can have
different capacities as well and the
size heterogeneities of both VMS and
actually servers it's a new and that
it's a difficult challenge of stable
matching theory so what we wanted to do
is that we want to adapt the algorithms
and the chief stable matching well we
first have to define what stable
matching means if we have size
heterogeneity of both VMs and servers so
VMs they require different amount of
resources and servers have different
capacities so we want to develop a new
stable matching theory with size
heterogeneity and here's an example of
what what we have yes so college student
admission
students have preferences schools that's
right so EMS why we have services in the
service so the question is why do we
have preferences for VMS to have
preferences of different servers well
the VMS they are 10 minutes basically
there are different tenants or different
applications belonging the same tenant
and you know different applications they
could have colocation preferences for
example they could have shared service
for example and you know we believe that
we should allow the applications or the
tendons to express these preferences so
not only the server's have preferences
of VMs but the virtual machines also
have preferences of the server's
together exactly so so the so the
question is the preferences can be quite
complicated so I'm going to very briefly
talk about that we have an engine
actually tries to convert you know
policies to the list of preferences and
when we have to be able to do that in
order to use this algorithm so we have
to be able to map the actual policies
for example load balancing consolidation
into the Preferences because it's it's
really you know it it's pretty flexible
to actually do that now what we have
here is that we have a new model we have
the new model with size heterogeneity
and what we have here is that different
virtual machines have different sizes
right now a virtual machine one has a
size two as compared to V and V through
v2 and v3 both of which have a size one
and then we have the capacities
different servers have different
capacities capacity to 11 so in this new
model again we could consider one kind
of matching matching v12s one both of
them have emphasized 2 and V 2 2 s 2 and
V 3 2 s 3 so that's something we could
do and then we could also think about
the typical notion of a preference
blocking pair so consider v3 and that's
one v3 currently is mapped to to the s3
and v3 actually prefers s1 more than s3
as one is currently mapped to v1 but it
prefers v3 to v1 so in this case this is
a traditional sense of preference
blocking so this is a preference
blocking pair v3 and s1 however we have
to change the definition slightly in the
sense that if he prefers as the virtual
machine prefers a server in the server
prefers a virtual machine to some of its
VMs because a server can accommodate
multiple VMs because of the different
sizes different capacities of the
servers so and by rejecting them some of
them you know the server can run this
particular virtual machine then that's
kind of the slightly revised definition
of preference blocking and we could use
that to define preference blocking pair
and if there's a preference blocking
pair obviously it's not stable it's not
a stable matching so the objective is to
find the stable matching in this new
model with size heterogeneity and
unfortunately the deferred acceptance
algorithm does not work does not work in
this particular situation so let's think
about one example here so we have in
this example four different virtual
machines the first one has a size to the
next three hat size 13 servers server 1
and server 3 hat size to server 2 has
size one and each one of them have their
preferences so let's try to run the
deferred acceptance algorithm using this
particular example in the first round
each virtual machine is going to propose
to their most preferred server so V 1 to
s 1 V 2 to s one v 3 to s to V 4 2 s 2
and now s 1 is going to take the
proposal from v1 it's going to reject
the proposal from v2 because v1 really
has a size 2 s 1 doesn't have additional
capacity run additional virtual machines
and v1 is more preferable than v2 for s1
and now after s2 rejects the proposal
from v3 and accepts the one from v4
tentatively me three crosses as two out
of its list and in the next
on v3 is going to propose to s1 let's
see what happens when v3 proposes to s1
so v3 is more preferable than v1 s1
currently has v1 v1 with a size two as
one with a size capacity of two so s1 in
this particular case it going to reject
v1 and another accept the proposal in v3
so it's a reject v1 accept v3 and by
doing so s one's capacity if you can see
increases before accepting v3 it has no
additional capacity right now because v3
has a capacity of SS the size of one s
one had one additional capacity and
because of that after additional round
these four is going to propose to s1
when v4 proposes to s1 s1 has this
additional capacity so s1 is going to
accept the proposal from v4 is it enough
septa proposal from v4 so s1 right now
we'll have both v3 and v4 so when after
s one accepts v4 you're going to see
there's a problem here the problem here
is s1 had previously rejected v2 but now
it accepted v4 previously when it
rejects v2 it's because that it does not
have additional capacity so it cannot
actually accept me too it prefers v1 to
v2 and now except before it's because
that it does have additional capacity
after it rejects v1 it takes v3 so in
that sense we do have a preference
blocking pair which is V 2 and s 1 v2
now prefers s2 s1 more than s2 and s1
prefers v2 more than v3 a more than v4
so that's the the preference blocking
pair and that's something that we don't
want to see so it's not a stable
matching after running the deferred
acceptance algorithm it only happens
with different virtual machine sizes
when we have sighs Heather virginity and
this is no good we don't want to have
this so how do we actually try to change
this well the intuition here is to
revise the deferred acceptance algorithm
whenever a virtual machine is rejected
we want to remove any less preferred VMs
from the server's preference so what we
wanted to do is that we wanted to say
you know can we actually after as one
rejects v2 it's gonna remove v4 which is
less preferred than v2 from its
preference list so if that is done after
running all of the iterations of this
deferred acceptance algorithm we can see
that you know Lee for is going to be
left unmatched after v4 is left on match
the result is still stable this is still
stable matching so the revised deferred
acceptance algorithm of crossing out the
left less preferred VMs after we reject
a particular vm he's going to reach
stability in its result of matching the
VMS to the servers but you know here
it's a kind of feels like it's kind of
strange because v4 is left on match but
it's still stable remaining a matching
is still stable so this is correct which
means that the revised deferred
acceptance is the is a correct algorithm
of achieving stable matching so we have
proved this theorem the revised stable
deferred acceptance always find a stable
matching in the same time complexity as
the original stable matching and the
stable matching can always exist in our
new model now if we take a look at the
previous example we can see there does
exist some kind of a better result of
stable matching so what do we mean by
better well a stable matching is better
than another one if each virtual machine
is at least as well off so let's take a
look at one example this particular
example here so think about v2 v2 is
currently matched to s2 but it actually
prefers s1 better so it prefers s1 and
s2 now v2 is currently match to as two
out of the river
is deferred acceptance algorithm but if
me to actually tries to be matched to s1
it's going to be a better it because v2
is better off nobody else is worse so in
that sense because s1 has currently is
mapped to v3 it has additional capacity
to actually accommodate v2 we call it
capacity blocking so it's not preference
blockings capacity blocking in the sense
that it is better off because s one does
have additional class capacity so if we
do something like that then we're trying
to say okay if we can actually make the
stable matching better in this sense of
you know using the capacity blocking as
the notion of improving it we could
achieve optimal stable matching so the
goal here is to find an optimal stable
matching that does not have capacity
blocking pairs and what we wanted to
achieve is we wanted to devise another
algorithm to achieve optimal stable
matching and this algorithm is a
multi-stage deferred acceptance and the
intuition is very simple we'd only do it
erated ly improve the stable matching by
allowing the VMS to re-propose to the
servers so we want to run multiple
stages in each one of these stages we
want to run the revised deferred
acceptance with selected VMs and servers
and the theorem here is that we can
prove is the multistage deferred
acceptance it actually finds optimal
stable matching so this is something
that we can actually try to get so what
we're doing here is that we're trying to
devise new algorithms to actually find
stable matching in this case optimal
stable matching with size heterogeneity
in the vm placement problem and what we
did is that we actually run some of the
simulations trace driven simulations to
actually try to evaluate the performance
of the result of running these
algorithms what do we mean by
performance well we actually try to
evaluate the
result of the matching and see how good
it is and we try to find out how good it
is by looking at you know the vm
priority the the application type the
server attributes we can't try to
combine these into a single performance
score based on related work 2006 in 2012
find a few papers that actually tries to
combine multiple multiple attributes and
try to in to integrate them into one
performance score what we wanted to do
is that we want to compare our deferred
acceptance the to either two different
versions multiple stage and revised
different acceptance and the optimal the
optimization and we try to say let's try
to increase the number of VMs from about
300 to about a thousand let's try to
look at the runtime and if you actually
run the optimization algorithm the
runtime will increase dramatically and
because of the linear time complexity as
compared to the number of VMs number of
servers of the actual stable matching
algorithms that we have the running time
is much better it scales much better but
what we do care is the performance so if
we actually have you know increase of
the number of VMs the actual performance
is not that much worse as compared to
optimal Aziz if we actually run optimal
an optimist could obtain an optimal
solution it's probably only about twenty
percent better than the result that we
get from the stable matching obviously
obviously this is just one set of
simulations we use traces from from the
existing papers and we have a number of
VM priorities and a small number of
application types and server attributes
it's just one instance of simulation
that is trace driven however it does
give a glimpse of the performance that
we have in terms of using a stable
matching hours and rather than the
actual optimal up optus or getting the
optimal solution and yes
questions here when you show you the
optimal algorithm um what's criteria
what is optimum so so so the so in this
particular case we're running the
optimum up optimal solution based on the
the performance score that we have so
we're trying to optimize the performance
score of combining the vm priorities the
application types and the server
attributes all this score is calculated
basically this basically server resource
and she is this vm time exactly it's
also so for any kind of matching of
matching BMS to the servers based on the
server attributes the different kind of
servers based on the the application
types of preferring different kind of
servers of preferring for example the
latency then in that case the CPU
utilization would be one possibility of
considering that and try to optimization
the algorithms trying to maximize the
aggregate of all of these doing
optimization and basically try to
calculate an open here on version um
what's the coal or isn't using opt the
reason is basically this the general
problem here is actually kind of
combinatorial optimization right right
you cannot basically afford to run a
true combinatorial
so kind of the running time is going to
be increasing but but we could still
compute because it's a small number of
VMs we can still compute the optimal
solution with with the running time
optimal on yes yes it's it's it's the
optimal solution exhaust a search
basically it's combinatorial
optimization yes yes the really is
usually incoming tutorial of Malaysians
they are other approaches for example I
mean basically in this approach I can
see you are trying to search for
deferred basically I mean basically you
are extending a prior over the news for
college admissions on this approach
right usually the other basically
approaches mean the general approaches
that first I get basically less a good
over them right let's say I say use
y'all a start they are going to look at
peers or triple pairs and try to do
swapping but it's basically is this i'm
looking at to pay off server and VMS its
current basically locations i try to see
if I flip the assignment is anyone
better off or I mean can i flip three
people they say they are I mean I'm
pretty sure you familiar with LDPC code
in these kind of literature right so so
they do such things so yeah so those are
more or less related to via migration so
so here we're looking at a simpler
problem in the sense of placement so so
we're not doing migration we're doing
initial placement so so we're not doing
migration in the sense that basement so
the cars here is this I mean you're
basically saying okay and look at your
algorithm that same pink revised dollars
come in then I try to develop a search
and try to see if I who's working on
this often care for the infants of
choose not to enter usually basically is
this you try to do it wrong let's search
another wrong but basically after I mean
basically you search through enter
basically cannot find any more this is
working you basically terminated so this
is like basically using the peacock
words of you over the head start and
then try to do gradient descent on this
compact surface in there I mean
basically it well basically I mean go to
a local uniform because that's the
definition of you cannot find any
further spot on this all I'm just
wondering basically if I understand your
point so we're where you're you're not
talking about migration so we're not
considering the cost of migration so the
cost will be zero we're just taking the
the stable matching result and try to
further improve on that so we haven't we
haven't tried that particular direction
of kind of first getting to a good
enough solution and try to see how
what's the complexity of further improve
on that so we haven't really studied
that particular direction of research so
what we did is we kind of compare with
the existing solutions in the literature
of just directly solving the
optimization so so if we actually try to
improve that it might be possible that
you know if after a smaller number of
steps you can actually increase the
performance quite a bit quite quite
dramatically i also have questions
basically are basically and
cavities I think too that probably would
drive this pot to following hmm let me
let me be shortly discuss with sure
after show on the stable matching side
but do you looking at the one resource
or multiple dimension resources and try
to match all of them everything so so in
this in this particular case we're
looking at just preferences so so so we
convert the actual resource preferences
including multiple dimensions into a
preference list so we we have to do that
there's no message also in this case we
have this one time so that's the size
and that's it that time of course if you
have multiple dimensions you're going to
have to convert and the conversion is
not precise but we have to do that
literature side whether that
no no I do not think so multiple
dimensional signal matching is not so
what we did is we also did an
implementation so we did an implement I
gave a fancy name called it anchor and
what it does is that it has a resource
monitoring system that actually monitors
the the capacity of the servers and try
to feed that resulting to the policy
manager and policy manager is basically
a conversion mechanism that converts
different policies of servers say
consolidation and load balancing into
the preference list and then the
matching engine just runs our stable
matching the deferred multi-stage
different acceptance algorithm so this
we believe is a nice approach because
it's a unified system that supports both
different placement policies was just
one matching engine so so the policy
manager can convert different policies
into the preferences but we can actually
just use one algorithm now that's the
beauty of this particular system is a
unified system for example if we
actually have the policy of
consolidation versus load balancing from
the operators side of data center
operators we could actually convert that
into different preferences and we
actually try to use those preferences
and feeding to the same matching engine
and try to get the result here's the
experimental result of allocating 10 VMs
using this real world implementation
every 30 seconds on the 20 node cluster
that we had in our lab what we that we
did is that we run it over time about
150 seconds and we can see that the
yellow dots they are consolidation using
consolidation policies which means that
we want to consolidate VMs to servers as
much as we can and the the blue dot is
use load balancing policies we don't
care about the energy cause we don't
want to turn off servers and we care
about the performance of the
applications by doing load balancing and
we can see that the number of active
service is higher than the consolidation
when we use load balancing over time
which means that really the differences
of the policies make a difference
the result of stable matching so and our
claim is the anchor as a as an
implementation of feeling different
policies into the differed acceptance
algorithm is going to be effective in
realizing these policies so in general
to summarize we have existing working
trying to provide optimal solutions
using combinatorial optimization and we
want to move back a little bit we want
to say can we actually reduce the
complexity because of the actual nature
of scalability we have to run run the
algorithm in the in the large-scale data
center we have more than thousand VMs
mapped to these data centers and try to
sacrifice some performance but hopefully
achieving much better in terms of
complexity so the second part of this
particular talk is going to be related
to workload management so workload
management is concerned with multiple
data centers and these data centers
there graphically distributed and we do
have multiple users and trying to send
requests to the status enters this is
again a traditional problem so again we
assume that we have geographically
distributed data centers in this case
this is from google the different
locations of data centers around the
world and we assume that we also have a
number of users trying to send their
requests to data centers so the
simplifying assumption here is that the
data that can be used to satisfy these
requests is replicated to in this case
this most simple case is all of the data
centers around the world so in that
sense if we actually have a user and the
user is trying to satisfy its requests
the request can be satisfied with the
combination of the responses from a
number of data centers around the world
so the result of the question that we
have here is how do we map the requests
to these data centers how do we map the
requests and this is typically called
request mapping and this is well studied
it's quite extensively studied in the
literature there's another problem and
the problem is what we call response
routing
this case what we wanted to do is that
we want to say you know we want to send
response back to the to the user after
processing these requests and how do we
send response back to the user from a
single data center so let's zoom into
one of these data centers say the one in
Seattle or Oregon what do we have here
we have this one data center and we do
have multiple isps of handling the
actual internet connection of this
particular data center we assume
multiple isps they actually have
different bandwidth they have different
latency in terms of sending these
requests out and they also have
different costs for bandwidth and in
this particular work we only consider
the different costs for bandwidth so
they have different charging models in
that sense and we have different costs
for mod a different cost for bandwidth
of course we want optimize in terms of
the cost of using bandwidth and this
problem response routing of a
multi-homed server has also been studied
in the literature so what we wanted to
take a look at is we want to take a look
at both of these problems of request
routing and a request mapping and
response routing so these two problems
they have been extensively studied all
the way back to 2004 to about 2011 in
sikkim papers and the like and the these
papers they are trying to study these
problems both problems separately
they're not trying they're not trying to
study the two problems in the same setup
however what we care about what we claim
is that these problems they're related
to each other their inherent ly related
and the intuition here is that the
request mapping from the users to the
server they determine the demand for
traffic and the demand for traffic
affects the response routing so we
because we believe that these two
problems should be studied twinkly
rather than
separately and if we actually study
these problems separately they will lead
to objectives that are misaligned and
the result the solution of solving these
problems separately will not be optimal
as compared to if we study the problems
jointly so that's the kind of the
motivation of studying both of these
problems in the same setup so we have
recently a paper Henry and myself joint
work and that studies the joint problem
request mapping and response routing in
the same problem in info come so here
are the system models first what is a
user in our model a user is a unique IP
prefix this is in reality common
practice for example it's used in neck
and I related to request traffic we
assume in our system model that the
requests are arbitrarily splittable
among the data centers this is also
common practice we can actually split
the requests using either DNS or HTTP
proxies and we assume that is the case a
little bit more problematic is the
response graphic the response traffic we
also assume that the responses will be
arbitrarily splittable among different
isp links over that is handling the
internet connection from the data center
to the internet it is not commonly the
case for bgp however we did find papers
for example there's an infocomm paper
called hashing eating for come two
thousand hashing based traffic splitting
that actually tries to study the
possibility of arbitrarily splitting the
traffic among different ISPs so here we
also try to assume that is the case
because if we if we cannot assume that
is the case we cannot handle we cannot
jointly study the problem the two
problems and finally in terms of the
time scale of running this optimization
we assume that we can run this hourly
and this is common practice in the
literature a lot of succumb papers
actually study
request mapping they run this
optimization hourly and they assume that
the near future traffic is predictable
so these are some of the system models
we actually try to assume in our result
so here's the the formulation that we
have first of all we consider the actual
mapping between the user and the data
center and the data center had multiple
ISP links multiple isps handling its
connection to the internet so we
consider a term called stop data center
and that is a tuple of data center and
isp link so different isp links will be
different stops and different data
centers will be different steps as well
we also have a user and the user user I
they actually try to send the proportion
of its requests to each one of these
stop data centers ok so the stub data
center is at the tuple of data center
and isp link and this is a proportion
between zero and one so what we wanted
to do is that we want to do this joint
request mapping and response routing so
we first consider our optimization
objective so first of all the demand of
requests so d sub I is the demand from
the user we assume this demand is
arbitrarily splittable two different
data center stops and then if we try to
minimize the cost we consider the cost
of the data center in this case we
assume that we know the energy costs
related to each one of these requests
for a particular data center stop and
that's something that we wanted to
minimize in terms of the actual
performance of using the data center to
satisfy the request we consider the
performance Reese regards to latency so
L sub IJ would be the latency of
satisfying the user eyes request using
the data center stop Jay and we assume
that we have this utility function and
this utility function is a concave
function decreasing and differentiable
so you know the longer the latency the
worst
so we want to minimize the cost in terms
of you know latency and after that the
first term is going to be the request
map it's going to be the costs related
to request mapping and what we wanted to
do is that we want to put the actual
cost in terms of the response routing
into the same system as well so what we
wanted to do is that we want to consider
the bandwidth cost of data center using
different ISPs so different stops is
going to have different bandwidth costs
related the unit of responses satisfying
the requests and this is this second
term is related response routing so if
we actually try to take the joint
consideration request mapping respond
smart routing and minimize the total
cost we actually have this entire
formulation of of a convex optimization
problem so typically we can solve this
convex optimization problem using
traditional approaches but the problem
here is that it's a large-scale problem
with respect to users because each user
is a basically IP prefix it's on the
order of 10 power 5 in terms of the
number of variables its order of 10
power 7 a number of constraints 10 power
5 related number of users so um
typically we can solve this using duty
compensation with subgradient methods
this traditional way of solving this
problem but we believe that this is
having two different drawbacks that are
traditionally associated with do
decomposition of subgradient methods
solving this problem in the distributed
fashion the first drawback is its we
have to adjust the step size using using
adaptive algorithm and this is a you
know a tricky problem of justing these
step size and then it even if we do sell
this convergence is still pretty slow so
what we wanted to do is that we want to
make it even better so we want to solve
this problem but we want to solve this
problem using a different approach so we
identify the structure of this
particular formulation then we want
solve this using a dmm alternating
Direction method of multipliers and this
is actually studied back in 1970s but we
examined in Boyds paper up in 2011 it is
basically applying to a problem with a
certain structure and that structure is
nicely observed in our problem
formulation of joint request mapping and
response routing so the idea of this
particular algorithm of a dmm is we want
to consider the each one of these
requests mapping and response routing
each one of these subproblems separately
but in the Nominating way so we want to
study each one of these separately but
we want to you know link them up in the
dual solution in each iteration so we
first solve for requests mapping and we
get the per user sub problem we solve
for that and then we also solve a
request map-request routing and that's
the / stop data center subproblems and
we actually try to update the dual
variables and then go back and try to
solve will request mapping response
routing yeah so this is the the basic
idea of a dmm this only works on it
converges successfully for a certain
structure of the problem and that
structure of the problem is observed in
our problem so here's an evaluation of
the performance we try to study this in
the trace driven simulation using
Wikipedia request races we look at some
of the empirical power and then with
prices that we are collected over the
internet and latency data using I plane
using planetlab notes also available
from the existing literature and so
using trace driven simulation what we
did is that we run this ad mmm um and we
want different lot of variations to
different variations of a DMM the first
variation is the is the traditional ATM
and gold all the way to optimality solve
it to optimality and second one the
green one is actually the one that we
run only 20 iterations rather than going
to for optimality so what
here is that we think about the cost of
the result of the solution here we can
see that the green is very similar to
the blue which means that if we run it
for 20 iterations it's about the same
performance in terms of reducing the
cost per request as compared to solving
for optimality and if we think about the
latency for each one of the users again
a situation for considering performance
with we also try to compare the two and
see that they're very similar as well so
solving IDM for only 20 iterations
actually tries to achieve about the same
result same performance as solving for
up optimality using a dmm so which means
that if we use a vm it's already faster
than subgradient in terms of then the
traditional method of using sub gradient
method and right now if we run it for 20
iterations the fixed number of
iterations is actually even better it's
even faster but it achieves very similar
performance so if you think about using
subgradient methods with a very simple
adaptive step size rule with diminishing
step size rule we compare the two
algorithms 80mm and sub gradient in
terms of the number of iterations the
cdf shows that a dmm converges much
faster so typically converges within 60
iterations and are using subgradient
methods it converges more than 60
iterations so this is much faster and
again if we just run it for 20
iterations obviously it has not
converted yet but the result will be
very close to if we solve it for
optimality so the conclusion is that for
using a DMM it has much faster
convergence and interestingly if we take
a look at this infocom 2013 paper that
we did last year later on after that
this year we saw you know more papers
done by other researchers there are two
sigmetrics papers this year coming
next month related to join request
mapping and response routing which means
that this problem is indeed an
interesting problem and it's an
important problem of getting to better
solutions and we also our selves we
studied request mapping as a problem
alone but taking into account the actual
cooling factor of data centers using the
external temperature as the empirical
traces basically we consider the energy
consumption of data centers and we have
a paper as well to appear in a
conference called ace icac and basically
these are some of the follow-up work
after our work last year related to
either request mapping itself or in
other groups joint request mapping and
response routing so to summarize in the
second part of the top we basically
tries to say existing work actually
tries to solve for optimality and the
performance is not good enough because
they consider different problems
separately and our work if we actually
try to consider requests mapping and
responsible growling jointly we can
actually achieve better performance but
we if we consider using a better
solution method of solving this optimum
optimization problem we can actually
reduce the complexity as compared to sub
gradient method in terms of the number
of iterations that we run the algorithm
so in general just to recap this entire
talk we first talked about existing work
using combinatorial algorithms
combinatorial optimization algorithms to
solve for vm placement problem and our
work is trying to say let's sacrifice
the complexity a little bit and get a
little bit less performance and
hopefully the performance trade-off is
not that dramatic and we are in our
simulations we show that the performance
is not that much better worse probably
twenty thirty percent worse as compared
to optimal solution in the instance of
vm placement problem
and then we think about possibilities of
doing work load management using
discredit optimization and if we
consider requests mapping and response
routing jointly in our info come 2013
paper we have shown that we could solve
for optimality but achieve less
complexity and better performance as
compared to considering these two
problems separately basically
considering for example request mapping
alone so that's kind of recap of the
entire talk and for any additional
questions one can check out my papers by
going to my website google my first name
to get to my website thank you
so I like it I like it better to because
that's why i spend about two third of
the time in the first half and you know
one third of the time in the second half
and in total is about 50 minutes
questions related to resource management
I think it's very rarely problems but I
mean I think as many of those will
probably discuss afterwards took the
curse
I think it's a basic probably is more
than a problem in smaller lighter
discussion right I just wondering if you
guys have any discussion discuss here
maybe you can just move our trucks sure
thank you so much for coming no problem</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>