<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation | Coder Coacher - Coaching Coders</title><meta content="Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MuBzH3nyd4A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's my pleasure today to welcome say it
amisa fith and then so it is a PhD
student for an University of Pittsburgh
and today he's working on machine
learning and data mining I'm specially
on large-scale data today's he's going
to teach us how to fast approximate
transition matrix for large NSA and this
is also related to his internship work
last year with autism ok thank you Scott
hello so uh so I'm not going to
introduce myself that's kinda already so
this Fork is actually kind of related to
my internship work last year here with
both yes and my dissertation my PhD
dissertation is about large-scale yeah
working with large-scale data sets
especially with graph based methods and
today I'm going to talk about at this
work that I actually presented last week
at UA I for basically approximating the
transition matrix for random walk on the
graphs when your data set is large
meaning the number of data points is
huge so I'm going to start with the just
a brief introduction of grab a graph
based methods and why they are useful in
machine learning and then talk about the
some challenges that we have when we
work with these methods and then I'll
present the variation of do all three
framework in general which was
originally introduced for density
estimation and then I'll talk about how
we can apply this
for estimating random walk and then some
experiments and some conclusion remarks
so why do we care about graph based
methods so usually in graph based
methods we have a bunch of data points
and data points and we can build a
similarity graph meaning that the so we
have our data points at as nodes and
edges basically show the similarity
between two nodes and this similarity
graph basically captures the geometry of
the data so it's closely related to the
distribution of data and actually there
are some works that tries to focus on
the connection between density
estimation and as the similarity graph
so the formal definition is that we
define a graph where the nodes are the
data points and the edges basically we
can have it we can have it an edge
between each to data point and if we
don't have it as the wait for that edge
is going to be 0 so the edges are
weighted and the higher the weight means
the higher the similarity between do
those data points so just to give you a
brief motivation why these methods are
important and why they help us the
reason is usually when we have a space
when we have some later point in some
space we usually have a metric in that
space and sometimes our data has some
you know weird shape you know manifold
structure meaning that the distance that
we have or the metric that we have in
that space is not meaningful locally and
globally
it is meaningful locally however when
for example these two points a and B the
Ophidian distance here is not meaningful
because what we have in mind as a
distance between a and B is this red
curve here not a straight line so the
global our distance is not necessarily
meaningful but the local in the locality
we have a meaningful distance and the
goal is to somehow use the graph of
structure build a graph and somehow
aggregate these local distances to infer
a meaningful global similarity metric or
dissimilarity distance depends on how
you define it and so that's the main
probably one of the most important
philosophies behind this frameworks so
usually we need to define a similarity
function which is basically transforming
the distance can be well here in this
talk we are talking about Arcadian
distance transforming the distance into
the similarity and here we use the very
popular Gaussian kernel which basically
has a bandwidth parameter with it will
talk about the bandwidth actually later
in this talk and how we can actually set
the bandwidth because bandwidth is a
design parameter meaning that if you
decrease your bandwidth was toward zero
you're going to end up with a very
sparse graph if you increase it you will
get a very dense graph so it's very
important where to set Sigma so so a
very
famous and very matrix that people
define is the laplacian matrix that
turns out to be very useful in basically
graph based methods the laplacian matrix
has you know we have different
operations this is probably the most
famous one on normalized laplacian
matrix so you can define this diagonal
matrix which each element is the degree
of each node and you can define your
laplacian matrix as the degree as the
diagonal matrix minus the weight matrix
all you have we have the symmetric
normalized laplacian which is a
symmetric all we have the random walk
laplacian which is basically the
identity matrix minus the random walk
transition matrix this is if you
basically write it down this term here
is basically the transition matrix of
the random walk so why do we care about
laplacian matrix in general well the
reason is the eigenvectors of this
matrix actually contains the cluster
structure of the date and basically the
eigen vectors that are associated with
smaller idea values encode the coarser
structure in the data so because it
encodes the geometry of the data the
eigenvectors of this matrix is very
important so we can use these
eigenvectors and eigenvalues to embed
our data points in a new space where the
arcadian distance is globally meaningful
and remember we talked about about the
the arcadian distance or our distance in
the input space is not globally
meaningful now we want to embed our data
set in a new space such that the
distance no matter is global or local is
meaningful
so one way to do it is this very general
form of embedding in the space so
basically for each point X i we can
define Zi as these are the coordinates
of the Zi the transformation for and
this uki is basically the ice element of
the case eigenvector and phi is a
decreasing function of the eigenvalues
so basically we can you can define
different files it's your design and
different files result in different
global this distance metric for example
if you if you define Phi as the
exponential function then you get the
diffusion distance which is the physical
diffusion gives you the physical
difference has exactly the same meaning
if you define Phi as 1 over lambda you
get the resistance distance in
electrical networks so with different
choices of files you can have different
global similarity matrix yes please on 2
so usually when people
we sing Jordan Weiss paper by they doing
spectral clustering yes there's a
crucial step after you take the
eigenvectors that you should normalize
by globalize each point so that each of
these each of a u UJ eyes has human form
and then they do canyons clustering in
that space so don't you don't do you
want to do some kind of mobilization
gear as well / / / I yes so the thing is
this UI these ID invector are assumed to
be normalized already so that's the
eigen each eigenvector here some across
all the eyes yes you know that norm is
constant I'm talking about the robe or
each I subtler ross and the eigenvectors
however many item vectors you take you
want to normalize that so that the
resulting points they they do this so
that the resulting points lie along
okay dimensional looks here yes or for
better k-means clustering yes so you can
summarize that into your fire function
in in fact yeah go ahead son so so then
the fine would depend on my right
because and normalize would be different
yes yes that's I mean five doesn't need
to be you know the same thing for if as
I said this is just one form of
transformation this is the this is based
on the paper by ghahremani that but yet
you can have different form of actually
the very very early form of spectral
clustering the Phi is actually is a step
function so you have Phi equal to 1 for
the first k eigenvectors and the rest
are 0 but the file can be at em as the
wastepaper it can be under pendant on I
its to transform it so the thing is you
have some sort of transformation here
but in order to make it an embedding the
most important thing is it should be
decreasing function in order to keep the
and by decreasing I mean and non
increasing basically because you want
the effect of them so as you go from
left to right you basically you are
moving from the coarser structure to
more finer structure so you want to keep
the coursers clusters fine
okay so so what are the applications are
so the applications I mean the first
very first application is dimensionality
reduction so if you your data already is
lying on some sort of manifold you can
basically take only a few I ghen vectors
to basically present your data in the
new space so effectively you decrease
the damage a spectral clustering you can
find non spherical clusters because the
eigen vectors represent the stretch
semi-supervised learning you can
propagate the labels from your label
point2 unlabeled data again using that
for example random walk kernel function
approximation for example you have a
reinforcement learning problem and the
state and each state is a note you can
try to basically estimate approximate
the value function of the reinforcement
for them for your reinforcement learner
on the graph which is a basic function
application so there are many
applications with the graphics matters
so what are the challenges well I'm
going in this stock we are going to
actually focus on you know large-scale
data so basically our challenge is the
larger scale eight so larger scale can
mean large dimension so what's the
challenge with the dimension well in
this like I'm not going to focus on the
dimension but this is part of the larger
scale problem actually part of my thesis
work so the problem is curse of
dimensionality you can show that if you
let your the bandwidth parameter change
with your sample size then the error
between the eigenvectors of your sample
and the eigenfunctions of the true
population exponentially depends on
and that's the curse of dimensionality a
secure error from the true
eigenfunctions of the population is
exponentially depends on language and
one way to solve this problem is using
the independence structure either you
know you are having some you are having
a data set a problem where the features
are independent or features you know are
divided into groups or you can impose
the independence with the cost that you
can you know you can have some
approximation error but at least you can
decrease the estimation error so if if
we have this independence of structure
or we impose it then we can decompose
our problem into subproblems each of
which has a reduced dimensionality
basically divide and conquer approach
the challenge that we are going to
actually talk in this stock today is the
large n when the number of data points
are huge so the first challenge larger
dimension is more of statistical
challenge this is more of a
computational challenge so in general if
you we want to bail and build these
caramel matrix and by Colonel matrix
laplacian or the random walk kernel
matrix it needs in general it needs na
square time and memory and therefore
it's not applicable for large-scale
problems so this you can we can see it
as the instance of the general embody
problem where you have you know n
particles in the space and you want to
compute the mutual effect between these
particles it's a famous problem in
physics and there are many solutions to
this problem so in the machine learning
the solutions
we have different classes of solutions
one solution is specifying the notes
this can be subsampling just take a
subsample of notes or building a
backbone graph you know making super
nodes or it can be a specification of
the edges so for example building KNN
graphs so but this is going to give you
a sparse matrix or epsilon graphs or be
matching these are off from the same
family of a specification of the edges
the third approach is basically okay try
to go around actually building this
kernel matrix so one example is if I
want to compute the eigen vector organ
function of the camera can I somehow
directly do it without building the
metrics and there are basically works
and papers on this how to do the other
idea is parameter sharing basically
instead of specification we share
parameters and our method basically
belongs to this group of parameter
sharing approaches so now I'm going to
actually talk about the variation of do
all three framework as the baseline for
our framework and please stop me
whenever things are not clear or you
have any question okay so forget about
random walk for now we are not concerned
with random walk or graph based methods
let's say we want to do kernel density
estimation for endpoints so for each
point I want to compute this kernel
density estimator and for all points it
requires me to do n square and
computations here this is this this is
basically your Gaussian kernel this is
basically the likelihood of X I
generated by this kernel and MJ is
basically the center of the cattle
and PR MJ for simplicity we can just set
it to 1 over N so the same weight for
all counts so if you look at this
problem each data point plays two roles
one as a data point X i where we want to
compute the density at the other is the
center of the Gaussian kernel m I ok so
we can go ahead and compute that but we
can actually reformulate I our problem
into a variational problem so i can come
basically define and basically this is
the logarithm of the likelihood i can
introduce the variational parameter and
use the Jensen inequality and get this
lower bound on my livelihood and this is
basically the KL divergence between my
variation on distribution and the true
distribution and this is the likelihood
of one data point I can sum over all
data points here and I can try to
maximize this lower bound with the
constraint that the sum of the Q eyes
should be one then probably cuz then
this is going if I solve this
variational problem this is going to be
my solution for q IJ so here if we look
at this this q IJ s so by P I hear the
true distribution I mean the the reverse
of this one so basically p of MJ given X
I so q IJ approximates the P of MJ given
X I and P of MJ given X I is the
membership probability so is the
probability that X I is generated by the
colonel
MJ membership probability of X I to
Colonel MJ so if we look at this
solution here so basically this is an
exact solution this is an exact solution
because the KL divergence is going to be
zero so the question is why do we bother
to do this at all I mean you can and the
answer is we'll see it later so we won
basically we want to come in we want to
compute approximate this posterior p of
MJ given X I using qij the variation
apparent and if we compute this thing
approximate this thing then we can
compute the likelihood okay and this is
this has a benefit over this one because
this one is a likelihood this one is a
probability distribution so this one
sums up to one this one doesn't so this
gives us a very easy basically a bass
amp method to to use the variation on
approximation because we can have a
constraint we can easily set up our
constraint with this one we cannot
easily set up our constructs so okay
what's the basic idea so the basic idea
is so if I want to compute the
membership probability so suppose these
are my Colonels centers out these are
guardians and this is my data point so
if I want to compute the P of MJ given x
i i need to do it for every pair here so
the idea is parameter sharing so the
idea is just group this kernel together
and just approximate the effect this
membership probability with one
parameter so here in this example we
reduce the number of parameters from
four to one
so you can see it's different these are
two different our approaches toward
reducing the number of parameters one is
the parameter sharing the other one is
sparse ification in a specification you
just zero out some of the edges you say
okay I don't look at this just assume
that that it's zero so these are two
different mentalities toward and
reducing the number of parameters our
method belongs to the parameter sharing
why for example k k nearest neighbor is
in this group okay so single three
parameter sharing meaning that okay i
have this data point and i can build a
hierarchy over my Colonels and this is a
cluster hierarchy of our Colonels so i
can just you know approximate the effect
of all of my Colonels with only one
number or i can make it a little bit
finer by going a level down one level
down and just approximated with two
numbers and so forth and the hierarchy
we assume all the hierarchies are binary
trees here but it generally can be any
hierarchy doesn't need to be binary
necessary so so if we do this well this
is a huge saving then suppose I have n
data points so basically I need to
repeat this process for all of the data
point this is just for one day to the
next idea then the next idea basically
kicks in which says okay why don't we do
the same thing on the data points as
well be the same hierarchy and let's say
we want to approximate the the effect of
all kernels on over all the Intel points
with one number of course this is a very
coarse rough as the
approximation but you can you know go
further down and again we find it but
the general idea is to use two trees so
that's it that's where the do all three
methods come kick same one for the beta
and the other one for canon and of
course here in the density estimation
the kernels and the data points are the
same set so this is going to be the same
tree so we don't actually need to build
two trees we have only 13 referring to
itself pointing to itself okay then if
this is the matrix that i compute the p
of MJ given X I the membership matrix
then I can basically say okay this is my
data Colonel this is my sorry this is my
data tree this is my kernel tree then I
can say okay I have this sub tree and
this sub tree these are two subtrees so
I want to to estimate the effect of this
sub tree over this sub tree with only
one number so I can block them together
and just represent the effect with only
one parameter one variation or parameter
qap and so then this is going to be a
blocked matrix of that transition /
sorry the membership probabilities ok
any questions over
get to this later but I'm just wondering
about the computational savings of this
approach versus the original just
computing all the N squared yet
similarity well good question we'll get
to it
look so so now suppose I give you these
threes for now you don't have to be in
the trees I give you the trees and you
you use these trees you come you come up
with blood partitioning of your metrics
then basically you can reformulate the
variational problem that we had before
using the blocks so we this is the block
partition version of the variational
optimization function that we were
talking about so before every element
was a block there was no approximation
there was no blocking now we have we
have blocks meaning that the parameters
inside each block so each block
correspond to one parameter so we can
basically just with some simple math we
can just reorganize our optimization
function and this is going to be our
lower bound lock log likelihood for the
variational parameter and for the
variation on optimization and we're
going to get back to this thing when we
talked about random walk with the
interpretation so we can actually solve
this problem try to maximize this lower
bound with the constraint that the sum
of each row should be one why is that
the constraint because we are
approximating p of MJ given X I and this
sum over all j's should be one this is
the membership probability so so we can
easily with some simple math we can
translate the constraint in terms of
blocks
yeah I'm just wondering how you
determine which sub tree falls along
that's a we get to that point yeah I
didn't I didn't talk about how to build
a tree and how to do the block
partitioning yet yeah but we get we just
assume that we have them now what we
want to do if we want to find the Q
values we want to fill the matrix so
this problem can be solved as the
previous paper Bible this problem has a
closed-form solution and the closed form
solution can be found in order of the
number of blocks and the number of
blocks is the number of parameters in
your and this is a this is a closed form
solution meaning that it's not an
iterative algorithm so it just makes the
two paths of the tree and you have your
solution okay now building a higher the
first step we assume we have the
hierarchy but this is not the case in
reality so there are many methods that
we can use for building the hierarchy
the first one is the bottom of egg
nominative clustering which takes order
of n s square we can use KD trees we can
use cover trees we can use ball trees we
can use anchor trees so for obvious
reasons we want to avoid the first one
because it's order of n s square this is
kind of antithesis to our motivation so
but except for this one the rest of them
can we you can use any of them but we'll
talk about it later in this talk that
well this is a very crucial step in the
whole framework and all of these methods
although they have you know these very
neat orders you know when you look at
them it's it's linear but what is this
see
I mean there are very controversial I
mean depends it really depends on the
structure of your data this can be as
high as in a square I mentioned the game
recover the cover trees yes it's
basically went on yet yes it's true to
the power a power of your intrinsic
dimensionality but given that you're
interesting language sentence is low if
it is not then you're in trouble or with
the KD trees depends on dimension anchor
trees have their own problem but here in
this I mean in this paper that i
represent i use anchor trees this is the
framework that actually we use last
summer in my internship in anchor trees
but you can replace it with any tree
anymore so just give a very quick
demonstration of anchor tree so you have
n data points you build a square root of
and anchors and i don't get into the
details but you can think about each
anchor as a clock cluster then you merge
this them into a you know in the agri-
way because it's any square root of them
so the merging is going to take you
linear or n square root of n times
square root of n then basically you
recursively repeat the process for each
anchor so the construction time in the
original paper they said it's n log n
but we did some analysis and in the
worst case it can be large as but it
still still it's less than an a square
and the worst case by worst case I mean
there is no structure in the data all
the data points are equidistant from
each other in the space so knowing so
the interesting dimensionality is the
same
original dimensionality so there is no
charge okay any question in this part
before we move on to the brandon walker
search okay okay now random walk okay
what's the connection between the random
walk on the graph and this basically
then see a variational method that we
talked about so the original plot
problem the original optimization
problem without blocking the Ori so we
wanted to maximize this lower bound
likelihood even that the sum of the Q
values are equal to 1 this is the
original a variation of power and if you
solve this problem this is your result
this is qi j and this this is your and
the basically the error is 0 the exact
but if you look at this thing look at
this term what is this this is the
Gaussian kernel right Gaussian kernel is
the same this is the similarity between
XIM MJ and this is the normalization
this is what you do you actually if you
want to compute the random walk you
compute the similarity you just divide
it by and you know and normalize it so
basically the Q matrix can be seen as
the approximate the blood partition
approximation of the transition matrix
so the p matrix is basically ease their
transition matrix and the Q matrix is
just a block partition approximation so
what is the interpretation then in this
new context this new view of the Q
matrix we have a new interpretation for
our blocked blocked version of our
optimization function
and this is our block version of the
lower borlock look like you the first
term is just a normalization and it's
constant in terms of Q however the
second term this is a this is a term
that people use when they want to learn
similarity on graphs meaning that if I
want to maximize this and this there's a
negative sign here so I want to assign
small if the distance between two blocks
sorry two clusters a and B is huge I
want to assign smaller Q values so the
higher the distance the lower the
similarity this is a very common
tournament people want to learn the
similarity this term tends to connect
basically each node to his closest
neighbor with probability 1 q1 and
disconnect it from the rest of the notes
with Q equal to 0 find the closest
neighbor and connect to that with all
power with all q what is this term this
term if we look at it this is actually
the sum of the entropies of the outgoing
probabilities from all know this is just
a Shannon entropy of the distribution
this one tends to because if we want to
maximize this entropy here therefore we
need to have a uniform distribution to
maximum for maximization of the entropy
so this one tends to connect each node
to all of the nodes with equal
probability so these two terms here work
against each other so you can look at
this term the entropy term as a
regularization term because if we don't
use this term then we are going to end
up with a disk disconnected graph every
node is just connected to its our
closest neighbor
and this coefficient here basically
adjust the trade-off between these two
terms and if we look at this term this
is actually the bandwidth and this
basically as a sanity check basically
the higher sorry the lower the bandwidth
you can see that your graph will be
sparser because this term becomes a
stronger and the higher the Sigma this
term becomes a stronger so that your
graph will be denser so this random walk
view gives us this new interpretation of
the objective function that what it does
exactly okay any question sorry your
lower bound compare in the transmission
quality to cannon with an added small
uniform
connection to all men
it sounds like this is what mm-hmm yes
so what we didn't perform that
experiment we did compare it with K&amp;amp;N
because we wanted to compare these two
different ideas of a specification and
parameter sharing but so the what you're
saying is basically instead of using you
know sparse matrix and use epsilon
instead of 0 in K&amp;amp;N yeah we didn't
do that experiment that framework
because we wanted to compare you know
sharing versus a specification and that
is already a sharing idea but again it
depends how you implement your cannon
because if if you wanted to relate your
KNM to likelihood you need to wait the
edges so KNN by itself is not enough you
need to wait them with the similarity
the Gaussian similarity then you can
show that as you increase K you
basically you converge to the exact
method it is yes yes yes I mean depend
again depends how you implement I mean
if even if you I mean if you want to
implement the very boot force K&amp;amp;N you'll
have problem in larger scale because you
cannot because both force K&amp;amp;N again for
the construction of K&amp;amp;N graph it's NS
square I even actually more than n
square and n Square log in because you
need the sorting and you want to avoid
that so you want to use again you want
to use the tree for even for KNN and in
our experiment we did that basically we
use the 34 k NN and but i wonder it so
in order to solve for the hawk you
the method folks meant that has the puck
city of
besides the largest block here and the
number of blocks that a number of large
space is not the size of Lux just a
number of blocks be the number of your
parameters right yeah and I assume in
the your K&amp;amp;N well the thing is these are
you know all independent problems so as
soon as you give me a partitioning no
matter how you get that partitioning you
get the partitioning using KNN or using
our method the rest of it is just
solving for Q is the same yeah but now
I'm wondering whether you can replace
the step where you solve for the optimal
q with this approximation regions where
you pick the maybe the approximate top k
and then you know set a large weight
from that for cute and then small
epsilon wait
yes that's that's definite I mean we
didn't do it but that's definitely a
valid method to do because you can
actually instead of actually computing p
of MJ given X I as you mentioned you can
just compute directly p of x j given the
likelihood just compute the likelihood
for the first k and just take and just
take the average or something the thing
is that becomes very ad hoc how do you
how you combine it to one parameter how
to share it here we have a very uniform
all method to to do it connected but yet
that's that can be okay so so this
parameter here this bandwidth basically
are adjust the trade-off between these
two terms now the question is how to
adjust this Sigma there are many methods
I mean in literature there are many
heuristics how to do it and we are not
claiming that our method is the best one
way to do it but at least it has some
good interpretation what it means
actually so as we turn said before the
bandwidth basically adjust the decay
rate of your similarity as a function of
distance and so if we look at our again
this is as I said we use this formula
and this objective over and over in the
slides so this is our objective this is
the black version of our lower bound or
objective this is if you look at this
term this is a quasi concave function of
bandwidth this term is constant in terms
of bandwidth but these two terms it
requires a concave function abandoned
uh okay so maybe oh my bad sorry I
actually have actually I put it in the
slides so concave function you have one
maximum there but in quasi concave you
still have one maximum but it's not the
second derivative can be positive then
yes yes yes exactly so this means that
in terms of the bandwidth or objective
function have one maximum and we can
find that maximum in close for if we
solve this thing for Sigma this is our
solution and if we look at this solution
this also has it has a nice
interpretation if we look at this term
here this is just so if you take out the
dimension out from this without this
diamond if you take the dimensional this
is basically the average expected
distance that the random Walker
traversed after one time step so this so
this one is basically the the nominator
is just to expect a distance that the
random Walker the sum of the expected
distances and when you normalize it by n
is the it's just the average expected
distance that you traverse and before
you ask me this question I'm going to
answer it right now that well here
how come I mean because intuitively as
we decrease Sigma as we decrease
bandwidth are likely who should go up so
there is no limit on it right as we go
towards zero unlike you who goes to
infinity becomes larger and larger but
the catch here is that our objective
function is not the log-likelihood
itself it's the lower bound on the log
like you and this lower bound gives us
this nice interpretation are very
straightforward method to find the
signal and question you can use the same
kind of idea but not necessarily the
same but same in the same so in the same
light you can say okay in this threat in
the exact method no Blackie nothing just
the exact method I can always you know
use the gentle inequality and this is
going to be a lower bound on my Latium
using Jetson then I can always try to
maximize this lower bound in terms of
bandwidth and I get this basically
optimal solution for Sigma because this
is again a quasi concave but this is not
a special case of or block partition
version and the reason is that basically
this is not a tight bond anyway but our
bound as you increase the number of
parameters as you refine it you get
closer and closer to the actual
likelihood but this one is just the
heuristic just uh but we use this
heuristic in our experiment to set the
Sigma for the exact method
okay fast multiplication so in many
applications if Q is basic you is my
transition matrix I need to compute the
multiplication of Q by an arbitrary
vector Y and this in general this is a
NS square operation a matrix times a
vector we want to do this of course
efficiently so we have developed this
simple algorithm under on our tree that
basically you can feed all the elements
of Y to their corresponding leaves in
the tree and as a first step there's a
collector step that you basically some
up on your children and you just keep
this statistic at each node which is
basically the sum of the un' the
children so this this is that basically
takes linear order of n then using it's
a statistic we can have a distribute
down step where you compute along each
path from the root to their leaves you
compute this sum and you can basically
use dynamic programming to save the sons
save the summation at each time step and
you can easily show that this is
basically order of the number of blocks
in your tree so the whole process takes
the number of blocks because number of
blocks as we see later is always greater
than greater or equal than n so this
gives us a very fast algorithm to do the
multiplication by an arbitrary vector
and why is this important because in
many applications like labor propagation
in labor propagation
if this is your initial labels for some
label data the propagation in in the
limit you have this vector of neighbors
for all for the whole graph so this is
this involves the inversion of this
matrix but if you don't want to do the
inversion you can approximate it with a
final number of iterations using this
process here and in this process you
need to do the multiplication of Q by a
vector over time or another application
is the eigen decomposition of Q you want
to find the eigen vectors of Q which
which we know by and we know why by now
and if we use power method or unruly
iteration we need to compute this
multiplication iteratively these
multiplications or by a vector so this
gives us a very fast algorithm
multiplication algorithm for these
methods now to your question complex
computational complexity so so so this
is the computational complexity for
Arman the construction time in the worst
case if you don't have any structure in
your anchor tree this takes you know for
anchor tree to make an anchor tree but
in average because we always have some
sort of structure this is n log n plus
the number of blocks to basically this
is the estimation of the Q values the
multiplication time is order of the
number of blocks so as we see here
number of blocks plays a very crucial
rule in the complexity of the whole
framework so everything depends on the
number of blocks or the number of prints
so the question is what is this number
of blocks and how we can actually change
it or
fix it so the number of blocks actually
we can show that it it changes between
the linear order and NS square at NS
square basically you get the exact
method no approximation so I think what
are the courses level the number of
blocks is 2 times n minus 1 that's the
minimum number of blocks that you can
have not less than this value and we'll
see why this value so the courses
Neville of approximation so to get a
valid block partitioning the sub trees
that we pair them together to form a
block needs to be non overlapping if two
subtrees if two sub Club two clusters
are overlapping to have the same points
the approximation is not a valid
approximation so we need the subtrees to
be over a non-overlapping but for any
given subtree in the tree the largest
non-overlapping subtree is its sibling
so for this subtree the this subtree is
a non overlapping subtree right but the
largest non-overlapping is just its
sibling yeah so therefore the courses
level of approximation in the courses
level we just pair each sub tree with
its sibling as here so the number of
blocks then is going to be the number of
internal nodes which is n minus 1 times
2 because we have two directions so it's
going to be 2 times n minus 1 this is
the minimum number of luck so a rational
approach is to start with the courses
level of approximation and refine our
model as we need more accuracy or as we
have more computational power to afford
its basic so and there what does
refinement me one step of refinement is
mean means that splitting your block
into two I
horizontally or vertically and what does
it this mean so if this is one sorry so
if these are two clusters and this arc
represent a block so i pair this 2 and
this is a block so i can even refine it
this arc this way with pointing this to
the children of B this is a vertical
refinement or horizontal the refinement
from children of A to B so these are two
different refinement that I can have so
every time that I split a block I
introduce a new parameter so i have a
new parameter here and so of course by
introducing a new parameter I relax the
constraint meaning that then the lower
bound on the likely under and the log
likelihood is going to be higher because
I basically I I basically remove one of
the constraints so you can show that
always you increase the log-likelihood
by refinement and mathematically is easy
to see it because you can always assign
the previous parameters in your new
optimization problem so now the question
is which block to split so of course all
of them will gives us some gain in terms
of likelihood so the very obvious answer
is that ok pick the one that gives me
the most gain in terms of likelihood
that is going to be like that and that
is going to be the blog that I want to
split but to find this block I need to
split all the blocks one at a time solve
the optimization compute the
log-likelihood gain and take the maximum
but this is very expensive we don't want
to do this for all the blocks
so our solution here is basically we try
to locally solve the optimization for
each possible split we assume that all
other parameters are fixed so we every
time we just solve a local optimization
problem and this local optimization
problem takes order of one computation
so it doesn't take order of a number of
blocks then we pick the split with the
maximum local gain of course this result
is suboptimal but we can very
efficiently implement it using the
priority to kick so is this clear any
question we can move to experiments all
right so in the experiment we try to
solve this semi-supervised learning
problem so given a small set of data
points we want to find the label given a
small set of sorry label data points we
want to find the label for the rest of
unlabeled data points using label
propagation on the graph so this is the
labor propagation and we don't want to
compute the inverse of the this matrix
so we do this this thing iteratively so
the performance metrics that we measure
our construction time propagation time
or basic propagation is the
multiplication time and classification
accuracy given that we know the true
labels for the cash limits
excuse me okay and the baselines the
very first base line is the exact method
basically we compute the similarity
between each pair then we normalize the
similarity to find the transition matrix
transition probability the other is the
fast carry nearest neighbor so as I
mentioned before we don't want to use
the straight kenya's neighbor because
the construction time for the exact way
nearest neighbor is large so we want to
use again the trees to find the
k-nearest neighbors so we use the same
anchor tree for k-nearest neighbor so in
for the fur comparison we use the same
cluster hierarchy that we use for both
approaches so for both of them is anchor
tree and just too I don't want to get
into details but how how the three use
and helps k nearest neighbor is
basically it's you can cut some
computations in the tree if you already
know the k-nearest neighbor so that that
helps you to cut the computations that's
why it makes the k-nearest neighbor fast
and these are the tail reticle orders on
the paper for these three methods the
exact fast k NN and our framework which
variation of do all three framework and
this is the construction time memory
that it uses and the multiplication time
so okay in the first experiment we want
to basically the goal is to compute the
construction I measure the construction
construction time as the size of the
problem is increasing and the size of
the problem is basically the number of
data points and then the the other goal
is basically to measure the accuracy see
how much we lose in terms of accuracy if
we do the approximation
and the data said that we use is sec str
it's a standard benchmark in some I
supervised learning and dimensions is
315 the number of data points we
increase it that's why I didn't mention
it here because it's going to change and
there are two classes ten percent of the
data points are labeled for each problem
size and we try to find the labels for
the rest of nineteen ninety percent so
the bandwidth for each method is
computed separately using the techniques
that we talked already and the
approximation level for our framework is
going to be at the courses level where
the number of blocks is 2 times n minus
1 and for the KNN for the k-nearest
neighbor we define the approximation
level as the number of as k so k 2 is
the roughest approximation level with
fast carryin an approach so we use k
equals to 2 and these are the results so
the time and the size are in log escape
here and these are the three methods and
this is the classification accuracy so
as we see we didn't actually lose that
so the red is basically the exact method
we didn't lose that much in terms of
accuracy however we gained order of
magnitudes in terms of construction
compared to two other methods and in
terms of multiplication we are much
better than of course the exact method
and comparable to the KNN because as
soon as you build a K&amp;amp;N you have a
sparse matrix and the multiplication can
be very fast for K in it the question is
how to but still we
how to build a cannon and that can be as
slow as we showed it here okay so much
time okay okay second experiment we want
to study the effect of refinement in the
model so the goal is to compute the
accuracy as the model is being refined
and of course measure the refinement
time how much time we need to refine
each model again the data set is a is a
benchmark data set digit 1 which has
1,500 data points and 241 dimensions so
we have two results here one time when
we have 10 label data points and the
other time when we had 100 late label
data points these are the standard in
the data in the data set and again the
bandwidth we computed you using the
method that we explained before as I
just mention it the refinement for the
k-nearest neighbor is defined as
increasing k because as you increase k
and assume that each edge in the K&amp;amp;N is
weighted with the gaussian similarity
then you can show that as k becomes n
approaches and your method basically
converges to the exact method basic this
makes KN consist a consistent our
approximation approximator of the exact
method so and if you look at here and
because we want to hear the we compute
the effect of refinement so we want to
make sure that for both methods the
number of parameters is the same so we
want to make sure that the number of
blocks in
our framework is equal to K times n at
each time step if we do this then the
number of parameters in book because the
K&amp;amp;N method has K times and parameters so
for fair comparison we want to have the
same number of parameters for both
methods so this is the result this is
the construction time again here and
this is a log scale I don't know why I
well I think I crop the figure so the
numbers are not here but this is order
of magnitude of course faster and this
is the refinement time our method
compared to K a net and this is the
result of refinement so this this axis
here is the number of parameters here so
as we increase the number of parameters
here we show that our method gets better
compared to K &amp;amp; N this is when the
sample when the label sample sorry the
the size of the label data is 10 this is
where in the size of the label data is
100 but for the 100 as we see here k NN
basically beats our method and off also
the exact method and our explanation for
this result is that this data must have
a very clear-cut manifold such that k NN
can actually because Kanan his parse can
catch it very fast and but in the other
one when we have a smaller label data k
NN didn't actually it was very variant
it couldn't actually get be improved but
our method improved as we increase the
number of parameters of course if we
increase the number of parameters more
we're going to have more improvement but
that's always a computational issue
that's the trade-off we want to stop at
some point because otherwise
the computational complexity is going to
be high so next one ok the next
experiment is okay so far all the
experiments that we did arm was you know
on kind of a medium sized data set here
we want to actually see whether our
method is really scalable for really
large scale data sets and the data set
that we chose as one is called alpha
these are from the Pascal larger scale
data a large scale data challenge you
can actually download these data sets
this is like a half a million data
points the other one is 3.5 million data
points and pretty much high-dimensional
as well so we we didn't compare it to
other methods because we couldn't run
other methods on these data sets because
it took forever for under med school and
also the memory issue but we did the
experiments for our method with at the
courses level and give this is the
result so so here is just just we want
to show the computational complexity it
takes like for the first data set four
and a half hours to build a model and 11
minutes for propagation for
multiplication basically for the other
data said it takes on almost two days to
build a graph on three-point half a
million data points which which
translates into half a sorry seven
million parameters and about 93.3
minutes to do the propagation so so
these are just these are all on the
serial computer
of course we can make it even faster if
we paralyze the whole framework and this
is possible because the underlying data
structure that we use and is is all
based on trees so we can always
decompose the streets on different
machines and do the computations you
know in pilot and make it even faster
but these are the results on the serial
computer and I think that was it if if
there's no question from experiments I
can move quickly to conclusions okay so
as we showed I mean in in average the
construction time can be as low as n log
n using our framework instead of an ax
square and n log n is basically for the
building the tree if you have the tree
it's going to be linear actually the
multiplication can be as low as the
linear because the number of blocks can
be as long as the linear time so you can
have a very fast multiplication and and
you can you and multiplication is very
useful for labor propagation and eigen
decomposition memory usage can be again
as low as the linear order because the
number of blocks can be as long as the
linear order and the framework provides
us a straightforward method to find the
optimal bandwidth with the nice
interpretation and also the the whole
framework is a multi-level approximation
framework meaning that we can have the
approximation at different levels and we
can refine our model in on demand
basically how much accuracy Vinnie and
also I mean the other the other from the
other point of view you can have a
maximum you can have a maximum CPU
resource you said okay this is my
maximum CPU resource gives me the best
refinement
so this is the maximum number of blocks
that I can afford give me the best
refinement for the matrix and we
developed this technique to find the
best actually suboptimal blocks to
partition and as I'm said before the
framework is not dependent on the choice
of tree that we use for cluster
hierarchy therefore we can easily
substitute this tree with some trees
that have some theoretical guarantees I
we couldn't find any work on you know
for some theoretical guarantees for
anchor trees but for example if you use
cover trees covered trees have some
theoretical guarantees however as I said
all of these three methods they give you
a bond and they give you a order with
some constant in it and that constant
can kill you in practice because that
constant depends on the dimension on
that many factors under basically on the
geometry of the data so in theory yes
you can improve the order if you for
example replace their anchor tree with
cover tree or basically you can have
some theoretical guarantees but in order
to assure that theoretical guarantee
your computational complexity that
constant that shows up in the
computational complexity can kill you
and yes
and I think that was it thank you any
question this competition terrible yes
so
this is the so what Pradesh quality more
power-efficient is the communication
time the affection so now Alan casio
history
yes but so i'm not sure case here they
commit any time is that you tribute and
you don't have to worry about that
well I mean I'm not very I'm not an
expert in parallel computing but all I
can say that in this framework all the
computations are done hierarchically so
if you're if you're doing the let's say
the computation at the courses level
right so this the left subtree is going
to be independent of the right subtree
and so you can cut and then again this
recursively goes down and but yet there
is some you know overhead for
communication but if you can keep the
communication inside its sub tree in a
recursive fashion then probably you can
I do not decrease them the the
communication time to the order of n
basically with some constant of course
because the number so it's going to be
there the number of internal nodes is
the number of pairs you know siblings
and the number of internal nodes is
basically n is n minus 1 something so of
course there is some constant out and
that probably depends on
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>