<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical and computational trade-offs in estimation of sparse principal components. | Coder Coacher - Coaching Coders</title><meta content="Statistical and computational trade-offs in estimation of sparse principal components. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical and computational trade-offs in estimation of sparse principal components.</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zBvqPLRtZ_M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thank you for the kind invitation to be
here I feel like a little bit of an
interloper I wouldn't necessarily
describe myself as a machine machine
learning i'm probably bit more of a sort
of mainstream citation but i hope what I
have to say has some interest to people
who are injured who do consider
themselves to be machine learners so I'm
going to be speaking about some very
recent work in fact this whole area is
very recent recently emerging area I
think it's quite an exciting one and I
hope that maybe there are some people
here who will think of other problems to
which they might apply these sorts of
techniques so what I'm going to be
speaking about is joint work with my PhD
student tenure wall and Quentin birthday
who finished his PhD in Princeton at the
end of last year he's currently a
postdoc at Caltech and he's kind of
coming to Cambridge as a lecturer from
this summer ok so the question I want to
ask is it is there some sort of
fundamental trade-off between
statistical and computational efficiency
so could it be the case that the only
way that we can be satisfied because
we're doing well we can see that our
algorithm has good statistical
performance is if we have really
prepared to wait an awful long time to
get the answer so this seems like a kind
of interesting important problem but
it's it's hard to know how you might
want to formulate it in a rigorous sort
of fashion and so what the way we're
going to be thinking about this in this
talk is to think could it be that no
randomized polynomial time algorithm can
attain what we'll think of as the gold
standard the typical gold standard for
statistical estimation problems namely
the minimax optimal rate okay so as I
said this is a recently emerging area
and over the course of last year or two
they've been several papers that have
talked about these sorts of ideas in
different sorts of ways I won't go
through them all but but you'll see that
there's quite a wide variety of
different sort of
Mystikal problems where people are
starting to realize that there may be
some sort of fundamental trade-off
between statistical and computational
efficiency and one of the things that I
find attractive about this area is that
it's forming new links between
statistics and what had historically
been somewhat distinct discipline of
theoretical computer science ok so the
context in which I'm going to be
thinking about these statistical and
computational trade-offs is is one that
should be fairly familiar to most people
here are few which is to think about
principal component analysis so this is
one of the most widely used techniques
in statistics for the dimension
reduction and we're going to just recall
how it works so let's assume that we've
got multivariate data at least two
dimensional data and i'm going to write
curly p if the class of all
distributions probability distributions
p that have mean 0 and a finite
covariance matrix okay so i'll write in
decreasing order the the eigen values of
p of the covariance matrix of P in this
way notice that I've got a a strict
inequality for the first one here so I'm
assuming that there's an eigen gap
between the leading eigen value in the
next biggest eigen value and then what
we call the principal components or the
first principal component would be a
unit length I can vector corresponding
to the largest eigen value okay so all
my eigen vectors in this talk we're
going to be unit length hi convectors
and if we've got a solvent of data from
p then we can try to estimate this first
principal component using in the obvious
way using the top eigen vector or the
the eigenvector corresponding to the
largest eigen value of the sample
covariance matrix and because i'm
assuming this distributions got mean
zero i'm not bothering to subtract the
the sample mean here okay and what may
be reasonably well known but perhaps not
universally known is that principle
component analysis really breaks down in
high dimensional problems so what do i
mean by this well let's think about
setting up a loss function so
so let's introduce the angle between the
two vectors this is the acute angle but
between unit vectors U and V as denoted
by theta and then L the loss function is
just there going to be the sine of that
angle and I can write it in different
ways in terms of the inner product of
these two vectors or in terms of the
Frobenius norm that the entry wise to
norm of these matrices uu transpose and
V V transpose okay and to understand
this breakdown in very simple context
let's suppose that I my distribution is
just a standard p variate normal
distribution 0 mean and where the
covariance matrix is got this sort of
spite structure so it's it's not just
the isotropic identity matrix but we've
got this theta which is a measure of the
the signal-to-noise ratio and then V V
transpose is this rank 1 perturbation of
the identity matrix here so that V here
is the principal component okay and I'm
going to consider an asymptotic regime
where the dimension increases with the
sample size in such a way that their
ratio converges to a constant and then
we're seeing that the squared loss
function converges almost surely to a
quantity which is is sort of quite
straightforward to write down but is it
a bit disturbing because it's telling me
that if my signal to noise ratio is it
isn't particularly big then the
estimated principal component and the
true principle combined are actually
orthogonal to each other asymptotically
so there's no information whatsoever
about the first principal component in
the sample okay unless in this sense so
that's a little bit disturbing what do
we do when our standard methods break
down in high dimensions we add the word
sparse in front and see if we can rework
it right so let's think about sparse
principal component analysis so this was
introduced in a paper about ten years
ago now and it's designed to remedy
there's this worrying situation and to
aid interpreted
so if you've got a sparse principal
component only a relatively small number
of nonzero entries it should help you to
to interpret the projection that you're
going to do of your multivariate data on
to the principal components so we're
going to assume that the this principle
component belongs to the case bias unit
ball in p dimensions that's the set of
vectors that have got a most k nonzero
entries another of unit length and
really a lot of work has been done i was
got a bit shocked when i looked at the
literature i found 11 different
estimators of v1 of p that have been
proposed in the literature I'd not sure
that there are so many other estimators
that you could think of in other sorts
of problems but but they fall into
different categories some of them are
designed to work computational reason so
they're designed to be quick and others
of them are designed to have good
minimax properties over particularly
either Gaussian alors sub great Gaussian
classes and we're going to be
introducing our own classes to study to
study sparse pca and we're going to
depend on a particular condition which
we call the restricted covariance
concentration condition it's a little
bit involved but not too bad it just
says that if i look at this directional
variance and its sample counterpart i'm
asking for this this directional
variance to concentrate around its
population version well if i didn't have
ass up here this is just sort of
standard parametric theory you would
tell of you that like this should convey
convergent rate 1 over root n but I want
this to be uniform in all sparse
directions this is somehow exactly the
right condition that you want in order
for just but sparse pca methods to work
well this is sort of some of you might
recognize this as a Bernstein type of
concentration condition and then a
little bit of notation I'll just write
I'll remove the N from from this this
are CCP NLC if it satisfies a restricted
covariance concentration condition for
all sample sizes and and if it's all
satisfied it for all sparsity levels L
then I'll remove the
as well and just so you get a little bit
more intuition for what this condition
means it's pretty straightforward to
prove that every sub Gaussian
distribution satisfies all of the
restricted covariance concentration
conditions just with a with a constant
shift okay a multiplicative constant
shift okay so what do we know about
these minimax rates over these are CC
classes well the actual class of
distributions I want to work with is a
class of distributions that satisfy
restricted covariance concentration
conditions at two different sparsity
levels that's 2 and 2 K and the leading
eigenvector the pro first principal
component is case part and I've got this
eigen gap which is at least theta so I'm
saying the constants at least one that's
a scaling argument tells me that I can
do that and then this theta is is a sort
of measure again of the signal-to-noise
ratio in my problem and how difficult is
this problem well in a sort of fairly
wide regime of parameter values there's
a minimax lower bound so a bound on how
well any statistical procedure can do
and the most important aspect of this
bound is this not don't worry too much
about the ugly constant it's a square
root of K log p over n theta squared
okay that's what we think of as the
minimax rate of convergence with respect
to or at least the minimax lower bound
as I'm presenting it here the minimax
rate of convergence for this sparse PT a
problem well that's a lower bound on how
well any statistical procedure could
perform now i'm going to introduce a
specific statistical procedure and it's
the most obvious one you could ever
think of i'm going to compute well right
this is a sog max that it's a non
standard notation sog max just means the
smallest element of the org max in in
the lexicographic ordering so it's just
making a definitive choice of which
element to choose and so if I'm telling
you that the first principal component
is is case passed its natural to try to
maximize this sort of quadratic form
over all K sparse unit vectors okay
and if we if we have that this
definition then we're getting an upper
bound on the supremum risk of this
estimator that's of exactly the same
water so square root of K log p over n
theta squared and most of the time and
statistics would say right the problem
solve give up go home okay but the point
I want to make is that if we actually
want to compute this estimator it's an
absolute disaster because we need to
search through all the K by K sub
matrices of this P by P matrix Sigma hat
so we've got peaches k different
sparsity patterns we need to consider
it's absolutely hopeless super
polynomial time algorithm so as an
alternative as a more computable
algorithm we're just going to rewrite
this optimization problem we just move
the move write this as its trace move
the u transpose across and then I can
write it as a maximum over this class of
matrices m belonging to the set of major
trees that have got trace 1 Rank 1 and
at most K squared nonzero entries and
this is called a semi definite
relaxation type of estimator it's a
fairly standard thing to do in these
sorts of problems so here's a
description of formal description of the
algorithm I just compute the sample
covariance matrix and I look for an
epsilon Maximizer of this of this not
the l0 penalty penalized version but the
l1 penalized version so we're looking
we're relaxing that to make a sort of
convex optimization problem and i
compute these semi definite programming
estimator like this well its
computational complexity isn't great
it's still the worst case of these two
things but at least it's polynomial time
and for us this is this means computable
in some in some sense but if we look at
its risk bound that we get here this is
a specific instance of the algorithm
with a specifically chosen value of
lambda and epsilon then the risk bound
has not too bad a constant but all of a
sudden we've gone from the square root
of K log p over n theta squared to
2k squared log p over n 32 squared so
we're fundamentally losing something
compared with the minimax rate of
convergence and at this point you might
say well is it just that my algorithm is
bad or is it that there is some
fundamental limitation about algorithms
that can be computed in polynomial time
and so the way that these are that has
emerged to try to to show that there are
these fundamental trade-offs between
statistical and computational efficiency
is to try to reduce problems to problems
that are known to be hard in some sense
in the theoretical computer science
literature and one such problem is
called the planted clique problem so
here we're thinking of blackboard bulgy
g.m being the set of all undirected
graphs with n vertices and we put a
planted click in there so we choose
kappa the vertices we join all the edges
between those kappa vertices and we join
all the other vertices with probability
half independently the question is can
we locate the planted click quickly so
here we go here's my my graph and here's
a planted click in here and this is the
adjacency matrix and here I've
conveniently highlighted the the squares
corresponding to the klieg but in
practice you don't get them highlighted
but otherwise it'd be quite easy to
determine which was with rusev logically
so you just see these and they're not
even just a block that could be right
spread out okay and you still got to try
and locate them so what's known well the
if you've got a clique of size at least
two long two bass two of em then the
plan to click is asymptotically almost
surely the unique largest click if
you've got a really large click sighs I
bigger than constant times the square
root of M log M then it's really easy to
identify the planted click you just have
to look at the the degrees of the
vertices so there's an order N squared
algorithm just compute the the degree of
each vertex and take the Kappa that the
K largest vertex largest degree vertices
and they're the ones in your planted
click well you can do a bit better if
you work really hard in fact
so Alana tell came up with an algorithm
spectral basis it's not exactly it's
sort of a bit clever it's not just a
standard spectral based method that says
you can do this in c times the square
root of m if you've got kappa being at
these c times the square root of m for
NEC but you need to be a little bit
clever to get down to it for NEC and and
there are several our several sort of
results that say well you shouldn't be
able to do it if if you've got kappa
being smaller than the square root of m
ok so here's the assumption that i want
to make the the statistical group and
computational trade-off I'll say exist
if we're willing to assume there's this
this hypothesis from theoretical
computer science which I claim is a sort
of widely believed hypothesis and the
evidence for that was on the previous
slide so I'm saying let's suppose that
for any kappa smaller than m to the beta
with peter strictly smaller than half
there's no randomized polynomial time
algorithm then that can correctly
identify the plan to click with
probability tending to one okay and this
is a relatively standard assumption in
several problems from theoretical
computer science if we're willing to
make that assumption then here is the
computational lower bound it says that
again in a particular parametric regime
that butBut index by any Alpha in 01
it's not possible or let me say that the
other way around every sequence of
estimators that I can compute in
randomized polynomial time has a rate of
convergence that's worse than K to the 1
plus alpha log p over n theta squared so
that's saying that up to sub polynomial
factors in k my semi definite
programming estimator actually attained
the optimal rate of minimax optimal rate
of convergence among those that can be
computed in polynomial time and there is
this fundamental difference between the
rates that you can get if you don't
place any computation
constraints or on the procedure and the
rates that can be attained if you insist
that a procedure can be computed in in
polynomial time so just to quickly wrap
up so we introduced some class new
classes of distributions for studying
pasta palace PCA under this restricted
covariance concentration condition and
we got the minimax rates of convergence
but found that the minimax upward bound
could only be attained by a procedure
that you could never actually compute on
even moderately large datasets in
practice and that if you are willing to
assume this planted click hypothesis or
assumption then these these rates of
convergence you can get with randomized
polynomial time algorithms unnecessarily
worse so with that I'll stop there and
thank you very tan she think about
whether I should interpret your results
as a negative or positive results I mean
in a sense it's a negative result
because you're saying that with these
computational limits you're not going to
get you know you're you're not going to
get as good as wrong as you would yeah
without the limits but you're also if I
got it right you're saying that the stp
method is going to be as good as its
that essentially essentially as good as
you can get with the polynomial time
algorithm so maybe that's a sort of
positive recommendation yeah I guess you
could see it but both of those a valid
statements like that I think III don't
think our main purpose was really to say
the s dps is what you should be using I
think that the the more surprising
result is that there is this this
fundament your this fundamental
limitation in how well any computable
estimator can do so in that sense maybe
the negative that ways of positive but
but we're describing how things are
rather than trying to place a judgment
on them yeah
I have a look into the theoretical curve
you sandwich my work like a case of
conclusion that from from the practical
perspective knowing something has no
ethic rats has no for the polynomial
randomized approximation scheme is its
kind of practically useful because it
stops you from trying to find the
solution but knowing it's polynomial
time you still might actually be
practically not very useful yeah well I
mean even this out algorithm I presented
it's not great is it p to the fifth you
know it's not it's not the sort of order
you'd really like to have for an
algorithm you'd want to advocate and
implement and and maybe there are more
refined results that one could look for
in terms of the polynomial complexity
that that would be really interesting
but I think that's a little way off so
far yeah each year microsoft research
helps hundreds of influential speakers
from around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>