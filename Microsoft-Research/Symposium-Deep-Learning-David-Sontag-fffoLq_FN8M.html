<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - David Sontag | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - David Sontag - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - David Sontag</b></h2><h5 class="post__date">2016-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fffoLq_FN8M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so
with David Sontag who did his PhD at MIT
with Tommy jaakkola he's currently an
assistant professor at nyu's Koran
Institute and center for data science so
david has made many fundamental
contributions in the field of
approximate entrance in graphical models
he's currently interested in
applications of machine learning to
health care and David received many much
recognition for his work already such as
best thesis award best paper awards at
mnl uai and nips and as NSF Career Award
so that's welcome David all right so
this is joint work with yuna kim in your
senior night and my colleague sasha rush
from harvard so ostensibly this is going
to be a talk about language modeling but
really behind the scenes I want you to
be thinking about representation
learning and i hope to inspire you to
think about new ways to try to do
representation learning so language
modeling is looking at really if you
have a sentence of length 10 how can we
try to talk about a distribution of such
sentences and language modeling is very
important for many downstream tasks such
as speech recognition machine
translation and text generation so any
distribution could be written using the
chain rule as this factorized form shown
up there and the simplest approach to
language model might be used might be
for example an n-gram language model
which says that we're going to look at
the conditional of the likelihood of
what a single word given all the
previous words and approximate that as a
simple as a simple distribution
condition on the last n words and now as
n gets larger and larger the ability to
estimate such distributions get worse
and worse and what needs to be smoothing
techniques for engram language models to
actually be effective in practice now
our work is going to be in the world of
neuro language models which takes a
slightly different tack at this problem
rather than representing words as a one
hot distribute representation we
represent words using a word embedding
and then one attempt to learn a neural
network in particular one takes a
composition function G a vector-valued
composition function G and attempts to
learn a softmax distribution where to
predict the probe
the probability of a word J is given to
as the exponential a dot product of a
output word embedding PJ dot product
with this composition function applied
to all the previous words to the word
embeddings of all the previous words now
they're many ways to try to implement
such an approach one of the simplest
would be a feed-forward neural language
model which takes the past few words I
comes up with a warden embeddings for
each of them then it composes them some
ways such as doing a projection into
some hidden space and finally takes a
softmax of that in order to do the
prediction of what the next words going
to be a slightly more library approach
is based on recurrent neural networks
where that hidden state h now the
function of all the previous words
rather than just the past few words so
that's done by having some nonlinear
function f which takes as input the new
word XT and this hidden state from the
previous time step h t minus 1 and then
produces a new hidden state for the
current time step HT which is then used
together with a softmax to predict the
current word so here's the picture of
their current no network setup we've got
all of the previous words so far we get
the word embeddings those word
embeddings are then used together with
the hidden state from the previous time
step to compute the hidden state of the
current time step and find out that
hidden stated then used to predict the
current word so essential to the success
of neural language models has been the
use of word embeddings and weird
embeddings has been shown time and time
again to to find another interesting
linguistic structure in particular
similar words tend to be similar tend to
be close together and the learned vector
space now the natural language
processing community has become really
excited about neural language models
because they simply work really well so
if you were to take for example that
count based n-gram model and look at a
data set drive from the penn treebank a
measure of held out likelihood called
perplexity where you want a lower
numbers a better number will be
something like one
40 for those older approaches whereas
the most recent neural language
model-based approaches can get
perplexities which are dramatically
smaller this is led to a lot of interest
in the NLP community on language
modeling now all of these previous
approaches use word not all of them I'll
talk about some exceptions later but the
vast majority of them use word
information at a word unit level and one
of the key challenges with such a thing
is that if you're to look for example at
the log frequency of of how often every
word appears in the corpora there are a
number of words which are very rare and
which might be very difficult to
estimate the corresponding word
embeddings from the data but they share
a number of similarities with previous
words that might be much more frequent
for example here you see that the suffix
ing occurs frequently throughout the
corpora if you look at prefixes as well
and all and many of these neural
language model approaches are going to
be ignoring the fact that there are
these similarities between words now
because of that it makes it very
difficult to learn learner language
models with rare words and in this work
what we looking at is how can one try to
leverage sub word information for
language modeling and from my
perspective as the introduction
mentioned I'm very interested in
healthcare and in health care we have
text which looks a lot worse than even
Twitter posts has a lot of words which
have misspellings in them and does miss
fellows might occur only once or a
couple of times in your core brass so
such an issue is really important to how
to solve so there have been a couple of
different approaches to try to tackle
this and the most common approach is to
use a morphological segment of words and
then to try to build up a word embedding
from that segmentation so for example
here you see the word unfortunately the
prefix is on the stem is fortunate and
the suffix is Li 11 approached by long
at all is to use a recursive neural
network to try to build up a word
embedding from the corresponding
morphological parse a second approach
from both and blunts them from a year
now
go attempts to learn an embedding for
each different morpheme and then to add
together those embeddings for the more
fumes in a in a word together with by
the way adding and embedding for the
word itself so the number of parameters
in those models are actually even more
than number of parameters and typical
neural language model but they do in
essence smooth away the issue of very
infrequent words by the more fumes
playing a bigger role in our work we're
going to try to take the inspiration
from these more theological approaches
but to you to try to learn a model that
works directly on the characters
themselves and this is why I'm trying to
draw an analogy to representation
learning because we're going to be
learning in essence the notion of more
themes as part of this approach so we
take inspiration from convolutional
neural networks and we're going to use
convolutional networks within the bottom
layer of our approach so here's the
overall network architecture this tells
the whole entire story on the bottom you
see the sentence that we're trying to
model and we're going to be using a
recurrent neural network at the very top
to model the likelihood of the next word
given the words that you've seen so far
now in order to represent a single word
we have a convolutional neural network
which I'll describe this in the next
several slides which takes that the
characters of that word and output a
representation of that word which is you
could think of as a constructed word
embedding for that word then we pass
that through a highway network to give
the input to the lstm recurrent neural
network instead of the word embeddings
which would have typically been used so
let's dive down and look at that
character level model so the first thing
that would do is will represent the word
as this matrix C forward of length L
where we have a mate we have each column
corresponding to the character
representation and we use a 15
dimensional representation for every
character and then we have these
convolutional filters H the goal the the
the convolutional network will first
apply convolution between the
matric implantation of the word see and
each filter age to produce a for every
character location it produces this this
constant F I which we then use a max
over time to summarize into a single
number for that filter so here's a
pictorial representation of it that's
the matrix for the word here's the the
filter for a filter with three we're
going to use filters on many different
widths from one up to seven we summarize
that word by taking convolution of that
filter with the with that matrix as we
move one letter at a time down
summarizing it by taking the dot product
of that filter with the sub matrix for
that that's that sub sequence we move
all the way down and this is now a
summarization of the filter applied to
that haunt our word we then take the
maximum of that haunt our vector to
summarize the effect of that filter with
a single number which in this case is 0
point 7 and we might apply a
non-linearity to that afterward
intuitively might think of each filter
is trying to pick out some Engram from
the from the word but and then the final
number is is somehow representative of
whether that engram appears anywhere in
the word so one might then do that for a
different filter and you and here's an
example of a filter of with two instead
of three and you're going to then
concatenate the output across all the
filters of all the different widths to
get one representation of that word and
that's the analogy of the word embedding
that we use one can then apply the
non-linearity to that before giving you
this input to the lstm so using this
very simple approach if you were to try
to apply it now to a to this this penn
treebank data set and if you were to try
to fix the number of parameters of the
model to be comparable to a word based
model so here's 20 million parameters
what you find is actually that this very
simple character based model can get to
perplex see which is as good as the word
based approaches which we found
extremely surprising notice it's a bit
slower to train but because you can
implement convolution is very fast
to use that that helps quite a bit so
the last question we ask is I can one
try to learn more complex interactions
by the use of this highway network which
was talked about yesterday in the main
session so one could attempt to take the
output of that convolutional layer and
apply for example a some non-linearity
and we did try multi-layer perceptrons
and got very bad results instead what we
succeeded at getting good results from
was using a highway network which in
essence has a gating function that tries
to decide between carrying forward the
output of the convolutional neural
network or transforming a corresponding
dimension be a nonlinear function and
here's the picture for that highway
network of what that transformation
would look like so if you go from using
no highway network we get the perplexity
of point eighty of 84 using one layer
dramatically reduces it to 79 and
subsequent laterz actually doesn't help
very much so that was all about English
but really this setting is most
interested most interesting and
morphologically rich languages and
that's where we get our stated out
results so here we look at languages
like check the Russian and we have two
different data sets a small and a large
one with varying vocabulary sizes as a
function of the amount of training data
so the bass lines that we compared to
our this n-gram based language model a
word-based lstm and then we use we
compared to both in blount tsums
morpheme model but because they used a
log by linear model which is relatively
weak language model we added an
additional baseline that hasn't been
published previously what we call the
morpheme lstm model in order to get a
more competitive comparison so here we
see across many languages the perplexity
of of the Engram based language model
using the morphine based approach
significantly improves if you were to
use an lstm actually does as well as the
morphine based module so then we used
that improved baseline we came up with
and finally you see here now the
character based lstm model again we
weren't really necessarily trying to get
better results this was really an
exploration to
c-can one attempt to learn such
representations automatically from data
but you do see it actually significantly
improved results and with a much simpler
method than you would with a morphing
based model because we don't have to
automatically segments each of the words
into the morphine's prior to running our
approach so let's take that a little bit
and try to understand which what each
piece of the model is doing so first
let's look at the the word
representations so we can try to
understand the word recommendations both
before the highway network and after the
highway network to see what they
effective that highway network is doing
we try to understand that by looking at
nearest neighbor words so here when I
was showing you the nearest neighbor
words output from the word based lstm
model and this is what you would expect
if you were to do anything like word to
that current or similar if you look at
the output of the character based
convolutional network what you see is
what you would expect in some sense so
the nearest neighbors to head trading
are words that share characters with it
like heading or training or reading
after the highway network however you
start to get semantic similarity despite
the fact that you might have
orthographic lead disjoint words so for
example the nearest neighbors to you
include things like I and we which have
no characters in common one could also
using our approach try to tackle
completely unseen words so for example
here I'm showing the nearest neighbors
to look with many O's inside it which
never ever appeared in training data and
you see that the model actually can do
something very reasonable which is one
of the big strengths of this character
based model besides the fact that it can
get competitive performance with
significantly many fewer parameters and
the last two things will look at is to
try understand what this is a highway
network doing so one hypothesis we had
was that maybe the highway network is
simply learning for some dimensions to
carry those dimensions and other
dimensions to do transformations but
ignoring the actual word when deciding
those those quantities so here what I'm
showing you is for every dimension of
the output embedding that corresponds to
one dot and I'm showing you what
fraction of the time the x-axis is the
average transform probability for that
dimension used by the highway network if
it was one then it would mean that we're
always going to
transform the dimension if it was zero
we'd always carry it forward but you see
that most of the time it's somewhere in
the middle furthermore the standard
deviation for that dimension is always
quite high which means that the highway
network is in essence deciding to either
transformer to not transform with very
much Bernoulli based fashion if you look
at the convolutional layer max this is
my last slide and try to understand what
is the filter for what are the filters
that are being learned is it actually
picking out an n-gram and so here's a
look at such a filter so what I'm
showing you here is a length 6 filter
the sixth character is the end of word
characters I'm not showing you that and
I'm looking at the moat the hundred
substrings which which hit that filter
with the highest strength and you see
that this filter is really focusing on
suffixes like Ed the however that story
is not always the same so for example
here's a different filter and you see
that it really tries to look at whether
the second letter is T and the sixth
letter is an S and so you get these
really interesting structures which are
learned automatically by the learning
algorithm here's and here's a last one
so in conclusion and we think this is a
part of approach to language modeling
there's actually been quite a bit of
recent work notably this paper by link
at all that was just published that also
tries to use a character based language
model except they instead of a
convolutional approach they use a
bi-directional recurrent neural network
to compute the embedding from the
characters before feeding to the lstm
and what Bradley I want you to think
about how to use such an approach for
other tasks I think actually that these
types of roaches can completely
transform data mining as ways to really
combine different data elements for
predictive purposes or data exploration
purposes thank you
thank you David so please questions
yes I'm not sure if I ask you this at
the poster if it's the same poster but
have you compared your results to bag of
letter trigrams approach you didn't ask
us at the poster because this isn't
appearing in touch okay I so we have not
compared to a bagger trigrams approach
but there's a one interesting baseline
that we would like to be with compared
to is one where you would say build a
set of n grams by and user instead of a
bag instead of using a one hot encoding
for words used in essence a encoding of
the word by what trigrams appear in the
in the word my guess is that would get a
little bit worse results due to the
large number of parameters one we need
ok next question did you consider adding
specific special symbol to the beginning
and the ending of the world yes oh so
you used it yes ok thank you not sure
did you not as some oh sorry go ahead
good do you notice some difficulties in
training some tricks that you used sir
yeah um so critical learning so we
almost completely followed the training
procedure of the Zaremba at all savoy
checks paper which was a stochastic back
prop trunk a truncated back prop with a
learning rate that decreases when she
perplexity stops improving what we found
to be extremely difficult to train was
the multi-layer perceptron that we
wanted to put on top of the
convolutional neural network any
everything we've tried failed for
training that but of course if one were
to simply use an identity function it
actually would would do pretty well so
we were very confident that it's the
optimization which is causing that to
not work well you tried bachelor
monetization I don't remember but that
would be an obvious thing which I agree
here's the last question if you keep it
sure can you say some words about the
computational complexity to train your
models otherwise can you scale do
hundreds of millions of words I didn't
billions of words yeah so as the
vocabulary size grows you start to hit
the same challenges of any language
modeling neural language modeling
approach in terms of having to normalize
the distribution
so we made use of a hierarchical softmax
for the large model settings except we
used actually a little bit as simple of
heart Gustav max and has been done in
previous work we simply randomly
partition to the words and then used a
hierarchy over that random partition we
found that I worked as well as anything
else and with that we were able to scale
up quite well ok so that let's thank
David again
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>