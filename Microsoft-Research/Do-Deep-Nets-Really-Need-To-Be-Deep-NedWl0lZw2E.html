<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Do Deep Nets Really Need To Be Deep? | Coder Coacher - Coaching Coders</title><meta content="Do Deep Nets Really Need To Be Deep? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Do Deep Nets Really Need To Be Deep?</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NedWl0lZw2E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello good morning everybody so it's a
great pleasure to have your our good old
friend and colleague rich from redmond
coming you know to us today and talking
about depth you know he's he's got a lot
of background on both decision forest
you know we we love in here in Cambridge
and also deep neural networks which are
a little bit less familiar with in
Cambridge so hopefully he will help us
you know bridge the gap thank you very
much okay thanks so it's great to be
here it's a very nice to see the new
building so the last time I was here I
think was just a month before you're
going to move from the old building so
so that this is really nice facility so
and we should definitely just watch
movies here tonight this is quite a room
okay so I'm going to talk about some
work I've been doing and trying to
understand deep learning and I should be
honest before I start this on not a
super expert in deep learning it's not
like half of my research is in deep
learning but I've been a neural net guy
for a long time as part of my work when
the deep learning stuff started to
happen it raised some interesting
questions that I wanted to try to answer
okay so this is a joint work with Jimmy
let's see so how many how many people
have trained neural nets or at least
feel like they're pretty comfortable at
neural nets okay okay that's good so
this is a shallow neural net this is the
kind of neural net that we were all
training circa nineteen ninety it's got
an input layer a fully connected
nonlinear hidden layer and then an
output or multiple outputs this is a
shallow standard neural net it's the
kind of thing we were all training for a
long long time for 15-20 years these are
the neural nets that ruled this is a
deeper neural net maybe it's the
shallowest model that we might want to
call deep so it has three hidden layers
in it so they're just fully connected
feed-forward connections each of these
is a nonlinear hidden layer and you know
as neural nets do they're trying to
learn a new representation on each of
these layers to then support predictions
at the output and you know this kind of
modeled this this shallow deep model or
deep shallow model is also something
that many of us played with back in the
early
DS but for the most part back then we
didn't get any extra accuracy out of
these things than we did out of these
things and if we did get a little extra
accuracy it was small and usually not
worth the incredible hassle and expense
of trying to train them so by and large
this is what everybody did in the 90s
with one exception which is this model
on the right this is a convolutional
deep neural net so now what makes it a
deep neural net is that it has these
multiple layers multiple nonlinear
layers like this middle model but what
makes it convolutional is that it has
this convolutional layer or set of
layers at the input of the network so
how many people are familiar with
convolutional neural nets okay not quite
as many I'll just give a really brief
summary of what they do this model is
also except for the fact that it's got
three layers on it if it was this
convolutional input attached to one
nonlinear layer then this would be a
common model also from the 1990s so
y'all McCune invented this a long long
time ago and it's been a very powerful
model in certain classes of problems
things like image recognition and
classification so let me just briefly
explain what convolution does let's see
suppose you were to try to take this
model or this model and try to train it
to recognize faces so and let's imagine
that it's sort of natural scenes where
the faces can occur in different parts
of the image and you're trying to
recognize is there a face in the image
if there is a face maybe where is it or
who is it something like that this kind
of model you know it doesn't know
anything about the adjacency of pixels
if you give this a hundred by hundred
pixel map as an input it just sees that
as 10,000 independent inputs it has note
you can sort them in any water it
doesn't matter it has no idea that this
is in fact a 2d representation of an
image and because of that if it learns
to recognize faces it has two separately
learn to recognize faces in the middle
of the image in the upper left hand
corner at the top of the frame and the
right of the frame it has to see enough
data with faces in all parts of the
image to learn separately to recognize
faces in all those different places and
hopefully that just sounds like a
terrible thing right yeah if I give you
sort of a lot of faces in the middle and
then I go to test you on a face that
happens to be more off-center it won't
recognize it because it never saw faces
there or at least it never saw enough to
learn a good model
they're so that's not a good thing right
I mean faces tend to look the same
wherever they fall in the image so
that's not a desirable property also
you'd really hate to have your training
set have to be so massively large that
there was a statistically large sample
of faces for every position in the image
but that's a painful thing so of course
what we would do is we would do tricks
and those tricks would involve sort of
external for loops one thing we would do
is we'd have the face recognizer only
look at a small sub image maybe 10 by 10
pixels and then we would scan that 10 by
10 window across the entire image and
whenever a face was sort of somewhere in
the middle there we would label at one
and when a face wasn't centered enough
in that image we would label 20 and we
would train a recognizer now on centered
faces or something close to centered
faces it faces that show up in this box
so that loop would be done externally
the neural net has no idea that you're
doing that to it it's just that you've
sort of increased the size of the
training set right we had this problem
that their trainings that would have to
be sort of massively large if it was
natural images to make sure it had faces
everywhere now what we're doing is we're
making it massively large by creating
all of these training points where we've
scanned all possible places in the image
and then we would also do things like
change the scale at which we would do
the scanning maybe we'd magnify the
image by one and a half or two or d
magnify o point 75 or point 5 before
doing that scanning so then we could try
to recognize faces of different sizes
different distances from the image so we
would do that sort of thing and that
work worked fine so we would get
reasonably good accuracy out of it you
can imagine if you've got a large frame
that it now takes some real effort both
to train the model because you have to
train it on this very large training set
and then also to run the model because
you're going to have to scan it across
the image many different positions in
order to to try to detect the faces but
that's how most face of recognizers that
use neural nets worked in the in the 90s
now convolution is kind of interesting
in that instead of doing this external
for loop where we're scanning over the
image convolution lets us put that sort
of knowledge that structure of the image
into the model and think of it as each
hidden unit saying this first layer take
that hidden unit right there that hidden
units has a retina
which sees a small sub patch of the
input image so right away the fact that
I say sub patch means that it knows
something about the structure of the
image that adjacent pixels are adjacent
to each other right so and it's up to
the engineer to have told the model oh I
want you to look at adjacent pixels in a
five by five patch of the inputs things
like that and then what that neural net
will what what that node in the neural
net will do is it'll learn to recognize
for that patch features that hopefully
are going to be useful for recognizing
faces but what's important is it will
learn it not just for the sort of patch
which you might think is falling under
it in the inputs it'll learn it it'll be
scanned internally in the learning
algorithm it'll be scanned so that that
patch applies everywhere in the image
that a five by five patch can be made to
fit and in the process of doing that it
will learn one set of weights for
predicting that feature for learning
that feature it'll just learn one set of
weights for that feature for the entire
image and it'll automatically be applied
at runtime at prediction time to the
entire image and wherever it sort of
goes off the most there's another layer
after it called a max pulling layer
which would then detect that it's gone
off the most and would say this feature
is high and then you might have another
layer of convolution if these are all
little small features with local
receptive fields there's another
convolution layer that's higher that's
looking at maybe a wider set of the
smaller receptive fields and you can
imagine that the the sort of feature
space grows to be larger parts of the
image as you go higher in the hierarchy
okay so that's a convolutional model and
yanma kun you know would often have two
three four layers of convolution in the
model and it was something that the
engineer would sit down and think hard
about the problem think what
convolutional architecture kind of made
sense just like here we would have
figured out how to scan appropriately
across the image then you would build
the architecture that way and you try it
you'd repeat a few times until you
really got it right and then it would
learn to do it okay so this is a shallow
model that's been around forever deeper
model has been around forever but we
just never had much success with these
things until recently and then this is a
deeper model deeper because it not only
has these nonlinear layers but because
it has these layers of convolution on
the input in this case just one layer of
Lucien pool okay and it'll turn out
we're going to be using these models
which is why I use these specific
examples okay today what I'm going to do
is I'm going to mainly talk about
results on the timet speech recognition
data set so how many people know Tim it
or have even used it okay so a couple of
you so Tim it what it stands for texas
instruments and MIT speech recognition
corpus right it's a data stuff that I
think's been around 20 20 or more years
so this would have been one of the early
large data sets that people were doing
machine learning with so this is a
well-established data set people have
been working hard in this data set for a
long long time and it really was sort of
the early a standard for anybody who is
doing speech recognition was to work on
this data set so we're going to use it
for two reasons one there's been results
for a long long time but most
importantly there's been new exciting
results with deep learning on this data
set so that's important the other thing
is it's a it's big enough to be
interesting for deep learning but it's
small enough to be practical to run a
bunch of experiments so that's part of
why we're using this data set I'm not a
speech recognition person we have other
people the lab who helped us do the
right things with this with this data
set so let's say just to give you a
brief idea so what do we got here this
is a visual representation of speech
this is like a second or two in time and
then these are frequency banks going up
vertically low frequency to high
frequency and then this is the power
over time that shows up in those bands
and this representation what we'll be
doing is will be slicing this into small
vertical rectangles and then that'll
become the input vector that's given to
the neural net and it's the neural net
will be trying to recognize phonemes
what phoneme is being spoken at this
moment in the speech and then by
recognizing a sequence of phonemes you
can try to put those phonemes together
and figure out what words are being
spoken so that's how people use this I'm
told that people who work in speech
recognition a lot and work with these
things all the time that they get to the
point where they can just look at this
picture and they know what's being said
so they get that good at it so it's a
really it's a really good representation
and it's kind of like an image to write
read and it works very well even with
our visual system so if all of us have
inborn recognizing speech this way
presumably I could be giving this talk
in these colorful graphs and Udall
immediately recognized it so okay so
let's see it turns out that the inputs
that we're going to give it we've got
you know about 25 milliseconds this way
I think it's a 40 feature banks is it
for you 40 filter banks that we're using
oh yeah it's even onslaught good I think
that gives us about almost two thousand
dimensional input vector and then the
output vector which is the phonemes that
are being spoken that's coded for in
this try phone kind of way so it's
actually more than that it's three times
the number of phonemes we're actually
trying to predict this is a way of
getting some it's almost like a very
cheap form of ensemble it's a way of
getting some redundancy in the coding
and letting the model try to recognize a
more sophisticated version of what it's
what it's seeing it's not going to be
important for the presentation but that
gives us an output which is a hundred
and eighty three dimensions and then
there's a well-defined train set
development set for validation purposes
and test set for this data we've got 462
speakers and the train set another 50 in
the dev set and then 24 in the test set
and I think the way this data set works
correct me if i'm wrong i think each of
the speakers speaks a standard set of
sentences and then that set of sentences
is is the same across all the speakers
so so if i understand it crackle real
vectors of the class and of 83 so yeah
it's a good question it's a class it's a
it's boolean there's there's a one in a
lot of zeros yeah yeah and feel free to
of course to ask questions ok so the
Train Set gives us about a million
frames of input and then you can do the
math the the validation set and tests
are smaller ok so that's the data we're
going to be using as I said it's a
dataset that's been around a long time
and people worked on this hard you know
as far as I could tell it was the
standard data set for people doing
speech recognition and the best model
turned out to be this Beijing hidden
Markov model that was created in 98 and
that model has what about twenty five
percent error rate so it's a hard hard
learning problem
um and that model was very very good and
the once they got to that point
basically little progress was made for
ten years so people had worked on the
thing they kept doing better better and
better they got to this model this model
had about twenty five percent error rate
and then that records basically the
field asymptote they found it very
difficult to do better than this it's
not that people stopped working on it
they just couldn't do better then
somebody applied deep learning to it
they added a deep neural net I think it
might be a three layer deep neural net
to the hidden Markov model suddenly the
error rate drops two percent that's
pretty significant if you've been sort
of stuck at an asymptote for a decade to
have somebody come in and suddenly get
two percent improvement right it's not a
point two percent improvement to
percents pretty big then they applied
drop out to this deep neural net goes
down another three percent the array so
drop out for those of you aren't
familiar with it is a regularization
technique that seems to work pretty well
with deep models so it's a way of
randomly allowing the weights to be in
or out on each training step so that you
don't over fit to any small set of
weights stuff and then in 2013 somebody
applied a convolutional deep neural net
to this they got another percent
reduction in the error rate and then
although I'm not going to talk about
recurrent nets at all we're not going to
use this last model a rumor is that
geoff hinton has gotten yet another
percent reduction in our by using a deep
recurrent neural net so so I just want
you to know that people still work on
this data set and and are still sort of
pushing the frontier for it but anyway
if you if you sit back and look at it
right people work on the thing for a
long time they get to the point where
they have what they think is pretty good
accuracy 25% sort of get stuck they
can't really do better than this model
and then all of a sudden deep learning
gets applied to it and sort of very
quickly we get a you know a two percent
reduction another three percent of
reduction another one percent reduction
I mean that's dramatic right we're
talking five six percent reduction in
our which is not an easy thing to do at
the high end of the scale it's not like
we took an inferior method and had a
five percent improvement we took a
really state-of-the-art method and have
a five percent improvement so there's
something really remarkable happening
here
this deep learning and at the same time
that this sort of thing is happening in
speech recognition similarly dramatic
results are unfolding in image
classification image recognition so so
amazingly good results are happening
there as well okay so there's obviously
something cool going on here what I'd
like to do is you know I'd like to have
some idea why it works and a group of us
at MSR got together her one day in a
room and we started filling the
whiteboard with possibilities for why it
works and it was a long list it was you
know more than a dozen things that were
serious possible explanations for why
deep learning was working better than
shallow nets that we had used before so
we were wondering like well how how
should we go about doing some research
to try to better understand this and I'm
going to show you one thread of research
that we've been using to attack this so
now the literature has a couple
explanations so one of them I think
yoshua bengio probably gets most credit
for this is that when you suppose I give
you a fixed number of parameters I say
you've got a million parameters to play
with or 10 million parameters to play
with for this problem you have a choice
you can sort of lay them out in one
large single hidden layer that's going
to be very wide or you can decide to put
less on each hidden layer and you can
make the model deeper and Yahshua Benji
Oh has argued that by making the model
deeper we actually get a more
interesting more complex function class
out of the model then if we had put
everything into a shallow architecture
and there are some examples of this for
example parity functions you can prove
that if you're going to if you're going
to represent high order priority
functions in a shallow model then you
need an exponential number of weights in
that shallow layer or layers in order to
do it whereas if you've got enough depth
for the degree of parity it turns out
you only need a polynomial number of
weights in a deeper model to do it so at
least there are some pathological
function classes for which in fact you
can prove that this is true that going
deep actually helps us in a way that we
can achieve any other way in a neural
net need a budget staying shallow so
that's interesting maybe that's the
explanation maybe it just really is a
more powerful function class
so another possibility is and it could
be that these are related but let me
walk through it because i think the
intuition is different behind this one
imagine we're doing face recognition and
i need to cite whoever i stole these
pictures from from someone's paper so
imagine you have the input retina and
then the first layer maybe it's learning
something like edge detection patch
detection you know small region features
think of these as things that used to be
safe if sift features and stuff like
that but now it's all just being learned
from scratch and then maybe a little
later in the hierarchy maybe you're
starting to learn small pieces of faces
so those might be parts like eyes ear
nose mustache you know hairlines things
like that and then yet further higher in
the hierarchy you're starting to put
those pieces together and you're
starting to recognize things that are
faces or parts of faces and then you
just go further up the hierarchy until
at the end you're actually doing some
sort of face recognition or
classification well it could be that
natural learning especially if you do
learning sort of from the raw input
signals to some high-level task like
recognizing a phase it could be that
natural learning really just tends to
have this property that it's easiest
given the kind of sample sizes that we
have and the kind of compute courseware
horsepower that we have that it's
easiest to do learning if you structure
it this way that you kind of first are
going to learn sort of local small
features that when combined yield less
local you know larger features that are
bigger pieces of the object and then it
moves up a step and moves all the way
until you're at the target sort of level
of recognition which might be faces or
bodies or things like that so it could
be that this is just a natural property
of a lot of learning tasks especially if
you're learning from raw signals people
who work in deep learning will often say
you know the goal is get rid of all
manually created features go as much as
possible to the raw signal and then
learn from that so maybe when you do
that maybe it turns out that learning
just naturally breaks into this
hierarchy and that's just a better
inductive bias that's a better prior for
learning so maybe that's the explanation
it's possible it's just an accident of
the training algorithms and methods that
we have available right now so so think
i mentioned drop out briefly before this
new regularization procedure thats only
come into being I think Hinton created
that in the last 5-10 years you know
dropouts a very very effective
regularization procedure on deep models
shallow models sometimes it helps
sometimes it doesn't it's not that
powerful on shallow models so we have a
regularization technique that's sort of
super powerful in these deep models
maybe if we had a super powerful
regularization technique for shallow
models well maybe we would do as well
with shallow models assuming these other
things aren't true so and there are
other things that we've got we've got a
much larger bag of tools available to us
now we have a rectified linear units
instead of sigmoid functions you know
we've got all sorts of we've got pre
training procedures all sorts of stuff
like that so so maybe maybe the reason
why deep learning is working so well is
just no all I don't want to say an
accident of the tools we have available
because the tools were created in large
part to make deep learning work well but
maybe it's just sort of a perfect storm
of technique that is sort of making the
the deep models work so well and maybe
if we had similar strong methods for
shallow models maybe they would be just
as accurate so so that's a possibility
you know it really could be all the
above right it could be could be twenty
percent that one fifty percent that one
another ten percent that one and then
whatever's left like 30 or 40 percent
you know for something that we don't
know about yet right so it's possible
that that's what it is it's possible
it's none of the above right it might be
that none of these are really quite
right or if they're right there only
right a little bit and they don't
explain the main part of why deep
learning is so powerful and you might
think well you know maybe maybe deep
learning is working because we finally
have data sets that are big enough and
because we finally have enough
horsepower in GPUs and stuff like that
to do it and maybe that's why deep
learning is working so well a problem
with that though is I'm going to show
you in a few seconds we can't still
train the shallow models using all this
big data in this horsepower we can't
train the shallow models to be as good
as the deep
so there is something that's happening
in the deep models that's not happening
in the shallower models that that's
interesting so it's not just a question
of horsepower and GPUs there's there's
no doubt of course probably a big data
GPUs is important to making deep
learning work but maybe it's not the
critical enabling thing okay so the goal
of this project is just to try to
understand why deep learning is working
so well and we're willing to tackle that
anyway I'm going to come clean now you
know I don't have all the answers you're
not going to walk out of here like oh
that's it I didn't see that coming I
wish I had that answer but we've run
into some interesting results and I
think the results are sort of fun and
you'll just enjoy seeing those and they
do sort of give some evidence about what
might or might not be be the reason why
deep learning so good okay now gotta be
gotta be fair here right so we're going
to be comparing deep in cello models
well again back in the back in circa
nineteen ninety we had theorems which
basically said hey a shallow model with
enough hidden units enough weights
enough capacity can represent any
reasonably well behaved function so in
the case of in the case of parody
function it might require an exponential
number of units but basically there's
are there a theorem which say you don't
have to be deep to represent things so
you might think well well who really
cares if shallow can do with deep can do
we already knew that 20 25 years ago
well first of all this is a
representation theorem which is
important it's not a learning theorem it
doesn't say that back prop will in fact
learn these things if you were to give
it a wide neural net so for example
these parody functions are particularly
hard to learn I mean if if if if life
depended on the ability to learn high
water parity functions we wouldn't be
here probably because it's hard so so so
sort of a proof based on things like
parity functions you know is maybe not
as convincing as you might want and it's
just a representation theorem it doesn't
really say what's learnable with back
prop remember one of the explanations is
that there's a good prior coming from
depth that we're missing when we train a
shallow model well the
be completely consistent with this I
mean maybe maybe the prior is what's
making making deep learning work and
there's nothing to do with
representational power so in which case
this theorem you know would apply but
not in any interesting way it's also
it's important to me that we're looking
at sort of the functions that we
actually need to learn in natural
problems so I'm less interested in sort
of pathological corner cases and I'm
more interested in the learning that we
actually have to do to get high accuracy
on real problems and maybe it turns out
that parody functions aren't very common
in the real world and in fact a shallow
model wouldn't have to be all that large
in order to learn these things so so
anyway I just want to make sure that
everybody knows we do already have a
proof that at least if you had a magic
learning algorithm a shallow model could
do everything a deep model could do if
you were willing to give it possibly an
exponential number of units so we're
going to try to not give it an
exponential number of units we're going
to keep the number of parameters in the
model comparable to the number of
parameters in the deep model and then
we're also going to use learning so
we're not going to answer questions
about what could be represented we're
gonna actually demonstrate it that it
demonstrates that it can be learned okay
so here's a peek peek at the result so
here's the timid data set when we train
this is now our result not not
historical results when we train a
shallow neural net that's like the
neural net we had on the left just one
fully connected hidden layer nothing
nothing exciting in it a good old
vanilla circa nineteen ninety neural net
when we trained it we get an error rate
of about twenty-three twenty-four
percent and that's actually very good
results I mean that's comparable to the
to the hmm Batian model okay so this is
actually quite good and one of the
reasons why it's a little better than
people would have gotten 10 or 15 years
it goes you know we've got more
horsepower now we're a little better at
training even shell a neural net so it's
just a little bit better than things
might have been 10 15 years ago but not
in any surprising way it's just just a
pretty good model if we then go to a
model that just has three hidden layers
so we take the same number of parameters
and we spread it now among three hidden
layers instead of one wider single layer
we see a two percent decreasing the Earl
Ray so the accuracy jumps two percent
and this is sort of the magic of deep
learning right just by taking the same
parameters making the architecture deep
are throwing back prop out it in this
case I think we are using wait to case
so we're using some of these new tricks
for training the model but all of a
sudden you know there's a two percent
increase in accuracy which is
significant on this kind of problem and
then if we add convolution to it so we
still have three nonlinear fully
connected layers plus a convolutional
layer we get even more accuracy we get
another percenter 22 of accuracy and the
convolution the reason I think it's easy
to see why convolution works in images
but remember we've kind of converted the
speech recognition into something that
almost is like an image and the reason
why convolution helps there it requires
a different architecture than was used
for images but the reason why it helps
is if you think about it people don't
speak at the same rate when they're
saying a sequence of phonemes so
convolution on the time axis gives a
little ability of the model to
generalize over different local rates of
speaking and then I think the more
powerful thing is the convolution that's
happening over the frequency domain some
people have low voices some people right
we all have different different pitches
that are natural in our voice and yet we
could be saying exactly the same
sentence and i think the convolution
gives a certain amount of correction for
small variations in pitch and again it
means the model generalizes a little
better by the way it's not the case that
you can just take speech from a low
speaker and you know increase its speed
fifty percent and have it sound natural
it turns out people who are naturally
higher pitched speakers do do different
things than so it's not like you can
just correct for pitch by by doing
frequency shifting okay so vanilla
neural net twenty-three twenty-four
percent add a hidden layer you get
another 2% in accuracy add convolution
you get another 2% in accuracy that
right 19 yet another 2% we're able to
train shallow neural Nets these are
neural nets with just one hidden layer
we're able to train a shallow neural net
that has the number same number
parameters is this and achieves
essentially the same accuracy as this so
it's not a deep model so it's better
than this it's equal to that and it's
using the same number of parameters as
that and if you let us have more
parameters we can actually train a model
that's a shallow neural net with no
convolution to be as good as the
convolutional model so that's a preview
of where we're going that's the basic
result
ok so what tricks did we have to do to
make that work I'm going to talk about
the three main tricks one is this model
compression thing which you may or may
not have heard of another is a trick we
had to do to speed up learning and then
the other is we're going to do what you
might think is an unacceptable dirty
trick we're going to do this ensemble
ensemble trick and I'll tell you about
that in a minute okay so model
compression anybody heard of Malta
compression before this is this is work
I did with students back at Cornell
about ten years ago so that the thing in
model compression is what you're going
to do is train a smart model any way you
can in this case you're going to go to
some deep convolutional architecture as
deep and big as it has to be to train a
smart smart model and then we're going
to train a smaller model to try to mimic
that smart model so we're going to try
to capture the function learned in the
smart model in a model that's somehow
smaller now in this case and when we did
the work we were really trying to take
these really large models like gigabytes
in size and compress them into models
that were only megabytes in size so
small they're really meant smaller in
terms of size and energy consumption and
speed here smaller is going to mean
shallower so what we're going to do is
train a deep model and then we'll try to
train a shallower model to mimic that
that deep model so that's the game we're
going to play and the typical way we do
this is you've trained that very smart
model and what we do is we pass if
you've got it a bunch of unlabeled data
through the model collect the
predictions of the smart model that
becomes a synthetic training set for the
student model and then you train the
student on this synthetic data set with
the labels from the smart model so now
it's trying to mimic you know predict
exactly the same outputs that the smart
model would have would have predicted so
that's the goal and if the student does
it perfectly mimic mimics perfectly and
you've got a large transfer data set
there then the student model will become
identical in function to the to the
teacher model it'll make the same
mistake so it'll get the same things
writer to predict the same values it
doesn't have to be the same
representation right it'll it'll
possibly have a different architecture
different size but as it in terms of i/o
function it'll become the same so that's
our goal so I'm just going to show you
this graph this is from the work
10 years ago just because understanding
this graph will make understanding the
real grass i'm going to show you make
more sense so so let me show you this is
just good old root mean squared error so
this is you know l2 on the vertical axis
so down is good we have some random
problem here don't worry about it has
nothing to do with the deep learning
stuff we've got some random problem that
we've picked and the best vanilla neural
net week a train which is actually a
very good neural net it's a carefully
optimized architecture instead of hyper
parameters this model has a l2 lost just
above point one okay so it's actually a
pretty good model this this would be
considered state-of-the-art in many ways
ten years ago by doing this other stuff
we did at the time where we were
building these monster ensembles the
ensembles had everything in them they
had SVM's they had boosted trees random
forests k-nearest neighbor methods
stumps naive bayes it you know every
learning method you could think of we're
allowed to be in this ensemble and the
ensemble would get very large but the
nice thing about the ensemble was it
could be optimized to any loss and it
could go places that no one model could
go so maybe the best single model had
accuracy around here but we could make
an ensemble of the better of these
functions that had much higher accuracy
and you can see the ensemble has l2
below point eight point oh eight so it's
actually a much more accurate model in
this particular problem okay so nothing
nothing too shocking there i think we
all tend to believe that ensembles tend
to work and the fact that one model is
this accurate and ensemble of models is
that accurate presumably doesn't
surprise anybody here's the surprising
thing it's the red line on the graph the
x axis now means something it's the
number of hidden units that are in this
neural net this neural net is the simple
neural net that has just one hidden
layer with that many hidden units in it
so and these are pretty small numbers
right this is 8 16 32 64 hundred twenty
so these aren't very big Nets they're
certainly not big by modern standards
but even by the standards in 10 years
ago they're not very big and here's the
interesting thing is of course if it's
too small can't do anything it's not not
enough capacity this model is being
trained this red model is being trained
to mimic this very accurate ensemble of
models and the sort of remarkable thing
I think is that we get
a pretty small vanilla architecture
neural net to be you know epsilon close
to the accuracy of that teacher ensemble
okay and we managed to do that with just
sort of 32 64 hidden units and that's by
playing this game of passing a bunch of
data through this model collecting all
the predictions that model makes and
then training a simple neural net of
different sizes on that new synthetic
data set now what should show larger is
the synthetic data set well we'll talk
about that a little bit so the synthetic
data set in this particular case we
actually didn't have any extra synthetic
data so we had to do some sort of
density estimation to hallucinate extra
synthetic data and then it's uh it's
roughly a hundred times larger so you
know if there's kind of 5,000 points in
the training set of these original
models think of them as being say five
hundred thousand points in the capture
set and that is an important part of why
it works so I I think your intuition of
why this is possible is correct now
realize we don't have any extra labels
there's no no extra real labels coming
from the true generating function but we
do have more data which and fortunately
in this case had to be created
synthetically from the original you know
training set so the amazing thing really
should be this gap between this thing
and the best neural net that we could
train on the original data right this
thing gets to see the right data this
thing in some sense gets to see it's
presumably not the real targets right
it's not the correct answer all the time
it's not like this model has zero or
anything like that so this thing this
red line isn't seeing sort of perfect
real data and yet it's doing
significantly better than the best
neural net that we knew how to train
using the original data so that's the
sort of magic of this compression
process and that's being used in many
different ways now so I might have a
chance to talk about that so just one
more the best in the neural network has
how many hidden units so I don't
remember for this problem I do know that
the way we were doing the experiment was
we were varying the number of hidden
units by powers of two from very small
to extremely large we were varying the
learning rates we were varying the
initial weights that we applied to the
model and we
were varying how far before during or
after the early stopping point on a
validation set that we would stop so we
were we were doing a pretty good job
yeah but but the this best vanilla
neural net you know it's it's it's not
ten times larger than this and it's it's
not smaller it's sort of in the same
ballpark good good good question thank
you ok so the this uh this extra
accuracy that we get in this neural net
by training training it to mimic a smart
model is sort of interesting and that's
the game we're going to play so sure if
your synthetic data set was infinite ah
yes then that red curve should basically
should not affect right it should meet
the blue curve very very very good yes
so assuming the student model has the
representational power and assuming that
the learning algorithm is good enough
then with an infinite transfer set the
gap between the models should go to 0
yes your intuition is right on the money
yeah and should stay as should stay at
zero because there'd be no overfitting
yeah and whether that's real overfitting
I don't actually know so your intuition
is exactly correct and in fact this gap
because we don't have infinite transfer
sets in Tim it is going to turn out to
be important and it's going to what
makes me play an ugly trick seriously
you said the representation only says
you can achieve sort of the you can
approximate the function very well but
it doesn't mean backpack will do it and
now you say if we have a large transfer
set anything up in units then magically
it will be 0 well it's important both
things one one that the model has the
capacity to represent an accurate enough
function and secondly the back prop the
learning algorithm is capable of doing
it if neither of those is true then it
won't do it presumably yeah so so i'm
not saying i'm assuming back prop is
powerful enough but if it is powerful
enough then those those lines should
should should merge and not separate
yeah okay so this this is good all right
so this is old stuff but we're going to
we're going to see a graph like that so
let me just briefly talk about you know
hopefully this is a little surprising to
you that that you can play this trick
and train a more accurate neural net
on fake data than you can on real data
right I mean that should sound a little
counterintuitive so let me just give you
some reasons why this this might work
well one is exactly what we were just
talking about we might have an extremely
large data set that we can use for
transfer to the model the original data
set might be small 5,000 10,000 points
maybe now we can use a million points
100 million points to do this transfer
it means that we can effectively
eliminate any concern about overfitting
in the student model the student model
maybe was struggling terribly with
overfitting on the original training set
and now it doesn't have to worry about
overfitting anymore if we have a large
transfer set drawn from the right p of x
the original density turns out it's
critical to stay on the right density
and we could talk about that later if
you want ok so that's part of it but
there are other things though that are
happening here that might not be so
obvious so imagine that you had five
percent label noise in the original
training set so five percent of the
labels in the boolean labels are just
flipped to the wrong state assuming that
you trained a really good smart model
either an ensemble or a deep model and
you regularized it well presumably it
did not learn to overfit to those labels
I'm not saying it's perfect predictor
where those errors were made but
presumably you you regularize dwell and
part of what makes that model so smart
is that it's not getting all of those
things wrong meaning it's not getting
them right ok so it's it's fixed these
and now when you use it to label data it
doesn't put in the errors the errors
actually disappear now whether the
things that replaces the errors with our
the right predictions or just other
predictions that that's hard to say
there's a good chance it just replaces
them with predictions there aren't as
confident as you know the original
targets so so so one advantage here is
that you can imagine that going through
the teacher model sort of help censor
the data in a way that makes it easier
for the student to learn it so one way
to think of this is you know the teacher
has had to work very hard to create
knowledge the student though doesn't
have to learn to create knowledge from
the original training set the student is
just learning to capture the function
that the teacher had and presumably it
just intuitively makes sense that sort
of creating the knowledge in the first
place is much much
then sort of then handing off that
knowledge to somebody else so I think
we're seeing instances of that so
there's also a censoring that can happen
even without a label noise just imagine
that there are parts of the space for
which given the sample size that we have
in the nonlinear in that part of the
space it's something you're just never
really going to be able to learn
properly it's like hard sharp corners
hard turns in the in the discriminative
function and maybe it's just very hard
to do that given the sample size that we
have well once again the teacher model
presumably will have if it's a really
accurate model if have learned to do
something reasonable in that part of the
space and now the student gets to see a
simplified version of the original
problem which has perhaps some of these
very difficult hard edges sort of smooth
off of it so once again the student can
have a simpler problem and you know one
way to think about it is the function
the students learning is already
learning friendly you know a learning
method already learned it whereas the
original data might have been sort of
challenging in multiple ways but by time
we get to the to the student we know
that the function is already a pretty
learnable function because it was just
learned by something this arguing hose
then it should be true to you should be
able to eat rate this right say if I
have a student of the student then the
learning difficulty has been digested
twice yeah it'd be fun to talk about
that in more depth than will be able to
go into right now we have thought about
this issue now for the reasons we were
talking about earlier if we do the mimic
process right the student will become
equal to the teacher and then a second
iteration wouldn't buy you anything but
in the case where the student right
because because if mimicry is working
perfectly then you actually do capture
exactly the same function and then if
you capture it again and again you know
you're still at the same place but to
the extent that there could be like we
could have a student that intentionally
under fits or student that intentionally
over fits both are interesting for
different reasons and then iterations
actually start to make sense because now
you can imagine a student of a student
learning to overfit less than the
student / fit to the teacher signal and
maybe
can get a sequence of models and now you
would almost do something like early
stopping on the sequence you would try
to find the generation of student that
is now fitting the best yes so I think
your intuition is correct that you could
imagine a sequence of such steps each of
which is censoring the data making the
edges softer making it yet more and more
friendly as a learning function and that
maybe we would get a benefit from that
we played with it just just for a tiny
bit when when Jimmy was here well you
know we never saw anything magic but it
was one of the directions that we
thought would be interesting to go and
we haven't gone there so if anybody has
some cool insights about how to go there
I'd you know I'd love to talk about it I
think it's whoops oh no oh my the first
person to destroy this room this will
forever be known as riches stain so
actually we should get it afterwards
because it sugar and sugar will be
sticky ok so there's another interesting
thing here that's happening though which
is think about it suppose you're doing a
prediction for a medical problem right
now the risk that you might want to
predict for a patient you know depends
on all the features that you have
available to you the history physical
lab tests and things like that but it
also presumably depends on a bunch of
things that you couldn't measure maybe
you can't measure the genome yet things
you don't understand yet we really don't
understand the genome yet and a whole
bunch of things we wouldn't even have
thought to measure yet so so the
function that we're trying to learn
might depend on the features we have
available to us but also depend on a
bunch of things that aren't available to
us and that presumably makes learning
challenging you know for the teacher who
has to create the knowledge the student
though by the time you get to the
student model the data is a function
only of the available features so any
dependence on other features is now gone
and that's because you actually have the
function there and you're actually going
to pass data through the function and
capture the labels right so so once
again the the job that the student has
to do is simpler than the job that the
teacher had to had to do in some cases
so and then another one which i think is
very important is you know the original
learning problem is often classification
on these hard talk
it's 0 &amp;amp; 1 so you might have you know
182 zeros in a single one someplace
maybe some of these problems look like
that they're easy enough that you
actually should learn probabilities that
are very close to zero and something
very close to one maybe some of these
things are very hard to learn and you
really would be comfortable with a bunch
of things that are like point ones or
point 05 or you know maybe for some of
these hard cases you shouldn't require
that somebody actually tried to achieve
something which is zeros and ones and
it's going to turn out that the soft
learning learning to soft targets we
think is an important part of what makes
the process work stone okay so summary
of all of this is that the student has a
simpler learning job than the teacher
had and it has a simpler learning job
possibly in a for a number of reasons
size of the data the 01 vs soft targets
of the data getting rid of our is making
the problem easier to learn for a
function smoother all those sorts of
things ya know you know we're now about
to do some experiments to try to do it
some of it is easy to do and some of its
not so easy to if you really want a
reliable experiment that you can trust
the results of some of these things are
not that easy to test for ya ya know but
but it's definitely a direction that we
need to go yeah okay up so this is the
soft targets so remember the original
teacher in this case is learning a
boolean classification problem what we
do is we convert that to a regression
problem and we do something that's a
kind of surprising i think which is you
might think well let's take the
probabilities that come out of the
softmax if you're familiar with soft
max's they basically made sure that if
you're predicting like a 10 or 100 class
problem that the probabilities sum up to
1 across all those classes it's kind of
what they're doing on the output layer
of them of a neural net you might think
well let's take all these probabilities
and then try to have the student learn
to predict those probabilities we
actually don't do that we go to just
before the probability normalization is
done to what is sometimes called the low
jets and those are sort of the real
values that would go into the
normalization step and we try to have
the student predict these real values
before normalization
here's here's one reason why why and it
seems to help us so on when we did
experiments without it we didn't get as
good results so so my team
counterintuitive but here's a reason why
this can help is imagine the teacher
predicts 1 11 11 11 11 11 for something
you normalize all that you get something
like you know point 0 1 point 1 point 0
1 point 0 1.9 point of one right or
point 99 something like that now imagine
that for a different case the teacher
thinks a point 0 1 point 0 1 point 0 1
point 0 1 point 0 1 point 0 1 point 9 50
point 1 point 1 point 1 you normalize
that you might get exactly the same
values after normalization as the
previous one which was predicting ones
and hundreds go further you know point 0
0 0 1 whatever point nine nine nine nine
whatever again after normalization you
could get the same values internally the
teacher is definitely doing something
different on those three cases there's
no doubt about it it is not the same
model internally for those three
different cases it's just the
normalization step that then makes them
look the same to the extent that that's
happening and I don't believe that
happens on all problems but to the
extent that that happens if you want a
student to mimic the teacher it turns
out it's better to give the students
sort of this internal access to the raw
values the teacher would have predicted
before normalization hides them and then
sort of makes it impossible for you to
go backwards to figure out what they
would have been so it makes the learning
simpler whether this is true on all
problems we have no idea but it does
seem to be at least on Tim it was an
important part of making this work okay
and then we just do regression it's just
good old l2 regression we're not doing
boolean targets anymore there's no law
gloss just just good old regression
which is pretty well behaved timett we
didn't have any extra unlabeled data and
it turns out speech recognition data
really depends on how you collect it and
Tim it was collected a long time ago in
a very special way so it's not the case
that we can just generate or collect
more data that would come look like it
came from the same distribution
so we're going to do something crazy
which is we're not have any unlabeled
data we just throw away the labels on
the training set we train the deep model
using the training set throw away the
labels and then we're going to real able
it with this the deep model and then
train the train the student on that data
set so we're just going to use the same
training set there so there's not even
any extra unlabeled data getting used
here it's all just the original training
set and there will be a problem and
we're going to have to use trickery to
this is not a good way of doing
compression in general you really do
want unlabeled data if you can have it
but but it's good for the pedagogical
purposes of this talk to do it this way
so so now I'm going to show you some
real results and then we'll wrap up so
okay this is a graph of the number of
parameters and models on a log scale so
this is a pretty small model a million
parameters two million parameters
hundred million the models around here
are getting sort of the size where we
can't fit them in a single GPU anymore
so we have reasonably good GPU so so
these are getting to be pretty big
models but they're not like the world's
largest models but they're they're big
this is a shallow neural net so it's a
neural net with just one hidden layer it
has a number of parameters we could
calculate what the number of hidden
units must be I guess it has a number of
parameters that's equal to to this axis
and then this is the accuracy on the
timid dev set so up is good and you can
see if the models too small it doesn't
do very well I don't think anybody's
surprised about that you finally get to
the point where you have enough capacity
to do well you start doing well you get
an accuracy just above seventy-eight
percent and then who knows maybe the
models that are too large start to
overfit or maybe that's sorry I don't
have our bars on this they are they're
expensive craps to generate so I don't
actually know if this is coming down or
if this is just noise so so I think if
you think of this as sort of hitting an
accuracy of about seventy eight percent
then assam toting that would be fine and
there is a chance that if things got
large enough with this training set they
would come down and over fit items i'm
pretty sure they would overfit
eventually whether that's real
overfitting I don't know okay Shaolin
neural net it's a model that except for
horsepower we could have trained 25
years ago not nothing unusual about them
all this is the model with three hidden
layers so this is our sort of small deep
model being trained on that same data
set and now since we've got the same
number
parameter saying this model is sort of
those models it means there must be
fewer hidden units on each of those
three layers for the number of
parameters to add up you got to be
careful the way you calculate parameters
are you have to multiply by the input
layer by the output layer but but
roughly speaking you know there's fewer
hidden units on these layers than there
is on on the single hidden layer there
and you know the magic of deep learning
works we get sort of an extra two
percent accuracy just by spreading our
parameters in this deeper architecture
and throwing sort of a pretty good
backdrop at it so that that's cool it's
not not unexpected whether there's
anything interesting happening there
with the crossing I don't know we're
actually we haven't spent that much time
trying to analyze what happens in the
very the models are too small and of the
graph in fact for the paper that we were
taking to nibs we didn't even show them
the left hand side of the graph we
truncated it below one and that's
because we didn't have an explanation we
would didn't want people to get
distracted we didn't think it was
important to the central story but your
friends I think I can show you the whole
whole graph so okay so nothing too
surprising this is the kind of thing I
think we all expected this horizontal
line this is a convolutional deep net so
it's a deep net that has three hidden
layers the three hidden layers have the
about this sighs I think I think it's
like that optimal architecture but now
it has has convolution pooling on the
input of it and i've done it as a
horizontal line that doesn't mean it has
all these different number of parameters
it's because i don't really know how to
count the number of parameters we could
we could discuss how we want to count
parameters in a convolutional model you
could talk about what's the actual
number of weights that need to be
trained and that's sort of small because
the weights are shared across the whole
image or we could talk about the number
of computations that you have to do at
runtime and if you're doing model
compression sometimes you're more
interested in the computations like the
multiplies you have to do then you're
actually interested in the number of
weights you had to learn so I just
decided to punt it's a very good
architecture don't worry about it sighs
that's not what's going to be important
to us so but it's a good model you add
compilation you get almost another two
percent increase in accuracy so take a
really good one of these add convolution
does even better so again the magic of
deep learning and then we're going to
this is where you might start to get
upset that purple model is an ensemble
of a dozen convolute
deep models and people have discussed I
think for example geoff hinton talks
about there's ensemble incite of these
very large deep models and i think that
is true but you can see that there's
still room for an external ensemble to
get more accuracy and we didn't do
anything special we just you know took a
bunch of models that we had trained that
we're all reasonably good as we were
playing with architectures and we said
oh it's taking average of them and sure
enough you get another percent increase
like ensemble ensembles rights think
it's the gift that just keeps on giving
I mean it just always makes sense if you
can afford it so and sure enough it
works here and now when people are
trying to win deep mining learning
competitions there of course building
ensembles of their deep bottles and of
course that's how they're winning so so
so nothing nothing really surprising
there we weren't 100% sure that an
ensemble of deep models would give us
that but but it does and I think other
people are seeing this as well ok here's
what I'm going to do that you might not
be happy with I'm going to train a
shallow model to mimic that very smart
ensemble of deep convolutional models
I'm not going to train you to mimic this
deep model or that deep convolutional
model I'm going to train it to do that
and that's because I already know from
previous experiments on this data set
that there's going to be this gap
between the teacher and the student and
the gap I believe only has to do with
the fact that we don't have a hundred
million unlabeled examples I think it
has there's no other explanation for it
i think so and it's sort of unfortunate
but you know if i try to mimic this and
i have a two percent gap oh I'm going to
be right down there so the gap means I
won't be able to train a shallow model
that's as good as this deep model but if
I try to train to that model there's a
two percent gap Oh it'll be as good as
that one and if I train to that one and
there's a two percent cap I'll actually
be better than it it'll still be an
existence proof but you could train
without any extra data a shallow model
to have an accuracy equal to or better
than a deep model and possibly as good
as a deep convolutional model it's it's
still a legitimate test of that it's
just that I went through an intermediate
state that's even more complicated than
just a deep model I actually went to an
ensemble of convolutional deep models so
so hopefully you won't you know
you for that one all right so let me
show you the graph this is like the key
key graph of the talk that is a shallow
neural net with different numbers of
parameters in it so it's the same
architectures as this red line like if
that lie if that's above this these two
architectures are identical those two
architectures are identical right so
it's a shallow model being trained to
mimic the very smart ensemble of
convolutional models and I think the
remarkable thing is that for a
reasonable number of parameters where
the deep models were starting to be
excellent where they were hitting their
peak performance we're hitting a peak
performance with the shallow model
that's actually a little bit better than
that deep model and sort of halfway
between the deep model and the deep
convolutional model and then if we're
willing to give it more parameters this
is a lot more parameters it's a log
scale right so it's that's you know
hundreds of thousands of parameters over
there we're actually able to do as well
as the deep convolutional model and
remarkably were able to do that without
any convolution in the model that's
something I never was confident would
happen I mean convolution really is a
magic thing it's a very very strong
prior as to what the model should learn
so it's a very very powerful thing and
the fact that the shallow model which
has no convolution at convolution in it
is able to do something comparable to a
convolutional net i think is very
interesting how to use a lot of
parameters there to get there okay so
this is sort of the key graph yeah and
time to this is we've been looking at
the dev set things look essentially the
same of the test set said don't worry
about it just the same results I'll just
say this briefly one of the key things
that makes the shallow learning shallow
mimic process work is we just don't have
an overfitting problem so when we when
we look at squared error on the either
the train said the test set with a dead
set on the shallow mimic model it just
comes down and it just never seems to go
back up so so overfitting if it does
happen happens way out there I think
that summary sort of makes sense right
now a shallow neural net can with the
same number of parameters actually
between that's more accurate than a deep
there on that a three hidden layer deep
neural net and if you give us more
parameters it can do even as well as a
convolutional
neuron that we had to speed up learning
let me just briefly say this the shallow
models take forever to Train weeks
months I mean longer than we ever
expected we found that by putting a
linear layer which is kind of like a
bottleneck in the neural net between the
inputs and the hidden layer that we
could speed up training by about an
order of magnitude and this was very
important now those of you know neural
Nets know that adding a linear layer in
the middle of a neural net does nothing
to increase its representational power
in fact it very slightly hurts accuracy
but for us it's sped up learning by a
factor of 10 and that was very important
to us so i won't go through that in more
detail i will won't show you the results
but it is important that we got that
speed up the ensemble of CNN's I think
you understand that let me just say that
you know there is this issue where so
here's accuracy of the target model
accuracy of the mimic model if they
follow each other you get a 45-degree
diagonal line right if the mimic can do
everything that the teacher can do if
the mimic has less accuracy than the
teacher well then the target has
seventy-nine percent but the student
only has seventy-eight percent and if
it's a 45 degree line then they still
are tracking each other the the student
always follows the teacher just a few
steps behind or it could be that the
slope is less and the student is
actually falling behind as the teacher
gets faster and better it turns out this
is for a small model and that's for a
large model if you squint it actually is
roughly 45 degrees and there is a gap
that's why it's over on the right-hand
side of this graph so there is a gap
between the teacher and the student but
roughly speaking we don't see any sign
that the student even the small one is
running out of capacity that means that
they can't follow the teacher and then
this is all on timid let the teacher out
of just small right what would happen if
the detective is he was like 60 instead
of 80 do you think that the whole
assumption would change yeah it's a good
I actually don't know it's a very good
question it's interesting yeah okay this
is on timid you know you all know not to
do machine learning on one data set and
draw general conclusions from it we've
done some preliminary experiments on the
c-4 image classification data
this is all going to show you a
two-layer net doesn't do very well in
this data set fifty percent error rate
going to a larger neural net using
dropout and some other tricks helps a
lot you get down to sort of twenty
percent error rate adding convolution
multiple layers of convolution you get
much better another five six percent
reduction we're able to train a shallow
model that has one convolutional layer
one nonlinear layer to sort of be as
accurate as this significantly deeper
convolutional model and we're now in the
process of doing a much more thorough
set of experiments to demonstrate that
and I think I should just stop I just
want to make sure i thanka colleagues
many of these are at Microsoft Research
and presumably you know them so lead
dang especially helped us an awful lot
with the timid timid dataset thank you
let's just take one question whoever
wants to leave of course is free to do
so and i'll be around the whole day so
if anybody wants to stop by and chat i'm
happy to do that one question of hope
soon there's no doubt that was on this
you know in fact it's funny so over in
the applied speech group we're starting
to do things i should say they are
starting to do things where they train
extremely large deep models that could
never be fielded on our servers and in a
few cases ensembles of these very large
deep models and they could never never
be fielded on the servers and then we
find that we can train a normal sized
deep model which is still pretty big
that does fit on the server to mimic
that thing and we get more accuracy out
of the medium sized deep model by
training it to mimic this very large
high accuracy model then we do by
training it on the original training
data and that's interesting because they
have extremely large training sets as
well large labeled training sets and
they have massive amounts of unlabeled
data also so it's interesting that it's
still working in that corner of the
space so so now the model that you would
put on the server would be a model train
to mimic a model that you couldn't
afford to put on the server and
presumably the models that we might put
into mobile devices would also be be
models that would be very
small trained to mimic these very large
models and you know they would be
affordable on small devices and my
understanding is Google is doing the
same thing with this my mate Steve oh oh
that's a yeah that's it that's a great
analogy and I hadn't thought of that one
I like that a lot yeah sure which is and
to someone working on speech the timid
results and your explanation would bit
and well then the reason people stopped
working on Tim it was because it was
huge is not relevant I ladies and speech
recognition so it's sort of a research
data said at this point that's to try to
figure out to what extent close phonetic
annotation could help with speech
recognition no people moved away from
that the results are interesting because
as you say it's hard for various reasons
but no no that's good to know now we are
seeing let's say qualitatively similar
results in the real speech recognition
that we're doing with the applied speech
group but there the experiments these
experiments are carefully designed
there's nothing practical about this i
mean the deep model if you can afford to
use you just use the deep model because
i'm making a shallow model it has the
same number of parameters i mean there's
no there's no real win there unless
shallows and advantage i'm doing this to
answer some sort of abstract question
about whether the models really had to
be deep in the first place but we are
finding on the applied work working with
real speech modern speech data that the
compression trick often does let us
train more accurate small models than we
would have been able to train looking at
the original training set and i think
for the reasons that where you mentioned
the middle of talk thank you very much I
think this speak it again
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>