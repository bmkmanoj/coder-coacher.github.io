<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 2 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 2</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8NuLx71EoEs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay hi everyone so my talk is going to
be about Frank wolf vision quadrature
for realistic integration with
theoretical guarantees and this is joint
work with Chris oats in Sydney marks
really me who's my supervisor work and
Mike Osborne from oxfords okay so why
should you actually care about the
method well as was described by zubeen
magnolia pro basic inference is one of
the main topics in machine learning and
in this case you have to do integration
all the time so for example if you do
marginalization conditioning then these
are all integrals and in most cases
they're not actually going to be
analytical and you're going to have to
use some kind of numerical scheme so a
typical way of approximating integrals
is using quadrature rules so you
approximate your exact integral which is
here on the left with a weighted
function of a weighted combination of
function values and you have to pick
some points the d axis and some weights
the w's now this is usually very very
slow and well of course it depends on
the method but why I mean by slow is
like your error between the exact
integral and the approximation decreases
very slowly in N and so for example if
you take Monte Carlo methods then you're
going to sample some X's frumpy and then
you're going to wait a more equally and
this will converge as n to the minus one
half and in a lot of cases this is going
to be way too slow and you might not be
able to afford taking a lot of samples
so one particular thing that you can do
is use Beijing quadrature and in this
case what you see idea is to use some
prior information you might have about
your integrand to guide your choice of
the design points and the weights so
what I mean by prior information is you
might have some idea of how smooth your
function is or other properties and you
can encode this directly into your
numerical scheme now ok so this is a
solution but it's actually just moving
the problem around BK
there's not actually any convergence
guarantees for vision quite a true for
now and so you don't know if it's
actually worth using it but that's
actually why you'll want to go and check
out the paper because in this case we
showed that if you use a particular
convex optimization algorithm called the
Frank wolf algorithm the noob Asian
quadrature scheme can have exponential
convergence under certain assumptions
and obviously exponential is much much
faster than one of the square root of N
and so you should definitely come and
check it out and it's post 257 and
there's also a whole workshop on Friday
on pro basic integration which I invite
you all to come thank you hello everyone
I'm my name is sushi chef is the de agua
de I'm from epfl the title of my
presentation is distillation robots
logistic regression this project is a
joint work with payment module and as
funny and Daniel logistic regression is
on one of the most widely used
classification technique in logistic
regression we aim to construct a
classifier where a maximum log loot
estimation and we also wish to quantify
the classifiers risk in both cases we
need to evaluate an integral with
respect to the distribution P of future
label course however and the
distribution P is unknown and must be
inferred from the and set of training
data and classical logistic regression
just replaces a p with the empirical
distribution P hat and on the training
samples however this leads to
overfitting our suggestion is to
construct a set of all distribution that
could have generated at training data
with high confidence we call this set as
the ambiguities that and so the proposed
distribution robots logistic regression
model and Troy's to
of finds the minimizer of the worst-case
expected us where the worst case is
taken over all the distribution in the
immunity set we also aim to quantify the
best and force and miss Keller
certification probability over the
ambiguity set the challenge here is to
construct an ambiguity set such that the
resulting classifiers offers
out-of-sample performance and it leads
to a tractable reformulation in this
study we use the day and we construct
the ambiguity set as a ball in the space
of probability distribution and around
the empirical distribution we use the
water h9 distance to quantify the
distance between two distributions more
specifically the water each time
distance is defined as the minimum
transportation costs for moving for
moving the probability mass from the
distribution p1 to p2 our approach has
several benefits first due to the recent
measure constant concentration result it
offers out-of-sample guarantees second
it leads to a tractable and program and
finally we can find a probabilistic
interpretation for well-known
regularization technique also as you can
see in the figure and we consistently
outperforms the regularization the
gorillaz logistic regression and
classical model thanks for your
attention
hello my PhD advisor thoughts on your
teams and I studied how we can learn
from the logs of interactive systems
that only provide us Bandit feedback we
identified an interesting new kind of
overfitting in this setting which we
call propensity overfitting are
conditional random field prototype that
learns from Bandit feedback avoids this
overfitting using the ideas from this
paper is available online the formal
setting is batch learning from Bandit
feedback imagine you have collected X Y
Delta triplets from your search system X
denotes the user queries y denotes the
action that was taken by the system so a
ranking was shown and Delta denotes the
users feedback on the presented ranking
typically we don't know how the user
would have reacted to some other
possible ranking this aspect of the logs
is called bandit feedback we want to use
these logs to find a good policy H that
can generate Y given X bandit feedback
makes this problem harder than
supervised learning where we are told
the best possible action on the training
set it's easier than reinforcement
learning where we also have to solve
credit assignment and it's not quiet
online learning for contextual bandits
because there is no trade-off between
exploration vessels exploitation going
on here this is a very practical problem
we have collected terabytes of logs from
search from recommendation from ad
placement and that is a general recipe
for reusing the logs from these systems
in a model-free manner first recognize
that these logs are biased the if the if
certain actions were preferred by the
system then those actions would be
over-represented in the logs so use
important sampling to d bias then next
do empirical risk minimization or erm
this gives you training objectives very
similar to supervised learning a trained
error plus regularizer except the
training error is now this R hat that's
computed by our important sampling in
practice erm with this R hat often fails
a common failure model is where erm
completely ignores the feedback that you
have collected in the logs just over
Fitz to the propensity of seeing your
data items in the logs and finds the H
that's very simple who's empirical
variance is going to be very small and
you think you're doing really good but
you're actually very very wrong no
capacity control or variance regulator
variance control regularizer zeeeee we
have so far fix this issue and the
insight really is these are hat
estimators are subsequently getting used
in an optimization problem so what we
really need is an estimator that works
well with optimization this our hat is
not one of those we identify a
particular property called equal
variance which allows for robust erm and
we point out that the self normalized
estimated is indeed equal variant and
it's much better than this picking the
best estimator for this problem is still
an open question but surprisingly the
self normalized estimator runs very fast
taking just a few seconds to learn
multi-level classifiers join me on
number 59
my paper works on asynchronous parallel
stochastic gradient for non convex
optimization I'm sure Julia and my
closers are each new home internally and
GD we're from University of Rochester
non convex optimization is quite common
application such as deep learning
natural language processing and
recommendation system and asynchronous
parallelism is widely used in deep
learning asynchronous paralyzing has a
variable property that it can
substantially reduce the idle time in
synchronous parallelism because it does
not need the synchronization stab our
people provide some selective analysis
for asynchronous stochastic gradient
especially for non convex optimization
in asynchronous stochastic gradient
there is surely a central node keeping
the optimization variable X and there
are many workers running traditional STD
algorithm independently and concurrently
the main difference from the standard
STD is that the ex had used to compute
the stochastic gradient might not be the
same as the current optimization
variable X in the central node in a
synchronization this is also a key
challenge in analysis the other key
challenge in analysis is that different
implementations lead to different forms
of X hat for example the X hat in
cluster implementation and what
competition I'm quite different our
paper actually covers both
implementations and shows that the
asynchronous stochastic gradient the
convergence rate of it is consistent
with stochastic gradient descent and we
show that a linear speed-up can be
achieved up to the order of square root
of iteration number walkers here is an
intuition of linear speed-up the
division from the true gradient caused
by sdd actually dominates the division
called by asynchrony thus the
asynchronous will not see this effect
the convergence rate with the encoder
rewards for their constructive comments
and we'll come to our poster tonight our
post number 863
good morning everyone so I'm Andre I'm
from Princeton i'll be talking about a
joint work with Angela wasti who is at
Rutgers about some cases where we can
say something provable about variation
in France as applied to topic models
okay so let me give you some context
first so in recent years it's been
getting increasingly more and more clear
that most problems that you want that
you care about in machine learning tend
to be non convex in nature and so to
give you some sort of stereotypical
examples here it would be things like
clustering map approximating posteriors
etc so from a theoretical point of view
these ought to be intractable but what
happens often in practice is very simple
local search or descent type of
heuristics tend to work quite well so
what am I putting in this category while
it's things like em variational base
back propagation other variants of
gradient descent etc so the sort of made
this sort of very big question for
theory here to answer is why does this
happen so because of time and space
constraints vastly an egregiously kind
of oversimplifying and omitting let me
just mention a few kind of a few success
stories where this has been done before
so this has been done for for various
variants of k-means for example in the
case of mixtures of gaussians
alternating minimization has been
analyzed for matrix completion for phase
retrieval for dictionary learning and so
with that in mind I can tell you what we
do so we basically add one more
heuristic to this list of things that we
know how to analyze and we analyze a
version of mean field variation
inference as applied to topic models so
the things that you need to know are the
following our assumptions are very
reasonable they're similar to what
priority theoretical works on topic
models assume but not have been at these
these works do not actually use these
local search for the same type of
algorithms our initializations are also
reasonable in fact they're inspired by
what we think people use in ldac which
is the most probably the most often used
software for for doing virtual inference
for lda and our proof methods are
somewhat different from these other sort
of prior works that I mentioned so
hopefully there'll be some other
applications of them so please come to
the
poster it's not like the number is not
on the slides but it's 48 please come to
the poster to hear more hello everyone
my name is Igor Cara and I'm here to
talk about extending gossip algorithms
to distribute the decimation of houston
TX this is joint work with a legend
belief from inner a lil and my advisors
which is epsilon Stefan chromosome from
telecom vitac let's consider the program
where some observations sample is
distributed over connected network and
nodes of the network can be smart phone
and some sort what you want and you want
to estimate a pairwise min statistics
for instance variance area under the ROC
curve or any x y statistics involved in
a machine learning problem what we want
here is narco wisdom that can perform in
a synchronous setting with respect to
some communication and local storage
storage constraints this this problem is
very fairly typical in domains such as
telecommunications or sensor networks we
based our approach on gosee protocols
that means that at each iteration only
one edge is activated this protocol has
been widely studied for estimating the
sample mean in this case selected node
just avoid their estimates however
extent knively extending such algorithms
would lead to massive data transfer in
the in the case of pairwise estimation
how solution consists in the following
each node stores three elements its own
observation eggs an actual octillery
observation why initialize to X and it's
a statistic estimate edge hard at every
step selected nodes average their
estimate as in the standard
mean estimation then they update their
estimates using the two observations old
and finally the swabs auxiliary
observation this algorithm can perform
in a fully asynchronous setting and is
shown to have a theoretical convergence
weight of 1 over T with the explicit
dependence in the spectral gap of the
network which is a measure of a measure
of communication capability of a network
it also shows good practical performance
in simulations thank you for attention
and I'm looking forward to seeing you at
poster 72 tonight hi everyone I'm
background mr. Solomon and this is a
joint work with a macabre see a 3-0 and
under scrolls and distributed sub
modular cover problem suppose that you
have a large data set and you want to
find the subset ideas as small as
possible that achieves the desert
quality q according to a utility
function in many applications such as
clustering recommendations document and
core personalization and nonparametric
learning this utility function is
monotones of modular which means that
adding an element to a smaller subset
helps at least as much as adding the
same element to a larger subset for
monitors of modular functions the greedy
algorithm that iteratively selects the
best element achieve the logarithmic
approximation guarantee for this NP hard
problem but for large data sets the
greedy algorithm may be impractical for
example if the data set is not even fit
on the memory of a single machine in
this work we propose the first
distributed algorithm for sub modular
cover problem where the data set is
partitioned and set up and machines
operating independently the key idea is
a reduction to the problem of
cardinality constraints of modular
maximization for which recently many
distributed algorithms have been
proposed a technicality of our approach
is to estimate the size of the optimal
solution
and by coordinating different algorithms
and sharing partial solutions between
consecutive rounds we also provide
approximation guarantees for our
algorithm in particular we show that the
solution quality in terms of the
solution set size is competitive to that
of the centralized greedy algorithm
while the number of rounds compared to a
naive distributed version of the greedy
algorithm improves from a linear
dependence on the size of the optimal
solution to only a logarithmic
dependence and optimal solution subsides
we also applied on algorithm and large
data sets here you can see the result
for vertex cover problem on a network of
two billion edges are I bottom has a
trader parameter alpha for smaller
values of alpha as you can see we get
very close to the result of the
centralized greedy algorithm which is
shown as dash line but in much smaller
number of rounds come to our poster
number 65 and I will be happy to discuss
this more thank you hello everyone I'm
rut and i'll be talking about Newton
Einstein method so in this paper we
consider the problem of finding the
maximum likelihood estimator in
generalized linear models when the
number of observations much larger than
the dimension of the parameter so we
minimize the negative log likelihood by
standard iterative updates but the goal
is to find the scaling matrix QT that is
computationally efficient and also
provides sufficient curvature
information so i'll give you the basic
idea about Newton's time method so we
start writing out the passion of the glm
problem and we notice that it's just the
sample mean as the major of its
expectation which is written on the
right hand side and if we pour this as
an estimation problem we apply steins
lemma and find an equivalent expression
which is written on the
second line and instead of estimating
the expectation the first time we
estimate what the steins lemma give us
so this algorithm takes a Newton step
relying on h9 type lemma so we call it
Newton stein method and just by using
this equation we can reduce the Newton's
method order MP square per iteration
cost to order MP which is just opposed
to computing the gradient so we show
that this algorithm newton stein method
gets a composite convergence rate which
is it starts with a quadratic rate and
then transitions into linear rate later
on this is also consistent with our
experiments as you can see that the
initial convergence is quite fast and it
transitions into linear near the true
minimizer so i'll talk more about the
composite convergence rate and give you
more compare experimental comparisons
with other existing methods in the
poster session thank you so this
concludes this morning session let's
thank all the presenters once more
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>