<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Algorithms Among Us - Ian Kerr | Coder Coacher - Coaching Coders</title><meta content="Symposium: Algorithms Among Us - Ian Kerr - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Algorithms Among Us - Ian Kerr</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zrrpELolpiI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm our next speaker is Ian Kerr who
is the Canada Research Chair in ethics
law and technology which have to say is
a terrific title I'm not really sure
there's anything else left to actually
be a Research Chair in at that point and
so in let's see if we can find your
slides yes
ah
okay I'll do the rest of your various
and then just a reminder to everyone if
you do have any questions for any of us
because please submit them at that link
that was displayed on screen before the
link is also available through the
website of the symposium thank you and
please Ian take away thank you very much
it is wonderful to be here I want to
thank you Michael and also your
colleagues Richard and Adrian for the
kind invitation to be here when I think
of symposium I think of Socrates and
Plato sitting around several bottles of
wine small venue I'm looking to the back
of the room here and seeing that we have
a bit of a different meaning so I'm glad
I brought my my slide deck with me I'm
going to be talking a little bit now
about some of the legal aspects in the
near term but I think also perhaps into
the medium term as well because i think
what some of us thought was the near
term is still we're optimistic about
some of those kind of things and we're
still in early days here and in thinking
about this from a legal perspective i
want to focus as my as my title slide
sort of indicates on a particular
concept within all of this and that is
this notion of delegation so what I'm
talking about in other words is the idea
of what happens when human beings
delegate certain tasks to machines that
were previously only within an
exclusively done within the human domain
and some of the legal and ethical
implications and aspects that come up in
the context of that delegation so I'm
going to talk about three different
examples I'll spend a lot more of my
time on the first example than the other
two just to sort of set the stage and my
first example really um takes us back
aways it starts with some of the work
that I was doing back before we were in
this millennium that we were now in in
the early days when I was just starting
out as a professor in 1999 and at that
time I was asked a question by the
Canadian government by a department that
is called the uniform law conference of
Canada and the question that I was asked
was can computers make contracts and
this is kind of where we're at now
with this questions of automation
technologies and algorithms and whether
they can enter into contracts in a way
that used to be exclusively something
that was achieved by human beings and
really I guess the way to understand
that question is can contract formation
be something which we can delegate to
machines from a legal perspective this
was something that was of interest
globally it was something that was
really part of the United Nations
working group on electronic commerce at
the time I was a delegate for Canada to
the UN on that issue and so the Canadian
uniform law conference asked me to write
this study about this which I did and it
was called providing for the use of
autonomous electronic devices in uniform
electronic commerce act you can see we
were grappling back then with the idea
of what to call these things now some of
these things were calling algorithms
it's a nicer name but in terms of
thinking about these the issues actually
haven't gone that much further so one of
the things that happened as a result of
this UN model law that we pre created
and then brought back to domestically to
all the different countries I'll speak
to the one in the jurisdiction that I'm
from which is Ontario is we enacted
legislation and one of the provisions
within the legislation looked like this
it had a definition for something that
was called an electronic agent you can
see the definition there a computer
program or any electronic means used to
initiate an action or respond to
electronic documents actions in whole or
in part without review by a natural
person at the time or response so the
important thing here was this idea that
all of a sudden we would let algorithms
enter into contracts and that's what the
section in in various jurisdictions I
pulled the one from from from from
Canada a contract may be formed by the
interaction of and we used that term
electronic agent and a natural person or
by the interaction of electronic agents
now the idea here was to recognize that
caught that commerce was automating
itself and that we had to have our laws
be able to accommodate that automation
in ways that would be good for business
and in terms of providing certainty but
also in
terms of allowing people to do things
with machines that they previously
weren't able to do and I would suggest
to you that that section 21 that I've
got up there on the screen is in fact a
revolutionary Act this is sort of the
first time in history we're giving
permission to the machines were giving
them legal human attributes of being
able to enter into and create legal
things which have legal force and so
really as someone who also teaches
contract law and was doing that this
morning in Ottawa as I would talk to my
students about this this was kind of a
weird thing from the perspective of
contract theory we'll start with sort of
a kind of transaction that we all know
very well right and this is with a
primitive form of automation for example
a Coke machine right when we put our
money into the coke machine that kind of
transaction is very easily accommodated
in contract law so here's a famous quote
from a from a very fun judge from the UK
named Lord Denning and he was
complaining about some of the automation
techniques in the early days the
customer pays his money and gets a
ticket he cannot refuse it he cannot get
his money back he may protest to the
machine even swear at it but it will
remain unmoved he's committed from the
very moment when he puts his money into
the Machine and the contract was
concluded at that time and so lord
denning goes on and i won't read the
whole quote here but basically to lay
out a theory of how we can understand
those things where a machine is
interacting with the human being and
really what we see is that the machine
is just holding out the offer of its
owner or proprietor and when the person
puts the money in they're accepting the
offer that is made by the owner not by
the machine I think that's consistent
with what Eric was talking about a few
minutes ago when he said this idea of a
I being like I like a hammer or an
instrument and so we start to see that
in the early days it was no problem
accommodating law to automation but as
we started to move forward and these
were some of the early slides when I was
looking at in these days with
intelligent agents as your community
would call them and early instances of
this like my Simon
and perhaps this one even better from
from MIT Media allowed this was an agent
called kasbah and kasbah was something
where you could in essence program
certain parameters and MIT some of you
in the room might remember this sort of
did this as a demo one year when it was
doing one of its fundraising gigs and it
have everybody come in and they get a
whole bunch of objects and they would
get to they would set a computer agent
while they went into the conference to
negotiate bids on their behalf they
would select for example a particular
kind of price function in this case a
price rising function so you can see for
example the one at the end that strategy
would hold on to the money and have low
bids right till the end and then raise
the price and but the idea was that a
person could set the machine into motion
and the Machine would generate the
contract and then at the coffee breaker
at the end of the day they'd see what
kind of things they ended up with all
right so this was sort of you know circa
nineteen ninety-seven pattie maes MIT
Media Lab and if you think about what's
going on there what we start to see even
then and things haven't changed
tremendously since then is that while on
the one hand of the continuum we can
think about things like Coke machines as
purely instrumental and therefore from a
legal perspective we can attach
traditional notions like absolute
liability to the person using the
instrument so if you use the coke
machine and the Coke machine enters into
a contract then you're going to be
responsible for that contract but at the
other end what we start to see is that
whereas these machine systems begin to
perform less and less like instruments
and more and more like persons it
becomes more difficult so what do we do
for example when the machine system does
something which was completely
unforeseen unpredictable and unexpected
by the person who set it into motion and
that's where in the study I talked about
agency law and the way that agency law
agency meaning something quite different
to lawyers than it does to people who
write about intelligent agents in this
community but that agency law has
something to add here in a kind of way
I'm not going to go on at length talk
about how agency law works but the idea
is to recognize
just going back to that model there that
there sorry my screen showing something
different from yours that that we can
think about what the attribution rules
are when we understand that AI occupies
an intermediate ontological category
it's not a person it's not quite an
instrument either and how do we set
those attribution rules I say agency law
works in a certain setting I want to
talk quickly into my remaining time
about two other examples just to get a
flavor for some of the kinds of things
that law is confronting in thinking
about some of the liability schemes as
AI gets better and better and as people
more and more become comfortable
delegating human decision-making and
tasks to machine systems so this is one
which was sort of already mentioned this
this particular autonomous vehicle
happens to be one of the Google ones and
when we start to think about these kind
of things it really challenges our
models from a legal perspective right
because when you think about what
happens in lawsuits involving vehicles
we currently operate on a model that's
called product liability okay the idea
of the prod in behind product liability
is to say somehow that there was a
defect in whatever the particular
product was that you're having here in
the manufacturer should be responsible
for that defect well that's not at all
what's going on with respect to the
Google car even in cases where the
accident isn't human caused what would
be happening with autonomous vehicles
very often as those vehicles are doing
precisely what they were programmed to
do and it may be something which would
be unexpected to the person who
programmed it and certainly to the
person who's occupying the car but at
the same time it is nothing like a
defect in the traditional sense and that
area of law just really doesn't fit very
well so once again what we start to see
here is it in some of these cases it
doesn't do just to treat the the AI
that's being used here as merely an
instrument it's not an instrument alone
at the same time it's not a full-fledged
person it's some kind of intermediate
kind of state so for those same reasons
and so for some additional
munds negligence also is not really the
right model here for thinking about
Google vehicles because negligence
usually involves some kind of duty of
care that's owed to a person and then
somebody who owes that duty living up to
a particular standard of care that just
doesn't work in the same way when we're
talking about algorithms not the least
of which because some of those
algorithms are pretty opaque and we
don't even know the decision-making
processes upon which they're based but
also we're not talking here about a
community of norms a community in this
case of drivers and and their norms in
the same kind of way so it creates
different difficult challenges I'll just
speak sort of for a couple of minutes to
a few things that are interesting that
come up in this context some of you may
have seen some of this work it's very
interesting from one of my former grad
students dr. Jason Miller he's gone on
to finish his doctorate created a as
philosophers often do a thought
experiment that he called the tunnel
problem and it's an extension of an
existing body of work that philosophers
have thought about for a long time now
philosophers have thought about this in
the context of what Philip a foot wrote
in the 70s about what she called the
trolley problem and the trolley problem
was a way to tease out on the basis of
utilitarian ideas how to determine you
know what would be the right course of
action so do you flick the switch and
the trolley veers off its original
course not killing one person with one
baby but instead killing five people and
there's been a million variations on the
trolley problem the tunnel problem is as
Jason Miller and others have imagined it
imagine an autonomous vehicle driving
within a tunnel and towards the end of
the tunnel all of a sudden there's a
child playing unexpectedly in the middle
of the roadway and the question is is is
the autonomous vehicle going to swerve
off into the wall of the tunnel thus
putting at risk the passengers in the
vehicle about putting less at risk the
individual pedestrian on the street or
is it going to plow right through that
pedestrian all of a sudden what we start
to see as philosophers and ethicists is
that these very interesting thought
experiments are no longer something that
is the luxury of a philosopher these are
things that people who are working on
these problems
are now having to figure out as design
items for the choices that they make
around these vehicles and it's
interesting some of the work that that
my colleagues have done on this through
the what's called the open Robo ethics
initiative is they've asked people on
their their their opinions on this and
they've asked if you find yourself as a
passenger in this problem how should the
car react and so what you see here
perhaps not surprisingly is that sixty
four percent of people said well if I'm
in the car I want I don't I don't want
the car to swerve into the into the
tunnel wall and kill me I want the car
to keep going straight but another
thirty-six percent of people actually
said the the opposite of that and we're
willing to take the risk at the expense
of the saving the pedestrian who has a
lesser likelihood of surviving that
crash they also asked some interesting
questions about how hard it or easy it
was for people to answer that question
most people thought it was more the
majority of people thought it was
actually fairly straightforward to be
able to answer that question but the
interesting question i think is to quit
this question which is a question about
who should determine how the car
responds to the tunnel problem and what
you see here is that people at least um
in terms of the majority of the pie seem
to think that the people who are the
occupants of the vehicle ought to be the
ones determining that question with much
smaller percentages saying that
manufacturers or the people who are
writing the AI code that they should be
the ones only twelve percent think those
people should be just designing and
thirty-three percent sort of think that
lawmakers should be deciding it so we
have to think really hard about certain
old legal principles like this one which
says that people who enter into risky
activities should assume the risk we
have to think about whether we want to
have fault schemes or no fault schemes
as one possibility and these are really
plaguing questions that lawyers and
ethicists are going to add to the plate
of people who are building the AI both
in terms of thinking about things at the
design stage but also thinking about how
the regulation that's going to come into
place after the design and I have one
more example that I want to talk to and
and and that will sort of build a little
bit on something that Eric
brought up which was Watson and Eric
talked about Watson as being really good
sort of answering frontline questions
both on Jeopardy but also for example on
on phone in when people are phoning in
for customer service and asking basic
frontline questions now many of you will
remember Watson when Watson sort of took
the human beings in jeopardy and made
them look if I dare say a bit robotic
and you might remember a Ken Jennings
sort of famous act in his last moment of
being defeated by by Watson in jeopardy
where he says I for one welcome our new
computer overlords right and so some of
you might not know that was actually a
reference to Kent Brockman a Simpsons
character which was itself a reference
to hg wells and the Empire of the ants
but I think when we think about what Ken
Jennings was up to when he was talking
about there's kind of a duality in what
he's saying on the one hand he seems
optimistic about AI and all of the
things that it will bring for us but on
the other hand he recognizes that this
is a kind of relinquishment of control
and in this case 12 what he thinks of as
the computer overlords and we can think
about that in the context of what we
want to delegate to AI and what we don't
want to and so you see that there's kind
of in some ways I think a kind of
interesting almost existential question
that's going on here in terms of how
much faith are we willing to place in
the algorithms and how much faith will
be placed in the power of algorithms and
when I think about this I'm reminded of
a concept that Robert piercing who once
wrote a book called zen and the art of
motorcycle maintenance talked about
which is this idea of the Church of
reason and when we think about this in
the context of algorithms it becomes
really interesting because when we adopt
some of these algorithms we have to
remember that the reason that we adopt
them is because of the success rate that
they have and evidence based practice
says if the if the algorithm keeps
continually producing successful
outcomes there's reason to continue to
employ the algorithm and so it's not
surprising that Watson has been give
more meaningful tasks than just
answering phones right this is a
headline from a couple of years ago
already from wired in the UK where it
claims that Watson's IBM is better at
diagnosing cancer than human doctors and
so it's not clear if we're if that is is
that is totally true or not it makes for
a good headline but in any event what I
think it suggests is something very
interesting which I'm going to try to
work with and finish up in my last few
minutes here what we're talking about
now and the way that IBM for example
puts forth Watson is to think of Watson
as a kind of tool and that's the
language that's been used here it's a
tool in the doctors toolkit to help
diagnose cancer but ultimately one would
say in these situations the human being
remains in the loop it's the doctors
decision to make using language like the
most responsible physician as is used
sort of in hospital settings and and
this is where we're sort of at right now
but I'm kind of interested and have been
working a lot lately on this question
about what happens next what happens
when we get to a point where there is
what one might call human robot
disagreement where the doctor has an
inclination or an intuition towards one
outcome the machine system gives the
opposite diagnosis and what is the
doctor who is the human in the loop
going to do right so I'm imagining this
kind of scenario and I'm imagining a
doctor of this sort right house who has
that sort of humanistic you know sort of
gut feel I think bones McCoy would be in
the same camp of doctor but but has this
sort of humanistic feel to what they're
doing here and yet the algorithm tells
him to do something else I would suggest
to you that we are quite close to being
on the precipice of this kind of
scenario which is a scenario that I see
as raising sort of the angst of Abraham
really in a kind of way Abraham sits on
the mountain hears the voice but
ultimately has to
I'd do I listen to the voice or do I
follow what I think I know and we're
getting close to that point so so me and
my colleague Jason Miller have been
doing some work on this and we've been
thinking about questions of moral
responsibility and legal liability from
hindsight in cases where there's
human-robot disagreement and so I'm not
going to go into the details if you're
interested it's very easy to find this
paper online but what we sort of imagine
here is a sort of different set of
matrices of outcomes where there is
human robots working in partnership of
the way that IBM's Watson is now doing
in cancer diagnostics and what we see is
that there's a couple of cases where the
human and robots agree the type 1 in the
type for in one situation they both
agree and the outcome is desirable in
the type 4 case they both agree and the
outcome is undesirable those are less
interesting cases because there's
agreement between the human robot but
it's the other two types that are kind
of interesting to think about and what
we start to see when you think about
that and I'll sort of skip through the
details of this but to say when we start
to think about responsibility the type 3
cases in which there's an undesirable
outcome the type 3 cases are when the
robot gets it wrong there's going to be
still a justification for having the
robot do what it's done namely that its
track record is far superior to the
human doctor so that kind of decision
process is still in line with evidence
based practice and that's not true of a
situation where the human gets it wrong
in that that kind of situation so I'll
leave you with this thought here which
is that as we start to contemplate cases
of human robot disagreement and as we
start to think about what's going on
here we sort of come to what I call an
asthma vian paradox and the paradox is
this that the normative pull that leads
to a decision to delegate to the robot
namely evidence based reasoning
generates ultimately a system in which
we now have no obvious evidentiary
rationale for explaining the outcome
generated by the X
ro expert robot the robot doesn't
operate on a theory and if it's a
private sector algorithm it were not
even subject to its its decision
processes to begin with all that we have
in essence is the hindsight case and so
we all know that hindsight is 2020 and
we're going to start to see here that
some of these legal problems become very
vexing as we move forward and
particularly as AI moves away from being
that of a simple hammer or tool and
becomes more of an intermediate inch or
intermediary occupying some space
somewhere between being merely an
instrument and not yet being a person
thank you very much I look forward to
the panel
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>