<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session 2 | Coder Coacher - Coaching Coders</title><meta content="Oral Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session 2</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5LSudePkwl0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
this is the first time nipsey's had an
award of this kind was introduced by the
this year's program chairs to the aim is
to acknowledge highlight and celebrate
major developments in the field from
nips the constraint for this award is
that though papers published in nips
between ten and fifteen years ago and
the idea is to give enough time to give
a more informed perspective about the
impact of the work so I chaired the
committee to selected the award and it
was certainly not an easy task there are
many strong candidates papers that have
had a big impact on the field so the
2013 nips 10 to 15-year classic paper
award goes to daniel lee from USU
pennsylvania and sebastian song from MIT
for the lips 2001 paper algorithms for
non-negative matrix factorization
so let me say a few words about about
the paper and invite dan to come up to
receive the award unfortunately
Sebastian couldn't make it today so Dan
and Sebastian were both at Bell Labs and
when this work was published Sebastian
was on his way to MIT and I guess damn
was on his way to the University of
Pennsylvania where he's now professor of
electrical and systems engineering their
earlier work had introduced non-negative
matrix factorizations as a way of
compactly representing multivariate data
the motivation came from from
neuroscience findings fast distributed
neural network representations and the
nips 2001 paper formulated non-negative
matrix factorization as optimization
problems introduced a simple alternating
minimization algorithms based on
multiplicative update rules and analyzed
their performance and these algorithms
are very widely used in practice today
beyond their impact in in machine
learning and and we heard about to him
one of the spotlight talks this morning
this work has launched a whole subfield
of signal processing where these methods
are a standard tool first bass
representations of signals in
applications like audio source
separation for instance so
congratulations to dan lee and sebastian
see on for the nips 2013 10 to 15-year
classic paper award
I'm gonna give you Sebastian
I'll make sure thanks all right thank
you Peter on behalf of Sebastian to
myself it's a we're very honored to
receive this nips classic award although
in my mind classic is somewhat of a view
foam is impor antique or ancient so it
kind of makes us feel a little old so I
guess as official nips old-timers it's a
pleasure to kind of to see the the kinds
of novelty and the mix of zaniness here
are nips that allowed our paper to form
at that time there was a lot of talk at
nips and interest in probabilistic
representation sparse representations
and on the other hand and neural
neuroscience throws questions about what
our biological constraints doing in
learning and so our paper was to take
some ideas in terms of simple
non-negativity constraints motivated by
positive firing rates and dales law and
neuroscience and looking at how those
could be applied to machine learning
algorithms and in the ensuing time
looking back it's it's it's great to see
some of these ideas still around and and
and being propagated for instance at the
time when we were doing our paper we had
considered binary constraints but we
thought that the combinatorial
optimization problem was too difficult
and now we see that some smarter folks
here are able to overcome some of those
difficulties there's also lots of
connections if you look at the
representations from a geometric point
of view you can see that these are low
dimensional kind of conic sections
there's now been a lot of work in nips
on other types of low dimensional
representations manifold learning in
particular and I just want to point out
you know kind of connections to ISO map
by josh Tannenbaum vin de silva and jon
langford mallilie by lawrence all and
Sam Rowe Weiss who unfortunately we lost
a few years ago as well as laplacian
eigen maps bye mija by belkin and
parthen yogi who we also lost so in
summary this is just to honor all our
friends here are nips whose influence
has greatly shaped all of our work and
all the people that we've lost over the
years samro Weiss partha yogi and then
task or a few weeks ago thank you very
much
right so we're going to start the the
next session so the first talk would be
by yo young you and on decomposing the
possible map Thanks masses so this world
in this world we are trying to look at
how to decompose the proximal map which
is used in many grading exams so oh
sorry so a tiny bit of motivation so
many machine learning problems can be
formulated as you know minimizing a loss
function rail and the regularizer f so
usually they recognize the same you know
and then all of recent work has been
focused on you know getting a sparse
solution and of course you are
interested in getting a computational
efficient algorithms so in order to
discuss a family of reaction we need a
technical definition which is called
long as a model envelope and approximate
map so here we are taking a convex
function f and we add a quadratic
perturbation join it and then we
minimize we take the minimum then we'd
be the end function which is known as a
mofo envelope so it's essentially a
smooth of the original convex function f
and the minimizer is called the proximal
map so you still know that PF @ point
why so here is a example so we take the
absolute function which is a black curve
you see and mohawk amulet which is a
smooth version of the absolute function
it's a blue curve is known as a hubris
North function and the proximal map is
just a well-known you know soft-shoe
estimate a soft during the operator this
is a red curve you see here to send a
truncated everything towards the origin
and in particular if you take indicator
function of F functions are you plans a
convex set then the proximal is just you
know the projection the use your
projection so based on the approximate
there is a family wagon down as a
proximal gradient some people so-called
is Easter or for backward or forward or
backwards meeting so you consider two
steps in the first step we take a
gradient step with respect to the lost
function a or only and in the second
step we take the proximate with respect
to the record wise f as I said before if
we take the l1 norm for sparsity you get
the shrinkage operator and this
algorithm has guarantee the convergence
and it's rate of convergence depends on
you know how nice the loss function is
and it's a natural generalization of the
projected greeting accent where the F is
just indicator become back set and it
also reveals that sparsit inducing
property of your recognizer as we for
instance if you have a l1 norm you get
the single operator that's where you get
sparsity in the algorithm now there are
a lot of a variation of this algorithm
some of the reference on this to the
pillow however if you look at the
algorithm carefully there is a miracle
there which is in the second step so in
the second step we need to compute the
Proxima of the regularizer f if it's the
alkalis the l1 norm it's not a problem
we know how to solve that is exactly in
linear time however if you go to more
complicated record rises for instance if
you consider structures project rise
which is written as a sum of simple
functions fi then the proximate of the
function f the sum is not necessarily
easy to compute even though we can
compute the proximity of each fi the
depressing is how can we explore this
structure to compute the proximate of
the Sun so here's a simple theorem which
says it's a known ziering it's not that
do to myself so if you take two
functions for simplicity you take f and
g two functions what is the proximate of
this son well it turns out to be the
parallel some of the proximal of F and
the proximate of G so this formula
computation respecting is not very
useful because you have these inversions
in formula and you can develop a
numerical procedure it's essential
iterative Amazon to compute the proximal
4f crusty using subroutines of the
proximal of F and the proximate of G
however if you practice stop rooting
into the Proxima creating angle you have
a two-loop procedure and if you balance
the errors carefully the convergence
rate can be very slow so this work is
motivated by two previous wonderful
results one is due to brain metal so
what they show that if you take the one
home and you take the total variation
seminar and the proximate of their sum
is just a composition of the tool
proximal maps and another interesting
result due to generating at all which is
about the grouping a group lasso where
you have the groups
I is a subset the variables and if they
form a certain system called the nominal
system then the proximal map again of
the some of these groups mi Nam's is
going to be compositional these proxima
maps and here the group seminar is just
a restriction of the LP nom into the
group in the subset GI so these two
results motivate the generalization
which is is it true that the proximal
the sum of F and G is just a competition
of the two proxim maps unfortunately the
short answer is no and you can build a
very single count example by taking two
linear subspaces here we have a blue
subspace and red subspaces and
approximating these cases just
projections into this subspace your mini
projections and therefore you can
represent the proximal as you know
symmetric matrix and if we compose two
symmetric matrices is not necessary to
be a symmetric right and therefore you
can show that does not even exist a
convex function edge whose proximal map
is just a competition so it's generally
not used for the prox decomposition
however if you are less ambitious to
require the accommodation the whole tour
certain functions of energy not all
function of f ng there might still be
some hope here we have three equations
which are essential and you know the
definition of the proximal map because
they were minimizing these functions we
just take the derivative and set to 0 we
get three optimality conditions and then
we add an ass to optimality conditions
we end up with two optimality conditions
and a little bit of comparison with two
optimality conditions you get the
sufficient condition for the proximity
of F prostitute is going to be the
competition approximate of F and
cosmology so in short the sufficient
condition says this if you do the
proximate of F then we only enlarge the
sub differential of G then we're
guaranteed to have the proxy competition
and unfortunately the sufficient
condition is not necessary at some
boundary points of the domain of the
function G and as you mentioned at a
special case of the serum peering a
proof of Joey tell so once we have the
sufficient condition now the rest easy
are we need to do is to find functions f
energy to satisfy the summation
condition in order to get the proxy
composition so here's a trivial result
so we fix one of the functions F and G
and then we require the prox
accommodation to hold for all functions
G or 4 of 19 the wealth of course as you
would suspect this will give us only
trivial solutions essentially in the
first case we are good in the constant
function for F and in the second case
we'll have a continuous linear function
for G so this reassures the
impossibility to have the party
competition in general however our show
you of two cases where we do get
interesting results so just to remind
you this is a sufficient condition we
have before so because many recognize
the politic are our seminars they are
positive homogeneous so we take a
positive homogeneous function G and this
is equivalent as requiring the sub
differential of the function G to be
invariant up to about positive screening
and because of the sufficient condition
all we need is the proximity of F to be
proportional to its input therefore we
can I have sufficient condition so the
work the remaining work is to catalyze
functions f whose proximal map is
proportional to its input and interns
that we have for cap equivalent
characterizations and I'll explain some
of them so the first two equivalents was
done by DiNozzo and shock of nausea and
lips so essentially the says if you have
a function f which is increasing
function of the Euclidean norm if and
only if for all perpendicular vectors x
and y FX plus y is bigger than ever why
so they use this result to capture the
prisoners dependent children in color
methods and now thanks to this serum we
have more characterizations of the
represented serum and the second
implication which is rachel says if ever
including functioning of the euclidean
norm then a proxima map of f plus any
positive homogeneous function copper is
going to be the composition that of
these two rocks and maps and one
particular example you will check the
function f to be the euclidean norm
squared and then you can compute the
proximal using this result and
particularly if copper is the l1 norm
you get the so called in a signal to
recognize of Joanna has to be so January
this serum says if you have if you add
an l2 issue regularizer to
then computing the proximal extension of
free to this arrow to each irregular
does not cause you any problem in
computing the proximal so this is
another result i mentioned before due to
generating at all so it's a group norm
and again can be covered in serum by
tech f to be the largest group and the
rest as a multi homogeneous function
couple and you do this iterative when
you get their serum for the euclidean
norm and they also prove the case for
the one norm and infinity norm which can
recover using a second result so here
again we have the self sufficient
condition so in this case in this case
we'll take a prima tation in varying
function f for instance you can take LP
norm which is mutating invariant and due
to the rearrangement incognita you can
show the proximal map of the environment
if a permutation invariant function is
going to be come on at all with this
input so here come at all means if you
look at the pairwise orderings the tool
of the two vectors they have the same
old information so using the sufficient
condition all we need is the sub
differential of the function G to be
invariant to the comment on oh come on
Tom vectors and we need to characterize
such functions it turns out these
functions are just the chakra integral
also known as a lot of extinction of
some set function so in short the chakra
integral is a generalization of them the
back integral into any more tongue set
function view which is not necessarily a
measure or charge so to put things
together we have this theorem which
states if you have a permutation
environment function such as the LP know
and if you have a function G which is a
shocker integral of some submodular
function then you have the product in
combination so here we need some
modularity because we need function g 2p
comebacks so again we have some
implications of this serum I will just
mention a result due to Frank's Park who
proved that the case for the function f
which is the l1 norm so here's a nice
trivial implication so this norm long as
an Oscar nom so it's just the sum of all
maxims of the power of the pairs and
this norm is used by boundary and ranch
for grouping features in statistical
applications and compute the proximate
of this now
is not entirely trivial and it's done it
was done in John Hancock and here using
this serum I show you before we can
easily compute the proctor map by
regrouping the law of the sounds so
essentially we decompose the Sun into a
sum of functions kappa i and then using
a serum you can show the proximal this
Oscar nom is going to be just the
competition of these Cup I functions and
because each cup of I plus 1 we only add
one more variable so given the previous
proxima we are going to need a constant
time to compute the next box or map
therefore if we compute the whole thing
we only need a linear time to do that so
so what if there's a vision so I show
you a sufficient condition which you
know is easy to improve and what if the
sufficient condition fails well there
are two results first a result due to
Martin's detail they show that if you're
wrecking right cetera so to ensuring the
assumption even if even though the proxy
contains not true you can still use them
in a submarine algorithm and you're
angling still can't guarantee to
converge so it's Sensen approximation
and another without due to myself is a
very simple linearization of the
proximal if you have approximately
similar functions you just need to just
need to compute it you just do a 10
expansion in sensor on it so it will
first order linear approximation of the
proximal of the Sun and then this
surprisingly this simple linear
approximation will give you a stricken
better result so to summary so we have
posted a question so we have two
functions F and G which are is that and
we also depends on whether or not to the
proximal of the Sun is going to be the
competition of the proximal and we
present a sufficient condition which
which extensional says if you do the
proximate of F we only nagy the sub
differential of the function G and we
identify two major cases where we can
show interesting non-trivial aprox
accommodations and these results are
immediately useful if you prac this
accommodation sing to the prox creating
amazon and will give you faster
algorithm for your problem sex
so we have time for a few questions
Michael microphone please maybe nuclear
well so do clean on this is not a
unitary invariant essentially microphone
so he's asking about the nuclear now so
essentially the decree no mr. you known
to a family of sakura unitarian variant
noms so essentially is a help you nor
the l1 norm on the singular values so
most of these days I'll extend to that
case all you need to do is require the
norms are you entering variant and you
need you just reduced to a vector case
the extension is il-1 known today and
you still just need G to be a lo vas
extension of a submodular function G
needs to be a spatula which is also a
long extension I think so you need to
enforce certain unit entering variance
in the function G as well ok another
question for yo young
so I have one so in the overlapping
group that's okay yeah you've shown at
43 based groups then use the clocks back
decomposes yes so other other sets of
books for which this is true this the
tree structures or equivalently the
lamina system is a large group I know
which you can show this result this is
another group in the essential if well
if you take a forest of extension if you
have this joint of these three strata
groups thanks Jojo but that's a naughty
school by now yeah so I don't know any
other structures cincy no more questions
so let's thank your yellow dog again
alright so those in a speaker is I shall
then and it's going to talk about non
you phone cameras check removal using
especially i baptise past minutes okay
thanks for the introduction so tight on
my presentation is non-uniform camera
removal using a spatially adaptive
sports penalty I'm Hedgehog down from
Duke University just a John were with at
Microsoft Research Asia so a problem
when consider here is the camera chic
blur which is actually caused by the
relative movement between the camera and
sin during the a pure period and
objective here is to recover the sharp
image from a single blurry of division
with unknowing camera shake this problem
is extremely challenging firstly because
it is well-known upholds the problem
even under to simplify the convolutional
of division model there are many pairs
of image and kernel can jointly explain
to observe the image equal well and the
fact that rogue board camera guy
actually specially wiring this makes the
problem even more challenging so here to
model the camera blur we use a
projective motion path model which
basically models the blurry image why as
a weighted average a projector we
transform the sharp image X so this
obsession of the observation model can
be written into the film that is linear
with respect to the combination week w
and hear each otha mean the dictionary d
would be a transform the champagne Minj
and also on the other hand this observer
model can be also written into the phone
that is linear with respect to the
unknown sharp image X and hear each atom
in the dick in the dictionary H will be
a localized doubler criminal assuming
Gaussian noise we will we can derive the
likelihood function from the observation
model and to elevate the elbows knees of
the problem image prior is typically
placed
 damn how sparse gradient image
prior and prior on the combination
weight W can also be used given the
likelihood n different priors a
straightforward approach would be take
the map estimation which is equivalent
to a standard regularize the regression
framework although this estimations
straightforward and simple this map has
the mission typically suffers from local
minima issue and often produced a nobler
solution which means explains a blurry
image as a as a blur image itself that
converter with a data solution in the
simplified of different model keys and
to make it work in practice different
empirical tricks such as initialization
and structure selection should be
included to make it achieve successful
deblurring here instead of direct map we
take a type 2 estimation with the same
likelihood function and a parameter is
the caution image prior with type 2
estimation we first to integrate out the
unknown sharp image X and they maximize
over all the other unknown variables
here a uniform prior on the combination
we parameter is used and this time too
as mission is equivalent to this
following minimization the cost function
the challenge here is that the lock
determine term is high dimension on how
to calculate we here introduce a
diagonal upper bound and this will
actually let us derive this final simple
cost function for now you'd volunteering
and the first term in this cost function
is a simple reconstruction error term
and the second leads us past penalty
function meaning that the function
dependent function fail here is a
concave non decreasing function of each
variable and the last term is a simple
noise noise level penalty term
so this made my vision the cost function
can be achieved using the standard major
ization minimization technique by
looking at these proposed the cost
function it has a very similar form as a
standard map has mission so we mean when
there was a real advantage early so to
see that we first go back to the second
challenge which is a real word camera
she really specially wearing this
special environment blur actually will
make the column of the matrix H
imbalanced the reason for this is that
each column each corresponds to a
localized blurred kernel and under under
the constraint that each localized blog
Colonel should have non negative
elements and the sample one tough a
large player will have a smaller l2 norm
therefore due to the spiritual
environment probably the blur columns
each will have different albums at the
effect that this will bias the image
recovery process and therefore affected
the overall transmission and back to our
model we mentioned that there is
automated column nom nom de addition
feature embedding we first note that
independent function there is a local
criminal in embedded this will actually
compensate for the spatially varying of
the blur to see to see this we first
they know X i multiplied by the Phnom
the local kernel with a new variable VI
and we can actually reform the cost
function on the top to the one on bottom
while the y on the top is a is a space
mission problem with unbalanced and
column the one on the bottom is a
standard sparse coding problem with
normalize the dictionary th tell her
this ultimately the columnar magician
feature will actually avoid premature
favoring a
Weidman have V over another and thus
avoid avoid the biased image recovery
and in recovery process only large
structure low blood region will be
natural info infra said the first this
ultimatum normalization feature is
crucial for non-uniform deep learning to
see this we use an example that compared
our result with with this feature
embedding and by removing the feature
you can see that by removing the feature
the algorithm can now achieving
successful deburring and this a color
monopolization feature is only the
principal first step towards non-uniform
deblurring and to move forward we we go
back to the first a challenge which is l
pose means to the problem to elevate
that your problem is the problem
typically spots penalty is placed over
the image and here we mentioned that
there actually two different effects of
the blur on the spaetzle maker the first
one that that blur will actually reduce
the signal sparsely therefore increase
the spa's penalty value and the second
effect is that blur will reduce the
overall signal variance therefore
decrease the specify a value for pendant
function that is not concave enough
meaning is not smart enough but its
second factor will actually dominate
meaning the spas panel function will
give a lower penalty function value over
the blur image therefore it will favor
the nobler solution based on this line
of reasoning to really favored the sharp
image over the blurry one a very concave
pennant function has to be used while
this is attractive conceptually it will
introduce a non convex optimization
problem and where the local minima would
be a great issue and with this analysis
we go back to our model the penalty
function in our proposed model
is firstly are qualified a very concave
smash penalty meaning as the noise level
approaching Darrow the penalty function
approaching a secure version of l0 known
therefore it will be helpful in a void
note the nobler solution and the second
property that the parent function can
adjust is shape according to the noise
level at the beginning when the noise
level is large the function parent
function is less concave and approaching
ass calvarium l1 norm therefore it will
has less problem with local minima and
as the noise level get reduced dependent
function becomes small concave and
therefore with those two properties
together eventually the noblest solution
will be avoided hopefully and along with
the estimation process the local minima
will be avoided progressively and for
the problem of commercial removal
initially as a noise level is large so
the parent function is less concave
therefore what the algorithm will be
emphasized hi blur regions and
first under large structures and low
blow regions and later as the noise
level get reduced to write it relative
concavity of the parent function is
increased and with this more fun videos
will be recovered just now we mentioned
the panel function is adjusting its
parent shape accounting according to the
noise level and the third property of
the proposed model is that it can
actually learn use a nice level
therefore make making the overall
algorithm is free of tuning parameter
which is desirable for a practical
problem like a dip luring and here we'll
show some experimental results using
real water images from later sure and we
compared with several state of odd
Messrs
and we note here that all the compare
the results are from the original ulcers
directly and this is a first attached
image the blurry image is from howling
napes 2010 here is our day blurred image
on the left that we show the estimate
the colonel patents actually show that
there's a camera rotation involved here
on the Left we compared with the result
from the original sir and this is
enlarged the region he can be seen from
the comparison that our temporary image
with sharper structures and this is
comparison with a messer by white CPRS
without 2010 here we can see that
although to post message can recover
very sharp image our recovery result has
less raining artifacts you know this is
another work in ECC v2010 by gupta at
all the result by gupta is has some
remaining blurry effects in some region
while has some ringing artifacts in
other region while our result has is
free from both effects hirsch neshaminy
and others have a recent worrying ictv
2011 we're here we compared with their
results our recovery image has actually
has more fun details and this is a
interesting comparison with a work by
cho chi sigma of 2010 the master of
jealousy is actually a camera shake
removal method using hardware assistance
by actually using inertial measurement
sensor and it is interesting to note
that although we don't any hardware
assistant our master can recover a sharp
image with even fewer artifacts than one
from
hardware systems and finally this is a
comparison with a very recent messer by
cho and i'll personally graphics 2012
the master pocho is a image pair based
method it requires too blurry image at
the input well our d blurred image is
using the only the first a blurry image
as input while postmaster can recover
reasonable a sharp image we can show
here that actually are measured even
with a single image at the input we can
cover better details such as this text
reading and to sum up we propose in this
work effective approach for non-uniform
camera shake removal with a simple and
clear cost function we then analyze the
model property including a automated
column nomina addition scheme which is a
crucial for non-uniform camera shaking
removal with this feature actually high
blur low structure region will be P
inverse at the first and will be
emphasized later progressively the
second property is not dependent on
motive a continuation and by learning
the noise level we can approach in
achieving a algorithm free tuning
parameter the proposal Messer can
achieve city state without performance
and reward blurred images and might be
equally applicable to other problems
such as structured dictionary learning
ok welcome to our poster tonight and
that's how thank you
so before we start questions could all
like spotlight presenters get closer to
the podium right now to avoid like
delays later on so questions for the
speaker
you know versus having ground truth data
do you you physically look at the
pictures and you look at regions with
sharpness and regions with blur we
actually also have a like a quantum
Abraham standard data set um like a
uniform deburring there's a dÃ©cidÃ©
balaban at all we can also achieve data
still on my circus terrible performance
here are you saw it might be more like
interesting to compare directly really
how different algorithm performs so yeah
so that online data set a ground truth
data set or is it a known good method
here all those test images are real bar
images so there's no ground truthful the
results shown here so when I'm just
thinking and it might be crazy but does
it make sense you could build some kind
of a camera rig right some half silvered
mirrors and things like that and send
the same lens to two different cameras
and you could create ground truth that
way potentially yeah you could
quantitatively do sort of a
least-squares comparison or something
yeah yeah I first I agree with your
suggestion and another point is that let
some Airmen likely square is actually
kind of inconsistent with a visual
perception so yeah that's why we didn't
pursue a non-uniform tested this that
quantity directory so yeah thank you
although question for the speaker
yes about that
oh you me a failure case yeah yeah the
question was are there any free do keys
yeah definitely there there are some
feeder cases because uh you know the
camera shake removal is extremely hard
problem especially when you consider is
an uninformed one so there are
definitely some feeder cases and what I
show here that those images are standard
test images and when I capture a real
photograph with uh with a like a
handheld camera duh obviously there are
many unknown factors that will cause the
algorithm field such as the like the
very hetero region due to the lights and
when you take the take the picture in
low light setting there are high noise
so I need those situations because the
current model doesn't model those
factors animal factors it will cause the
algorithm feel ya thank you okay so
let's thank the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>