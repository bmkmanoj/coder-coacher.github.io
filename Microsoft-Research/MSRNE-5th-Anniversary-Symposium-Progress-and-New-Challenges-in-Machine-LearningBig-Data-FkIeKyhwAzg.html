<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSRNE 5th Anniversary Symposium - Progress and New Challenges in Machine Learning/Big Data | Coder Coacher - Coaching Coders</title><meta content="MSRNE 5th Anniversary Symposium - Progress and New Challenges in Machine Learning/Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSRNE 5th Anniversary Symposium - Progress and New Challenges in Machine Learning/Big Data</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FkIeKyhwAzg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay our next session is shared by Shaam
kakara he is a machine learning
researcher in our lap yeah so as Peter
Lee and others mentioned there's really
been tremendous excitement in progress
in machine learning and our next session
is going to bring together a number of
interesting researchers who are looking
at some of the challenges we're going to
face in machine learning and many of
these challenges at their core might not
even be machine learning questions so to
start us off we have Antonio taraba who
has really been doing a lot of very
thought-provoking work on you might you
might think there's just visual
perception is a lot of a lot of his work
thank you so I have 12 minutes 50s
lights and a strong Spanish accent so
what white have around here okay so
there is this is a really exciting time
for computer vision there are lots of
different applications you can find that
use vision to to sell products like
connect or digital cameras now you can
have face detection and was really
really well so people get really excited
on your students community to the field
and I want to give you a sense of what
is the you know the first experiences
for students that have starts doing
computer vision so it sees that there
are lots of different models for of your
recognition he says oh wow I going to
start see the lots of code compiles it
doesn't work then compares again it
works then he picks lattice head and he
says okay I'm going to train my object
detection system to detect something and
then it takes an image it runs the
detector and it's a scar okay so you
know who is to blame here is the student
incompetent or you know maybe maybe
there is a car there so who is to blame
here and there are a number of things
that go into place in order to make all
the systems world representation the
features the training set that you use
the machine learning algorithms that you
use so we are going to look one by
who is to blame in this failure or maybe
it's just student the way to flip so how
it works object detection so let's look
at the representation first the master
sexual systems 44 up the detection what
they do is what we call the bounding box
they take an image let's say that you
are looking for faces you take a box out
of this picture and you ask the question
is this a face yes or not and in order
to learn your strikes on the scriptures
from here and then you apply just a
function that will just look at this box
and then if the face is not there just
move one pixel and you're addressing
question and then one more pixel and one
more pixel and until you cover the
entire picture and if you don't even
find the face it's not because they face
is not there maybe you just got the
scale wrong so you tries a different
size and you keep doing this over and
over again so this is very different
from how humans do object detection so
just to illustrate that let's look at
this video this is a very low resolution
video and you can still tell everything
that is going on here this is about
twenty pixels worth of vertical
information so you were just to look at
this ball boxes here you wouldn't be
able to tell anything and yet you can
recognize everything that goes on so
this is just a normal day in my office
when I was a postdoc this was my
calligrapher goose and so while how are
you able to recognize all these things
well because do you actually don't just
look at boxes you put everything in
correspondence and you do seem reasoning
and in fact when you look at this video
are putting into the interpretation a
lot more under is inside so let's look
at the high resolution video here it is
so you can see you know that there are a
few themes this is the type of things
that we are doing in the office
that's not really listen music anyway I
think there is some progress in Mouse's
recently anyway so the representation is
wrong and there is a lot of research
trying to resolve as I'm not going to go
there but let's move to the next thing
the features the features so why is that
we get this car here okay what is that
the detector sees if you look at that
this is the patch this is whatever the
detector is going to see just this piece
of the image and what you do is you will
apply to the patch some descriptors and
there are lots of different descriptors
that exist in the computer vision
community here I just show a few of them
you can see that have four letters three
letters there are all kinds of acronyms
so the most popular one right now is
hook a hook is something you know the
extraction orientations edges and it
applies some kind of complicated
non-linearity to the picture so this is
the recipe to compute hot you take the
image you do a bunch of stuff and then
at the end you get a vector for each
window that will tell you something
about that window so how do we know what
the tectonic policies this is the
typical visualization that people will
use to try to get a sense of what this
descriptive sees on the picture you have
this picture and well you don't really
do this there are a lot of
nonlinearities so people generally show
things like this and what they are
showing it at each location what is the
dominant orientation this is actually a
representation of all the numbers that
they are on the hog descriptor is just
not very clear what is going on here so
two of the students in my lab Carl
Aditya they were really working on this
and they wonder ok what is that this
detector actually gets to see on this
picture so what they tried was to see if
you could take this and actually try to
recover the input image she's a very
nonlinear problem so it's not easy to do
but they were really hard and at the end
they got something out and this is what
they got back so you can see that is a
nice bow tonight actor it looks like the
water and it also reveals some of the
things that the descriptor is doing like
you can see that there is almost no
contrast in this part of the picture but
in the reconstruction you can
see the back of the bottle showing that
this descriptor is actually doing
contrast normalization all kinds of
things that we know are very important
for recognition but what is else going
on maybe there is there are some
advantages but maybe there is something
lost in this process and we don't know
what it is so what they did was to take
the detector a detector trained to
recognize objects based on this
descriptor and they're running on images
and they pick them all the places where
the detector had high confidence that
the object that they were looking for
was there so what I'm showing here are
the dispatches of the images where the
victim fire and then I'm showing the
reconstruction of those descriptors so
you can see here there are a mixture of
things that are right and wrong and can
you tell which these are actually wrong
is pretty hard three guesses which one
is a false alarm the detector throw the
object is there is not there they are
all wrong these are all false alarms but
when you look at them they look very not
bad you can even make stories about them
like here is somebody look into a mirror
and somebody walking and you can tell
stories all all these objects and people
agree about those things but they are
all wrong so it's just nuts so here is
what is going on now we can go back to
our original picture where the students
are okay this is not working you can
look at the patch and you know there is
no reason why this should fail why was
there a car there you can you know look
what scientists normally see this is
what they see is a still general totally
clueless but if you look at what the
feature actally sees this is what it
sees and he actually looks like a car so
the algorithm is working there is a car
there
and the funny thing is that you can
actually look at the performance is that
people will get in guessing the presence
or absence of an object by looking at
this as these reconstructions and you
can see that the performance that they
that they achieve is basically the same
performance is that the best classifier
you can put the than a classifier that
you can put all the descriptive
themselves I mean this is just some
curves so in something and the red is
humans this is the classifier and they
are just doing a slightly better by
looking at the Rec instructions so there
is not a lot of room of improvement that
you can achieve by improving the
detector did you maybe you can just
bridge this gap but it is really not a
lot you can do so the features have to
blame in this case so what else is to
blame well there is another arbitrary
choice which is the training set which
images did you use to train your
algorithm and there is a lot of effort
nowadays in computer vision if you try
to build really big databases of images
that you can use to train algorithms and
to evaluate them and there are two
things that you need to do you need to
collect images and you need to label
then you need to put ground truth to
them so how do you collect images there
are many different ways and each way
high seas all problems one of the most
popular ones is to go to the web and
collect images by you know googling them
or something so let's say that you are
looking for max do you go to Google
require it for mac and this is what you
get and you can see this is not a random
set of mugs they all have the handle on
the right beside maybe three that have
the handle on the left so why is that I
mean this is not an unbiased set or you
can go and look four bedrooms and these
are the pictures that you get back and
you know this is not a random set of
bedrooms for first the the bet is always
present in the picture okay that's fine
you know but look at the bed how
organized it is know how clean I mean
this is not a random set of beds you can
look for a student bedroom and you get
the steal this and we no steal the
bedrooms and on like this okay this is
not how my bedroom looks like of course
I couldn't post any picture of my
bedroom on them on the web this is the
kind of thing that we look for
this is a real estate and bedroom ok so
that's one thing and the other thing is
image annotation we need to get the
images label and one very popular way of
doing it is by crowdsourcing consumption
is now a very useful tool if you wanna
be a very big database and then a number
of crowdsourcing tools for image
annotation like one is labeled me which
we create it in 2005 that allows people
to go to the web and deletion annotation
to like to trace the boundary of objects
and that gives us information about all
the objects that we can label the data
question of the picture and there is
also Amazon Mechanical Turk which is
this very popular website that you can
post a small tasks and people will do
them for general one cent or something
like that and it's very useful in
computer vision a lot of people are
using them but the question is when you
put all these tax on the web who is
actually doing the work no because there
is a lot of random people go in there
and if you look at the plots of how many
tasks do each worker if you do something
like this so these plots have ranking
workers by the amount of water they do
so the vertical axis is the amount of
task completed by each worker and this
is you know rank order in this list you
get to see this always this power laws
this is this happens in Amazon
Mechanical Turk and also enable me you
get to see the same distributions very
robust so it's like that is one guy here
the real is doing all the work and then
all the other just having a little bit
of fun and moving in it's something else
so my suggestion is let's just hired at
time that guy there so I have no idea
who is in Amazon Mechanical Turk but I
know who is a label me and that's my
mother my mother has labeled a lot of
things so here is a random working on
Amazon Mechanical turn label in an image
you know there are somethings are useful
but there is a lot of noise and this is
my mother ok if she really does
beautiful work look all the gold of just
label things in the back is just
beautiful you know here you think this
image you put it on Amazon Mechanical
car and somebody will say Welsh binders
my mother was born by one you know label
them on ok of course there are a few
things that she refuses to label like
this you here anyway she has label a lot
of a lot of things and just to put my
mother board
in context let's look at some of the
most famous database is out there like
Pascal which is a very important
database in computer vision the way it's
been labeled is to take 10 readily the
students you close them in a room and
you say you cannot get out of here and
you have level of these images it's very
successful strategy so they have they
have been working for about 10 days or
so and they got labels about 28,000
bounding boxes so this is given an image
you put a rectangle on top of the object
and you say what the object is then
imaginate another very famous database
and this was this has been labeled with
Amazon Mechanical Turk they have more
than 25,000 workers going on here and
most of the annotations are single
images with one word sayin if the object
expression there or not and this is my
mother my mother has labeled more than
three well what about a million of
objects but this is not just putting one
water abandon box this is full
segmentation look at all the clicks here
if you can the number of clicks this is
comparable to about 10,000 amazon
mechanical work turkish so yeah she's
done a lot of words i'm looking for my
parents if you have any desire anyway
the conclusions is who is to blame about
these things will actually everybody
there is a lot of project that needs to
be made still it's a really exciting
time for computer vision there are lots
of things to do but fortunately you
still have a lot of work to do otherwise
you know I'm out of business and and my
mother yeah so I'm not talking about the
learning everything that's for the
machine learning people to do so thank
you for your attention
yes where my mother lives she lives in
Palma Majorca in the Balearic Islands so
she has better things to do than label
in emergencies yes if somebody has a
study oh I see if it helps to do to get
your brain better I guess no i don't
know if anybody has to study that but my
mother is learning English professed so
I guess it's helping my mother in some
sense she was playing a lot of solitaire
so I thought that she could do this yeah
oh no
and actually you're evaluating
okay so ensure the question is if there
is a way of defining a good gold
standard for up the detection and if you
inject in the trainings had more
variability of you know different
viewpoints of marks if performance get
better that's the summary of the
question and well the problem is also
the test set if the test set generally
these data sets are done for both
training and evaluating the performances
and if your training set has a lot of
richness and so on but then you are
testing it on upward on a pure on a poor
a benchmark then you might actually get
hurt by by learning from a too generic
that has helped but the idea thing will
be to have had also tested that is very
very very genetic and it's just very
hard to sorry we can talk later it's a
it's a complicated problem but I agree
with your Pickers Hannah Wallach hannah
has had an interesting trajectory in
that she started as a machine learning
person and become an expert in the area
and has now been addressing a lot of
social science question with these
different techniques all right great hi
everyone I want to start by saying that
my mother is not so detail-oriented in
fact she has recently joined a
contemporary interpretive dance troupe
for the elderly and I feel like my talk
is therefore more in that style than
perhaps some of the more mathematically
oriented things today anyhow with that
out of the way I'm Hannah I'm a
professor at UMass Amherst and I'm also
a member of you masses interdisciplinary
computational social science research
initiative so as charm said my
background is in machine learning and
you know I spent my 20s developing all
kinds of very technical algorithms for
all kinds of things but more recently
I've been working on machine learning
methods for analyzing complex social
processes um that's that I'm not
actually going to be talking about my
specific research questions today
instead what I'm going to be doing is
I'm going to be talking about four
challenges intended as talking points
that I think really lie at the heart of
computational social science
the first of these challenges Jeanette
Wang actually stole my thunder on this
one so um you've kind of heard a bit
about already and it's really intended
for everyone working in this area of
computational social science the last
three are sort of more oriented at
machine learning researchers and
computer scientists so computational
social science is inherently
interdisciplinary and in order to make
truly groundbreaking advances
collaboration is absolutely necessary
social scientists provide all kinds of
important context into the most
pertinent research questions data
sources acquisition methods and things
like that while computer scientists and
statisticians provide significant
expertise in developing new mathematical
models and computational tools when I
first started working in this area I
felt as if there was maybe a disconnect
between the research done by computer
scientists and the research done by
social scientists and I kept overhearing
conversations between people from these
two groups that involved the sentence I
don't get it why is that a research
question so I'm just going to launch
right in with my first challenge which
is establishing a common language for
interdisciplinary communication in order
to establish productive collaborations
between researchers with really
disparate backgrounds it's absolutely
imperative that those researchers
understand each other's cultural norms
each other's values each other's
research goals and each other's
methodological frameworks and this might
sound simple but in fact it's a really
non-trivial task people trained for
years in order to understand their
chosen discipline and its culture and
values and so there's no reason why we
should be able to launch in and
immediately understand those things from
another discipline overnight so as a
result I think it's really important
that researchers in computational social
science really work on creating a common
language by listening and talking to one
another by participating in you know
research institutes such as Microsoft
Research here in New England but also
seeking out informal conversations in
today
the plenary workshops seminar series and
even publication venues so establishing
a common language is really especially
important when the objects of study are
as widespread and diverse as complex
social processes sure you know informal
collaborations and conversations between
friends or colleagues or social
processes but so are the activities of
entire corporations or entire government
organizations so as an example consider
the US patent system this is actually
the pattern for the glass staircase and
every single Apple store why not so the
US patent system aims to promote
industrial and technological progress to
strengthen the economy to protect
intellectual property and so on and
there are a whole range of people
involved in achieving these aims you've
got inventors you've got assignees
you've got patent examiner's patent
attorneys each of whom have different
goals and in some cases their goals
might even be explicitly contradictory
so patent attorneys want to make sure
that their clients aims are expansive as
possible while part of a patent
examiner's job is to make sure that
claims are not overly expansive and
there are a ton of different types of
information arising in this entire
complex social process and all these
interactions as you can kind of see from
this you have things like dates you have
addresses you have categories you have
names and related literature related
patents as well as the text of the
patents themselves parts of which are
written in scientific language and parts
of which are written in legal language
and intended deliberately to be obtuse
kotori so what if anything do complex
social processes have in common well
they all consist of individuals or
groups of individuals interacting in
order to achieve specific and sometimes
contradictory goals but there's more
than just that they all possess three
common characteristics the first of
which is structure who's interacting
with whom the second of which is
associated content in other words
information used in or arising from
these into
or actions and this might include
categories or financial data and other
kinds of numerical data but it can also
include texts and documents and informal
communications written by people to
interact with other people not written
for researchers to study and finally
most social processes exhibit dynamics
of some sort their structure or content
or both can change over time or space
and these changes can be gradual or
sudden and either permanent or transient
and this brings me to my second
challenge developing models that
simultaneously capture multiple
modalities of the same process moving
beyond static snapshots of interaction
structure models of low-dimensional
metadata and other kinds of simple
temporal changes in order to build
accurate models of complex social
processes we need to capture their
nuanced multi-faceted nature and this is
where kind of rich textual content and
complex temporal or spatial dynamics can
really reveal important latent
information that that is crucial to
reasoning about those complex social
processes furthermore these three things
structure content and dynamics are not
independent and so it's really necessary
to build integrated joint models so one
of the biggest areas of disconnect
between computer scientists and social
scientists is the end goal of any
research project and this disconnect is
nicely summarized in this quote by Gary
King and Dan Hopkins I'm going to read
it to you policymakers or computer
scientists might be interested in
finding the needle in the haystack such
as a potential terrorist threat or the
right web page to display from a search
but social scientists are more commonly
interested in characterizing the
haystack since reading this quote I've
done a bunch of thinking about the
different types of modeling tasks
undertaken by computer scientists and
social scientists and I think they fall
into three categories the first is
prediction and in general it's computer
scientists or decision-makers who are
most interested in predictive tasks that
said there's increasing interest from so
shull scientists because you can use
predictive tasks even if they're not
directly related to your end goal to
validate models that will be
subsequently used for something else the
second is explanation so here the focus
is on why questions in other words
finding plausible or probable
explanations for the observed data and
in general its social scientists who are
most usually interested in explanatory
tasks and finally the third is
exploration so exploration is all about
uncovering patterns in beta usually
patterns that we don't already know
about a modeling tasks are falling to
this category are also kind of
characterizing the haystack tasks
although contrary to the quote by King
and Hopkins both computer scientists and
social scientists are interested in
exploration though interestingly neither
one quite prioritizes it as a
first-order modeling task um so my third
challenge and this is specifically for
machine learning researchers really is
there for developing models that are not
just black box prediction engines but
instead facilitate explanatory and
exploratory analyses as well as
predictive analyses and in order to do
this such models really need to encode
modeling assumptions so that conclusions
can be drawn in an appropriate context
they need to represent and maintain
uncertainty and they need to have
interpretable model structure the
framework of hierarchical Bayesian
latent variable modeling is particularly
well-suited to these kinds of tasks so
my final challenge is a little
controversial given the title of this
panel that's kind of focusing on big
data but nonetheless I think it's really
important I just spent a week at a
conference in the UK discussing these
issues with a bunch of social scientists
so I wanted to bring it up um with in
computer science there's a ton of
enthusiasm for big data at the moment
and this is totally unsurprising and
completely reasonable we finally have
the kind of computational infrastructure
that supports the analysis of really
massive data sets and this brings up
some really exciting computational
challenges but here's the thing
many social scientists especially social
scientists in academia don't have big
data or at least not big data in the way
computer scientists usually think about
it sometimes this is because they're
trying to answer questions for which big
data just doesn't exist for example
consider political scientist trying to
look at the US Supreme Court opinions
that's the opinions of nine people you
just you're just not going to get 10,000
people there as another example
sometimes social scientists have many
different small data sets which each one
has like a different emphasis but when
combined they reveal a complete picture
of the under or more complete picture of
the underlying social process so here
consider for example publications and
patents are raising from the same
institution publications are useful
patents are useful but when you combine
them together and work at you know look
at differences in kind of language usage
and stuff like that you actually get a
much more complete picture of
technological innovation from that
particular institution and then some
social scientists have longitudinal data
so data collected regarding a small
number of entities over a long period of
time at different intervals and again
that's something that needs to be
considered and then finally my favorite
many social scientists work with what I
call artisanal data so these are data
sets that have been carefully selected
hand labeled or curated in some other
way by a group of experts for some
specific purpose and its really really
important than our enthusiasm for big
data we don't forget about those as well
so challenge number four is really this
develop models for composite
longitudinal artisanal and small data as
well as models for big data and without
i'm going to start because i'm over time
but computational social science is
awesome and MSR is doing really great
work in this area that's it
okay yes right exactly okay so um okay
sorry the question was could you give an
example of an artisanal data set so one
example vote this kind of came up last
week because I was talking to some
political scientist who were analyzing a
differences in opinion between of
politicians in the primaries versus the
general election so how people kind of
change their opinions and go back on
stuff that they said previously and the
particular data set that they were
looking at they had had political
scientists sit down and hand annotate
certain opinions and whether subsequent
opinions contradicted them and stuff
like that so that's the kind of thing
that I'm thinking about that yeah sure
okay so so here's the thing i think
there's this tendency so here's what the
issue is i think with a large data set
it can sometimes be relatively easy to
learn about coarse-grained things from
that data you know you can look at a
massive data set and draw these very
coarse-grained conclusions about a large
population of people and stuff like that
but when working with a much smaller and
you can do that using a pretty simple
model or pretty pretty standard
statistical techniques I think when
you're working with a much smaller data
set you still want to be able to extract
signal from noise but often that signal
is much finer grained sure
so I don't know if that's quite true
here's why so I've been doing a lot of
work with social scientists who might
have only 100 documents that they want
to apply machine learning and natural
language techniques to your right that
machine learning and natural language
techniques were often you know pioneered
for small ish data sets but we're still
talking on the order of thousands not
100 or 50 and so I'm really interested
in sort of thinking about some of the
challenges that arise when you do have a
data set that is small enough for a
single human to sit down and look at but
there might be things in that data set
that a human with their own particular
biases and background might miss so why
don't we so a next speaker yes Annette
speaker is David Ross child David is an
applied economist who has been looking a
lot of interesting forecasting problems
he is now at our sister lab in New York
for everyone in New York thanking
Jennifer and Christian for us being here
today they opened up the New York lab a
little over a year ago and we're just
thrilled to be here and be part of this
community and today I'm going to talk
about polling lay people and it's going
to be a little bit about forecasting and
indicators this year is what we had up
as our indication for what was going to
happen in the 2012 election in February
of 2012 we did pretty well these are
probabilities and the darker Reds met
their very likely for Romney and the
darker blue is very likely for Obama and
Florida we were a little off on but we
had that pink anyways and this comes
from data that we pulled a lot of it was
from fundamental data these are kind of
things about past election results and
and who is incumbent and economic
indicators and during the course of an
election cycle we kind of change over to
other types of data polls prediction
markets social media data and what I'm
going to talked about today is kind of
how that's going to be changing in the
near future and what we're doing about
it because maps like the
in some ways are going to easier to make
in some ways they're going to get harder
to make and what I want to keep in mind
as I talk today are two different
stakeholders one the business and it's
weird to think about if politics but the
business is a six billion dollar
industry and add basically an ad
campaign that lasts about a year year
and a half it's continuously flowing but
anything I talked about today with
Romney and Obama you can picture Ford
and Toyota or Apple and Microsoft it's
all the same and then you can also think
about the research angle which is that
it's not just one side is is providing
the efficiency that a good indicator
provides to a business or a company the
other side is thinking well what do we
really learn about why things change and
how things change over time and thinking
about it from the kind of the research
angle as well interested to both parties
and we have two main stakeholders and
and I kind of think about four main kind
of key goals when when I describe an
indicator or forecast to somebody and
the four of them are going to be here
it's going to be making sure that's
relevant timely accurate and
cost-effective and scalable these all
seem like painfully obvious and if this
was different talk I can go into how
many examples of how horrible we are and
thinking about these main goals whether
or not it's relevant you know in
elections we generally tend to talk
about the latest poll which is just a
raw data point that looks something like
an expected vote share which is not the
probability of victory that most people
care about timely we don't make
forecasts when they're most necessary to
make them when they're most convenient
accurate that's a tough one to describe
for some people but you know people
think about the simplest kind of mean
absolute errors and types of things
rather than thinking about how well
something's calibrated and whether or
not its robust to describe in the future
or describing the past and of course
scalable cost-effective which is what
all this machine learning big data
everything is going to open up as the
ability to answer tons of new questions
and tons of new domains but of course
coming from academia you know if I spent
1 billion dollars and made it infa
testimony significantly significantly
significant people are like that's
awesome totally forgetting and not
caring about
scalability to practical implications so
talk about a little bout today this is
some of the data which which I generally
deal with on a regular basis and the
challenge is thinking you know how do we
put this together to make things that's
sing for people and in the past it's not
that hard in some cases and some kids is
a little harder but you know you look at
something like a GOP primary this is
kind of aggregated from poles and some
fundamental data and a little bit from
the prediction markets and it shows this
is the the general support for
candidates any given time you know it
bounced back and forth and this is the
probability of victory at any given time
this goes to this question of relevancy
and thinking about what you need to do
these are often kind of traditional data
sources and these paint very different
pictures this is what the news media
focused in on more this is what I think
we should have been talking about a lot
more because this is what was most
relevant to most people Romney was going
to win the whole time seems meaningful
and and when you thought thinking about
things like kind of how new data set
start playing into it it's thinking
about well in the past it was very hard
to kind of get timely update some things
because polling is done every few days
it takes a while prediction markets
social media data help you kind of
construct things that say so let's say
we all started at the same point so this
is saying at the beginning of the debate
you start at 0 this is the movement of
the likelihood of obama winning the
election as the three major debates
moved this is this granularity is
extremely important to answering
questions about the impact of events and
the timeliness is extremely important in
people who are making investments are
moving money around based off of how
things are going but since i have so
little time i'm not going to talk about
all this stuff which is kind of standard
stuff i'm going to be talking about
power how we're basing the future and
how things are gonna change with online
data and kind of experimental polling
and predictions and i'll start out by by
posing the challenge the challenge is is
that the main data source for politics
and in a huge amount of market
intelligence has been representative
polling representative polling you've
got a random sample of a representative
group of people it used to be very high
amount of people you can contact and a
high amount of people then responded
that is is almost gone now so you're
down to less than ten percent of people
that you've randomly
chosen to poll actually enter the poll
this cuts down the Cystic Oh benefit of
the randomness it also makes
increasingly expensive to do this
representative polling basically is
dominant because of the literary digest
failure of 1936 freaks people out
literally die just went and tried to
just interview as many people as
possible they didn't do anything to
correct for it and they had a horribly
terrible poll they interviewed two
million people out of 10 million people
they tried to get these are mainly car
owners and subscribers literary digest
and they were much more likely to be
Republicans and they predicted a land
and victory was about won in a landslide
but times have changed since 1936 and
there's new technology shocking as that
sounds and it's increasingly cheap to
get non-representative people that took
months for literary digest I could do it
in days to contact millions of people if
I really need it to and these are out of
order for some reason but we know that
non-representative polls if you can
unleash the power of them we know we can
ask the right questions we know we can
get people quickly we know it's from a
cost effective because it's pennies on
the dollar compared to representative
polling the questions can you make it
accurate that's the challenge and then
we're making that accurate and we're
trying to visit by kind of four
different things we're hitting it on
kind of itself incentivizing the right
people to answer the polls thinking
about new questions including graphical
interfaces that can reveal new
information from people thinking about
new ways to aggregate that data and
thinking about new incentive structures
to get people to respond truthfully and
meaningfully and I'll give one example
here this is work that I did with Justin
Wolfers at who is now at Michigan and we
basically said well maybe we can ask you
different questions so we had the st. we
found examples 345 times same people
same time or asked who you're going to
vote for an election who do you expect
to win when the polls pointed in
different directions when people
expected someone that they didn't intend
seventy-five percent of time the
expectation pointed to the winner ok so
just thinking about that and what we
were able to do is then so we went on to
xbox and we asked people who they
expected to win their state and 51 out
of 51 states including the district
Columbia point in the correct traction
people expect it the eventual winner in
their state okay so that was a really
simple
wedges ask the right question and you
start tackling that problem and then we
get some nice press for it because this
is like a screenshot I don't know how
much to add like that would cost but
that's pretty cool also graphical
interfaces right so we're like can we
get really complicated things maybe to
get revealed things that would be even
harder than you would think normal pole
that can actually do something this is
work with Dan Goldstein we showed one
hundred a hundred digits between one and
ten two people that were come from six
different distributions you can see on
top and then when we aggregate up their
responses when we asked them to reveal
the distribution they saw we using this
new graphical interfaces we've been
working with we're getting results like
this which is not perfect but people are
actually kind of revealing that they
they really got the distribution you can
see a little skew in there some kurtosis
I mean they're hitting all the key
moments right so that's kind of cool but
let me jump back into kind of an even
bigger project which is this full
breadth of this xbox project we
interviewed three hundred fifty thousand
users they took 750,000 polls which are
three to five questions long over the
last 45 days of the election and we had
a lot of repeat users so we'd massive
quantity and we had massive panel we had
five thousand users who came back at
least 15 times or more in 45 days to
answer questions from us and so we can
see how they moved standard polling
basically has this idea that you just
let the data flow through you chop off
the chunk of the people who were not
likely voters or if you think about this
in terms of market not likely shoppers
whatever it is drop them off just say
what did the polos all say what we said
is that we want to use new tools in
order to allow different cells to inform
each other so essentially we broke
everyone down into their demographics we
broke down into 6,500 different cells of
combinations of demographics and we said
what's the likelihood of any cell
pulling for any given count in any given
day and that then post stratifying that
over a set of what we assumed the likely
voters to be we'd use something very
simple which was the 2008 exit polls we
did the same thing with 2012 exit polls
you get the same thing but we wanted to
show that it's robust to two simple
choices key thing here number one is
that we
panel which I'll talk about a second but
number two is that we get 20,000 people
a day so even though we had very little
amount of over 65 year old women we
still had you know four times as many
than say like Gallup gets on a regular
basis and what we able to do and you
knock me to see this but I'm comparing
our results to pollsters results these
are kind of mean pull aggregating things
on state by state level we show a lot of
robustness kind of a little cleaner
thing to show is that what I broke down
here is that on the x-axis this is our
estimate for any combination of
demographics too so like say over 65
year old women or white males in a
certain age group whatever whatever it
is and this is their actual to party
vote share so this is breaking down all
these subgroups I'm able to show that
our estimates were actually pretty on
par sitting on that 45 degree line is
the goal and so step one is to say we're
seeing something that looks on par and
just remember there's no ground truth
here so it's a tricky question that
sense that we don't actually know
baseline levels of support we don't
actually know the likelihood of obama
winning the election on october second
or october 10 all we know is would
happen on election day but similar to
the main stuff that's good and then what
we see here is on the on the left here
this is the percentage of democrats
after accounting for demographics who
showed up in our polls and this is how
our top line pole moved and this was
very similar to gallup you and
everything else I mean we SAT then we
thought we said these look these look
really similar these looks shockingly
similar and from that we were able to
determine things like well it's actually
this partisan non-response that's
driving a huge amount of this movement
on these polls that are making all of
this investment decisions they're making
all this noise essentially what's
happening is this Democrats were really
sad after the first debate and like I'm
not going to answer a poll so when if
you assume that a polling company was
looking for like a white male in a
certain age group or whatever like that
what they end up it's just being
more likely that a Republican is going
to answer on the days where the
Democrats are sad and the slightly more
likely to Democrats going to answer the
days days that and what happens is if
you kind of look it also the movement
between that we see this is the panel
aspect comes in we get
to see about you know where people are
actually switching where would a
tournament is so rare you get to use pie
charts in academic talks so it's pretty
stoked about this I just want to make
this really clear that if you looked at
the total shift that happened during
election cycle but about two-thirds of
it is being driven by people not
answering the polls which is very
loosely correlated with not likely to
vote but it's actually very little and
that one-third that's a shift in support
actually the vast majority of it is a
shift from like other to a major
candidate or between a major county and
other essentially the kind of these time
shifts that aren't that meaningful is
this tiny portion of a tiny portion
there actually people moving between
candidates the thing we think of when we
see these gigantic shifts and I'm at a
time through convention stuff and so
I'll leave it at one more thing and that
the real challenge then comes down to us
can we start applying this to this
another set of polls which I really care
about which is social media data and
online data which I look at is saying I
have a pole I got a huh couple hundred
million respondents a day we sorta know
their demographics we sorted another
question can we start applying these
same techniques to start breaking down
what is unrepresentative data that moves
around has all these unique things to
actually answer questions that we care
about so thank you
yeah so yeah so the question is about
about artisinal data and pulling and and
big data and the answer really comes
down to is that most of the stuff I
showed you and most of stuff we're using
really is I think falls under the idea
of artisanal data in the sense that it's
very small data sets specifically
collected to fill fill answers because
you look at these massive data sets and
you're not sure what the question is
you're not sure who's on it and the the
amount of work to possibly answer some
of the questions like predictions would
be devastating compared to actually
going out there and and directly
answering directly asking people to
answer the question you care about
that's not to say though that we don't
want to kind of break that key open that
up and if we could do that then you have
this cost-effective way to answer all
sorts of questions all sorts of the
mains but until we get to that point we
still need to go out and actively
collect small amounts of data that very
efficiently answer the questions we care
about for a final speaker we have it's
eric is whenever own at Microsoft
Research it's really a pleasure to have
Eric here because he's been one of the
people throughout his career who is
really been making in fundamental
contributions toward you know pushing us
toward a I looking at decision making
under certainty and you know fundamental
reasoning questions some so I thought
I'd change gears and focal length here a
bit and answer the question that Tom
asked me to think about which was in 15
minutes or less what is progress being
in machine learning in what our future
directions so it's a it's a quite an
interesting a challenge I thought I'd
stop by reflecting back about 30 last 30
years which I found it very very
interesting and exciting time in
particular it's a time when the
computational lens the lens of actually
or the goal of implementing procedures
and algorithms representations has
increased our learning about probability
theoretically the foundations of
representing probability distributions
for example and a lot of this work was
done by people pursuing machine
intelligence what does it mean
to reason under uncertainty to represent
knowledge what does it mean to to reason
tractable and to learn from data and to
do science and one of the reputational
advances and I think of cetacean is a
theme for moving forward into the future
as well has been moving to computational
graph theoretic models of probability in
particular graphical models directed
graphs for example capturing
distinctions one cares about in the
world and the dependencies in
representing them and coming up with
tractable procedures in fact redman lab
results about a decade ago proved some
of these even though greedy search and
greedy in nature were optimal it would
find the optimal way to express at the
optimal graphical model to express a
probability distribution and loves us to
build models that can actually be used
to diagnose illness for example in
healthcare from large sets of data
moving ahead from directed models to
undirected graphs another shift of
representation which has been very
interesting for the field and the link
back to Peter Winkler's talk these
presentations including Markov random
fields and conditional random fields are
very akin to icing models in statistical
physics and from my point of view these
models have actually reputations have
actually given us a little bit of magic
that's jumped out the ability to
represent simultaneously and jointly
different kinds of information for
example taking a a personal photo album
and folding in events and time and
locations and all of a sudden to triple
the accuracy with representing
individual faces in the whole library
for example and that these kinds of
undirected graphs and methods were used
in some innovations at our Cambridge lab
in for example segmenting out background
from foreground what was there a street
versus a road so I wrote or sky or
building and this cut myth methodology
was was actually the ancestor of some of
the work we've heard about it with the
Kinect work meeting changing consumer
expectations as
what computation might be doing moving
forward representation is also very
interesting and it's gotten some new
attention in thinking about deep neural
networks these are basically stacked
with citations the models aren't very
new however in the old days in the in
the late 1980s and the last wave of
neural net interest and excitement we
didn't have the kind of data to really
throw across to blow across the high
dimensionality of these models these
large parameter spaces and now when we
have that data we're seeing surprising
gains for example in the accuracy with
which we understand a workload called
the switchport data set of conversations
in the wild on low bandwidth phones
these methods also discover their own
features which has been an interesting
area of study to understand how these
systems with simple relatively simple
algorithms back prop and optimization
can discover higher and higher levels of
abstraction of how we look at the world
and combine information about the world
of course one of the interesting
challenge right now is given the
large-scale nature of some of these
learning processes even though the
algorithms are straight forward there
are some interesting systems innovations
going on right now with implementation
this is actually a competitive space
among corporations and some people view
this kind of race using even using this
methodology right now as as a modern-day
Gold Rush for tech for talent and also
for the latest accuracy results in neck
and neck for example and who's going to
sell more phones and so on so moving
forward and this obviously doesn't have
a very long transmission length you have
to say but staying at the antenna here
another to another theme I think we'll
be seeing more and more will be to the
future is not just worrying about
prediction but thinking about decisions
getting to the utility structure of our
problems this is where the rubber meets
the road in intelligent systems and so
away from data to predictions to
decisions is a fabulous way to think
about our implementations because when
you think about the learning problem
about what we want to do the utility
structure actually has implications for
the very first stages of this chain
what data do we get how much is it worth
what does what where should we put our
computational effort for example even in
a deep neural network system to put the
effort where it counts for the utility
structure of an environment with active
learning and we've seen such little work
in this space given how promising this
is we ask what additional data given
what I have so far should I collect for
goal x how much is it worth in an actual
live situation with usages working with
computing systems a stream of evidence
comes flying through the system and
sometimes we need to understand and
interpret state which is an effortful
task of labeling and value of
information computations can tell us the
value of a probe or a label versus its
cost and that could be very critical in
thinking through how to limit for
example human effort and attention for
example in annotating a personal photo
library I need to know about these
pictures tell me about those two nothing
else I'm okay with everything else well
I'd even know about this meeting was
this meeting an important meeting for
you or in real time are you busy now I
know this is interrupting you but if you
tell me right but now I'll be able to do
much better in the future value of
information computations can power that
at work thinking about utility and
decision-making comes to mind about a
project that was a collaboration between
the New England lab and the Redmond lab
early on in in this labs history when
two postdocs motion bharati and Mark
rubberman approached me about healthcare
work and do machine learning in
healthcare and what you see here is a
screen of the form being used throughout
the world now in a live system that's
predicting for patients so you discharge
them the probability they'll bounce back
to the hospital in a certain period of
time now it's really great as we speak
that these systems are actually learning
locally and tree and training up models
and predicting but have hospitals send
us the dollars are saving and this is
like your actual data from a hospital
coming in readmissions going down and so
on but one thing there and this will
frame the next version the next stage of
this project they're making decisions
about well if the score is greater than
25 I all do this intervention I will
take this action and see what happens in
reality we can look at a disease
like heart failure for example look at
the utility structure of what you would
do and how much it would cost and the
probability it would lower the reduce
their emissions rate and generate over
uncertain parameters the expected value
of putting a live decision system in a
hospital system this utility structure
helps us understand the whole end-to-end
value proposition you might say another
direction on a theme for future work
looking back and looking forward is
temple reasoning and temporality we've
seen about space and time a lots been
done in this space but it's always been
viewed as a combinatorial explosive
dimension to get into most machine
learning really reasons about the here
and now classifying X with some staple
with some static a temporal features and
time is very very powerful an elegant
and simple yet sophisticated in its in
its thinking for the Foundation's
methodology for temporal reasoning beat
out other methods in a in a showdown to
get to basically come up with the best
way to prefetch content and pre launch
applications in Windows 8.1 and an MSR
windows collaboration and that's running
now and it makes it such that when you
touch any tile it'll be up instantly
with all its data it needs eighty
percent of the time temple region is
also important insert this shows what
happened after the Fukushima tsunami
with certain kinds of pages and their
popularity on the web this is very
useful for understanding how to rank
over time with with temporal functions
back to health care again in
collaboration we're doing at Redmond
with I have to say MIT again with donggu
tags team we're looking at predicting
hospital acquired infections in
hospitals really happen within 48 hours
this hospital acquired infections are in
the top 10 killers in the United States
for all causes of death and we're
predicting quite well in many cases that
this patient here is at risk of becoming
ill by moving into a time domain we're
looking at infection as being a time
function of susceptibility and exposure
we end up in a situation where we can
enhance the the power of our methods so
a page
usually go through a number of different
rooms in a hospital we can compute over
time how many others were infected in
different wings of the hospital and
based on that bring up our accuracy
rates significantly by temple reasoning
it's a hard challenging area but in
useful area for innovation causality is
another really really a hot topic here
are some distinctions that the machine
learning procedures said we're important
for predicting hospital acquired
infection the unit who's the doctor some
medications they're all kind of
surprising if we very cautious because
it turns out in this case this is the
attending in the ICU gets all the hard
cases causality is critical and it's a
really interesting area lots of
innovations there in the graphical
modeling space of late few of which have
been applied in the real world get and
causality applies in many places this is
a chart from our Silicon Valley lab
Moyes's Goldschmidt and others looking
at data centers trying to figure out
what's the actual root cause of the
slowdown in this data center we reason
about the same methods in the hospital
as in our data centers today now I'm did
it just highlight two areas before right
before I end today that I think talk to
this lab in particular because i was
talking to christian and jennifer early
on about social computing and theory and
interesting wow let's do it and this is
the world of people and machines for
learning and interaction for example
some work that Adam Cole I did several
years ago the basic idea was he's an
active learning algorithm to ask people
about similarity to structure the triage
their effort and then build a kernel or
a whole lattice of similarities that can
be used in a live system that actually
runs based on the human input so people
are playing up an important role in
building systems had a triage their
attention is going to be critical in
another system in the Redmond lab users
play a game trying to get misdiagnosis
onto this line to make the classifier
better moving around the wheel of that
weights different classifiers
multi-class furrow setting and
partitioning and based on the data
coming from this we can build watching
how humans do machine learning build a
better machine learning procedure that
can see further out than typical
the myopia some of the procedures that
exist today I'm thinking about people in
machines this whole area of
complementary computing as machines get
more and more powerful to do more and
more things what's the role of humans
and how we work together in a
collaboration so this is a fun work and
pointing out here using the da Vinci
robotic system it's typically robotic
because it's more about kind of a of a
transmission for surgeons in terms of
direct being a direct manipulation
system but Greg hagas group at Johns
Hopkins has put the system that
basically watches what's going on stays
in manual mode goes to automated mode
and starts taking over the actual
suturing for example or other surgical
interventions in a mixed initiative back
and forth but speaking of this kind of
thing in the intellectual space we can
sort of this is at MSR Redmond take
machine perception and numerous options
together and do citizen science with a
tool that understands how to best use
imagery done with machine vision and
human effort to tag for example
different kinds of galaxies we can half
the amount of people needed we show and
get more accurate by ideally fusing and
routing tasks now end by talking a
little bit about another interesting
trend and I think a direction for
machine learning in this in the social
computing world and this was alluded to
by the last couple of talks social
behavioral data leverage for people and
society so this shows up is highlighting
some work we've done in the Redmond lab
led by monday tutor e what you see here
are characterizations of twitter feeds
and we have we have five calculations
here looking at the volume replies FX
activation and first person being
mentioned this red line is when hundreds
of people had a birth and said i just
had a baby girl isn't this great and we
have a methodology for identifying these
and then lining up all these people then
using some well-known tools coming out
of places like UT austin for capturing
effect over time and watching what
happens when this major life event
occurs in the lives of women we know
that fifty percent of women
and so I underreported suffer from
postpartum depression and blues this
leavin in the news lately with a car and
DC area unfortunate tragic event but
what we found is we can actually not
only detect severe changes in about ten
to fifty percent of women we can predict
it in advance of it happening which is
interesting locations ethically and also
towards public health this offer us we
also consider aspects of the social
graph as it evidence set on this and
just a couple more examples here this is
just being public just being submitted
this week similar were looking at
identifying in large scale search log
this was with Michael Paul from Johns
Hopkins an intern in our team that
someone has had a breast cancer
diagnosis and what happens at that point
using classifiers to discover these
people as well as the timing to
different kinds of classifiers and watch
what happens with questions about
chemotherapy and radiation at the point
of diagnosis for thousands of women we
believe who had just had their breast
cancer diagnosis in the news this year
also was using large scale search logs
to reason about drug interactions people
searching on one drug and then another
and then terms that reflect
symptomatology not just showing this for
one combination of drugs but looking at
sixty two sets of true positives and
true negatives and characterizing the
web the noisy center of the web can be
making it a really more reliable sensor
for public health in this space now I'm
just going to be seated a couple more
examples using really exotic data like
using 70 years of New York Times
headlines and other data from the web to
compute and discover that a strong
drought followed by a flood in certain
regions of the world for their
properties is an indicator for a high
likelihood of cholera that's gotten
gates involved and so on so I'll I'll
stop there by Justin my last slide here
before Christmas ostomy is saying that
the dream of machine intelligence really
is driving a lot of this work even
though
the way which we also have fun things I
just want to make a comment that that
work in both the fabric in the
foundations as well as in the
integration of these components I think
will actually lead us to the initial
dream that led to some of these initial
advance as we see that enables some of
these applications today thanks a lot
see I think we will have to put the
questions for the lunch break but let's
also share charm and all the speakers of
this session for this beautiful session
okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>