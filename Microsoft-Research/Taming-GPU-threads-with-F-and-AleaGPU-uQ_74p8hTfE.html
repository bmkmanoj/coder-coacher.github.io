<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Taming GPU threads with F# and Alea.GPU | Coder Coacher - Coaching Coders</title><meta content="Taming GPU threads with F# and Alea.GPU - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Taming GPU threads with F# and Alea.GPU</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uQ_74p8hTfE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thanks everyone for coming it's my great
pleasure to interest to introduce Daniel
Daniel dr. Daniel off Daniel is one of
these people who've migrated from the
world of theoretical physics into the
world of high finance in the sort of
Citadel finance in in Switzerland and he
is a consultant in the quantitative
finance industry and is a part of a
company called in cube advisory in cubic
and consulting but as part of their work
with using F sharp in up to I think in
four major European finance institutions
they have developed what is a really
fabulous stack for GPU programming with
with F sharp and it's one of these you
know one thing that particularly
attracted me to this line of work is how
solidly it's been proved with the
utility of the work has been proven in
in in in the in the what can be you know
quite a difficult area to work in to
actually get really solid business
relevant results in the area of
quantitative finance and you know again
and again I know I've been watching
Daniel and alia Kuan Dalia and in cubes
work over the last four years and
they've made you know just remarkable
use of the combination of F sharp and
GPU programming now the the framework
which Daniel will be talking about today
is available for much broader use as
well and there are free license options
available for that it's available as a
nougat package you know the blogs about
it you can just download I know it's
been used for machine learning
applications deeply for machine learning
applications as well and so it's one of
these areas of work for you know applied
programming frame
working in heterogeneous systems really
applied through the real-world use and
it's my thank you for coming along in
talking to us today thanks for welcome
everybody thanks for coming so today I
speak about taming GPU threats with F
sharp and our new tool chain a layer GPU
so that's the new version that will come
out probably early next year what you
will find on the web is still the old
technology well all it's not that old
but still a bit older which is under
name earlier cubase if you need to have
more details just grab me afterwards and
I can give you further pointers good so
a brief introduction myself
I'm partner at in cube and managing
director of chrome Taliah in cube is a
technology driven financial service
company and quanta layer as a software
and solution provider for high
performance and GPU computing both
companies are based in Zurich and since
beginning of this year canta Lea is a
hundred percent subsidiary of in cube I
start with a brief introduction to
general purpose GPU programming because
I'm not sure if everybody is familiar
with these technologies of explain a bit
the background about different software
stacks that are available and how you
use them then I introduce our new Alea
platform is now the version 2 and I show
you what will be available in this new
release that is coming up then I give a
few examples how you actually use this
technology this will still be based on
the actual versions or you will see
actual code that you can also run and
play with and then I finish my talk with
a short introduction into a very new
programming model that we developed
together with the Swiss University it's
the reactive dataflow programming model
which should simplify GPU programming
and heterogeneous programming a lot so
that a programmer doesn't need to know
any chip you know how in principle in
order to write GPU code so GPUs are
getting more and more important thanks
to advances in cheap your hardware and
software the world of high-performance
computing is undergoing revolution
GPUs redefine numerical computing in
many fields and industries first of all
in oil and gas that was mainly the
starting point where you call big
success about 5 years back life science
by informatics medicine engineering
defence and finance nowaday one of the
biggest GPU market is probably machine
learning where people do deep
architecture training with GPU
significantly faster than with clusters
of CPUs GPUs can process heavy compute
load much faster than CPUs they improve
all the user experience with better
graphics more reality and completely new
user interfaces and they deliver
performance with less size less weight
and less power and therefore directly
reduce costs we believe that
GPUs will be more important in the
future even more important than they are
now first of all there are more and more
compute intensive applications because
of bigger data's and there is a
fast-growing mobile device and embedded
systems market and this will boost again
GPUs on the software side we have CUDA
and OpenCL these three technologies
simplified general-purpose cheaper
programming a lot because the developer
did not need to think anymore
how he would map a computation to a
graphics context and then use a shader
language decode the algorithms but CUDA
and OpenCL are not the only technology
that are available for
a general-purpose GP programming you
have Microsoft direct compute you have
C++ amp also by Microsoft both running
on Windows then recently Google came up
with Ranger script running on Android
and even more recently Apple introduced
metal on iOS these technologies are not
used as widespread as CUDA and OpenCL
but they're clearly underlining the
importance of GPUs because nowadays on
every platform you have at least one or
more GPU technologies to do
general-purpose computing so why is CUDA
and OpenCL so popular I think there are
many reasons for that first of all it's
a very simple programming model for
massively parallel Hardware with many
threads based on a single instruction
multiple data idea that's the most
important factor I would say then they
deliver performance because the
programming model is actually somehow
reflecting the chip you Hardware in a
very good way and then they are easy to
learn because couldn't open still
they're both both based on C and C is a
well-known general purpose programming
language algorithms as I said before can
now be coded in such a language and you
don't need to go out to shaders there
are many books tutorials examples and
training so you can learn it easily the
scalability comes right away from the
programming model itself which is
another important factor then you have
good tooling support you have support in
Visual Studio for debugging for
profiling you have supporting act in
Eclipse also for debugging and profiling
which simplifies developing a lot and
then you have good platform support it
runs on almost any operating system
it runs on desktops on servers and
and now there is growing support for
mobile devices and embedded systems so
the CUDA and OpenCL software development
environment is based or provided by the
vendors like nvidia AMD or intel they're
they're based on C C++ I wouldn't say
that C C++ is obsolete as a programming
language or technology but today there
are more modern alternatives so we may
ask our we may ask the question what
would a modern language bring us as a
benefit program GPUs to understand this
we have to look at the anatomy of a GPU
accelerated code we find that a
significant amount of code is actually
CPU code and the CP code is actually
needed to prepare a data and to
orchestrate the computation so data
preparation my require reorganization of
the data bundling transposing padding
computations have to be delayed until
they can be fired off on the GPU then
different problem sizes and hardware
architectures require different
algorithmic implementations and then
some standard use cases can be very well
done with a collection of the
combination of performance primitives
like parallel map scan reduce or matrix
multiplication and only a fraction of
the code base is really hand-tuned
optimized GPU code and for this you need
the full flexibility of CUDA and OpenCL
so our project experience really is that
such a code is easier to be developed
with a modern programming language such
as for instance F sharp or C sharp and
that was one of our main motivation to
actually build our own
lack of GPU software or GPS development
to take it to be faster and to be more
productive for instance functional
programming pattern can be used very
nicely for the delayed computations
pattern matching can be very nice used
for dispatching of complex algorithms
good container support and link is very
handy if you need to prepare the data
and automated resource management and
garbage collection gives us cleaner code
and improves runtime robustness so from
a developer point of view a native
implementation of CUDA or OpenCL in one
of these modern languages is a win so
first of all more productivity another
important aspect now is cross-platform
that's the second big benefit this is
much easier to achieve with a language
like F sharp or C sharp and around temp
system like top metal mono
now there are several available native
implementations of CUDA and OpenCL in
the market so for instance you have
number pro the number pro compiler for
python which has a minimal set of CUDA
exposed to peyten
then there is a per RP which compiles
Java code to open CL this project is an
open source project now and initiated by
AMD and then there is our tool chain
earlier GPU which is capable to compile
all dotnet languages to TP use all other
approaches that we are aware of are
somehow either kind of wrapping solution
they expose an API with Interop in a
managed context or they create
intermediate C code which is then
actually compiled with the original
vendor specific open seal or CUDA
development tool chain to
executable GPU code so the second
approach of just generating secret
underneath is from a developer point of
view not that what you really want to
have because you can't really get a good
tooling for instance debugging profiling
at the source code level of the original
host language is relatively tricky and
it also is not so well suited for
cross-platform portability because you
always pull in some C runtime
dependencies so that's the reason why
we've actually built our own platform
Aleya GPU and I would like to explain
you now what this platform is so a layer
chip is a platform for professional GP
development on your net we really want
to be professional in the sense that we
have based tooling and best developer
experience in the market our upcoming
version 2 is targeting just CUDA open Co
can come in the future it depends a bit
how open Co will evolve in the market
and we target mainly nvidia gpus because
they are there supporting CUDA we
support GeForce consumer GPUs we support
the Tesla and quadrille enterprise GPUs
and we also support the Tegra mobile
GPUs well it does yes use your support
yeah we support all CUDA TP well there
is a small exception there is a little
difference between mobile and non mobile
GPUs but so now it will be available on
drawn on the Linux Windows Mac OS and
it's fully cross-platform so whenever
you build an assembly on one yeah hard
on one operating system it will wrong on
the others as well so that is already
working very nicely we have
almost a public beta ready so let's have
a look at the different layers how the
system is built so at the bottom you
have a cool enable GPU and then on top
of that you have the early around time
the layer on time is responsible for
actually compiling the the PTX code down
to executable CUDA code and launching
the computations it also manages the GPO
resources that are available on on a GPU
the next layer are the compilers we have
multiple compilers I will detail that
just afterwards the compilers generate
efficient GPU code from any dotnet
language and they also create meta
information for debugging on profiling
and they provide the tooling support for
inside debugger within Visual Studio
then left the language light layer here
we have to enable CUDA programming in C
sharp V B and F sharp which means we
have some how to extend the language
appropriately or write CUDA in terms of
an internal DSL in F sharp that's the
way how we do it in F sharp in C sharp
you have more an object model and also a
well it's not a library so actually we
have library components but then we
actually analyze say F sharp quotations
and then we compile those to LLVM ir and
from there to keep your code and it's a
bit different for the c-sharp there and
VB that's the different compiled seem
well I I don't know a bit ahead there we
actually analyze IL code and compile
il-2 LLVM IR and then to TP code so it's
actually really kind of two parts it's a
library plus a compiler
so that's the language layer and then on
top of the language layer we have the
library layer we've realized that's
really important because many of our
clients they they could use the language
to actually code GPU code but whenever
they faced complicated problems they got
stuck everybody who has written a very
effective scan or reduce algorithm on a
GPU knows about that
so we've came to the conclusion bindings
for important libraries that are shaped
within video tools like cool askew FFT
or now very recently they just released
a library for for AI primitives neural
network training the Q DNN library so
these libraries have to be exposed in
our system and then we also should have
hand-tuned performance primitives for
the basic parallel algorithms likes can
reduce map sort parallel random numbers
matrix multiplication and other kind of
like like for instance reading all
system solvers that should be available
so that you have a complete ecosystem
for computing that's in the library
layer and then also in the library layer
you will find earlier active dataflow
programming api unfortunately not
unfortunate that's one of the strengths
of Fuda cuda has a much wider and larger
ecosystem than open Co
especially in the scientific computing
domain so you have a very good library
of Blas algorithms high highly tuned for
a transform that's all not there in
OpenGL it's getting more and more
complete but it's still behind so now
the compiler half of it is already
answered by the question that just got
asked so we have actually mainly two
compilation streams one compilation
stream is in a sense the old scheme
which is implemented in F sharp so you
take F sharp you're right
after quotations with CUDA DSL and these
quotations are then compiled to LLVM ir
and from there to p TX with the cuda
driver that's version one point X now
with version 2 we have a second
compilation stream which actually takes
again F sharp C sharp code there you
have to code the kernel a bit different
you have to overload a certain member
function of a class when you have to
tack that with certain attributes and
then we understand that and again you
can use certain specific symbols which
are in a sense an internal to your cell
for CUDA and then we can compile this
code to we can analyze Al and we can
compile al to IR to LLVM IR and from
there again to p TX with the cuda driver
what is also important is that actually
we have two ways of compilation we have
the so-called dynamic compilation which
can happen at runtime which is very
handy because it allows you to do cheap
your scripting for instance in the F
sharp interactive console that's a very
handy feature if you want to explore
cheap algorithms at runtime play with it
and prove it see the effect understand
it or using in a notebook programming
style that's also a very nice
application for this dynamic compilation
banks use that in a different way there
a trader enters a new derivative
contract with a new formula for the
payoff and then this formula is directly
compiled into the GPO kernel at wrong
time so they don't need to actually go
through the whole process of kind of
deploying a new system for just this new
derivative contract which is a slight
modification of an existing one but has
a very specific new formula in there so
that's a real-world use case of dynamic
computation the other one is ahead of
time compilation dairy actually trade
for fastest
application launch time trade against
flexibility there you just embed your
compiled resources into the assembly so
that you can load it again at a Polish
startup that's the traditional
completion scheme our performance is the
same as CUDA C C++ because we use all
the optimization passes that LLVM and n
nvm so that's the Nvidia back and for T
P computing all these optimization
passes are used and therefore our code
runs at the same speed so now I come to
the heart of the talk I'm not sure how
many of you know about cheap computing
already
ok so quite a few so but nevertheless I
think it's it's good to start with the
CUDA crash course or open seals pretty
much saying the programming model there
are so close they use different words
for the same things but that's about it
so the first important abstraction in
CUDA is kernel a kernel is nothing else
than a function executing sequential
code in parallel in many threads then
threats are not just there one by one
they are grouped in blocks and then
multiple blocks of the same size they
are grouped into a grid now a thread in
a block has an identifier the so called
thread index it's a three dimensional
index XY said then each block has a size
locked in XY said also along the coding
axis of your block
remember the blocks are always of the
same size and similar a block has an
identifier in the grid which is the
block index XY said again three
dimensional these three dimensions comes
that's come that is coming from graphics
obviously there is a little typo
directress dealing with block index set
is wrong then we have great size again
which tells you how
many blocks in each different coordinate
axis you have and then what is important
now is these threads are in a block or
not or scheduled one by one they're
always scheduled in a in a group of
typically 32 threads and that's called a
warp that's a final top level and we
will have an example just now which
makes use of that why is that important
a warp is executing as a unit which
means each thread does the same
instruction at each step
if one threat requires a branch because
of an F then all the exact the
statements in the if or else branch are
executed by all the threats even if they
don't need to do that so one issue is
actually branch divergence in in a warp
which would slow down the performance
and that's one of the most important
optimizations at the low level of CUDA
coding 3d chunk of threads you get your
pictures of fan of 1d yeah well yeah
it's one but it took three yes yes so
these warps are then like some chunks of
blocks over 32 yes it's actually going
like that you have a block of threads
and then they start to enumerate the
threats in the block and just taking the
first 32 that's the first warp and goes
on right and it if it goes first along
the the x-coordinate and then set a y&amp;amp;z
coordinate so it's linearizing the XYZ
coordinates yes yeah and then obviously
you have also different memory memories
like shared memories device memories
constant memories textual memories and
many other details which we don't need
to know at this stage to understand the
example so I skip that now what I want
to show you is a an advanced chip you
algorithm that you can write in f4 I
will show you several implementations
and the reason why I chose this
algorithm is because it shows you what
you can do in a
you can really extend F sharp so nicely
so that it becomes a new language
knowing about Kula and not just knowing
about a small part of CUDA knowing
almost everything of CUDA also the the
detailed things like loop unrolling like
warp level synchronization all specific
performance primitives like a shuffle
instruction including spar over many
folks is that a grids worth of code in
effect it's kind of maybe multiple it's
actually one function but for each
thread this function is being executed
so you can understand I did like that
you have many instances of that function
operate each one operating in a threat
and this thread is in a block and this
block is part of the grid and by the way
why do you have these blocks these
blocks are somehow reflecting the GPU
Hardware a GPU is not a linear
collection of threats it has this
architecture this kind of grouping you
have single threads which are then kind
of scheduled as a group and each keep
you consists of multiple streaming multi
processors and the streaming
multiprocessor looks like a block in a
sense and typically one of these
streaming multi processors is executing
in fact multiple blocks so you can
switch very fast between the blocks so
one of these fits these Wiggly things
you George yeah could that be multiple
curls or was it the other way around
that a kernel consists of multiple
they're sequential grids the kernel
consists of multiple threads
yeah so you have one one kernel running
in all these threads
yeah what when you don't you colonel you
have a specification that says I want
this number of threads and I want them
arranged in this in the acquisitions
right so they can then be split up to
the stream of processes okay so that's
an additional information you have to
provide to the function you have to tell
the function how it should be executed
so you have to describe which thread
topology you will going to use which
means how many threads per block as he
said and then how wide the grid is in
each cone in taxes and you have to tell
all the things like how much shared
memory you want to have for instance in
that block and these shared memories
then used by all the kernels running in
that block so they can communicate
between each other there are some more
details there okay so now the first
algorithm I want to show you a warp scan
performance primitive which does a scan
at the world level so we just look at 32
threads they have data and we want to do
the scan which means just each element
or each thread has the sum of its L of
its predecessors we want to do that in a
very advanced GP programming way it
should be highly optimized we want to
dispatch four different architectures we
want to use different optimization
schemes in particular for compute
capability 3 which is the so-called
Kepler architecture of Nvidia we want to
use the shuffle instruction and for pre
compute capability 3 we want to use
shared memory and this dispatching logic
is done with pattern matching so in this
picture you see how it is working I show
you the implementation with a shuffle
shuffle instruction is a very nice
communication operation so if you say
shuffle a variable of 1 you do a common
occasion between the threads in a warp
so you shift somehow a data element that
you have in thread one to thread
- and the element you have in thread two
two three three and so on what
so that's depicted here so let me see if
you have so that's kind of just shifted
out then there is a rotate as well where
it comes in at the bottom so there are
different variation of short-form so
here you have the data in the in these
32 well here I have a little reduction
it's just 16 so then with a shuffle you
move this state element up to here this
up to here and so on and then you add
this and this together and this one is
just copied down so that's the first
step of your scan then add or multiply
or whatever you have a binary operation
which does your over which you scan
right it doesn't need to be add that's
another aspect we can actually nicely
take any extra function just decorated
with reflect the definition and use it
in the framework also very nice
application of absorb code quotations so
then labeled second round is now you
shift your data by offset to and you add
it again up then by four and then by
eight and then you're done
so that's kind of a relatively simple
way of doing your scan in a parallel way
and the interesting thing is that this
shuffle up instruction doesn't need any
further memory and it is being executed
roughly in in one step it's a very fast
implementation so
have a look at the code so anybody not
familiar with f-sharp so everybody is
familiar with Ephram okay so let's start
with a with a test so we have written a
test here you see a computation work
flow computation work flow the CUDA work
flow in this you actually describe your
computation you manage all the resources
and you do all the memory allocations
you need to do and you have a so-called
entry function which is just the entry
function to start the computation on the
GPU
it takes a program which is the compiled
GPU module from there you extract
resources like the kunu they do memory
preparation here I've just coded also
the test for convenience and what is
important here you describe how you
would like to launch your kernel that's
really the block and grid size which is
specify there once you've done that you
can just launch your kernel with these
parameters and you supply for instance
some memory for input and output that
you allocated before here our allocate
input I use the use binding to profit
from garbage collection automatic
resource management here similar output
and ones I'm the ones if I'm done with
my kernel I can actually just copy all
the data back from the GPU to the CPU to
some further work with it this way I can
code any
the secret function which does
underneath some cheaper calculation here
I don't need to give any further
arguments I could supply here additional
arguments which are then exposed again
to the workflow and you get an ordinary
function what I need to do now I
somewhere define my computation here
that's my template it's kind of my
computation workflow we call it template
because nothing happens yet so this
template is has an entry point of a
function unit to unit so it doesn't
expect anything and it returns anything
so what do you do now down here you grab
your default GPU here and you just tell
the GPU load please this template in
down from CPU GPU compile it with this
compilation configuration that can be
just optimized configuration or
Diagnostics configuration for some
profiling and we do that right after and
then you just call run that's all so
here all the compilation happens and
here you actually run it the run method
then kicks off this function which runs
in which ends up in this function and
does the computation so that's the the
big picture and now we have to look at
the at the fine details so but actually
before let's let's try to debug that so
if you install inside debugger you get
here a menu where you can select start
CUDA debugging so that you in order that
you can debug you have to compile it
compile your code with the right
configurations I've done that here
in my test here I use this Diagnostics
configuration which brings in all the
meta information and then let me see
which one would shuffle GPU debugging
including protein what's the difference
well that is mainly needed for graphics
OpenGL stuff which is also supported by
Nvidia and that's really the CUDA we
just use that one and you also have
graphics debugging here ok so I need to
select now this project as a start up
project what is very nice now I can set
breakpoints all the in code Co tations
see
here is a quote quotation this code
quotation actually prepares the data for
the warp scan and then then actually
calls the warp scan here right so a code
quotation f-sharp is within the limiters
here and here so this code is actually
compiled to a syntax tree and this
syntax tree is then interpreted by our
compiler what is very nice with the new
tooling I can now set a breakpoint we
did a code quotation that was not
possible with older tooling thanks to
Microsoft to fixing that so here I put a
breakpoint and then actually here I call
out to my inclusive scan algorithm I
just give it an operator and a flop
operator for the scan and this can is
actually just a certain implementation
whatever that is depending on the
architecture which is selected at
runtime so that's a special yeah that's
a special function which runs this piece
of code on the GPU so it's kind of a
wrapper because that is not necessarily
a quotation it can be just a function so
this is part of the DSL for CUDA
programming that we provide it takes an
expression and splits it off what no I
think you should read this philosophy in
one syntax program into into that expert
eat eat eat yeah so it sounds like it
takes us into yeah but because it's in
the it's it's just a quotation because
lining it and everything so it's just
meta program including another minute
program so I've set a breakpoint
now typically you need to rerun or
recompile it because there are some
tooling limitations that the debugger
does not recognize if the code is dirty
so let's quickly build that it's
relatively quick then I start my debug
so I'm here in my quotation so now I can
step through by the way that's an
implementation detail typically some
algorithms they need some additional
resources like shared memory so they can
advertise that and then you you actually
allocate it where you need it so here I
have my temp storage which is required
by the algorithm then I do some thread
index stuff
prepare my data and now I actually go
into my inclusive scan implementation I
go through my next quotation here I'm in
this implementation of you see I've done
a pattern matching based on my
architecture what it is I'll and in here
I have your a record
of different implementations for the
different scan variations that I need to
do I can step in further I come to this
actual function which does to work and
Here I am in my function doing the
actual scan and here you see a few
additional details you have a function
to get the lane of a of a warp that's
the thread index in a warp from 0 to 31
you have unrolling features for loops
that this loop is really fully unrolled
and here there is this shuffle up
instruction that you can use what I can
do now I have some work watch windows
bring that over here and you see I have
32 threads and I can now say kind of I
need to stack one more sorry I pulled it
out again
step one more oh I lost it sorry tooling
is not yet fully precise sometimes I
have to be careful because I lose my my
line number let me do that again
sorry
so I step in
so now I can use my work watch windows
to display variables like lane ID I can
also check what isn't going into my temp
I can check what is going into my what
is coming from my output so you see now
what happens with the shuffle
I had the output where I had original
values and then I did this shuffle which
which is done with a an index one so the
four goes to here the six goes to here
and so on and then I do the next one
offset to I do the next one with offset
four and so on so you can analyze
precisely what happens with your data at
each step of your algorithm you also see
that some values here if you look at at
your locals that they do not display any
values anymore that is a problem which
is also in the CUDA seek debug debugger
because the registers are highly
optimized because otherwise the kernel
cannot even run so very often these are
out of scope and therefore they don't
have any values to get around that issue
you would normally have to store them in
to some device memories or additional
variables so that the compiler is forced
to keep them
so that's the debugging and how you can
step into the different layers of our
implementation you see here very nicely
you just give an F flop function to this
cheap u code you decorate this member
function with reflect the definition and
if you pass in such a binary operator
you also have it you have to give it
this attribute in order that you get the
abstract syntax tree through code
quotations otherwise you can program
with types as you like you can decorate
any member function with reflect the
definition and use it on the GPU and you
can use kind of data structures like
tuples and records and you also can use
pattern matching within GPU code not all
the language features of f sharp are
exposed in a code quotation to write GPU
code there are some limitations but
limitations become less and less over
time then the compiler reports you an
error messages with which is more or
less useful in order that you can find
out what the problem was it's pretty yep
so it is actually the compiler which
then compiles the GPU code or the
quotation so yeah exactly you always
have two levels of compilation you have
a design time compilation of your effort
code through a fork compiler and then
once you start your TP code it's
dynamically compiled at wrong time
that's option one or it's compiled ahead
of time again through through some
tooling it integration which we
currently do with 40 and then the errors
are reported at
computation time depending on the number
of threads available or the size of the
kind of device was available or the
input sizes you choose different paths
for the scan yeah operation so if you're
just using the dynamic compilation there
could be and you you know you'd only be
going down one of those paths presumably
so there could be sort of compile time
bugs hidden away in those other paths
which you don't know similar similar
thing with the c++ make the template
programming same problem what the
compile time so everything done a
compiler is using everything that's
involved yes there you actually really
have to say which variations of the
algorithm you have the compiler yes some
other interesting implementations if you
look at now the warp based
implementation which is pretty much the
same thing but instead of using the
shuffle we actually copy everything into
shared memory and then accessing it from
there and we allocate a bit more shared
memory in order to avoid if statements
to figure out boundary conditions so
this comes in here this little trick
avoids you one if and here you see you
you can do static asserts like for
instance that this one is constant again
the loop unrolling step and offset need
to be compiled time statics so you can
enforce that which helps a lot to make
the code more robust right so primitives
like scan step and the other ones are
they're really low very important very
very programming as you and as you move
up to use this obviously to get more and
more functional you're stealing while
you still have this imperative yes you
have this imperative memory
process but besides that memory manager
memory transfers as Malick's and and
gather process when your upper level
okay that's more fun yes and it's all
built into the computational workflow
idea with the CUDA workflow and this is
composable again very nicely so
depending on how you organize it you can
put algorithms together you can share
resources and so on right yes apply
practical applied work like you know
when working on for finance institution
how much of your time is spent writing
the imperative primitives and how much
it's been composing we bring these into
into each project because it normally
takes too long
such an algorithm can easily take a
month probably not this one but that's
just now the lowest time that's at the
world player then from the world player
we have to go to the block level and
then from the block level you have to go
up to the device level so you have
multiple levels how you need to scale
the algorithm and the code gets more and
more complicated because you have to do
more and more things like aligning data
considering special cases like non equal
power of two cases and so on so that's
quite tricky also to test and normally
we we deliver that as an asset for the
customer and then most of the of the
coding effort is wiring these components
together and writing then some specific
kernels but as I said roughly its heart
isn't a small fraction of the code is
hand optimized GPU code most of it is
really plumbing and there're again
f-sharp is a great tool because it
facilitates that significantly reusable
assets intellectual property which you
come to the customers with I mean even
Brazilian configuration
even writing a good parallel random
number generator is a big is a big talk
well they do
yes yeah yeah you could actually
directly use them some of them can be
used because they have a good interface
others are hard to actually put into
this framework because it's a see
function which you then just can call
you need to interrupt with it and that's
probably not what you want to have
because it hinders your flexibility
quite a bit so whenever you want to
share or optimize the resources you need
to have full control on your algorithm
because all lower algorithms they need
to somehow report what shared memory
they need and you have to take that into
the consideration if you build the top
level algorithms so that's the first
example so here you see you can do
pretty much everything you can do in
CUDA CUDA see again here some cost of
some Union storage memory which is also
a feature that is needed to optimize the
usage of shared memory because that's a
very scary use resource so you you have
to be careful how you use it and we've
seen that in many low level primitives
it's it's a handy thing to have this
kind of Union storage okay so the next
algorithm is a bit higher up
the next algorithm is a specific matrix
multiplication and this comes from a
real project so quite often you have
more or less rectangular matrices big
matrices that you want to multiply and
you can pack a big matrix this is a
standard use case which is very well
handled with by Nvidia coulis they have
very powerful algorithms for doing that
it gets very different if you have say
slim matrices so a is very has very a
very large number of columns and a very
small number of rows we speak about half
a million to a million columns and
roughly 10 20 rows then be similar half
a million rows and only a few columns so
that's one thing and then another thing
is sometimes depending on the context
you need to wait a matrix with a
probability vector V which you expand
row wise to a matrix of the same size as
a so the rows are all the same vectors
and then you do an element wise
multiplication then you get a weighted
matrix and you need to multiply this
matrix a weighted with me with B and
then you get X this is a use case which
appeared in a inner loop in an optimizer
to calculate the Hessian and this very
special structure is not very well
handled with by qu+ so you had you have
to implement your own algorithm now how
can you do that one approach is to use
some performance primitives and build on
those the idea is you have first of all
into two step reduction you reduce your
big matrices into a small one X prime
which you then do which you then reduce
in one step down to the result so the
interesting thing is how you reduce a
and P to X Prime so what you do for a
combination of rows of a and B U and V
you actually have per line per Lane you
have blocks of threads so that's your
great okay
so you reduce first of all a thread it
uses these elements here and then within
a block you do horizontal reduction this
can be done very nicely again with
primitives with performance primitives
and now here things get much simpler so
again here I have a slightly different
approach here we do not use the khuddam
computation workflow but instead we use
the new class infrastructure where it
that create a member function of the
kernel that's an alternative approach
and then in this function you organize
your threads you do the first reduction
over the different blocks over over the
whole row and once you've reduced them
you do the reduction within a block with
a performance primitive here you just
call out to this performance primitive
so the code stays relatively small and
simple and then calling it is simple as
well you just have your multiply you
organize your threads you launch it you
launch the second you will define the
threads for the second iteration and you
launch it again so that's simple as soon
as you have these performance primitives
you can combine them and get your
kernels down much quicker
yeah so conclusion is that with this
solution you can program GP algorithms
at any level of detail with f-sharp very
easily you get the full power you get
the same performance you get a much
better productivity and it's much easier
for you to assemble a date and prepare
the data for us is it's a big win on the
project because we are faster we make
less errors in the data before we
actually go to the GPU because debugging
on a GPU is relatively hard and if you
figure out that you have an error just
because you send the wrong data to the
GPU it's quite stupid because you can
avoid this problem even before with
better tooling and a better language
cool ok let's um first thank them for
thanks I think we've all got a stronger
feeling for what's involved if they're
sort of nuts and bolts of PPE you
programming and how that maps up through
a complete compositional stack as well
I've had a lot of questions along the
way
are there any we'll take one more
question from well it's not difficult
the problem is more shall we do it or
not
because now OpenCL has the Spear
representation or the spear
specification is finished it's already
there in version 2 unfortunately not
that many vendor support spear version 2
and until we see that vendors implement
this spec of a specification we would be
ready to do it but it's more like the
market needs to evolve and settle before
we can do that ok any more questions ok
let's thank them again thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>