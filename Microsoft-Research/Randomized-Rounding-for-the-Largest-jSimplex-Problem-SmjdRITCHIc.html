<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Randomized Rounding for the Largest j-Simplex Problem | Coder Coacher - Coaching Coders</title><meta content="Randomized Rounding for the Largest j-Simplex Problem - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Randomized Rounding for the Largest j-Simplex Problem</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SmjdRITCHIc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay hi everyone ah it's a pleasure to
introduce social Niccolo sasha is doing
a postdoc head this year but he has done
a lot of nice work in many areas
including discrepancy differential
privacy and lot of things on convex
geometry as well and he's going to tell
us something in that area thanks thanks
good to give a talk here so I'll tell
you about an approximation algorithm for
a geometric problem and i'll start with
the problem because they think it's a
natural problem oh please stop me
anytime is there any questions ok so
here's the problem um so the input is ah
me see if I can do this no the input is
endpoints in D dimensional space and our
goal is to find the largest J
dimensional simplex in the convex hull
ok so here those red dots and my points
in imagine this is three dimensional
space ah this cube I've drawn is
supposed to be the convex hull and our
goal is to find are the largest simplex
of some given dimension inside that
complex how largest I'm measuring by a
basically volume ok so let me cool all
right so we have points of endpoints in
D dimensions we're looking at the convex
hull and we're looking for the largest
volume simplex so a simplex J
dimensional simplex you just the convex
hull of j plus 1 points ok so a
two-dimensional simplex is a triangle
three-dimensional simplex is a
tetrahedron and so on ah and we're
measuring largeness in terms of volume
okay so when we're looking for a
d-dimensional simplex in an hour points
in D dimensions we really are just
looking for the D dimensional simplex of
the largest volume uh if j is less than
the full dimension less than D it's kind
of the obvious things so we would look
at the the flat in which the simplex
lies that the FI span and we will look
at the lib back measure on that flat so
in so in this
we're in three dimensions if we're
looking for the largest two dimensional
simplex we're looking for the triangle
that has the largest area make sense
okay um and to sort of make the problem
at least sound more combinatorial let's
notice that arm we can always assume
that an optimal simplex is the convex
call of some subset of the input points
of the various right because okay there
might be some other optimum simplex but
you can always just move the points one
by one until they snap to a input point
without changing their I mean there's
always a way to do that without changing
the objective either it is without
decreasing it some excess so we're
really looking for a subset of j plus 1
points from the input that have the
largest volume simplex I mean whose
convex how is large to autumn simplex
okay so that's the problem good cool um
so I'm claiming it's a natural problem
in computational geometry it definitely
has been started for some time you can
see it as one example from a general
class of problems approximating some
complicated object say convex body with
a simpler one say contained inside of it
and you can in particular see it as a
sort of a polyhedral analog of the
largest volume ellipsoid problem dodge
ellipsoid problem that's or the john
ellipsoid problem is the same problem
but you're looking to find the largest
ellipsoid containing the complex however
viewpoints it's also much harder problem
in the representation which i'm working
but that doesn't matter okay oh right
and there are some applications of this
in low rank matrix matrix approximation
and also in discrepancy theory and i'll
tell you a little bit about the second
application make sense cause you know
more almost no different if i see it's
also the rural division resolve
themselves I think you can define it the
same way i'm not sure if people have
looked at it um that's all i can say
okay all right so let me tell you about
this connection with discrepancy theory
and it's a little bit of a loaded slide
so I'll go slowly over it so I want to
tell you about something called linear
discrepancy that's a measure of how well
you can round in a certain sense which
I'll define how you can how well we can
round so the linear discrepancy of
vectors again a point V 1 to V n.e.r.d
in some measure of how well we can round
with respect to these points in the
following way so all right so we given
some real weight x1 to xn those are real
coefficients that correspond to each one
of the vectors v1 to VN and our goal is
to round them too so they're in plus
minus 1 but they could be anything it
could be you know um square root of 2
over 2 ah but I would always to round
these things to plus minus ones due to
the end points to the integer n points
so that the linear combination with the
given real coefficients is close to the
linear combination with the rounded
coefficients and we say we are measuring
closeness in Euclidean norm so what
linear discrepancy is is for a fixed set
of vectors what's the worst-case error
over all possible linear combinations
for rounding this make sense okay so
that's something that comes up in
approximation algorithms for example
where you solve some linear program you
want to round a solution to linear
program to an integer solution without
losing too much ah okay and what if
you're right here right so here is the
connection to this largest volume
simplex problem so let me denote cursive
V sub J the maximum volume of a J
dimensional simplex in the convex hull
of the points okay so here is a theorem
one side so the lower bound is due to
love a Spencer investor gumby the upper
bound is due to matousek from a few
years ago ah doesn't matter exactly what
this you know this formula is but it
tells you that basically
this linear discrepancy is up to log
factors equal to some function of these
these V subjects in particular you can
are you can approximate this if you can
if you if you know some approximate
values for all these these subjects okay
and also notice that we're normalizing
here so this is jay dimensional volume
so we're raising to the power 1 over J
so this kind of gives the right scaling
factor and this will be nice you see
once like why it's nice for this result
Nexus the reason your series ah that's
one they're definitely one reason why
I'm interested uh I also um I realized
so I saw a recent soda paper on that
this is the honest answer and uh I
realized I already had an improvement
but it just didn't know people care
about this problem so it has been
studying computational geometry for some
time I guess you could argue it's
natural on its own for me as far as
applications this is sort of maybe the
main application I'm interested in it's
not wrongly John a nice way that I think
yeah I think it's I think it's a natural
problem um I could dig out more
applications the connection to
low-ranked approximations it is also
interesting but it would have taken a
little more time to kind of define what
it is exactly this look D in inductor
well the United that's optimal or
probably probably we know that it I need
to think about it a little bit for
exactly this measure I think it's not
known to be optimal but some poly log
factor is necessary yeah
why Sri huh um yeah it's a good question
so it's a little hard transfer without
going into proof so oh the lower bound
at least is some covering argument and
that's kind of what comes out so you're
covering like one object with simplices
basically or something like that and you
know when you compute the volumes that's
what comes out it's it's some function
of the volume of a unit ball and the
volume of a sort of regular simplex I
need to think about it a little bit when
you normalize the volume to the one over
dimension maybe it's the ratio of the
two and which is what comes out of work
when you just do the obvious volume
lower bound for covering does this make
any sort of sense I can I can think more
about it and come back to look okay so
it's not the ratio of the two but it
sound fun of those two things so let me
move to the memories out unless the more
questions oh so here's what I want to
prove our I mean here's what I proved in
this work and what I'll talk to you
about so shortly there exists a
deterministic our polynomial time
algorithm that approximates the volume
of the largest J dimensional simplex up
to a factor which ok so the analysis
exactly gives this J to the j / j
factorial under square root oh and this
by sterling is sort of like e to the j /
2 okay and notice that here we always
take VJ to the 1 over J so this that and
this I mean that observation in this
approximation factor gives us a constant
factor approximation to this linear
discrepancy problem arm all right sorry
it uses a cost factor approximation to
this volume lower boundary
discrepancy problem it gives us log
approximation to the actual linear
discrepancy problem also this e to the I
mean this constant to the dimension sort
of approximation factor is a optimal is
the optimal kind of approximation you
can get in the sense that we know that
the approximation cannot be better than
exponential in the dimension so the
exists some constant bigger than one
such that this problem is np-hard to
approximate better than the constant to
the J to the dimension okay this
constant is really much closer to 1 and
E are so it's still not clear what the
right cost and is there but at least we
get the right order of dependence and
this impedance is true as long as so
obviously J is constant you can just
brute force the problem so this NP
hardness holds as long as J sort of big
enough ok again polynomial time here
means polynomial in both dimension a
number of points this is not the usual
computational geometry setting where you
know we're in the plane because then the
problem becomes sort of too easy oh and
it's a significant improvement what we
knew before so previously for general J
we knew Jay to the J over two
approximation I'm losing some constant
to the J here is was due to kashien and
packer and very recently i forgot the
reference ok ah this summer free ties
and Brandon a few other people ah that's
embarrassing sir apologize to fritz and
everybody else so the they improve this
only for the case where you're looking
for the largest d-dimensional it lips
are so only for the full dimensional
case they get something which is like
logged e to the d over 20 this is a
soldier 2014 pay for 15 2015 with the
last soldier right and so we improve on
those two results I think significantly
this is the my result any question um
okay so I want to first tell you about
the D dimensional case because they
easier it contains a lot of the
intuition and I kind I want to give you
as much of a foolproof as a can mean sir
the time that I have for this case
because i think it's simple enough okay
so go slow or please stop in again if
you have questions okay so here's some
simple sort of reductions that we can do
the problem to make it slightly more
just slightly easier okay so we're a
bunch of them so go sort of slow over
them so let me use the delta because it
looks like a triangle to be the optimum
simplex okay says the comments have some
subset of D plus 1 points from the input
let's guess the first vertex we can
always do that the only n options in
reality what we will do is we will just
kind of run over all the options I mean
we'll run the algorithm once for every
option but for now let's assume we just
know the first verse take someone tells
us what it is let's replace each point
VI so but your place I mean let's just
move each point uh okay yeah let's just
subtract the first vertex VI VI one from
each point VI so this just kind of
shifts the whole set of points so it
obviously doesn't change volumes but now
we know that one of the optimal points
is the origin it's just convenient it's
convenient you've seen a second why but
now this after we do this the new
problem is to find not D plus 1 but only
d points from the input so a subset of
dew points will dimple so that the
volume of the convex hull of these D
points and the origin is maximized and
that's convenient just because it lets
us make the problem a linear algebra
problem as opposed to geometry problem
okay so this is equivalent to we have
our points so let me write something
okay so we have our points and let's
sort of think of this matrix maybe a is
not the right thing sorry about that Oh
matrix V which is just the matrix which
has columns the which are the points
our accept the zero point which we don't
care about anyone okay so v2 and so on
ok and now I'm claiming that this
problem of finding these D points that
whose convex how together with zero has
the largest volume is the same as
finding a deep ID so ok so here the
dimension is this dimension is D and
this dimension is n ok so that problem
is equivalent to the problem finding a d
by d sub matrix of this it first has the
largest determinant in absolute value
why because the the volume of this thing
is just equal to the volume of the
corresponding sub matrix times the
scaling factor that doesn't start the
volume of this is equal to the absolute
value of the determinant of the
corresponding sub matrix times 1 over d
factorial let's um yeah let's just take
that on faith I mean I think that's
convincing for everyone right ok go ah
alright so now we have this problem we
just have this n by D matrix and we want
to find the d by d sub matrix of it that
has the largest determinant in absolute
value that's our problem right now it's
just do one more simplification and then
we're kind of ok almost done um um let's
just add also the inverse I mean the
right let's add so after after doing
this this first transformation now let's
also add minus VI for every VI ok what
this lets us have is that ah it makes
the set of points symmetric around the
origin that's just kind of convenient it
also obviously doesn't change the
optimal solution why because okay um
let's see so um
so if we if we take this up make sure
the sub-matrix it has both vio- VI the
determinant is zero so okay so we always
so for a non zero solution will always
take either VI or minus VI but which one
we take just changes the sign and we're
taking absolute values so it doesn't
really change ah the value of the
optimal solution so but now we okay so
remember we kind of changed the problem
so that we always take the origin to be
one of the vertices so say we're just
looking for the optimal two-dimensional
I mean sorry well it's kind of yeah
let's just do two dimensional right so
we look at this and this so my point is
that this triangle and this triangle
have the same area just the length today
when society VV we have some kind of
base and one left via the only care
about of the high right right so why are
they why are they the same because this
length and this length are the same and
the height doesn't change when I do this
and you have the same for sort of larger
than matches you always have like based
on type ok cool ah alright so now those
are the basic transformations now I want
to sort of move towards the actual
algorithm a little more okay so we are
approximating a hard maximization
problem okay so the usual game is to
find an upper bound on the optimal
solution then we compare how well our
algorithm does against that upper bound
oh I am worried that mushy doesn't
synchronous
cool it's okay so here's the basic upper
bond we use with some modification ah so
here's one doesn't okay cool here's one
upper bound that we have available I
remember we have this problem now of
finding the largest determinant of a d
by dr square sub-matrix oh okay so i'm
claiming that one thing that's true is
that if all columns have length at most
are of this matrix then well any
determinant of this matrix is absolute
value at most are to the D and that just
had a marg inequality you know basic
statement about determinants in matrices
okay alright so that's it's clearly not
a good upper bound because so our points
could look like well like this i guess
but i mean our points could look like
this right so we could have you know
maybe a few long points i mean if you
sir yeah a few long vectors and they're
far from the origin and a bunch of
really tiny ones right and uh say we
just have one big guy all right so now
this upper bond will be huge because of
just this guy but you obviously can get
something that's comparable large in
terms of an actual simplex right so it's
not a good upper bound itself but we'll
fix it somehow
product of what doesn't move which is
like that you can maximize the better
program is maximized overall subsets of
size d uh-huh product of the VIS can you
compute that I guess you could compute
that yeah that's probably also not very
good just eating that's probably also
not very good because they could be
pretty close together and you'll still
be a problem you would have a bunch of
long ones but they're very close to it
still right um all right so here's
something will do to try to use this
upper bound okay so uh i will introduce
one more object which is the lawn or
ellipsoid of this set of points so if
the columns of this matrix arm and by
that i mean the minimum volume ellipsoid
that contains all the points okay
because we did this transformation that
makes sure that our set of points is
symmetric around the origin we can also
assume that the minimum volume you live
story sort of a standard argument is
Senator zero as well so I mean it it
will be centered 0 okay so so how do we
use that okay so the point of this is
that you sort of fits tightly around our
set of points so we want to kind of use
that so here's what we'll do so this is
what what it looks like but we have our
upper bond which is somehow in terms of
l2 norm so in terms of containing things
in a sphere so we want to kind of map
this whole thing to a sphere so what we
do is the following so we have this
ellipsoid al which is the minimum volume
ellipsoid it contains our set of points
we can always compute a linear map that
maps this city upside to a to a bowl to
a standard you euclidean bowl and um we
can also do that with the linear map
that actually has determinant equals one
so it doesn't change volume of any surf
I mean it just doesn't change volume so
the water the volumes of the other
spheres is the same yes the volume of
the sphere would also not change yes the
volume of the sphere would also not
change in the volume of any simplex any
food dimensional simplex would also not
change because the determinant of you
know this linear map says a matrix is 10
ok so that means that are in particular
has to be the product of the axis
lengths of this episode to the power 1
over d ok since Princess I thought that
some something was wrong with that
statement ok so what we do is that we
compute the smallest volume ellipsoid
that's a convex program we can do it up
to any approximation believing that
approximations are fine for us it's not
immediately obvious but they are and all
right then we'll compute this linear map
or in other words i mean this result
will be given by either this map its
inverse all right so and then we just do
this transformation where we apply this
map to each point in the input each each
VI and now we have the de lunar
ellipsoid is a bowl of some radius R and
our upper bound will be R the radius to
the to the power D ok because we just
applied this linear at it doesn't change
volume are and it's a valid upper bound
here after the transformation and the
transformation didn't change the value
of any solution this is a valid upper
bound so excess questions in other words
it's just the product of the eigenvalues
yes it is it is just the yeah for the
determinate actually it is just the
product of dagon iris and that's another
or like the product of the axis lengths
of this a deploy so that's another way
to see this upper bound so not sure
which is easier maybe maybe what you're
saying is easier action ok yeah kind of
one reason why I did with this
transformation is I want to give some
intuition why it's a good upper bound
and the point is again that well the
small
volume ellipsoid somehow has to fit
tightly around the set of points why
because if it doesn't so if our points
down sort of touch in some vague sense
the ellipsoid in every direction so if
there's some direction which is big gaps
and kind of squeeze the ellipsoid and
get a smaller volume this is sort of the
rough intuition why you know this does
give you some information about how well
remember the bad example for the upper
bound was sort of these thin bodies so
we want to kind of avoid that and this
is why this sort of helps and so one
nice thing about the loaner ellipsoid
that was used in previous algorithms but
it's sort of too weak for the kind of
result I prove is that so one thing we
know is that ok if we have some comics
body whose loughner episode is a bowl of
some radius R then if you scale the
radius x 0 0 and it's a symmetric body
as well if you scale the radius by Rudy
then you get a ball that I that's
guaranteed to be contained in the
college body that it's not going to be
the content as tightly as it is here but
it's always going to be contained ok so
that's one thing that tells you that
sort of the upper bound is good in the
sense that well um this convex hull has
to contain sort of large things because
it contains this relatively large bowl
inside of it and this was used by
previous algorithms but it's not quite
good enough for us but what I'll use is
sort of the arm the fat from commerce
optimization that let's just prove
something like that in other words i'll
use the duel of the loner ellipsoid ok
so this is what we get from complex
duality about this loner ellipsoid this
is sort of a dual characterization of
the of what the episode is so you can
say this is I mean I guess this is jon's
theorem in some Restatement and here's
what it says so it says that if the
smallest volume enclosing it is sort of
a ok it
it says that the small volume enclosing
a dip side of the convex hull of points
V 1 to V n is a bowl of some radius if
and only if there exists non-negative
weights that you assigned to the points
so that are the sum of CI times the
outer product matrices I mean this rank
1 VI VI transpose matrices has to be
equal so okay so the smallest volume can
so the lunar ellipsoid is a ball Freddy
sorry if and only if they exist weights
such that this thing is equal to the
radius squared times the identity matrix
ok so this the intuition is that I mean
this is what comes out and you also and
the other thing is that the ways have to
sum up to D ok this is just what you get
from writing down the KKT conditions for
the smallest volume containing a diploid
problem that's one way to look at it if
you want some intuition except as the
manipulations to get the dull this sort
of says that these V eyes have to sort
of point in all kinds of directions in
order for you to be able to do something
like that in order for you to able to
decompose the identity in this way ok ok
so this is what we use and the main idea
of the actual proxima chenal ger if is
to treat the sea is in some senses
probability weights let me show you
let's fake so let me show you
immediately what actually the algorithm
is because it module all this is sort of
simple so they are going to actually
find approximately largest simplex
d-dimensional simplex or rather d by d
sub matrix of this matrix would be to
sample our d columns from the matrix
each column is sampled independently
with replacement and column i sample
with probability proportional to C I and
the CIS are non-negative and they sum up
to D so the probability of sampling is
just see I over d ok that's the
algorithm so we compute this
loughner ellipsoid we can also get these
weights are some of the algorithms that
compute the lunar ellipsoid first
compute these weights so it's already
sort of for free then we use these ways
to UM run the sampling algorithm is what
I call randomized rounding ok let's now
try to go so let me now slowly go
through the analysis is just a couple of
slides so why is this good so let's
compute the expectation of the squared
value of a sample solution turns out to
be more convenient to work with the
squares for some reason you see why
that's okay ah alright so let's just
write that out okay um alright so what I
am so I said a sample with replacement
so if I um but if I sample the same
column twice I obviously get determine
on zero so those don't contribute so in
my expectation i only have contribution
from the terms well from the cases when
I sample d distinct things okay so for
each such set a good example I could
sample it in D factorial ways just they
all distinct so the difficulty of ways
to sample it and the probability of
sampling exactly those elements is
exactly the product of the probabilities
because it's sample independently okay
so this is the probability of our
sampling this set s i can sell pony
difficult or your ways and the value of
the city's actually this so just the
square I mean for me value right now is
the squared value so it's just the
square determinant okay ah alright so a
few more manipulation so again bi is
just see I over D let me just and there
and all these sets of sets of our of
size D so this gives me a D to the D
factor up front and there's also this D
factorial that doesn't depend on s so
just things that don't depend on s I
brought up front so k so i have this
remember this is actually
um the inverse of the approximation
factor i'm shooting for so i want to
somehow show that this compares with my
upper bound okay which is this radius
for the square value the upper bound is
the radius of the loaner ellipsoid which
is in this we make sure it's a ball to
the power 2 d now okay oh right and I
get just one more thing so this is right
oh I just brought the constants in right
so what I have is is some sub matrix
right and I'm taking product of CI over
I in this sub may in the sort of index
set of the colors of the sub matrix
times the determinant of technical vs
the sub matrix okay so what I'm doing
them just instead of having these two
things separately I can just multiply
the the eyes column in the sub matrix by
its coefficient R but it's not right but
because i'm taking squared determinants
i'm multiplying by the square root of
the of the coefficient of the column so
that i get the same thing this is max s
okay
so this last step is just sort of
bringing those things in um okay why is
that what I want to okay so this is the
this is the expectation of the squared
value of my algorithm of this randomized
algorithm are all right and I said it's
equal to this big summation times this
factor so this factor is the
approximation factor i'm going to be
shooting for and now i just want to show
that this is at least as big as my upper
bound on the optimal value and my upper
bound was this radius of the containing
ball r to the power D was my upper bound
for the biggest apps right of the
determinant but i'm squaring everything
so i just added squares on both sides so
I just want right so this my
approximation factor and this term i
want to show is this is at least as big
as my upper bound of the optimal
solution yeah hmmm uh yes fine it is
enough yes ah well for this
approximation okay yes fine yes yes I
agree I agree agree okay just go okay so
let's let's do that right so we have
this summation so what is this basically
are in my matrix have multiplied every
column by armed by square root see I so
square root C 1 square root C 2 and so
on and now this summation is our sum
over all d by d sub matrices of the
square determinants of the sub matrix
okay so i want to relate this to this
radius using john's theorem um and
I this turns out to just be a to follow
immediately from the been a cushy
identity for determinants okay this is
something I yeah right so it's one of
these things you just write out the
formula for determining tivity kind of
follows who has seen the binnacle she
formula before battery yes 12 / two
people cool okay so let me sort of say
in a little more generally so what it
says is that say you have some matrix a
so the essay this matrix ah looks like
this right is this shape I mean lady
actually doesn't matter but the makes
more sense in this way so and i'm
interested in the determinant of the
matrix x transpose right so the
determinant of a times a transpose okay
so if n is just a click a single row
this is just the inner product and it's
just the sum over all you know one by
one sub matrices of this of this Rho
squared right see if it's just single a
times a transpose the determinant of
this is just I mean this is just the
numbers which is the number and this is
just equal to sum of a i squared okay in
general this is equal to the sum over so
say this is again d by n sum of s subset
of and size of s equals d of the
determinant of a sub s where a sub s is
just the sub matrix determined by this
set okay so it's sort of a
generalization of this thing that this
makes sense all you can say sort of
makes sense for the you know one
dimensional case it also makes sense for
the case where this is a square matrix
because then it's just the trivial
identity
alright so what this gives us in this
case are what we have here is exactly
this summation where a is this matrix
and what it tells us is that this is
equal to the determinant of this matrix
x times X transpose this is exactly this
summation of outer products okay and of
course the two square roots see is we
can bring together now we have this
summation of CI times VI di transpose
that's exactly what John's theorem tells
us that this is equal to R squared times
the identity obviously the determinant
of this is just our to the 2d okay so
this kind of completes basically nicely
so what we derived was that the
expectation of the squared value of a
solution is at least this factor of de
facto rio verde to the D times well R to
the 2d which was on our upper bound to
the square of the optimal solution okay
and again you can use sterling to sort
of see that this is basically e to the
minus D minus little of T arm right so
this tells us that ok this samp this
randomized sampling algorithm are in
expectation gives us something that's
good and good here means sort of large
enough the problem is I think their
cases where this can give you horrible
things except with exponentially low
probability because well you only get an
exponential approximation factor here so
I think it's totally possible that with
high probability doesn't give anything
good so let's not run that algorithm
let's instead you randomize it and you
can randomize it using sort of standard
conditional expectations machinery so
once you do n demise it the guarantee
you get is that you get a deterministic
algorithm that has a guarantee at least
as good as this expectation guarantee
and that's enough for us and because it
is deterministic we don't you know it
just works I mean we don't care about
probabilities anymore
right and randomizing is a little
technical but it's sort of standard you
just have to compute the actual
conditional expectations okay okay so
that's yeah I waiting for the food
dimensional case oh and i want to a
little more vaguely tell you what what
what what happens when j is less than D
so when we're looking for less than a
full dimensional simplex that turns out
to be a little more technical but sort
of similar ideas work just have to work
harder okay so first give you a second
okay so switching context we sort of
back to the beginning of load sort of so
we can do basically the same
transformation sis before I we can guess
the first vertex of an optimal J
dimensional simplex against shift
everything ah so that so subtract v I'm
vi1 the guest vertex from every input
point and also take the symmetric point
around the origin so now we transformed
our problem too okay first it now
because we took both of these guys now
we have a problem that I mean input that
symmetric around the origin and now
we're looking not for J plus 1 points
but only for J points so that the
simplex they make together with the
origin are is as large as possible in
volume and again you can transform this
to a determinant problem and now it's a
little so again we have this matrix
basically let me not put this here can
we have this matrix arm but now the
problem is to find ok ah a d so d is
always the animation but we're looking
for a deep times J sub matrix it has
large is determinant well the problem is
ok what is the determinant of a d plus J
sub matrix and you can see that the
right thing for the pro
is to look at you know the the sub
matrix transpose times itself look at
data square matrix look at the
determinant of that and take the square
root this up to sort of a normalizing
constant is the same as this volume of
the J dimensional simplex ah what is the
probably easier way to think of this is
what this is is actually just the I
guess it would be natural also define
that as the determinant of open
rectangular matrix it is the product of
the singular values okay all right all
right so this is our problem now we're
looking to find a d by J sub matrix our
big input matrix are so the disarm a
tricks has large determinant a by
determinant I mean product of singular
values or this thing okay and are you
could again use the same algorithm as
before you know it's well-defined you
just sample J times instead of G times
and it does give you an approximation
factor but the factory looks like j
factorial over D to the D so if j is
pretty big it's not too bad it's not too
far off from what we're shooting for but
in general it's it's not good enough at
least it's not the right order of
magnitude sir okay so we will need a
different upper bound basically I mean
kind of make sense that this sort of
food dimensional okay maybe Sam quite
clear white doesn't work but i'm pretty
sure that the loner ellipsoid upper
bound doesn't work immediately so i'm
going to modify to something that I know
works okay so here is the upper bar i'm
going to use I don't know what said
upper bound age is the upper bound okay
so one thing we know it's not very hard
to show is that if all the points all
the columns of this matrix are
containing some ellipsoid e
and well because our point symmetric
around the origin we can always assume
this is centered at zero and say the
major axis of the ellipse order a 12 ad
and I've order them by length okay so
one upper bound is true is that the this
so this is my value of the optimal
solution that's what i call this subject
so the maximum determinant of D by J sub
matrix we know that this is bounded by
the product of the jailer longest major
axis for any ellipsoid that contains the
points it's not very hard to show um
it's sort of the right analog say of the
this harm our inequality plus loner
ellipsoid upper bound okay so let me
call this a jail on an ellipsoid it's
just so sorry what I koj down no jail
owner ellipsoid is the ellipsoid that
minimizes this upper bound over all your
choice that contain the points governor
love lastly I look at V so but right
should I bounce pronounce the umlaut
like oh no well I can Oh mwah you know
opener I'm not shaking ok so the J love
nor ellipsoid let's see it minimizes
right this upper one so this product of
the J longest axis over all users should
contain the input so this is this will
be so this achieved minimum movie this
upper bound that we're going to use
please problem and i also want to sort
of give you an algorithm that competes
against disapproval um let me because
you can i give you too much details
maybe it's motivation naturally it's
helpful but let me talk through sort of
the most trivial case where we were just
looking for the longest basically column
of that matrix so it's clearly to your
problem we just go over the columns take
the longest one so give you you know an
exact algorithm for this problem using
sort of the same machinery that i use
for the general case and we shouldn't
touch this probably all right so
all ah so the optimal value is the
length of the longest column and my
ellipsoid my optimal ellipsoid is it
doesn't have to be a ball but you can
see that are always an optimal solution
is a ball in this sort of trivial case
so we're just looking for an ellipsoid
whose longest axis I mean I mean name so
that contains the all the columns of
this matrix n is longest axis is a is
minimized okay so in other words you can
just look for a ball that contains all
the points and the radius is minimized
in the upper bondage is the radius okay
and the way the journey is going to work
is k you I'm going to write this as a
convex program I'm going to again take
the look at the KKT conditions take the
dull what the dough is going to tell me
is that whatever the optimal r is there
exists are non negative coefficients you
want to see and let's sum up to 1 so
this one is this one right it's the
dimension 1 remember in the food
emotional case this was d and what I
know about these things is that I can
look at the same matrix summation civi V
transpose now I know that the trace of
this thing is equal to the optimal r
squared okay but the trace of this thing
well Tracy's linear you can just take it
inside and the trace of the ID a
transpose is just the norm of V hi V
transpose square I mean the norm of VI
squared so what this tells me is that
there exists weight so that summation of
CI length of VI squared is equal to the
optimal solution which is my upper bound
okay so now if I sample v i would
probably see I this is the expectation
and its optimal so okay so this is an
optimal algorithm take take this program
right the dose of the dole take the
weights sample according to the weights
that actually in expectation works okay
so that's a very bad way to compute a
very simple thing but the thing is it
generalizes
for general J so this is basically what
we're going to do our we're going to
write the dual of the ellipsoid
minimization problem ok and the duel is
going to tell us that there exists there
exists the non-negative weights did some
up to j where j is the dimension so that
some function that's complicated i'll
tell you what the function is but for
now take it as a black box so that some
you know well defined but complicated
function of this same old matrix is
actually equal to the are optimal upper
bound that we get from the ellipsoid
minimization problem and this one all
I'm saying here is we know that this is
an upper bound on the optimal solution
ok and AH what I mean here is sort of
the challenge in kind of deriving this
whole thing is that for the jail
alternative side the object when you
write it as the solution of a
minimization problem the objective is
not convex but it's not differentiable
so that makes deriving the dual and
writing the KKT conditions a little more
complicated it's it's not a big deal but
it's just a little up here and it also
makes this function a little more
complicated um but the algorithm will be
the same so we'll compute these dual
weights are and then we'll scale them
down by jay so that they give us a
probability distribution will sample on
sir this should be j for example j times
independently with replacement using
these weights and this will be the
randomized algorithm again would you
randomize it at the end which we analyze
this i guess all i want to say is
probably sampling without replacement is
a i think it can only do better it's
just I can't really analyze it better I
cannot give a better analysis for that
ok so and why keep pointing it out maybe
just to be clear that that's what I'm
analyzing
actually it's all the same because for
all practical purposes i can replace any
point that's actually traveling once and
give the way the week it's probably true
ya think I think you're right okay so in
general how does the analysis work so
again I'll compute this is the
expectation of the sum of squares in
your values of the matrix that I get
this is this determinant ah ok so again
i have right so if i sample something
twice that gives me no contribution
again ah because right I'll get zero so
I'm i only get a contribution from all
the cases when a sample a set of j
elements of j distinct elements each
such said i can sample in j factorial
ways and the probability of sampling it
is the product of the probabilities I've
independence and again i have this j
factorial I should have put it up here
baby but be I see I / j so i get this
jay jay to the j as well because i care
product j terms um right and this is my
expectation now before i had this magic
okay sorry before it heads you know some
sort of magic formula that told us what
this was in the food dimensional case
well now there's some analog of this
which tells us what it tells us that
this big summation is equal to UM ID to
the degree J elementary symmetric
polynomial evaluated with the arm eigen
values of this matrix that we've seen
before ok so this you have to believe me
for this desperate sort of the technical
detail but it's not very hard once you
know it so useful fact basically right
so this is the sum so what's the degree
J elementary symmetric polynomial I'm
almost done um
you just okay so you have all the
eigenvalues here you just take every
step subset of j eigenvalues take the
product take the sum over all these
things okay so this is the expectation
and right and once again this is the
approximation factor that I'm going to
prove so what I need to show is that
this elementary somatic polynomial is at
least as large as my upper bound and by
duality I got that my upper bound is
actually this function whatever that is
so I need to compare basically this and
this I need to show that this is at
least this not a need it's sufficient to
show that this is at least this it's
enough I don't necessarily need it but I
really like if I can do i agree i would
really like it if i can do that all
right so let me just so that you don't
complain that I haven't shown you this
this is the function I guess maybe
doesn't look so bad so this is what the
function looks like okay so it's again
the lambda I are the eigenvalues of this
PSD matrix and it turns out to be you
take the product of the top few top k
and then you take this it's kind of like
an average to the power j minus K in
case well defined by this I mean k is
the unique number that satisfies this
you have to so I had to show if this
from it wasn't obvious that this is ever
satisfied but it turns out to be
satisfied for unique k and this is the
function it's a little strange because
it's not differentiable so you get this
yeah in the duality you get that some
what's the word sup some sub
differential of something is equal to
something else it gets a little okay but
what we can I mean what I can show is
that sted the right thing yes sorry this
is reverse yeah this this this is the
wrong way uh so what I can show is that
this function is always bounded above by
the sedimentary symmetric polynomial
I'll just give you the kind of the key
word there is that this follows by
staring at it and using short convexity
of elementary symmetric polynomials okay
so it's a short convexity is very
general tool for proving inequalities
turns out to be useful here okay oh all
right so that's it so in summary I
talked about the first polynomial time
algorithm to approximate the maximum J
dimensional simplex problem within a
factor which is exponential in the
dimension which is kind of which is the
right kind of dependence for the
approximation factor it gives a constant
factor approximation to this determine
alluro bound for linear discrepancy and
so there are a bunch of open problems
what's the right constant in the
hardness is one problem um for these
ellipsoid upper bounds is this the Titan
alisis can is that all they can give
maybe that's an easy problem but I
haven't thought about it too much oh
right and is the approximation factor
optimal maybe it is its natural is this
e to something um okay so this is this
is this promo hit and whoever else is
interested but that's what we've talked
about a little bit so super disparate so
I'll tell you just very quickly so the
following function R is modular it's a
somewhat your set function so the sets
are you know subsets of the columns of
this matrix and the function is log of
the determinant of the sub matrix and
again the determinant what i mean by
determinant is the product of the
singular values okay so that is a
modular function and our problem is
really maximizing this module function
um module on with the constraint that we
need get a set of size J so the
constraint is a
okay there we go sorry just a second so
the constraint is that the sake you pick
should be a basis of the uniform may
Troi the problem is that the function
that sub module is is a is a log of the
objective function so to get an
approximation guarantee for the actual
objective function I need an additive
guarantee for the submodule maximization
problem and I'm not sure maybe you can
get that with sort of standard machinery
but at least using black box results arm
I didn't quite see how to get it does
this make sense is far as connection to
submerge your maximization so if if you
can get it using just some modularity
somehow maybe you can generalize to
something else it's basically why I'm
asking you might also give some
intuition what's happening that's thank
you is there like okay so you always
present in this duel board of the Texas
is implicit in this upper bomb that you
have but I if I just like can be like a
conventional ization directly which is a
relaxation for home yes video because I
mean that's a good question so SVP for a
volume problem or determinant problem
seems a little some comics program yeah
so for the full dimensional case so from
the door you get that it's just that for
the less than four dimensional case it's
a little strange so for the full
dimensional case is just a maximizing
log of determinant of summation see IV i
v-i transpose huh let's convex right I
mean that's familiar ah subject to GI
variables CI subject to summation see i
equals d CI bigger than 0 for
I so that's an okay you can also see
immediately that that's a convex
relaxation it's just not how I came up
with this and it's also not it doesn't
allow me to easy to generalize it to the
general JK's yes um because you saw it
turns out that well at least if I go the
ellipsoid way the dull that I get for
the general case has a much more
complicated objective function that I
would have never come up with just by
looking at the door this you could come
up with the full dimension case you
could come up with why can you see it's
a it's a relaxation just the usual way
just take the c is to be the an
indicator of an optimal set so feasible
but yeah maybe there's a more general
relaxation is what I'm saying okay maybe
there is a much more natural relaxation
it you can come up with without looking
at ellipsoids yeah more questions thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>