<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits | Coder Coacher - Coaching Coders</title><meta content="Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mi_G5tw7Etg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so thanks to the organizers for inviting
me this work is about retractor machine
learning what are the monsters involved
here we'll see as we go through the
slides this joint work with my
collaborators Daniel Hsu from Columbia
Satine Collier from Yahoo research John
Lankford Lee Heung Lee and Rob spirit
Microsoft Research so interactive
machine learning might mean many
different things to many different
people so let me start by sort of
setting up the problem a bit so the
first motivating scenario we can think
of is one of personalized medicine so a
patient is manifesting some symptoms
they go to a physician they have their
medical history they might have their
genomic data that's also available to
the physician and whole bunch of other
information right and based on this the
physician prescribes a treatment and the
patient's health response in some way
improves or worsens and hopefully the
goal of the physician is to find
treatments over time that lead to
improved health outcomes in a completely
different context you might think of a
user interacting with some kind of
online web service such as a search
engine such as a news portal such as the
social media website whatever you have
so this user might have a profile on
this website they go to the website
maybe with some cookies detailing their
browsing history session information so
on they might have issued some search
queries based on all this sort of
content contextual information the
website renders some content maybe you
know this displays some search results
some adds some new stories and and then
they use a response in some way they
they might click on some things they
might explicitly hit the like button or
they might just get bored and navigate
away right and typically the website
designer here has some desirable goal in
mind they maybe want to satisfy the
users information need maximize the
number of clicks or likes or whatever
and they want to present content that
will lead to better user behavior a
desired user behavior over time now
these applications might look very
different in the face level but they're
actually sort of instances of a broader
problem setting that we call the
contextual bandit learning problem and
this is actually it's nice because this
has been touched upon couple times
already in today's talks by by Leon and
by Johan in different contexts so this
conditional bandit learning problem
think of this really as a sequence of
repeated interactions so we first
observed so we'll have round index by
little T and we have capital T of them
at each round we observe some contextual
information denoted by XT you can think
of this as the user profile as the
patient's symptoms there is some
simultaneously there is a reward vector
generated we'll talk about that in just
a moment so based on this context we
choose an action a T for simplicity I'm
going to just think of the number of
actions being fixed at some value K for
now although this can be generalized so
these actions are like the different
treatment options available to the
physician or the different choices of
ads or content that I might select now
the word bandit in the title sort of
denotes to the refers to the fact
there's a partial information setting
which is you get feedback only on the
action that you took so the there is a
there is an underlying reward vector
that reflects the quality of each action
in this context you don't get to see
that you get to see the reward only for
the action that you took and you have no
idea what would have happened if you had
you notice prescribed a different
treatment of course because you didn't
do so and so this sort of relates to the
multi-armed bandit problem in statistics
that's why the word bandit comes in now
since these are rewards a natural sort
of desirable goal is to come up with
algorithms that over time learn to take
actions with deadly to large rewards for
simplicity we're going to assume that
the pair of contexts and rewards is all
wrong iid overtime this can be relaxed
in different ways in other words
of course the distribution is fixed but
unknown we're going to also assume that
the rewards are bounded in zero one okay
so the this problem turned turns out to
have not empty but actually quite a few
challenges so there there is sort of a
classical issue in almost every
sequential decision making problem which
is this exploration exploitation
trade-off so on the one hand you want to
exploit what you've learned so far you
want to sort of keep prescribing the
treatments that do well according your
past experience but there might be
better ones out there and so you when I
explore you want to find behaviors that
lead to higher rewards of course you
have to be careful if you explore too
much you might spend a lot of time
exploring suboptimal things if you
explore too little then you might end up
being stuck in some local optima now
sort of a unique a more unique challenge
here is that you need to take this
contextual information into account very
effectively so different contexts might
have very different optimal actions so a
treatment that works well for one set of
symptoms naturally does not need not
work well in a different set of symptoms
right it's not like I'm always going to
prescribe the same drug at the same time
you might not even see the same context
twice so even if you visit the doctor
twice you might have very different set
of symptoms so you you want a sort of
leverage in that can take the contextual
information but you don't see that
context over and over again repeatedly
so you have to learn how to transfer
information across them in some sense
and doing this in the right way is quite
challenging and finally we would ideally
like to do this extremely rapidly even
at a web scale in real time so we want
the approaches to be extremely
computationally efficient right so the
the first two challenges here are
statistical and then the third challenge
is going to be computational so in this
talk I'm going to describe sort of a a
new approach that tackles this problem
in its full generality and can guarantee
optimal statistical performance
computationally it's much faster and at
least you know simple
city lies in the eye of the beholder so
we claim it's conceptually simpler than
many of the predecessors okay but just
to again relate it to things people
might know a special case of this
problem is the multi-armed bandit
problem so just like our setting in
multi unbanded you you pull an arm and
you get to observe the revolt only on
that arm right so you get the same
partial feedback however there is no
context so there is sort of an expected
reward associated with each action and
that stays same throughout and so your
goal is to basically find that best
action that used to the highest expected
reward without having to worry about
these changing contexts on different
rounds and kind of you can think of this
as being a tacit assumption that well
this is a desirable goal if there is a
good fixed action over time so really
maybe you are thinking that there is a
single treatment or single set of ads or
recommendations that every user is going
to like that seems like a bit bit too
much at least in this in these types of
problems so maybe we want to sort of get
away from this assumption and do
something richer and one way of doing
something richer is by lifting your
reasoning from actions to what we call
policies so policy is simply a rule that
takes as input a context information
about your current user or patient and
maps it to an action to a treatment so
the reason why a policy is powerful is a
good policy can take different good
actions in different contexts it doesn't
have to do the same action over and over
again and at least very in a canonical
way you can think of almost a policy as
being something like a decision tree for
instance you know it's sort of branches
maybe on your sex your age your location
all kinds of simple demographic
information and adn decides okay based
on this information this seems to be a
good action for you right so that that's
kind of the intuition we want to have
and the goal now changes to try and find
a good policy pie okay
so I have to qualify that a little bit
more previously we wanted to find
one of K good actions so now I'm talking
about policies so I'm gonna in
particular think about having some fixed
class of policies that's rich enough and
I want to find the best one amongst them
and the way you should think about these
policy classes maybe again things like
for instance decision trees of bounded
depth or linear models of bounded norm
or neural networks with given size
constraints right so these are things
that are all fairly rich fairly
expressive and so now we are thinking
that given all these rich expressive
policies hopefully there is at least one
which will lead to high reward in my
problem and this seems like a more
reasonable assumption to make then
expecting that one good action always
works so the goal now again is we're
basically running kind of an experiment
here right we're we're gonna enjoy this
why this is an experiment and versity
we're sort of randomizing and so on will
become clear in in the next few slides
so through this experimentation really
we are trying to discover the best
policy in the class and this is great
because these policies are very complex
and expressive which makes the approach
powerful but this also leads to some
severe statistical and computational
challenges because a we have to be able
to do our statistical analysis over
these last class of policies without
losing power and computationally you
have to be able to handle them
algorithmically
and you know wonder realizing why these
why this becomes much more challenging
is previously you are doing explore
exploit in a multi unbounded problem at
the level of K actions where K was
something modest and now you're sort of
thinking about doing the same reasoning
but at the level of all linear models
for instance or all decision trees of
bounded depth so you've just sort of
blown up the scale of the problem by an
enormous amount okay so now roughly what
our formal model so this part stays the
same as before this part cannot changes
so as before
we are trying to maximize our rewards
but given a policy class we now get a
benchmark to compare our performance to
so suppose we were to use a policy PI to
take the recommended action XT on round
T then this is how much reward we would
have picked up using policy PI and so I
can compare myself to the cumulative
reward of the best policy PI right so
this is the best I can hope to be given
my constraints that I am using a
particular policy from this give it
given that I am using a policy from this
particular policy class and then this is
how much I actually get right so this
quantity instead of game theory and
learning is often called regret this is
my you know this is how much I get and
this is what my regret is for not using
the best possible policy that I could
have used you can also think of
basically this difference as defining a
risk function so you're trying to sort
of Mac minimize your average risk over
time or minimize your average regret
over time what I am the same so this is
this this problem actually goes goes
back quite a long ways I mean you can
also think of this first of all as and
as a SM adaptive experimental design
problem but in probably the most related
in this context the first word that
comes to mind is Thompson sampling which
were proposed in 30s it can be
empirically quite effective off and
although there is not a very general
theoretical analysis for this algorithm
we know results in some special cases
probably the most general result that
came out first was in 2002 by this
algorithm called exp for which
remarkably was already statistically
optimal but computationally it require
exponential time essentially it was
enumerated over sort of all possible
policies that you might want to reason
about then so in 2011 there was sort of
a breakthrough this paper came out which
was colloquially called the monster
paper by their own authors for various
reasons and part of the reason was
despite having optimal regret
actually the computation really the best
you could say is it was poly time it it
did require a monstrous amount of
computation so in this work we sort of
improved in the on this 2011 paper in
multiple ways and in ways that retain
the statistical optimality but bring
down the computational very
substantially so it's still super linear
but in some sense it's kind of
unimprovable for certain class of
approaches at this point okay so in the
remaining time I want to give some key
ingredients of our solution so let's
begin with a fantasy setting we're going
to pretend that we we get full
informations on each context I can see
the reward of not only the action I took
highlighted in red but actually of all
the possible actions I could have taken
right so obviously given this
information I can compute my own total
reward that's just the sum of the read
entries but I can take any other policy
pi see which actions it takes
highlighted in green and then I can
compute the reward of policy pi right
this is easy in fact I could do this for
every policy in my class and then find
the best one according to my data right
so that gives me sort of an empirical
estimate of the best policy and you can
very easily show using standard
statistical arguments that this using
this policy at each round leads to a
small regret so this this is a good
policy to use this this is statistically
unimprovable anyways so so this this is
as good as it can be you might also ask
what about the computational properties
of this procedure right so as long as I
can find the best policy
the one with the largest empirical
reward I would in fact in this full
information setting have a
computationally efficient solution so
this this at least gives some hints as
to what we might require computationally
in the partial information setting
because this seems to be something we
require even in the full information
setting and so we give this just a fancy
name we call this an R max Oracle and we
assume this is efficient so a nod max
Oracle is basically something that takes
in contexts and reward vectors overall
the K actions and spits out the policy
that maximizes the expected reward
essentially you can think of it as an
optimization Oracle for your policy
class and just sort of allows you to
exploit whatever structure might be
present in your policy class
computationally and the the reason why
this is practically reasonable is for
many policy classes of interest we know
how to we have practical implementations
of these Oracle's so for things like
again linear models or decision trees or
neural nets that the kinds of function
spaces we are used to dealing with in
statistics and machine learning we do
have efficient targets Oracle's exactly
or approximately so this is going to be
yes so for simplicity I'm going to
assume that the policy class is finite
everything goes through if essentially
your policy classes VC class or
something that essentially has a log 1
over Epsilon type covering number two
accuracy Epsilon so but but I'm not
going to do that version in the talk ok
but coming back to reality we don't have
this fully filled out table of rewards
we only get to see these red entries so
we could still accumulate our total
reward but now if I take some other
policy PI and try to assess its quality
and I hit a bunch of blank cells in this
table right I have no idea I didn't take
those actions I didn't observe their
rewards so I cannot compute PI's total
reward and if I cannot do that even for
a single fixed policy PI I can have no
hope to just optimize over all policies
in the way I was able to do before
however this is the place where sort of
an old trick from Statistics comes in
handy this has I'll be a little quick
because this has been already talked
about twice today at least so the idea
is you don't act deterministically you
act in a randomized manner so the action
you choose will be drawn from some
distribution P
peaty that i haven't specified yet but
but you'll choose an action to take
according to some distribution PT and
now you'll create a fully specified
reward vector reward estimate RT hat
which will have entries for each action
a so obviously K minus one of them kind
of have to be zeros in the simplest
reasoning at least because you didn't
observe any information on any of these
this is the only one where you got some
information and that information was RT
of a T but you also divided by the
probability of taking that action so
that when you compute expectation over
your random choice this quantity ends up
being unbiased so our t hat it's easy to
show is an unbiased estimator for the
true reward when biasness is great
although it does leads to some
challenges because the range or and
variance of this guys are going to be
bounded by 1 over probability so how if
you have small probabilities you're
going to run into troubles statistically
and we have to deal with that but
conceptually this object is great
because now you can take these are hats
pretend that you know you have full
information so I define the quality of
any policy pi the same waiver we were
doing before but now under these R hat
vectors right so you can define the cost
of estimated cost of any policy PI and
you can even plug that into a narc max
Oracle and find the best policy pi
according to these are hats of course
that's not same as finding the best
policy pi according to the true rewards
of its fishing and so how exploration
comes in basically will come in through
this distribution PT and exploitation in
some sense you can think of as finding
the best policy pi according to this but
it's going to be a bit more subtle than
that ok so yeah so a template level you
can think of a algorithm now which
observes the context and uses the
distribution over actions takes an
action observes the reward and creates
these importance weighted reward
estimates right and sort of the detail
to specify is how this distribution is
constructed now
one thing that becomes pesky here is the
quality of actions like I mentioned
changes from round to round based on the
contexts so it's really hard to think
about given a given a context how do i
construct a distribution directly over
actions what ends up being convenient is
to think about distributions in the
space of policies because the quality of
a policy kind of remains same over time
a policies is that either good or bad
and it stays good or bad you might not
know whether it's good or bad yet but at
least in distribution it stays good or
bad and so the idea is you use you
basically maintain distributions over
policies and induce distribution over
actions using a distribution over
policies this part is simple you just if
you have a distribution or policies you
draw a policy from that distribution
take its action this since it induces
the distribution over actions right so
conceptually this part is easy and then
each time you get some more information
you compute a new distribution over
policies PI right so you you've figured
out something more about which policies
have better and worse reward properties
and you can use that to update these
distributions right so how we get these
distributions over policies we actually
specify an optimization problem over
distributions on policies and the
optimization problem it is a feasibility
problem so it has a bunch of constraints
some constraints say that you should
exploit enough some constraints say that
you should explore enough and the upshot
is statistically you can guarantee that
if you find policy distribution
satisfying this optimization problem and
act according to them at each round then
you're going to be statistically optimal
so this this optimization problem sort
of sufficiently encodes all the
statistical properties required from the
distribution running really short on
time so I'm going to skip most of this
except I'll say that using sort of this
same R max Oracle abstraction that we
mentioned before you can use that
abstraction repeatedly to essentially
construct algorithmically a solution to
the optimization problem of interest so
you can find the distribution over
policies by
making sort of repeated calls to the odd
methodical and you have to make about
rooty actually rooty calls over all over
the length of the algorithm and each
call sort of takes about order T time so
that's fire computation scale says T to
the three-halves roughly and you can
prove that this is optimal for a certain
class of algorithms okay so one way to
think about sort of this this whole
framework is it's taking this contextual
bandit problem which is you can think of
it as I mean this is definitely a much
much harder problem than standard sort
of classification or regression but it
tries to reduce contextual bandit
learning to classification in a way so
that whenever you have an efficient
classification algorithm you can try to
build an efficient contextual bandit
algorithm and the key object for doing
that is this computational abstraction
of an alpha X Oracle and the way we use
it is resolve this code new coordinate
descent algorithm we came up with that I
didn't talk about actually the the
upshot of the overall development is
that you get something that's both
statistically optimal and
computationally practical you can extend
it in different ways to handle sort of
structured actions such as rankings
lists and so on and we have ongoing work
in this direction but yeah I'm going to
stop at this point so we have time for a
question or two
I'll ask a question and you mentioned in
passing you have a K times T to 1.5
complexity mm-hmm you mentioned in
passing that this is unimprovable is
that a precise statement or yes
Morrowind yeah yeah yeah there is a
lower bound so there is there is a lower
bound that says that so so we have a
particular exploration constraint we
enforced here and the lower bound says
that if you want to find a distribution
that has that satisfies this constraint
then it needs to have a support on it at
least
rooty unique policies and and you have
to sample from this distribution
basically at each round so the cost of
sampling scales with the support of the
distribution since you have to do this T
times so at least for all distributions
that satisfy this constraint we have
complexity is unavoidable so the
additive one certainly multiplicative 8
is yeah that cannot right because I mean
that now now there depends on how big
that quantity is that the multiplicative
factor is hitting but I mean one day one
day so the way I like to think about
this so consider for instance like this
in the simplest case you can you can
think of this Oracle as something that
would efficiently minimize something
like a zero one classification loss
right now obviously you you're not going
to be able to do this do that in a
assumption free way but for instance we
have various statistical assumptions and
various restricted families of problems
where we do know that this thing is
efficient by using either you know
logistic regression and linear
regression perceptron or whatever based
on the assumption you make right and in
all of those settings suddenly
you can then do this as well right so
really the statement I am making is
whenever that underlying classification
problem is efficient then I will have an
efficient solution also so I'm thinking
how we can apply this issue whether you
can Morros forecasting Awards so the
rewards are drawn the rewards and
contexts are jointly drawn from a
distribution know there it's a it's a
joint distribution right so I'm trying
to find the oh yeah there so so so you
have a joint distribution yeah otherwise
it will be not particularly interesting
if the distribution was not and the
distribution of rewards has to be
conditioned on the context for things to
be interesting I completely agree so
that's a perfectly valid thing to do and
one thing so I have two answers to that
actually some approaches explicitly to
contextual bandit learning explicitly
operate by first trying to train a
reward predictor and then you will just
take the best action according to the
reward predictor you can show that these
approaches work under certain
assumptions for instance if the
parametric model you assume is basically
close to correct then you can do this
the other way you can use this is so
here I am describing sort of the
simplest estimator for building our hat
which is just this importance weighted
estimator but for instance as Liam was
mentioning in his talk you can also use
these doubly robust estimators which
combined importance waiting along with a
reward predictor so that's maybe a safer
way to use a reward predictor in the
setting because then it exploits a
reward predictor when good and falls
back when bonds waiting when bad so
there shouldn't be any confounding here
because we are using randomization over
the over the treatments right so unless
so what do you mean by doctors making
the right treatment choice no so this is
this is really what is telling the
doctor you can think of this is telling
the doctor what treatment to prescribe
right so I specify I'm trying to learn a
distribution over the treatments and
then when you see a new patient you
should describe you should prescribe a
treatment drawn from this distribution
know this is this is this is this why
this is interactive learning there is no
data that first comes in right so if you
have a observational data you will
always have the confounding problem and
I I can't deal with it because that can
potentially have uncontrolled amounts of
bias so so this sort of is this is
actually data collection I'm running an
experiment and collecting the data from
which I know I can do principal learning
right so a simple analog of this that
you can think of is you would just run a
randomized trial up front and then try
to do daran try to find the best
treatment from this randomized trial
post hoc and this is kind of doing
simultaneously the optimization and
randomization so that you can quickly
focus your randomization over better the
good treatments only and discard the bad
treatments very quick
but but there is no sort of data that
comes up front because that can have
uncontrolled biases and obviously I
cannot deal with it then okay let's
thank Alec and also thank all the other
speakers in the session each year
Microsoft Research helps hundreds of
influential speakers from around the
world including leading scientists
renowned experts in technology book
authors and leading academics and makes
videos of these lectures freely
available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>