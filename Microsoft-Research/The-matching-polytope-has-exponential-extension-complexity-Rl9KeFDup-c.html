<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The matching polytope has exponential extension complexity | Coder Coacher - Coaching Coders</title><meta content="The matching polytope has exponential extension complexity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The matching polytope has exponential extension complexity</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Rl9KeFDup-c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay hi everyone ah it's a great
pleasure to introduce Thomas who just
came to Seattle area is you dumb and you
will talk about some very exciting work
that is done on matching polytope ah
thanks a lot wait yeah yeah thanks for
inviting me um yeah I want to talk about
the one of my recent papers which is on
essential complexity of the matching
polytope okay so what are we actually
talking about now let's imagine you have
you have a polytope p and when usually
the way how you're a person to put
little bits by by giving giving me the
inequalities but you could imagine that
you have a you have a different way of
writing the polytope so you could
imagine that you have you allow some
extra variables and then you take all
the points so that they're there is a
solution for your eggs are variable to
that it's a different constraint system
is satisfied and what it what this
geometrically means is that you have a
higher pulley to Q which essentially is
the solution to that second system so
that if you project it down on the X
variables that gives your original
pollito p now you might wonder why
should you do that and the what the
point is that there are many examples
where the original inequality
description needs let's say
exponentially many inequalities but it's
possible by adding those extra variable
to reduce the number of constraints to
let a polynomial and if you can't here
we have we have apple it'll be with with
with eight inequalities and that higher
dimensional polytope actually needs only
six ok and now formally you can define
the extension complexity of such a
polytope p as the smallest number of
inequalities that you need to define
such higher dimensional polytope that
you can project down and and get p okay
so this is somebody this is a good
measure for the complexity of a
political and that there are tons of
examples where the the extension
complexity of the polytope is bounded by
polynomial even though you're
the number of inequalities of the
original palitos HH exponentially large
so for example this is this works for
for the spanning tree polytope or the
primal children in this talk we actually
we're going to care about the other the
other direction so for which pulley
tubes can you show that the pulley tubes
are they are really hard in the sense
that there is no way to significantly
reduce the number of of inequalities
they also has been some work there was
some work done and the whole story
actually starts with the paper of Vienna
caucus beginning of the 90s where he
showed that are there is no symmetric
linear programming formulation of
polynomial size for actually for the
matching polytope and for the TSP
pollito um now this is a this assumption
that he talks only about two major
kelpies well that really restricted the
number that really restricted the
applications but essentially what he
wanted to do is he just wanted to show
that then a bunch of papers who claimed
that NP book NP equals P that all those
those approaches were wrong because they
were picture based on on a symmetric LP
so this worked to reject those papers
but then actually later it turned out
that this restriction of looking only at
symmetric a piece it's really a strong
research then well actually if you if
you look at at random 01 polytopes using
some counting argument it's not so
difficult to show that the extension
complexity must be large also if you
look at non symmetric apiece but that
didn't really lead anywhere until there
was a big breakthrough of uranium Asarco
kuta Tiwari and the wolf and they showed
that for the for the Traveling Salesman
problem the linear program must have
exponential extension complexity for
generalities now these techniques you
can you can extend them and you can you
can also show that
are certain polytopes that you cannot
you cannot approximate well so this
approach action was somewhat very
flexible to show that there was a there
was a essentially some some probably a
line of work there was some very
recently by by James and and others and
what they showed is that you cannot have
a linear program for macs card of
polynomial size which which gives you an
approximation factor of better than two
that's somewhat surprising because you
probably remember that the SDP it gives
you a factor which is much better which
is just 1 point 1 3 so this already
shows that LPS are much weaker than then
what you can do with st p's but you
might argue that actually those
polytopes here are for which you see
unconditional lower bounds the
underlying pulley tubes are all NP hard
ah well it's a little bit debatable well
max cup the underlying polytope is
np-hard well you have a spectra helium
which is better than 2 minus epsilon but
okay if the underlying polytope is
np-hard yeah we have a couple of
examples so what about a polytope which
is actually which is nice for little in
the sense that you can optimize any
linear function polynomial time um can
you get any kind of lower bound on that
and the prime example that then exists
for this is actually the perfect
matching polytope now let me just
briefly remind you what is the perfect
matching polytope we're looking at a
complete graph on their notes and while
it could you prove you remember what a
perfect matching is so now the perfect
matching Pullo to bits it's the convex
hull of all perfect matchings in that
complete graph and if you if you try to
write down linear linear constraints
that are supposed to define your pollito
then you start with these constraints
you say well kind of for every noise you
want to pick one edge outgoing but then
you quickly realize that this is not
really defining you the perfect matching
polytope because this does not rule out
extreme points of this form actually it
does not rule out
feasible point of this form so you could
have some odd cycles with very one half
everywhere so you need some additional
inequalities that also kill these kind
of solutions and that you can do by by
looking at every every set of odd
cardinality and then you can require
that well if you have a map if you have
a perfect matching then you must have at
least one edge leaving leaving dead set
you so you can write down some more
constraints which actually exact ask for
exactly that and this is a very
classical work of jack evans and he
already proved in the 60s that this
really describes correctly describes the
convex hull of our perfect matchings and
okay edmonds also given algorithm that
that can optimize any linear function
over this polytope in a polynomial time
just by finding a maximum weight
batching and you could use that already
to solve the separation problem but
there's actually also different
different wages of the separation
problem which is maybe a little more
elegant which is due to pack a growl so
it's kind of late ni spoleto but if you
look if you count the number of
inequalities that you have ah there are
exponentially many so you have really
exponentially many facets um and then
you wonder can you can you significantly
reduce the number of inequalities that
you have by introducing something so
variable ah and what this is what we're
going to see here this is this is
actually not possible so you can the
only thing that you could present put
that you could do is actually reduce the
constant that you have in the exponent
but apart from that this description is
actually almost october so there's no
way to be sub sub exponential in the
number of inequalities and everything
that was previously known to the best of
my knowledge was only they essentially
trivial lower bound that you get from
the dimension okay yeah before I go into
details and show you how to prove that
let's talk a little bit about the the
theory
for example formulations in particular
we need we need to talk about what's
called the slack metrics ok let me
remind you what is the slack matrix of a
polytope e you can you can look at the
pollito when you cook and look at the
burgesses and you can also look at
inequalities and now the slack matrix
it's a huge matrix is that in
exponential size matrix so that you have
you have a row for every facet and you
have a column private vertex and the
entry that you have are for a vertex and
faster it's it's the distance it's the
essentially the distance that the vertex
has with respect to the facet so it's
it's the slack that the vertex has with
respective the facet ok we need one one
more definition which is the soap with
non-negative rank of a matrix that's the
smallest number of columns of the matrix
U and rows of a matrix V so that you can
find non-negative matrices so that you
can factor you can factor your original
metro excess and if you if you would
drop this non negativity condition then
you would recover the usual rank that
you know from linear algebra so this
quantity is always at least at least the
usual rank why why is there is no
negative rank why is this interesting ah
because of the theorem of yellow car
keys from his classical paper where he
showed that you take any polytope do you
take any slack matrix and then actually
the extension complexity of the polytope
it equals the non-negative rank of the
slack matrix and this is a very nice
elegant relation in particular this is
very helpful for proving lower bound
because instead of arguing on some some
kind of higher dimension put it away
actually you don't know the polytope you
need to argue only about one slack
matrix and you really know the slag
metric so you'll have to only to argue
only about one object which makes things
much easier if we have two minutes I can
actually just briefly outline the pool
of it you like um okay so one direction
this is its kind of easy so let's
imagine you do have a non-negative
factorization of the slack matrix then
what is the what is the extent the
extended formulation well you can just
write your region polytope as all xo
that there is a non-negative y so that
you take then you take the original
inequality system and then you just you
add the left side matrix plus that
non-negative why that should equal the
right-hand side and that kind of makes
sense because if you if you imagine that
you have a you have a vertex X I and you
want to prove me that this is in the
this satisfies this this thing so this
lies on the polytope then you can just
take you can just take the corresponding
column of V and that's just your Y and
then you multiply this stuff and then
this gives you precisely that the slack
vector that you need to make this in
equation okay the other direction uses
duality and how this works is the
following let's imagine that this is
your Pluto and suppose you do have you
do have a higher dimension for little Q
that projects on p let's say with this
description and now I claim that UK you
can also find you can also find a
factorization where the size of the
pressurization is the number of of
inequalities that you need to describe q
how that works is the following well
what do we need to do we need to find
for every for every inequality of the
original polytope we need to find a non
negative vector U and for every vertex
we need to find a non negative vector V
so that their scalar product gives gives
the slack which this vertex has with
respect to that inequality the way how
you do that is ok now you have a you
have an inequality here of the polytope
and baidu ality you can use some
inequalities of that higher dimensional
pollito
and you can add up non- multiples of
them in order to combine that inequality
and you take those non-negative
multiples and those coefficients give
you the U vector and then you look at at
the vertex and you also want to find a
non negative vector for it the way how
you do that is you look up into your
high dimensional pollito and you take a
point here and then you take the slack
vector which this point has with respect
to all the inequalities of that higher
dimension pull it up Q this is again
owner- vector and if you if you
calculate what's there in a product and
it turns out this is precisely the slack
of this vertex with respect to that
inequality okay good the extended
population
the first direction yes
howdy content is finally campos
testing process
oh ok ok so now the number of equations
is as large as a number of inequality so
that might be large ah but the number ok
the number of inequalities that you have
it's oh yeah yeah the number of
inequality is just just a number of
columns of you so you have a tons of
equations but many of them are redundant
so you could essentially throw out
everything which is redundant just take
take a non-redundant subsystem yeah but
but you're right I this might still be
large but you can take a subsystem with
which then smile why is not yes yes ok
um yeah they also this paper of fury
niet all that i mentioned and the
technique which they use to show that
the extension complexity is large and
actually the non-negative rank of of the
slack matrix is large the way all they
do that did that was by using the super
rectangle covering all about and what it
says is just that well the de- rank of
any slack matrix of any matrix must be
at least the rectum covering a number ok
so let me just let me just give you a
quick picture now imagine that this is
this is any metrics and inter- metrics
and let's say that this is this is a
non-negative factorization and now let's
let's just forget the numbers let's just
look at other number zero or the other
positive then actually if you look at
the if' column of you and you look at
the i row of v then this this induces a
combinatorial rectangle and so that the
rectangle it has only positive entries
in that slack matrix s and if you look
at all those are non-negative rank many
rectangles then you can see that they
cover they cover the other positive
entries of of the slack metrics yes
okay okay and it doesn't cover any any
zero injury okay so the the natural
question is if you want to show a lower
bound for the perfect matching polytope
we could try to apply this rectangle
covering low about so let's try that
well it actually will turn out that this
doesn't work but let's see why doesn't
work okay now I imagine that this is
this is a slack matrix and this is let's
say this is the slack matrix of the
perfect matching color taupe okay the
perfect matching polytope it actually it
has a bunch of different constraints
that the the degree constrains the
non-negativity constraints and there are
these odd whole inequalities but only
the ordinal inequalities there are
exponentially many of them the others
are on the polynomial main so actually
we only need to care about that part of
the snack matrix which comes from the
auto inequalities let me remind you that
that we have entries of the following
forms of this slack matrix and then we
have for every odd cut you and for a be
matching perfect matching em we have an
entry which is of the form the number of
edges which they had they have in common
minus 1 because this is the slack okay
and I I claim that we can cover the
positive entries in that partial slack
matrix with only into the for many
rectangles how that works is the
following we take every every pair of
non incident edges and we okay and for
every such where we get one rectangle so
what I need to tell you is what is it
instead of matching their lies in
directing and what's the set of cuts
they lie lie in the rectangle and the
set of cuts is simply the set of cuts
which cut both edges and the set of
matching is simply the set of matchings
which contain both edges ok now this way
you see that every entry um which lies
in this rectangle must have a positive
slack because they share at least those
two edges he 182 and the other way
around if
have any entry where you have a positive
slack then you must have at least two
edges in common well actually by parity
reasons at least three so you can just
take two of those edges and and you see
that that the entry isn't in at least
one rectangle if you look a little
closer if you know generally if you have
a if you have an entry um and then share
K edges then this this entry will be in
precisely k choose too many rectangles
okay now we could be a little bit naive
and we could say that well now this
lower bound doesn't work so we have an
upper bound on a lower bound which is
actually useless but this might give us
some ideas in fact now let's try to to
take this construction and let's try to
get a factorization out of it so let's
try to do the following let's try to
override the slack matrix as the sum of
all those into the for many rectangles
and what I mean here is that you have
that rectangle and that gives you a zero
one rank one matrix where you have a one
entry if that entry lies in the
rectangle in the 0 otherwise and now now
I'm wondering why is this not a correct
factorization well obviously it is not
otherwise that the title of the talk
would be different but what's going
wrong ok now let's let's look at an
entry um where they share K edges then
you know that on the left hand side for
that entry you expect a value of K minus
one on the right hand side you know that
this and this entry is in roughly is a
quadratic number of rectangles so the
value that you get is like this curve
okay so it doesn't work now I do have
some freedom I could put some non
negative scalars here in front but you
see that there is no way how I could put
scalars there and make this curve equal
to that curve so that doesn't work
you say that argument again only a few
people her when I wasn't missing I mean
I was trying to figure out that okay
that's an excuse is going to that twenty
percent slower than you so you could
said I'm going to get in a future okay
fine okay so let's if you if we take
this construction and we consider it as
a as a candidate factory non-negative
factorization of our slack matrix ah
then we wonder why this is going wrong
and issue with the multiplicity of the
cover that's really nice ah so if you
look at the entries um and you you look
at an entry which shares where they
share k edges then this is the slack
that you you should get and this is the
so this is saying that this curve gives
you the value that you expect on the
left hand side in this quadratic curve
this gives you the value on the right
hand side because every entry is in a
quadratic number of rectangles okay okay
and now you okay cause Marcus excess
dough is all visible
get your school's over ah yeah okay okay
yeah let's get the point what I think
this is this is one and this is like two
or three or so okay anyway yeah okay a
ok so the the point is the point that I
try to to illustrate here is that if you
have entries where you do have a large
slag think of this as being some large
constant then then it seems that this
kind of entry it's it just in too many
rectangles so we could very naively ask
maybe maybe every are every covering of
the slack matrix with only let's say
with polynomial in many rectangles maybe
every such covering kind of covers the
entries that have a large leg too many
times maybe it's a bit naive the strange
thing is that it turns out that the
answer is yes this is precisely the case
ok ok now this is actually what we are
going to prove mmm but what kind of
proof approach should i use if this
rectangle covering lower bound does not
work I need some kind of a proof
technique that I can I can work with and
the technique that I'm going to use is
the so-called I miss this is the minimum
rectangle covering exactly
no no it's just they could be very
different okay so you're going to lower
bound this and that's going to translate
on on the non- rank yes yes but in
general it could be larger so um the
directing and covering lower bound it's
just a lower bound it could be very
loose verenia and others here in this
case it turns out to be very loose for
the TSP polytope well if you find some
tricky inequalities you can actually
then do then this reckoning covering
globe on is actually surprisingly good
here in this case it's terribly bad this
is why we kind of we need to find
something something different something
better and square TV long can you get it
even a little bit to do this one rhythm
um another where of any publication
where where anything better than that
was mentioned maybe some somebody came
up with with something polynomial
burning of me Benny better but didn't
publish it actually not it could it's
not so I don't know whether it is really
an square or whether it is n to the four
it's also do not not hard to show that
it's at least n square yeah instead of
two are to show that it at most n to the
floor maybe yeah it might be actually
closer to the 22 the four that would be
my guess but that's just a speculation
okay um yeah now they this kind of
stronger lower bound that that we're
going to use it it's the so called a
hyperplane separation lower bound and
this was suggested by seven Fiorini hmm
might you it it works as follows okay
imagine you you can you can take a magic
linear function while a linear function
in the space of your matrices with a
foreign property that for every
rectangle that you can pick this linear
functions is very small and if you want
a geometric picture then you're in some
high dimensional space the end this the
linear function it gives you some kind
of in an inequality some hyper plane and
other writings lie on one side and now
let's suppose that you were so
lacking to pick to pick that the linear
function so that the the slack matrix
itself has a very high value on that
that linear function yes rank 10 1
matrix yes our switching is notions okay
now then the claim is that then the
rectangle then the non- rank of the
slack matrix is it's essentially lower
bounded by the ratio of both put values
of that linear function w so the biggest
entry in the slack matrix in our case
it's at most n so that's it's not a
value that we need to care much fun now
what what's the intuition I'm imagine
imagine your your slack matrix would be
the sum of rectangles would be the sum
of rank ones you're one matrices then
what is claimed says is that look you
have a linear function and its small and
every one of those rectangles but it's
very large on the slack matrix then you
obviously you you had to use a lot of
rectangles but now the segmented is not
necessarily the sum of rectangles but
you you can imagine that the slack
matrix it's the sum of rank one matrices
where all the entries are between 0 and
1 so we would only lose a factor of n to
go to that view and then it actually
turns out that if you look at the convex
hull of the rectangles that's actually
this is precisely the set of rank one
matrix with entries between 0 and 1 I
think this has also caught the bell
pollito I think in quantum physics
disappears also ok yeah so we have this
linear function end and all those rank
one matrix is with androids between zero
and one day they they have a very small
value and then the slack mattress has a
very large value and then that just
means that you had to use a lot of these
Rank 1 matrices you see
assume that the tangles do have very few
the function to be small yes yes because
you add up that matrix that you have
with some other non- matrices you add it
up and then you know that the thing that
you get the slack matrix it doesn't have
to do large entry so then obviously each
of the cement was not large okay good so
the tricky question is how do you come
up with those magic linear function okay
and yeah now let's try to apply this
rectangle covering lower bound to the
matching case and okay let ya let me
introduce a little bit of notation let's
say RQL this is the set of entries with
anarchic you and met in the perfect
matching em so that they share alleges
and now this is going to be my magic
linear function where for an entry which
has select 0 I put a minus infinity
yes for every entry where you have where
you have a zero slag you would put a
minus infinity which essentially means
that you only need to look at rectangles
where you're not containing any sex 0
entry and then you put you would put
something positive on the others
in fact if you in about you could you
can really see in this framework so
essentially they they have a measure and
they distribute it on a particular set
of of entries where the slack is
positive I think what it's like is one
actually okay
suppose I just put practice infinity at
one yeah yeah yeah that is
more or less modulo some small effect I
think yeah
this is essentially what you would do
for directing and covering low bond it's
you would take let's say you take a
measure you look at all the entries this
is the smallest positive slag due to
parity reasons you and you take a
measure and you you take a measure of
one and you distribute that uniformity
of all those entries okay this is
essentially directing the covering robot
okay well we have one row left and we
know that directly and covering lower
bound does not work so we should do some
put something here and that's what we're
going to put so we look at some large
constant K in fact 501 works and then
also we distribute the measure uniformly
on all those entries adjust that we
scale it a little bit down and the
important thing is that we put a
negative number there it's negative now
imagine what would you what would you do
here imagine you want to find a
rectangle which maximizes this in a
product this Rubinius product how would
you do that you would like to collect as
many of those entries where you have a
small small slag but you would be
completely forbidden to take any entry
where you have flexible to contain too
many entries where you have a large slag
we will see that you cannot come very
far with that okay now first of all
let's start with the easy things first
if you look at the inner product of them
that linear function with the slack
matrix you get you get you get a nice
concept value because okay the slack
matrix has a it has zero entry for this
that gives us 0 then we get what this is
a measure so we get one but all the
entries have select two so this
multiplied with two and so here we put a
a measure again and with the select that
we get is actually k minus 1 and that
came as one cancels out with this K
minus one so we send you have a minus
minus 1 here and some console is left
okay the crucial thing is that we can
prove that for every rectangle the inner
product with any rectangle is
exponentially small and then the inner
product with the slack matrix is a
constant the inner product with any
rectangles exponentially small and then
the hyperplane separation lower bound
then gives us an exponential lower part
in this research into the robot delicate
a rectangle is a set of cuts and a set
of perfect matches yes
who say that
what are these numbers so these are the
entries blue and where they share three
edges these are the end of the voyage a
shelter or what you want it's the color
signal expansion singly the topic of
this proposal exponent single
explanation
okay should we go okay um yeah a little
bit more rotation intuition provides
really should hope so
yeah okay the intuition is that now if
you look at the construction that we had
this example rectangles there you
contained actually injuries that had a
large slag they were contained kind of
too many times it means that if you look
at individual rectangles then the
measure the measure for this is
quadratically larger in k than the
measure that you collected for those
entries okay and this actually means
that if you take the direct angles that
we saw couple of slides ago then the
contribution that you get from these
guys is much larger than the
contribution that you get from those
guys so the value that would you would
get food for those rectangles would be
very negative so on the middle name is
just to let the techs are just cause for
a bunch of Carson magic theta so he's a
victim and you want them to just have a
slacker fill most of these pets what you
think that is not possible is this is
because yeah you will have many pairs I
mean either if you ever tiny rectangle
you can do that but if you have a larger
rectangle at some point you will contain
a lot of Paris um so that they have
slack precisely k tons of them and then
that that that contribution is going to
do kill you
ok I think I have a picture later with
trysil so exactly like just buy it we
just lying in traffic or like once you
have enough matchings and enough cuts
you have to get like name just counting
arguments actually at the end of the day
yes yeah like I have to get enough
matchings crossing these oh but I guess
you also get more watching this time
like no carbon particles yeah yeah yeah
you need you also need to use later that
the rectangle that that you're looking
at doesn't contain any any entry with
select 0 that's important it was it
would be wrong otherwise ok ah ok so I I
tried to introduce this the Magic mu L
which is essentially this is just the
uniform measure that we had on those
entries ql so this is just the it's the
fraction of entries a um where they
share l edges and it's the fraction that
i have in my rectangle are they say are
is my rectangle that i'm talking about
for the net for the rest of this time
and actually the technical lemma which
takes let's say 10 pages to prove is
this one this is the this is the the key
technical emma and this is essentially
what i already said just written a
little differently so if you have a
rectangle and the one measure is small
so you don't contain so this is your
rectangle and these are the entries um
where you have only one where they have
only one edge in common so this is like
zero entry then you have the following
poverty that if you look at the MU 3
measure so you look at the entries where
you share three edges then this read the
fraction that falls into your rectangle
is quadratically smaller than the
fraction that the rectangle gets for
entries that have a decade that have
select k minus 1 because they did they
share K edges
ok
ah yeah for for any constant k what
actually the content moves also here
into the exponent um yeah and this is
module or some some little error term
some exponent a small error time does
the same technique give things for like
you for vs UK ah probably I I'm written
it down mr. probably get some some some
general is it like you want equals zero
means that you have some pseudo
randomness property yes it's really very
important that you have this condition
without this condition you could just
take let's say everything if the
wreckling is everything then this is
this is one and this is what the
inequalities wants it's very very
important that you have this condition
and that actually changes the whole
relations okay now first do you do you
see why this main technical Emma this
actually this implies this lower bound
because ok ok modulo this this tiny
error this contribution will be
quadratically larger than that one well
you divide by 1 over K but it's still
much larger than then that contribution
ok and so the only the only thing that
you could collect is that exponentially
smarter ok ok good so the the technique
that we are using to show the this
inequality this measure inequality it
actually originates in a paper of Sasha
razbor off so in the beginning of the
19th year the paper that was that showed
some kind of measure inequalities for
actually for four pairs well he didn't
talk about matchings and cards but
essentially we can adapt the technique
to also work in this case we use a
slight extended the rough idea of the
roster of proof technique is the
following
so this rectangle are that we have it's
an arbitrary rectangle so we don't know
any kind of structural properties of it
so what he does is that I said so what
we're doing is we look at at a more
structured rectangle that's call it t
later this is going to be called a
partition and then we show that for this
structured rectangle the inequality
holds in fact we're going to show that
one for ninety-nine percent of those
randomly taking structure rectangles
this inequality holds if we kind of
intersect it here and ah you will take
this kind of rectangle you take it at
random but it's not uniformly at random
it's in a very structured way you taking
that right now okay
good you want that new one of our to be
equal to 0 for this group and other
girlfriend
uh for for this one not necessarily it's
more that you look at the risk you look
at the restriction here of that
rectangle and then you you want to argue
that this inequality kind of still holds
ok say the restriction yes
oh I'm fine yeah then I should probably
define this this kind of rectangle and
okay so essentially if you look at the
picture we're going to define this
rectangle T so that's also a set of
matchings and it's a set of cards now
i'm going to call this a partition now
this partition will be defined by as
follows now imagine these are the nodes
these are all the nodes in my graph and
then the partition is going to be
defined by by set a of nodes set B of
laws set C of nodes and it's addie of
nodes and the set a it's partitioned
into what I'm going to call blocks and
they said B is also partitioned into
blocks and they have they have very well
picked sizes the blocks so that the
numbers are really cooked up so that
things work out in particular we need
later much later in the talk we need we
need some symmetry and the symmetry that
we get is the following that if you if
you forget three nodes you take the rest
then this actually looks like one of the
blocks if you forget three nodes in C
and D and you take this thing this
actually looks like one of those blocks
so we get a lot of symmetry that we need
later in numbers in the number of nodes
yes okay okay now let me associate a set
of edges with this partition which is as
you can see here so we have edges
running inside the blocks and running
inside C and D and also running between
C and E okay and now the the set of
matchings that I'm that belong to this
partition it's the sort of matchings
that you can build only with those edges
that you see here and the set of cuts
that you can you can build its the set
of cuts
that you can can get by taking some
notes of C and then some blocks on in a
but the point is that you have to I
force you to either take take a complete
block or not to intersect the block at
all okay why should I do that now if you
if you look at the cut and you look at
at the matching then the only way how
they can intersect is actually here
between CNT and now this is the sizes of
C and D that's just my big constant K so
essentially i have intersection going on
only in a constant number of nodes and
edges so i can really control how the
matching at the car intersect okay now
this is my partition and now i want to
rewrite my measures using these dis
partitions now essentially what i'm
doing is well what is the measure
essentially you imagine you you generate
one of those entries um where the UNM
they share three edges and instead of
uniform you're generating it we generate
it differently we first pick the
partition at random and then we we take
a random entry in that partition you get
the same measure if you do right okay
okay so the first step is we we take the
partition at random and this is a kind
of uniform in the center that you take a
unit of a then you take a uniform see in
the rest you take a univ of d in the
rest you take uniform be in the rest and
then you take take you in uniform random
partition so everything's nice
asymmetric and okay now once your you
are in this this partition are you do
the following you take three edges
between C&amp;amp;D you call them h and then you
take sorry then you take a matching
then you take a matching which contains
those edges and respect to the partition
and you take a cut with cuts precisely
those three edges and respect the
petition what I know is that the cut and
the matching they're going to intersect
precisely in those three edges that I
selected okay the good thing is that
once I have picked the partition and
once I have picked the three edges the
choice of picking the matching and the
choice of peeking the cut our
independent because I know that they
cannot intersect anywhere else okay if
in fact it means that this probability
that care about I can I can separate it
because they are independent okay so we
can we can play the same game for the
measure meuk again we pick a random
partition now we pick K edges running
between C&amp;amp;D so it's essential we would
pick a perfect matching perfect
bipartite matching between C and D and
then we pick random matching containing
all those edges that we pick a random
cut cutting all those edges ok so I'm
good good what does it do
it's my symmetry I mean if you yeah now
i think it's it's it's pretty clear that
every every pair um which shares
precisely cages must have the same
probability of appearing here okay good
um ok now for for the next let's say
five minutes let's forget everything
that I told you about cats at matchings
and let's talk a little bit for James
okay you don't need to do anything you
can just okay okay and let's talk a
little bit about the the behavior of
large sets okay now imagine you have a
give a set of vectors capital X and the
entries of the vector there between
let's say 1 and Q and you imagine q is
some constant that we don't care much
about and okay now let's go get a little
random experiment let's say I take I
take one vector at random from that big
set of vectors X and now i'm looking at
at one of the coordinates of the vector
and now I I observe the random
experiment I I look how is the ice
coordinate behaving so this is maybe one
choice of little eggs and this is
another choice and then look how how
does this how does the coordinate X I
look like and what I claim is that if
the set of vectors is large enough then
promotes indices I that you could select
the random variable that you observe
here this is going to be roughly a
uniform distribution you can formulate
it the other way around and this is how
are you can prove it nicer it's if you
have let's say if you have a linear
number of coordinates where that
coordinate is biased in the sense that
the the distribution that you see there
it's a little bit away from the uniform
distribution it doesn't really matter
whether you look at the statistical
tease diff
turns on the maximum difference
improbability just a little bit away
from the uniform distribution then you
can prove that the density of that set
of vectors that you had in that whole
set q to the end let's actually
exponentially small this is already
essentially some standard arguments that
are a pillar in the roster of proof okay
let me just give you the quick proof of
this fact okay essentially you get the
result by a Pike County the entropy you
can't the entropy of that that little
random variable X what is the entropy
well it's that the size it's the
logarithm of the size of that bigger
said we take take the element from and
then well you can just pound it using
some additive ax t and then ok now let's
let's breathe the Indies as I look at
the biased ones and the the unbiased in
this is separately and then you know
that that the entropy of a random
variable when you take entry speed some
numbers between 1 and Q that the entropy
is at most log Q and you know that this
is the unique case of distribution where
this is attained is the uniform
distribution and if you're a little bit
away from the uniform distribution then
the entropy is also a little bit smaller
yeah I okay i guess i can probably more
skip the picture for 4q occurred so I ok
so this is the entropy if you just have
a coin and if you're a little bit away
from the uniform distribution then then
you're a little bit away from from Locke
look you and you can quantify this
that's not a problem then anyway if you
do have a linear number of a pious
coordinates then you know that there is
a linear number of n review that you're
missing and you rearrange this and you
you get that inequality ok ok good
there's actually a different way of
seeing this essentially the same claim
it's the following now if you if you
have a large sufficiently large set of
vectors let's say at least at least that
large whether the densities at least
that
and you you actually you want to talk
about the density the density is it's
you take a vector X now you take it in
the set of all vectors and you want to
check is it in my set X this is let's
say the density ah then actually
foremost coordinates you're not changing
that density if your condition that the
coordinates has some particular value of
J foremost I and for all J yeah yeah ok
this is yeah this is essentially a
reformulation of what we already have
before and this is very useful for us
because that's kind of the business our
business model we have that rectangle
and we want to to look at the density we
want to look at what's the measure that
falls into that rectangle and this says
that we can essentially locally
condition on everything that we like so
we can say we want to look at matchings
but we want that these edges are in the
matching and those are not yeah yeah
yeah this this depends on cue and on
epsilon yes ok
hi now we again remember everything that
I said about matching that cuts James
you don't need to do anything okay and
then yeah let's try to do to formalize
this ok now let's let's look at one
partition tea and let's look at one
choice of those three edges age let's
call that pair good with respect to the
matching part if the following is true
if you imagine you take a random perfect
matching from your rectangle which
contains those edges and respect the
partition then I want that the
distribution that you see on these nodes
this should be uniform in other words i
want that the matching it induces a
uniform random matching here or the
other way around ah this says that if i
have a good partition and I've I mean
with a good triple age then it means I
can condition on everything here and
this is not going to change the outcome
of that probability okay good yeah so
how much time do I have because we had a
10 minutes okay
okay and we can do the something similar
for four cats so I also I yeah again I
want to call a pair T of a partition and
in the age of three edges about to call
it good with respect to the cuts if
essentially have the same number of
cards containing those three edges and I
have the same number of cards containing
like everything okay hmm good now why is
this useful okay so first of all let me
just this is the measure that I want to
bound and let me let's just express it
the way that we saw already and now
let's split this according to whether
partitions are good or bad okay you get
this picture so this is ok so we would
split the measure this is the good the
good part and then you can be bad either
because the matching part went but went
wrong or the cut part wind went bad and
actually what we're going to do is we're
going to bound each of those quantity
separately now in particular this is
this is the probably the most
interesting case this is this is kind of
the generic case where you have you have
a good good partition with a good triple
of Ages age and in this case we see
surprisingly that this this measure the
three measure is only is quadratically
smaller than the k measure and probably
this this is probably the surprising
part the rest is a little technical
probably not super surprising ok so I
try to describe in the next maybe five
minutes why we come up with this bound
and this is I think this is the key
reason why the matching polytope doesn't
have a small LP and the rest are some
technicalities that you need to work on
ok now okay so I want to show that this
is
quadratic factor larger smaller than
than this and I want to compare this
essentially for every partition ok now
let's say let's say we we fix the
partition tea and let's say we also we
fix K edges f and now we we compare the
contribution that you had to one measure
with the contribution that we have to
the second measure and then we will see
this divergent okay now what's the well
the contribution to the K measure that's
well that's the probability that this
random card is my rectangle times the
probability that this random matching is
in my is my rectangle okay that's easy
now I want to compare this essentially
to the contribution for the three
measure and okay I do that as follows I
have this K edges and I take three of
them at random and these are my edges h
and then you know then you then you take
a random cut cutting those edges and
then you take a random matching cut any
of those edges okay the point is that if
the partition and then that Triple H if
they are good then it actually doesn't
matter what I'm conditioning on then it
doesn't matter if I condition that I cut
all the edges or I cut only those edges
so this probability it doesn't actually
change if I condition on every on
something else ok ok so essentially what
i need to bound is what's the fraction
of true of triples age that are good and
i claim that for every partition tea and
every set of K edges f this is actually
quadratically small so the point is that
you cannot have this kind of nice pseudo
random behavior for like for all triples
only for very small right and the reason
is the following I claim that age NH
stars so 22 set of triples they can only
be good if they share
these two edges okay I mean if you if
you imagine the beginning if you
remember at the beginning of the talk we
have this this construction of
rectangles and the way how we did it we
took two edges and then we took all the
cards cutting those and we take all the
the matching its containing those two
edges and this essentially says that
look this is the only way how you can
get a decent rectangle okay now let's
say we have we have a we have a triple h
and a triple H star and they are both
good good means that you have some nice
tool or any behavior in the rest okay
now we know that those three edges they
are good it means that whatever I
condition here on whatever kind of
matching I condition on here are that's
fine i'm not going to change the outcome
in particular i know that there is some
kind of matching in my rectangle where i
connect two nodes that are in the other
in the other triple okay but also that
other triple is good and then i know
that there must be a cut which cuts
precisely the three edges in that red
river okay and now let's have a look so
we have a matching and we have a cart
and they intersect only in one edge so
we have a select 0 entry in our
rectangle and that's a contradiction and
that's it yeah okay so essentially it is
this pseudo random behavior it it kind
of forces this this result okay now
there is some okay if I have three
minutes then maybe maybe I quickly
outline how you how you prove the
technical part now what you're
essentially showing is what you say
essentially you want to show that is
that you if you take a random partition
tea and you pick randomly three edges
then the chance that this is bad because
of the matching part went
when that their chance is actually very
small you can make this as small as you
like and okay that's nice in fact
something much stronger is true it's not
just that for a random partition t-this
dis holds you can fix a lot of things
you can you can pick the three edges as
you like you can pick the a part as you
like ah you can essentially pick the B
part as you like and you can you can
give me any kind of partition of the the
B blogs into two halves the only thing
that is random is how you pick how you
pick the remaining C and D part in fact
let's imagine we take okay we fix this
arbitrarily and then we pick one of
those guys at random and then we split
this and that's the the remaining CNT
part okay now what I'm claiming is that
you can fix everything on to this point
and now with a good probability still
the remaining out coming T and age it's
going to be good with with a good chance
the reason is now imagine imagine you
have you have other matchings that you
that you can conform with those edges
and you have exponentially many many of
those in the rectangle and then
essentially they must be absolute almost
uniformly randomly on in each of those
blocks and if you not pick one at random
then you will see an almost uniform
behavior there and that's it okay now
finally uh I believe that we have a very
good understanding now how strong linear
programs are but I think we have very
very little understanding how strong
stps are and I think the the next step
would be to try to generalize any of
those
ants to semi definite programs but that
seems to be quite a hard problem so yeah
but that's a very nice a problem for the
future thank you are you going to
formatting business what is geeky ah
yeah why not so so so so so what breaks
if you if you want to do proton progress
BTW everything or acacia and so sorry to
you if you want to blow some organs btk
ah actually we're already stuck at the
very beginning ah we are stuck at the
very very beginning we are stuck here
now this lower bound only hoods for the
LP case not for the SCP case in in
particular for the non- rank aids for
the LP case you're some kind of atomic
view in the sense that you can you can
write your slack matrix as a sum of
rectangles and it's kind of the
rectangles you can look at them as like
atoms and you you select one and then we
show that look you cannot even take take
one which is good but for SCP is this
doesn't work for SCPs you have
essentially PhD matrices and there they
are all interconnected so you cannot
argue that there is not a single good
SCP matrix so and now this I did this
theorem of Jana is there are
extensions for that so that's not a
problem
this one this one this is essentially
works for for a CPS as well ah yeah it's
the segment Jake's is the same the
factorization is different now hear this
says you look at the inner product of
two non-negative vectors and for the for
the stp KSU you have to PSD matrices and
then you have the Probie nose product
yeah when you restrict is the dimension
yeah yeah yeah
any other wishes
in</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>