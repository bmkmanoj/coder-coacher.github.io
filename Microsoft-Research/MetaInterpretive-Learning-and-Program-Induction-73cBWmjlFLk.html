<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Meta-Interpretive Learning and Program Induction | Coder Coacher - Coaching Coders</title><meta content="Meta-Interpretive Learning and Program Induction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Meta-Interpretive Learning and Program Induction</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/73cBWmjlFLk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm going to be talking to about a
novel form of machine learning and the
the novelty of this should become
apparent but it plays on a particularly
kind of fundamental property of
computers that they can interpret their
own programs and programs which can
interpret all other programs so so the
central idea of a universal Turing
machine this is built into something
called a matter interpreter which is a
logical description of a universal
Turing machine and it's augmented in a
way that it learns ok so we're taking a
general model of computation which
learns by adding new facts in when it
needs to I'll be talking a bit about
program induction that is how to
construct programs from examples which
is relevant to some ongoing work at
Microsoft Redmond by some wood bhiwani
but again that should become apparent
during the talk so my starting point in
the talk is about if you if you go back
to the 70s and 80s the idea of using
logic as a programming language was one
which was advocated by Robert Kovalsky a
bit later out of that various different
pieces of work Shapiro earlier work by
Plotkin and so on were came together
into an intensive study of something
called inductive logic programming in
around about 1991 which used the
framework of logic programming as a
basis for machine learning so it used
the representation of programs in the
form of of definite clause logic which
is a fragment of first-order logic to
formulate a machine learning approach to
machine learning in that early work the
one of the characteristics that was
slightly different at the time
was this idea of predicate invention
that is if you are trying to construct a
program out of a given set of predicates
which are predefined then in some cases
you cannot complete the definition
because you need certain auxiliary
predicate so if you imagine trying to
learn reverse you might want to have a
penned invented or if you didn't have it
already so that idea of predicate
invention was was was characteristic of
the early 1990s work if in a
retrospective at of the 20 years of a
vial of inductive logic programming that
I and some other authors wrote back in
2011 it was with some sadness that we
recognized that both predicate invention
and he even the learning of recursive
programs had been dropped along the way
because these were too hard for the
earlier mechanisms that were being used
they led to inefficient searches and
people preferred to concentrate on on
the on not non recursive logic programs
for applications in knowledge discovery
largely so uh in 2013 having written
this orange after having written this
perspective I I thought it would be
worth going back and seeing if there was
a different way of looking at the
problem and out of that emerged the work
that I'll be talking about of meta
interpretive learning and now there
there's a paper on grammar learning in
the literature and h chi main joint
conference on AI we published some work
now i'll then show you what the basis of
this idea was so the starting example
that i had in mind from my thesis in
edinburgh was actually the fact that
other forms of machine learning had been
applied since the 1950s onwards looking
at things like the automatic or like
induction of finite state machines or
finite
from examples and these had not been us
these had not been these are not avoided
the issues which were related to both
predicate invention and recursion so to
exemplify this what I've shown here in
the diagram is a simple finite state
acceptor each circle here represents a
state q0 and q1 a double circle
represents acceptance state which shows
that the string so far has been accepted
in a language or in these target
language here if you get a zero to begin
with you'll cycle around here if you get
a 1 you'll change state and then you'll
carry on with zeros and then come back
with another one so this finite state
acceptor it accepts precisely the set of
parity strings those that have an even
number of ones because as you start off
here you have seen no one so far if you
see 11 then you go to the odd state you
see another one you come back to the
even state and cycle on zeros etc now
it's clear that from work by warren and
pereira or perera's work in fact that we
can easily formulate these kind of
grammatical representations into a
simple form of definite clause program
and represent these in a language like
prologue and even if you're not familiar
with prologue you'll find that in fact
this has got a very simple structure so
if you look here the except estate is
the one that goes from and the empty
string to the empty string the this the
the arc here the zero arc here goes from
q0 back to q0 this is if you like a
recursive loop in this third case you go
from q0 to q1 if you get a 1 on the
input and so on so each Clause actually
is associated with one particular arc or
the acceptor state for the first one
and examples that you might get might be
something like the empty string 0 or
double two ones or 101 these might be
examples of the that you might be given
in order to do the learning so if we
look at this what we see is something
that could not be learned by a standard
inductive logic programming system
because it contains not only mutual
recursion here in this loop but also so
it's a self recursion but also a mutual
recursion between these two the
predicate symbols although you might be
given let's say q0 as a starter state Q
1 would certainly something that
wouldn't be given as part of the
examples that you get and you have to
somehow invent that given these examples
because you don't see the cute ones in
the examples so there is both a
predicate invention problem and a
recursion problem in trying to learn
this this this logic program and so
inductive logic programming systems
typically cannot even learn regular
grammars which is extremely
disappointing given that part of the
reason the motivating reason for using
logic programs is that they were very
rich representations and this is a
frankly this is a simple fragment of
first-order logic now one thing that you
can notice though if you look at these
clauses is that they have a lot of
similarity there's not a lot of
variation between the clauses so all of
these four clauses here have a very
similar pattern to them the only thing
that varies is in this first place the
q0 the constant here which might be 0
and 1 the first thing might be q0 q1 and
over here q0 and q1 vary the rest of the
structure of the clause stays the same
and so it seems natural that you might
capture the regularities of these kind
of clauses beforehand if you were aiming
to learn BCG's and in fact that can be
done just using these two patterns of
which occur the second one occurs
repeatedly the first one occurs just
once which is the accepted state but you
could have an accepted
that had multiple accept estates in fact
you will never see any other claws like
apart from ones that have this form in a
regular grammar because this is another
way of describing the Chomsky
description of regular grammars and if
we look at these we can also consider
that they look a bit like clauses
themselves the one difference though is
that we've got something odd here that
these variables will substitute not for
constants in the language like zero and
one but for predicate symbols okay so
this is essentially some form of higher
order logic statement that would need to
be given to the learner in order to be
able to direct it to do this form of
learning so the idea came to me that
what what needs to do is to somehow put
these structures into a general purpose
parser and use just those in order to
direct the learning and this is
demonstrated in this slide so the idea
is to try and do to direct the invention
of new predicates using these two
general clauses that I showed you before
implanted within a metro interpreter now
if you can think of this matter
interpreter as being a general purpose
parser which has some specific types of
rules in it so for instance here the
parser starts by calling this particular
pars predicate which contains if you see
the first rule and if it's if that first
rule matches then it will match because
acceptor Q is true and similarly here
the second one passes through here
you've flattened out the structure of
this clause and made it into meta
variables within the passer you think oh
the parser as being a kind of higher
order predicate itself applied to these
other predicates you can do this with in
Prolog but
you would need these auxiliary
predicates acceptor and Delta to fill
out the details of the grammar that you
are actually trying to learn so if we
put these ground facts in then we would
pause exactly the parity language rather
than some other language because the
acceptor is we're told here is q0 and
this represents the transition table if
you like of the finite state acceptor so
we told here that Delta takes q0 state
with a 02 q 0 and K 0 1 2 Q 1 and so on
now suppose that we did not have these
grand facts to begin with we can use a
mechanism from logic programming called
abduction in which if you do not have
the fact that you need to prove you
simply assume the facts that are
necessary in order to complete the proof
so by starting with an empty set of
ground facts and using these the same
general purpose matter interpreter we
can abuse the ground facts necessary to
represent parity and in fact what the if
you think about that what we've done is
we've turned the problem of inducing
what was a first-order program this one
here into the problem of abdul singh a
zeroth-order program a ground program by
abduction now learning with respect to a
propositional logical zeroth order logic
is a much simpler task than learning
first-order logic because this is a less
expressive language the what's happened
with this matter interpreter is it's
acted as a as a kind of projector its
projected you from first-order
representation onto a zeroth-order
representation and allowed you to use
abduction to carry out what would
normally require induction hello sir
price what you see is that the key sign
and just by abduction in for all this
you go in first
that's what abduction does is once you
have a theorem and when you see missing
some assumptions to prove so we can
actually learn these assumptions by
abduction but that actually implies that
you have a theorem in the first place
and very in the first place is the
interpreter the matter interpreting it
feels like you're theorem is dedicated
language that is accepted says even
number of once no not so because any
regular language could be represented as
a set of ground fact I've just
represented the the parity language if
that's what you're saying i'm assuming
before her what you would like to learn
the gremlin is there a presentation
of adamant on that gives you even number
no that is what you would like to learn
and your starting point is to have a
matter interpreter with no ground facts
the grass a set of ground facts that is
assumed will then represent a particular
language in this case parity after the
the grand factor generated by abduction
as you go through the proof procedure
you abuse things as needed the groove
procedure of a particular theorem what
all the theory this theory she it looks
lazy plus the input ID I'm sort of a
system for doing induction for us is
probable pulling prologue programs is
that idea Britain yeah jumping ahead a
little bit so what I'm trying to do is
simply learn regular grammars and so
I've given a string a regular string
like 101 and I pass it through this
general purpose acceptor if there's
nothing there that says where the
acceptor czar or what the deltas are it
will introduce appropriate acceptance
and Delta's that passes that particular
string so with 101 it will start off by
saying well I need to be able to get a 1
in and who needs to go from zero so I'll
introduce this atom okay i still have I
haven't got the parity grammar yet I've
just got the first step in it let's say
and now I need to go with a zero I need
something that goes from state 1 because
we've now but pound that we introduced
this one
and then we again have a one in it and
we end up with a an acceptor so we'll
have generated part of the finite state
acceptor given some more examples we
might be able to generate all of the
transitions necessary to accept parity
rather than some other language starting
material is a bunch of streams and
they're parities yes that's right those
are the positive and negative examples
see many of those you want or something
yes you've got a set of positive
examples maybe some negative example so
you must make sure that the negative
examples fail in this procedure on any
set of ground facts yeah okay so um and
the completeness of abduction actually
ensures that this that this procedure
will learn any regular language so an
implementation of this learning a pro
for this learner for regular grammars is
actually pretty much what you're seeing
here so you have a learning program
which is about three clauses long
anybody can implement this in five
minutes and you will be able to learn
the regular grammars by extending this
by 1 Clause an extra one which deals
with the chomsky normal form for the
additional general case of context-free
grammars you have a learner for
context-free grammars for all
context-free grammars and although the
learners for regular grammars are
standard and there have been many of
them developed since the 1950s you will
not find a single general approach for
context-free grammars that precedes the
publication of this one in the machine
learning journal last year now it's not
efficient but it does learn a class with
very little effort in the programming
and the efficiency might be achieved by
by here improving on that program but it
it tells you that there's something
interesting in this way of describing
the bias over a particular language
class now you might decide that this is
something that is really rather local to
the learning of languages or learning of
formal grammars so in the next step that
we took we looked at another fragment of
logic to see whether a similar thing
could be achieved ie very simple
learning algorithms that achieve
efficient learning by backtracking okay
so in this case what we took is we
decided to look at kinship problems
these have been studied by Hinton by
Quinlan I know I LP these are a standard
and in logic programming this is the
kind of very first thing that you taught
how to do as a logic programmer over
here we can see a made-up lat family
tree here are some standard kinship
relationships so parent X Y of mother XY
grandparent if parent of parent and so
on ancestor if ancestor of ancestor or
ancestor of parent so we've got a set of
of clause of set of target clauses that
we're trying to to learn in these
kinships now interestingly when you
write these out sorry when you learn a
grammar what's the criterion for getting
a good answer I mean on any finite zone
training data it's always regular so so
you know your regular expression thing
would have matched all your context-free
the examples as well right with a
agnostic so so the standard techniques
that are those from computational
learning theory in which you assume a
randomly sampled training set from the
target theory and then you test on an
independent random sample from the
target through the same target theory so
we're not doing anything unusual in
machine learning terms here so the the
if you look at the performance graphs
they're cool they're over predictive
accuracy and just at the you know on the
unseen data you can test given a lot of
given a large bunch of examples and bows
is it rather than a theorem that you can
say given this input this was no no no
this is machine learning so you're yes
okay so um so the question is
so this is a this is a different class
of logic programs but do we have similar
characteristics that we get repeated
patterns again and again and it doesn't
take long to look through this set of
descriptions to realize we're getting
similar patterns appearing repeatedly so
we got one how many of these have we got
we've got we've got that parent of
mother apparent father we've also got
the special case of ancestry of parent
all fit this form and this one here
we've got here we've got another one
down here and great-grandparents we've
got three of those in fact these
patterns appear quite frequently but if
we consider that the language that we're
dealing with characterize a bit like
that what we see here has a certain
properties where we have binary
predicate SAT most and we have at most
two atoms in the body of the clause then
the various different templates of this
kind are not very numerous we can
actually enumerate those these are two
which are the only ones that appear in
here they appear repeatedly in fact they
appear in other things if we define
cousin or we define sister sibling there
are slight variance of these but there
are not too many there may be two three
four of those that appear repeatedly in
standard kinship descriptions so this
little language I'm calling h 22 h 22
has two predicates are arity to maximal
you're going to one or two you can have
one or two atoms in the body and by
incorporating again these clauses in a
similar matter interpreter to the one I
just shown you you can get again
behavior in which you are inductively
learning these rules by abdul singh
ground facts which a substitute for the
capitalized variables here the peas and
the ques the X's and Y's always stay as
they are just as before and you can also
get
predicate invention and recursion
recursive learning so you need recursion
to handle ancestor in this case double
recursion but you can you can handle all
of that if you have a large enough bag
of predicate symbols that you can choose
from you can introduce predicate symbols
that haven't been used before and build
up the definitions as a side effect of
simply passing through your examples and
testing them against the negative
example so it's a fairly simple general
approach but again you might argue h22
well what kind of a fragment of logic is
that but there is an answer to that in
1977 the this little fragment was was
demonstrated in the one of the early
logic programming conferences if you add
you know a function symbols you get that
you were able to implement a universal
Turing machine within this very
impoverished language ok so just to
explain what's going on here imagine
that you have a large latter-day an
infinite set of symbols and thus this
the s's here and T's represent
particular cheering tapes ok and if you
like the the second argument represents
the after case and the the first
argument that before case this first
rule tells you the rule of halting so
the UTM given s and s hits a halt
statement on s and that's it it stops
the second case represents a cycle so a
recursive cycle you go from s to t by
first of all executing from s to s 1 and
then s 1 to t where execute fetches an
instruction from s it extracts the
instruction F and then applies f2s to
give you tea and you might notice that
something a bit weird this is not a
proper logic program because we have
here a higher-order variable being
applied to a to a pair now surely you
can't do that well actually this
represents a little fragment of higher
order logic which is called diet which
is called
which is which is a data log fragment of
higher order logic and given that its
data log amazingly it's decidable right
so higher order logic nobody uses higher
order logic for anything that you would
hope to execute efficiently because
there are terrible properties to do with
unification giving an infinite set of
answers and so on but for a higher order
data log that's not true higher order
data logs decidable and so this
procedure is a if you only limit things
to a finite set of symbols gives a
decidable procedure for which it
approximates a universal turing machine
given an infinite set of symbols of
course we all know the universal turing
machines have undecidable properties you
don't know whether they're going to halt
or not but interestingly what we thought
was a very limited little fragment of
higher order logic is sufficient to
implement the universal turing machine
or an approximation to one we can also
think of these templates if you like
four clauses as being most general
templates of a kind we can view programs
as being generated by printing our
versions of these in various different
ones and you'll see that the structure
of these we've already seen so if you
see these clauses here we've at least
seen this one here in the middle already
in as an instance in what we've looked
at I call this the chain rule because it
appears again and again it has the same
general rule as transitivity are the
same general form as a as a transitive
relationship but it's in a more more
general form okay so we're going to have
to given that this is a very strong
language we're going to have to wait
find strong ways of bounding it in order
to guarantee termination of the learner
because the learner has to at least be
able to terminate when it's constructing
its solutions and testing them so the
implementation of a system called meta
galdi has a control part it hasn't just
got the matter interpreter I mentioned
but it also uses a bag of
niques the first of which is called the
opera the ordered her brand base this is
an idea from Luthan bendix 1970 who used
it to develop confluence rioc confluent
rewrite systems Yaya Fernandez and
Minka's Minko used this within deductive
databases for data log programs the
approach is very simple if you think
about try to guarantee termination of
recursion imagine that you're her brand
base was ordered in such a way that
whenever you apply to rule a fact that
was higher in the order had to be proved
by facts that were lower in the order
and each of those in turn must be proved
by facts that are lower in the order
eventually there's a base to that order
and the recursion terminates so this
simple approach requires simply any
total order over the her brand base to
guarantee termination of derivations
there are various different total
orderings that have been looked at the
most obvious first one we've looked at
booth by Newfoundland Bendix which is
LexA graphic if you imagine the
telephone order over the her brand base
then the laksa graphic ordering will
will do it's got certain problem so it
doesn't support mutual recursion there
are other orderings and we use one
called the interval or during which bait
is based on the assumption of a total
ordering over the constant set as well
and so with a total ordering over the
constant set we can treat any binary
predicate as being an interval if we
then require that the predicates that
are called have interval calls which are
included within the cold interval then
we can again guarantee termination
because of inclusion of intervals so
these are proper inclusions of intervals
we again produce guarantee terminating
behaviors and the various orderings
interestingly enough are associated with
various different time complexity
classes for the learned objects which is
again an interesting topic that needs to
be further explored but I've already
seen these in the experiments so the
second idea is that of episode
it's and the idea here is analogous to
human learning to episodic logic but the
idea is that you carry out a sequence of
tasks in which a program is built up as
a series of definitions one on top of
another now in the case that you're
trying to learn a very large program
this produces great increase in the
efficiency because instead of searching
the cross product of all of this the
hypothesis spaces for a large number of
predicate symbols in each episode you
concentrate on getting one predicate
definition correct before moving on to
the next one which is higher up in the
order remember our predicate symbols
have an order so have building one
predicate definition on another ends up
producing the summation of the size of
of the individual hypothesis space
rather than their product which is a big
gain the the clause hypothesis classes
are then tested progressively that what
that means is that we we test a small
number of clauses for for for each
definition and then consider a larger
number of clauses so we're we're looking
at larger and larger spaces this is if
you like the very similar strategy to
what's called iterative deepening in
theorem proving and in fact because our
metro interpreter is a little theorem
prover we can view this as a way of
enlarging the language by iterative
deepening so we're looking at small data
small sub definitions with one clause
two clauses etc and we don't move on to
the next class until we found we cannot
find the solution in the class so far
now it's very easy to show that this
guarantees that the solution will have
the minimal number of clauses and the
proof of that is is obvious if you
because if if there were a smaller one
you would have found it in it
applause by adding a further notion of
log bounding that is we only look at
classes up to size log n when we have
any examples we can actually get a pack
sample complexity of result this is the
this is valiant it's probably
approximately correct learning framework
where we can guaranteed polynomial time
construction from a polynomial number of
examples by considering that the
hypotheses are much smaller than the
number of examples no this is a this is
a quite a strong constraint but it's a
constraint that guarantees high accuracy
in every one of the definitions and high
accuracy actually helps if you're doing
these episode lonely learning tasks the
implementation of all of this is less
than hundred lines long and it's
available to anyone who wants to try it
out we published this at each guy this
algorithm last year okay so that's a lot
about the implementation so what uses
this so the first interesting tasks that
we came across was was something
presented at the inductive logic
programming conference in 2012 by Claude
samet in which he had been using a
Microsoft Kinect 3d camera to try to
build a definition of the staircase for
use with in something called the RoboCup
rescue robot competition into the nan
you'll come petition for trying to build
robots that help firemen in our burning
building so you the fireman wants to be
able to recognize a staircase and when
summit looked at this problem he took
the plot the point clusters from the
Kinect camera he then clustered these
into planes and he used Aleph ILP system
to learn a definition when I asked him
afterwards was this definition
recursive he said no and kindly offered
to let me try this this data out with
with our newly devised Metro
interpretive learner with I I hope that
we would be able to learn a recursive
definition maybe with some predicate
invention from this rather messy data it
took about a day to figure out how to
put it into the right form and this is
what popped out ok so the pairs here
represent each 2x represent the
intersection of two planes and y
similarly so a staircase is defined by
two lines of the intersection planes of
lines we're given a line at the top of
the scare case and a line at the bottom
of the staircase we get this little
tight little recursive definition which
goes staircase XY if ax said and stares
at Y and so on and a is defined in this
way here this is an invented predicate
so it's introduced automatically from
the symbol set anybody want to hazard
the name for a step so the interesting
thing is that this is instantly
recognizable as a recursive
general-purpose definition of what a
staircase is it's a pattern with which
repeats consisting of steps and I don't
think you'll find many forms of machine
learning that can actually get this from
a single image this came from a single
image built in point oh eight seconds on
a laptop and it's using essentially the
same set of metal rules that i showed
you for the case of family relations
it's finding this is a bit like an
ancestor where you're inventing this
particular thing here to describe step
sorry
pregnant no it's real enroute it's
within we've got to parity two
predicates everywhere at most two atoms
in the body symbols predicates symbols
were individual or you go originally we
had vertical and horizontal were these
were primitives if you like which are
associated with a test on one of salads
planes that it's roughly up and down and
horizontal roughly horizontal and the
examples the set of examples were all
defined as stair pairs so we given them
as stair line 12 line 28 and stare
symbols yes along the river so you and
this is pretty robust you can take so
there are a number of these lines and
you can take from a single staircase a
single image you can take groups of
Paris from within their in roughly
anywhere you like and it'll come up with
this definition and interestingly you
can see the puppy computation path that
the learner took right when it came
first to the first pair it tried to say
oh well could I just apply something
like this first rule well that would be
decades ex-wife staircase XY so that
doesn't work so then I'll just introduce
something or another I'll just leave
that for the moment and then go down
into that and what do I find with the
first part well it's vertical you could
have said well why didn't a test
vertical here because it's got a
boundary of two clauses in the body it
wouldn't have been able to fit
everything in so it's with a extra
predicate it can go vertical horizontal
and now it sees its stare problem it
recursos back to here we'll keep
recursing down it until it hits the
bottom of the staircase and then he'll
say oh that's another one of those it's
another step ok so the construction path
for this is the execution path of the of
the solution which means that the
solution does not need to be tested
again on the examples because it is
it is it's true by construction if it's
been proved by the metro interpreter to
work already here so you go essentially
sort of your backward is the steps so is
there so if you let your system run for
a little bit longer is it going to come
up with differing definitions yes in
general it will find a set of solutions
in this case i'm not sure but what you
in Prolog this is just a prologue
program you run the matter interpreter
if you want another so it'll give you
the first solution if you want another
solution it'll give you another no it's
the first solution it's the fact it's
the first solution and part of my
surprises that was so easy once the data
was there it just came out immediately
wasn't and and I think there are um did
you um I so I yeah I should I should
still test that I haven't tested that
but I'm pretty sure it it's ro actually
no we did some robustness tests in the
this is in the grammar this is in the
machine learning journal paper we did
robustness tests on the remainder of the
images and it got very close to a
hundred percent I mean I there were some
noise in the data so didn't get up
absolutely hundred percent ok ok so
interesting to this is an example of
what you might call what Josh tenenbaum
and MIT call us one shot learning that
is learning from a single instance and
you'll see this again okay so there are
some other tasks that we looked at so
learning a robot strategy can be done in
this fashion also and here is a program
again in the HK paper where we learned
from various different we given given
actions and Minh addicts map onto binary
and so that actions influence that's
tests and actions for this planning
problem a given as dyadic and monadic
predicates and this definition is
learned for for building
sets of bricks it's a recursive
definition it introduces extra
predicates here f 1 a 2 and so on build
if you've only give a positive examples
it builds a simple recursive build
structure where it fetches and puts
things on top of if you give it negative
examples from piles of bricks that have
fallen down then it will build you these
these additional parts which test the
robustness of the solution I'm running
out of time so i won't go into this into
much more detail but these are the kinds
of examples that were given as pairs
before and after pairs we have a pile of
bricks and the after one and these
before and after states of the world are
represented by variables x and y okay so
f1 is just a test on the state this is
the learning performance on unseen data
so as we increase the number of randomly
chosen examples for the wall building
from 0 to 70 the accuracy goes quite
rapidly up to near a hundred percent and
the learning time increases roughly
linearly because we're using and into an
interval constraint and these are so
these are these are random ly chosen
training sets and tested on the
remaining test sets ok so we can we can
learn little robot strategies in fact
these are non deterministic strategies
but they become more deterministic as
you provide more examples ok so i
mentioned simmered ghoul whining at the
beginning so number of you may be aware
of his work but I only became aware of
us in December and the flash fill
program for those of you are not aware
of it is an amazing program which allows
you to take an Excel spreadsheet given a
certain in a certain column of values
the user can then specify a particular
transformation string transformation
that they want ok so in this case the
user types in Brent Harold and the
pewter is then supposed to work out what
the transformation is as a program and
the reason it's called flash fill is
that in a flash it fills the remainder
of the column with the transformations
based on a program that's been built out
of what is called a domain-specific
language a restricted language it works
off over a class of problems that have
been entered by hundreds of thousands of
entries given by users and is part of
the 2013 XL a release okay so this is a
pretty solid piece of work of using
inductive programming and it's one of
the most prominent examples or probably
the most prominent examples in the world
now of efficient and effective inductive
programming from a small number of
examples by its small in this case we
mean on average according to his test
1.2 after a lot of a frequency analysis
of the data returned and careful tweaks
at the the Redmond group this works
really really well okay so having seen
this I was quite inspired to the idea of
trying to apply matter interpretive
learning to functional programs to see
if one could get some similar effect to
this but maybe go a little bit beyond
what what this brilliant work by sumit
diwani has achieved so this is a pople
paper in 2012 that was also became a
highlight paper in the communications of
the ACM in the same year so there's a
very recent work but one thing that it
fails to do in my regards at the moment
is first of all show you the underlying
program that's been built which is
slightly complex from having read the
papers and secondly to improve learning
so it's you're given in that you're
given a domain-specific languages has
some specific actions that can be
applied how could we build a system that
would get better at this task the more
often it did it okay so I've been
working during during January with the
brain and cognitive science group at MIT
and we've come up with a first cut at
this we're partway through a set of 30
examples which are like those given by
summit and here's a particularly
interesting interaction so the same
example is given here Brent Harold we're
assuming that we are using that same
diet achmet a goal system but with a
test that the relations that are learned
are functional that is for any input
there must be precisely one output for
every a there must be precisely one be
for the solution to be acceptable so
with that constraint we get this program
built for here now I can actually show
you what this program is doing so this
is episode 4 is this particular problem
it breaks the problem into two parts EP
41 42 EP 41 calls to EP 43 and and
similarly ep 4 2 goes goes to EP 43 so
there's obviously something going on in
eat before three this does make up a
case in copy word that's building that
word Brent it's taking the input and
it's making it up a case to capital B
and then it's copying the rest of the
word re NT and if you see what happens
after that here you skip the that you
skip one here that skips over the dot
and then you do it again you do it with
Harold and that's the program that does
it right so this program is built using
the same approach matter interpretive
approach it takes nine seconds to build
that okay that's much much slower than
flash fill right so what's curious
though is that we've got this reuse of
the invented predicate which happens be
because we're minimizing the program
that's built the space by the way that
that's being searched to find this as a
size around about 10 to the 10 10 to the
15
but very little of that space is
explored because we're only looking at
things that are consistent okay so what
happens if we if we look at several
episodes when in fact episode two out of
the set of 30 problems is this one take
James and capitalize it to James with a
full stop at the end now that problem
produces this solution so it says okay
make up a case in word copy word and
then write a dot at the end but you
notice that this make up a case copy
word we've seen that before it's a sub
procedure that has been invented and
then when re it can be reused so the
solution now for EP for this problem is
one clause shorter than before because
we are reusing this EP 02 one twice over
here which shortens the program a
shorter program is found much faster
look the time here is three seconds to
do two problems to episode learning
problems rather than nine seconds to do
one right so we're getting a big speed
up well relatively big three in fact
there are other pairs of problems in
this set that we've looked at where the
speed up is up to 40 times faster
because of this kind of reduction
because you're finding useful sub
predicates that can be used the other
sub routines or sub functions that can
be used again so I I would claim that
this is actually a rather nice example
of whores law of large programs that
every large program contains small sub
programs within it and more than that
that these can then be reused and
produce more efficient programming or
learning in this case so I'm having to
watch the time now wrong a bit short so
I've got five minutes left the the issue
struck us at one point about how do you
turn this into a probabilistic reasoner
and it turns out that we can take a
matter interpretive approach like this
we can view
the space the hypothesis space as being
the set of prologue derivations that can
be produced and the set of abductions
from those are sets of ground facts
which can be treated as program so for
learning finite state accepted here's
part of the derivation space so here's
one loop being introduced and that's
extended with a one and so on now if we
take this derivation graph or refinement
graph as it's called in the inductive
logic programming literature and label
the individual outgoing arcs of the tree
with probabilities that a decided that a
chosen uniformly at each level so if
there are ten branch out of ten here
each one of these will be point one then
we define something which is essentially
a bayesian sampling distribution and in
this case it would be for the finite
state except azure for programs in the
more general case the property is that
each one of the leaves at the end of the
tree is consistent with all of the
examples and as you get more examples
coming in positive and negative examples
the derivation spaces grows and shrinks
according to parts that have been pruned
out by the exam being consistent with
the example so you can think of this as
an updatable bayesian posterior
probability distribution it's an
efficient way of updating the posterior
distribution over a structured
distribution so we can set up don't want
to go into detail with this but you can
set up all of the standard things the
prior the likelihood posterior and so on
in terms of this graph and compute
probabilities of hypotheses given
evidence in the standard way as the
product of likelihood and and prior with
a normalizing constant so skipping
forward because of time one nice
property that we found is that you can
efficiently sample from this derivation
space by choosing target probability so
one of the difficulties to begin with is
when sampling from such a tree that's
built on a bust casting logic program
for the matter interpreter you get lots
of repeats of
Lucian's if you do the simplest method
of sampling with replacement however you
can produce a sampling without
replacement approach by simply choosing
a target let's say point five and that
you now know that the interval of which
represents the sum of the probabilities
up to a particular point a particular
hypothesis that cumulative probability
if you sum that up under the first
branch it'll some to one-third under the
second branch to one-third and under the
third branch to one-third because of the
uniformity assumption so if we're
looking for the probability associados
associated with point five we know it's
down the second arc and this holds for
every level okay so we can walk from the
root of this derivation tree to a
particular hypothesis associated with a
target probability and x if x we can
then use that to do sampling without
replacement so if we choose the series
point one point two point three we can
systematically choose distinct
hypotheses which are maximally spaced
across the across the space they'll each
be different so we're doing sampling
without replacement in a in a regular
fashion and this is very efficient
because each sample just takes log n
times to get to the consistent
hypothesis when walking through the
derivation space ok so we've used this
we use this in order to build a a
bayesian predictor or an approximated
bayesian predictive where we build in
this case regular grammars in this case
family definitions we've compared met
based prediction with sample sizes of
thousand seven fifty against a map
solution this is a minimal maximal
posterior probability solution we find
as theory predicts that the base
predictor does better bathe prediction
is a theoretical large
a theoretical thing if you look at the
machine learning literature here we're
doing it for real we're sampling from
the space and we're doing the
predictions and we're finding that it's
at least in the case when you have small
numbers of examples the differences
between the base predictor and the map
prediction are significant if you're
going to learn from a small sample user
base predictor that's the the outcome
from this initial experimentation a
second idea was that of taking these
solutions which are each consistent
hypotheses from across the space and
superimposing them so this is a
deterministic regular otamatone here h 1
h 2 h 4 sampled across the space in this
case these are for that are consistent
with these two examples so if you had
only two examples with a maximum of two
states then these are the four
consistent hypotheses if you superimpose
those you get a kind of otamatone with
probabilities associated with the
individual arc so as zero goes from q0
to q 1 with probability point 5 and back
here with probability point 5 but a one
goes back from two forwards here with
probability 1 these probabilities are
simply formed by summing up the
appropriate probabilities over here so
so this one is formed by seeing where
the zeros are in that that correspond to
that arc and point 2 5.25 is that point
5 there so we can build a probabilistic
logic program in this fashion and you
might say well what kind of problematic
logic program does is this and in fact
by looking at the the way that by
looking at the construction it turns out
that this is a standard form of what's
called Pro blog these are this is a form
of probabilistic logic program developed
by in leuven their method of estimating
these labels is using expectation
maximization which is a standard
statistical approach here what we're
doing by taking samples you can simply
sum up those that have the appropriate
arcs and you can sum up the frequency
with which they
curr and converge on the same solution
this convergence is guaranteed by the
central limit theorem to get accurate
more in sufficiently accurate labels
here expectation-maximization actually
is not and in tests these pro blog
programs are being learned more
efficiently we found using this form of
estimation than the original form okay
conclusions so what I've taught you
about talk to you about is the is a new
form of machine learning called matter
interpretive learning based on ground
abduction it carries our two tasks that
were previously thought very hard within
the inductive logic programming
community that is in efficient predicate
invention and learning recursion the
definitions are simple they're compact
they produce nice readable logic
programs we're looking at we've managed
to look at tasks that go beyond the
simple a bunch of classification based
learning tasks that you see in many
forms of machine learning we are able to
approach tasks like hypothesis-driven
visual pattern recognition in the
staircase example just one example it's
not demonstrated on very much but all of
this work has been done within the last
year and a half okay so robot strategy
learning we've shown how you can do that
to build an effective strategy which
which improves with small numbers of
examples string transformation we've
started looking at this as a form of
bias change related to the gulanee
problems we're finding things that I
think are quite interesting to do with
how this approach can demonstrate
problem decomposition and introduction
automatic introduction of sub programs
in minimal program construction in a
minimal program construction can
approach and we're also showing that we
can build a bayesian
version of this matter interpretive
learning the reason that the progress
has been fast here is because the
learning algorithm and variance of it
can be written in a few lines of
prologue we're able to build these Metro
interpreters and test them within days
and that has led to very rapid progress
in developing this approach some if I
leave you that's the bibliography of
papers on this so far goal one is as the
last paper down here which isn't poeple
I'm curious so there's this search space
for example for this Asia two clauses
and it looks like one of the four main
techniques is you do these kind of
Bendix completion yeah so do do anything
else beyond that I described the other
things that we do in the implementation
sorry here that's one thing the second
thing is episodes so building one
solution on top of another you saw that
in the gulanee episodes two and four
here bill so searching from smoke from
small numbers of clauses in the solution
to larger number of clauses the log
bounding that is put a bound on to the
total the maximum size of program that
you're going to consider and yeah so
those are the main those are the main
three I suppose for things that are done
in order to produce efficiency over and
above so this is the control strategy oh
which is an extra logic program over and
above the basic matter interpreter the
very earliest forms of matter
interpreter would looked like the ones I
showed you the form that I've got now is
a general-purpose metro interpreter
which you just load up with metal rules
of the kind that I showed you so you can
you can reformulate using a general
metric turbo you can just choose your
metal rules
and there's a further approach that
we're looking out at the moment where
you step up classes of program classes
by adding a startup sod off with a small
number of metal rules and then add in
progressively more in order to search
larger and larger spaces you will stop
whenever you succeed right because what
you're doing is trying to find the first
solution unless you're searching
stochastically I thought yes very
interesting work I was wondering when
you do do these super imposed logic
programs yes is there anything stopping
you from weighing the superimposition or
the averaging really of the
probabilities by the by some kind of
likelihood term that would be driven by
the number of errors that a given
hypothesis makes on the training data
well including in your space hypotheses
that maybe get one of the examples wrong
and give them lot but there would
presumably be more of them to make up
for that we could deal with noisy data
in that way yeah we haven't looked at
that what we have taken a look at is
using bayes predictor to try to detect
noisy data okay so if in the situation
in which you use the bays predictor you
get a probability associated with a
prediction even if you're using
deterministic solutions like
deterministic grammars now in the case
that that probability is close to one
it's predicting that it's it's a
positive example if it's actual label is
zero and you've got a number very close
to one then it looks like noise or the
other way around if it's predicting a
base prediction very close to zero with
a with a positive label on it looks like
noise okay so what we're looking at at
the in this work is which I didn't
describe here is using the bays
predictor in order to do things like
noise detection and also for active
learning so for identifying instances
which you would
like to get more information on those
ones tend to be ones which have
probability close 2.5 which bisect the
budget exactly yeah hello you see a
little bit about it seems like there's a
correspondence problem here how do I
know that q1 in one of these figures is
the same as q1 and one of the other
figures you're right about that so we're
just doing something simplistic so we
had here a bag of of symbols and we're
just generating all of the solutions and
then and then matching up on on the same
symbol in each case right so I i agree i
don't have a particular answer to your
question but it's an interesting
question well that's the noise question
so if we've got wrong classifications on
the examples then I'm suggesting we
might try to detect that using Bayesian
predictions okay and then and then
eliminated well we haven't we haven't
tested that approach but that may be an
approach did we haven't we haven't dealt
with noise up to now after all of you
you get you make a mistake in the tree
beater yeah we will not recover yeah
flash fill the setting of flash fill
makes that particularly hard i think the
noise problem because the noise could be
anywhere normally with like binary
prediction the noise just changes the 0
to a 1 in the case of flash value this
it's quite a complex output and a single
piece of noise might just be an unusual
an unusual thing that you've been asked
to do right it's not present in the
other example so it's yeah it's it's
tricky I'm not sure how to handle them
questions that space even for a
fascinating</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>