<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Pit and the Pendulum | Coder Coacher - Coaching Coders</title><meta content="The Pit and the Pendulum - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Pit and the Pendulum</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x77yvcCMrgY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">nets well welcome professor motor indoor
of easy
well it's a well it's a big honor to be
here and I'm only sorry that you cannot
see yourself because you're beautiful
but I have some of my current students
that used to be where you are just a few
years ago and when I ask them for advice
about this talk they told me make it
exciting so I thought that what i would
do is to tell you a horror story so
imagine that you are tied to a table
somewhere deep underground and the only
sound you can hear is the swish of a
gigantic pendulum that is moving above
your head there is there are a couple of
problems the pendulum at the end has a
blade and is getting closer and closer
and closer and that this setting is the
setting of a classic story by Edgar
Allan Poe which is you know a classic
writer of thriller story American writer
and I'm sharing this to you with you not
just because it makes for a great story
for Halloween but because I think that
this image captures the condition of all
people of today's database programmers
see database programmers also have been
at the mercy of a pendulum that has been
swinging above their head and sometimes
inside their head between you know
swinging between two seemingly
irreconcilable goals ease of programming
and performance the pendulum started
squarely pegged on the size of ease of
programming with abstractions such as
serializable acid transaction but then
it started moving and over the last
decade with the surgeons of the NoSQL
based movement it has really started
swinging fairly dramatic
towards performance until people have
started to recognize that it's really
hard to program in those models and the
pendulum has started to swing back again
but still is not a comfortable position
the one of our friend database
programmer so let's actually look a
little more in detail about what is
motivating the swinging of this pendulum
it are all started with acid
transactions and their promise of
simplicity and generality and indeed
let's face it acid guarantees are nice
it is nice to know that all I have to do
is to wrap my code inside a transaction
to have the guarantee that it will be
executed atomically that it will be
isolated from other transactions and
that its effect will be durable and it
is nice to know when I'm reasoning about
the correctness of my code but all I
have to do is to make sure that each
transaction individually transitions the
state of the database from a consistent
stay to a consistent state without
having to worry about the internal logic
of all the other transactions in the
system so there is no question that acid
guarantees are nice but the question is
can we afford them and that's a question
that we have been struggling with for a
while actually it was the database
community that started moving the
pendulum somewhat to the right by
starting to explore notions of isolation
and you see some examples up there that
are weaker than serializability and
basically being willing to give up some
simplicity of programming in exchange
for performance but during the last
decade this the movement has become
significantly more dramatic with the
development of this approach called the
base approach so base supposedly is an
acronym that stands for basically
available available
soft state eventually consistent but
truly is just a nerd joke right they
just wanted to say that they're not acid
and so their base and in fact the base
approach has really not a lot of
patience for acid they take
transactional guarantees and I just blow
them away they destroyed acid storage
and substitute for it something that has
a much simpler interface something like
put and get on top of a key value store
and all of this is supposed to give
programmers the capability of squeezing
every ounce of performance from their
code by writing custom code the problem
is that with this case with great power
comes great complexity without the
guarantees of transactional storage it's
up to the developer to actually
implement at the application level they
consistency guarantees that are
necessary to make the application be
correct and that is hard because one has
without transaction one has to think
about all the possible interleaving
between all possible operations and
that's something that becomes
exponential in terms of the interleaving
that one has to consider with the size
of the application and soon complexity
gets out of control and that gets us
back to our story and what I would like
to share with you today is what my
students and I have learned trying to
get this guy out of this situation okay
and I hope it's going to be you know
you're going to get something not just
out of the technical solution that we
have come up with but also about the way
in which we have been trying going about
it so the challenge here is that the
very thing that can give us more
performance which is increasing
concurrency is really what makes program
in difficult and
you can see an example for instance if
you look at this simple transaction a
transfer transaction that transfer ten
dollars from a checking account to a
saving account after checking that there
is enough money in the checking account
and one can imagine increasing
concurrency by actually splitting this
transaction into two pieces one piece
that takes care only touches only the
checking account and another piece
attaches only the savings account an
idea is that I get more concurrency
because instead of executing say two
instances of transfers sequentially now
I can overlap their execution to some
degree I can have the second part of one
instance of the transfer overlap with
the first part of the second instance of
the transfer so I can possibly go wrong
well what can go wrong is imagine that
there is another transaction say that
we're going to call balance that
actually accesses both the checking
account and the saving account to try to
compute what is the total amount that
this individual has it used to be the
case that balance was going to be run
either before the transfer transaction
or after the entire transfer transaction
and so it was going to compute the
balanced correctly but now having split
the transfer transaction we are actually
exposing to balance an intermediate
state that was not visible before and as
a result of that as you can see balance
may actually compute the wrong result
what is really disturbing about this is
that as soon as we try to increase
concurrency and even if we touch a
single transaction that may actually
force us to have to rethink correctness
about of every other transaction because
we have to think what are the
consequences of the new interleaving
that we are allowed and and it seems
that actually this is an instance of a
fundamental problem if I want to have
concurrency
I have two for better performance I have
to increase the interleaving that are
visible but if i increase the
interleaving that are visible it becomes
more complex reasoning about the
correctness of my program and so we have
this dilemma and this is where we turn
up to this guy his name is a vilfredo
pareto is a fellow italian and he's
known for most known for something that
is known as the Pareto principle the
Pareto principle says that in many
system twenty percent of the causes is
responsible for eighty percent of the
effect in computer science sometimes we
have an even more extreme version of
that we hear of the 9010 rule right so
what we did was actually to download and
inspect applications you know database
application open source database
application and we saw that the Pareto
principle all's also there so if you
want not all transactions are created
equal there are only few transactions
that actually stretch the performance
capability of the acid paradigm most
transactions don't they don't either
because they run infrequently or because
they are lightweight they have little
they run for a short time or they have
little conflict with other transactions
and so if it is true that there is this
imbalance between the behavior of
transaction that actually brings new
light to these trade-offs that exposing
interleaving brings up between
complexity and performance see for most
transaction exposing more states and
allowing more interleaving only brings
complexity doesn't really pay up very
much in terms of performance it's only
for a few transaction this few
performance-critical transaction that
the Pareto principle says that exists
that the complexity actually produces
also a payoff when it comes
performs so then if if that's the case
then an idea that comes to mind is what
if instead of exposing this greater
number of interleaving indiscriminately
we were to do it in a selective way what
if we were to expose those interleaving
only to the few transactions that can
actually gain performance out of it and
make them completely invisible to the
rest of you know the remaining
transactions and if we did that then we
might get a solution that gives us most
of the performance that we would be able
to get if we were to actually expose the
interleaving to everyone but at a
fraction of the complexity and this is
the idea that guided us in designing a
distributed database system that we call
salt because we are also nerds and salt
is what you get when you put a a base
and an athlete together the idea of salt
is to actually leverage the observation
that not all transactions are created
equal and if you want to specialize
provide a flexible abstraction
transactional abstraction that allows
the few transactions that are
performance critical to selectively take
advantage of these exposed States so the
abstraction that we have introduced we
call it a base transaction and the way
in which you can imagine basa fying if
you want your asset transaction is as
follows you take the Icee transaction
and then you partition the operation in
a sequence of of sub transactions that
we call again a sort of silly joke we
call alkaline transactions and there is
something and you can think of alkaline
transaction as mini acid transactions
and there is something there are a
couple of things that are really nice
about base transaction the first thing
is that they too
some of the simplicity of the asset
paradigm they continue to provide
atomicity and durability at the
granularity of the entire base
transaction but when it comes to
isolation they actually allow to as we
will see they allow to selectively
expose the state that are intermediate
between us successive alkaline
transactions so that they actually this
base transaction can actually look
differently depending as you will see
depending at which other transactions
are interacting with at the core of this
capability is a new isolation property
that we call salt isolation South
isolation creates an illusion a little
bit like the image that you see there
see if you look at that image if you
tilt your head on one side it looks like
a duck if you tilt your head on the
other side it looks like a rabbit and
salt isolation plays some similar trick
to base transaction another base
transaction looks as a sequence of
distinct alkaline transaction and allows
other base transaction to interleave at
the granularity of alkaline transaction
but instead to all remaining the large
number of acid transactions that I
remaining it looks like base transaction
are just acid transactions as before
nothing looks to them as it has changed
as so so they look like a single
monolithic acid transaction this means
that again if a base transaction runs
concurrently with another base
transaction they can interleave at the
granularity of alkaline transaction if
instead he runs concurrently with a
nasty transaction the only states that
are visible are the states that are are
the states that are either before or
after the base transaction as if that
base transaction where a nasty
transaction so you may ask what kind of
results that this word provide well
there are two questions that we wanted
to answer what kind of performance we
are able to get by using this salt
approach as opposed to simply the acid
mechanism and the other thing that we
wanted to ask is what programming effort
is actually going to be required in
order to get reasonable performance so
we I'm showing you just two experiments
to simple experiments on the are about
to application one is T PCC which is a
database benchmark that models online
transaction processing and the other is
fusion ticket which is an open source
online ticket purchasing application and
as you can see in these graphs when
compared with mysql cluster we are
getting in both cases a throughput that
is at least 6.5 times higher than the
throughput that one could get with a
simple the original acid mechanism how
much work was it required to actually
get that kind of significant throughput
gain well it turns out that for tpc see
all we had to do was to base if I just
two transaction in order to get eighty
percent of the benefit basa fying a
third transaction gave up one hundred
percent of the benefit with fusion
ticket we base ified one transaction got
up remarkable improvement in throughput
then we started visa firing a second not
much different basic buying a third not
much different the problem is the more
transaction you start to be safai the
more difficult becomes thinking about
the correctness of what you're doing
because you start having this problem of
having to think about the exponential
explosion in inter leavings and we
didn't want to basa Phi the 11 or 12
transactions that we would have to do in
order to base if I everything so we
actually took a very extreme position
all with what we did was to chop each
transaction in its raw operation and
just run them as raw operation
of course the result was incorrect but
he was giving us an upper bound on the
amount of performance that we would be
able to get if we had base if I'd
certainly we would get something no
greater than we would get just by
chomping without worrying about
correctness and what we are seeing is
thus basa fying a single transaction
allows us to get ninety percent of the
performance that we would get if we had
just chopped blindly so you know when we
looked at these numbers you know we felt
pretty good we thought you know perhaps
we have actually done something you know
created some damage also to that to that
pendulum that was swinging around a and
and then we knew I started going and
telling people about the result that we
have had and people would say ah that's
great but what about those few
transactions that you have to base if I
how do you do that and I would say well
you know that is hard basa fighing
transaction is always hard but the good
news is that you have to do it only for
few you don't have to do it for all the
transactions and I would say that's
great but how do you base if I those few
so the problem is that unfortunately
base affiant transaction is still hard
and an hour pendulum is far from being
destroyed and so you know when we got
this kind of feedback so we started
asking ourselves I don't know I mean
what what can you know what do these
programmers want I we had the feeling
that they were trying to send us a
message but we just couldn't quite
understand what was the message that
they were trying to send us and then it
dawned on us that what they were trying
to point out is that we were trying to
find a sweet spot between the base
approach and the acid approach but when
it comes to ease of programming for many
application there is really no sweet
spot outside of the acid paradigm as
soon as you move out it's not a question
of sweet
is a question of how sour it is going to
get so what we then decided to do was to
say okay you know what what we are going
to do is we are going to instead of
trying to weaken the acid paradigm even
if selectively we are going to instead
embrace it completely and we are going
to try to find a way to provide
uniformly this acid attraction for all
transactions but we're going to try to
do it if you want in a way that is a
little more performant if you want so
the observation that we are going to
leverage is once again the observation
that not all transactions are created
equal before we were trying to leverage
this observation by providing a flexible
abstraction instead here we are going to
use a uniform abstraction for all
transaction including the one that I
performance-critical what we are going
to instead do is to tune the mechanism
that is going to be that we are going to
put in place in order to implement that
abstraction and that led us to develop a
second system database that we call
colors if you don't know who this person
is you do yourself a favor being her
Maria Callas one of the best opera
singer ever and so callous provides a
uniform abstraction acid abstraction to
all transactions that allows that
abstraction to actually be embodied if
you want in a number of different ways
okay and the architecture of colors is
actually based on a key observation the
observation is that if for ease of
programming uniformity the uniformity of
the abstraction is fundamental when it
comes to performance having a single
uniform mechanism can actually hinder
performance and preventing too if you
want to be able to take advantage for
opportunities for optimization the
observation if you want to put it in a
different way any mechanism concurrency
control mechanism that has to work for
all possible abstractions for all
possible transactions we have to
necessarily make a conservative
conservative assumption and in the
process give up the opportunity to
actually leverage some interesting
optimizations so then what colors does
is it decouples abstraction from
implementation it provides a uniform
acid abstraction to everyone but it
introduces a new technique that we call
modular concurrency control that
actually allows colors to partition
transactions in groups and then assign
to each group customized concurrency
control mechanism that only applies to
the transactions that are in that group
and then have a cross group concurrency
control mechanism that is going to
handle conflicts that may exist between
transaction in different group the idea
is that we can be more aggressive by
when when we are trying to just apply a
concurrency control mechanism on a per
group basis so if we are focusing it did
the observation that we are making is
that modular concurrency control allows
us to leverage another fundamental
principle of system design the press the
principle of separation of concern
separation of concern allows us when we
are thinking about the correctness of
our in-group mechanism of worrying only
about the transactions that are in that
group without worrying about the
transaction that are another group and
this is important of course for proving
correctness it makes proven correct as
much much easier but it is a crucial
also when it comes to performance
because the idea is that if we we can
actually be much more aggressive when we
are trying to optimize the concurrency
control mechanism within
each group because the optimization
doesn't have to hold for every
transaction just has to vote for those
few transactions and before I'm going to
tell you more about how these modular
concurrency control mechanism works I
want to tell you you know at the end of
all of this you have to be confident
that what you're building is actually
correct and the foundation the
theoretical foundation for our work are
in the beautiful elegant work of
idealistic of and O'Neill that actually
identify conditions for you know what
are the condition for defining different
notion of isolation in particular what
condition must occur in order for
isolation to be violated in particular
one of the thing the key condition that
they say must old to prevent the
violation of isolation is a condition
the condition of no circularity
circularity that is defined on what they
call a direct serialization graph this
is a graph that has as nodes committed
transactions and edges capture
dependencies that exist between this
transaction let me give you an example
so for instance here I have two
transactions and on top i'm showing what
you know the direct dependency graph
would look like suppose that the first
transaction rights variable a the second
transaction reads it then transaction to
depends on transaction one and we are
going to express that notationally with
an edge with that connects transaction
one to transaction to in the direct
serialization graph now suppose now that
transaction to actually write B and then
transaction one right B then there is
now a dependency from two to one and
there is a cycle in the dirac
serialization graph and that's not good
okay if there is a cycle there that
would mean that serialize ability in
this case would be violated okay
so what we then want to guarantee is
that our modular concurrency control
approach it's going to ensure that there
cannot be cycles among all the
transactions in my system and I'm going
to split that enforcing that guarantees
in two parts I'm going to say that I
would like to make sure that there are
going to be no cycles within each of the
groups and then under the assumption
that there are no cycles with energy
within each of the group that there are
no cycles that actually involve
potentially multiple groups and what I
would like to do is to start by telling
you how we can handle the latter part of
the problem so the mechanism that we use
to prevent these dependencies across
multiple groups is something that we
call Nexus locks is a new flavor of
locks okay Nexus locks are again
supposed to prevent circularity zor
handle if you want better said handle
dependencies or conflict that exists
among distinct groups transaction in
distinct groups and next two slots are
ubiquitous every time our transactions
wants to touch an object it must first
acquire the nexus law for that object
and Nexus lock have a dual nature when
it comes to transactions that are in
different groups then Nexus locks behave
like regular logs they prevent
transactions to proceed unless the
operation in this to transaction do not
conflict like Reed &amp;amp; Reed but instead
when it comes to operate two
transactions that are trying you know
are conflicting on a lock with in the
same group they're just going to say
please go ahead the goal of Nexus locks
is to try to guarantee correctness while
trying to stay as much as possible out
of the way of the potential performance
gains that using the in group mechanism
can give us if we don't want this
access logs to be inclusive so um so it
sounds great unfortunately if I just
acquire Nexus locks it turns out that
that's not enough to guarantee that
there are not going to be cycles across
groups let me show you an example here
is I have I'm going to have two groups
the red group and then you see a green
group in the red group I have a
transaction that starts by touching a
and writing any requires the Nexus lock
on a then there is another transaction
that is running concurrently with in the
same group also tries to write a but
because they are in the same group it
can just simply acquire the Nexus lock
okay no problem then the transaction
there is a dependency of course that is
generated between t1 and t2 because they
are touching the same object then
transaction to say rights object B and
then acquires the corresponding
corresponding next tues lock and then it
commits after it has committed there is
another transaction a green transaction
from another group that tries to that
wants to touch p of course you can't
touch me because there is no running
transaction that has a lock on be
transaction to has already committed so
it can acquire be and in so doing
generates a dependency between t 3 and t
2 and now say that transaction 3 then
touches see and then commit and now to
just make our life really difficult
transaction one is going to touch see
and when that happens that generates a
dependency between transaction 1 and
transaction 3 and actually closes a
cycle and you can see that i can have
now a cycle that crosses no spans
multiple group even with even if there
are no cycles within each of the groups
that's really bad so to try to solve
this problem what we did was to go back
and look at how transitional locking
takes care of this problem traditional
locking what it does it actually ties
the notion of depending so depends on is
tied to
completes before so here if I have a
situation you know this situation that
you see their transaction to is actually
not going to start executing until
transaction one has released these locks
and transaction three is not going to
start executing until the transaction to
as release this lock and that guarantees
that there can be no circularity
fundamentally completes before is a
non-circular relation and by tying
depends on to complete before one
guarantees that also depends on cannot
have cycles okay and now if we now look
at the problem that we had it's clear
what was the problem the problem is that
nexus locks only tie depends on to
complete before for transactions that
live in different groups for
transactions are in the same group it
doesn't any it chooses to do that on
purpose because if it did tied then it
would prevent transaction 1 in
transaction to from executing
concurrently and so we would lose
performance if we would become intrusive
and we don't want to be inclusive so how
do we solve this problem what we person
you know the way in which we ended up
solving it was by making an observation
that i had the opportunity to make many
times in my career sometimes you see
that there are systems that are
providing a certain guarantee and there
is a certain implementation that
provides that guarantee and if you look
carefully at that implementation you
discover that that implementation
provides that guarantee but actually
provide something stronger than the
guarantee that you need to enforce so
you have the implementation if you want
is sufficient to provide the guarantee
that you want to enforce but is not
necessary and fortunately this is the
case also here it turns out that what we
end up doing is instead of time depends
on to complete before what we do we tie
it to the weaker notion of releases
it's Nexus locks before so we have wins
you know we enforce this nexus lock
release order that says that if t2 and
t1 are in the same group NT two depends
on t1 then t2 cannot release its Nexus
lock until t1 has released this nexus
lock let me show you what that means in
our example in traditional locking these
two would have to execute sequentially
right and instead what we are going to
do is we are going to allow them to
execute concurrently and in fact at this
when transaction to is committing
transaction to will notify the user that
it has committed and will actually
release any resources that transaction
to was keeping in order to maintain
isolation within the group that is going
to hold on to the next knocks until
transaction one has given up its Nexus
locks and this is enough to prevent the
cycle from occurring with transaction 3
while allowing transaction 1 in
transaction to to execute concurrently
so i told you about isolation across
groups let me tell you about isolation
within groups my solution across groups
was fundamentally trying to be safe and
not be in the way of performance
installation within group once of course
to be safe but it has to do it while
trying to squeeze performance out
otherwise where is you know where is the
game and it turns out that the ability
to think modularly is going to play a
critical role here because it's going to
allow us to use in a more you know
intuitively is going to allow us to use
optimizations that may not work as I was
saying for all transactions that may
work instead just for the transaction
that are within each groups and it turns
out and in fact modularity is going to
help not just with new techniques but
also with techniques that have been
around for 20 years and it's going to be
able to
bring new life to these techniques one
example is a technique that was
developed 20 years ago by Sasha and
others call transaction shopping this is
a very very very nice idea that I let me
share with you the ideal transaction
shopping is to take transactions and
then using static analysis identify what
is the finest shopping of this
transaction that is going to guarantee
that serializability despite the
chopping will be satisfied okay use
static analysis to be able to provide a
guarantee and that has the beauty that
every any performance that I can get out
of this technique comes for free because
the programmer has no additional task
that is required to perform so when
transaction shopping works it works
great unfortunately it doesn't quite
work as often as we would like it to
work or as effectively as we would like
it to work the problem is that in order
for a chopping to be safe what it has to
do is to prevent what are known as SC
cycle to understand what these SC cycles
mean imagine that I take as you have
seen these transactions and I chop them
and I connect the pieces within a single
transaction with something that is
called an S edge and if there are
conflicts between pieces in other
transactions they are connected with
something that is called a CH okay a
situation like this no good because it
contains a cycle that involves both s
and C edges and so what we would have to
do is to say sorry this chopping was too
aggressive we have to make it coarser
until we have eliminated all the cycles
okay so the idea you know the way in
which modularity is going to help is
that instead of having the requirement
that we don't want to have SC cycles
among all transactions now
can actually specialize it only to the
transaction with that are within a group
as long as there are no se circle within
this group then I can shop even if that
shopping would be unsafe for all
transaction it might just be safe for
the chopping within each group and it
turns out as we will see that modularity
can already helped significantly in
taking getting performance out of this
all technique but it turns out that even
with the you know this inside
traditional transaction shopping
traditional transaction shopping still
often provides shopping that is too
coarse to actually get performance up
and one extreme example of that happens
when i'm considering potentially self
conflict between transactions for
instance consider this situation which i
have two instances of the same
transaction these transactions are
writing variable x y&amp;amp;z what i would like
to do is to try to chop them well if I
chop them what I see is that the only
way to actually chop them that does not
involve an sc cycle is not to chop them
and I would have to actually keep them
you know all together getting no
concurrency after them so what instead
we are going to do is to introduce a new
mechanism that we call run type I
planning that is going to try to solve
this problem of self conflicts again
let's look at these transactions that I
was showing you before now suppose that
actually at runtime what happens is that
transaction one touches x and then
transaction two touches x what that
means is that there is going to be if
you want the dependency that is
generated between transaction 1
introduction to going from left to right
the observation is that we can avoid
cycles from this point on to avoid cycle
all we need to do is to make sure that
all the other dependencies are going to
have the same direction
so intuitively what we are going to do
is to if you want use the arrow of time
to break the cycles at static analysis
time the sea edges are undirected edges
but at runtime they have a direction and
if we make sure that these different
dependencies are all in the same
direction if we schedule transaction
appropriately we are not going to have
any cycles what are the consequences of
this well we can take this situation
that was unsolvable before and now
instead we can pipeline the execution of
these transactions so that as soon as
the first transaction will execute to
the first piece we can start executing
the first piece of the second
transaction in parallel with the second
piece of the first okay and there is
another advantage that comes from using
this runtime mechanism the fact that at
static analysis time of course I have to
be pessimistic when I'm considering
possible conflicts I have to consider as
conflicts every conflict that may
possibly arise at runtime I can actually
detect if precisely when a conflict
arises I don't even have to trigger this
runtime pipelining mechanism until a
conflict has actually risen now if you
look at this simple example you see that
they see edges one of the property that
it has is that the sea edges are
parallel in general if I look at a
transaction that I'm trying to apply
this technique to there is no guarantee
that the edges will be parallel they may
well be crossing and one of the things
that we had to do was to develop a
general algorithm that actually is
guaranteed to take whatever we throw at
him and basically eliminate the crossing
between C edges so that what I'm going
to have and the result of this is going
to be another graph where the sea edges
are parallel to the to each other
so actually it turns out that this you
know this technique that we have
developed if you want takes transaction
shopping that was providing the finest
safe shopping and refines it it actually
does allow now chartings that are unsafe
but only those unsafe chopping that
runtime pipelining can then break at
runtime so at this point one thing that
I have in disgust is how are we going to
group these transactions I thought about
grouping how are we going to group them
well it turns out that once again we can
rely on the fact that there is this
Prince this observation that not all
transactions are created equal see for
most transaction it doesn't really
matter how we group them because there
is not a lot of performance that we can
get any way out of them so what we can
do is to put them all together so only a
few are going to matter so the remaining
we can put all together in what we may
informally call the lame group but let's
just call it the generic you know some
kind of generic group in which i'm using
some standard concurrency control
mechanism and when it comes instead to
the few transactions that are
performance critical since they are few
i can actually exhaustively explore what
is the grouping that actually provides
the best performance and we have some
kind of greedy algorithm that
approximates that idea so finally how
does something like this work there are
a lot of questions performance why that
you may want to ask the only question
that I'm going to answer now is how good
does it do when it comes to end to end
performance and here are some graphs
again 40 PCC and fusion ticket and let
me walk you very quickly through this
graph the first thing that you can see
is that in comparison with mysql cluster
infusion ticket for fusion ticket we get
a throughput push of you know a factor
of 6.7 for
ppcc 8.2 times I mean it it's kind of
encouraging and it's nice to see that if
I just use transaction shopping without
modular concurrency control you see that
it helps to some degree in the case of
TP CC but it doesn't help at all in the
case of fusion ticket it doesn't find a
way to actually chop the transaction
safely but if I'm using modular
concurrency control with traditional
transaction shopping you see that we get
a pretty substantial gain from
modularity just by using traditional
transaction shopping of course if
instead of transaction shopping with use
what we use in colors which is runtime
pipelining we solve the problem of self
conflict and and throughput release
shoots up and what is remarkable is that
in both cases we are able to get within
only five percent of the throughput that
we were able to get with salt the
difference is that insult we had to
actually ask the programmer to base if I
certain transactions here we are asking
the programmer to do nothing at all and
we are getting within five percent of
the performance that we were getting for
salt so that actually will come to the
end of my talk I showed you two
different system assault and callous
that you start from the same observation
not all transactions are created equal
and actually draw very different
conclusion in very different ways in
which they're trying to leverage that
observation and when it comes to our
friend in our story I like to think that
you know he's a little a little better
off you know what what can possibly go
wrong so that's the end of my talk
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>