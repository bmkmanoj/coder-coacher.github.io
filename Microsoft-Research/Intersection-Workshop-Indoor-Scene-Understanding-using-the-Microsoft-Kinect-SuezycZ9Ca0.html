<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intersection Workshop - Indoor Scene Understanding using the Microsoft Kinect | Coder Coacher - Coaching Coders</title><meta content="Intersection Workshop - Indoor Scene Understanding using the Microsoft Kinect - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intersection Workshop - Indoor Scene Understanding using the Microsoft Kinect</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SuezycZ9Ca0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so this work is all about inferring the
support relationships between objects in
indoor scenes we've captured with a
Microsoft Kinect and it's part of a fun
collaboration of work that we've been
doing with Natanz civil and my student
push meet here at MSR and Derek I'm at
UAC okay so much of the work in seen
understanding has been focused on the
sort of recognition element that is
you're trying to take an image and
pretty each pixel you know provide a
discrete label wall picture floor table
and so on but as derek was saying this
really ignores a lot of information
about the scene for example the geometry
layout of the room or the physical
relationship between objects and this is
what the the latter is what this talk is
all about so why would you care about
such things well first of all if you
want to interact with the room at all
you know if you're a robot and you want
to pick up one of the books you better
understand there's actually a mug
sitting on top of it so you have to move
the mug first and then of course you
also have the the ultimate goal perhaps
that if you can understand the support
relationships well you could use those
to go back and help your object
recognition problems so if you know that
there's some objects that signal top
this desk that dramatically limits the
the sort you know possibilities for what
it can be for example it's unlikely to
be a fridge and it's more likely to be
something that's you know it's very
small and sits as you've seen before
sitting on top of desks so what we're
shooting for is something like this you
want to take a scene I complicated into
a scene like this and you want to
understand that you know the lamp is
supported by the nightstand and the
nightstand is supported by the floor and
the pillows are supported by the bed now
what you might just think what this
would be easy if you had a kinect you
could just you know get your depth data
and then look at what was beneath each
object unfortunately in real indoor
scenes there's an enormous amount of
occlusion so for example this little
night stand over here you just don't see
the floor that supports it okay so you
have to do some sort of quite
complicated reasoning to understand that
actually you know this piece of ground
here is supporting this thing over there
now I'm the use of sort of geometric
prize
do this kind of reasoning we need to use
sort of geometric prize about the way
scenes work and there's been a sort of a
lot of work by Derek and you know don't
forsyth and you know Varsha head out and
people like that on doing this this kind
of task from single views perhaps the
most closely rated work to our own is
this blocks will revisited paper Valley
Usher now been a forgotten Michelle
habere so there they use of various
heuristics based on physical reasoning
to try and understand and the layout and
and the sort of volumetric shapes in
outdoor scenes so this work is very much
in the same spirit we're going to be
addressing indoor scenes so which are
much more complicated and cluttered but
we do have the added advantage of course
that we have the depth information from
the kinect sensor now to be able to do
useful things we need a course a data
set of indoor scenes from the connection
in previous work we introduced a sort of
what we're now calling to a version one
of the data set which is a variety of so
indoor scenes we captured around New
York and we have now a new data set as a
reversion to of this which is bigger in
terms of it's certainly more diverse
it's actually slightly fewer frames but
each one is much more distinct the
problem one of the problems previously
was that all the shots worth of
essentially these rather cramped
manhattan apartments and of course
that's not really representative of the
way the real world is so we got a ten
and sent him off to various places
around the u.s. and if you want to
capture you know big indoor scenes with
you know big people's houses then we're
better together than texas so we sent it
in a turn-off down to Houston where his
parents live and so many of these shots
will be a you know complicated you know
large indoor rooms with enormous numbers
of objects in so for a small I mean we
captured about half a million frames of
Connect data and then we took
essentially from each scene you know a
small number of distinct views and it
got those density labeled on Amazon Turk
okay and the Turkish you know that
resulted leo about 30,000 separate
objects labeled across about a thousand
classes and it's separate instances word
labeled so if you had a table with
multiple cups on each cup instance would
have a separate label which is important
the kind of reason that we're going to
do so just show you some examples so
these
the kinds of shots here you know this is
you know living rooms bedrooms and so on
this is the raw depth information we
also have a version which is aligned to
the RGB image and in painted to try and
fill in some of the holes and these are
then the dense labels provided by the
Turkish so you can see that the scenes
are complicated there's an enormous
amount of occlusion you know for example
these chairs you don't really see where
that you know any see the top of them
you don't see the bottom and then so it
consists essentially you know wide about
this if you go back to this plot here
this table here there's about twenty six
different scene types so it's you know
all manner of indoor scenes that you
might encounter now the because we want
to reason about support just having
these recognition these sort of you know
class labels for the pixels in the scene
isn't enough we also need support label
so the data it also has for each label
image these were recording support
labels which are essentially saying you
know for example for this map region to
it is supported by region 3 from behind
so this in kids case it's a horizontal
support relationship and then you know
we also know that region 4 so region one
rather is supported by region for the
floor from below this is a vertical
support relationship and so we have
those densely over the data set as well
ok so the particular algorithm I'm going
to describe is structured as follows so
the our input is going to be an RGB
image from the Kinect and a depth map
from the Kinect which has been aligned
to the RGB image and then it's going to
be some initial processing phase where
we're going to essentially identify the
major surfaces in the scene and align
them to some canonical reference frame
so we know for example we know what's
the floor and what are the walls and
then we're going to do a segmentation to
try and discover you know just just
think regions within the scene and then
at that point will then start doing some
more sophisticated reasoning about the
relationships support relationships
between those segments and to produce
the output of the system so just to go
through these the pre-processing phases
quickly essentially we're going to
assume that there is some sort of a plug
and I'll see between the wall on the
floor
no not in the Stata Center or something
I hurt at MIT the the idea is to sort of
essentially take the point cloud we get
from the Kinect fit planes to the major
surfaces and then rotate everything so
that the floor is pointing directly
upwards and you might think what you
could do this automatically with the
connex accelerometer unfortunately it's
just not sensitive enough but using the
sort of software based approach this
plane fitting approach we can actually
get you know about eighty percent of the
scenes to within about five degrees of
in the floor being within five degrees
well the normal vector from the floor
beam within five degrees of vertical
there's a small number of scenes for
which the floor isn't visible which
makes it much more challenging so this
is why you know there's still a sort of
a number of scenes up this sort of small
fraction of scenes for which it's
actually quite difficult to get the
floor exactly aligned now once we've
done that we then um compute super
pixels on the scene like so and then
we're going to treat this can use the
approach that Derek and others followed
in some previous work which is to take
pairs of regions in this in the
segmentation and train a little
classifier using the ground truth
regions to predict whether we should
merge or keep the region separate and
those are going to be based on lots of
features like you know sift computed on
each of the regions or little bag of
words color and things like that and so
that's good so our little merging
classifier is going to say which of
these regions we think we should combine
and that will give us then this kind of
output here in stage two then we train a
second classifier on the segments and
arrive at stage two in our training set
to emerge to stage three and so on so
this gives us a sort of hierarchical and
of agglomeration of these segments
resulting in the end of five stages
hopefully we have sort of fairly large
objects or merged into one region so
this perfect this process of course
isn't perfect and so in some experiments
well show you the effect of using these
segments we've obtained from the images
versus the ground truth segments we've
obtained from the Turkish now the major
part of the system of course is how we
do the support classification and this
is you know quite complicated and we're
going to need to introduce various
something versus um shins about the way
the physical world works in order to
make this tractable so the first
assumption up is that we're assuming
that each
yes it is it is it uses that we have an
ax sort of ablation study where we take
out that the by default we use the depth
information from the connect and it does
help if you take we'd have have a view
graphs at the end which we take out
different components and you prob yeah
okay so the first assumption were going
to make is that each object is supported
by a single other object in the world so
we can't have a to an object can be
supported by two separate things it's
going to be sort by one thing and
there's Anna sort of special the one
exception to this is the floor so the
floor can isn't supported by anything
and can support multiple objects on top
of it the second assumption is that from
most of the objects hopefully there'll
be some sort of visible support surface
for example the deodorant is supported
by the sink but in this image you can
see the floor is invisible so we have
actually a sort of hidden region which
is really the survey hidden floor so
this cabinet is going to be supported by
this hidden floor region and then the
third assumption is there's only two
types of support that can exist
essentially a sort of vertical support
or horizontal so that in this case the
joint is supported vertically by the
sink on and then the other form is that
the mirror is supported horizontally
which mean in the mirror is supported by
the wall from behind now we don't want
to sort of get into the the complexities
of trying to solve the full recognition
problem so what we're going to do is to
take on a slightly easier challenge and
for the purposes of trying to understand
the support its adequate we're going to
essentially break all the objects into
the scene into sort of for structural
classes it's either though the ground
structure which has really means walls
furniture large objects in the scene or
props which are sort of small objects
that typically will be you know
interacting in some way with the
furniture and so we're going to build
essentially a classifier to predict for
each region locally is it which of these
four possibilities that it is so yeah in
this little color scheme here we're just
showing the typical scene the flaws in
magenta the chairs room purple and the
props are in blue and the walls are in
yellow
okay so just to set up the the formal
problem we've got we've got this scene
we have a set of sight regions and so
for each region I we want to try and
figure out which is the supporting
region is it one of the other regions in
the scene is it the hidden ground or the
ground itself and then given if we want
to identify which regions doing the
supporting is it supported vertically or
horizontally a bullet from below or
behind and then for each region is it
one of these four structure classes
floor furniture proper structure and so
the problem is is given the image try
and figure out what the optimal you know
assignment for each of these different
variables is for each region so we can
take this problem and it boils down to
essentially minimizing a cost function
which has three separate terms and I'm
just going to go through each of these
in turn so essentially the first two of
these are just going to Valle vocal
information we got from looking regions
or pairs of regions and then the third
term is more in some ways that much more
interesting one the prior which is going
to encapsulate ass or geometric
understanding of the world yeah you can
yeah well I think the the support
raishin ships are quite complicated so
in the you in there well they just sort
of modeling there this building is
sitting on this ground plane and stuff
like that in our case we've got sort of
multiple layers of support so you know
got you know a little book signal top of
TVs into the table right but it was like
a seven nation with that humanity or
today and it's really you you need to
understand these pieces to understand
who addressed it it's not even about the
reconstruction is really just
understanding yeah yeah it it's without
without supports you're just looking at
you know a backlog of floating geometric
have a very underlying political support
is a thing about the world humans in and
of itself it's not a route to proving
reconstruction you don't understand
support and you can't lots of what will
happen without you have to be able to us
just a combination working on this end
to show that reconstruction get much
better do you think like Georgia do it
better and I can read yeah yeah but yeah
but I think it's good for many different
things if you if you understand some
work better then you understand the
scene better once we understand see that
better you can do prediction you can do
3d reconstruction dip in the reason you
any okay that's fine so okay so let's
just look at the local support so this
is very simple think we're going to take
pairs of regions in our scene we're
going to extract a variety of features
that capture somehow the relationship
between them for example the you know
the distance between them the you know
the ratio of the bounding boxes are they
in physical contact seemingly in 3d is
one object completely contained by the
other when you look from above and
things like that and we're going to feed
those into a classifier which is then
going to come out with one of three
predictions either they have no relation
whatsoever perhaps region two supports
region one from behind or it supports it
from below okay so just don't be clear
here this is a purely local reasoning
Yeah right it's true so there's actually
a whole repertoire features here it's
possible they might be included I can't
remember exactly they aren't not it also
occluded quite often as well often you
don't see them as much as I might think
yeah right right ok so the okay so this
is just going to take pairs of regions
in the image and give some predictions
to whether it thinks they should be
supporting 11 supporting the other or
not the other local term is going to be
this structure classifier so this is
going to be using the sort of standard
recognition features color histogram
sift and so on and also be some
information perhaps about the relative
depth in the scene and and the height of
the region from the floor and this is
going to predict for each region which
are the four structure classes it is
okay and then we come on to the prior so
the prior is going to capture our
physical sort of you know understanding
of how scenes work so the first thing
we're going to do is to use those
structure classes to define somewhat
it's plausible so for these you can
think of as little sort of a judge
adjacency matrices or transition
matrices so in other words if you're if
you one region is the floor it can
support furniture props and structures
but if but the floor cannot be itself
supported by anything else okay this is
why this is black here so white means
it's allowed in practice of course I'm
doing this showing this as a binary
matrix and practice of course what we
really do is impose a sort of penalty a
large you know penalty to the cost if
you're in the black in a black square
and then you can see this is for the
sort of vertical support relationships
if you have the ones from behind then
essentially a prop can be supported by
furniture another prop or a structure
but that's the only sort of horizontal
support relationship that's allowed okay
so the second pride that we use is one a
pretty simple one we simply say they
look if two regions with one region
supporting another it probably going to
be quite close by it physically so if
we're talking about a horizontal
relationship so our horizontal type of
support then were going to be measuring
distance in the XY plane if it's a
vertical type of support we're going to
measuring distance vertically okay so of
course the you know there are some
examples where that one object is being
supported by something a long way away
in the image but this this term would
prefer that essentially the the
supporting object is close by the
supported object we also have a term
here which which is related to the fact
that ground has a sort of special status
so the ground doesn't itself doesn't
require any support but it can own that
can only be true if the region in
question is actually labeled as by the
structure prediction as being off of the
floor floor class okay says you know you
can't be a prop you know prop must have
support but the but if it's labeled as
ground it's okay not to have any other
region supporting it and then the final
upright term in the prior is essentially
a sort of global consistency term for
the ground that is that the all the
regions that we think might be ground
they must have the same vertical height
okay so we've done that initial
alignment phase correctly we can then do
some things like for example this book
that's lying on the ground you can it's
got a slight different height to the
right to the surrounding region so we
know it can't be part of the ground so
therefore it gets must be a prop instead
okay so this we take this big energy and
essentially formatted as an integer
program so we have its actual tends to
be pretty big integer programs we have
to sort of come up with variables for
you know all the different regions in
the image and then coding these
interactions between the regions makes
it also a bit bigger as well but and
then so to solve it we at relax if into
a linear program and just throw it into
my labs Lynn Prague okay so that's the
overall approach so let's look at some
mix
moments now so well in practice I think
so that yeah in practice there are sort
of very large constants in there just
you know it seemed interesting question
yeah I mean it's truly I'm gone there's
certainly might be cases in the results
where things are physically implausible
but i'll show you them in a moment most
of them tend to come out you know there
is some rationale behind the prediction
it provides it there aren't don't sit be
too many physically implausible things
happening
I think it's big it's because a lot of
these these see so I think a lot of it
has become because you actually want to
have a single assignment for each region
so you really want to and they're also
exclusion relationships as well so that
once one region becomes the floor the
others nothing else can be the floor so
those kinds of things would be
essentially these of high-order clicks
in the MRF which might be hard to code
inside that although in italy i'm there
are no some fancy MMRF techniques from
push meeting others that might be able
to do such things ok so now in terms of
the experiments we need to have some
bass lines here so the first thing to
say is that the first approach is that
we essentially use our full classify to
determine the floor and then we do the
dump do it
that's not experience okay so first of
all so in these baselines will have bass
lines which consist purely of local
reasoning and then we'll show that they
don't work quite as well as if you bring
in the sort of more global reasoning for
these scenes okay so so for example the
one this the first base line you can do
the dumb thing which is to just you know
look down from above and see you know if
we what's underneath the region and that
would be the supporting region okay so
that's sort of a very local prediction
there and then if you also look in
horizontal support you'd look for this
red region being to be enclosed on you
know as much as possible by the region
behind it if that's the case then you
say okay that red region supported by
the orange region okay so that's just
using somehow the information we get
from the connections simplest possible
fashion the second base line also brings
in this local structure classifier so
now we have information we run under
structure classify and if we predict and
structure or furniture then you just
assume it's automatically supported by
the closest floor region okay and then
the props that is the smaller objects in
the image you're going to assume we use
the previous idea just looking from
above to see what the object is directly
beneath it so again so both so this one
essentially has all the information from
the first two terms in our costs but
doesn't have that the prior terms which
encode the sort of the more global
constraints about what as bought as
possible and then the final baseline is
essentially just to use local
information from the support classifier
so that's the one that took you know
pairs of regions made a prediction about
whether they will should be one support
should be supporting the other one art
okay so in this little bar chart well
first of all going to start off using
the ground truth regions that the
Turkish provided okay so this is not
using the segmentation pipe i know i
described this is the sort of gold
standard this is the best we could
possibly achieve in our support
reasoning so the first three sets of
bars here are the baselines and the
last one is our approach here so what
you can see is that the blue bars that
way you don't care about the support
type that is you've correctly identified
that the chair is being supported by the
ground but you couldn't didn't figure
out you don't care whether it was a
vertical support a shin or a horizontal
support relation the red one is when you
say okay now I do care whether it's a
horizontal vertical type of support so
you can see when you care about the type
of support the performance does drop the
this one here is the one which just uses
the sort of local information in our
cost function and doesn't use the sort
of the priors so you can see it does
pretty well here if you don't care about
the type of support but if you do care
about it the performance drops but once
you bring in the sort of the prize which
will do this more long-range reasoning
then you do see a sort of reasonably
significant jump in performance there
now of course this was using the sort of
gold sandal regions what we'd like to do
is to in fact use the region's we
produce automatically so this is the
performance this is what the bar charts
look like so that all the numbers drop
if you look at the y axis here the
numbers drop a fair amount but that the
overall trends remain the same again
you'll see this interesting thing where
if you don't care about whether it's a
horizontal vertical support you predict
you do almost as well as overall scheme
but if you do want to correctly
understand whether it was supported from
below or from from behind the
performance does drop without this sort
of the prior information in our model
okay so this is just showing you the
side by side and the performance of our
overall approach on the ground chief
segments and then the segment's produce
automatically so just to finish up i'll
just show you some examples of these
diagrams a little bit complicated so has
earn the image from the Kinect of it in
this case I'm office scene on the right
the colors show the different outputs of
the structure class classifier so it
correctly figured out that the floor was
this thing in pink and the debt you know
the major piece of furniture in purple
this monitor was incorrectly thought of
as a piece of furniture which is why
it's in this funny hashed thing
indicating incorrect structural class
prediction and then the arrows indicate
the predicted support relationships so
if it's red we've got it wrong if it's
green we go right and then the circular
Terminator shows the horizontal support
that is supported from
behind and the green one shows support
from below so for example we got this
lamp wrong so here's the lamp you can't
see the base of it it thought it was
attached to the wall okay that's
incorrect then you can take something
like this this fridge or something I
think it's a fridge sitting over here or
a cabinet was not some type this was
correctly reasoned that was supported by
the floor over here so that there's this
this whole segment was essentially the
chair so there was a tiny piece of floor
underneath but it didn't use that when
he was making that prediction so you can
see that there's quite and then you can
see also for example this thing sitting
on top of the microwave it understood
that the microwave was doing the
supporting there but then it got
confused and thought the microwave was
somehow attached to the wall which is
this is why this is in red has another
scene and sort of desk so you can it
understands that you've got objects on
desks which then in the desk is then
supported by the floor and the you can
see that thinks this cook this lamp
which is you can only see the sort of
head of the lamp it can't see anything
underneath it so therefore it thought
that was supported by the window which
is not unreasonable given that it's you
know the rest of the lamp is completely
occluded so okay so this is using the
ground truth regions of course things
are going to look nicer now it's swapped
the automatically generated regions so
here we can see a bathroom scene a
little alcove here in this case it
thinks that these little shampoo bottles
are supported by the wall it does figure
out though that these ones are supported
by the bathtub in this case there's no
ground plane visible so there's the
technique would have been essentially
using that hidden ground region to do
the reasoning and then this is a no
kitchen scene so you can see here it's
getting most of the things right it
thinks the sink is supported directly by
the floor which I guess it's a bit of a
debate exactly what is supporting this
think I suppose technically it's the
countertop but you can see here it's
getting most of the the cloud support
class classifiers broadly understanding
that these are sort of small objects
sitting on some large piece of furniture
and most you know it is get this plate
you know it's reasonable to assume that
is supported by the countertop in fact
it's sitting in the dish rack which of
course is a fairly small and you know
non visible object so it's not
unreasonable that it's got and that one
wrong yeah yeah that's a good point i
mean in this case we just I mean we want
to keep things simple that we just had
this sort of binary thing yeah yeah yeah
it's a tricky yes it's a good question
i'm not quite sure how to do that I mean
I mean the same thing arises in any kind
of hard categorization problem I mean
you just you have to quantize to some
sort of label set
and classify
yes I think it would go crazy so I think
it was a very strong constraint that you
only had one region being unsupported
that ie the floor and that this
constraint that some every other region
must be supported by another one it was
pretty critical yeah I think yeah yeah I
think they tended try this actually and
it's all chaos ensued so that I just
wrap up because Derek's sharing things
ending okay so what we talked about here
is essentially to support reasoning for
indoor scenes they're really difficult
because of all the seclusion going on
and they're genuinely you know
complicated interactions between
different objects we had in this work we
haven't closed the loop somehow and used
our support reasoning to go back and try
and improve identification of the
objects we'd like to try and do that
something we're working on and encourage
oh but you know if you're interested in
our data set its freely available online
and you know it's hopefully do let us
know if there any problems with it but
would be very willing to to fix it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>