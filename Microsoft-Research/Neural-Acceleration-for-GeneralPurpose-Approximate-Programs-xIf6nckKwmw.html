<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Acceleration for General-Purpose Approximate Programs | Coder Coacher - Coaching Coders</title><meta content="Neural Acceleration for General-Purpose Approximate Programs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Neural Acceleration for General-Purpose Approximate Programs</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xIf6nckKwmw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning I'm Doug burger I'll pass
on the opportunity to say anything
inappropriate given there's only a few
people in the room but there may be
people watching online so it's my
pleasure today to introduce hadi as
miles at it how was my pronunciation of
this quickly okay thank you I try hard
hadi is a PhD student at the University
of Washington who started with me at the
University of Texas moved to Washington
is now working with myself and Louise
says I and several of the other graduate
students in Louise's group he has had a
amazing track record of of output and
top quality research over the past
couple years with several award papers I
mentioned in the front page of the New
York Times on your work and he's now has
an incredibly interesting result in a
new area that we believe is going to be
very important and its new it could be
profoundly important it could be
completely irrelevant we just don't know
but it's very exciting so thank you for
spending your summer here with us doing
your internship he's got done and I'm
really looking forward to seeing the
results of all your hard work ok good
morning today I'm going to talk about
the new class of accelerators that use
machine learning to accelerate to
approximate general-purpose programs
this is a project which is done in
collaboration with between university of
washington and microsoft research and
the university of texas at austin i have
collaborators from these three
institutions parts of this talk is going
to be appear in the next one
international symposium on
microarchitectures so people have been
using accelerators and there are
different kinds of accelerators like
GPUs and fpgas that each of them take
advantage of certain characteristics in
the program to accelerate the programs
like fpgas use can accelerate
applications that have abundance of
parallelism but they are compute bond
and GPUs are good for applications with
a lot of parallelism but but they can't
you know work well with the applications
that are have very divergent control
flow and there has been recent you know
a proposals in the literature for
different accelerators that argument
processor and try to let's say
synthesized parts of the program on on
hardware or user configurable fabric to
actually accelerate the programs but we
are looking here is that you know we
want to take advantage of the dis
characteristics in many applications in
divers domains that the full precision
is not always required you either there
is no certain one ansible for the
application or a little bit of error at
the output the best example is graphics
and we we see it like in JPEG
compression is tolerable terrible and
the perception of the you know the human
plays a role in this there in this
process so sure why has your belief that
the opportunities for approximate
computing and applications have
increased
I think because like machine learning
and is taking over and we are facing the
era of big data the V we need to do a
lot of you know computation with the
vast amount of data there are drawn from
Big Data really search is one of them or
with you a clue that look at data mining
don't I got in your paper right but so I
guess has dude i guess the root of my
question is has there been some trend
that is making approximate computing
more possible or is it the case that we
could always have done it for for
example hpc were close to numerical
simulations you know with you know
converging algorithms gradient set type
approaches and and we've only just
realized this now or a backs against the
wall I mean is has it always been there
or has it grown because of the emergence
of some of these areas so there is
another trend that is emerging maybe
it's because of the challenges that we
face in the silicon the applications
right that's like you know that's the
energy efficiency application we can do
and drive them that way but that's not
for driving a lot of these big data is a
buzzword of you yes it's important but
right so do you think something has
changed in the application domain are
there are a few obviously identifiable
trends g'morning is clearly one machine
learning is what one of the things and
then you know so another trend that we
can maybe look at is the you know doing
computation with limited amount of
energy and then trading off compute
power sorry trading off battery or
harvested energy for the quality of
results that they're really trying a lot
of these rating
occasion today
vision vision is emerging despite energy
limits not because of it
okay let's let's put that on the queue
so sure so isn't it that that
algorithmically things like approximate
algorithms are are gaining in in their
acceptance and induce of these things
that might be the cake the family case
because a lot of you know let's say
recognition or you know even gaming they
don't o'clock an algorithmic level they
don't require like perfect you know used
to help reduce overall computation time
that's Jarius properties about that's
convergence or things like that yeah but
I'm not sure if that a serious question
I just wonder if it's that you know if
you if you're summing up your checkbook
you want it to be precise but whenever
you're interacting with the real world
the real world doesn't have a precise
digital representation and so and so
there's necessarily going to be fuzzy
estas we do more things that interact
with the world vision and dark method
reality front extract trends from the
web they're all in some sense
approximate because they don't have a
precise answer
you can also go and say that there's an
acceptance of doing that so 10 years ago
we were working on compression of NASA
images and the answer was yeah you could
do it it doesn't hurt the evidence but
they wouldn't accept it because who it's
not exact and therefore each wall and I
don't know if that's the facility casing
quarter older now their business gotten
worse
okay so what we are doing is that if we
look at this design a space of
processors we always trade off power and
performance and move over different
points in this design of space each
point in this design is based a
processor and the line shows the trick
pretty to front you what we are doing is
that we are exploring a new dimension
which is approximation and what we want
to do is that we want to compromise a
little bit of accuracy and hopefully get
better performance and lower power by
compromising that the you know the
amount of a cures and for this we are
proposing a new class of accelerators
that need to be trained rather than be
program so what I mean by trainable
accelerator is that if you look at an
imperative code you pick a target region
that is hot and it's like the
application spends a lot of time on it
then put a learning algorithm besides it
running the application the learning
algorithm will observe the inputs and
the outputs from that target code and
after it reaches a certain level of
accuracy then you replace the target
region with a recall to that machine
learning albert so we call that
algorithmic transformation a parrot
transformation which mimics the target
region of course that allows us to run
the program on the core on the processor
while accelerating the machine learning
algorithm on a dedicated hardware since
this is replacing the at target region
we are indirectly accelerating the whole
application by doing this transformation
sure learn the
that you shouldn't apply this and drop
back to the old code for those
situations and so we have safety
criteria like let's say you are doing
jpg right so when you are writing the
header of the jpg right it should be
precise because if you even like mess up
one bit the image is not unreadable but
then you are processing the pixels and
the bits then you can do approximately
approximation in that code so what we
require from the programmer is to
annotate regions of code that are
approximate without being conscious
about which technique of approximation
we are going to use just tell me which
regions are safe to approximate and have
a little bit of in accuracy and error at
the output so using your JPEG example
you have a notion of a quality factor
jpeg compression I can set a quality
background the compression or you so
small in your approximation that it's a
it's just a little bit of noise around
this coarser grain that's an interesting
point but right now I don't have it for
this technique we have other techniques
to use you know different approximation
methods then we can actually tune up the
approximation but now afford this so
there's there's a whole body of work on
synthesis where you know pearly my
examples become kind of invoke right and
I'm really struck by your hunger your
title here accelerators be trained and
not program you can almost using some of
the ideas from Smith olaniyan
and roz book on synthesis here you could
think about thinking about programming
in general for a large bought a large
class of programs as being trained not
programmed right you specify very simple
poor quality algorithms and then train
whole bodies of work and it's not really
to sail about accelerators are
accelerating out here is just as I'm
sure you sure so I think the generality
here is great that's actually very
interesting so how do you sorry guys
this it's really interesting happen so i
think when we get a little bit deeper we
do have a non cpu accelerator but how do
you tie program synthesis to this
training that you can use to make things
more efficient yet we're doing Ponte
doing this algorithmic transformation is
there is there a tie there or they just
intellectually interesting no I think
there is a time yeah genesis as train
right so so the program about example
that's examples much like you have a
learning problem here yep and in the
same vein those examples are used by the
synthesis engine to come up with a huge
 you can think about synthesizing
a large number of possible solutions and
picking the one that balances
approximation for pi au pairs of power
icic so it's more like week we can think
about the ones that are most amenable to
this transformation that's right okay
for general purpose code right right
are you remember that yes ok Kenya
actually hit me like it links and as you
know papers I my realtor it's good I've
been we're actually working on a similar
problem for synthesis in this example oh
right that's right your talks already a
success Soho so to do the pirate
transformation so they said this has
been an internal debate between me and
dog you about the naming of the pirate
transformation so if you have better so
you see and I'm willing to like here I
like the pirates he doesn't like the
fire card out delicious the empty chair
so to realize the power transformation
we actually duh you know we need to come
up with the learning algorithm that can
learn regions of imperative code and
then we have to come up with the
programming model the lets the
programmer you know you know think about
this transformation and annotate the
code for us and then we need to come up
with a compilation workflow that
actually carries out this transformation
and then we need to think about the
implementation of that learning
algorithm as i'm going to show you and I
think I gave it a you know out with the
title is it we we improve Lee found out
that neural networks can actually mimic
parts of the imperative code and the
good thing about neural networks is that
we know how to implement them very
efficiently in hardware and they are
inherently parallel so they're good
target for the acceleration at the end
and I'm going to talk about different
opportunities they're using your
networks will you know provide us yeah
but but in general I think potentially
any regression mechanism can be used for
a pirate transformation I don't know if
it's going to be the
fish halt because I haven't tried them
but I think potentially you can regress
any region of code but you know let's
support vector machines or largest egg
regression or any any kind of these
transformation to do this so for first
let's talk about the programming model
so the developer should be involved
because of the safety as I thought as I
told you guys before it's important
because programmer has the only view
which part of the algorithm is safe to
approximate so the programming needs to
annotate the source code but which which
kind of source code is good for pirate
transformation so aside from the being
you know approximate that needs to be
hot code because we want to you know we
don't we are bound with Amdahl's law and
since we are limited by the topology of
the neural network when we are doing the
parrot transformation the region needs
to have fixed size inputs and outputs
that I can as a compiler identify at the
static clear compilation so let's look
at an example so here I'm showing this
whole you know edge detection algorithm
so what what you do is that you take an
image you convert it to a grayscale and
then you slide the window over it and
you do the convolution operation the
convolution actually a proper you know
estimates the gradient for one pixel
with respect to its surrounding other a
pixel so it's inherently I approximate
so the Prairie army is going to do an
annotation that this is a proximal the
programmer is not a varied that I'm
going to use neural network just this is
a proximal and this is a good code
because it has fixed number of input and
just one output so what I'm going to do
I'm going to do the pirate
transformation in three steps the first
part is that i'm going to take their
annotated source code put probes on the
inputs and outputs of the candidate
function run it with a certain number of
input data collect the training data
which are the inputs and outputs to that
candidate rich then I will feed that
training data to pirate generator which
is going to explore the topology space
of neural networks and we do a search
space we have a social space of neural
networks and v2 we try to find in your
network and then that para tutor is
going to give us a topology and then a
number of bits and then we take that
annotator source for the game and take
the neural network and replace the calls
to that candidate function with the
calls to the neural network and if we
have a special hardware right we are
just invoking a special hardware behind
the scene instead of running the
original function so in our example our
pirate generator has found an 928 21
these are the neurons these are the
inputs and that's the output neuron that
can mimic the you know that reach and
sorry so what happens here is the
instead of calling the original function
I'm sending the inputs to the neural
network and receiving the output instead
of so if we run this on an image the
neural network is trained on a different
image and this is the running yet an
original code this is running it on a on
a pirate transport quote there a little
bit of difference I don't know if you
see here but I think perceptually this
passes sure
I realized that you're looking at
applications where there is this fan of
perceptual aspect to it but because
you're just doing transformations right
in these ops is there any way to reason
about the error bounds yes so for the
new your left words i'm going to show
you like different applications and how
we use in the black dahlia so this is a
good example for showing the to the
audience but you can reason about the
error of the neural network it's a
standard is MSC minimum square there and
then for the application level dependent
depending on the application you can
define an error metric like an average
air or root mean squared error at the
application output level and see what so
that gives you a mathematical framework
to reason about there in the application
do a Maps error you could do you could
you can define so it's your play the
other developer you can define any error
metric that suits you right you can do a
maximum where and if it doesn't do
maximum error over the training set not
over any possible input set right over
soon so when we are doing this we have
an input for training we have an unseen
input a for test and evaluation so
you've defined the error metric for that
test and evaluation metric you know for
that test and evaluation data but you
could if your data set never passes a
negative number 2 square root you will
never know that the code needs to see
that and flag an error run either the
sides but later on it's a desert right
right so I can't so like similar to any
applications that use learning I can you
know bound the air and say that I will
mathematically guarantee that the area
is going to be less than this I can what
i can do is that I can say that that's
going to happen infrequently and
hopefully since your application is
approximate that the final output is it
still going to be okay
but they're like we can do one more step
right we can put the mpu and a predictor
right which predicts if this error is
going to be too large or the input is
unseen right and then decide if I should
retaliate the original coding instead of
running BNP so that's one of the
approaches that we are thinking you want
I have an implement throw an exception
right exactly and use the rid of the
original sin exactly exactly that's
actually done it like there is the work
called the relax which does our
approximation for different regions of
code and they throw an exception and
retaliate back if there is like more
than a certain level so another
application is inverse kinematics so you
have a two joint are and you have the
X's and Y's and you want to figure out
the angles of the joints if I can play
this that would be awesome okay the
circle is the original application the
original quote but the arm is moving
with the pirate transform so we applied
this to six different applications what
for FFT from signal processing universe
kinematics robotics I get into three
joint arm because T joint arm hasn't
doesn't have a like a closed form
solution and the speed ups are huge
because you can just learn where the arm
goes be there for a part of the J monkey
game engine which you figure out if two
triangles in the 3d are intersecting
gonna lead it for parts of JPEG we used
it for k-means and the Sobel filter for
King ins to make it easier to understand
the errors are used k-means for him
image segmentation and these are the
neural network ms ears
and these are the application level
earth the VC for FFP air right so for FF
D inverse kinematics i use the relative
error as the metric for jay jay i mean
to either you're hitting or missing if
do triangles are intersecting or not so
this is the Miss rate and 4.jpg k-means
and Sobel i'm using image difference
between the original application
original code and the parrot transformed
coin at the upper ter pixel difference
average penis where the pixel
differences on the other number jpg is
here usually like cluster to a few
really bad ones or is it just kind of us
it's a small theater is smoother so we
can do pará transformation but what
opportunity is there for then be are
doing the yin sure if i were to do a
normal jpeg compression to an image do
you have any idea how that relates to
your error metric oh i haven't measured
at but this image is worse than normal
JP so the policy or what is it is it
something you can visualize you can see
the difference or they can visualize the
image difference and see see how that
can you post process to computer quality
factor jpg
the final yes obsessed images yeah use
their metric yeah actually they like
this the average rmse a difference is
the image is an image difference that
you know image processing people use
that's the reason I used it and this is
between like you know after you do the
JPEG compression and then you do the
pirate transfer here you see like an
average of ten percent air so so now
that we have the S pirate transformation
we have we can have different
implementation of that neural network we
can have it as a library it can just
call a library of neural network or i
can use digital mpu Andy this should be
analog sorry I fixed there the big drop
but I didn't update yeah so this should
be a lot that's what so so that's why
like I came up with this idea because i
wanted to do analog computation while
having digital normal programming
interfaces but use analog circuitry and
there is a whole body of research that
shows we can do analog implementation of
neural network so we can use analog and
i'm going to talk part so so let's do
the software first and see what happens
when we do this offer so if i use a
library this is i'm using this is
application slow down and this is the
you know different applications and i'm
using fast fan fast artificial neural
network is a widely used open source
library c c c++ implementation you see
that this is kind of you know this the
CPOE implementation is not working but i
did these sure
this is if you do your technique how
much the coat slows down yes if I just
use a library of neural networks without
any hardware supply to go this is how
not to do about it yet okay good Nick
but you liek a lot more'n so you consume
a lot more energy because you're running
so slowly and so it's a huge opportunity
for speed ups and improvements
change straw man like our consumption
branch predictors I'm trying to make a
case for two things right one is that
how can we change the process so that we
can you know we can actually gain
benefit from this without adding any
extra hardware to the processor without
adding yeah you know actually actual the
big accelerator to the processor but we
flip back one please sure does the
difference in the heights relate to the
size of the input or the computation
size or what what what's the difference
between 5 and 75 typically so it's that
it depends on how much of the
computation goes into that region of
code right so in the FFT we are spending
like you know thirty percent of the
computation in that region so I'm
slowing down that that's region and then
I'm nas law translates to this 4.5 in
like jme int that region is most of the
application and the implementation
actually is a pic based on a paper that
you know is a very efficient
implementation and I'm using a large
neural network because it's a very
complicated control flowing intensive
region to emulate the to approximate
that region so I'm trading off a little
bit of computation to a large but
jiggler a computation so I see a huge
soda does that make sense yeah I guess I
was expecting a statement along the
lines of if as the number of
instructions imma put replacing goes up
my penalty goes down because the
a NN is sort of fixed time so you want
to increase the amount of code you cover
exactly that's one of the things but
there is another another side to it is
that how much computation you are
replacing that region right as you
enlarge the region then the neural
networks potentially can grow and you
may not end up with again again there so
the example you gave where you have the
approximate annotation on the code
didn't hell do I don't think it was just
straight floating-point computation if I
have a loop then I could potentially
have a program that doesn't terminate
and you're so I'm trying to get to the
point of when does this breakdown and
when when can you not learn a particular
function have you guys tried to classify
effectively you can reduce this down to
the halting problem Isis you can drive
because now you have a narrow network
that's going to try and Procter &amp;amp; Paul's
right so when does this breakdown and
when can you but but but for some
programs i can actually deal with loose
rocks right 4.jpg there is a loop inside
this goes over the pixels of like 64 you
don't like implement it in a linear but
it's not a loop there you know goes
around for for everything so the region
of code should not you know changed any
states besides its output so then your
network that I am replacing that region
of code right takes a bunch of inputs
and generates a bunch of output right so
and if that code if that code is
changing something besides the output
either I have to hoist it and do a bunch
of computation to come up with it so
that this region of code is pure or I
can't do one simple as much simple
answer so you
slimy if this is just way off base but I
think you can move to the cider but I
think I mean you can you could row the
bounds that you were considering in the
nerve the neural network for until you
found a program region that was well
structured with is in foot out the
behavior so there might be lots of
internal state communication book that
you don't you lied that and in your
example you know you could have a you
know you have some inputs which is the
program and then an output which is you
know will this program hall right and
you can try to train the neural network
to do that and that has well-structured
inputs and outputs but you're not going
to be able to train a network to do it
so I think it's to your point about
quality product I think it's really just
that some functions are amenable to
neural networks and some are not and the
less they're amenable the more error
you're going to get and with the halting
problem you're just going to get peer
entropy you're going to get a random
result and and you basically have zero
signal level because you can't train a
network to do it and so it's really just
a you know there's a spectrum of how
much error you're going to have and for
some things the air will be incident one
usually it's random point five you can
have like random like in your netbook if
that's a lucky classification problem
you can't have the random right so it is
that is that right do you think yeah I
think that is really I think the idea is
that as you if you have any input and
inputs that your training on you're
going to get some answer by the network
and you use n plus 1 that the entropy
say it's the same or the answer stayed
in the amount of error is the same it
doesn't go down right right as you add
that training
and if you keep adding and adding and
adding you're never going to get to the
position where you actually converge
that's right that's it's really just a
question of how horrible is the function
and the diminished dimensionality of the
space of the data like the impact that
if you grow the dimensionality then
training is going to be your partner and
partner so I have another question I am
eating it too but it kind of comes back
to the algorithmic aspect that if these
applications know that they can tolerate
a certain approximation and their
results where would changing the the
software implication not using your
library to learn and train a neural
network but changing the implementation
of the code to tolerate that much less
quality and their resolve sit right I
mean I'm trying to get at that the
change that you get from invoking the
neural network with some special
hardware or whatever right versus
algorithmically doing less work and
getting a less accurate so i can give
you one example a sphere so admit i was
doing jpg i downloaded the code of jpeg
and i was using and when I did the
pirate transformation it was getting
linked like hundred x speed up the
reason was that the DCT part of the JPEG
was it like a cosine exact cosine
transformation and that's very very slow
then I changed the implementation I
found another implementation that D city
was approximation of the exact and my
speed up was around like seventy percent
so even at algorithmic level you can do
approximation but there is a limit and
then this goes beyond that the other
thing is a vot another paper call we
propose another architecture which is
changing the processor itself and
supporting approximation instruction
that's one of the ways that you can deal
with approximation the gains that we
were saying is that no speed up yeah
like twenty percent to forty percent
energy reduction so the we were we were
going after like twenty percent energy
action and here we are saying you look
around like 2x speed 0 3 X energy
reduction when we are doing this at but
one of my colleagues Adrian you know
Adrian so he's working on compiler
optimization unsafe compiler
optimizations there you can apply to
regions there are approximate and
probably execute less code and see how
much you can get get away from that I
don't I don't have a head-to-head
comparison to that technique so so
during my internship so this is what I
did before so what started with
generated code instead of using your
library I can actually get the neural
network and generate the code it
switches efficient then let's assume
that one of the things that i'm going to
show you the results and you you're
going to see one of the things that
causes this is slow down is the sigmoid
is a sigmoid function which takes a lot
a large part and then like I finished it
like two days ago I did a VX
cogeneration with intel Ivy Bridge AVX
extension to see if we can use
vectorization and ging bit the results
so before I show you there is also i
want to talk about the ABX code
generation so we can do vectorization in
two different models one is that each
neuron is something multiplying you're
selling off so I can use the parallelism
inside in your on and do the victor
ization like this different input
getting multiplied or i can do across
neurons and just one input for different
neurons in each vector I took this
because I think this is a better it
gives a better victor ization a freshman
at the end I have to do a ladder of
additions to get the final is also i can
do the others for a dish
four different newer at the same time I
implemented this one let's look at this
result so before I show you the slowdown
a byproduct of this doing this work is
actually let's say we want to do Darrell
network execution regardless of pirate
transformation how much game we can gain
with this technique so there the this is
speed up over there fan library when you
are doing neural networking book in
vacations so this is the generated code
you get around 1.5 x speed up fifty
percent and then you add the sigmoid
instruction you see a huge bump and the
geometric mean the speed-up goes up
around seven point three x and then when
you do the vectorization with Victor
support for Sigma D listen you get an
order of magnitude the speed up for New
Relic secure loads these are for single
precision and the one thing that I found
surprising is that AVX sometimes is also
in slow dance for small networks but
this is the biggest network death I had
1832 82 and I see a huge bump here alone
you know 13 x speed up over in a
hardware sigmoid code generated so this
is the application slow down when we
apply these techniques after i did so
after I did the code generation and gain
something then after I added the sigmoid
instruction to the processor actually
two of the application series speed up
even bet without any hardware support
for neural execution just the Sigma
instruction and the factorization takes
the slow down from 15.7 22 so this kind
of makes the case for using a dedicated
hardware to do the bureau networking
cution so I'm going to talk about that
they just all hardware implementation so
for this we needed a configurable
hardware that can realize different
neural networks different regions of
code require different neural network so
the hardware implementation it needs to
be the configurable then we needed and
micro architectural interface between
the accelerator and the processor and we
needed is a extensions that can
communicate with the IMP you the proud
lets the processor to communicate with
the MP and at the end i thought that i
was talking you what this is like so at
the end since we are doing very
fine-grained acceleration this
integration of the the hardware neural
network should not hinder a speculative
execution or out of order execution in
the press so so we design a
reconfigurable reconfigurable mpu
digital mpu each neuron essentially is a
multiplied ad unit with weight cash and
just crunches through the multiply and
add and then with a sigmoid hardware
sigmoid unit and then these are the 35
folders that are exposed to the process
so the processor sends inputs and out to
this 50 reads it from that file or
configures and sends Devon you know
weights to the where to the NP so I used
more 686 cycle accurate simulation it's
configured very closely to intel core
architecture and an AP via NP you and it
compiled the applications with dash 0 3
so that i don't buy us the results where
that's fine so these aren't application
speed us the the dark part shows the
actual speeder with the impudence showed
you the light bar shows the idealist
video that i would have get it i would
have gotten if I had a Z
odelay mpu so achieve 2.3 here and one
of the applications actually see even
with hardware slow down because that
region of the code for cami's is the
euclidean distance calculation it's a
very fine region it spends like thirty
percent of the computation and in there
so they're actually implementation even
though the network is a small the actual
code is very efficient so Adam finger on
3x slow down sorry energy savings here
for the application but I would have
gotten 3.9 X if I had a zero energy NP
so the question is that can be moved
this further can we push it further this
is actually analog in più so what part
of my internship I study the feasibility
of moving toward analog implementation
of neural networks so to do that so as I
said the ANP you needs to be
reconfigurable so what we are going to
do here is that you are going to do the
computation in analog and the storage
and communication between the units of
analog computation in digital so we call
each of the computational units that
carry out the computations of a neuron
an analog p processing inch so so we
have to so we can have an array of AP
eseye like peas and then we have to
figure out how to map in your network to
it so one is to time multiplex this a
time multiplex neurons over the aps and
that happens that we're going to do the
computation that one two and three first
and then use the same PE to do the 45
the other approach is to have a two
dimensional area of APs and
geometrically mapped in your network to
this the good opportunity that the
geometric design is that the
communication between the APS can be
analog instead of converting it to
digital and then communicating it to to
the 2d version free male audience
you can do it but here you don't they
have to do it if you want to stay in the
analog domain then you need the multi
parts design you didn't want your metric
Design yes yeah yes exactly and there
are other factors these resource
utilization and fall to the earth to
second understanding as to why I would
want to use a logarithm the jeweler's so
the reason is that the analog you can do
addition but just having the point here
like you have multiple lawyers coming to
a one point and use Kirchhoff's law to
do the computer to do the audition so
you don't have to convert it to bits and
you know things like that and you can
actually you do the multiplication here
we are not doing actually multiplication
we are scaling input you can actually
use a you know resistor ladder pass a
current through it and use that
registers to scale that current and then
do that ish ensue you're just using
Kirchhoff's law to do the multiplication
and addition that's much you know much
more efficient than doing the digital
computation interested in the analog
circuits you can implement much more
efficient computational primitives you
can do integration addition just by just
by building a circuit that physically
mimics the
you just don't get digital precision
yeah sorry I have two wires with
currents on them and I tied them
together that's an ad it was pretty
efficient didn't hit a lot yeah probably
better than that it had a life precision
because arguably better in some
screaming let's take that one off line
it's not wrong so what we are going to
do is we are going to have the
communication between the neurons in
digital and but the computations were
with the inside the AP is in a line so
we have to decide how many inputs we're
going to feed to that analog in it like
perceptually like theoretically you can
have multiple as any wires coming there
to do the audition but alex is tend to
work in a certain small signal region so
you have this region of current that
everything is linear and you're getting
that you know addition effect but if you
blow out that region then the
nonlinearities in the analog circuit
will kick in and the you know screw your
your precision for that so one of the
thing is the computation bits how many
inputs we are feeding to that hollow log
P the other other thing is the number of
bits that you are going to choose to
represent any number a good single
precision you after you two-bit you have
a very large dynamic range but with the
analog circuit you are moving to our
fixed point our operations and as you
increase the number of bits the speed
the energy is gonna change drastically
I'm going to show you some assembles
before I actually show you the results
i'm going to show you a little bit of
the circuitry so what we do we we
convert the inputs the bits two currents
right you can have a search and another
search which is two times the other
search at four times a time and then you
have ones in each position then the
current that goes through this these
searches gets multiplied by a factor and
then you have a current value which is
representative that the the bits that
you had in the input and then you can do
the scaling order multiplication with
the register either and then do the if
you want to choose you want to subtract
your ad do the addition you have to
choose if you want to get the negative
current or the positive car and then you
so this is this unit does the
multiplication for eight inputs and then
you have the addition which is just
tying together the wires and getting it
and then you have the HUD conversion
which also applies the sigmoid that's
rally to it and you get the output so we
went with the time multiplexer anp you
design this is a conceptual design it's
not like realized the yet but you have
these AP years with 18 put these are
they input output five-fold and then you
you know do the communication digitally
between these the DS units our
methodology for the design of space
exploration of this is that we are going
to do cadence transistor level
simulations and then feed it to a
software simulator that realizes the
entire a and P you and the first thing
that we did is to how far we can push
the bit fits right we want to identify
ultimate we can use with it this is the
single precision and that's the air
right and this is the number of bits for
the input and this is the number of bits
for
a for the rates which goes through the
resistor addict you can see this like
behind these lines that 8-bit is enough
right ok let's to look at the energy
projections this is energy this is the
number of bits that you use in the AP
because of the design of this a
digital-to-analog conversion they said
that the the size of the searches are
increasing exponentially the energy is
going to go exponentially as you
increase the number of bits so if we
look at here this is 16-bit digital FB
this is the 16-bit digital FB two
different frequencies around eight bits
of input we see it at 10x energy
reduction this this has given the fact
that we are doing a 2 D and D to a
between the neurons and doing the energy
their communication in digital time if
we do the geometric the design this is
well beyond 100 x you know energy
efficiency with analog so for this I
have worked with Doug Louise and
professor has to be from UT Austin on
the analog part and I have also worked
with the Aegean from University of
Washington Renee from University of
Texas and we have a new guy in
University of Washington theory is
working on fpga implementation of this
so that we have a conceptual a design
that actually accelerates an arm core
which is on the fpga we got the board
and we are you know pushing that forward
we have a vet page for this project and
we're going to you know provide the
compilation for flows the tools that I
developed the full code generation of
neural networks I think that's important
because you know Google had this project
that they did a very large scale neural
network on their cluster
so the work that I did which was toward
using your networks for accelerating
general purpose cool but the by-product
of the code generation and using gevey
eggs and things like that can be a fish
you know beneficial for such projects
when they are doing you you know your
network or if Microsoft isn't interested
and this the the thing that I didn't
talk about is for the moisture
submission I was the compiler but a part
of the you know internship was developed
in there compliation foot portable and
doing it right now we are doing gift
pragma basu the user users pragmas and i
did it more flexible to elect the the
you know developer specify errors ranges
of inputs or different things that you
know can be used and during my
internship so i worked at the the camera
tease for these two papers as well so
that's all I've got this is fifth day of
creation that's kind of that where we
are and give you some advice about your
talk show because you're going to be
giving a variant of this when you go out
of the interview circuit ah you should
render this with the approximation then
and end on that and not tell them and
then and then flip to the original and
say here's the version that was done
digitally okay right and then that's and
then because because everyone every talk
you give it a major university people
are going to jump on you and say I don't
believe you can give up error how did
you give up the general precision bra
bra bra bra bra and they're probably
right so this this will just that little
trick will sort of kind of anticipate
that objection and head it off at the
passage say well as I've just shown you
you know there are cases where you can't
tell the difference I mean you do it
with the monkey right and you might
that this that would be you know it
would just show that you've anticipated
it and then you can do it with a little
smile and you know oh and that would be
a really nice way to not that bad i have
a question sure if you go back to your
the slides where you have the next one
this one right where you've got the
energy so so you could use this to
indicate how much the algorithm
algorithmic changes would have to write
how much you have to reduce the
algorithmic complexity in order to match
your energy savings right all right okay
that's
I'm a little bit because we haven't like
done this like implementation we were
planning to do it for iska this year the
the you know the analog implementation
so it was a little bit cautious it was
like talking too much about it right now
but you're
because that look for nibble jpg
Jinping's a good one because it's got
the ability to tune it right built into
the algorithm right to get the same ten
percent error that you had right if i
set the quality factor at that how long
use that as the thing that you normalize
to just show what your energy savings
remain so this is how rhythmic change is
actually occurring and you want to beat
that right some curious we thought about
following this up the examples that you
gave our very circle and the
approximation is pretty obvious right
and maybe this goes a little bit to
Doug's point of the very beginning of
the talk there's a lot of situations on
the phone for instance where everything
I sent my phone is effectively sensing
the world around me health time and all
of my programming models that currently
exist for that for that for that sensing
data are discreet and and work on facts
when the reality of the situation is
that those sensors are not giving me
factual input that tells me my
approximate location that's exactly
right and so I wonder if you thought
about how do you change I mean as a
programmer how do we top start to talk
about this error is great but I don't
think it's the right solution because
it's very problem dependent so how does
a programmer do we start talking about
dealing like putting this bubbling this
kind of approximation up to a programmer
of the level of you know the type of
people that are writing JavaScript right
how do we how do we allow people who are
writing these new very high-level api's
to reason about approximate approximate
computations so so this is a hard
question i don't mean many that i'm just
curious if you pull out the given any
thought to it can i add to the question
please talk so I want to make the
problem bigger okay so if you're if
you're building an app for mobile phone
and we're trying to do a lot of stuff in
our group around inference and in our
extraction of this I will semantic
signal
noisy sensors and things you do like
browse the web and all that level okay
so we have a really good understanding
of the third or appreciation for the
problem a quick will in this energy
limit in world in some sense what you
want to do in which we don't know how to
do either is give the programmer an
energy budget and say what's the best
answer I can get with this energy budget
and it's not that your thing is going to
add error your thing might allow them to
do algorithms and that fix energy budget
to give a much better result with lower
error but if it's if it's a digital
representation versus the trained
representation with error it's exactly
right you know you it's worse but the
answer is you'll be able to do much
better stuff with this because your
energy limited and you and so how to use
how do you say the programmer here's a
hundred joules and you want to figure
out whether the users at work or home
right now now this will actually look
probably let them do a better job but
it's not about error because the the
algorithm you would use if you had to do
it digitally would have more error as I
think that's what you're saying right at
the Thunder so in some sense you want to
give the the use of the programmer bag
of energy and say here you go wonder
what's but you know yeah now you got all
these different different choices you
can do and you kind of want to run it
through a tool flow and say you know
this this feature energy budget this
doesn't and maybe you'll maybe insert
the disc relation of your gps sampling
actually i haven't seen any work on this
and this would be really cool
interviewer versus factors in the giving
giving a programmer in in Visual Studio
you have an energy model a model of the
system and processor and you have an
energy budget aesthetic yeah I have a
static in any budget you know you've got
some you know
I provisioned million joules and you
have a template you a bunch of templates
sketches right and you compile the code
you'll do the analysis you'll run it
against the model you know and then this
the system can automatically adjust you
know discretization of the data number
of loops and and and scale the sketch
down to meet your budget and then you
can say X approach XYZ ABC which one
gives me the best results you know
that's what we're going to have to do in
the future yes exactly and if the
compilation welcome to the Cowboys
problem actually this this is a route
would be a really interesting project
sounds like you're turning normal
software and then PT a completion
and time including no no no no no no I
mean there's an element that that
proximate energy yeah yeah you got a
model yeah but you just got to expose
the model right right I mean you can
slice it two ways you can say here's you
know I can have some trace like user
profile like I can take these different
sketches and say here's the quality
answer you get and here's the amount of
energy that each of them consumes it
something like that you prof. which
instead of giving you the timing that
you you know spend in each function
gives you a lot of energy that your unit
yes this is prof energy and error yeah
it's like that problem yes hey so we've
started doing a piece of this customer
killing myself for the work we've been
instrumenting windows phone to provide
effective exactly what you talked about
as a healthy budget yep we haven't
gotten to the point where and then that
budget then is used to inform its
predictive on the idiot to predict what
the budget is on for today I think that
normally you turn on your power I'm
talking plug in your phone at six
o'clock at night yep and so that gives
me now look at the battery now I say
okay I've got an estimated amount of
power that I have to get through the day
and then you go sorry energy gets and
now it's a pep you Mike and then you
have to make decisions based on that
right but but we're nowhere near I think
the what you just described yeah yeah
they think you like last ecosystem
you know fitness take that portrayed
yeah this is this hey this is coming
that we have the rest of the people the
room and marcin yep
you did a great job today Adi good
talking kid</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>