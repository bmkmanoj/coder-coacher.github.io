<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From LSI to Probabilistic Topic Models: An introduction to Topic Models | Coder Coacher - Coaching Coders</title><meta content="From LSI to Probabilistic Topic Models: An introduction to Topic Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From LSI to Probabilistic Topic Models: An introduction to Topic Models</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x1WTwbEezwI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay
come everyone to the final talk today by
Professor ng Bhattacharya from computer
science and automation department right
here at IAC so yeah Chiranjeevi for
Chiru as he is known popularly and you
can also feel free to call him zero yeah
so yeah so as you to ask a lot of
questions and this side in the states to
tell you about LSI improbable stick top
models Thanks okay so I guess all of you
must be very tired a wonderful series of
workshops lectures going on so so today
we're going to talk about LSI two
problems topic models so this talk is in
two parts today we have some of them and
and tomorrow morning will be sound them
so so now okay can I see what I'm sorry
that this thing got goofed up so what is
this image this is the picture of my
table after the examinations I was
joking
this is the last text corpora right so
this is a something like scrape of the
Internet
so so so basically you see that a lot of
large corpora is available every day you
can look up Google to look at the
numbers so the thing is that I want to
understand I want to one would want to
comprehend what is in those corpora so
what what we are what the problem is we
do not have adequate set of truths if
some for example if I am correcting
answer sheets this is okay these are the
good students and these are the bad
students are very nice right the bad
students or students not sorry I
shouldn't say the bad students all
students are good but soon so a week you
know learn to give them some extra mark
you know so something like that I know
it's a bad example I am giving
so so basically there are not enough
tools out there which can help us
organize large corpora that is the
starting point of this discussion so
topic models have tries to fill in that
gap so so basically these they try to
discover what is known as hidden themes
we'll talk about what is themes as you
go along so this has been around not
after David lies paper in Lda which most
of you know I guess but it's around much
earlier than that this has been the
cornerstone of modern search engines so
we so as part of this lecture or this
talk we are going to emphasize on this
point so the key is that so suppose you
could discuss water whatever you want to
call themes and then using those themes
we could probably label those documents
and then use this labeling for whatever
things you want to do like searching you
know organizing etc so the best
advantage for these is that they do not
require supervision so now if you go to
any company a lot of companies have
search engines all that right so see I
got a lot of this text or whatever
documents and the biggest problem is who
will annotate them if I don't annotate
them I can't learn models from that I
can't do service learning so annotation
is costly now topic models tries to fill
in this gap yeah whatever it is it's not
perfect this is but this leg but this a
great area our excitement is going to
happen and it's happening so with this
nice introduction so let's say what are
talks
yeah so suppose you see this I want some
help here so you can help me out here
suppose I run inning hit season game I
see these five words and say these words
are offered in document what is the
document going to be cricket
ah Sakura so was this a document
separate in New York Times then words
answer baseball okay
saying you say it could be cricket
really baseball right but the idea is
this that if I see this group of words
occurring together it is giving me a
hint that somewhere is supposed to some
topic right so that is the idea so the
idea is that can I get those things
can't discover these things all right so
for example when we took New York Times
corpus and ran things right supposedly
this would be baseball or fit it could
have as well be if it's time 70 articles
it could have said its cricket so what
is this what is this recipe anything any
another answer go a recipe okay what is
this region what is this hospital or
health care and that is what it is right
so Easley so so basically these are the
things which we come up with names but
this all is trying to tell you something
right so now the topic models I you know
what they try to do is to capture these
things so now so this field went about a
revolution in a hundred revolution with
this very interesting paper by David lye
and co-authors it has changed the field
completely so this is an example of a
probabilistic model so so just to read
off this diagram it is it defines what
is known as a generative model of a
document and so what it tries to do is
it tries to generate words these so a
document will contain n copies of this
word the rain words there each word is
generated from a variable set and some
variable beta and this Z is internally
drawn from another variable theta we'll
hope we have to make this clear as we go
along
so and now we have M documents all right
so this is sometimes called plate models
so this is describing a probabilistic
process of generating a document just to
summarize how do you get a document you
suppose you know theta so if you know
theta you randomly sample Z from some
distribution which we will say later
whatever it is once you have a Z then
you give it word a document is
collection of M words okay so that is
going to be filed by this plate model
and now and there are M such documents
right so this is a sort of a graphical
model so it was a graphical model
discussed in this part of this workshop
holliford is tall person never mind so
the so that's what it is so this is an
example of a generative model so the
generative model it's a problem stick
model which is cries for me doctor so
now so these beta variables are what are
known as topics so they will give us
what we call as what we are calling as
topics so now in this model topic is
defined as a probability mass function
over words so what I am assuming is the
same vocabulary at an disposal so topic
is nothing but a probability mass
function over this vocabulary so the
goal of hopping models is find them all
right so so now so some motivating
examples so I mean there are out there
in the web that is very interesting and
browsing corpora so this so the what I
said before right so if you have large
collection of corporate one to make
sense of the corpora so what people have
done is they have taken a Wikipedia rent
topic models on that and reporting this
so this is a browser which is available
here so this is a browser for Wikipedia
so here they are listing down the topics
all right so for any one of the topics I
guess this one you know these are the
documents where this dog
this topic is present in a in a big way
alright and these are the list of
related topics if you will so then if I
want to search for topic like this now
you're welcome to make up your name for
it as we'd give a name right you know
you said recipe and cooking and all that
right you can make a name for it may be
household population female family media
and I don't know you want to understand
what I don't know how to give a name for
it I guess so there are some how new key
PD articles are talking about these kind
of things you know so a list like that
I'll list some things like that so some
amazing examples so it is one more
so from 1880 to 2002 so again our David
Bly and his co-authors just took all
articles in the journal Science brand
topic models on trouble so this also
available this code and all the results
are available so this is this is a
screenshot of that
so so they find topics like this and if
you go in 1960 you know so they find
topics like you know these these kind of
things so certain if you want to look at
this five words what does it mean some
marine biology areas so this would be I
don't know if ranked physics yeah I
don't know so so on this axis at the
various topics in 1960 right and on this
axis they are trying to study how the
topics have changed if they change at
all
pretty interesting piece right so so
this helps we understand in a large body
of work like science so you know if you
dig deep you know the first one we
talked about you know these are the
related you know articles so you see
here example of retrieval so basically
if you think like this if I key in
energy electron and all that out comes
this articles the basic premise of any
search engine
addresses an example of the attack so
what would what do they require no
annotation Justins topic models
automatically this is so so so that is
the excitement about these as I said
they also studied you know how this
topic has evolved over time and all that
right so so these are fascinating
examples now I myself was all this is
good to hear but personally we I myself
saw the power of this when we were doing
a research project about searching in
Indian languages so now in the languages
by enlarge are known as suffer from the
problem of resource
it's called resource cares languages why
are so scarce we mean for English the
lot of stammers this that available for
Bangla Kannada Hindi I don't know there
are very few things it's available and
if they if they're not there it is very
difficult to do automatic understanding
all these documents so now so let us say
I don't understand the documents very
well I just want to do retrieval that is
in India most of us can speak and read
at least three languages English Hindi
language minimum right but if you search
a query in Google or anything like that
they only give you an English one lot of
exciting things happens behind English
we are in English so you want to address
the space
don't say can you do something not that
we are successful but this is but this
is where we started
got into a project so so and so another
thing was that okay in many cases in
many cases so you won't understand topic
models you understand machine learning
you may not know the first step what to
query existing search engines require a
key word may be the key word correct
will give us good answers but suppose
you don't know what key word to give in
the first place so how do you go about
doing this so for that there is very
interesting idea which isn't proposed
long back in 1992
it's called scatter together so the idea
is this
you take a collection of documents you
scattered them into various clusters and
then summarize the clusters show it to
the user now the user decides ah okay I
don't need this cluster I want to this
cluster so usage and clicks on that
whatever cluster and they again you
click documents on that do the gathering
process and scatter that is the main
idea right so so basically existing
document collections are mainly
hierarchical in nature
that is fixed tree so it's not very
tuned to give very fast answers to this
kind of search mechanisms to address
this new scanner gather was invented so
it allows some dynamic cluster was
browsing whether what I mean is this
that the user can interact with the
search engine like this and give you
answers now there are some systems
available like character ep you know you
can some name to there so but so the
essence of this thing is that clustering
is there are many tools available but a
census can isomerize the clusters well
because that is what the user sees there
is the mini sense and that's the
challenging problem one has to solve in
process so know as a so for example if
you take this just to give an example
see logitech webcam right you just
search this so i do not know whether you
are looking for a driver or you're
looking for a software or you're looking
for our web camera i don't so it
summarizes like this on this side so so
now we we thought that okay now can we
do it in indian languages where see this
is this is what he called monolingual
one language now suppose i throw in that
that's the thing we had in mind that all
of us know at least three to three
languages so if i want to give a search
query on i do not know for example
robots one of my favorite authors so if
i search in google i will get i mean if
you search in English you're not going
to get the best of result
but also some interesting results or
will not so we know nothing you know I
want sinks in bangle also right so how
do you fill in this gap so but I am NOT
that educated Oh engine at all so to
formulate careful queries so can scatter
gather help what do you want to know you
know the entire called pure solvent or
not what is written what kind of
articles they have so to this end so so
can you see now what is the challenge of
solving clustering is easy but the
challenges the remodeling will document
some of the English some of them are
Hindi or Bangla in this case how do I
summarize them how they summarize them
and the problem is is useless case so I
don't have a good translator I do not
have things - for example in English you
know you can I mean seeing least to
French for example there a good
translator is available I do not know of
anything you need to bound I mean there
are some things but they are not that
great right I also don't have explicit
language models which I can use so no
translation models no language models
okay
now Google Translate is coming around so
that's a great job but one could use
maybe today but let's assume Google
trust does not exist in that case for
sure I do that's what I started right so
for example so for example here we see
that if I look at the documents
collection whatever that is I get words
like philosophy Jesus Plato occurrence
so there are some things you're talking
about right so Spanish island Spain
Gibraltar music Presley so these have
been so this data has been collected
from Wikipedia corpus which has three
things English Bangla and Hindi
so you go to Wikipedia you can we
manually annotated found that these are
collected because how they are connected
there we have links to one another for
now we assume that okay maybe they're
talking about the same thing that's why
they have a link we could find that
there is a overlap of something like
three thousand documents or of that
order so the challenge is how do you
fill in them so they want to know what
is going on I in Hindi I don't English I
don't know what's happening in Hindi
why should happen English always right
so now so you can see topic models
allowed us to fill this gap very nicely
you didn't look at anything it just
required a corpus of it it amazingly
filled this up okay we are doing extend
we have to use a substrate atomic model
than what we showed you so that's where
one of the great examples we saw that
how powerful this thing is and it has a
gradient
implications for resource quest
languages so this this one more problem
I want to talk about which which we
again saw there really the power of
topic models so this was so so we we had
this industrial problem for a company
came I told us look here is a corpus you
only told the following this corpus is
about a particular product and users
have written some reviews about this
particular product one product that's
what's that what you do now the
challenge is you do tell me two things
can you automatically identify facets
and sentiments by facet I mean you take
a laptop what is the most important
thing please help me out here size
weight display all kind of thing right
so these are let's call this as a
pheasant now I want a generic tool which
will not only work for laptops but will
also work for camera which will also
work for things right for laptop if you
say laptop okay I can do some things
right you see for example on this the
funniest things you know it's expensive
4G phone you also have a discussion
features is a different implication
machine learning right so let's let's
not go there so let's define that size
we are talking about right so these are
the distinguishing aspects of a of a
product okay so I wanted to let
automatically identify them second I'd
like to talk about what are people
talking about this see I I think all of
us look at these expensive phones which
are come out right and they are
amazingly priced so now you want to
think twice before buying any of them
that's what you do you go to when none
of them are the III
thing I muscle doesn't don't qualify for
that the tech savvy enough to understand
that you know one feature against the
other 41 megapixel Canada was a 30 min
of Excel Canada price shoots up by
10,000 rupees right so so so can you so
that they wanted that can you identify
automatically these facets and what are
people saying about so now they're also
we found so so we have to do some things
we had extend topic models as is so what
we found was amazing
so so this Amazon has is what is not a
structured ratings Amazon has given
manually somebody hand tuned that okay
for camera there are some keywords
alright so these are called and on those
keywords you should score high okay so
what what could find is that our topic
models can discover a large fraction of
those keywords so for example if you're
looking at the secret example like a
display so I can you read this so
digital viewfinder short lens so you
could see there's a large overlap order
the Amazon is defined ok these are
things are talking about if you find out
so as an example and completely
unsupervised model knows nothing about
it nothing about camera and anything
like that but is able to just discover
given enough reviews this interesting
facets and sentiments right I hope by
this time you're all dying to use topic
models if you're not okay these are
motivating examples yeah so so this and
these are all basically extensions of
this ldm model it's amazing what are the
extensions we have to do but this is
amazing so so now one nice so this model
is so powerful that it has gone beyond
text though disposed for text for
example there is this interesting I mean
there are a lot of papers this is just
an exact example paper we talked about
so this for image annotation or
understanding image hierarchy their
image they talking about so there's this
work on LD a for this so this is work on
videos where you want to see whether
certain subtle behavior
real behavior is occurring I think in
this video in this paper they're talking
about traffic yes I think they had a
camera out of MIT roads and then can you
identify that a car has broken the
traffic light draw again using just el
dia model was amazing acquire for the
things so images talk about videos you
talked about now it is one more
interesting area I think it's not
getting noticed in machine learning too
much but it is about I give you a piece
of software code large code base without
running the code continue something
understand is is not they can read it is
like a million lines so people are
finding that okay a code is written by a
software engineer a group of software
engineer who follow some practices so
there must be some regularity in that
can topic mollusk as well so this is
being Aryan itself I mean this is a
world reference but there are more
papers as you go forward and then
interestingly people are now going bold
enough and get throwing everything at it
now their own musical and as there is
this interesting paper by I already
solid we're talking about India for to
understand musical influence right so
this is sort of some introductory you
know think about why topping models are
interesting and how can the whole widely
they're applicable so in this part of
this series what as photos and now most
of you probably heard were topic models
are also doing some research in topic
models I guess so I thought has that and
this is a very big area if you look at
Google Scholar
ld8 paper a citation of ten thousand and
although there are really excellent
papers coming out so there's no way I
can summarize them in one to two hours
and it's also I think futile so what I
thought was that to give you or at least
what what I understand as the basic
evolution of ideas why is why is that
wise
you propose like this otherwise so so
we'll start from model which has been
around 1918 it's called LSI so it
started as you said now I think you
should have got the hen that we're
talking about the information retrieval
information retrieval there's something
called vector space models and greater
space models where at some point reads
some when there are some fundamental
issues so LSI came and was the attempt
to solve them and then from LSI but then
LSI also hit some bottlenecks we'll try
to as you go along how to explain that
what are what are those things and then
then came this probabilistic topic
models so first instance was probably
take little semantic analysis so you see
this word I eyes for indexing as you
talked about right you put a label and
then you're using those labels UNIX
Atlas lots of documents so whenever some
key word is been fired we will retrieve
them so that's the that that helps in
search right so pls say was the first
one who tried to try the problem stick
model for that then any I came along and
improved upon that so there's one more
model which I thought was not good
didn't get too much notice is also
another attempt at solving the problems
way less I didn't call it as gap no this
is not a gap is not so named a fashion
store right no so is this is training
for gamma poison so so in this sort of
talk what we will do is we'll review
some of these ideas and then at the end
of the talk will say that see most of
things are probabilistic so I can't
guarantee anything but off late recent
couple of years there's an amazing work
and just breakthrough piece of work
which can get into you something so you
can have to review some of them so for
example have you ever used Lda and if
you okay so have you tried
Lda on Twitter
right so a question question asked is
this so we can ask so should I try LD on
Twitter data or should we write LD a on
blog posts should I try LD on books we
try to discuss these things what do you
know today okay and then so more
recently that alternative to LDS or el
dia models which where you can exactly
give sample complexity statements by
that which means that how many documents
do I need to see before I can guarantee
you that I've got the right topics
please remember is the topic here topic
is a probability mass function over the
words all right so how many documents we
need to seize this is some deep
questions right so now can I answer them
obviously this is a lot to be done here
both on this side and this side so as
part of this talk I'm going to pick out
some papers and discuss the key ideas
behind them okay so that's going to be
outlined but yes so okay so now we will
come to that point but not now right
let's at the last so please hold the
question till then you know good
question and that's let's see you can
see what is the what is the field
understand the words now but today what
I'm going to do so when does the talk
end 5:30 right no so this study studies
has to end at 5:00 I guess you guys are
tired right I'm sorry about this so so
what we'll do is we'll try to cover some
background related to this while because
I'm told that the talks has to be
self-contained so what we will do is we
will try to cover some very basic
background so that means which should
not trouble you a lot this so the
background what we try to do is the next
half an hour is about these two things
um algorithm in traditional
approximation why this why this
because we will devote a lot of time on
these three models so this so these are
the cornerstone which algorithms are the
cornerstone of these right okay so most
of you I'm sure will know em algorithm
and traditional approximation right so
you already know about them it's very
very easy so refresher you can also go
to sleep if you like and 4s2 doesn't
know this may be a good starting point
you can thank you any questions so far
okay so let's get started and now I will
not do it in slides I'll do it on the
board
okay so can your lights off please or
say another thing here welcome to go to
sleep but if you don't wanna go to sleep
then I suggest that you take a piece of
paper and try to work with me that's a
good word staying awake not that you
have to do it because I understand at
end of the day doing your math is not a
good idea
every Tannehill be also I agree okay so
for now so we could do two things we
could do exactly the earlier model the
EM algorithm for that but then it would
be too tied to LD a model because you're
talking about three things here right
we're talking about we will talk about
three probe three models so idea is to
help you enough tour derive all the
questions which are there and also ideas
to do some homework right and they
already give your homeworks
okay the thing is you are on be graded
so it's fine yeah so maybe the idea is
that if I can walk you through some
steps of iam algorithm then you can
start applying it to them I'll go back
to the papers and see how things go
it's okay so we'll start that's why I in
my opinion the best way to reach em
algorithm is through understand how is
applied to mixture distributions okay so
start with the simplest possible mixture
distribution there is mixture of
gaussians
centreboard this hour is good yeah oh
this doesn't you can't see okay black
pants blue pants yeah good is there a
switch switch things off if that helps
so ever have a mixture of questions so
from now on you have conveniently forget
to were popping molds okay so I talking
mixture of questions right so now we
assume that so basically the idea is
this the random variable X is coming
from a PMF given something like this if
I order that if I your teacher I'll
define it like this
so let's say theta is mu I Sigma I and
WI I 1 through K and F I of theta is
okay so by this I mean yeah right so I
should also write an X here really this
is what I mean suppose X random variable
which has a following distribution okay
so what is this this term is nothing but
e power minus half X minus mu I
transpose Sigma inverse X minus mu I
okay so this is lecture of questions
right no the idea is that given me some
in samples is given to me these are all
random draws iid draws from this
distribution this all is given to you
task is find W ice new ice and sigmaz so
now so the idea is that you're not
allowed to
so you're not allowed to write for this
so how will you go about doing it so you
can the standard trick is we try to
define the likelihood these are iid kata
so like you set up so let's call this
log like your function of theta that
idea is to maximize L of theta and take
it at that so this is cotton like it's a
likelihood estimation why you do that
okay there are reasons for that but
statistics books so ml estimate is a
good thing to do
yeah also is problems let's leave it at
that now some of them already have an
estimation of this how do I go about
doing it and the idea is that maybe I
don't have to I cannot so L of theta is
a complicated function of the variables
W I mu I and Sigma I can go to my
optimization toolbox run some things but
to see this functions are very
complicated and in many cases for
example hidden Markov models and stuff
like that this may be this may not be
very easy to do so so it's very
interesting algorithm invented for what
are known as LVM s I mean listen name I
came up with latent variable models so
we written variable I mean the following
suppose you're given a general modeling
something like this okay suppose your
model like this how is the model
specified this is only specified for you
okay so that is what is specified for
you and now you have to tell me so you
don't see said I noticed a random edible
set right i sum over all possible sets
to get right that's one specific from
you know from that we have to now figure
out what is the value of theta or the ML
estimate of L of theta now to do that so
look at the computation computation
involves that this is equal to sum of
sum of these things now for a mixture
okay let's make it very explicit here
for mixture distribution we write it
like this so it like this so I okay so
what is this this corresponds to these
these guys right and this corresponds to
our these guys right but they're a large
variety of models can be described by
this setup in fact all our broad topic
models can describe by this model where
you are given what the joint probability
of xn set and from that you have to
estimate theta where you only observe X
okay so so that is why so there's a very
interesting algorithm kalium algorithm
that's what I do not talk about now so
let's first quickly do a very quickly
review what happens on k equal to 1 when
k equal to 1 there is no hidden variable
okay so now this is normal
u comma Sigma so question is very
quickly this will be very useful when
you derive formulas product so that's
why I went through this most of you most
of you know this so what is likelihood
here for this the lucky would be the
rather the log likelihood would be what
I equal 1 through n take log already
been there right it is minus 1/2 X I
minus mu transpose Sigma X I minus mu
minus log root 2 pi D centreboard yes so
this you can't see right yeah why not
can you see this for now okay so okay I
guess what I did was I ready I'll do it
for here what I did was I just took the
log likelihood of these of this
distribution that's all right I would
never do now I'm going to quickly equal
to 1 what happens that's all you're
going to know okay now the question is
what is theta here theta is again mu and
Sigma now how do you maximize it
how will I maximize it say L of theta
right Adam u comma Sigma so now so now
the idea is that see there is this
little bit interesting math here so the
math is radial of theta is equal to
minus 1/2 you write this trace of Sigma
inverse X I minus mu or that I let you
take this Sigma inverse of C minus D by
2 log 2 pi
- log so what is CC is nothing but X I
minus mu X I minus mu transpose so
alright so let me call this instead of
this alert NC as capital NC
so this is 1 by n so why is this true
this is true because trace off ABC some
of the ABC are three matrices we can
write it as trace of BCA okay so so
that's my love theta that's fine
so now let me do one more thing let me
Reaper a might rise in terms of instead
of Sigma it will be parameter is it in
terms of Sigma inverse so now my so now
this L of theta becomes what L of theta
s becomes now minus half trace s NS C
minus D by 2 log 2 pi minus or plus half
log s and your s is nothing but Sigma X
so what are you - sorry parametric
variables in terms of s and mu and W so
is of Sigma now my variables are s W and
we just V parameterization assume that
the matrix invertible otherwise this
instance time so what are these what are
these two lines this signs means what
determinant okay there's a determinant
now so here is a - right so minus will
become plus because YS is Sigma inverse
okay so it's my my theta is now instead
of theta was what theta was new Sigma
right is it am you Sigma my theta is now
mu s various is Sigma inverse right
now now this matrix C can be written as
X I minus mu hat I'll define you out in
a moment
plus mu minus mu hat into mu minus mu
hat transpose what is mu hat I equal 1
through you what is new every word is X
I I equal 1 through N 1 by n right so
what is what is this new header talking
about yeah we were talking about the
sample mean now you see so now what is
this term no trace of SC now you see it
so in this term there's no mu ok so now
trace of s into this matrix so trace of
this quantity right what is Trace
whether what is Trace I forgot to ask
what is Trace sound and sound agonal
elements right ok so what is this value
what is this value
so you see s is what inverse of a
covariance matrix covariance matrix are
what they are positive definite or semi
definite so now this is this is a vector
right so I can use this recycle
inequality so this will become something
like nu minus mu hat transpose s nu
minus mu hat so this term is always
positive
why because s is a PSD matrix so once we
are gone with this now we see that C is
written as C written something as this
Plus this right so interest so let me
call this matrix a see hand which is
also the
sample covariance matrix right so now
when you plug this back in I get trace
of s and I worry that so I should go
back here n s C hat plus this other term
right s into mu minus mu hat mu minus mu
hat transpose ok so there should be an
end here now so Plus this whatever term
we had no half log s and all that now
you see we just discussed that this term
is going to be positive but it's a minus
sign before that so then I can
definitely write that L of MU comma s is
necessarily less than L F mu hat yes for
all is agree with for NES the term is
very positive right so so now given any
s what I should do I should choose new
hat I get a better result for likelihood
right so so this is so that so then that
this automatically settles for us a mu
equal to MU hat and what is mu naught mu
hat was given here right next you
realize that this is a concave function
in s so log determinant of a PhD metric
of a matrix that matrix is a concave
function in s in a matrix right
unfortunately I can't prove this but you
can look it up it's simple the proof is
very simple ok so now I want to maximal
conduct concave function what should I
do
how should I do that so forth so
minimizing a convex function or
maximizing a concave function a
necessary condition sufficient condition
is take the derivative set it to zero
okay so we said we do that I get my
answer as s to be C hat this is homework
okay so this is a simple a thing for how
do you say 1 k equal to 1 case right but
that's not interesting or do you know
that now the thing is I want to do it
for for many case right so now
so no so now let's let's let's jump into
this with this aside okay now we need to
know one inequality again this so so I
write KL of Q even P where Q and P are
two pianos probability mass functions
okay and then I define the following on
let's say key support is on K the first
K integers okay so what are Q and piece
there probably mass spec sure that is
what this sum to one okay assume for now
all of them are positive not there was
you okay make life easy
so now KL Q comma P is a distance
sometimes called the Coolbeth Laird
divergence so no one can prove this is
greater than equal to 0 and 0 if Q equal
to P so we need this so ideas if this is
0 then P must be equal to Q like please
note that I can't just swap this row of
these things become different right okay
just check the formula you can't just
swap this ticket so we will not we will
need this now so here is an interesting
thing which we will need so interested
in this letting pebble models right
where this variable Z is what we don't
know so now we define so our model it
will be the mixture distribution model
that I am going to talk about so now for
that we define so I will denote this by
this is
okay what is P G of theta P G of theta
is probability of the random variables
said is equal to J conditioned on
observation X so these Zed's are called
he denied that I don't see okay now how
can i compute this I can very easily
compute this because I know using this
formula okay ten more minutes sir that's
going to rush okay okay so this is
specified for us how to compute
denominator the denominator is obtained
by summing over all J okay now now as we
are doing this try to think about
following problems what if I have to the
summation is huge think how many ways
can break down right so if I need it
later but for now for mixture
distribution this is very easy okay if K
is born now so now so now let us try to
do the following let us say I define one
more PMF so J is the Q J is some other
p.m. something else so what is this this
is nothing but the negative will be act
like your divergence between Q and P
okay now here please please work with me
here so so what is it this would give me
what this would give me J equal 1
through K QJ log P X is equal to X is
equal to J by theta minus log P X is
equal to x given theta what happened to
this part
there's one more thing it is QJ log Q J
so whatever you see the Q just
disappeared here why because sum over Q
J is equal to 1 okay so there's no Z in
the summation here there's only X right
so the summation works only on q JS
right okay so this is very good so what
can I say about this identity so this
term is always equal to this equals
minus K all right so we see this term
must be less equal to 0 because of KL
divergence what does the immediately
imply it implies the it implies the
following thing remember my interest is
this one so obviously then I can take
this outside and say this is so that
means it tells me the following it tells
me that you choose any QJ you like as
long as valid PMF is fine it can
immediately give me a lower bound on
this so this is the basic idea behind
variation approximation also called iam
algorithm so where I cannot compute P
J's or some problems with PJ's right
then this is the thing I should go for
okay
so this is a very important point
thank so now what we have now so we have
the following thing now J equal 1
through K QJ log P X is equal to X is
equal to J by theta minus J equals 1
through K QJ log QJ that is less equal
to in a log P X is equal to X by theta
now suppose there are n draws what
should I do
X I equal to small X I I equal 1 through
N
and now let's assume for each of them
you will have so what is it
what is it in this term I have what is
it here what is it I have here this well
a cute is that I feud right and now what
is it I have for any Q any choice of Q
is I get a lower bound on this okay
now Peter tell me what is the difference
between these two this so for any Q if I
choose I can compute this number I can
also compute this number this is road
bound right is lowered one and L theta
right right so now the difference is
nothing but the KL divergence isn't it
okay so now we have this interesting
identity that L of theta is equal to
this so this plus KL of Q Y comma P I I
equal 1 through n okay now so so P I of
theta right I have this clear it so what
is P I of theta so PR theta is that for
the I ate example for the I ate example
the posterior distribution okay for a
given choice of theta
TK now the EM algorithm proceeds like
this
so suppose I have a guess of theta it's
correct it a zero so now let me define
so what is Qi elephant when qi j is
equal to p z i equal to j x i equal to x
given theta 0 right
right so what is Qi Qi is nothing but
the PMF defined on this right for that
example so it is this what it is now
then some moment I plug this value in
here this this so this will become a
what will this become so let me choose Q
I guess to be like this right so I
should be Qi is here right Qi just so so
then this I can look at it as a function
of theta given theta 0 mr. Q ideas is
what Q H is nothing but the posterior
distribution right so the function of
theta and theta 0 okay so so I should be
very careful here see your is a function
of theta what theta zero I is what I
know so the function is defined
especially constructed for this theta 0
what is a function of only theta right
that's what you look at it so this is a
function of what this is a function of
what this is a function of say let's do
some you know theta 0 only every right
and what is this now this says I will
write it something like this I equals 1
through n KL p i theta 0 comma p i of
theta ok now this is exactly equal to L
of theta I'm sorry I want to fast here I
hope this is this clear so how did I how
did it reach this point
how did is this point we first argued
that L of theta is all bounded by this
and that is and that is what that is
given where so that gives us so now you
plug this for any Q so you plug which Q
the Q computed at theta 0 okay so this
becomes so this term becomes a function
of theta given theta 0 right for theta 0
now
so what happens setup if I put theta
equal to theta 0 in this equation but
what do i get if i put plug 3 doubles he
does it in this equation I will get L of
theta 0 is equal to correct what happens
with this term 0 fine okay now so you
algorithm now do the following it says
see I'll give you a 0 right
I learn right either M the got the right
theta or I can do something better let
me maximize this function so let me
Maxwell let me my algorithm is theta nu
equal to R max of theta Q theta given
theta 0 okay so now so so so now now
let's check this L of theta nu can all
of you see you can't see okay so I think
we're almost done here I will take only
two two more minutes of your time
yeah that's exactly five one way you
should do it here so now how do I get nu
theta okay I say that maybe it's easy to
maximize Q as a function of theta so now
let me get a new theta let's call this
you hit on you okay so L theta nu is
what that is Q theta nu theta zero plus
all the other things f theta zero so now
this term is going to be what this term
is going to be positive let's drop it
so so or should should equality change
this
now this term and this term which one is
larger which one is larger this term and
this term would just compare these two
terms and which one is larger this one
of this one this one is larger right
why by choice I have computed theta news
so as to maximize Q so is that Thomas
that one must be larger right or young
or equal so what is this now L theta so
what have you achieved now so now I'm
giving you a sequence of Thetas so
2-mile a clue increases okay okay I
think I have to stop here so but visit
is the basic idea
right this is basically am algorithm now
homework for you what I wanted to do I
didn't get time was that you try to
derive this step what is Q and what is
the maximization step for mixture of
questions you can see it get five lines
of code when you put the coding
perspective okay fine so with this
thought I will give you and will meet
Torro</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>