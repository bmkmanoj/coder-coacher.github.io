<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Parallel Bayesian Network Structure Learning for Genome-Scale Gene Networks | Coder Coacher - Coaching Coders</title><meta content="Parallel Bayesian Network Structure Learning for Genome-Scale Gene Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Parallel Bayesian Network Structure Learning for Genome-Scale Gene Networks</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QCV4ITn6BDU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so yeah so I work for Intel
parallel computing lab in bangalore and
i graduated from iit kharagpur in 2005
then I went to Northwestern for PhD my
PhD was in high performance
computational biology and I'm kind of
containing the same work here at Intel
so without delaying any further let me
just get to the slides so this work we
did we have been in fact business still
continuing so we have been doing this
for about one and half to two years now
and we have a publication in
supercomputing for this particular work
which I am going to present right now
this was nominated as the best best
paper in supercomputing we didn't win it
eventually but it was at least the
finalists so as you can see there we
have a bunch of collaborators so the
first two are from our lab this is first
is me and then we have Karen and then we
have people from iit bombay nu DET china
and georgia institute of technology USA
and professor srinivasulu is a key
collaborator in the send you can say
that he is the lead from outside Intel
in this project so what is the
motivation ok the other thing is that we
have about one are right when are for
the talk this talk is about 25 minutes
to half an hour so feel free to ask
questions any time even along along the
way so just to give a background of why
we are doing trying to do this so our
main goal was to study bayesian networks
but we wanted to have a vehicle to to be
able to do that and we found that this
there is this nice plant arabidopsis
thaliana which is considered a model
plant organism and it it has about
22,500 jeans and 35,000 proteins in fact
it this is this was declared model plant
decades back and in 2001 NSF launched a
10-year program to discover the
functions of all the genes of this plant
and after about the quarter
four million dollars spent for over ten
years and make other and some more from
other space other places like European
programs similar European programs the
current status is forty percent of the
genes still have no known function and
even for the other remaining sixty
percent at least one function is known
there may be other functions which we
are still not aware of so this and the
the process of finding the function is
is is more of a biological process so
you use a wet lab to do you have a
hypothesis you basically try to guess a
hypothesis that this is a function then
you use wet lab to test that that is a
very long process and in many cases your
hypothesis might be wrong so it just
takes very long so we thought maybe we
can use computer science to help with
this problem so instead of so one way of
doing that could be just suggest more
experiments for the wet labs and then
collect data through that and then do
use some computer science on top of that
but we thought that instead of going
that way let's first see what is already
available what all data is already
available so there are a there's a bunch
of experiments which have been done on
this particular plant and there was a
huge amounts of data available online so
we thought that let's construct a gene
network based on that data and gene
network is basically you have a big
graph where you have nodes as genes and
interaction between genes as the edges
no this was declared model long back so
i think that the lot of experiments are
because as a result that it was declared
mortal plant because i know so how is it
is the scale and a few years come by the
human genes oh yeah so he says react is
a model so so similarly for client this
is the model so that everybody works on
this it we know that it takes maybe 11
persons 10 years of research to just
identify function of one ye so unless
the whole community works so yeah that's
how I said yeah yeah and just represent
the interaction so okay so interactions
can be in various ways so one possible
is possibility is that there are at in
any function in any biological function
multiple genes interact work with each
other and when they work they actually
produce a protein which actually does
the work so to give some background of
the biology so our DNA just encodes the
information of how the body supposed to
function but they don't do the work so
the DNA parts of the DNA which are
called genes are converted to or use
that information is used to make
proteins which do the work so for
example hemoglobin is a protein so it so
similar things are done so in many of
these functions multiple genes are
involved together so one possibility is
that you just find correlations what all
genes are working together that is an
interaction between the genes the other
possibility is what happens is sometimes
one particular gene the function of that
is just to enable some other gene so
this particular gene creates a proteger
creates a protein that enables the
chemical process which creates the
protein of the next gene so that's
acting as an enabler so there is a cause
and effect relationship there so the
various kinds of interactions are
possible here we have a correlation
based we have a combinatorial
correlation based
relations that's true so so that's why
you when you do that use the
mathematical model to model this thing
you you go with that information that
goal in mind that you want to inter you
want to capture these kind of
interactions so in our case we were
trying to capture the correlation based
ones which basically means that they are
either co-expressed or they're not co
express something of that sort so there
are like I said there was there are
various methods mathematical models had
proposed and there's a just a pearson
correlation based one there are
graphical gosh gaussian models are also
available information theory based
models are also there and then we have
Bayesian networks and what happens is
that from top to bottom the accuracy and
applicability of the model increases but
at the same time the time required to to
learn that model from the data becomes
gradually becomes a bottleneck it be it
gets bigger and bigger from my clarity
yes no these are all mathematical so
there are computer programs to do all of
these yes yes so for example vision it
works is an np-complete problem so it
has exponential complexity so what there
was a study conducted across all these
models and what was found that many of
them do really poor they perform really
poorly and one in three do no better
than random guessing so the state of the
models is really bad right now and so it
it sort of makes a case for a more
complex and more more accurate model as
compared to just doing anything okay so
so Bayesian networks as they are among
the most accurate models for this so far
from what studies say we decided to use
Bayesian networks for this and what it
does is that it outputs so the up asian
network is a graph is a directed acyclic
graph
they're based on a scoring function you
are trying to maximize the that
particular scoring function for that dad
so you are trying to find the best dad
which maximizes the scoring function and
I've come to the scoring position later
on it has it was found that it has super
exponential search space number of
possible vision it works you have to
search through is super exponential and
it is np-hard even for a bounded node in
degree so if even if you bounded in
degree of every node even then it has
and it's NP hard so there was this nice
serial method which was developed which
is which is a complexity so if n is the
number of genes or number of random
variables then it has exponential
complexity in n and even for n equal to
20 it takes 50 hours and like I said in
the beginning rabid opsys has 22,500
scenes so n is that big there so we
definitely cannot use that there are
various other parallel algorithms based
on the serial algorithm but they are
also much slower compared to what we
need and then there are various
heuristic based algorithm so one for
example is what they do is they take
they are creating human gene network
they take these thirteen thousand seven
hundred and thirty-one genes randomly
create so 50,000 times they do this they
randomly take thousand of these genes
create a network which is it's easier to
do that for a thousand one and then
combine them together to create the
whole network but that will never give
you the accurate picture because across
these thousand subsets there might be
interactions which will miss so coming
to the scoring function so we so as I
said that we want to find the best dag
which maximizes particular scoring
function in this project we focused on
decomposable scoring function such that
for each node if you can compute this
particular function and then add up
across all the nodes that is the that is
the scoring function you have so and
this what this does is this basically
says what is the fitness of choosing PA
X as parents of X where PA X is a subset
of all the genes all the other genes
so since it's decomposable what you have
is suppose this is your patient network
so for example B has these two parents
he has this parent and so on so for each
of them you can compute this coding
version then some of them over all of
them that is your final score so based
on this we came up with this heuristic
parallel algorithm what you what we do
is that we conservatively estimate the
candidate parent set so for each gene X
we estimate this is this should be the
candidate parent set and we are going to
look into only these as possible parents
and so and we do it conservatively so as
to not miss out on actual parents then
what we do is so to do that we have used
the pairwise mutual information based
implementation which was published by
these guys and ok I'll skip this so then
after we have this candidate parent set
what you what we do is we define a
threshold T for all the candidate
parents for which the size of the
candidate parents at is less than T we
directly compute the optimal parent sets
from those sets for the remaining ones
where the size is greater than threshold
T we use this particular expression to
reduce the size of them based on what
you have already found out and then even
then if there are some which have which
are still which the candidate parent
sets are still bigger than this ice tea
we just take the top T correlations so
this is where we apply a heuristic here
but after beyond this point what we do
is once we have so now we have only
candidate parent sets of size T or
smaller we use an exact algorithm to
find the best optimal parent sets from
those and then once we have this there
will be some cycles in the graph which
we detect and break because we wanted a
dag so through this method watch what we
do is we combine the precision of exact
learning with the scalability of a
heuristic learning solution and idea is
to push the limit of T as much as
possible using massive massive
parallelism because the higher the value
of T the more accurate this output would
be
so questions one in reality jeans can
have feedback cycles so listen to assume
back here so it no actually so there are
two things so we we have two outputs
when is before we remove the cycles and
one is after we do that the one which is
after removing the cycles in a Bayesian
network because we wanted this have to
have general applicability also we
wanted to output bajan it was also but
we have the other solution also end the
two biologists we have given the first
one which the cycles so you said you
keep some estimates like one out of
three aesperus random guesses or
something like that so how do you know
what is correct and not so you have the
ground truth I will come to that I'll
come to that we we don't know the ground
truth per se well for some cases we do
but I'll show you we have done in some
experiments to evaluate this method i
will show you those okay so like i said
we want to use massive parallelism to
push the limits of tea so we have used
these two machines tianhe-2 and Stampede
tiene to is the top most supercomputer
the fastest supercomputer in the world
right now it is been there for almost
two years now in fact stampede is also
another one among the top ten and one
other reason of using this is that they
have intel cpus and intel core
processors so so like i said the earlier
solutions were too far away from solving
the actual problem so we have through
our implementation we have got a
speed-up of about two hundred thousand x
over the earlier solution to to be able
to do the entire plant and now we can
actually create gene networks of the
entire entire adopts is planned so the
baseline was working 1024 course this
was non vectorized code what we did was
we implemented a new parallel algorithm
and since we were trying to scale it to
up to 1.5 million course even though we
are increasing the number of course by
only
1800 the speed of the gate is 5300
because of the new parallel algorithm on
top of that we have tried to avoid as
much as possible we have tried to avoid
redundant computation and got another 6x
there was load imbalance across the
nodes so we have implemented our own
dynamic task scheduler that gives
another ten percent and ten percent
might look like a small number but for a
55 petaflop system it's 5 petaflop
switch is bigger than most of the
systems in the world and then on top of
that we have applied vectorization
because we wanted to maximize the
benefit of the modern architectures and
in total we have 200,000 X performance
improvement so to coming back to the the
problem so now for each node what we are
basically doing is we have a candidate
parent set for each gene and then we are
trying to find the optimal parent set
the exact way of doing that is compute
the squaring function for all subsets of
CPX and then find the best one but so
okay so that can be that can be
represented as nodes of this hypercube
where each node represents one of the
subsets so you just compute all the
notes of this hypercube and you have the
final and find the best one and that is
your answer but maybe there is a better
way of doing it maybe we can reuse some
of the computation so for example if you
are processing this set 1 comma 3 maybe
the work which we did for this set can
be used here because at least one of
them is common and similarly for 1 comma
2 comma 3 maybe we can use computation
than in any of these things so that's
why there are these arrows here and the
other thing so we actually we are doing
that so when we use the reuse the
computation on a node from a node in
which the difference is only one in the
subs in the subset size the difference
is only one and there's only one element
which is missing they're the best way of
doing that would be a DFS because that
way you will ensure that for example if
you're processing this all of these have
been done before that and you can reuse
from any of them but DFS this no this is
a this hypercube is only of dimension 3
but we have the tea which we are using
is of size 32 the best
St which we could get to so this
hypercube will be of dimension 2 to
power 32 there you cannot store it all
the notes of one level in the memory so
we we could not do BFFs but we have done
DFS to avoid getting into that problem
and we don't lose much on performance so
the challenges of now we have to
paralyze this the challenges are that
the available parallelism is limited to
the number of genes which we have and
the amount of work which has to be done
for each gene that varies exponentially
in the size of candidate parents it so
this huge variation and there is very
little parallelism available so we need
to do something about that so for
example suppose we have this
four-dimensional hypercube which we have
to evaluate for a particular gene
instead of processing this in as one
what we do is we divide this into
smaller hypercubes and we define a
threshold are so any hypercube which is
bigger than our is split into our
dimension sub hypercubes and any hyper p
which is smaller than our will stay
intact so this gives us a very large
number of work items so each sub
hypercube can be our work item now we
are we have to now we have to compute
all these work items in parallel and as
quickly as possible so then but before
that sorry let me just mention the
benefit of this reuse of computation so
we have conducted this experiment from
128 nodes to 2048 nodes and we we see
roughly a speed-up of 4.8 to 6.4 X in
these cases so there is a huge benefit
of reusing the computation so we would
want to do that but that sort of
introduces this dependencies which
between one compute node to another
computer node so but ok ok that's fine
so ok so as I said we have this work
item large number of work items which we
have to process together so what we do
is we use dynamic work distribution and
load balancing so instead of so in on a
small system what we could do is we
could
have a master-slave kind of a system
where one node is distributing work pay
via the node but since we are working on
system which has about 8,000 nodes in it
we that one master node can become
bottleneck so instead of doing that what
we have is a hierarchical tree structure
so if this this blue thing is the
initial amount of work which we have so
that is that that is all the work items
together all the survivor cubes together
initially the root node allocates keep
some to itself and allocates part of it
to these other nodes through the child
nodes and then they in turn do the same
thing so all of these nodes all these
nodes which are parent of any node are
keeping some allocation to themselves
and allocating the rest of their
children suppose one of the child gets
done with its work it can request more
work and this node can use a remaining
allocation and get give some work back
to that if this one runs out then it's
goes to it its own parent and and so on
so through this dynamic task scheduling
we have got a performance improvement of
about seven to eighteen percent over the
static static allocation of work
so we just this is a software tree
structure it has nothing to do with the
actual topology of the network so we
just create a software tree Network the
fan out I think we were using of 16 so
each the root node has 16 children and
then they have their own sixteen
children and so on or because we the
other possibility could be just use one
master node and all the other could be
slave nodes but then since we have about
eight thousand nodes one master and
eight thousand child nodes the master
node will get bottlenecked so to avoid
that we have created a hierarchical
structure so the first one has 16 or 32
children then each of the next layer
ones have 16 or 32 children and so on so
I think with 8,000 nodes we were done in
three layers so we had a three layer
structure that is a parameter which you
can define while running running it what
we found was that I think 0.2 so twenty
percent of you you can keep twenty
percent to yourself and remaining if you
can distribute that works very well for
this particular application but this may
be different for application to
application and we will just provided a
parameter which you can keep tuned to
okay so just a little bit more detail on
the reuse of computation how we do that
so suppose you are calculating the
scoring function here of of a gene X for
considering parents as x1 and x2 so
suppose we this is the data which you
have so there are these various cases or
various experiments which are done by
biologists for which you have got the
data for these three genes now different
colors they present different values
here so what you want to do is so one
particular key step key bottle next step
in in the score computation is a disk is
a winning of these these this data based
on the Equality of the vector so I
this one is not included in that binning
so for the first two if you see these
two are equal then these are all equal
this is this is this is a pretend is a
separate so that that binning process is
the key step there and then based on
that billing some probability is
computed using which entropy is
estimated to get the final score the
colors indicate different expression
values so so think of this as these are
just random variables with different
values possible so different colors are
just showing those different values of
the random variables all the experiments
or all the genes very sparse vendors no
we we have expression values of all the
genes for all the experiments which we
have been closed the expression value
for all the genes yes yes so now suppose
we want to compute this new one in which
there's one more added here so that that
basically is is represented in this data
so instead of doing this from scratch
what we can do is we can we can add
these this last this data corresponding
to x 32 these existing pins and then
reven the existing Vince into smaller
bins so this this process basically what
it does is that no matter how large is
your set you and you only have to do
this last step of ravening what is
existing there already so this is the
reason why we get the benefit of reuse
and I I won't go into more details of
that okay so experimental results so we
have taken the data set from eleven
thousand experiments each measuring all
the 22,500 genes various cleaning
methods have been performed on the data
set then what we did was so it it helps
if so since I said that different subset
of genes participate in various
functions so not all the things are
going to participate in all the
functions so it helps too
to to categorize a data or classify the
data based on functions or body parts
and stuff like that so we have used to
particular classification techniques one
is the tissue type which tissue in the
plant is it so the whole plant or very
ways for example seed and leaf and all
that and there is experimental
conditions so what what biological
process is going on so for example based
on what is a reaction to light for
example so based on this we have two
classifications and using these two
classifications we generate about 100
different data sub data sets of this
large full data set and for all of these
data sets we have for some of these data
sets we have created these networks and
this is the overall strong scaling
results which we have so for two of
these the largest data sets which we had
we go from to 56 notes of tianhe-2 to
8000 notes of DNA to and the straight
lines are the linear represent really
near scaling then as you see that the
scaling is not that back in fact we
achieve about seventy-five percent
efficiency going from 256 to 8,000 nodes
also we did another study to find what
is the best possible for our hardware
for the for the entire system so
possible efficiency was this much and
what we have measured was about this
much so it's not too bad the other thing
is that we just just to show what is the
time to solution and the other things
related to this one so there are these
four datasets and for all of them we ran
them on Stampede and for example these
two took only 2 69 and 501 seconds so we
did not bother to run them on Tiana to
which is a bigger much bigger machine so
these ones which we did on ran on Tiana
too they finish in two minutes and three
minutes so the entire data set can be
done in just two to three minutes this
before this result this was could not be
done in less than six months and the
other interesting thing to note is that
remember i had this heuristic where only
candidate parent set which are smaller
than the threshold process in the first
iteration and the remaining in the next
iteration so this is just to show how
many of them
actually fit in the first citation
itself so for example in this case I
think it's about more than ninety eight
percent in this case it's about ninety
percent so even in the worst case we fit
about ninety percent of the of the genes
in the first iteration itself so there
is no heuristic involved there so we
have accurate results for a ninety
percent and in some cases of
ninety-eight percent of the genes and
four remaining two percent we might have
some inaccuracies so to evaluate this we
studied the the photosynthesis process
so on the right side you have the
photosynthesis all the genes which are
involved in the photosynthesis process
this is a pathway and on the left side
is what we did was we had this entire
network we took some of the genes which
are involved here and using them as seed
genes extracted subnetworks of the big
network we deliberately kept some of
these genes out of that if we did not
use them as seed genes to check whether
they appear in the sub networks so this
these pink ones okay this color is not
looking similar to what I had intended
but this is pink so the pink ones are
are the seed genes the the green and the
blue so this is blue and this is green
so the green and blue ones were which we
deliberately kept out of that and we
found that they also appear there so
they we were already aware that they are
part of the photosynthesis process and
they appear in that so that sort there
is some sort of validation and then we
found some of these yellow jeans which
also appeared there for which there was
no known function before so we picked
two of them and I'm obviously showing
you the 2k the cases where we got more
interesting result there are other cases
where you may not but we pick these two
and what we did was we took her normal
plant in the seed we disabled the first
gene here and disable the seconds in
here and this is a normal plant and they
are all grown to the same number of days
so we see that the growth is stunted if
you disable these two genes
no this this was done in Georgia Tech
here so profession immerse eluru knows a
few biologists he is collaborating with
them to do these experiments so to
understand that further that results
further there are we kind of created
these graphs so the first one is a
normal expression level of this gene on
the x-axis the second one is the
expression of this gene when the first
gene is disabled the third one is the
expression of the scene when the second
gene is disabled so you see that in some
cases the second bar are much bigger so
these are the genes for which the that
gene was acting as a controller and
since that is removed they start spiking
up similarly third bars are somewhere in
some case of the third bars are smaller
so this the third in the third case that
gene was acting as an enabler and when
you remove that that that thing goes
down so using this we can understand
that exactly what genes are being
affected by those two genes which we
found to be important in the
photosynthesis process so this gives a
very good result to the biologists to go
and do further experiments and figure
out what all can be understood from here
so this is the final result and then
just to conclude we have contributed to
bayesian networks this is the first
massively parallel major network
structure learning algorithm this is
applicable in various of their
application areas whichever application
areas are using the same scoring
function this can directly be applied
otherwise the scoring function will have
to be reimplemented and end-use in the
remaining framework stays the same
contribution to systems biology we have
a full gene network of arabidopsis
thaliana now and this can serve as a
vital resource for arabic future era bit
of this research and there are other
observations are some of our some of the
things which we have done in this
project are very widely applicable so
for example this dynamic works really
scheduling algorithm in fact we just
submitted a paper just on the dynamic
works till you scheduling algorithm
which was which was initially done for
this work but then later on we made it
we improved it further and then
submitted a paper recently
and yes and just because I'm part of
Intel I need to show you this
performance notice and we can get back
to questions okay
so the so so yeah I understand so some
of the things here are a little
confusing and I understand that so what
happens is that the tree which is used
for dynamic work scheduling and the dag
which is produced elsewhere and this
reuse and through in which we have
created this hypercube there are three
different data structures they do not
they are not related to each other in
any way but because we have very tree or
graph like structures in all these cases
they seem that there is a some fearsome
connection so so so yeah let me just
answer that so this is the DF DF s so
okay so what we have is for each gene we
have a candidate parent set we want to
evaluate the subsets of all of them to
to come to the to get the final best one
right so that gene no no this is farah
for a single gene okay so for a single
gene for each gene we have these
hypercubes now we have to paralyze that
entire in across the entire system since
there are only about 22,000 genes and we
have 8000 nodes if we just give some
genes to each node that is not going to
give us good load balancing so we want
to reduce the size of the work item
which we are distributing so what we do
is we take this hypercube of each gene
for the hyper cues which are bigger we
divided into smaller pieces so so for
example if a particular gene has a hyper
okay so let's say our is 16 and let's
make it easier let's say our is 3 and we
have a hypercube one gene has a
hypercube of dimension for so we will
divide that into two hypercubes of
dimension three each if one gene has a
hypercube of dimension five will divide
it into four hypercubes of dimension
three each so for some genes will have a
much larger number of
possible hypercubes which we have to
compute for some genes we might have
just one if the dimension is less than
three less than or equal to 3 so this
this gives us a huge number of work
items which have to be computed so now
we take each of this we call this 11
work item and then we use this
distributed scheduling to schedule the
work items so so the work item which is
scheduled is one hypercube to the two
one node and that hypercube is evaluated
by one thread yes yes so that is not a
bottleneck at all
sorry
so generally people think that a gene
interacts with four or five or in most
cases 67 other genes that is the general
assumed knowledge but nobody has done
that work to at the entire genome level
so nobody actually knows what is the
reality so one thing which we were very
interested in knowing is what do we get
and we actually got very small number of
genes which are interacting with each
one so that I think the numbers which we
got our also 45 sometimes two sometimes
one is what we we also got so that kind
of validates that assumption but will
that that is a much bigger question than
what this study can solve so maybe in a
few years we will find out
these bus you can actually recover the
global structure wildest party no
so again it depends on what sort of
scoring function you have to use against
a PR problem in general but if you're
scoring functions are nice it is
possible to use a show you things it can
be again they get a bad signified life
you can recover noble structure of the
without if you have done great with an
somehow like that's an adult step is not
accessible but might want to look into
that
because that just takes a different only
it's not compete okay so so I would like
to understand more of that because so
far haven't understood much and the
other thing is we are not breaking any
tags here we are just ok big to make a
dag yes that's what direct approaches
because just the global solution okay
one other I'm not sure that these
methods are capable of discovering tags
is required they will give you the best
possible solution at the best possible
solution as a cycle i'm not sure what
even as we do but they will definitely
be able to
given the gene expression data the best
network that explains that that's what
maybe we can meet offline and if you can
show me give me some pointers to this
thanks reason for using bayesian like
your church was motivated by what other
people have been doing in the past yes
the the common knowledge was that vision
networks are most accurate or among the
most accurate in this in this field yeah
but the other thing is that nobody has
created a Bayesian network of this scale
so you cannot really say it but unless
you have actually done the experiments
impressive you are talking off so here
the Bayesian network you also get the
probability values along the edges right
so that is a what a surprise to arguing
that with you might want to use an
existing
so the clear expression data is inclined
towards finishing or rather it the data
is indicating towards a certain types of
a certain kind of interaction to say and
let's say you want to import an
interaction as a probability value if I
father was a chance at me or something
like that so these metals will again in
code that is if that wave it requires
some function let's say less activation
function to take something out throat
property value here is the will keep you
probably values for each of the edges
but the thing is the nice thing of these
messages there they will directly output
a very small improvements will not
require you to discover enlarge
which many techniques
graph recovery has been steady
yes sir these degrees are lying because
indirectly give you the optical
structure
but to get that optimal structure you
might have to evaluate the possible
search space right okay so the exact
mechanics of the method will depend on
what is the problem you're trying to
solve what it is calling function again
there are there not just one taking
there are trader techniques there are
breeding methods there optimization base
to this again for many of these problems
can convert the entire problem to a huge
NP or a QP and then spend all their four
tracks all that tubular gasps as
possible and where you solve that the
better look at what these can be very
non scalable so there are you shredder
techniques as well waiting to send like
the keys and reading methods which so
the greedy methods might resemble
of steps you're performing as an
estimated parents take structure and
refine and so breeding methods to
something like that but is not exactly
again the company now the guided
computations you require again something
more local but then they will be lower
computations report as well so i'm not
sure whether these global steps can be
bad like that well hello who says okay
any other questions so are you planning
to apply this method to other model
organisms like equal area yes eventually
currently we are doing a lot of
experiments with arabidopsis itself or
my collaborator yeah so first first as
soon as we have a good enough validation
done and we are very sure of these
things then we can apply start applying
to other ones popular in computational
neuroscience people love switching
there's a there's a company which is
studying the interactions between these
neurons in C elegans and they have a
graphic interface also if you want I can
send you the the website
the master view
operations
okay any other questions
live in the morning
we
so it was very interesting there
manually PP
so this particular black was believed to
have the smallest genome come on all
flowering plants
I
but later it
so that could be the reason when
he believed that the reason why
like that for instance is because it is
most close to human genome then equal I
i think is the smallest ticket you can
your genome if you alone is made
restaurant if you hear ya plus you
cannot torture humans yeah yeah yeahs
very close to chimp is closer but in
doing all these experiments on team is
more expensive than rats wretches of
ninety-five percent okay thank you thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>