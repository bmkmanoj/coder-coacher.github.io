<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud-Enabled Mobile Sensing Systems | Coder Coacher - Coaching Coders</title><meta content="Cloud-Enabled Mobile Sensing Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cloud-Enabled Mobile Sensing Systems</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5dpfT1CcxBw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's my pleasure to welcome neuron draw
from University of Southern California
he will be talking about cloud enabled
mobile sensing systems a large part of
this talk is in fact a preview of what
the rest of the world will only get to
hear at nst I so this is faster for you
guys with that welcome me wrong and take
it from here thanks for the introduction
a month first of all thanks for inviting
me today as I candidate so I don't say
that it's my honor to give a talk here
today today my name is Marie umrah from
USC and today I'm going to talk about
cloud enabled mobile sensing systems and
especially how to enable efficient
processing and secure sharing of sensor
data using the cloud so nowadays almost
everyone has one or more smart mobile
devices so these mobile devices are not
just the selling where but deeply
changing many aspects of our lives the
changes are from how we interact with
each other to where we eat very meet and
how he experienced a visit to the doctor
and how you pay be receptive far so with
this math mobile devices numerous elves
have been developed and this have become
indispensable in our everyday life so
compared to desktop is your laptop so
one of the distinctive features of smart
mobile devices is the existence of
sensors so modern smart mobile devices
already have a variety of sensors such
as camera gps microphone compass and
motion sensors like accelerometer and
gyroscope and many others so these
sensors on mobile smart mobile devices
provide you just reach contextual
information to enable novel features so
based on these sensors many use for
mobile sensing applications have been
developed video sharing the arguably
intelligent personal digital assistant
and photo sharing and location-based
services are widely used and some of
them have humongous video-based the
Facebook has over 1 billion active
monthly users and Google Maps for Mobile
has
more than 100 million active monthly
users and note that all these
applications are enabled by the cloud so
this cloud enabled will bear sensing
systems and applications are the focus
of this tall and all these applications
use the cloud mainly because they are
leases constraint so the computing power
is less powerful than the desktop PC or
laptop and they have smaller storage
space or so in order rapidly evolving
the wireless network is less reliable
than wired Network and not always
available anymore or battery problem is
well known in mobile computing community
for example ever since mid 1990s the
battery density has been improved only
two times every 10 years so cloud
certainly gives a great opportunity for
mobile devices because of its high
availability nearly infinite storage
space and millions of compute cores as I
described there exist many use for cloud
enabled mobile sensing systems and
applications the mice the problem is
people always desire fast application
with more features and there are growing
concerns over security and privacy so my
face is focused on the system support to
realize these growing demands and to
resolve eliciting concerns on cloud in a
briton world sensing applications so in
this context we have several challenges
to overcome in sensor data sharing and
processing first we often face
performance problem when you deal with
compute intensive computing data
intensive mobile workloads and second
whenever we share large volumes of
sensor data with others using the cloud
there is a tension between efficient
sensor data sharing and privacy
protection this is not an easy problem
to serve immediately and third when you
share the ROG volumes of data from the
corpus of smartphone users here is often
very challenging to efficiently deal
with labor-intensive subtest so the
question is what kind of programming
obstruction do we need to address this
problem and lastly the whenever we share
large volumes of data using mobile
devices the energy problems are always
there
so given the challenges my little score
is to enable efficient processing and
secure sharing all sensor data using the
cloud so as I described my physics log
is tightly connected to a sensor data
sharing and processing so I made an
effort to overcome several challenges
that I described in the previous slide
so this is about enabling mobile
perception application focused on
performance and ps3 is about how to
protect users privacy against providers
ensuring photos and meta size the high
level programming framework for crowd
sensing and sirsa is about how to trade
off energy and delay when sharing large
volumes of data before they are being
into the details I would like to briefly
cover some technical aspects of my
thesis research project they notice that
project ID rate dataflow programming
language and built on time based on
workload characterization in p3 I
developed an image encryption and
decryption algorithm based on sera our
immediate processing marriage and I will
tow software I built a system that uses
that uses of the interposition
architecture that re engineers photo I'm
not Ian download protocols of existing
photo-sharing service providers any
metal sub-project I designer
domain-specific language and build to a
partitioned runtime across where devices
in the cloud for those are interested it
i release the medicine implementation on
google code this is open source project
an inertia i exploited application
delayed tolerance to design online
network interface selection algorithm
using option of analysis so this
underlying systems is deployed at Los
Angeles International Airport and other
universities and companies for more than
Twitter's and in addition when I was an
intern at here at MSR research group I
focused on the continued assessing
application and characterize the
workload based on the simulator and
actual measurement on two very different
types of processes so in today's talk
I'm going to cover the first two
Jack's in depth and I'm going to
summarize the other two pieces of work
at the end of the top so here's the
outline of the top so I already
introduced my problem space and my
research I'm moving to the first part so
how do we how should we offload the
computation to the cloud to enable
demanding applications and why the
existing approaches now are not directly
applicable says I already mentioned
smartphone have sensors and these
sensors enabled a set of sensing
applications such as activity
recognition our center field monitoring
location-based services etc but recent
advances of computation sensing and
communication capabilities of smart
mobile devices create a new crest of
application that we called interactive
procession applications they are like
the other sensing applications in this
slide the mobile perception applications
make use of high data rate sensors like
comrades here are some examples of such
application so we use three prototype
the interactive perception applications
the first one is faced air-conditioned
application so at a conference for
example one-size people's faces to
immediately a kunai voice in the room
and second application is object and
post recognition application it will
enable augmented reality and third the
application is a gestural condition
application to control tablet device
using simple hand gestures to navigate
the dry though these emerging
applications have the following
characteristics so they are interactive
typically requires the crisp response
time in the order of 10 to 200
milliseconds and high data rate because
of video data this is real-time video
data and compute intensive because the
control vision based algorithms are
typically used so when we run these
applications on mobile devices we will
have significant performance problem so
to understand this two measures of
goodness will characterize the
requirements of interactive perception
applications the throughput is how many
many frames the system can process per
second also often denoted as FPS frames
per second and makes plain is end-to-end
reiten co will compute pipeline for a
given a single
frame which is basically a response time
of a given recognition task so in
general we are not achieve high
throughput and low may expand to verify
the how severe the performance problem
is here is one experimental data on
three prototype applications so each
application runs locally on mobile
devices and as you can sense from the
video on the right side this is to throw
to use so note that the number on object
impose recognition application in the
table is actually 10 times throw then
the video playing on the right side then
how do you solve this problem on
performance so fortunately these
applications are naturally presented as
a data flow graph as in the slide
suppose we have mobile devices on the
bottom and cloud infrastructure on the
top and there are kinetics through the
network so first technique that we can
use is offloading so which moves
demanding stages from the mobile device
to the cloud to reduce the execution
time and second technique that you can
use is using pero revision so by
increasing the number of workers 40
demanding stages you can further reduce
the execution time significantly and
additionally we can process multiple
frames simultaneously using pipeline
parallelism so given these techniques
our focus in high river is in the
context of computing data intensive
mobile applications which can be
structured as data flow graph how do we
design short it seems like mobile
devices that sounds are getting fast
like people don't know what to do is
like
okay same time i think so i won't just
that aspect solve the problem but versus
taking everything to the cloud oh so can
you rephrase the kitchen on i'm sure i'm
not sure right 10 inside the device
right i'm saying you guys are getting
faster you know you wait a couple of
years and then you can run this
application on the device itself rather
worrying about this restructuring of the
app itself know which actually has some
fundamental to mix around how far the
cloud is ok I think this perception
application the three postal application
may be easily enabled by the future
maybe three to four years of mobile
devices but I think people will create
multimedia application with higher
accuracy always exceed the capability of
mobile devices and there is a problem on
energy always so I think there is a need
there will be a need to use the cloud
whatever the device reforms that's my
opinion is that the answer for your
question
so yeah given these the application
structure the data flow structure how do
we design the underlying system that use
of rhodium palladium techniques together
to enable such applications this is the
height of a focus of this work to enable
the core two to achieve the wars to
these three fundamental questions need
to be answered the first what factors
impact off roading imperialism between
mobile devices in the crowd and second
how do you improve this route food and
make Spain simultaneously again by using
off-roading and paralyzing techniques
together and third how much benefits can
you get compared to other strategies so
total understand the problem space so
you measure the workload and identify
that there are not so variability in the
system the graph on the right side shows
the resort own object in post
recognition application that sex is a
frame number and y-axis is a number of
sift features detected on the button and
make span values on the top so next
values the end to end the computer
agency of an entire pipeline yeah for a
single frame so the upper graph three
issues may explain values and actually
there are huge variability in seen
complexity in input which caused
significant fluctuations on my spam
values the whore example if we see the
frame number 200 the maximum very is
relatively low because in complexity is
relatively moderate but if we see the
frame number 300 it has much longer may
expand because of more safety features
on the photo facing not Indian it's
should turn to read a graph you're
seeing about 12 seconds of latency per
frame
yeah in this application everything runs
locally it is very very strong tyra show
how my system improves the performance
to our usual ever great but looking at
that red line just that would say 12
seconds to get your right back and this
is very complicated very complicated
what did you say about it
we go every stage is runs rocker on the
phone okay so that's why this huge may
expand happens that's just what the
capability of the phone which phone did
you use for experiments so it was a
netbook at the time 1.4 you got hurt
Singapore yeah if you know the galaxy s3
has quad-core 1.5 gigahertz CPU now but
at the time you don't have this Bo
buddies so from this we learned that the
system should adapt to the input
variability at runtime because of huge
variability in input and in addition to
input variability we also explored other
domains which can affect the performance
the additional domains include the
different mobile devices and network
condition and different choices of pipe
parallelism so all these additional
domains in core more variability to the
system so we conclude that or
off-roading and para regime decisions
should be adaptive to input and platform
variability the from the lessons from
the measurement we design the Odessa
runtime system and let me give you a
high level description of Odessa so
let's i saw one time built on top of
sprout sprouty is a distributed and
parallel runtime engine developed at
Inter all the sizes are mechanisms
provided by this product unless the
wrong time is mainly comprised of two
components the application profiler and
decision engine and this application
profile will deliver sequencer
statistics to the decision engine using
lightweight piggybacking mechanism and
thereby decision engine can adapt
off-roading anti-regime decision to
improve throughput I may explain
simultaneously so lets us decision
engine runs on mobile devices this means
some part of data flow graph will be
placed on the cloud if necessary our
computer stage might be offloaded from
the mobile device to the cloud from the
crowd to the mobile device and it can
also spawn more workers for demanding
stages
then how the decisions are made this is
obvious question let's look at how this
makes this decision so when the
application starts the entire pot is on
the smartphone so easily return for this
framework or are you doing all this
automatic I will say we take for this
frame up the application developer
should provide the data flow structure
to the runtime again the smartphone is
out on the bottom and cloud
infrastructure on the top layer
connected through the network so based
on the profile data the decision engine
knows this stage is a battler and then
in estimates the migration costs and
expected execution time on the cloud and
no further stage only if the remote
execution cost is less than local
execution cost and after that the
visitor engine again identify the stage
be as a bachelor and off-road it and
after that it spawns one more workers
for stage be since the stage B was the
slowest one steer and it at some point
network edge could be a bachelor then
this is the most estimates the
off-roading possibilities on both ends
and takes the relevant action so in this
particular example the system i greased
a distinction stage to the cloud so
overall this as decisions are
incremental and so it adapts quickly to
input and platform variability so before
talking about the performance research
here are actual data flow graph are four
hours three prototype appreciations so
these applications runs on top of odessa
yes yeah quickly usually deal thanks
kill baby so our digital engine for the
decision-making only 22 less than two
milliseconds so this is quite quick with
so we can you know frame labor
how many frames would it take for that
it happened so it depends on the
parameter setting we see we actually the
profile the actual execution time of
every frames using our profile engine so
we set our window size as 10 so see the
statistics for the recent tell frames
and makes the decision so to finish a
adapter on this finger understand does
it mean that right now on Wi-Fi I woke
up water the building I'm not good Wi-Fi
anymore and on 3g now that pretty much
immediately switched the phone que podem
you switch the local processing yeah ok
it might bring back the complete stage
back to the euro co device how long does
that take because you have to figure out
that or you know the agencies are
increased yeah we see the lessons ten
frame statistics so usually it depends
as long as we don't lose the
connectivity within five to eight frames
we can prepare that you can see all
these applicants have applications have
varying structure and different number
of computation early depending stages
what would it be so this is a lot of
complication how difficult it be for me
as a programmer to go in and add
annotations to say if if the latency if
you're on 3g run this locally if not
from this
or vice versa 48 a handful of stages I
guess the question I'm getting at is how
complicated are these applications and
do they actually require your adaptive
mechanism celebrities in develop oh I
don't need not to know the these
dynamics they just provide these are
overly fine-grained the data flow
structure and the wrong time takes care
of the rest automatically your standard
but my question was how complicated are
each of these pipeline stages could I
give is an expert developer maybe I
don't want to use your framework could I
come along it just may provide those
annotations to use this for underlying
scrub framework to do this statically so
I can tell our performance with domain
experts later but yeah the domain
experts cannot know the every decisions
correctly actually this slide shows the
Detroit computing so our main the
problem was performance so let's compare
the USS performance with other
strategies so we compare Odessa with
three other competitors as well as one
imaginary strategy that is optimized by
off right method the rocker runs every
stage run locally on mobile devices and
off-road or runs the stage that reason
is e frames as well as the stage that
displays the result on the mobile device
and all other leisure or other stages
will run on the cloud and domain
specific uses the partition size to buy
the domain experts which is the
application developer in our case and
last competitor is a friend optimizer it
basically exhaustive research every
possible partitions and take pics the
one that gives the best result so sensei
the computation required is too
expensive and they require statistics on
all possible partitions it cannot be
done online what has to be done of rhyme
to remind you this throughput is better
the bank span royce better yes there's a
lot of questions about what this data
represent you this means your medians
register bench work
it is the fraction of time spent in
different network conditions oh yeah
this resource is on object to impose
recognition application and we use the
best quality network hundred megabits
per second I talk for this experiment
and we use the 1.4 gigahertz CPU the
netbook as a client and for this
experiment we run the object tempos
recognition on the mobile device setup
at the beginning and wait until the the
petitions is saturated and then see the
average the frame rate average
throughput and I will you may expand
after hundred frames to the end with
that actually animal a bandwidth wise
and after all going to give you the
highest range a shocking
a fraud or why the old photo is not
going to the heist throughput that
Scipio question and Louis yeah if you've
got effectively as your own latency or
very low latency network with unlimited
resources and
got 100 megabit never why is that why
would that have a lower bridge per
second and something that
ok so the why do desta performs better
than off-roader and domain-specific
technology that's the the abstraction
question for of your DIC ory the reason
is I think two things one is parallelism
choice and the other is devoid the way
of partitioning the application across
to the available resources for
parallelism the domain-specific makes a
wrong decision the in terms of pipeline
parallelism domain-specific the
application developer doesn't know what
is the right number of tokens existing
on the pipeline so how many many frames
the system should process simultaneously
that decision depends a lot of on the
this device capability so it should be
based on the actual profile rather than
just fixed number at the beginning so
that's one very crucial region about
this performance difference and the off
road or the amount of data parallelism
is was also important it uses just a
single the detection stages single a
convention stage while the
domain-specific an odd SI uses multiple
stage for such demanding States recently
and so for example Hank I don't fully
understand what the stage is do so I you
change and y para la wasn't enough
basically yes so hope you see it's fair
to say that after all which implemented
early or so for all or choose wrong
number of data per origin and pipeline
terrorism so for example the object to
impose recommend application there are
three demanding stages fifty-fifty
section modem matching and clustering
but you know domain-specific and
off-road off-road or especially they use
just one instance of safety feature
extraction a one walk or thread for
fifty fifty restriction clustering and
mother matching so that caused the huge
performance difference and the other is
pipeline parallelism so in the given
pipeline you know that you have ten and
drain pipe stages computer stages and
designing the right number of frames in
the pipeline at a given time is not
revered decision it should be based on
at your profile data and so on asking is
if all the trek or all the changes
you're talking about an Odessa don't
sound like adaptive they might be sound
kind of like better programming
or better use of the data and if those
same things could happen in offload at
all what off would all be as fast as our
vessel or is there something going on
sort of almost in real time and adaptive
which seems to be the special thing
about Odessa but that which is making it
different than a little
so what your doesn't seem to me like a
like way
so he doesn't seem like an optimal
parallel like if I if I figure the cloud
is free and I just want to burn as many
resources as possible you would presume
I would go for the maximum level of
parallelism and just burn the heck out
of the CPU units in the cloud and if
that would seem like one of the naive
strategies I want to compare to it might
not be terribly efficient in terms of
use of CPU resources are from throwing
out state that would be useful in a less
parallel situation but that would
certainly be maxing out parallels would
seem to be the fascinating the maximum
frames per second so the one problem is
they don't know what they make what the
right number of maximum para la Rosa
means for a given environment that's one
thing so let me show how I decide the
pipeline parallelism after all is that
is that something that you implemented
or where you're using some other system
they just you know was offloading this
so usually machinations provided by
sprout so they provide the basic yeah
off roading structures so yeah let me
show
um
I think left rasterizer depending on the
application since it is adaptive to a
given environment so the degree of
pipeline perigee should change
accordingly right but the domain exports
and this overall strategy cannot know
this right number of degree iframe para
region as well as so in my experiment
the off-road or doesn't use the maximum
degree of data parallelism that's why
the performances to row so the set so
which kind of set a fixed low level
whereas your system will very
dynamically and will go higher level is
proper
what if you just picked a higher level
own with that I suffer badly error hi
Oliver what with our local parallelism
in the absolute all case then the the
makespan your software because the poor
consecutive frames will wait before the
padlock stage so we need to be careful
about choosing 0 degree of pipeline para
q Podesta is doing three different
things you described so far in terms of
improving this one is deciding which
pipeline stages to upload to flow to the
cloud okay secondly is deciding what
degree of parallelism to put it each of
those stages right and thirdly it's
making adaptive decisions about those as
conditions change right can you give us
a break down for this example you've
been showing us the one of them the
previous slide before you jumped here
what are those things matters it seems
like in particular the decision about
which they just upload is not relevant
it's not the reason that Odessa is being
awful at all is not because there's some
stage it's really important to do on the
client I'm guessing and if you're wrong
can you give us a breakdown which of
those things matters in this example or
anything example he's gonna head before
that chart
how's everything
so this is an example the so let me show
the resulting partitions this is the
result own object tempo spread function
application the resulting petitions are
something like this so there are three
demanding stages but the usf road only
two stages and increase the data
parallelism like this and control the
pipeline paralyzed more so the notable
difference is it executes clustering
stage locally so the Odessa can use the
more resources on the cloud for the
other stages I compared to a fraud or
the domain specific technology which may
use the maximum data parallelism
whatever we set then may be wrong right
but adesso uses the necessary
computation resource locally and utilize
the clouds that resources more more the
right way it's actually optimal here
doing for network round-trips rather
than taking that middle stage and
pushing out to the cloud that the
performance would be worse with that
little blue rectangle there who pushed
up under the cloud depends a lot on the
Mount of data that will be transferred
between the stages so in this possibly
be worse English so actually Butler to
my Odessa algorithm is works based on
the padlock stage so I measured every
stage with execution time so it's action
time of blue rectangle and insufficient
the delay of the every network edge and
in second time of resurrecting or so and
try to reduce the execution time of the
battle a link or stage so that make sure
this is better than the other positions
right
I'm asked if I'm having skeptical of
this result oh this number is if you
push that rectangle if you push that
blue rectangle up there you're going to
save to network round-trips sees me here
is a 1 network round trip to network ops
and you're going to be able to use a
higher performance score up in the cloud
than you are download on your client
machine it's it may be a small land but
it's got to be a win in terms of
performance the text depends on the
congestion on the cloud side right so if
you off road the middle stage on the day
on the top maybe maybe performance is a
little better but disrupted is governed
by the battle exchanges 2 so 4 we not
increase right so so the I'm trying to
optimize both they may expand and throop
the simultaneously whatever the cost is
offered this single stage to the cloud
may not increase the throughput at all
because but Alexis in 2nd time Rick it
surely will not hurt it it would not be
any worries at me well I'm not saying
the mighty shorty partitions is globally
optimal I just increase the post metrics
simultaneously keep going on so you are
some journey that the cloud itself could
be resource limit at some point yes
compare is movement here is a commission
sport professed his own question
no questions let me get back to the
resort yeah oh that's I'll performed
against three other competitors and even
compared to the run optimizer it gets
compared with Rupert and order different
there are considerable amount of related
work in this space yeah first cerebral
approach is using the integer linear
programming and second set of approach
sorry it's based on graph-based the
positioning method to optimize custom
utility function and third approach is
using static partitioning scheme the
application partitions will be
determined at compile time and force
ones are switching between priest face
value partitions either by the
application developers or domain exports
so these are not providing the level
solution for hours because the
objectives are different and because of
huge variability so static or fixed
partitioning schemes will not work and
none of these considers the
parallelization of demanding stages on
mobile devices so that's a so Odessa
achieves our core using the incrementer
in dynamic runtime which adopts
imprudent platform variability at
runtime the summarized Odessa some
emerging applications are too heavy to
run on mobile devices so design enables
interactive precision applications by
dynamically adapting to the input and
platform variability so I'm moving to
the second piece of my work so when we
just enable the mobile perception
applications so when you want when you
want to share the raccoon eyes result is
in the cloud we may have a privacy
problem so this work is about how to
protect our privacy ensuring the photos
so cloud-based photo sharing services
henceforth psps are becoming very
popular nowadays the people uses various
mobile devices to share photos and
unload it two psps using wireless
network but we either have cd
as privacy concerns here is an example
suppose Ellis has a sickly picture of a
nice guy and want to share it with
friends using PSP first possible concern
privacy concern in this situation is the
unexpected exposure always just photo
which could happen either by accident
are bugs or Kelly system design by PSP
the second problem might be we don't
have any mechanism to prevent psp's data
boots so in this particular example the
people may use their best possible
inference algorithm on you just photo
and may control the following so this is
obviously not all desire visionary or
for Alice but currently there is no way
to prevent this scenario we need to
completely trust PS PS in order to share
our photos so I was not making up at
official threats but they are real ones
here are four recent news headlines the
photobucket system unexpected it exposed
the users photos because of their
nervous system design the problem was
their photo URL was too easy to guess
thereby do tech what the attacker needs
to know is just a user ID and besides
the facebook had a face recognition API
in their web based api data api
specification but because of privacy
issues partially described in this right
they eventually shut down the again and
not a long time ago the Instagram try to
change their terms of service saying
that they can serve those photos without
having data owner's permission it caused
the big raucous so the company devoted
back to the original top so these
privacy concerns are real too many users
so on the other hand PS PS provide the
used for processing for mobile devices
again suppose Elise has a brand new
smart camera and takes a higher
education photo and I'm not lead to a
PSP then you know and the whore the
alesis friend may have a mobile device
with different screen sizes in order to
provide desirable user experience the
PSP sphere scare to image appropriately
and send them to the different mobile
devices
the dis types of processing so-called
immediate scalability service are very
useful for users to reduce network
latency and then this is significant
trick also it is possible that the psps
can perform other types of processing
for example filtering operation to
enhance image quality so the cloud is
already doing useful processing for
mobile devices and people get tremendous
benefits from them and the problem is
that we all have both privacy protection
and cloud side processing so solving
this problem especially under practical
constraints is quite tricky so may
immediately think that as a potential
solution why not just encrypt everything
but if the photo is fully encrypted and
the cloud psp's cannot perform any use
for processing on you just photos so as
a result for example the mobile devices
should download full resolution images
regardless of their screen size and
storage size limitation this is
unacceptable so if you use for
encryption we lose images scalability
service as well as the other benefits
provided by the providers and once you
do we do the before describing our
approach so I will describe our course
stream motor and assumptions that we
made so again our core is to protect
users privacy with the cloud side
processing and our strap motor covers
two categories reps so one is the
unauthorized success and the other is
the application of automating
recognition technology on youjizz photos
in our trust boundaries in between
mobile devices in the cloud which means
that we completely trust mobile devices
hardware and software those include
senses operating system codes and abs
etc and we don't trust others including
eavesdroppers on network and PS PS 4 PS
PS we assume that they are honest to god
curious so which again means that they
will not change what they are doing for
you just photos no matter what
but they will try to infer you just
possible information using their past
best possible method so now I would I am
describing the our approach in Hydra
bear so again suppose at least one to
share a photo with pop from the photo we
first extract small but has very
important visual information which
record a secret pot so one can think it
as I one can think it as the most
significant significant significant bits
of the entire image and I will describe
how exactly we consider the secret part
later after this ride and after removing
a secret pot what remains has a large
volume but has little visual information
which cheek or public pot and this top
again one can think it as least
significant bits of entire image data
and public part is distended jpg image
so the psps can accept it without
changing their system so ensuring the
sickly pot will be encrypted and ideally
embedded inside the public pot and then
image will be unloaded to a PSP and this
way this piece can perform any
processing and useful processing on on
the public part in this particular
example they scare down the image for
serving the mobile device when Bob wants
to see the photo it download both public
and secret pot and combine those two to
reconstruct the image to enable this
capability we have several important
requirements see our our algorithm
should ensure has to be as to ensure
privacy on the parrot pot and storage
overhead should be minimized our
encryption and decryption process should
be a lightweight and our public party
should maintain on standard compliancy
in our case jpg image and the cloud
should be able to process the public
part appropriately and the resulting
system should transparently work with
the existing PSPs so overall the our
algorithm system the collectively called
p3 achieve discourse and requirements
and I will describe why our system and
our
reading books in later thrive yes same
processing should be enable you assume
what what they can do it to your data
how you describe it babe so before
describing actual encryption and
decryption algorithm I want to share the
intuition behind dp3 algorithm so how do
you extract small body important
information from the given image so in
this work we focus don't know idly amid
use the image format jpeg image
compression standard so in jpeg when
compressing the image on images divided
into many small patches the size of one
patch is a pie eight on these patches
the JPEG performs DCT the discrete
cosine transform then each location is
left grid corresponds to different
frequency values so if you draw these
two grams of our coefficients from the
or patches and image they will looks
like this in each histogram the center
position have 0 values denoted as blue
line here then the first fact that you
can exploit is that the DCT coefficients
of detrol images are sparse so in
general more energy is concentrated on
the top left corner which has low
frequency values especially zero
frequency values are called DC
coefficient DC component and has
significant visual information and the
second thing that we can use is we see
that designs of coefficients are evenly
distributed because the histograms are
mostly symmetric so if we take out those
values is very hard for attackers to
correctly recover the values and third
certainly magnitude of coefficients have
some information so to exploit this fact
p 3 takes all three components out to
degrade the boring part as much as
possible now I'm ready to describe how
p3 encryption works from the given image
we get quantized dct coefficients first
we take out the DC components which has
significant visual information and four
remaining HD coefficients we cut their
magnitude using fixed threshold
ki then each cropped regions are treated
separately with their signs so in a part
of the coefficients will form a public
pot which become another jpg file and
will be stored and processed by a PSP
system an outer part of the coefficient
will be combined with dtc components and
form a secret part which has small size
but has important and significant visual
information and the secret part will be
encrypted when it goes out of the mobile
device so note that this office pre
eliminate three important component that
I discussed in the previous slide TC and
magnitude by thresholding and science
taken for a secret part the next
question is now we have this history
encryption algorithm so how where these
are growing works in practice so I
implement this algorithm and first
reader to show is threshold versus
storage trade-off then this yes when you
cut the threshold in this higher than
the tracker
story or even in the Delta first of all
there's ample remove the special and
remain this threshold in the public pot
so we messed up the sign information
so this graph we applied our algorithm
in India dataset which has 1491
different images an x axis of this graph
is 50 threshold used and y axis is
normalized average fire size compared to
the original image so naturally original
sizes are all one in blue color and
secret parties in red and public parties
in green and some of the public and
secret parties are inbred the results
are very encouraging the even if you
consider the worst case the total file
size increases only by twenty percent if
you see the individual fire size the
size of public and secret parts are
almost even at threshold one after that
the volumes are moving to the public pot
as we increase the threshold so based on
this resort and the privacy valuation on
the public pot we set our operating
range the p-series operating range as
one to 20 then the next question might
be what information will be exposed in
our operating range in the public part
so I just one example image from the
accp data set which has some the
canonical images so when you set the
threshold as 20 which is the strongest
privacy setting in our scheme the image
looks like this so if you are familiar
with the image data did you SECP their
asset you may recognize some structure
here but depending on who you are what
may have a hard time to recognize what
is in the image if I decrease this
restaurant the image becomes more secure
this is 15 10 5 and 1
so if you set the threshold as one the
visual rear is almost impossible to that
one has anything and for your reference
I represent the original image it's
reflective and I will present a secret
part with your threshold as I increase
the threshold this is secret part with
sort 1 5 10 15 10 right as i increase
the threshold the naturally less
information will remain in the secret
part and more more volumes will go to
the poor people so we have seen how
history increase the image and its basic
trade-off yes adversary who's trying to
return this information is useless
I just a standard
so it might be possible to create more
initiative
try try to do sit Oh answer the question
we have we don't have the mathematical
proofs of our security energies in the
evaluation i will show our the
tribulation methods we use automatic
recognition technology and so on I mean
Fisher in particular in krishna says
there's a line of work out looking
English statistics in hybrid statistics
damages and detecting things like
modifications damages and all that work
works because there are pretty strong
structure there's strong structure and
natural urges and I'm wondering whether
the my intuition is that you've won can
look what complies similar techniques
here and your priors about the
relationships among nearby pixels and
wheel in natural images and recover a
lot of natural images from not very many
bits before these LSP that's it
I think I think that's I think I'm
thinking about the adversary your
probable it matters a lot because how
much because because there's so much
because it makes us strong assumptions
about the about the set of images it can
be the can can make it through your
filter right thanks for coming but okay
yeah we haven't tried such the immediate
forensic techniques in our scheme yet so
we wish OD how history encrypts the
image and its basic trade-off then what
about the decryption how about the
decryption so for decrypting decrypting
the image we are facing one very
interesting challenges because of this
cloud side processing suppose again via
least one to share a photo it Bob since
the public part is told and processed by
a PSP system the receiver will get the
unprocessed the secret pot with a
processed version all probably part then
the challenge is can you reconstruct
three can you reconstruct the prophet
processed version of original image
using the given information on the
receiver side so if you can express the
original image as a linear combination
of security and public pot so this
problem becomes more straightforward but
this it is not the case in our setting
because we our PC our encryption
algorithm hi is designed information
from the public part then how do you
solve this problem so as I mentioned the
original image is not just a linear
combination of security and public pot
so it turns out that the correct
equation for the original image must
include the compensation Thumbsy and our
analysis result shows this see this
compensation MC can be derived from the
secret part as which we already have on
the receiver side therefore we still can
hinder any linear processing and for
photos this linear processing can handle
many useful functions the scaling
cropping sharpening blending smoothie is
that the answer for your question before
so based on this PCB encryption and
decryption algorithm we design a ps3
system that can transparently work with
existing PSP's so peace retakes on
interpolation architecture the really
requires trusted man in them in a proxy
on the device on the mobile device and
as well as the crowd side the storage
space so it would be ideal if we can
store the secret part together with the
public part onto the PSP system and the
JPEG standard does allow the embedding
application specific information into
the binary but in reality most PSP's
will eliminate this application specific
information when they receive the photos
from the users so we take this
alternative approach based on the
external storage space so the on device
proxy will perform peacefully encryption
and decryption we need arose or download
the photos and crowd aside storage space
will store encrypted skripsi crippled
and poverty part will be stored and
processed by a PSP so PC architecture is
very easy to be implemented with
existing PSP's and we don't require to
change the PSP infrastructure so we two
copies of the same image
and then if I use the same threshold and
then the public park should be the same
right right so and i'm certain that
public park is also kind of since it's
sparse it's kind of you can use public
part as a time signature of an image
okay Oh naturally what we are yours in
the attack canary all right let me know
tanker or whatnot right so let's say I
have an image of I don't know Justin
Bieber and then the public part of the
image right so let's say there even if
there are so I don't know hundreds of
millions of images out there if the
property part encodes kind of unique in
unique bits are corresponding to an
image then just annoying the public part
you can identify what a original image
if you access to secret pot that means
you are my friend is having the public
part you can almost guarantee that you
can identify you can map to an original
image definitely originated you also
feel burnt food we dig up seven theories
about the one deciding if you have to
trust the software the hardware the
whole body wise move out less impressive
the user is no one's playing their image
music likes it I want to find out who
has posted the picture of Justin Bieber
right for instance then I have a picture
of Justin Bieber I create a public court
then I can just scan the entire publicly
available images and then find out that
which public part of the image is the
just universe the quarterly part of the
justin bieber's doesn't you're saying is
like if they're identical images it hash
the public part is a hash of the image
right right and then this looks like
this public park since it's very sparse
it looks like you know could be very one
on one mapping that many to one mapping
right country nice King doesn't solve
your problem i assume kind of this image
is not publicly available all the time
but i think one way to address that
problem is where we unload the public
part to the PSP we may inject a random
overlapping images you may add random
overlapping images depending on the
user's then may hide
capability to reconstruct the original
image with this sacred word if you
inject so difficult if the receiver
knows the yeah about yeah you've buried
point yes and for the setup Constance
record you have some randomized
reference and encoded padded the private
party
for this first
what is that this isn't really
encryption as much as reversible
obfuscation because encryption requires
we're encryption implies that you have a
key and if you're not the person who has
a key you can't under most definitions
of security for an option it means that
you without the key you can't tell if
the encrypted message is the encryption
of a given plaintext but in this case if
you have a if you have a plain text you
can tell whether this is the encryption
of that plain text
so let me move on to the next spot so
actually implemented all necessary
component on their devices and with
Facebook system this prototype is built
on top of the one of the latest
smartphone the samsung galaxy s3 enjoy
the screenshot on the device and the
delay numbers on the device or so so
receiver without relevant password or
relevant key we're basically see the
gray image on the right side depending
on the threshold it may change a rather
than the original image on the left side
and also delay numbers are moderate so
the pc is practical and can be
implemented with a real system like
facebook let's say two other friends a
new poster this image under facebook
that you want to share good lyrics are
the shading work in that case because i
should be able to see you that you're
different all right so what do i need to
so you need access to the frames graph
from facebook a vision right right and
you need to disseminate this information
to make the secret part the main and
somehow there needs to be some layer
which combines the story all right so
that i described in our system in the
previous right so this rayo the p3
trusted yeah that part we will do
encryption and decryption
and extensive evaluate P threes privacy
aspect using Kevin R &amp;amp; R identity of set
of is computer vision algorithms
computer vision based algorithms
essentially all results are saying that
PSN air is so low and all these
recognition technology becomes useless
with the public pot so please refresh
our privacy so in this talk I'm going to
show the two results ecchi detection and
face recognition the for edgy detection
technique the first digit is on the
education typing we applied Kenny G
detection on the public pots so these
images are from New CCP data set the
frequent canonical images if you use
threshold as one and apply Kenyan tea
and agitation technique they will look
like this it's almost impossible to
recognize anything but if we increase
the threshold to 10 David looks like
this so again if you are familiar with
the data set you may lacuna is
especially in the mirror the image but
still how to recognize is something
under the right side if you increase the
threshold to 20 which is the wiki
staring day looks like this ok connect
in the next slide I will present the
original media streaming images and
together with the Kenya chief detection
result from the original image so there
you look like this
and the second order to show is on face
recognition so we use eigen phase
algorithm with the colorful face
database for the evaluation and we use
Colorado State University's face
recognition evaluation system which is
basically devised for evaluating the
comparing the different face recognition
algorithms so we try we examine the
recognition performance under various
settings so different proving set which
is in the database and different
distance metrics and different pistol is
resort used and public paths as a
training set as a new training site for
limiting the advanced article here is
the resultant is economical face did you
process them through the seminar organ
on you had this is go back to the rails
or you keep the original place as the
so I I tried both so I'm going to
present the resort on the latter case
public pot also trained defaces using
the public part of the training set and
yeah so here is the resort on the worst
case so x axis is a the recognition rank
and y-axis is a community with a
commission rate I am following the
methodology provided by the ferret
database community and the upper upper
rhine uses the normal training and
public domain training and proving set
and the below two lines uses public part
as a training info results so each line
uses different threshold the green line
uses training and red line uses one the
first if we consider this point which
gives the best recognition rate from the
perspective of all of an attacker so
which has the 50 rank 50 and about forty
percent recognition right so intuitively
what it means is that for an unknown
phase there exists the right answer
among top 51 candidate faces with forty
percent probability right so if we just
consider the top leg commission rate
which is those two points the Green Line
has about fifteen percent higher
commission rate and the red Ryan has two
percent recognition rate so note that
even if attackers get the fifteen
percent of commission right she already
have she only have this public pot so it
would be very hard to verify but whether
the rigid 330 is right or not so other
results using different threshold and
the normal training set shows the worst
recognition rate then the green line
here so overall the face recognition is
broken terms are not used for for our
purpose the video consider want a
relative on this space again so three
homomorphic encryption enables auditory
processing on the encrypted data but
they are too expensive to be used in the
high high dimensional data like photo
and video and they require to change PSP
infrastructure student
and the second walk second set of work
is on the privacy on video surveillance
literature so they do masking bloating
exhalation and scrambling coefficient
etcetera but they are either Frazier to
recognition techniques or the increased
fire size too much and first out of the
third select there are considerable
considerable amount of related work in
selective encryption literature they
don't use for things but all these works
are done at in late 1990s at that time
they just focused on reducing the amount
of computation on the device so none of
these can hinder the flow requirement of
history algorithm for example the
recursion challenge due to the cloud
side processing none of these existing
algorithms contender Soapy's three is a
kind of selective encryption algorithm
but a unique one tailored to de novo
requirements so summarizing p 3 the
cloud service providers already
providing useful processing for mobile
devices mp3 protect our privacy against
providers while maintaining the crowd
side processing this definition for
useful on the web site processes what
operation
um as I described or linear processing
that we can have this money on the train
so let's say you're talking to mark
zuckerberg and you have this technology
where if you need to spend a couple of
million dollars in preventing it so that
he gets less greater than ninety percent
of the users are enabling give him what
would your argument to any of these
providers be for providing any of this
stuff so what is benefit over the case
we provide this technology or what are
the incentive for any cloud provider
computers so right t we kind of assume
that the PSP's will eventually cooperate
to our ski the argument for this is for
facebook for example that they may want
more users so there are privacy concerns
users who is very reluctant to use this
kind of sharing environment then they
facebook may device or paid these
services for that kind of users to
increase their user base right that kind
of argument that i have right now we
have we don't i don't have these
concrete numbers but there are you know
in dust in this space there are many
startups nowadays so actually soon yeah
certainly some people are interested in
this direction
so we have you come in two examples of
how we enable efficient processing and
secure sharing of sensor data is in the
cloud so now I'll give brief overview of
other two pieces of my work and control
the top I also explored the other
interesting domains so the first there
is an emerging demands on Roger scare
sensor data collection and processing
from the corpus of smartphone users the
crowd sensing is a noble capability that
combines power crowds with sensors on
smart verbal devices the main key
observation here is that there is a lack
of support to automate this labor
intensive task so I built a high level
programming framework for crowd sensing
applications now the users can just keep
a high level description and the runtime
takes care of the rest automatically and
second whenever we share raj volumes of
sensor data is in the cloud we will have
a energy problem energy concerns the key
observation here is given the delay
tolerant mobile applications existence
of multiple wireless network interfaces
and time-varying wireless network it may
make sense to defer the transmission
opportunity rather than sending it in
here to DVD Utley so I designed online
algorithm that governs this transmission
decisions and the algorithm called zarza
can effectively trade-off energy and
delay by intelligently deferring the
transmission opportunity so I'm
summarizing my entire work in high
rubber so now we have Odessa to enable
the mobile procession applications which
is data and compute intensive workloads
you'd p3 we made already step forward to
protect users privacy while maintaining
their crowd size processing and with
Medusa we enable the largest scales and
so there are collection and processing
from the smartphone crowds a new salsa
so we can effectively trade of energy
and delay when using the delay tolerant
mobile applications so at this time I
want to thank my collaborators so
without their support I may not be here
as a canted today so finally few chalk
so in the future I want to broaden my
research horizon and make our personal
computing environment
more efficient and secure so i
categorize my future work as two things
the first I'm interested in building
infrastructure support for mobile
devices in the future so which may which
includes the essential primitives for
the mobile devices like the location and
notification services also making the
mobile systems scalable and privacy
preserving the second I am also very
interested in making multimedia data
sharing and processing secure and
efficient in our personal computing
environment the examples include deeper
with preserving video sharing and the
making the heavy processing on media
data efficient and secure on our
personal computing environment and thank
you I conclude my talk at this point and
I will be happy to take any more
questions
okay it was a little bit about how you
disseminate this secret data I'm a
little curious about that so you have
you know this public in public in the
secret but this can you tell us a little
bit about how you designate the secret
sure for this secret part of the image
that is going to the cloud side storage
when when you unload the photos on the
device you divide it image into two
apart and public party will go to the
Facebook for example a secret part will
go to the dropbox for you and then when
the receiver wants to see the photo he
download the public part from the
facebook and it also gives the unique
photo ID using the photo ID you retrieve
the secret part from the drop box right
then in that way you can reconstruct the
secret and public together on the device
ID and yeah that's how it is and for the
key we assume that the key should be
distributed over our event yeah
okay they don't have any more questions
k</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>