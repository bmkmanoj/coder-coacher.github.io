<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computational tools for biodiversity conservation. | Coder Coacher - Coaching Coders</title><meta content="Computational tools for biodiversity conservation. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Computational tools for biodiversity conservation.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7tpkWC9vcxk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
we good okay well thank you for coming
everybody
today we're gonna be hearing a talk from
Piero Visconti who's currently a
contractor in the computational ecology
group working with all of us really on
projects across the the portfolio of of
work that we do in the group he's gonna
be talking about what he's focused on in
the past and what he would like to focus
on over the next two years as a postdoc
in the group
it looks like were amongst friends so if
most of you I think know Piero if you
don't I hope you like this talk and for
those of you that are on his interview
panel thank you very much for taking the
time to to chat with him today and for
offering your feedback to the ecology
group at the end of that so with that
I'll turn it over to Pierre
Thank You Lucas so start off by giving
you some context about the work had done
in the last 30 months your Microsoft and
then I'll move on to talk about what I
would like to be doing in the next
coming months with a vision to towards
the next couple of years so to give you
some context about my work I start off
by talking about this so-called a new
geological era the Anthropocene defined
as the giorgia in which one species us
as altered earth system processes to
extent to which the actually entering a
new unstable state they are pushing the
boundaries essentially beyond the
stability that we enjoyed in the these
nine earth system processes during the
Holocene a group of scientists led by
john roxton defined these nine earth
system processes for example Commerce's
instability ocean chemistry land use and
others and defined thresholds tipping
points which if crossed will lead to
system stability for example for climate
they have identified co2
Tricia as a control variable and
identify 250 part per million of co2 as
threshold because according to global
circulation models if we cross this
threshold then we're going to see
widespread warming and melting of the
ice sheet in Arctic and Antarctic ice
cores which essentially will trigger
positive feedback to by law small bit of
fat and essentially more warming other
boundaries are for example the nitrogen
and phosphorus cycle concentration
according to recent estimates so we are
fixing from the atmosphere more nitrogen
days actually fixed naturally globally
and that is leaking through the soil
into the water and provoking widespread
eutrophication and essentially also
cascading into other system processes
just about your see loss because it's
actually causing widespread extensions
in lakes for example so we according to
this paradigm we are actually entering
instability but there is so much we
don't know about it yet and also this is
all about past observational models but
what we really need to know is actually
first understand better each of these
processes and also understand what the
future will bring about to it
potentially 10 billion people on the
planet in the next 40 years and so how
each of this binder is actually gonna
change in the future so not surprisingly
this group is actually working on
several these boundaries for example we
are getting better understanding on how
temperature is affecting co2 fixation
from plants we are understanding love
you are understanding better how plants
grow and how they use water and
fertilizers and this is important to not
only for the nitrogen phosphorous
boundaries but also for the land use
boundary here and the fresh water use
boundaries and my particular interests
among all of these is actually on body
where state boundaries barceló this
perhaps
Fazio one because although we know that
the current Exeter race is rate is at
least two orders of magnitude higher
than background Accenture rate perhaps
three order money desire we don't really
understand exactly what the implication
are on eco system stability and our own
and well-being we just know that this is
way more than what from foster records
we have observed in the past and so
that's for example my own work that I've
been doing in the last two years on
Accenture risk modeling which is now in
review and there is a lot more that I I
would like to be doing with you in the
next two years but let's give you some
more context about why I'm interested on
body Restless because I like birds but
an early because of that I'm generally
interested in understanding pattern of
biodiversity what are the implication of
a body Restless for ecosystem stability
and this has been perhaps one of the
scariest findings in science at
university understanding how much body
restless tourism going for example if
you look at this graph here this is
looking at the extinction rates across
different taxonomic groups and no matter
what the taxonomic group you look at
accorded to the IUCN the extinction risk
is increasing dramatically especially
for corals and amphibians corals because
of habitat loss and also bleaching due
to extreme climate and also because of
runoff disease again the nitrogen and
phosphorus problem there is affecting
corals amphibians primarily because of
habitat loss but also because of
infected disease are called the kidney
to mycosis that is affecting them
primarily in subtropical and tropical
regions the relative positive news here
is that for cadmium fairy animals the
ones that have received more
conservation attention such as birds
mammals the situation isn't as bad yet
again we don't know enough about many of
these species to be able to say what is
gonna happen to them in the future and
also I would argue that we need to go
beyond understanding what's happening to
them by actually we need to close the
so-called data to decision pipeline
from data to model super understand
species to actual decisions about what
we can do to protect them and that's
what I've been doing in there doing my
PhD and also that's what I've done in
the last 18 months since I joined
Microsoft and I'm gonna give you an
overview of the first two projects that
I worked on since I'm here and then some
ideas of what I've been lacking to do it
to be doing in the next coming months
with a sort of a longer-term vision so
let me start with this project
identifying global priorities for plant
conservation the relation for this work
was that countries are committed to very
bold ambitious targets by 2020 to
protect 70% of the world land and 10% of
the oceans in order to achieve broader
biodiversity goals such as the global
strategy for pond conservation which
called for their protection only 60
percent of plant species why plants
because they are good surrogates for
other species not only in terms of
spatial overlap of areas or I witness
and endemism but also because if the
autotrophs they provide essentially all
the services that any other species
needs including us for example oxygen
but also just food and so the question
that we wanted to ask is can we actually
achieve this goal which will require six
million a square kilometer of extra
black to their own land and 29 million
on water to achieve this other goal and
beyond that we wanted to really
understand exactly what are the pattern
of species richness and endemism of
species endemic species and live all in
one region and nowhere else
it's not that we're the first one in
ever asking this question there was for
example this paper in 2004 Norman Myers
identified the body versity hotspots as
the regions which also at least point
five percent of species globally as an
Amex all living there nowhere else and
also that lost at least 70 percent of
their natural extent since but since
pre-industrial conditions
and so these regions are they were very
useful for sort of raising awareness on
biodiversity loss and also they informed
a lots of conservation action is argued
that battery loss actually attracted
almost 1 billion dollar in investment
for conservation
however the caveats are several first
they are androne based on expert
information nothing against experts
about of course this was heavily relying
on subjectivity and whereas we actually
use more transparent and accountable
methods to identify priority for
conservation and also these are based on
outdated data fourteen years ago so now
there is more information more species
unknown most peas have been mapped
accurately and so we can revise this
information but also this is just one
snapshot of what's the sort of the core
regions are more important but we wanted
to be go beyond that we actually wanted
to investigate how can we optimally
accumulate species so if we start with
zero none of the world is protected or
considered for conservation and then we
want to build up optimal accumulation
curve saturating at the entire world is
pathetically protected we wanted to see
how can we actually build this curve to
maximize the number of species protected
in the least amount of area so that's
what we done that's what I've done by
using a greedy algorithm and I'll go in
there essentially was starting with zero
and then each time step was selecting
the next region of the world
either with the most species so when I
was optimizing for species richness this
black curve or endemic species endemic
species again is species the only living
one region and nowhere else so if we go
along this line essentially we start
with the region that has got the most
species per unit area and then we select
the next region that are the most new
species per unit area and then we gone
and then until saturation whereas with
the gray line we only count species that
would be only living in the sector has
been selected so far so only in the new
region and the region there have been
already selected and nowhere else so
these are more complicated model is
nonlinear
not be actually optimizing as well with
integer programming for example and so I
use a glue sorry global search are going
to actually look for solutions for this
and so that's what I've done a to
actually look at the optimize the order
of the combination curve what I mean by
that is rather than just looking at
increasing the amount area protected I
was specifically optimizing for single
area targets for example let's say we
want to know the best 5% of the world in
terms of endemic species what would that
be and I've done that with increasing
target from 5 to 30% and I found that
actually optimizing for species specific
targets actually aligned very well along
this accumulation curve that means
essentially that the greedy algorithm
that I was using that was actually
useful also to build increasingly large
areas of reserve networks optimally so
that also helped us in identifying
whether or not we can actually achieve
together that to our target 17%
protector than 60% of the species
globally protected and the answer is yes
we can get 67 percent of species of
plant species globally protected within
the best 17% of the world's we're not
suggesting to protect the entire Costa
Rica all these colored regions here
which are the 70% we're just saying that
this macro regions should be the ones
that would need to receive more
conservation attentions in the coming
years in order to fulfill both of these
targets and so the this paper received a
lot of attention and answered some
questions about what we can do about
large-scale priorities for a
conservation but we actually when we
zoom in and trying to identify exactly
what to do with in this region so where
to implement conservation actions we
need to think about extra dimension
just spatial button of our priorities
but also timing or priorities what to do
now what to do next
the reason for this is that conservation
agency typically do not just get I want
some one lump sum of money to buy land
for conservation but they need they
spend budget idly every year they would
have some part of funding to spend and
and so they need to think about what to
do now what to do next and in the
meantime I be the losses of course
ongoing and adequate ongoing and they
essentially reduce the opportunities for
conservation some of the land that we
want to protect may be lost before we
intervene to do that and also on top of
that there is uncertainty about what
would be available not only what will be
available in terms of is still extant is
still there and natural with the
landowner want to sell the land to us so
this uncertainty essentially calls for
dynamic optimization problem not just a
static snapshot about a scheduling
problem and so to give a practical
example let's say the Brazilian
government who wants to buy back areas
that are under forestry concession and
applies for funding and has got funding
from UN for example red schemes reduce
emission from deforestation and forest
degradation schemes that can give money
to the government to actually buy back
this first two concession and so they
need to do that through a sequence a
year and you can think of these regions
as a jigsaw puzzle where each piece
essentially is a different parcel of
land with a different landowner that
needs to be approached convinced to sell
the land and and the line then can be
protected but to be the to do that
effectively we need to think about
objectives what is that we want to get
for example an objective could be we
want to have three unit or leaders and
three butterflies and so if this was our
was not your problem you could actually
solve for it say that we have decision
variable a vector decision variables in
fact zero meaning the site is actually
both sorry zero is not bought and one is
both so in this instance we are ten
sites
we buy two three or four and then for
each to decide we have a state variable
which is a much we have of each species
and then we have set of species
objectives for each of them how much we
want this is a three for each and then
set asides so we can so we have
essentially a species abundance matrix
where we got species on the columns and
yeah on the columns and and sight on
each row and then essentially how much
we got of each species is the matrix
product of this vector and this matrix
there and so if we want to solve this
formally essentially we just say we want
to have at least all of each species and
then we want to minimize the overall
cost so how many sites we purchase to
get that we assume that essentially each
site the value of each site is
independent of the other spatial
connectedness or the so if we use for
example integer programming to solve for
this then that would be the most
efficient solution we need these two
sides to get three of each species at
the minimum cost but they said what if
actually this is done actively one step
at a time we don't go in lost of other
sites let's say we start with this site
we protect it then we get a Lockean
forestry company actually clear the
other side before we go there and try to
buy back there for a sequin session and
so then to get our tree butterflies we
need to continue doing this until
eventually after four time step we have
spent twice as much as we initially
planned to get over our objectives and
more importantly initially we wanted
these two sides and we end up actually
buying three sides that were not
originally planned to be both why our
change is important why do we care about
these three changes to different two
different reasons the first one is that
every time we actually approach the
landowner to do conservation and
a transaction cost involved for example
in meeting the person organizing
workshop meetings also just setting up
the contracts so that's time money and
effort that is involved so it's time we
change our priorities to time we
actually involve somebody else that
incurs extra costs but also our social
scientists have found over over again
that people that are brought in late
into the process of conservation tend to
have less buy-in and do not want to
collaborate so there is risk that if we
approach people later on in the process
after initial implementation of
conservation action started already they
may not want to collaborate so they
might not be willing to sell and so we
approach the person that they say no
sorry I'm not interested and so that
reduces further our opportunities to
achieve all our objectives so in
retrospect then given all this one could
have argued that these perhaps is a
better solution because if you try and
go for this incrementally starting from
with one no matter what happened you end
up always needing only three sides to
achieve your objectives and also no
matter what happened you always end up
having only one change maximum in the
worst case scenario the question is how
do you find this solution how we can
identify given that often time the
problem space is very large we have
perhaps thousands of different
alternatives we can just not workout it
by hand so I propose a a two-stage
process to do that first is rather than
just giving one solution give a family
of solutions to our global search
arguing and before I explain how the
genetic argument that I use work I'll
give you some sort of background on what
the fitness of each solution is so
essentially let's start with the random
set of vectors each one having values of
zero one again one being we buy a site
at zero we don't and here's our species
abundance matrix that tells us how much
we have of each species in each side so
if we do the maddox product of D so we
work out a much we end up protecting of
each species and we assume they're
essentially any other side that is not
protected eventually it's going to be
deforested so this solution will get at
least 40 know its species this one will
give us at least 20 then let's say we
have objectives of having at least 20
individuals so each of these five
species so we can measure the shortfall
so I'm not short we went around jetties
which would be the subtraction of these
to that so we we went short all of our
objectives by using solution one for
each species where a solution to
actually meets all of them we don't care
about exceeding the target we just care
about getting there so there is a
minimum of zero there and so then
fitness of a solution essentially is the
shortfall summed across all species time
five which is a arbitrary number that I
have set in this exercise to just say we
care a lot about chipping off target so
the shortfall is has got a high penalty
if we don't meet the target plus the sum
of the day essentially the cost of the
solution in the distance is three three
sites so it's a twenty four the sum of
this time five is 120 plus 3 which is
the cost of solution so desired solution
said there's got no shortfall so this
product is zero and then the cost of
solution is 4 so the Fitness overall is
4 and for Fitness the lower body the
better
so essentially genic are going to start
off by having a series of vectors of
with with zero one values that can be
see leader can be best guesses of what
works best for conservation according to
an expert or could be just a random
series of vector let's say we start with
a population of 100 vectors the fitness
of each of them is is evaluated and then
what the genetical going does
essentially pairs up vectors first thing
it does is select the best say ten
percent of these solutions the one with
the ID with the best fitness and then
these are promoted as they are to the
next generation of the
and then the remaining 90% is created by
pairing up these these vectors the
solution vector and then swapping for
like the genetic material essentially
swapping the control variables of them
with the assumption that essentially we
can explore the solution space better if
we criss cross different solutions so
for example in the distance this
children solution as you know that white
white alleles the control variables from
parent one and the gray ones from parent
too and then also to escape local minima
so spaces in the solution space the
point in the solution space that seems
to be optimal actually aren't we need to
increase induce some mutation
essentially some of the control
variables are swapped randomly from one
to zero to zero to one since it is the
idea behind that evolution is that we
build in new traits that maybe give a
better fitness solution and then we do
that several times say 100 times by the
end of it we have a population of
solutions each of them with the various
degree of fitness again measured as the
aisle cost effectively they achieve the
objectives so at the end of this let's
say we have 30 solutions each of them
achieve the objectives with various
bearing cost and we can map them in
terms of dissimilarity each of them I
would have would be ranked by the
cost-effectiveness of solution one is
the one that's most cost effective and
third is the least cost effective and we
can measure between these vectors at 0 1
that the degree of similarity so in this
instance that your customer ID index
between two different solution is the
size of the intersection of the two sets
so how many sites are selected by both
solution divided by the union of this so
divided by the unique number of sides
are selected the selected no solution so
it's one essentially two solutions are
identical and zero if they don't share
anything together so the idea is that we
can use the jacquard to create a
dimension
narrative matrix where the diagonal is
one so it's a solution compared against
itself and then each row is essentially
the abbe dissimilarity between one
solution or any other solution so let's
just do it with the first four sites for
solution sorry and you can see that
solution 1 is the most cost-effective as
I said Beringia and is quite different
from anything else
whereas solution 2 &amp;amp; 3 are almost as
efficient in terms of cost effectiveness
but also they are quite similar to each
other which means essentially that if
you try and implement these solutions
through time you may have more leeway
they you can swap things around and
adapt your plan through time by adopting
a new solution if that fails so it
should them it's just I use the term
interchangeably solution and
conservation plan because that's what
they are really and and so the idea is
that she selects this one years ok
that's the one conservation plan they're
gonna try and implement if something
doesn't come up available because it's
lost before you intervene you can swap
and move to a different conservation
plan and so the idea is that the
centermost
solutions in this similar space the
other ones are more flexible they've got
more different alternatives they are
similar to it and so what I propose is
essentially do combine these two things
cost effectiveness and the similarity of
these solutions to calculate this value
of V which is virtually the matrix
product of this vector of cost
effectiveness and Jaccard similarity to
measure the solution to identify the
solution that is cost effective and is
similar to many other cost effective
solutions so that if something doesn't
come up available there are still lots
of room to maneuver for conservation and
so I tested the policies of whether or
not is actually this approach works best
in terms of scheduling through time with
uncertainty fermentation by simulating
for 1,000 times stochastic loss of side
sir through the first station and
incremental protection one side at the
time
from this solution so one thing that I
should say is that people could have you
I could have for example used sigasi
dynamic programming to solve for this
the thing is that these assume that we
know the probability with which each
future scenario is going to be occurring
so the probability of the first session
essentially where as my approach doesn't
consider as all probability of loss so
if you don't know anything about the
sister if you don't know how likely is
each side to be lost so you can use this
approach so the way I went about it is
essentially testing this by simulating
ongoing loss of these sites and
protection and looking at how many times
also these 1,000 simulation the eventual
value essentially the achievement of the
jetty source was satisfied or not so are
many times all species were protected to
the objective that I set and how much
each of these costed that yeah and how
many sites was needed to achieve these
objectives and so this is one of the
results on the so the problem so I did
incremental problem from 30 to 100 to
look at whether or not there was sort of
differences in problem size but
typically will there words it could be
even thousands and so on the y axis I
have the expected return on investment
which is their number of times out of
this 1,000 simulation in which the
scheduling approach achieved all
objectives divided by the cost of the of
the solution how many sites were needed
to see the objectives and the next axis
is one of the variables that are tested
which is threat bias as I said I assume
that there is no prior knowledge about
what sites are more likely more or less
likely to be lost and what I changed is
the correlation between our variable is
one side for conservation so how much
more rich in species it is and our
likely it is to be lost so from minus
one meaning that all the most valuable
sides
for biodiversity the least one likely to
be defrosted to one which is perfect
correlation and so what I found is that
when you use of minimal such solution is
the solution essentially that is the
sort of most efficient one so in going
back to our graph my graph before in
which I had the rank from 1 to 30 would
be the solution number 1 essentially
most cost-effective
whereas flexible is the one that I found
that through my Magic's multiplication
on the jacquard and cost-effectiveness
so what I found is that the blue line
and the flexible the minimum set
solution tends to be better with low
threat bias meaning when the sides are
more valuable for bodywear cells so the
least likely to be lost
whereas they're flexible and then the
flexible solutions is that all are to
perform the minimum such solution for
the rest of the parameter space so for
most most of the parameter space you're
flexible the flexible solution actually
give a better return on investment as I
said the important thing is that TVA we
don't know really our track distributes
space and also this one essentially is
quite stable it's quite robust to this
uncertainty and this is one of the
results I've done that also with
different landscapes what I mean by that
is different species distribution
whether species were more clumped in one
region or more over dispersed and I
found pretty much the same results
across the board so the mean the
flexible solution was quite robust to
each of these uncertainty so this paper
just came back from revision with small
revisions and that's good news and
that's about all for what I've done in
the last 18 months and then Agoura
talked I talk now about some things that
I would like to be doing in the coming
months the first one is about optimizing
trade-off between economy by diversity
so so far I've been talking about ways
to inform conservation managers about
what they could do to protect
biodiversity because efficiently but one
could invert the problem and think about
what industries and government can do to
minimize the impact of what they're
doing and so the motivation for this
came from talking to colleagues
especially sadhya ahmed and lucas about
informing essentially government about
deforestation plans for the coming years
and so i started service more pilot
study last week to think to give a proof
of concept of what i would like to be
doing and i focused on the brazilian
amazon two of the primary economic
activities in the region's our timber
harvesting and crop plantation
especially soya and sugarcane and of
course these type of land uses are very
incompatible with biodiversity and the
brazil amazon is one of the richest
places in the world in terms of body
restive and so the idea is that we could
use timber timber market value from
saudis database she actually at the
model of different station based on
timber market value and proximity to
roads and so essentially we have an all
expectation of what would happen until
2020 based on present drivers of
deforestation and we worked out
essentially about 13% of all suitable
area for the first session will be lost
by 2020 and that would generate a
revenue about 160 million US dollars per
year from timber as a one-off sell and
then ongoing selling of crops and then
we have a birder banks mother from
senior board essentially it's a
abundance model that is function of land
use and land use and the news intensity
and so we can work out a much of the
individual complement of body Rossio
much of each species will be lost
based on the first station so what I did
is essentially look at whether we can do
better than their expected
deforestation whether we can actually
get the same amount of revenue with less
impact on policy and also less amount of
error deforested and so
the goal of this essentially to minimize
the important birds subject to we want
to have at least some acts of combined
revenue from timber and crops and I had
done it incrementally by essentially
sending a first goal of 10% of the
maximum possible revenue from
deforesting all the suitable area for
the research in the Amazon to 65% which
is a very ambitious goal and so that's
how it looks like on the x-axis we have
the revenue here is a under the million
dollar and here's a as a percentage of
the total maximum that we can generate
and on the Left y-axis we have
proportional specie persisting Amazon
white and then on the right y-axis we
have a proportion of the suitable area
for deforestation is actually first it
and so here we have the expected
scenario from the different system model
that says that by 2020 about 10 percent
of the suitable area for the first
session would be defrosted and they will
generate about 160 million dollar and
this is the impact on species Amazon
wide about 95 percent of the total
abundance in average almost per bird
species would be retained so 5 percent
is lost and so what I did that by using
a similar annealing algorithm I
essentially optimize this problem say ok
with the same amount of revenue are much
less deforestation we can have a much
the first session we can avoid them and
what is going to be the important Birds
and so with the same amount of revenue
we can keep persisting 9 9 99.5% of bird
species and that would only the revenue
would require defrosting about two
percent of the overall suitable area and
I've done that incrementally with the
more ambitious targets so the
interesting point is here with 55% of
the entire of the maximum potential
revenue that you can get you still only
need about 10 percent of the Amazon to
clear and the reason for that is that
most of them
most valuable cropland is the most
valuable cropland is very close to the
markets and once you defrost so it was
the valuable land for cropland is the
one close to the market so if you
depress this land you get the most value
per unit area but then when you run out
of this area there are more valuable and
you want to increment your revenue
further then you have to move more
towards interior and clear forests there
is less valuable both for timber but
primarily for crops because that's of
the two commodities the one that gives
you more money so essentially once once
you run run out of the most valuable
land and you have to push further then
for each unit area that you DeForest you
get much less revenue so if you go from
55 to 60 percent of the world revenue as
a goal then you actually need extra 25%
of the forest to be lost and that is
going to have a huge impact on birds so
I take a message here is essentially we
can still generate 400 million u.s.
dollars revenue by only deforesting
about 10 percent of the suitable area in
the in the coming years so of course
there are assumptions in this one of
them is the non spatial context as I was
pointed out before each of these models
assume that what happened locally is
just function of water locally whereas
but piece of course respond also to a
rapper in the neighborhood so in our
next instance of this what I would
really want to do is actually run the
abundance model within the objective
function so inform the objective
function it it's time to give spatial
context so that the local value is not
only function or what is there for its
own home force but it's also functional
what's what's happening outside and we
can do that it's just going to be
computationally heavy but we can do that
and the other thing is relax the
assumption that maintaining the forest
standing adjusting hers foregone revenue
essentially
costs for the society whereas actually
as I said Brazilian government could
apply for funding from the UN to keep
the forest standing and so there could
be positive economic
gain to be made by keeping the for
standing so the overall cost is not
going to be exactly what I calculated
but something less than that
so this is just to give you a flavor of
what could be done in this is really say
just just sort of thinking through it
but the last the last project that I've
been I wanted to do here is about
improving conservation monitoring so
this is more about gathering better data
to improve the data to the Jim pipeline
and what I mean by that is we can arnis
these many citizen science efforts are
being going on in the last year's lots
of amateur and people that are
interested in going out in the field and
provide the data they the photos and the
information they take on the field to
scientist for example there is this
platform here is called AI naturalist
that is quite popular in the US and is
spreading out outside u.s. essentially
people take photos of what had they've
been saying and they provide this
information on the website
this is scrutinized by expert and Danny
face of scientific quality essentially
is correctly identified it's given to
scientists to use for example for
ecological modeling but this essentially
relies on passive learning on people
voluntarily wanting to go out and give
their data back the idea is that we can
actually direct this search to tell
people what is the best place that could
go out and sample in order for us to get
a better understanding of specific
ecology and so for example in other
disciplines people have been using
active learning techniques essentially
optimal sampling strategy based on
relative sampling and modeling some
response for example could be logistic
regression model of presence or absence
or breast cancer so the idea is
essentially you can have increasingly
larger training data sets each time you
build the next increment of data set
you're using an are going to tell you
what would be the most informative point
to label
and then you evaluate your performance
by fitting the model and then evaluating
these accuracy against our independent
data set so we can use this active
learning technicals in ecology to
identify what would be the most
informative data points to some people
to collect data and return back for us
to do better ecological models so their
overall framework is first I would like
to start with the synthetic data make up
data to get myself familiar with the
techniques and have some sanity check on
whether or not these approaches work or
not and then once that's done I will use
real data from citizen science data set
for example there is one on invasive
species in New England that our
colleague Cora marrow has volunteered is
willing to provide and also cryptic
species so species are rare encrypting
and hard to find and so the idea is that
these would be the one most valuable for
this direct search method so use the
first system until a few data points to
train some models we can use machine
learning method or regression methods
and then get the first predictions such
a probability of finding the species
based on the data in the model and then
the idea is that once we have the first
set of prediction we can use active
learning methods for example we can use
the prediction to identify what is the
most uncertain data point the one with
the maximum entropy so maximal entropy
with a probability from zero to one
point five and the idea behind this
intuitively is that the most in certain
data point is the one that is going to
tell the classifier better apart
absences and presences and another
technique is quite by committee so you
can take an ensemble model for example
random forests and then look at the data
points that these models have least
accordance congruence between so the the
data pointing which is more discourse or
between classifying it as a presence or
absence and then another one is for
example is expected model change which
essentially would
the next best data point is the one that
is expected to give the largest
variation in progress for the model so
they several them are still exploring
all this work and I'll be testing many
of them to understand how they work and
compare the relative performance but the
idea is that essentially at step four
you use these techniques to identify
what's the most important important and
formally data point you query that you
get the information and that you fit a
new model and so you had to rate them
from two step four and five to build an
incrementally large training data set
and fit the model and then make
predictions and each time you do that
you evaluate the accuracy of the model
we can evaluate for example the area
under the ROC curve through skill
statistics but also other more
qualitative measure of motor performance
such as whether or not they're able to
detect the presence of rare species and
also finally some more derived measures
so rather than just probability
distribution what we can get from them
for example the ever occupants you're
the species what is the broader and
which species are kurtz
and these are important for example for
Red List classification so I understand
in exchange of risk of the species so
this is the overall framework and I just
started thinking about it and I'd like
to be doing that in the coming months
and understand that lots of people
Microsoft have been working on active
learning so I look forward to engage
with them to talk more about this and we
won't just I don't want to just stop
there and publish a paper as a proof of
concept or be nice actually to turn this
into a tool for example going back to a
naturalist we could turn to turn this
into a Brigitte essentially that
suggests what is the most informative
part of data point and also matches up
matches it up with people interest much
like Amazon tells you what book you
should be buying based on what you
bought in the past the ideas that we can
first find that what's the next most
informative point and then find out
which people who is there in that region
is interesting that species and then
suggest this point to the
people so that's the sort of short-term
plans and going beyond that more sort of
longer-term projects what I would really
like to do next couple of years is
thinking about again predictions about
the planet but specifically how
socio-economic scenarios are going to
affect equals some function is structure
so rather than the focus on species that
I add in the past I would like to go
beyond that and think about our species
of assemblages and community and tropic
structure can be affected by global
changes such as land use and climate
change and also we can use that to
create back-cast scenario so rather than
projecting forward based on a given
scenario of a socio-economic development
we can actually do an inverse process
start from the future from a desired
goal in the future and they walk back
and think about what steps was suicidal
steps are needed to actually get to that
future and I have a Mattingly model like
a general equities a model Assessor of
the background model that tells us what
are the changes in the system that are
expected based on input such as land use
and climate change so that's all for me
and but first I wanted to thank the or
compression ecology group for helping me
in thinking about this idea and also
collaborators such as Corey marrow
Tonya burger wolf and Sian done thank
you
there is a lot of scheduling arguing
already out there and they haven't found
one that includes expected changes to
the initial plan within it to be honest
Ireland so which so very you mentioned
sorry that's free yeah
I'm not familiar with that but yeah we
could exploit that solution from the
ones that I know of for example sir
economic programming I don't find any
one that actually can include expected
changes to the solution which is one of
the things that our careful
so is well not only about whether or not
you can achieve your objectives are
given a large variety of scenarios that
can evolve is also about how different
is going to be what you end up doing for
what you wanted to do but if that can be
included in that tree then yes to
characterize the good solution so to
speak you ended up with a set of like 30
or so and then you sort of showed how
like solution one was quite dissimilar
from say solution to 3/4 so yeah so if
you only use the kind of the end state
of the GA then how do you know that
there are solutions near to number one
that are also good but just never came
up there again I have a final solution
so I guess I guess one way to avoid that
is having a large population size at the
end and also reduce the mutation rate to
avoid actually yeah sorry no no more the
numerator you should avoid our Heil it
is man so essentially elitism is how
much percentage of the population is
promoted as is in the next generation so
if you have if you have like 50 percent
of the population is promoted disease
essentially you you eliminate less
things and so you retain more of what
has been generated before so between
these two parameters
how much is your final population size
and a much essentially evolutionary
spitting speaking selection you've done
during the process you can actually
avoid them in these these losses and so
far I found that the best solution tend
to be inflexible only in particular
cases and it depends on the species
distribution so I don't found I've shown
these results but I can show later if
you want essentially there are different
nested nests of species distribution
that I tested and when species tend to
be clumped in one region the most
efficient solution tend to be also on
flexible meaning that is very unique
very distinct from anything else in this
situations then having a flexible
approach like the one I suggested this
best but otherwise actually the most
cost efficient solution could also be
the one that ends up being most cost
efficient in the end because it just
last changes less choices that yet to
make so the let least the choices you
have to make the least the chances for
changes right in terms of making
decisions the main thing that I found is
that the key parameters that you want to
know about the system that you have to
invest money and time in actually
understand it better is is once you've
worked out what you want to do which
species you want to present a no matter
which of them is exactly are distributed
they are again going back to this
clumping problem because that's the key
parameters that is going to affect you
and the next next most important big
thing is understand your threats as a
showed as I showed you with the minimum
such solution it was really much better
when threats were separated from
biodiversity but otherwise there was
much worse performing so these two are
the keep army so if I were to talk to
this in like I would say okay before you
even make any decisions try and
invest some time and effort in getting
these two parameters and then you can
have then the solution is actually quite
straightforward then what you do really
then also the other one is try and get
as much money as possible right now and
spend it all together</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>