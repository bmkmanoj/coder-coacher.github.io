<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - Pieter Abbeel | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - Pieter Abbeel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - Pieter Abbeel</b></h2><h5 class="post__date">2016-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/T3mkz-zuzGY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it is my pleasure to introduce with
rabia peter is an associate professor
the electrical engineering and computer
science department UC Berkeley he is
super well known for his work on
friendship learning for and mercenary
for robotics and using reinforcement
learning and deep learning and he has
been leading this field and we are
honored to have you here and to learn
about how to combine any personal onion
dip learning robotics thank you and
thanks everybody for being here actually
do believe deep reinforcement learning
will result in very significant advances
in robotics in the foreseeable future
but let's first take a look at what is
reinforcement learning so any
reinforcement learning an agent is in a
state takes an action then it lands in a
new state and it gets a reward
associated with that new state and this
process repeats over and over and over
and the goal of the agent is to somehow
maximize the expected some of rewards is
accumulating now what are some
challenges are not necessarily present
in supervised learning when you do
reinforcement learning first one is
stability so when you change your policy
it will not only change your actions it
will also change the states that you
visit as a consequence of the change in
actions and this could destabilize your
performance another change is credit
assignment let's consider an example
reinforcement learning problem an
agent's supposed to get out of a maze
maybe you only get a positive reward
when you're out of the maze zero
everywhere else then once you're finally
out of the maze the question is what
what was it that you did that got you
out of the maze you might have done a
lot of things that we're not relevant
how do you tease out what mattered what
didn't matter third one is exploration
don't get given a bunch of data you
actually have to go explore find out
arter bigger rewards somewhere than I've
seen so far and figure that out on your
own so now that's the talk in this
symposium the invitation sided three
papers who recently posted on archive
that actually look at three quite
different ways of trying to tackle this
problem and there are more different
ways to tackle this but these are the
three different ways that we've been
looking at in those three papers what a
miss policy optimization is one approach
another one is you can
use reinforcement learning in some
settings to supervised learning there's
the second thing I'll talk about and
then there are pure value function based
methods which is the third thing I'll
talk about a little bit at the end let's
start with policy optimization so
impossible ization the goal is to find a
parameter vector theta this could be all
the parameters in a huge neural net such
that the mapping from stage to actions
optimizes expected some of rewards one
reason you might want to do that rather
than let's say exploit the dynamic
programming property that you have in
the value iteration equations is that
sometimes or even often a good policy
can be simply to represent then a good
value function and the other reason is
that actually when you're developing
this and you're trying to get something
to work the beautiful thing about policy
optimizations that you actually can
measure how well you're doing you change
your parameter vector theta you can
execute according to that new parameter
vector theta and see on average have all
you do is it better it worse than before
and then you can keep the update or go
back it's a lot of existing work in that
area I think the big challenges how to
speed up the learning in this problems
like if you take tiny tiny tiny tiny
steps along the gradient direction you
might only get there but how do you take
larger steps and so on things we started
looking at then is how to bring trust
regions into policy optimizations we can
define how far we can step and so in the
precision policy position paper which
was presented icml this summer what we
looked at is a trust region of the
following type what you see here is we
have some approximation of the objective
gradient times delta parameter vector
theta and then subject to a constraint
that the distribution over States being
visited under the new parameter vector
theta plus delta theta has to be close
to the original distribution under
parameter vector theta this allows you
to stabilize your policy updates so you
don't get that destabilizing effect that
I mentioned earlier know sometimes that
much easier to pick epsilon and to pick
some kind of way to trade off between
different terms now let's take a look at
the gradient part so the typical
gradient that you would see in a
reinforcement learning algorithm using a
reinforced gradient would be grad log PI
theta action given state and so saying a
point and direction of the law of
probability of the action given state
multiplied with something if that
something is positive then you would
increase the probability action you just
took if it's negative you with decrease
as
it's multiplied in there is the reward
you got after taking that action minus
value which is the average you would
have gotten from that state and so if
you're better than average when my
ability goes up when you land he what is
P P okay so p here is the distribution
over trajectories that you would
encounter when acting according to a
policy with parameter vector theta so
tau is a full trajectory these are
distribution over trajectories and so
you want to keep the distribution over
trajectories close thank you and so we
see here is that you would compare
rewards he encountered with what you
would get an average and the problem is
that this could be a noisy estimate
because the rewards you've gotten one
execution could be quite different one
where you would have gotten the next
execution so we looked at how to make
this less noisy using something called
generalized advantage this summation and
the idea here is rather than using
actual rewards you encountered literally
as they are sitting there just made
something like a cube function so it's
combining method 1 and 3 in some sense
and we use a deep neural net to
approximate the value of function that's
sitting there and a particular trust
region optimization method to be able to
learn a high-dimensional value function
in a reliable way once you do that be
able to get results like this this is in
mu Joker simulator developed by mo
todorov at the University of Washington
and you see here as a character supposed
to make forward progress and initially
it's mostly falling over but over time I
figures out the reward is higher when it
gives makes further progress forward and
so it learns over time something like
walking or running but we don't tell
that needs to walk around we just give
it positive reward for forward progress
negative rewards relate to how hard it
makes contact with the ground and every
tradition here corresponds to a new
setting of all the parameters in the
neural net where it then does about 500
seconds of rollouts under the current
parameter settings to approximate that
gradient approximate to approximate the
gradient and to compute that trust
region exact same algorithm different
robot the same reward function again
further forward is better and over time
it figures out a setting of the
parameters in a neural net that allow it
to make
fast progress in fact for this
four-legged robot it's probably not very
realistic what it finds but it's
exploiting what would Joker allows it to
do it so it learns to go very very fast
here the reward is related to the height
of the head so there's nothing there
about what it means to get up is just
the head needs to get to a certain
height to get the maximum reward and
over time it actually figures out pretty
proudly how to get there so let me
contrast this for a second with what we
saw or some of us saw in June for the
DARPA Robotics Challenge and this way
you got to see with a lot of really good
robotics teams competing over two
million dollar prize so this is a hard
problem I'm not saying these are the
best performances you would have seen
there but it indicates that there is not
no such thing as let's just download a
locomotion controller and use that
nothing we try then is to apply this to
the Atari games environment we were
definitely not the first to test things
there but I think what's intriguing is
that the approach we develop with
locomotion in mind did also perform well
not as well as the other two approaches
listed but pretty well on learning to
play Atari games which is a very
different kind of setting where you take
pixels as inputs right and joint angles
and join philosophies but how about real
robot skills a robot that can really do
something for that problem we've
actually looked at the second type of
approach and actually many of you
probably saw eager mortgage present
something along those along those lines
of work in the main conference and this
is reducing things to supervised
learning and the idea here is we still
want to find a policy PI theta that
maximizes expected some of rewards but
we're going to do this indirectly we're
going to find auxiliary policies pi I
which are solutions to specific problems
that we know we can solve more
effectively and these don't have to be
general policies pi I they are specific
to those specific problems but then we
have a constraint or term and objective
that says our general policy PI theta
should be quite similar to these
policies pi I and if
that's the case we are effectively
looking at solving the first part to
solve very specific problems find very
specific policies and then supervised
learning to match PI theta to each of
the pie ice things you can do we are
interested in PI theta that go some
pixels to actions we might say well at
training time we're going to simplify
for our policies pi i would give them
full state and so it makes it easier to
learn you learn with full state
information but whatever actions they
take in a certain state the policy pi
theta has to agree based on just getting
the pixels as input okay the known that
we used here is inspired by the computer
vision typical architectures but a few
slight differences that matter a bit
first in the convolutional layers
there's no pooling because you want to
retain the information about where
things are second thing redbox there
that's a softmax layer what that is
doing it's outputting coordinates of
where the features are active okay and
those coordinates are then fed into
fully connected layers that do motor
control or learn motor control some
tasks we looked at can you insert a
block in a tightly fitting opening can
you hang a coat hanger can you place the
claw of a hammer underneath a nail and
can you screw a cap onto a ball which
the exterior why is actually very
difficult so the approach we used called
guided policy search here is the
learning in action what are the policies
pi I that have access to full state for
every location the cube the big cube is
ant there is a specific policy pi being
trained in parallel there's a policy PI
theta being trained that needs to agree
with those that just uses pixels as
inputs you see that over time it's able
to learn to insert the block in the
matching opening the policies pi I here
are learned with something called
iterative LQG which is an effective way
to learn for specific tasks and it's
learned is using Gaussian mixture models
and local in your models to fit models
to the data and encounters during
execution and then the policy PI theta
is a big 92,000 parameter a neural net
here are some results on the other tasks
so this is what the robot is seeing
explai singh the coat hanger
on the bar you can put other objects on
the bar it doesn't get distracted by
them here is placement of the block we
saw the training for that here is
different configuration look at how it's
holding the hammer it's actually holding
the hammer in different ways so when
it's learning that policy PI theta it
realizes it needs to pay attention to
both where the nail is and how it's
holding the hammer to be able to
generate the right actions and then this
dexterity wise is probably the hardest
one of these tasks now let's take a look
at the third family of methods where you
learn a value function the idea there is
that this relationship that says the
optimal expected some of rewards v-star
from a state s is equal to the reward
you get instantaneously plus then the
expected some of your words you'll get
from the next stay downwards under the
optimal action and so you can set up an
optimization problem like thing where
you say I want this set of equations to
be satisfied for all states and that's a
more indirect way of trying to find a
policy so people a deep mind it is very
successfully and a lot of a size that
nips two years ago as a big surprise
being able to learn all the way from
pixels to Joystiq actions with Q
learning which essentially tries to get
those equations satisfied the
performance is very good very impressive
and above the horizontal bar here means
human level or above this is taken from
their nature paper this year now one
thing to also think about is how quickly
does it learn it's one thing to look at
final performance another thing how
quickly do you get to that performance
that's the exploration problem can you
explore the world quickly to learn
quickly that's one idea we started
looking at is to bring some of the older
reinforcement learning ideas about
exploration bonuses into making them
applicable in this high dimensional
spaces where you learn something high
dimensional like maybe a hundred
thousand or more than that parameter
neural net so looked at exploration
bonuses and the way we set this up was
we said we learn first an auto encoder
for the images we see in Atari then
we'll use one of the inner encoding
layers as our pseudo state so to say and
we'll from experience learn to predict
what the next state is going to be then
while we play we give a bonus based on
how inaccurate that prediction is so
visit a state where prediction is very
inaccurate that's probably one you
should visit more often to learn more
you get a high bonus that's what we run
we compare this with the standard DQN
approach which uses epsilon greedy
exploration which means every now and
then well ninety percent down the five
percent you act at random and we compare
this with Boltzmann exploration which
looks at the current Q values and makes
higher two values more likely but then
absolutely chosen and Thompson sampling
which is a kind of Bayesian approach to
do exploration and so the plots that you
see here i think defining the main
finding is that being explicit about
exploration helps whether it's through
thompson sampling through bolts and
exploration or through our approach each
of them tends to outperform the standard
epsilon greedy approach using the model
and the bonus is based on that often
helps but doesn't necessarily always
outperform boltzmann or thompson one
thing we start thinking about then is
well can we do more want to learn more
than just a Q function let's also learn
a dynamics model in parallel let's share
let's look like a big neural net that
has everything in it seems a reasonable
thing to do that way we could learn more
quickly because if you learn about
representation maybe you can learn more
quickly how to act I should try very
hard can get this to work talked
informally some other people couldn't
get this to work either so start
thinking about well what is it the hope
have been this accelerate accelerate DQN
didn't really work so maybe we're not
learning a good representation maybe yet
something else so the investigation we
did then which i thought was kind of
interesting was well what happens if we
pre trained by running DQ until
convergence take the entire neural net
train that way cut off the last layer
and just retrain the last layer or
retrain just the last two layers or
fine-tune from there so to say so some
things that's like the ideal stuff you
could have put in the early layers is
sitting there from the start does that
at least allow you to learn more quickly
and here are the learning curves for the
four variants are described and then
traditional dqm you can see such it very
hard to distinguish their performance
suggesting that even initializing the
initial layers perfectly set to say
doesn't speed up learning so maybe
tation learning is not the bottleneck in
these learning to play these games but
it's actually something else ok so what
are some of the frontiers we're looking
at right now one of them is shared and
transfer learning how'd you learn
multiple tasks with a big neural net
into some shared way across robots or
across tasks exploration i think is
still a very big problem to resolve much
better than it is resolved right now
memory is something that in none of the
things i described is being used but
naturally in any kind of robotic setting
you want to remember things you've seen
in the past you also want action
hierarchies which means internal goal
setting which means memory it's not in
there right now and then we've been
looking at a lot of tools for
experimentation such as a stochastic
competition graphs that john chill and
presented here and the competition graph
toolkit which is a awesome attic
differentiation library inspired by
Theano but different in a few ways all
right just want to thank my
collaborators that's it thank
question yes hello so I have a question
because you showed three different
examples of things that you did and one
of them you reduce to supervised
learning task what is a control task and
generally they standard way of doing
that is with reinforcement learn so my
question is can you discuss a little bit
what would be the benefit of reducing
just supervise a task because you have
no idea iid samples and so on so work
can you get and what why didn't you do
the same for the others tasks is oh ok
so the this is the kind of summary or
you're referring to right in the middle
one is where we reduce it to a
supervised learning task something
that's been done by Sergey 11 and his
PhD work even watching his work drew
bagnall with Stefan Ross and so forth
what they called a dagger Sergei's works
called guided policy search so the idea
here is that sometimes there are
versions of the problem that are easier
to solve and easier could mean many
things but in our case what that meant
is if the world is fairly repeatable
that is even though it's very hard to
simulate the world very hard to predict
if you were to go through the same
sequence of actions it's likely that
something very similar word to happen
then by focusing on a very specific
setting repeatedly something called
iterative learning control which is a
very special type of reinforcement
learning it's only applicable bear you
can actually learn relatively quickly to
succeed at at a specific task and so
that was the idea there if we're in a
setting where if we focus on a specific
task and repeat over and over we can
perfectly reset so to say then we can
leverage that to then first succeed at
those and use the successes in those
supervision to train a policy that
generalizes there are also other
situations for example in Stefan Ross
and drew bacchanals work called dagger
or that was then used by Joshua Hong
Lachlan stand satinder singh for their
atari game work they use Monte Carlo
tree search and so the idea there was
we're willing to spend a lot of
computational cycles initially to search
through this game tree to find good
execution paths and then we use those
that's far from real time we you
those as supervision to train a policy
that can run in real time so that's
another reason why you might want to do
that but there are some assumptions here
and so it's not necessarily always
applicable to be able to apply this okay
thank you any other question
so I have a question so in practice how
far in the future how much can you delay
the reward and when do things break and
how do you yes so that that's a really
good question that's really at the
foundation of a lot of reinforcement
learning is analyzing in some sense how
much delay you can deal with and how
noisy things get so this equation kind
of highlights that here so you look at a
standard policy gradient calculation
it's a sum of rewards and minus the
average that you would get and
essentially a lot of the tricks played
relayed relate to how do you deal with
that if the reward is very far delayed
if it's extremely far delayed probably
it'll be very hard reinforcement
learning problem we might not learn for
a very long time if their word is not
super far delayed then the thing to
think about is well maybe I should up
way the early rewards not because
they're more important but because they
are less noisy is that because these
pulses are all stochastic policies and
if you equally way later rewards
compared to earlier rewards the later
ones are in some sense a noisy
measurement before you would get later
where the early ones are very related to
what you did early on and so that's one
of the things that this particular paper
she looks at in a lot of detail is how
how to deal with that noise effect that
you get there thank you all right let's
think Peter again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>