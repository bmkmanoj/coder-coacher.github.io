<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Satisfiability of Ordering CSPs Above Average Is Fixed-Parameter Tractable | Coder Coacher - Coaching Coders</title><meta content="Satisfiability of Ordering CSPs Above Average Is Fixed-Parameter Tractable - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Satisfiability of Ordering CSPs Above Average Is Fixed-Parameter Tractable</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s44Ii8Lskd0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's a great pleasure to introduce to
you my character from Toyota
technological institute at Chicago and
to his visiting microsoft research for
three days and today he will tell us
about satisfiability of ordering GSP's
above average thank you for the
introduction so I will talk about
satisfiability of Ordinances is above
average and I will show you that this
problem is fixed parameter tractable
this is jarret we are Chris question and
with John Joe from a meeting so let me
first define the problem define what
what diminishes PSR so in another nice
ESP we are given a set of n variables x1
and sub X N and M constraints by one etc
p.m. and we want to find a linear order
of the variables that maximizes the
number of satisfied constraints and what
are the constraints are so each
constraint PI R depends on at most key
variables where key is a given constant
and each constraint prescribes what
relative ordering of the variable of the
variables it depends on are valid and
which are not or in other words for each
constraint we have a list of possible
relative orderings of the variables it
depends on and if the ordering of the
variables is on this list than the
constraint is satisfied other otherwise
it's not and let me just show you two
very well-known examples of ordering
CSPs the first example is the maximum a
cyclic sub graph problem in this problem
we are given a directed graph G on a set
of vertices X 1 etcetera xn and our goal
is to find a linear ordering of the
vertices that maximizes the number of
forward going ages and it's easy to see
that this problem is just an ordinary
CSP of aerie 22
so each edge xixj corresponds to a
constraint X size is less than X G and
then the number of a forward going ages
is equal exactly to the number of
satisfied constraints for every ordering
of excise right another well-known
example is the betweeners problem and in
this problem so this problem is
basically in ordering CSP or very 23 and
in this problem all constraints of the
form X I lies between XJ and X key or
more explicitly of the form x JX j is
less than X I less than X key or X key
is less than X I less than X G what is
known about watering CSPs there is a
very simple trivial approximation elgart
for the problem let's just randomly
permute all excise and output a random
or drink how well does this algorithm
pure form for the maximum a psychic sub
graph problem this algorithm switches
files each constraint whispering builds
a half right because for every X I + XG
two possibilities are equally likely
that X is less than X G in that x g is
less than X I and there are for this
algorithm satisfies em over to
constraints in expectation and let's
denote this number the expected number
of constraints these algorithms is
satisfies by avg average for the
betweenness problem or this algorithm
satisfies each constraint with
probability of answers because each of
the three vertices the constraint
depends on I equally likely to be
between the other two and so in
expectation this algorithm which is 5am
over three constraints now this iWitness
video almost trivial algorithm and it's
natural to ask if we can do better than
this algorithm
and in general this problem for
different types of constraint
satisfaction constraint satisfaction
problems has been studied a lot so the
basic question is to get some advantage
over at ravel random algorithm yet
advantage of a random and the first
result our e in this area was due to
hosted hope to show that the and he
studied regular CSP is not audiences is
that the 33 C&amp;amp;F and flicks or csps our
approximation resistant meaning that no
approximation algorithm gets a better
approximation richer than this drivel
random assignment algorithm and if if if
there is no nontrivial approximation for
a constraint satisfaction problem we
call this constraint satisfaction
problem oh and the corresponding
predicate approximation resistant and
it's an interesting question to classify
all like regular predicates which of
them are approximation resistant which
of them are not we still don't have the
complete classification for instance in
Austria and Mosul proved that a really
wide class of predicates ready keys that
could kind of come from pairwise
independence our approximation resistant
there are also some positive results
showing that some pious is uh not
approximation resistance for instance
who have stabbed show that any to CSP is
not a proximity resistant and also
hosted showed that bounded occurrence
ESPs a note approximation resistant but
there are many many results in this area
inverse still many open problems this
problem the advantage of a random was
also studied for ordering sheer Spears
and let me first tell you about positive
results and then I will tell you about
negative results so
first burger and shore we long time ago
in 1990 showed that we can get advantage
of a random for bounded occurrence
instances of the maximum as a saitek sub
graph yes and then any joint work with a
cherry car and costa we showed how to
get some advantage of random for the
maximum i say a cyclic sub graph in
general grass and the dependence is
different gross slimy and jus showed
that we can get advantage of a random
for any what we see SPS of ATT three who
that have bounded occurrences of
variables and this result was
generalized to all bounded occurrences
piece by questa on the negative side
goes by me hosted monica rendre gavin
drew and share class showed the there is
no nontrivial multiplicative
approximation algorithm for any order in
CSP of any era Tiki so unlike a sign
macias is weaker there is no better
approximation algorithm for any
audiences piece or formally this means
that noble animal time algorithm can
find the solutions to just find at least
one perception times average constraints
for instances with optimal eight even
for instances result in greater than 1
minus epsilon m for every a constant
value of epsilon so in other words no no
algorithm gets a so we can we can get
any a constant factor advantage over
random right and then it's natural to
ask if we can get it with some additive
advantage of random so we cannot improve
the approximation factor maybe we can
just do a little bit better than random
and this question was posed by gutom
with Ursel niche and he'll who
conjectured that there is a fixed
parameter algorithm they decides whether
the optimum is greater than average plus
T or not and here key is a fixed
parameter so in other words this means
that there isn't an algorithm who's
running time is some function of T and
this function media for instance
exponential in T times a polynomial in n
plus n that answers this question so
they decides whether the option is
greater than average plus G or not what
is known about this conjecture first of
all let me mention that a similar
conjecture was proved for assignments
ESP so alone guten king slider and here
show that for any assignments yes pew
regular CSP this like the counterpart of
this conjecture is true for Roger Alicia
spear there has been a number of of
peoples and by goethe natal who showed
that this conjecture is true is true for
the maximum a cyclic sub graph problem
for the betweeners problem and finally
for all audiences piece of a return
three however however I in general the
problem seems to be much more difficult
and in particular booting at all know
that it appears no noted that it appears
technically very difficult to extend
results obtained for every tues turn 32
air is greater than three and in this
paper we show that this conjecture is
true for csps for audiences piece of all
editors and also we are
generalize this result and show that
actually a similar conjecture is true
for a very wide class of constraint
satisfaction problems constraint
satisfaction problems that have some
structural properties and in particular
this class includes all ordering csps
enable see a few words about that later
also to prove every result I be proved
in you when i met a plumber for the
front stein decomposition and we believe
that this result is of independent
interest and i will see if he works
about this I result later and they will
also define what the front Stein
decomposition is if the high level we
use the approach of alone at all that
they used for approving the counter part
of this conjecture for regular csps and
recall that we in this problem we are
given an instance of an ordering CSP and
the parameter T and we need need to
decide whether the optimum is greater
than average plus T or not and we proved
that there are two possibilities the
first possibility is that the instance
depends on at most quadratic in T number
of variables so in this case we can just
try all possible orderings of this t
squared variables find the best solution
and test whether the value of the
solution is greater than average plus T
or not this was cheesiest also vision
the key system are tier changes by the
right time may depend on T for different
values of K we can have even different
algorithms with the fish farm
intractable for polarity unique
or for every key there exists it is true
but the polynomials may be different
particular here this country this
constant ck depends on key any questions
and let me know if you have any
questions about the problem or the
statements and also here I mean this set
of variables is called a colonel and so
in this in this case we have a kernel of
quadratic interior size and in the
second case the instance depends on more
than seek a chi-squared variables and in
this case we show that actually the
optimal value is greater than average
plus T so in this case we know that the
answer is positive moreover in this case
B we can actually find a solution whose
value is greater than average plus T but
but of course not necessarily the
optimal solution for my shooting for
last G so often resp yes so we can
always assume that all constraints of
the form X I 1 is less than X I 2 etc is
less than x ik
so II be replaced because for it for
every constraint we have a subset so
each constraint is just a conjunction of
such a row of such clauses and then we
can just replace it with these clauses
than the number of constraints will go
up but the optimal value will not change
and the expected number of cities right
constraints will not change so each one
is regardless is basically what would
you do so serious yes yes for for such
constraints years teenie other questions
um okay now how do we approve these that
there are only these two possibilities
if you've just let's consider a random
or drink of X 10 X n and let's denote by
z capital G the number of constraints
satisfied by an a random or drink and
then by definition the expectation of g
is equal to this number evg right now it
just be proved that if the instance
depends on at least our variables then
the variance of this random variable Z
is large it's at least some positive
number constant a times R and so if you
are not in this one if the instance
depends on many variables then we know
that the variance is high now we prove
theorem to that says that if we are not
in the first case if the variance is
high then the option is greater than the
average plus some positive constant
times square root R and here i want to
know that this result is not revel in
the sense that of course if the variance
is high then the standard deviation is
high and we know that this random
variable z delete from the express
value by at least square root are right
but the distribution of Z is not
necessarily symmetric and moreover in
most cases in most cases it's very
asymmetric so we don't know whether it
divides only in one direction or in both
directions so it's possible that it
takes on values that are much smaller
than the expectation but it never takes
values that are considerably greater
thans expectation and what this film
shows that actually are in these keys
for these are random variables here we
have missed you have a grant you that he
attains the algae that are considerably
larger than average and now as a as a
corollary figure that if the variance of
G is at least each square then the
option is greater than average plus G so
we are we are in keystone and we know
that the answer is positive so this is
our plan so he is our random variable
the number of satisfied constraint bring
your drinks ESP it doesn't do not assume
the ball to this and there's no you can
envision the key sixth are the number of
variables each constraint depends on and
but other than and yen I mean this
Constantine depends on key ah so now we
need to prove two theorems with feeling
one and freedom to and to those so we
use the free analysis and more
specifically the front Stein
decomposition alsa in order to prove
this theorem to you prove open anytime
llama for the front Stein decomposition
and I will tell you about that in a few
moments and
so we want to use free analysis to solve
this problem one problem is that free
analysis works especially well on
product spaces and in our keys the set
of solutions is not a product space
right so the set of solution is a set of
limitations of variables X 1 etcetera xn
and it's not that convenient to use free
analysis on this set so let's instead
they assume that each variable X I
belongs to the segment 01 and then for
ev assign values to X 1 etcetera xn from
this segment 01 then we can consider the
case pointin a linear or drink of excise
right excise I just ordered a cogent to
their values moreover we assign values
to exercise randomly then this ordering
will be also random and we will be just
distributed uniformly right so this is
our plan and here i want to mention that
it's really important that this domain 0
1 is a is that continuous infinite
domain so some previous approaches to
this problem I mean some of some of the
papers i mentioned on ordering CSPs user
so called the black kitchen technique so
what they did they considered a fixed
domain of bounded size and assume that X
I excise belong to this domain in this
problem we can't use this approach so we
can you we cannot use a fixed the mean
of constant size instead of this segment
01 because if you try to do that but we
will not preserve the optimal value and
so this approach will not work in order
to preserve the optimal value we can try
we can take a very large domain of size
and/or polynomial mm but that approach
will not work because then all the
parameters will depend on
and in particular the size of the
colonel will also depend on an and so we
will not the result we want to get so
it's really crucial here that our de
meme is fixed it doesn't depend depend
on em and we we can use it now I want to
define the front Stein decomposition and
roughly speaking the front Stein
decomposition is very similar to the
free analysis on the bouillon cube so if
you have a function on on the bouillon
cube then we can write it a sum of sum
over all subsets as of one of the set of
numbers from 1 to n the Fourier
coefficient is pointing to this set s
times the character for us and it has
the following very nice properties first
of all all these functions f as high as
dependent on variables in this are
variables X i beez ahi NOS then all
these functions are mutually orthogonal
and in particular because of that the
variance of F is just equal to the sum
of variances of its four-year terms and
also are we have linearity we have this
property for the standard free analysis
of boolean functions now the phone Stein
decomposition is very similar properties
so now let's assume that F is a function
from the cube with segments 0 1 to the
power of n 2 real numbers and the front
Stein decomposition is a representation
of F in in the form like this and
actually we can we can write the front
stein the composition for 4 function
defined on any probability space so here
this set doesn't have even to be the set
of this hat the segment 0 1 and it has
really similar properties so first of
all each function f
fast depends only on variables X I with
I can s then all functions FS a mutually
orthogonal and because of that the
variance of F is equal to the sum of
variances of these functions f s and the
decomposition is junior let me now
define the afrin Stein decomposition and
they will not be very precise but let me
just tell if you viewers that explain
what difference time the composition is
and let's first start with the keys n is
equal to 2 and moreover let's assume
that F is a product of two functions so
is a product of two functions are g and
h and let's write f as the expectation
of F so G empty set is the expectation
of f and g one R of X 1 is just G of X 1
minus the expectation of of G and
similarly let hm chesed be the
expectation of h and h 2 of x to be each
minus its expectation then then clearly
f is equal to this product now if you
expand this product and we get this
expression and in this expression the
first term is this function f empty set
this function depends only on x1 so
let's f1b this function this will be F 2
and this will be F 1 2 I and now let's
assume that n is greater than 2m again f
is the product of terms like this then
we just decompose
gee I in a similar way say we write the
GI is equal to its expectation + GI
minus its expectation we substitute each
GI in this product by the corresponding
expression and then expand that and we
get 2 to the power of our and charms in
each term is is what is a German the
front Stein the composition of F 0 and
in general of course f is not
necessarily a product of and functions
but we can extend this definition but by
linearity and we will get an a valid
definition for all functions f so this
is very behind the front Stein
decomposition do you have any questions
so that they see more about this so in
particular what we can do we can
consider the standard free ad
composition of F right and let's see
these functions media sine and cosine
functions and then we can represent f as
the sum of the sine of x 1 x cosine of X
2 sine of X 3 with some coefficient so
it may be sine of 5 X 5 and so on and
then we apply this are to each term so
is there a way to have a more watch a
description of this decomposition using
just directing for your explanation
usual yes so what we can do so we have
the Fourier decomposition so suppose
that we have a three-bay basis of Phi 0
Phi 1 Phi 2 etc infinite free basis and
let's assume that 50 just is the
constant function so it's equal to 1
right then what we can write that F is
equal to some
of F Assyria Phi R let's see I one of k5
I 1 of X 1 Phi let's see I 11 I 1 2 of X
2 and so 15 I end of xn and now what we
can do we can define F s is the sum of
these terms where this index I so some
some of those turns k such that I k j is
equal to 0 if g is an earth so in other
words okay let me say that in other
terms so what we do we we have this
infinite number of terms here right and
now we kind of and we have now now we
want to find these two to the power of n
terms so forever sad we have a term and
now you want to assign each term to one
of these sets right and the chance for
each set will be just the sum of the
functions which we assign to the set and
so we look at one product like this and
and look at those terms at those
function Phi that are not equal to 1 so
basically I mean if for instance is fine
is equal to 1 this is X to this is C I
don't know sine of X 3 white and this is
again 1 then this chore in this churn
this job depends on n x2 and x3
non-trivial so we put it in this seller
is pointing to the set 2 and 3 and this
is how we get this decomposition this is
another explanation is it also we can
actually write very simple explicit
formulas for computing reference time
decomposition and different standard
composition is defined uniquely so idea
is only one in front stein decomposition
for every probability space so first we
define these functions F sub psyche but
by this formula so it's the expectation
of F given x i's with I in Tia and then
we use the inclusion of Scott exclusion
formula to give this expression for F s
ok now let's apply the front Stein
decomposition to our problem so recall
that we consider this random variable Z
which is a number of satisfied
constraints and you can just use these
explicit formulas to compute the front
stein the composition of 0 0 and what we
get ok and ok if you just let us know
that Z is just the sum of indicator
functions of basic of basic ordering
play decades of this form X 1 is less
than X 2 etcetera less than X key and
because the front Stein the composition
is linear it's enough to compute the
front Stein decomposition of just these
predicates and here we just use those
formulas and we get that the
decomposition has this for so it's some
constant times are some overall
imitations
imitations of the indexes one etc key
and hear some polynomial P with integer
coefficients the des peres of degree at
most key that depends on variables X 1
etcetera X key and times the indicator
of the event that X our eyes are watered
according to pi so we get that the
decomposition has this form and because
of the because the front Stein
decomposition is linear we get that the
decomposition of ye also has this form
now what is important here that these
polynomials g prime have integer
coefficients and so the set of all
functions of this form forms a discrete
lettuce in a finite dimensional space
and because of that if this term GS in
the front stein decomposition is not
identically equal to 0 then it must be
bounded away from 0 and specifically its
variants should be at least some a
number that depends on the on key it's a
positive number and to prove that we use
a very basic compactness argument ok now
we are ready to prove our feet in one so
I recall that we we assume that the
instance depends on at least our
variables and you want to prove that the
variance of G is large so we compute the
front Stein decomposition idea and now
note that each charm GS depends on
utmost key variables right the big big
because each predicate depends not most
key variables are there are four and
since she depends on at least our
variables there are
further at least are over key terms
right each term contributes that is be
key to the variance of Z and so what we
get that the variance of G's at least
are the key / key so it depends linearly
it is but it's lower bounded by linear
function of R and so this concludes the
proof of theorem 1 now we want to show
to prove theorem 2 and the plan is as
follows so what we want to prove is that
if the variance of G is large then the
optimum values at least average plus b
square root R and we we show using in a
new enemy type lemma that we prove that
we proven which I will show in the next
slide then the fourth moment of Z is
upper bounded by some constant that
depends on one key times the variance of
discworld front and from this it follows
by result of Allah at all that he
attains values greater than the h plus b
square root r with positive probability
and this result of a long at all is
similar to the markov inequality so it's
not it's not difficult to show so let me
now see if u verse about this I benami
lama and so what you show that I mean
let's just recall what the standard
benami llama 4-1 when the variables is
so the bonhomie Lana says that if f is a
polynomial of degree at most key then
the first moment of F is bounded by some
a number 9 to the power of K times the
second moment of F squared now what you
should we prove that if F so let's
consider a function f now from this
segment 01
to our or the cigars for any probability
space and let's assume that all terms in
the front and decomposition has degree
at most a key then are very similar in
the quality hold but we also need to
assume additional that this condition is
true that the expectation of the
protocol vania for terms is bounded by
the product of the Avedon sustained some
number and the condition like this is
net system without this condition with
alema is not true and we use this be in
for four hours in our problem we show
that this condition is true and we get
that the fourth moment is bounded by the
second moment squared so this is how we
apply this lamb in our keys this of
dignity so if I if I standard now on the
feedback continues have a coupon rate
doesn't really sound like this is that
does it have a friend Stanley combustion
of the weekend ah unique standard just
wrong some extent by just saying just
round it right right yeah Oh bye bye bye
doing what bye bye bye round unit or
buying narrator kind of x by linearity
just so you consider the simple and I
know yeah it will have an affront Stein
the composition but it will be different
from the point of origin no prob eh it
should have degree care if your car too
quickly and work for the last condition
mean reductase Oh
positive EB just leave this function I
mean it should be true so for kind of
normal function it's true so basically
it's a problem with the function is
unbounded if it's bounded by some
constant eh killed it and yeah I mean it
will help with small Kozma see any
questions so let me now tell you how we
prove this the new me llama for
different Stein the composition and the
idea is is to reduce this problem to the
standard bulimia llama so we are going
to define an axillary polynomial G &amp;amp; G
proceed as follows so let GS be just the
variance of this term FS and let Kai as
high as be the character for the set s
and so we define this polynomial jeebies
Fourier coefficients GS and now you want
to compare F F and G and the
distributions may be very different but
even just to compare their second and
fourth moments you know that since F has
affront the composition of degree at
most K G is a polynomial of degree at
most key I first of all by our choice of
Fourier coefficients the variance of g
is equal to the sum of GS squared and
this is equal to the sum of variances of
terms of s so this is equal to the
variance of F so these two functions
have the same variance and now what we
want to probe so we want to upper bound
the fourth moment of f now we want to
what we are going to prove that the
fourth moment of f is
founded by the fourth fourth moment of G
times some constant C and using the
standard benami llama this was less than
some number times the variance of
civilians of G squared then the visions
of G squared and this is equal to the
again to the same constant times the
variance of a square so this should be
just F ok so this is our plan and the
only thing that we have to prove is this
neck waltz right then the fourth moment
of f is upper bounded by the fourth
moment of of G and let's just expand the
fourth moment of f so the fourth moment
of f is equal to the sum over subsets s1
s2 s3 s4 the expectation of the product
of FS 1 FS 2 FS three FS for right and
the fourth moment of G's is given by
this formula ok now
we want to show that this neck Walter
holds charm by charm so that we can just
write that this charm for every set as
one as it's a tres four is less than the
corresponding terms here and recall that
this additional condition that we
require says that the product the
expectation of the product of F s 1 FS 2
f as three FS four is less than some
constant C times the product of the
variances of g SI square so basically we
get that this coefficient is greater
than this expectation up to this
constant C so now the only thing that we
need to show is that this expectation is
at least one now and here we have we
have a slight problem so if some
variable appear does not appear in this
s1 s2 s3 s4 then so so sorry what we
have we have here just the product of
variables here right and we can write it
as the variable Z 1 to some power x
variable Z 2 to some power and so on and
this via bizarre independent so if we
can consider each variable separately
now if some variable doesn't appear in
this set s1 s2 s3 s4 at all then of
course it doesn't contribute anything if
it variable appears one time then its
expectation is 0 and then this
expression is is also 0 but it's not
hard to see that in this case in this
case if some index appears in this sets
on the ones then this expectation is
also 0 so that's not a problem if the
Ebola peers two or four times then the
expectation of its square and it's
fourth power is equal to one so we are
we are in good shape there is a problem
however if it be evil appears three
times here and three times here because
in this case this expectation is equal
to 0 and this expectation is not
necessarily equal to 0 however it turns
out that this is easy to fix so
basically instead of considering plus
minus 1 variables we consider a
Bernoulli variable with 0 first moment
but we spotted a first second and fill
these positive so your second third and
fourth moments and specifically we just
consider a random variable the tekes
values 3 and minus 1 these probabilities
1 cup water and three quick waters this
variable has expectation is hero but it
has positive first second and third
moments and so instead of current and V
be correspondingly consider characters
for this for this variable we also need
to show kind of van laack of the
standard benami lemma for functions
defined on this probability space but it
turns out that this video easier and
then from that we get the result of this
we get this when I'm ulema for the front
standard composition any questions
that doesn't also that that that is not
the board calm this one I mean basically
if the function or the last well too but
doesn't belong to I mean if this
function right so even if all these
terms are equal if the function belongs
to L 2 but doesn't belong 12-4 then this
is infinite and this is bounded any
questions okay and using this banana
lemma for the front Stein the
composition we get cerium too so this
concludes the proof now let me mention
our other result so we proved actually
our result for a white class of
constraint satisfaction problems defined
on the main 01 and it holds for all GSP
that have certain structural properties
in particular it holds for csps these
constraints defined by linear equations
with small integer coefficients or with
these constraints defined defined by
inequalities inequalities or which I
defined by a bounding bounding degree
polynomials the small integer
coefficients and just to give you some
idea what these constraints might be for
instance we can have constraints of the
form X I is greater than the average of
XJ and X key or say excise closer that
to XJ then takes key also all ordering
CSPs are in this class they can be
defined by
by the by trivial linear inequalities
and those all standard assignment csps I
in this class okay let me summarize what
we do what we did we proved the
conjecture of goethe natal and moreover
we generalized at the White class of
constraint satisfaction problems and
additionally we proved the enemy type
Lamb of at the front Stein decomposition
and it would be interesting to see if it
has more applications in computer
science thank you questions how does the
CSP is also possible lift up leg is it
clear the mass quantity mentioned that
you could possibly do not get
multiplicative I mean so this class is
very generalized as I described it and
in particularly includes all or doing
CSP so yes we can't we can't approximate
them but I mean if you consider sansome
subclasses then they not they difficulty
in particular or just standard GSP's
belong to this class of suspicion for
them for many of them we do have good
approximation algorithms yes is it's
really because the ICO's imagine is very
simple and you expect me to take more
out of your analysis and get a better
belong with complexity maybe put the tea
instead of two to the disk well I don't
know but I think you will be in case one
and you would say I don't know something
like know something more refined than
just how
I don't know there's no obstruction I'm
not sure I think that it might be
possible you know I mean of course it
might be possible to have some improved
approximation factor but by improved not
by a constant but but but by some
Chanel's for instance media it's
possible to get a result of the form
that we can get 1 plus 1 over log n
times average let me do something along
that line left sector forward the next
say to each other so the maxima sockets
are brat are we can get average plus or
minus average divider my lord am so that
basically this is i will just happy it
is right yes so this is not fixing i
meani so for the maximum the cyclists
are glad we can look at kind of a
different objective what is our
advantage over random right and then for
this function with the logan
approximation factor this is a lot and
approximation algorithm yessuh is the
smart exact the pasta machine you will
help you so does it generates to all the
computations US News and this result
yeah no I mean that would be really
interesting to get something like that
just for other quadrants jet skis but
that's not these techniques are very
different from these words 00 for this
result the pickle is a packaging
technique yeah the technique is very
different for
we do we use here die kitchen or so we
we use here by kitchen ah here we can't
use by ketan aziz and after that it's
very different I mean it's fine that
here we also use for Ian Ollis but the
free analysis is very different from
this free another's keep passions
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>