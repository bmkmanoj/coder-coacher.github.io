<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Newton Methods for CTR (Click Through Rate) Prediction | Coder Coacher - Coaching Coders</title><meta content="Distributed Newton Methods for CTR (Click Through Rate) Prediction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Newton Methods for CTR (Click Through Rate) Prediction</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kZnw6sklQHM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so in this talk I will talk about a real
mission lending application so we apply
some distributed machine learning
methods to to a real application this is
actually joining the project with a
internet company in Taiwan called bridge
whale
it's a taiwan-based a company
specializing in Internet advertisements
yep the name of the company's corporate
will result I want based a company their
business is many right now in in East
Asia for computational the word Huysmans
by the is in China right now they don't
have big business in Taiwan I think and
also pie dough is not only is mainly a
search company ok but the real
advisement is part of their BIOS
business
okay so another target application is
for click-through rate prediction so to
talk about CTR prediction first we need
to say a little bit about Internet
advertisements the main purpose is lad
to find out a match between users and
also those eggs so for example if you
are if you type a query at Google and
they want to show you something
well they need they need to match your
interest and also they're available ads
in general and the red Huysmans system
is like this so let it go so called the
pay-per-click model of course there are
many different models but this is one of
them
so that means you will be charged if
your ad is clicked
so roughly so if you have solicited for
search or Huysmans is like least in in a
whole framework you need to do you need
to have a landing procedure to predict
the click-through rate then once you
have low click-through rate using the
CTI information and there's some other
information such as the the amount of
money that people would like to pay then
you decide how to show for
advertisements then probably you do some
post-processing then you display the s
through you ters then from users click
behavior then you get some pastry heat
feedback and you so then you collect an
airlock now from the log you do the
training again so really such a cycle as
a typical way for computational at the
word Huysmans so this click-through rate
is an important component okay it's not
the only component visible whole
advertisement system it's very
complicated but CTR prediction is
certainly an important one the
definition of CGI is like this as
basically are the number of clicks
divided by the number of
impressions so impressions basically
means a kind of pageviews
so like among so many page views that
users have provided then how many of
them they they click on the s so this is
the CTR rate then follows internet
advertisement company then usually you
calculate this CTR times this amount of
beat to rank those are the red Huysmans
so roughly using such a value then you
can have a rink of all those are the red
Huysmans now of course you want to show
the top ones so that that increase your
revenue
there are many existing works on CTR
predictions here at least two
representative ones in this kdd paper
they talked about the google CTR
prediction system and in this paper this
is about the Cynthia predict at
Microsoft then I want to argue lat CTR
prediction can become a binary
classification problem the way it works
is that all the click-through events can
be considered a sequence of data
instances so at one time point you have
features collected for for a user as the
behavior of the user so you have a bunch
of features and in the end let me know
whether this user clicks on this page or
not so this is like a negative label but
if for the next instance you also have
features but you get a Creek instance
then that's like a positive label
therefore you have a binary
classification problem
yes yeah yeah in general CTR said we
have only positive and negative so
positive means that means click and the
next negative means not click ok so now
we have a binary classification problem
there are some special properties about
C Kia prediction first this data set is
highly unbalanced because most popular
data instances are negative only very
few of them they are positive therefore
evaluation criteria cannot be accuracy
anymore usually people use a you see
this is basically area under the curve
that's the major evaluation criteria and
there are a bunch of other criteria for
imbalance the data can also be used in
the above features so for example the
type of these are the vert Iseman and
the browser that the user is using all
those things can be considered as
features but I'm not going to focus on
features in my talk today so now we have
a binary classification problem we are
going to train the logistic regression
to get a model so what is the logistic
regression the basic idea is that for
any label feature pair of Y and X then
you assume a probability model so let P
of this label given X is this 1 over 1
plus exponential something that's the
logistic model so this vector tablet is
a parameter to be decided
so let's low probability model now let's
assume that we have a patent a bunch of
training instances that's our training
set so here I denote why I X I which I
from 1 to L as my training set so there
are my training instances then logistic
regression they find this weight vector
topic by maximizing la notte likelihood
now we are given
so many training instances you just
calculate the product of all the
probability values and you maximize this
so this becomes a maximization problem
but usually we minimize the negative log
likelihood then press the regularization
term in the end then we get a so called
regularized logistic regression that
takes such a form you minimize over a
vector variable tablet and then there's
a regularization term this W transpose W
over a 2 then plus C times a summation
of this is actually logistic loss as a
kind of trending losses so C is a
parameter to balance between
regularization and the training losses
so this becomes the standard form of
regularized logistic regression but now
for Sigma prediction the data set can be
pretty large so we want to design a
distributed Newton method to trend
regularized logistic regression and we
intend to use a Newton method what's the
basic idea of Newton methods first you
calculate the second order approximation
of your objective function and you
minimize over it to get in a search
direction and here we call it the Newton
direction so specifically if WK is a
recurring the iterate of your model
vector and then the second order
approximation is actually the gradient
times your our search Direction S Plus
this is health times s transpose times
this Hessian matrix which is the second
order derivative then again times this
vector s so we hope to find that is s
which is Renu 10 direction so we know
how to update public a to the next
iterate that's the basic idea of Newton
medicines enemies minimize this
minimization of sub-problem can actually
be written in in in the same way of
solving a linear system ok please notice
that this is an anchor
strength quadratic function so
minimizing it you just make a little
riveted over s to be 0 then for you all
you need to do is to solve this linear
system
professor problem of our of solving this
linear system in order to gather the
Newton direction
well the main problem is left this
Hessian matrix is going to be too large
which we too large to be stored so how
large is this Hessian matrix the size of
this Hessian matrix is n by n where n is
the number of features all the reason of
blood is because our number of variables
let's actually the size of the vector
table is exactly the same as the number
of features so so this is going to be
large if your number of features is it
is quite large yeah okay it depends it
depends for for it's in the experiment
I'm going to show for CGI x-ray right
now my number of features is not so
large it's not so large but number of
data is pretty large so not only the
storage but also the calculation of this
Hessian matrix that's another issue but
in my experiments I do have some other
data okay other than CGI their number of
features is pretty large so now we have
basically recognized a lot the storage
of the Hessian matrix or an articulation
of Hessian matrix may be difficult but
fortunately for logistic regression
this Hessian matrix picture has a as a
special form as a special form it is
equal to this is the identity matrix
plus C let's remember that's the
regularization parameter then we have
this matrix X transpose times a matrix D
times matrix X so what is X X is
actually a data matrix
so each row of Reis X is actually a data
instance we basically put all the ink
instances together into one matrix
that's X then the T is a diagonal matrix
and the follow GC regression that
diagonal elements takes this form so the
dii will be this exponential something
over 1 plus this exponential
paper so this Hessian matrix is a
special form then now what we can do is
to use country a gradient method to
solve this linear system so remember
that to get in a Newton direction all we
need right now is to solve this linear
system then we do we decide to use a
country of gradient method that's a one
type of iterative method okay for for
any linear system you can use either
direct or iterative methods the
reclamation means like Gaussian
elimination but for iterative method
conjugate gradient is a very commonly
used one and for this country a gradient
method all you need is a sequence of so
called
matrix vector product and right now our
matrix happens to be recession matrix so
all we need is a sequence of Haitian
vector products if you look at this
these are haitian times our vector s
length because of the special form of
the hessian then our Haitian times s can
be written as this sequence of matrix
vector product so remember this
transpose DX is the most important part
of the Hessian matrix but now instead of
trying to calculate that matrix first in
the multiply it with s I calculate x
times s first to get a vector then T
times this vector because these diagonal
so this multiplication is very easy then
I do X transpose times this vector so
now we have a so called hessian-free
approach for the Newton method I use
traditionally for applying Newton
methods you got to have the Hessian
matrix but now by this by this technique
we are still using the Haitian but
without explicitly form in the Haitian
matrix so this is called patient free so
this has this has been developed for
machine learning in some pastry works so
for example in this Joan of machine
learning research paper in 2005 they use
such a technique for arrow to loss of
support vector machines
I mean linear case not a little kernel
case and this is our adjourn of machine
learning research paper Inglot we we
apply this kind of technique for
logistic regression and the people have
also try to paralyze such a technique so
this is actually a term project in
University of Connecticut so this
student actually apparently use just up
actually paralyze the procedure in this
paper in the right now we are I'm also
going to have a have a parallel
implementation and this implementation
is actually extended from the that the
implementation English software live
linear yeah
oh you mean I'll do those functions yeah
oh yeah okay so roughly speaking if
you're Haitian matrix has this kind of
spatial form then you can do Haitian
free Newton method that's yeah yeah yeah
right roughly speaking so for example if
you have have l2 loss support vector
machines then you can do elect but on
the other hand you can sync from so if
you can see the cosine Newton method
like l-bfgs so a V of G is actually
intends to approximate the Hessian
matrix by a finite set of vectors so
that can be considered as kind of a way
to do Haitian free Newton but an
approximate Haitian free yeah so I
believe there are many possibilities but
here we are still thinking about exactly
using that Hessian matrix but happens to
be let in had such a special form so we
can do lat
oh here I'm not doing l-bfgs our failure
ization is mania how to paralyze this
patient me not sharing ok so here of
course there are several ways to
implement this matrix vector product so
that's a change you can say that because
of n different implementations may give
you different scalability and the for
l-bfgs potential you also need to do a
matrix vector product so the correlation
techniques are similar I think so
basically his question is is to ask my
opinion about Newton method and LPF jeez
Oh beef juice is a quasi Newton method
if you look at the history of
optimization after like three or four
decades cosine Newton and Newton are
still there so the short answer to yours
is they have less strong points in
different situations yeah
okay so from the optimization viewpoint
I will usually for optimization
researchers they don't particularly say
problem structure okay so the difference
between cosine Newton Newton is run
Newton
basically you get a so called the
quadratic convergence in the end so for
a lot of scientific applications
sometimes you need very accurate answers
in the end then in other situations
quadratic approximation is good but
sometimes you don't need that right but
in our case probably you want to go with
cosine Newton when you have first order
methods you're roughly it has the
difference
oh well that's a good question yeah the
short end there's no right now this
doesn't work forever
yeah yeah yeah yeah that's right yes
that's right that's right
so yeah yeah that's right yes sir yeah
yeah yeah yeah oh yeah okay yeah that's
a good question
so essentially what you are saying is
that the comparison between Newton and
the quasi-newton on one computer and
distributed systems may be different
that's right so we need more a bunch of
serious comparisons oh yeah so to answer
you I we are trying to do a serious
serious comparison between Newton and
cosine Newton in a distributed system
but that's not the focus of this talk at
this moment it was for fuller java re'
stock I want I need to finish that
product with that company we need to
have something deployed yeah so yeah
yeah so so usually the the the number of
CG iterations in general only you don't
need many situations yeah yeah yeah Otto
is a very small usually verify or attend
these kind of things for Newton Mason it
okay it depends but I would say in
general okay if that's a good situation
that's like 30 40 in these kind of
things that's possible yeah and also
depends on when you start the whole when
you stop the whole procedure yeah it
depends
yeah
oh okay so yeah yeah yeah right okay you
whip the number of total country a
gradient iterations it also depends on
like at each Newton iteration so
remember that at each Newton iteration
we need to solve this linear system so
we have a sequence of situations in fact
you can just stop at any time you can
say okay I just do 10 CG iterations
right so you have if I outer iteration
then the total becomes a 15 that's
possible and actually use yeah okay then
there are many different kind of
stopping criteria and but all you want
is to ensure that the overall
convergence of the whole newton
procedure so what i what i I'm using
here is the so called that's the so
called a relative stopping condition of
solving a linear system so you just
check the residual of the left hand side
minus the right hand side then divided
by the norm of the right hand side so
that's a relative error that's a very
standard way to start an iterative
procedure of solving a linear system
yeah okay so now let's try to check how
how we do the miss Farrell a trick
vector product so solar for data matrix
is now distributing the restored so we
have x1 now this part this segment of
data you know the what and then assume
we have P machines so it's like this and
we notice that if you want to multiply
this X T transpose X T x times s is
actually the same as the summation of if
you use the first segment to do X 1
transpose d 1 times X times X 1 times s
and a plus the end so now we want to
paralyze this summation so each machine
will be responsible for one part so we
use a known redo procedure basically the
first node is responsible for this
calculation and again we use a
a Haitian free approach so we do this
matrix-vector first so in the end we get
a vector and after that we use old or
reduce procedure so that all the notes
they gave is matrix vector product and
there at this moment we are using MPI
for the implementations but even though
we can use other programming frameworks
so
okay yeah yeah yeah yeah yeah yeah
that's a good question
so for CTR prediction basically the data
X is sparks is very sparse and so it's
it's true that now synchronization is
going to be an issue is an issue we are
developing some other techniques try to
make the computation to be more balanced
so that can be done but at least right
now for this we don't say we randomly
partition data to several segments and
do this proceed
mmm yes there are different way to store
sparse matrix and also for computation
I'm not sure because right now these are
this is little bit spatial is remember
we are doing that hessian-free these
kind of things so not sure I need to
check I would say that okay yeah oh yeah
okay right now we are using the row
oriented so that's the instance wise in
storage because if when you do matrix
vector product you don't have to get
transpose so even if you do a row wise
storage for calculating X transpose
times a vector that's very easy
yeah but then of course we can also do
this feature wise storage that's another
story
so essentially what we we do is that we
later age every node to have this oh oh
sorry I need to put s because this is
this becomes a matrix so we let every
know that where this patient x vector s
then it's like after getting that
Haitian vector product then each
individual node they run a Newton
procedure to update their atomic vector
so immediately every node has this
weight vector W so they maintain okay so
we don't need to to Dula communication
and this weight vector then I want to
talk about our system architecture
oh yeah okay so yeah so probably you
want to ask odd equal a number of
features is larger than the capacity of
a computer then the answer is right now
our design cannot handle that so you
have truly large-scale number of
features then no we need to do something
else so right now we basically assume
that our weight vector can be stored on
every single computer yes thank you
No so sorry I don't get your butt okay
so but for but I don't quite understand
what you mean numerical stability this
this kind of Newton approach that's a
well developed technique so so usually
when people talk about stability of
Newton messes now that means the
convergence people or say the number of
iterations so of course if the problem
is more well conditioned then the number
of Newton iterations that you need is
smaller
otherwise you will be you need more
iterations and in in our situation here
that is related to the parameter C or
Sailor regularization this open C is
small then the regularization term
dominates then you have a very well
conditioned problem trouble so you can
finish in one iteration for the Newton
method oh oh yeah yeah oh yeah right
that's right that's right
yeah for the whole system if we are
running this Amazons system so the data
is actually stored in the Amazon s3 so
that means for the for the company
product all the new data will be well
goal there will be story there but for
the training for the training we need to
to move data to different local disks I
will talk more about this later
this release also takes time right now
after the after each local disk gets
data then we need to do the so-called
encoding and a training procedure so
encoding means feature generation I need
to generate
features in a parallel way hmm oh but
because we we only move some kind of raw
data into each local the local disk but
from the different features probably we
need to do some kind of interaction to
generate new features
so there is an encoding procedure now
after that we do a parallel training
then okay so after training then we get
a model then that's for practical use
then again we we get a log and we go
back to last orig so roughly you master
system let's do some experiments
well this table X which shows you the
cost of renting sync on Amazon where for
us well it's just not cheap but anyway
we use left for experiment and therefore
computation computation is done at
Amazon ec2 so what we use is basically
rent each node to be such a machine and
in the way at Amazon to say how powerful
a machine is they have a standard unit
called ECU so easy.you is like a 1
gigahertz of a particular CPU so now I
say ok I want to rent a CPU with such
power it's kind of setting and also
about the cost of renting a machine
actually have two types one is let you
share that machine with others another
is that say ok
Irina B's machine and I want but
dedicated dedicated one for my use then
of course for the second type is much
more expensive and also we need some
additional tools for the whole
configuration of the system and I show
experiments on three data sets so the
first one is the Rio city
as soon as this is an average one okay
so so later their unit is like one key
backers of a 2007 CPU and the mais leg
so called 66.5 such unit this is the
typical machine out oh well that depends
on for the experiment setting so if I
164 I go to render 64 if I want six
thing and I rent a six thing so results
of using different a number of No
yeah
yeah there are all kinds of such issues
but busy now we just want to construct
assistance so that we can use yeah so
yeah and the to answer your equation is
that this is not a very high-end emotion
when you're reasonable what I don't see
we use well enough familiar with things
you talk about I think we just use some
typical ones yeah so now about to the
number of our data in the features for
this city our dataset so we have like
about a Sri hungry million instances
before right now for our basic setting
the number of features is not large it's
only about six thousand but we also take
to our data sets publicly available data
set for experiments so this web spent
has about three hundred and fifty
thousand instances but has a lot more
features is about almost 20 million
features and this option is a tense data
so it has about 400,000 instances but
four hundred features so just try to
have a tip a set of different data with
different properties to see what happens
oh no no this is not from Yahoo this web
span was from the ICM a large scale
machine learning challenge they
generated okay late on Holland it is
where a kind of artificially they
download a bunch of web data and they
have spent or announced spec then they
generate a lot trigram of last day has
it so that's why that's why you get so
many instant there are so many features
yeah but you can download it say from my
my web page let's check the running time
analysis so this for this app show so
remember is tense data and I try to list
different time for different components
first days with a loading height and the
length the computation time and the
length at communication time and in this
table actually looks very nice because
what we can see is that the important
thing is about the reloading time
actually Larry the reduction unloading
time is almost perfect but this is
reasonable right because there is no
interaction between lost loading tasks
and computation is also reasonably good
but communication then of course when
you have more noise than it is but for
this case is very good so the overall
scalability is actually very quick
iterations know I but I can give you a
numbers later yeah
yeah okay I can try to check such no we
do binary or diviner
yes so there are many many all kinds of
weird issues in this kind of comparison
yeah Toto optimization okay so in terms
of running time that means for this
payroll a of setting that means
computation plus communication visit
then usually you don't count the loading
time as part of the optimization
procedure but I still need to the
loading time here because for a real
application then you need to consider
total time
yes
oh yeah oh yeah yeah yeah okay
it seemed yeah that's a good thing
well I need to check I need to check
probably probably synchronization is
something that we didn't show here
if this one is really good this one is
yeah this one is real yeah okay so I can
go to check details
but yeah I think probably we miss out
some information in this table I believe
lead may be synchronization or something
yeah yeah we should we can do let oh
these are the seconds is the seconds in
seconds but this is a good case this is
actually a good case this is now see
here is nothing
can you say that again me coefficients
oh you mean a tablet vector performance
okay now this one actually we for this
one make sure we don't care about
performance right yeah yeah for this one
for this one okay but for CTR I'm going
to yeah forsythia we I was sure force it
here we were sure that so this is the
and this is a result I'm sick yeah so
this is a small subset only 60 million
the reason for using a subset is because
so we are able to run to use one node of
Amazon machine for the whole for the
whole job this is still good
this is still okay still okay oh you
want okay so the total the total now the
total time this is a for using one node
is about seven hundred and ninety
seconds and therefore 16 nodes this is
about 79 seconds 16 one six yeah and
then here is the result of using a
larger one and therefore a larger one we
can start only from the four nodes
so for the four nodes we have about 1200
seconds for the training and now when we
increase the number of nodes up to 64
then this is about 125 seconds
yeah oh no no they didn't see no I think
there's no yeah yeah this okay stood
okay yeah right I think so yeah I think
so I think this is the problem that when
when we try to generate a table by
having only one digit after a decimal
point then apparently we made made some
mistake
yeah so yeah so you are right that this
should be 1,000 yeah this one should be
one sound
so they are but this is only one note
yeah so I think probably but well the
submission loss to Vedas is almost real
I sent a slightly different so you need
to check like how we pretend this is the
one note this is one note yeah but the
summation is already very close to this
one but there's a small difference so
there's something we need to check I
think that may be related to how we
calculate the time of different
components yeah yeah yeah yeah so
probably we probably we we we just we
mainly located a total total time at
these moments that's more an iterate so
let's check the scalability so bicycle
bility I mean that we we fire we double
a number of notes
we also toggle the data size and then of
course we hope that under perfection
scalability the total time will remain
the same
no actually this is a shirt with other
people
yeah yeah this is a kind of wall clock
time yeah yeah so all those are issues
and the ink getting such information
well I was sure I will show something
lat yeah so we try to check that if you
double the number of notes and also the
size then how is it a result I guess
let's look at lost hoof okay yeah okay
we're running out of time so let me just
show two figures so let's look at this
web spam this web Smith is a large
number of features with a number of
features that I showed earlier seems to
be smaller yeah okay so this is actually
the okay there is a reason I mean look
pull a design of Lahore parallel
experiments there are many many issues
for this one I'm not using the whole set
user subset then I keep babbling the
copy date had to give two different
notes because I want to do certain
control
yeah so that's the reason why my number
of features is actually smaller you
start from a smaller subset but anyway
let's just look at the result so the
main problem is that we see that
communication cost is actually a concern
that's a remain conclusion another
reason is it is it is so so we see that
scalability is going to be paid if your
number of features is smaller user then
your communication cost rates lower
right but when you're a number of
features is larger because you have to
transfer so many elements for different
amounts then of course then your
scalability is not so good then in in
this slide we try to check the
performance and what we are interested
in here is to escalation in a real real
world application so we say okay if we
keep increasing the number of training
data can we really improve the
performance
so we chose such a figure this is a UC
versus that they'd have size but
actually a you see we we can only show
the relative improvement because it is
related to the business information of
that company
so roughly you can see that when we
increase the data size from a smaller
one to about 20 million then we
basically reach the pace tray you see
after that even if you increase the
amount of data at least a full occur in
the feature setting then you don't get
much so oh here we we haven't done any
experiments on nonlinear classification
at this moment so I just say that if you
have 20 million instances then let's
already give you basically la paste a UC
prophets the size of data with of such
data set actually the size is only
around 18 gigabytes so naturally us
equation do we really need a distributed
setting so why not just copy this the
whole data to one machine and we can do
it a training right yeah so this is
actually an issue worse for that we
should try to address so now I will have
some interesting discussion so the first
the question that I asked myself is that
do I really need a distributed system so
I try to check least one one machine
versus distributed environment I mean
let's just look at the CTR result if you
look at the total time so this blue
sorry the line is the total training
time of using different a number of
nodes in the least Hulu palette RT the
line is the training time of using a
different a number of course and what
but I also listed a loading time so if
you look at the recent read sorry the
line that's the loading time of using a
distributed system and if you look at at
least read in the party the line that's
the loading time of using one machine of
course it is a constant right the young
you have only one one disk and a fixed
amount of revenue so the loading time
remains constant this is actually an
issue and because of this reason you can
argue that yet probably doing
distributed setting is useful because
then you get parallel data loading yeah
so not even talking about the
performance of doing parallel
optimization the parallel loading is
something very useful in the folder the
the optimization algorithm whether we
have found out is left after we increase
the number of cores then up to a certain
point then the running time does not
decrease that much so the program is
that even though you have eight or so
for right now for updating we have eight
cores but just in a one much in stating
you need to move data to the upper level
of the memory so CPU so those cores can
use them right but but for our
calculation CPUs are very fast but while
moving data up to say level three cache
is relatively slow that's the reason why
we don't get good speed up
oh you mean for were distributed like
moving data told a local okay I will
talk about that yeah yeah and for our
stating that part is is not an issue so
I will explain why so this is a simple
comparison between the the the
advantages and the disadvantages of two
settings let's skip please
so this answers your equations so so in
my my figures in the above slide
actually I didn't include the time for
moving data to local disk before I
started running this MPI job for
parallel Newton method so how do we
handle that this is not an issue for us
so for our application should we use
data in a sliding window so that means
suppose we want to trend data in the
past week length then after today then
for tomorrow then we just keep on moving
and we throw away paste the data so we
are not going to trend data in say three
weeks ago at least of all occur in the
city so what we have done is right now
we just spend some money to keep renting
some local disks and for any new coming
instance they just they are immediately
dispatch that through a local disk so
it's likely I already stored layer in
the length so this data moving is
completely completed before training so
when we when we decided to do training
we could so at Amazon we can render
machines to mountain lost local disks
okay so now we don't have the data
moving from as a result this
and but babysitting of course data also
constantly removed from the local disks
then I want to talk about the issues of
law workflow because this is the whole
real application yeah yeah I'm going to
finish so I want to up say that
distributed the training is only one
component of the whole workflow if we
check the system architecture so I need
to as we have mentioned we need to
handle this data movement but also
featured generation so training is only
one part and I have mentioned that since
we are we are able to handle this data
management issue but we found out that
for our real application data encoding
that means visual generation can be very
time consuming so here then is a reason
suppose we have a feature called the
fruit and each node contains some kinds
of fruits like leaves
then in practice we may not know what
kind of fruits in add events but then
now we want to encode them through
different numbers so eventually we want
to generate such Indic indices on
different nodes so this again is a kind
of parallel process this is actually
quite complicated for our case so in the
end at least right now for our system
what I can tell you is that this
encoding takes more time let may take
more time let the training
oh but I need to say not for that
setting of 5000 features because of this
for this experiment I'm not doing I try
to use some only simple features but
what I want to tell you here is that if
you are working on a real system then
training is only one part of it there
are many other weird issues so this is
the last slide so for the optimization
part of course being an optimization
researcher we are interested in
other optimization algorithms such as
cosine Newton or a DMM and a lot of
others but the lesson of this work is to
show that first looks at least for these
applications distributing the Newton
method on earth for our use in the land
for a practical peak data application
there are many many issues beyond this
mission Danny yeah that's my conclusion
so thank you very much
yeah
yeah yeah yeah that's very right yeah
yeah yeah yeah okay so essentially okay
what I have said is that even for CTR
prediction is already complicated enough
but you can even argue that maybe city
aphrodesia is now a useful component for
a whole advertisement right so who knows
that so then being an Internet company
length if it was a that's a good one
then usually need to have very many
smarter designs so for example you can
easily do a/b test rows things are very
very complicated but but you argue like
to say probably to use some simple
methods but for me this distributive
Newton is simple enough so I just take
it just plug it into the system for my
use so that does my answer to your
question
yeah yeah you're saying probably those
experiments are not Latin meaning for
well that's partially right but this is
the distributing a machine learning
workshop so we try to show this out some
surgeries are
yeah yeah okay yeah what we she said is
that there are a bunch of existing
parallel so optimization people they
have done some existing parallel
frameworks yeah so but maybe my answer
to that is because to implement a lot
Carol Newton is probably easier for me
to figure out how to use those the
existing optimization take so but to him
to embed so so you mentioned this how
how is a thousand MPI based numerical
there's a that's an MPI paste numerical
optimization and PDE packages so the
most time-consuming part for us is for
all the other part to construct the
whole system yeah so so for us say
writing the Newton method is extremely
easy but then we need to spend some time
to some time to connect this Newton
implementation and then the whole system
right so even if we use other things we
still need lat that part can be
time-consuming
yeah yeah once we had let that's that's
right yeah oh well that's a whole other
issue how to handle imbalanced data but
here we yeah we we don't try to address
that we just think a we just try okay
first we have an absolute value of a you
see right but the more important is
thing for for the the company is
probably to check if this new system is
better than the previous one but even
the answer is yes then that's good
enough
that's actually the situation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>