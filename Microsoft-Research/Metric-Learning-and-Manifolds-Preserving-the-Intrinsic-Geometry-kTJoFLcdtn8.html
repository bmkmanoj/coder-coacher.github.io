<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Metric Learning and Manifolds: Preserving the Intrinsic Geometry | Coder Coacher - Coaching Coders</title><meta content="Metric Learning and Manifolds: Preserving the Intrinsic Geometry - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Metric Learning and Manifolds: Preserving the Intrinsic Geometry</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kTJoFLcdtn8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
great
today we have the meek Barranca coming
across the frozen lake all the way and
the meaning has gone to my McGill before
he came to you dub he recently won the
Bernstein prize at Toms disks department
and he'll tell us today all about
manifold preservation and metric
learning Thank You Misha
so I'd first like to thank mission Hoyt
Forgan izing this talk and for inviting
me here so I'm gonna talk today about
metric learning and preserving the
intrinsic geometry and this is joint
work with marina mela professor mela at
the university of washington
who's my adviser for my PhD right now so
a brief overview I'll give some
background on the problem I'll cover
some differential geometry theory just
to get you acquainted with with the
framework and because I think it's good
to be reminded of it even if you're
somewhat familiar with differential
geometry I'll discuss the discretize
problem and the algorithm and then I'm
gonna show some example an application
I'll give a brief summary and future
research and maybe I'll get to the
consistency of the push word metric but
maybe not I'm not sure that's the most
important thing to do so the problem
we're trying to tackle here is the curse
of dimensionality there's currently a
abundance of large data sets of that are
highly high dimensional and this leads
to some problem in terms of
interpretation in terms of computation
computation and in terms of analysis and
so the idea is to try to reduce the
dimension of the data set and underlying
this idea is to try to find some low
dimensional representation of dis data
but that will preserve all the important
information at least most of it so
that's kind of a goal today and so I'll
start by giving you a toy example that's
kind of
gonna follow us through the talk and so
it's it's worth going over what this
example is so effectively what we're
going to be looking at are images so
it's gonna be faces gray faces 64 by 64
pixels and so and we're gonna have about
700 of those images so we have a fairly
high dimensional data set I mean it's
still small for by today's standard but
it I think it captures the idea but this
data set has some very interesting
attribute which is that it represents
one specific face that's only moving
right to left and up and down and so in
this data set what we have implicitly is
only two degrees of freedom and so we're
going to try to leverage this attribute
of the the data set to help us in our
analysis so it's all images of the same
face David
which of you
you show you the data set itself so this
is actually one embedding of this data
set every single ellipse on here is one
of the faces and I'm showing you a
subset of those faces so it's all the
ways this particular face has been
rotated one way or the other or that's
tilted up or down so the difference is
just a point of view camera in some
respect
so if either the head or the camera
depending on how you want to see it and
so we have two degrees of freedom even
though in terms of representation every
image is 64 by 64 right and so the idea
is to effectively find an embedding like
this one of the data and try to use this
embedding to do analysis or to do some
interpretation so in this case it's
fairly easy we found that one dimension
represents the angle of the head in
terms of life then right and the other
dimension represents the angle of the
head from up-and-down so so far so good
and so there's actually there's a lot of
embedding that exists I'm showing you a
subset of here and they all do various
things the question then is which one
makes the most sense so the underlying
idea for dimensionality reduction is
what I said before is to assume that the
data a lies on a lower dimensional
smooth manifold that lives in high
dimensional space so the the high
dimensional space is the space of the
data and the low dimensional manifold is
the data set or what the data set would
look like if we had every point and the
idea is to recover this now there are
the standard techniques such as pca
principal component analysis that are
going to make an assumption that the
manifold is a linear space and so
they're just going to project on the
linear space but in in practice this is
often violated the data set will curve
in twirl and do many things in the high
dimensional space and this has led to
nonlinear dimensionality reduction which
means trying to find what's the curve of
the on the data set in a high
dimensional space and try to take that
into account when you project it into
the low dimensional space so there exist
already many algorithm to do that I've
already showed three of them but this is
I think a list that does certainly is
not exhaustive but covers some of the
important ideas that have been developed
in this
in this feel so local linear embedding
is a fairly well they all work with the
same idea which is that you use local
information and you try to propagate it
in such a way as to recover the full
manifold and so looking local linear
embedding what it does effectively it
tries to express every point as a linear
combination of its neighbors and it's
going to use that to then recover a
lower dimensional representation that
preserves this local weighted average
the laplacian again map in the diffusion
map work slightly differently in that
day what they do is they define a random
walk on the set of points and use that
diffusion process so to speak to to find
an embedding so it does an eigen
decomposition of the random walk or the
diffusion process and use this
eigendecomposition
as the embedding and then you're gonna
have other algorithm like linear tangent
space alignment that is based on the
idea of doing local PCA and then
patching those PCA together into one
embedding or as no map which tries to
preserve a very specific geometrical
object which is the new geodesics so
distances on the manifold is what you're
trying to preserve when you do either
map but there there are known problems
with these methods often they will fail
to recover the geometry and there exists
no formal framework to wreak impair them
and I showed you a few but there really
is a very very long list of method and
it's not clear when you start which one
makes sense to use and why and so to
answer those two problems what we first
have to do is actually define what it
means to recover a low dimensional
manifold for example if I use a very
simple manifold a classic example which
is a Swiss roll with which has a hole in
the middle of it we have two potential
embedding this would be the ISO map and
this would be the linear tangent space
alignment and it's not clear from this
image with saurons correct we kind of
know this
probably wrong in some sense but we
don't have a framework to say whether
this is correct or not
so underlying all of this is we're
assuming that recovering the geometry is
an important goal and we know that this
is possible that if there is a manifold
we have theoretical guarantees that
there exists at least an embedding of
the data so if we know we have a
manifold of dimension D we have
guarantee there exists an embedding of
this object into a lower dimensional
space which is of dimension to D so you
can any manifold of dimension D you can
embed in our to D effectively and so we
we know there is a way we know that
there's an option for us but rather than
try to do what a lot of manifolds are
doing manifold embedding methods are
doing which is to try to find the proper
embedding that preserves what they care
the most about we're going to try to go
around this and say well is there a way
to correct an embedding is there a way
to say well I've distorted my space and
I'm gonna I'm going to recover the
proper metric even though my embedding
is wrong and this is called recovering
the Romanian metric so to formalize
things a little bit when when I'm
talking about an embedding I mean that I
have some manifold and I'm finding a map
from that manifold to a new manifold and
that means it's gonna sense every point
from one manifold to to every point and
the other manifold or truer to the image
of the map and it also has the meaning
that if it's an embedding it's
differentiable which means it also Maps
point in the tangent space of the
original manifold to points in the
tangent space of the manifold in which
you're embedding which in our case is
going to be a clean space and this is
important because geometry is defined in
terms of the Romanian metric which is
just a
inner product here that is defined on
the tangent space and so what that means
is if you know the inner product on a
tangent space for a given geometry then
you know everything you care about and
the idea then for us will be to say how
do I recover what the inner product
should be in the embedding to correspond
to the original geometry so to drive the
point of what it means to have the
rehmannia metric it means that you can
do inner product but then that means I
can do all the usual geometry I can
compute angles between vectors in the
tangent space which means if I have a
line like two lines crossing each other
I can try to say well at what angle are
they crossing each other also means I
can compute the length of the line on
the manifold and it means I can compute
the volume of a subset of the manifold
or the full manifold and so effectively
I have the volume element which is quite
important if you're trying to do say
define a Hilbert space on your manifold
which is kind of key to do kernels a
Gaussian process so if you have a space
of functions are on your manifold and
you want to use those function to do any
form of regression of classification
well what you actually need is the is a
volume element so this is where we start
thinking a little more in terms of you
know learning and statistics and less in
terms of geometry but the key idea is
that all of this is encoded in the
Romanian metric okay
so before we go forward it's still worth
defining one more thing which is nice
amma tree if I have an embedding or a
map from one manifold to the other we
say that the manifold or isometric if if
this map satisfy the simple condition
that under the Jacobian of the map the
inner product in both space is equal and
so all we're saying here is that if you
do know your inner product then you've
you've defined your geometry but then
the point of having an isometric
embedding is just a question of
preserving the inner product that means
that given a an embedding I can try to
define what's called a push forward
which is to say if I have the original
manifold then I'm mapping into a new
space a new space has its own inner
product how do I correct the inner
product so that it's equal to the
original one and this is effectively how
you do this it's by looking at the
inverse of the Jacobian or the pseudo
inverse depending whether this is full
rank so that the inner product in the
embedded space is now equal to during
inner product in the original space
I mean the identity map would be an
isometric map you could you could unfold
the the I don't know if that's what you
were thinking of but if you have the the
Swiss roll you could just unroll it and
it would preserve all the the geometry
it wouldn't have the same curvature but
that's because the curvature you're
losing is his yang try extrinsic
curvature rather than intrinsic
curvature so that would be an example
but in effect what I'm doing will be to
define isometries right instead of
trying to find the map that creates the
isometry I worked the other way around
by saying given a map how do I turn it
into an isometry what interpret what
Romanian metric must have existed in the
manifold in which I'm mapping to so that
now it's isometric why two images
because if your if your mapping into
occlusion space right 2-dimensional
occluding space the occluding space has
its own metric which is the identity
matrix right and that metric no longer
preserves the inner product in the
embedding and the idea is to say what
should have been the metric in your
cleaning space so that now it preserves
the inner product right so when you're
doing an embedding there's a question of
does the manifold in which you're
embedding already has a metric and
usually it's an implicit metric and
including space is the identity one but
I could have defined a kid in space with
a different metric so that when I mapped
it turned out to be an isometric
embedding right so there's the that's
kind of the the the subset or the this
is the topology of the objecting to
which you're mapping and then there is a
geometry which is the metric eventually
this is Betty
which represented the
which represent
so it's going to be a it's not going to
be a victor it's going to be a quadratic
form that's going to define the inner
product at that point mentioned that
your math is higher than
the dimension dimension of the pleating
dimension of the space because you also
have this but right so in terms of
memory yes your your keeping around d
squared so if you're mapping in
dimension D and you also have to keep
around this quadratic form which is
going to be d squared and the hope is
that D is so much smaller than the
original space that you're still doing a
considerable dimensionality reduction
yeah that's that's the underlying idea
does it seem like they're playing the
practical application abilities with the
vegetable inspirations that as long as
long as the preserve that's been to get
I don't think you do you know I think
it's an interesting question that I'm
actually wondering about because you're
right you can define your hilbert space
just in terms of on your volume but as
it turns out I think it's actually
simply to just get both at the same time
you it's true that you don't actually
need your tangent space it can be useful
in some cases and I know examples where
it's useful but it turns out that it's
it's actually very easy to obtain the
metric or to obtain the object that can
tell contains it so and so okay the idea
is how am I gonna find this what's
called a push forward metric which is
the corrected metric for the embedding
and the idea is to use the Laplace
Beltrami operator that's defined in
terms of the metric and it has some very
nice property one of which that comes
out of this work is that it contains all
the geometry of the object and at the
same time it's coordinate free I can be
expressed in coordinate local
coordinates but it is coordinate free
which means that if you've computed the
Laplace name for the object any way you
represent the object the laplacian stays
the same and it contains all the
important geometry and so what happens
is if you compute if you want to recover
the metric give
the laplacian all you need to do is
apply the laplacian through a product of
coordinates and that will tell you what
is the metric well actually the inverse
of the metric for those two coordinates
and so you compute it for all pairs of
your embedding or your manifold and
it'll give you the full correct or full
inverse of the quadratic form which
means that if you have an embedding the
same trick applies and that by
definition gives you actually you still
need to prove it but this will give you
the push forward metric and so the point
is if you do know the laplacian you know
how to correct your space and so in
practical Turman i think i've kind of
explained that already but given a
sample of my manifold sample according
some density and embedding of that
manifold into occluding space or some
space i can recover the full geometry by
simply applying this trick locally and
so I will recover at every point a
quadratic form that expressed the inner
product at that point it gives me the
volume element or any other geometrical
object I'm interested in and so the idea
is to get an approximation of the
laplacian on the discrete space that
will then give me all the geometry I'm
interested in and the idea is that this
already exists it's a common trick that
probably some of you or most of you are
familiar with which is to construct a
graph laplacian by applying a kernel
between every pair of points here I'm
using the classic Gaussian kernel and
then defining this thing here which is
just a random walk on the graph
laplacian or on the graph and this is
called a normalized graph laplacian and
if you take the limit as you let the
bandwidth goes to zero this is known to
converge through the laplace Beltrami
operator on the manifold now here I'm
assuming actually that the sampling
density is calm
and if it isn't there's a trick to
renormalize the the graph so that the
graph laplacian now converges through
the sorry what how would you tell
well so that that's kind of the idea of
renormalizing the the graph which is
that you can renormalize it without
knowing what it is and it will give you
the same object in the limit so you
don't actually need to know whether it
is uniform or not and so I'm showing you
just it's just to avoid being overly
complex in this slide but there's a
trick that guarantees that you'll have
the right reply seein okay and so this
means that for us to recover the
remaining metric it's a simple case of
obtaining the graph laplacian finding
and embedding and then applying the
laplacian to the coordinates of this
embedding and this will give us the
geometry so we're going to end up with
an embedding of every point with a
quadratic form at every point and this
is what it looks like so I'm I'm taking
this object which I call the hourglass
and then I'm using the I believe the
diffusion map or the eigen map to embed
this object which kind of changes the
form it makes this part less curved and
this part a little flatter and so what I
get effectively is the subject so every
line here is expressing the graph it's
just telling you what your neighbors are
so you still have a notion of topology
and every elif's represents a quadratic
form at that point and so if an ellipse
in that space is circular that means
that locally the embedding was isometric
if the ellipse is distorted it's telling
you by how much you've distorted you
your geometry at that point and so we're
finding that in the middle this is
fairly isometric but as I move towards
the edge of the hourglass I'm starting
to see that the metric is being
effectively pool which means that every
unit of point is worth more so that
means it's being compressed so what
you're seeing is actually the opposite
of what's happening the longer the
metric becomes the more every point is
worse so it means it's been compressed
compared to the original metric
and here actually at the edge we're
starting to see some artifact because
it's starting to be difficult for the
curvature so when you're defining the
laplacian you're starting to see points
that shouldn't be observed because it's
not sample densely enough and so there's
kind of an implicit idea that this works
well provided that the manifold is
smooth and doesn't curves too fast but
if it does you need more sample points
ok so what does it mean in terms of the
original data set I presented to you
guys well it means that if I look at the
three embedding I first showed we get a
clear idea of what's happening so if I
use the ISO map I'm seeing that
effectively the the embedding is fairly
close to isometric it's not quite
correct it's being stretched well it's
actually being stretched in this
direction and it's being compressed in
that direction but otherwise it seems
like it's preserving the the geometry
data set well if I'm using lineage
tangent space alignment I'm seeing quite
a fair bit of distortion at the edges
here and finally the Laplace and eigen
map which is still one of my favorite
has a lot of distortion in it and so
this now allows us to make actual
explicit statement about what's
happening in the embedding
various
the shape of the quadratic form so if
the quadratic form is distorted by the
equivalent as we seen as a bond it is
distorted but it distorted at the same
fashion that actually
right and actually that's an important
point because now if it's only being
linear distorted in different dimension
it's easy to use one point and then
rescale everything which I'll do in a
second so because accuracy the suppose
right there is on the safety pin
I mean that's kind of what I'm trying to
I'm using their code but that's why I'm
trying to show is that actually we think
we're doing a good job even the
algorithm that are trying to be a
symmetric
present everywhere
so for this this one no choice and
everything that allocation
okay yeah so if you cover yourself for
that
I haven't tried I mean to be able to
define an algorithm that will always
isometrically embed your data up for any
manifold is equivalent to solving Nash's
theorem and so I take for granted that
we don't have an algorithm that will be
isometric there exist theorems that you
can show it's isometric if you in l2 and
so if you have an infinite space it's
easier to construct something but if
you're trying to embed an occluding
space of low dimension it's very hard
you could use Nash's embedding theorem
that is guarantees isometric but it
means twirling around around dimensions
and so numerically it's very very
unstable
system innovation including Sydney as
imagine that the business or product to
the analysis imagine as American beer as
magic butter has melted it carbon if it
is a salt be generous an identity would
fail then they were gonna try to evict
John are purposely approach to fix a
problem like that because analysis Chris
videos they're not special tracing for
demanding from cousin from the TV
geometry non-space or computers to keep
the computer Keys fence your do
statistics
so I'm I'm not sure I'm gonna fully
address your question but I think the
main point that comes out of this is
assuming you do have an algorithm that
will always give you an isometric
embedding that preserves everything you
want most likely you're giving yourself
too much trouble because the idea is
it's true this embedding is no longer
isometric but I dunno by how much that
means I can still do computation in this
embedding without having to worry by the
fact that if I ignore the the quadratic
form it's not isometric which means I
can instead of trying to find the best
embedding I can find a quick embedding
and then correct it so the idea is that
it can save you in time so ISO map is
already known to be a computationally
intensive approach to the problem and so
you could try to define something as
simple as like random projection that
are super fast as long as you have
enough of them you're defining and
embedding and then you can correct it
and so that's kind of where I'm heading
to I'm not trying to say there doesn't
exist the perfect hang algorithm maybe
there is I don't think it's easy to find
one that work all the time but I'm
trying to say maybe we shouldn't try to
find one because we can do better we can
given an embedding we now know how to do
all the geometric operation you may care
to you and so it's about finding quick
and efficient algorithm and then
corrected them I think is the best way I
can approach a problem I don't know if
that's what you feel as I answer your
question
so serious about his approach
that you're given
but you have non-spiritual face
actually perhaps at that time without my
didn't realize is an approach actually
perhaps just another version of a
special magnet those are potentially
franchising because some of the people
and as that so if it was his network can
be as imaginative suppose mrs. Billings
was
yeah I think we should cover that fling
but to me to make a final point and I'll
continue after that we can talk is the
the Judy's ik is not the thing and the
most interested about the volume element
as Misha pointed out maybe the only
thing I actually care about okay so
LTS a which seems to work initially but
actually turns out to be slightly
distorted but this can be corrected as
you pointed out by just essentially if
if the distortion is along two dimension
it's easy to use the metric - then we
scale everything and so this is what I
did here I have something that looks
like so my trip is actually slightly
distorted and then I used one point to
locally transform everything and then it
turns out that pretty much everywhere
it's now isometric but if I have
something that's distorting too much
like that naive ISO map then I can only
apply this locally so if I try to use
one point here to correct a space I will
be able to correct around that point but
the rest of the space remains distorted
that's fairly intuitive but this is
where I think it becomes slightly more
interesting which is to say well
actually I don't need to worry about the
embedding if I know the metric for given
embedding I can do computation and so
what I do is I consider an original
manifold and I have a line around it and
I'm trying to compute the judy's ik I
can do it say in the izo map that's
supposed to preserve the judy's ik or
into any other space and so if I found
the distance in the the embedding I'm
getting wildly different results if I'm
using the shortest path in the graph
defined by embedding I get very
different result but if I correct the
distance here by the metric I'm
effectively getting the same thing now
it's not perfect
but I'm seeing that all the embedding
now has the same geometry I can compute
the same judy's ik irrespective of which
the embedding am taking and I can do
this for volumes
to approximate them
it's right
the proposition is that
yeah so yeah the idea is that that
Colonel the Laplace and I'm using to
define the metric it's kind of key
because it contains all the geometry now
it doesn't mean it if there are errors
they might not no longer be the original
geometry so approximation of it but any
of itself it will always compute the
same geometry and that's actually a very
important point because it might turns
out that the original Dmitri is not what
you care about that just means you have
to find a different laplacian and that
object will come give you the same
geometry irrespective of what embedding
you have so I can play the same trick
with volume element and effectively I'm
finding the same thing more Eretz in
part because I'm using a coarser
district ization of the space but
effectively I cannot even compute the
volume element for some embeddings
because it doesn't always make sense but
if I use the correction I'm all pretty
much all in the same ballpark
all right so so that kind of ends the
geometry part and now I'm trying to move
and explain what it means in terms of
learning and so now that I've
established effectively that the
laplacian contains all the geometry the
idea is how can you use that to your
advantage well it's known that in
Euclidean space this operator which is
the usual Laplace operator if you look
at this s PDE it defines any special
Gaussian process so I'm assuming that
this here is Gaussian white noise and
you can show that the covariance matrix
of this object well first this is a
linear operator that means that use a
linear combination of the Gaussian white
noise and so it will be Gaussian and
then the question is what's its
coherence matrix and you can show that
it's a matern Gaussian process and so
the idea is this operator effectively
define what a maternal process is and
then you can use that to advantage by
saying actually I can replace the
laplacian by the laplacian of my
manifold and now
able to define a matter gassing process
on the manifold and I can use that
Cohen's matrix now to do semi-supervised
learning whereby I know the value of a
function at certain points and I'm
trying to predict it over the rest of
the manifold so return refers to the
covariance matrix or the covariance
function it just says so a classic one
is the square root X financial coherence
function but it has the disadvantage
that it's infinitely differentiable so
it's a very smooth process and the
matern is one whereby you can actually
control how smooth it is so the Alpha
parameter here determines how many
derivative your Gaussian process will
have yeah and so it gives you much more
control in terms of when you're trying
to do prediction how smooth is your
kernel or your Gaussian process
depending how you want to think about it
so given the laplacian now I can define
a mattress and I can define a covariance
matrix actually this sort of being with
respect to the manifold but I can then
do learning you know predict what the
value of a function should be at other
points given what I know based on the
geometry so implicitly I'm learning the
geometry of the manifold and I'm using
that to propagate the information and so
you can do that you can think of this
also just as being a kernel or a kernel
Reger Iser for your regression or for
your classification what's actually
interesting is that because now we have
the metric you can also define the
precision matrix not only on the points
but through the embedded space that
means that instead of just learning on
the points I can learn around the points
so if I have a new point that arrives
and I've nowhere to embed it I will
immediately know what the value at that
point should be or what the class of
that point
and so this is a slight departure from
traditional assuming semi-supervised
learning and that I'm actually doing
inductive learning but in the embedding
space so here's an example of how this
this works out so I'm using the three
embeddings that we've followed through
for the the phases data set and what I'm
doing is I'm embedding the points for
all embedding of interest and then I'm
applying a matern kernel with respect to
the embedding space so I'm actually
assuming that this is the correct
geometry and applying a guess in a
matter internal to then predict what the
value of the function should be so what
I'm doing effectively is I'm keeping
track of about a little less than a
quarter of the values of the heads
position and I'm using it to predict
what the value should be elsewhere right
and so if I'm using the embedding space
I'm getting quite a fair bit of error
but if I'm actually using the entrance
in geometry I'm doing a lot better right
and so that's kind of the idea which is
to say that the original geometry of the
data for this particular data set turns
out to be the right way to propagate the
information
oh this is how wrong a particular point
is right because these are angles that
goes from 100 0 to 180 and that's
telling you compared to what the actual
value of the point should be training so
these are just statistics I'm actually
showing you both and a train set so the
tests are about yeah probably should
remove them but there'll be I'm using
about a quarter of them and they'll all
effectively be right you're going to
have the right value at those points so
they're not contributing to the error
the lesson at hiding Maps is the reason
so it seems to not be performing well is
it because the sparsity on on the right
side so what what is
I just think that what it's doing it
it's not recovering the right geometry
and so it's actually well first the
information doesn't propagate stage like
the kernel is a stationary kernel and so
effectively the the by stretching this
space it's it's not propagating the
information the right way but you're
right that the sparsity here is a
problem and what's happening is
effectively you're doing regression on
part of the space that is now part of
the ambient space because you'd assume
that there's no manifold here but
because your kernel is defined for the
whole of your cleaning space you are not
using the space intelligently well if
you define it here you're only defining
it on the space you know right so this
is actually not inductive this is now
low one here is just transductive which
means I can only compute it on the
points I know right so I think it's it's
both the fact that there's you're
computing it with respect to the ambient
space but also because you're distorting
too much of space
how much our machine from when we were
using for timepiece encounters so the
heating iron in bankers actually think
because I've been an admirer of Allah
pausing for romantic and then evening
classes would be real clustering that
mean that would be that's why about the
canoes
agent right and so you're not a paper
buyer and endemic Jodha they do an
aperture to cluster so here's a lead an
active accuracy because I'm gonna run
the last employment Betty and look I see
different classes will be willed Varian
yeah and you raise a very important
point which is that I think I imagined
earlier the original geometry is not
Leslie the best geometry I'm focusing on
this because it's it's the first step
saying well first can I recover it but
second what comes out of this is the
fact that all the geometries containing
the laplacian if you think of different
geometries more appropriate you should
focus on defining L apply seein that is
appropriate for that geometry now when
what you say you use the first few
eigenvectors of the the laplacian for
your embedding and that helps with
clustering what you're doing implicitly
is defining new geometry so the the map
that uses the eigen vectors to embed are
effectively this map for this particular
data set and this is now a different
geometry it turns out that here is the
wrong geometry but for clustering it
tends to be better and so the question
is and it's actually one of my open
problem of how can we use what we're
trying to do to select what geometry you
want what what laplacian we want for
this particular problem and so I think
this is what comes out of this I'm
focusing on recovering the geometry but
an offshore of this is that we now have
a way to characterize geometry and we
should think about what kind of
operation we want which then leaves what
kind of laplacian we want
process that generated the data like
when I look at this problem to me it
seems like there it's natural to think
about the some kind of model that
generates this data
sculpture
essentially what you're trying to do is
recovering the parameter so while not
not really members but we get like the
scent
the settings of that model when it
generated this data is important to
study
rotation
and but you know there's also a lot of
other things that you can vary about the
model Imogen
correspond to other dimensions
what we house how does repeal of this
held us back to that view of the world
so here I'm using a fairly
straightforward vision of the sampling
process I'm assuming iid according to
some distribution on a manifold but you
make a very interesting point that there
might be a different process that's at
play and is of interest and I think the
best answer to that is to say well can I
get an idea of what operator is
generating this hopefully a linear
operator like a translation operator in
your case I in this case and if you do
that in the same way that this defines a
Gaussian process if you have a linear
operator you can define a different
gasping process that is representing the
transfer of information better right
actually there's a paper on this by a
scull cough I think 2008 paper that says
well if we look at say Kalman filter
which is actually a generative process
more in line at what you're saying you
can turn this into a differential
operator the operator leads to a
Gaussian process which is also
equivalent to a kernel and so there's
there's a kernel that's intrinsic to
your process and so what I'm using here
I'm using this operator because it's the
simplest was one to use if all I'm doing
is try to kind of interpolate between
points but if I know that there's
something generating the points I'm
gonna use a different operator that
tries to mimic that what I think is a
generator and that will lead to a
different redress err I'll get
differents covariance matrix and I'll be
able to use that to effectively transfer
the information but hopefully I mean
hopefully this would be with respect to
the Laplace and that's that
representative geometry but it might be
that it isn't and if it isn't that's
when you start saying well if I want to
do what you said but I want to do it in
a lower dimensional space I have to be
able to represent this operator with
respect to the
the correct space and then I can start
computing the metric and what is the
operator with respect to that metric so
I guess what I'm saying is your operator
so that it becomes geometry invariant
you may need to compute the metric
explicitly in this problem I didn't go
through the whole issue of saying what's
the metric at every point because I only
use the laplacian but there are cases
where the process might require you to
find a coordinate chart in our
coordinate system which is an embedding
and then compute the metric there to
express the correct operator
maybe I'm telling you more than you
really wanted to know but yes I think
there's a natural way to think about
what you're saying which is in terms of
what operator generated the process
right so I guess I've summarized a fair
bit already but the original idea of
this work was to say well we know that
the embedding algorithm fell in some
sense I recovering the the geometry of
the manifold and most of the solution
have been about trying to find one
algorithm that does better than the
other ones and instead what we try to
say is well actually don't worry about
finding the correct algorithm try to
find how to correct it in a meaningful
way by defining a metric that's that's
faithful to the geometry the intrinsic
geometry of the data or the geometry of
interest to you this means that it frees
you from having to use more complex
algorithm you can use simple algorithms
and then recover the geometry for that
embedding also means we've kind of
unified all the methods by saying
actually they can be made equivalent
through this object and now we can say
to what extent so when I say they're
equivalent I mean only mean asymptotic
limit that means that they might be more
bias or more variance for specific
embedding in the discrete case and so we
can start trying to say well how much
does that matter in practice and now we
kind of have a tool to compare the
various manifold through the metric and
now it also means it gives
kind of natural way to different Gusman
process or Reger eyes errs by which I
mean like L to type reg Arizer such as
kernels that are faithful to names in
the geometry or to your geometry of
interests and the only challenge we get
out of this is that now we have if we
actually work in a coordinate chart we
have to carry around a quadratic form
that's going to be of dimension d2 and
it also adds some complexity to the
implementation of any method and so
where I see this going the question to
me now is more about trying to say maybe
the intrinsic geometry is not the
correct one what I care about is the
geometry to help me the most with my
problem and can I put a a prior on what
type of geometry of interest and use the
points then to find which one is the
correct one to use here and how do i
define a laplacian that's robust to
noise or variability and now this is
kind of a tricky question which is why
people don't always like manifold
learning which is that the laplacian or
the embedding will be notoriously
sensitive to noise and I think there's
some work being done in that respect
which is to try to think about what
geometry you're trying to learn so it's
kind of related to this but there's some
very nice work by Stefan Mela that's
trying to define a geometry with respect
to groups by saying well what are the
natural environment in my space and that
gives you much nicer embedding much
nicer geometry and then I didn't discuss
this but I can show the consistency of
this algorithm but only for one
embedding that is the laplacian eigen
map and so for me it's a question of can
I extend that to general embedding and
then try to think about what are the
bias of my method and the variance so I
think that sums it up
that's where you are so if you wanted to
push back
this is provision the linguistic the
label dimension into the algorithm how
straightforward or not would it be even
old understand recently in this super
right metric learning again or
semi-structured learning so so the idea
is that I if I don't know if I don't
have a supervised context
I'll go unsupervised and I'll just
assume that one geometry is more
interesting than another either by
arguments or just because it's the one I
have if I do have a super
semi-supervised learning problem that's
where I'm thinking about well I can just
do the unsupervised part and then use it
afterwards to help me with my supervised
or I can try to define you know a subset
of geometries and then try to find a one
that match the best and implicitly
that's what people all redoing what I'm
thinking of here this year so here I
have a bandwidth right that's gonna
define my graph that's gonna help me
define my laplacian now if you're in a
semi-circle eyes learning you're gonna
try to use cross-validation or any other
method of model selection to find what
is the optimal epsilon and that is in
itself defining a geometry the geometry
change as you increase your bandwidth it
becomes less and less interesting at
some point but if it's to localize it
becomes a geometry where the points are
disconnected and so I think this is
already being done in effect
what do you have the mentality being
influenced by this provision we're doing
it in a different way you can say
synthesis what this process is
unsupervised so you can say I'm afraid
to see them you say you have data much
is it for different purposes you want to
compress it so you want to find out your
presentation which will be useful for
many tests that you don't know when at
that because if you know the testing was
quite go this way you know what it is as
you say I might go directly to find a
representation that will be best for the
test that I'm interested in but if you
have multiple tasks or even unknown
tasks this is poor this kind of
representation
then question then is that can you have
a sort of a like an in Midway where you
have their presentation but then the
task could come in and then sort of
stuff some sort of what you want that
like ramen solution worry where you have
it you can have part of the inventing
but then it could be corrected using the
supervision of the task whether you have
to keep also that which all data but if
you keep the original data so what the
point of this oh well it depends what
that next bit was just compression or
it's actually it's just impression then
sure as long as you d squared less than
there is Asma finally that you can give
another Croatian B to the top for Scotch
oh yeah without wanting to make it home
asleep
if if it's not very related you'll just
move in the space of potential
geometries you're learning
I mean implicitly here I as I said you
can rescale the the graph to so that the
sampling density doesn't matter but if
you don't rescale it it does matter
and there's actually parameter that
likes you tweak that right and so you
can let the sampling density affect your
embedding and that kinds it's kind of
clearer than just simply changing you
your ball you're saying to what extent
do I want a sampling density to affect
the representation of my data and I try
to find with respect to a task or not if
I do it without respect to a task and I
say well maybe I know I'll be doing
clustering but I don't know what's the
clustering yet and so I'm going to let
it be an effect right and we make
certain assumptions you're ready to make
us the assumption that the test that
will come will be expressed on the
manifold and not on its embedding in the
high dimensional space because if if you
know that's assume that I have a
classification test if actually in order
to solve it I mean to understand the
embedding of the manifold in the
original space well I just dumped this
information when I did this wait
Betty are you implying that the ambient
space of the original space is important
is that we make we make involve this
plan of work to make an assumption which
that they could readable assumption that
if I have low dimensional data embedded
in high dimensional space then for most
tasks we need to really care all the on
the low dimensional day HIV testing
we're not we do not care about the high
dimensional so think about your faces
right if the task eventually someone
tells you that the task is picked up you
know I will pick a specific pixel and
say whether the intensity on this pixel
is higher all over that certain
threshold well this is has nothing to do
with a lot of dimensional representation
of the day
and any such embedding that you'll find
even if it's the best bet it will fail
miserably on this test so we've made you
make that line assumption that any any
visible test that you will ask to do
with respect to this data will have will
actually care about this little
dimensional information or not the high
dimensional so I think we just agreed my
question man that the ambient space of
the data is important and that's what
you're saying effectively it's a
fundamental question of whether the full
space has some important information to
tell you the sufficient statistics for
that task are all intrinsic are the
measurements in the intrinsic
dimensional space which is like a very
reasonable or something but you know
this is going back to what you say
obviously you know once I compressed
okay I lost something and someone
couldn't tell me exactly in this well
the addition is that I can't even have
to compress halfway in such a way that
the rest of the compression could then
be informed by the task only if it's a
necklace impression well so I think
what's the main quiz it here is which
part you're losing which is the
extrinsic geometry which is the ambient
space and but you're preserving all of
the intrinsic geometry but it turns out
that for high dimensional data it turn
it's often the case that the intrinsic
space is actually washing off the
information right if you try to
supervised learning in the high
dimensional space you can tree propagate
your information because you have so
many dimension to move around that
nothing gets propagated or you all gets
propagated uniformly and so tends to
perform better it might not be the right
one but we just don't have the tool to
really express what's happening in high
dimensional space and so I feel like it
turns out that in many cases is the best
assumption we can make because it's so
hard to deal with high dimension and
what it means to propagate in high
dimension propagate information what's
the covariance that's natural for item
space
you were doing someone where we're
currently on comparing this to Colonel
traffic right so it's the same idea of
like using this to define a GP and then
compare it to people who are doing
Colonel's but are using a Laplace
entrega right sir actually
Belkin zoo in Bell King have just come
out with a paper and a on stats saying
well they consider the higher-order
regulation of the laplacian that defines
a kernel and this kernel is equivalent
to the GP process if you remove this K
right so it's really an equivalent
approach is what I'm saying is was there
more do you wanted to know about this or
like yeah I mean this goes back to the
Scala cough paper that I mentioned that
says there's a link between Gaussian
process kernels and differential
operator you can move between the three
pretty much as you will or regular yeah
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>