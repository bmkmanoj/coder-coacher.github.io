<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Networking Technologies for Real-Time, Interactive Applications | Coder Coacher - Coaching Coders</title><meta content="Networking Technologies for Real-Time, Interactive Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Networking Technologies for Real-Time, Interactive Applications</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SK5-7PGkLW0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by Microsoft
Corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
okay so the title of the talk is going
to be networking technologies for
real-time interactive applications and
this is work that we've done over the
last four or five years and a lot of
this work has been related to
applications like remote desktop and
video conferencing Lync and Skype type
applications so just before starting
I'll just give a background of some of
the stuff I've worked on over the past
15 years and so I joined in about 97 and
I joined in the media division and in
the media team most of my work was
related to codecs so I worked on screen
codec and then audio codecs and then
audio processing technologies and prior
to moving to MSR worked on adapted media
streaming technologies like smooth
streaming so we developed a prototype
for smooth streaming and then after
joining MSR I shifted my focus from
codec development towards more in the
networking technology side and have
worked a lot to weigh the teams like
remote desktop and Lync and that's the
work I'll be presenting today is going
to be on this improving the real-time
interactive applications so first of all
I just wanted to kind of clarify what
are real-time interactive applications
so basically a real-time interactive
application is any application where you
have low you require low end to end
delay so things like of course things
like file download there are not
interactive because mostly it's just
throughput that matters and of course
video conferencing and VoIP or interact
but the energy but contrary to what many
people believe not all media
applications or real-time so-so video
streaming like YouTube and those types
of applications are actually not
real-time they do have some requirement
on you know by when packets should
arrive but they're not really real time
and but some actually some software
applications are real time like
interactive client-server applications
to take away kind of is that not all
multimedia applications are actually
real time and there are actually non
multimedia applications which are also
interactive in real time so as I said
what matters in for RTC applications
it's not just the throughput but it's
also the end-to-end away and this is
essentially the intent delay between
when you generate the content to when
you consume it so essentially like in
void you know it's the time between when
you speak to when the other guy listens
to it and that's what now what matters a
lot so if you think of like network
latency as you know on the x-axis we
have packet delay and let's say network
latency is on the order of 100
milliseconds 200 millisecond and if you
try to plot you know quality versus you
know network latency then you know for
videoconferencing essentially if you
have more than like 250 milliseconds of
latency then you get very poor
performance whereas for streaming media
you know you do get poor performance but
it only happens after you essentially
the buffer empties and you cause a
glitch so I mean this basically prevents
you know things like YouTube and things
like that from being you know classified
as real time applications because you
know the antenna latency is an order of
magnitude
the network latency and RTC applications
of course have been increasing rapidly
in usage as of July 2012 Skype had 115
billion minutes per quarter and out of
that in roughly half is his video
conferencing which is not just low
latency but also requires high bandwidth
because if you're doing like HD video
conferencing you want tight throughput
and besides that there's also increasing
use of software applications such as
remote desktop and this of course is
becoming very useful because of VDI
scenarios um which is essentially a
virtual desktop infrastructure where you
know you have you have lots of VMs
running in the cloud and you know people
won't have machines sitting in their
offices right and and of course
applications like office 365 where
you're not necessarily streaming the
whole VM but you know you're essentially
interacting with an application running
in the cloud and things like online
multiplayer games where you know lots of
state needs to be sent across so that
you can have multiple parties plane so
you know for non RTC applications only
long-term average throughput matters so
for file download it just it only
matters like what the overall long term
average throughput it is which
determines how long it's going to take
to download the file
however our TC applications you know
since the intent delay requirement is
there you can't use standard protocols
like TCP for for transporting the data
and the main reason is essentially is
two things
TCP realized solely on retransmissions
for recovering from lost packets and
this adds to you know long delay and the
other reason is that TCPS congestion
control essentially is going to build up
long queues and this causes additional
queuing delay that's detrimental so what
I'm going to present today is
essentially three technologies that we
worked on to improve real-time
interactive applications and this is in
the order they were kind of done rapid
it stands for reliable adaptive protocol
for improving delay and this essentially
is a hybrid forward error correction and
they are cue protocol so it relies on
retransmissions but envision proactively
sends forward error correction the
packets to prevent retransmissions and
the second is so this this relates to
like providing reliability for the
application the second is a universal
rate control protocol and this is the
relates to replacing the congestion
control portion of TCP and this is a
rate control framework which provides
simultaneously low queuing delay and
high throughput and doing this
simultaneously is very challenging over
you know over noisy networks like
cellular networks and the third is
Proteus which is a work we just
presented Mobius and this is trying to
predict what kind of queuing delay and
loss rates you will see on cellular
links so that you can the application
can be more intelligent and how much
error correction to insert and you know
things like how much you know how long
it should wait before declaring packets
to be lost so
it relates to like the jitter buffer
sizes so it's just going to present you
know how an application a real-time
application kind of works and this could
be sort of like video conferencing or
remote desktop type application so the
application generates packets which you
know it wants to send they let's say go
into some queue and then this
transmission strategy basically decides
you know I should send the original
packets or I should send forward error
correction packets and you know how much
error correction to insert and this
portion decides what's the overall rate
you can send so this includes both the
source rate and the forward error
correction rate so the first thing is
you know we given you know and tend
route you know what's the total
bandwidth I can use that's the job of
this congestion control and the this
guy's job essentially is given this
total rate how I should partition it
between you know forward error
correction and original rate and the
application of course can this then
using this knowledge can decide you know
what Bill wait I shouldn't call that
let's say for video conferencing or
let's say for Remote Desktop it it
essentially can decide to you know
slower the frame rate at which it's you
know painting to the applications or
join updating the screen it can be said
it can essentially be a combination of
you know two things one is it can be the
total rate that the congestion control
module is saying so basically it can be
total rate and then the loss rate
something like that okay or in cases of
remote desktop it can just be you know
the buffer is kind of full so back off
on painting so I mean it essentially can
be any knowledge such as like
how full this buffer is what rate is the
buffer draining and what kind of loss
rate you're seeing so this can work on
TCP also the only difference in TCP is
that you cannot be more aggressive than
TCP of course so you know you can your
rate transmission rate can be lower than
what TCP tells you but you can't make it
of course higher than what TCP tells you
and the second difference is that in TCP
you don't have any control on how you
recover from lost packets so you cannot
you know there's no real advantage to
doing FEC right but the you RCP work the
you know the rate control work that can
be applied to TCP as well Skype actually
should be mostly UDP traffic on cellular
it may be currently TCP but for most
other cases it's UDP and I think that
that in itself probably will change as
time goes on
and actually TCP is much worse on
cellular links than what any UDP based
solution can be you okay so first I was
just going to talk about the reliability
portion which is the rapid which is the
hybrid FECA or a queue protocol and this
is explicitly designed for applications
which require lossless in order delivery
right so this is like remote desktop it
requires lossless scenario delivery and
just like because of this requirement
remote desktop was using TCP prior to
Windows 8 because they needed TCP
essentially gave them what they would
needed except that it also gave them
hideaway so what we did was essentially
for Windows 8 we developed a hybrid FEC
Araki protocol and that is shipping as
the remote effects for Vance solution in
in Windows 8 and so I can just firstly
give no brief introduction to a very
brief introduction to FEC so forward
error correction allows you to recover
from lost packets without requiring
retransmissions so essentially you have
let's say you know some certain number
of packets and you know you encode
certain number of packets and you add
some redundancy and then you can lose
some of these packets and if your code
is optimal then you know if you insert
you'll say you're for source packets you
put two FEC packets then you can lose
any two out of those six and you can
perfectly recover your data right and
essentially even if you think of a block
code it's pretty simple you can think of
these as like these are your original
packets and so you send your original
packets that's the identity matrix
representing the original packet and
these can be sort of FEC packets which
are some you know linear combination
let's say you lose these two and then
recovery is kind of easy you you just
take the rows of the matrix
corresponding to the packets you
received you do a simple matrix
inversion and you can recover right and
the only difference is this is not you
know like actual integer math or
anything it's it's operating on fields
however for you know for a remote
desktop you require lossless in order
delivery so there's no point to using
actually block codes because in the
block code if you have you know block a
then block B and the block B comes after
block a there's no point having a TC
packets which just encompass block B
because block B by itself is useless
unless you get the original block a so
instead what we use is a we developed a
sliding coding structure which
essentially operates like this so let's
say you send some packet which is the
first packet then you send another
packet now you can decide you know I
want to send some FEC packets so you
essentially your FEC packet becomes a
linear combination of everything that's
in your queue so you you know you have
these two packets you have FEC packet
one FEC packet two and now you decide to
send a new original packet right and now
if you decide to send an FEC packet you
kind of just keep growing your code that
kind of makes sense you essentially keep
basically FEC Code encompasses all
packets that are in the unacknowledged
queue okay so this is kind of like a
sliding coding structure which is not
bought based
so didn't so this code is a there is it
turns out there's there's no you cannot
make proof of optimality on in the sense
that you know even if you have the
sufficient number of packets you you may
there may be cases where you cannot
recover okay but but empirically we can
show that you know ninety nine point
some percent of times you can yes yes
yes yes so we're actually using what's
what's a couple it's called a Koshi
construction for if codes but we're
essentially modifying that to a sliding
block code so it starts with you know if
it was a block code it would be optimal
but since it's the sliding structure you
can't guarantee hundred-percent of
optimality yes
yes it will be you know I mean it will
be in order
I mean you it will be an order if you
have like a buffer Hey
so you still may require a buffer for so
let's say like you know here let's say
you lost two packets right okay then
well you have to here you have to it but
let's say you lookie let's say you get
me up to here okay you lost let's say
you lose these two packets okay then of
course you cannot yet recover you can
only recover up to here right so it's I
mean what it guarantees is that unless
you get an original packet you it will
always be in order see like if you're
recovering from an FEC packet the packet
that you will recover will be up to
every pair all the packets up to the
previous packet so then they're just
going to talk about sort of an
optimization for how you insert FEC so
let's say like this is a queue of
packets you know you know you have all
everything up to here and in between
here you may have some you may have lost
some right these are kind of in flight
and you don't know okay and then these
are like new packets that are kind of
waiting in the queue okay so if you
think about you know what kind of
choices you have first thing you can do
is you can just retransmit this packet
right because that's a valid thing to do
you can just say okay maybe this packet
has been lost I'll be transmitted the
other thing you can do is you can send
an FEC packet which encompasses you know
first in the second packet the third
thing you can do is you
take the first three packets you can
take the first four packets okay you
won't you won't you'll never just take
these two packets right because that you
know is not really the correct thing to
do or you can send the new packet so you
have like a list of choices you can make
right so how do basically how do we
decide which of these choices to take
and that's kind of where we did this
optimization framework where here this
this is a let's say this x-axis in D is
the index of the packet this is the
probability of what we call sequential
decode ability which means this is the
probability that I have all the packets
up to you know this packet J okay and
this can only decrease because the
probability that I have everything up to
here has to be less than or equal to the
probability that have something up to
here right and of course it drops to
zero because there's no data sense here
right so you can be hundred percent sure
that nothing is there so we essentially
do is like if you send some FEC packet
what you do is you raise the probability
of these in-flight packets but you don't
cause any change here right
but if you send a new packet then you
don't change any of the probabilities
here but you kind of have some
probability that you've gotten a new
packet and so you kind of do this
analysis for all the potential cases and
that's kind of how we decide you know
which packet to send and and the details
have had like computers sequential
decodability probability that's kind of
detail
so you're basically trying to optimize
the expected sequential sequential
decodability delay for all the packets
in the queue yes so each of these
packets is gonna have some sequential
decodability delay up to that point
years yes exactly right yes yes for all
the packets that are under clause or
waiting in the queue yeah and to do that
expectation you need these probabilities
essentially that's what it ends up being
yo which gives you the maximum area yeah
sure yeah yeah yes yeah exactly so
basically if I just do a FEC packet up
to here then I'll only increase up to
here and then the rest of it will kind
of be
you see of just the first two packets
will be higher that I retire yeah but
actually won't there will be no
discontinuity actually because just by
doing an FEC of these two packets you
still increase the sequential the
credibility probability of this packet
as well because maybe this packet is not
sequentially decodable because it's
waiting on something missing so it will
probably it'll look something like this
way will be higher up to here and then
it will cross over in these it would be
smooth yeah there's only discontinuity
will occur yeah okay so that's so
basically the only information you have
is that you know whether it's an
original packet or if it's an FEC packet
which packets and encompasses right
whatever is your strategy we started
strategy bro why would it change
overnight okay so there's two reasons
why it would change one is that the
actual lost probability is different
okay so for given lost probabilities
these curves would look different the
second is how many packets you have that
are waiting in the in the queue right
because if I if you have lots of packets
waiting in the queue then there's no
reason to I mean if you hold off then
all of them are going to be held I mean
if you just keep sending FEC then you're
delaying all the packets that are
sitting in the queue so the red ones
the red ones you know everything's of
your packets on yes so ik
acknowledgments give you
acknowledgments give you it changes the
probability of a packet being lost to
receive right so when you send it you
have a certain loss probability let's
say it's 2 percent right so you assume
you assume with 98% probability these
packets will arrive at the other end
no see you see use so what you do is you
invoke it at every every transmission
opportunity if it's very early then your
your expected probability that a packet
has arrived is the estimated one minus
the estimated loss probability as time
goes along right then let's say you you
make exactly right
so basically after certain amount of
time you decide you know this packet has
been lost right then the probability
that that packet made it through is 0
and then that updates all the
probabilities in the yes so now we're
coding is pretty much FEC right it's
it's same similar right now we're coding
is essentially same concept except in
network coding you don't have you don't
have a systematic portion you
essentially always just keep sending
with your combinations right but the
principle is the same so I'll show a
demo of the remote effects later yeah so
this applications not one of them so
this kind of shows this is this is the
simulation essentially but but this
essentially shows a CDF of the
sequential decodability delay and this
is kind of an extreme case where you
have 15% loss but most of time the law
states are much lower but this is kind
of just to illustrate the gains you can
get so so the right curve essentially is
retransmission only this green curve is
opportunistic FEC which means whenever
the sending queue is empty just send an
FEC packet okay this purple curve is FEC
with the cost function invoked so
basically trying to optimize when to
send FEC packets and this blue curve is
essentially you know prior knowledge so
I know which packets are going to be
lost I'm just going to send them always
and this essentially shows this the
sequential decodability delay
ZDF so with the optimization it's kind
of close to what you know prior
knowledge yeah you'll try to approach
that yeah so that's kind of under rapid
um okay so the bandwidth site
essentially is showing how close we can
beat the capacity and so this is the way
the simulation was done was we would
send the burst of we assume the traffic
is bursty so every so often they would
wake up and then we try to send certain
number of packets and this essentially
shows you so what the decision
essentially is doing is that how much of
the burst actually can make it through
so this is the CDF of what percent like
what's the bit rate that you can sustain
so next I was going to kind of switch to
the other aspect of interactivity or
someone was the reliability and the
other is the congestion control of a
control so basically the motivation for
this is that you know there's many
networks where you know rate control
strategies work well for interactive
applications so like Skype works pretty
well if you use it over you know
landlines connected you know wired
connections but it doesn't work so well
when you're doing it over let's save it
you know even Wi-Fi or or 4G or WiMAX
type things and and and the main reason
for this essentially turns out to be
that you know these networks are kind of
noisy and what I want what I mean by
noisy networks is that if you consider a
network and you let's say you send at a
very low rate so you send packets that
you know something that you know
shouldn't be exceeding capacity right so
like you're sending at 20 kilobits per
second or 30 kilobits per second right
and what you do is let's say you you
look at the delay you get which is
essentially you know one-way delay or
the rtt and what you do is you you plot
the delay minus the minimum delay so the
minimum delay you assumed to be sort of
propagation caused by propagation that's
something you can't overcome and you
plot the distribution so it turns out
that you know for like cable modem type
networks this distribution is very
narrow meaning you know there's there's
very little portion of delay that's not
just propagation delay okay but if you
do look at the other networks like 4G or
the WiMAX
then you see this distribution is very
broad right so that's what I mean by
noisy networks which is that even in the
presence of a traffic flow which is you
know very low
with and shouldn't be contesting the
network you're still seeing a high
variability in delay okay and and this
is kind of the same same thing but you
know in a slightly more complicated
format which is what we did for this
experiment we essentially send at a
particular rate for a certain duration
of time and then again just measure the
delay and we kind of plot the
distribution along the y-axis so so this
this the the length of this bar is you
know a minimum delay to the maximum
delay you see when you're sending at
that rate and then you know we plot like
10th percentile 50th percentile 90th
percentile and mean that kind of makes
sense how this plot is looking so what
you see is like you know here you think
if we look at this kind of like visually
we can kind of see this looks like
congestion right because now we're
consistently seeing high delay right but
this stuff is kind of noise in the sense
that you know you're seeing a high range
you know this is a very broad
distribution that we even when you're
sending it low rates and that's the same
thing for the loss there's no particular
pattern so this is the maximum delay you
see this is the minimum the way you see
so yeah you see something like five
seconds the way which i think is pretty
pretty much what people reporting
so there was some packet even when
you're standing at 300 kbps yeah yeah
right yeah this was on versus on 3G this
is using a USB dongle is a Verizon this
is essentially not even moving just
sitting in one's own place saying the
variation reduce just below this so yeah
so that's just a random artifact
there's no I mean there's no reason why
you shouldn't see tall bars even there
now we kind of do this for various
networks and this is WiMAX and this was
this was done like a couple years back
so again you can kind of see that that's
congestion and this is more recent this
is for 4G mmm-hmm and 4G you know the
the distribution is still broad but like
the 90th percentile is much better right
but the maximum is still pretty bad and
just just is just for comparison this is
cable modem so this is just the kind of
show you'll hear it's very nice oh okay
yeah
so you think that's the porter position
somewhere somewhere in between there
where here the one-way delays high rate
higher but it is not yeah that's the
buffer size yeah the buffer is equipment
but the loss rates linear loss is
linearly increasing okay um so that was
kind of just like a overview of why it's
kind of leads into why these types of
networks are problematic for RTC
applications so RTC applications
primarily use delay based rate control
so if you look at Skype or link or other
interactive applications there they're
using delay based rate control and since
delay is a noisy signal we can't make a
good judgment as to when you know if
you're just looking at one delay signal
you can't make a good indication whether
you're causing congestion or not causing
congestion right and unclean networks
delays clean it allows for full link
utilization on noisy networks it
actually results in link under
utilization meaning you often confuse
you think you're congesting the link so
you reduce your rate but you're not
actually causing congestion this is kind
of just the random delaying the network
loss based protocols work better because
loss is actually a cleaner signal
usually but we can't use lost based
congestion control for our TC
applications because it always implies
the buffer is full in the queuing delay
is going to be hi hey
and if you think of like the nth and
delayed that you see and let's say video
conferencing you know there's lots of
components to delay but the main one
that's the main one is that's variable
is the networking delay so I just gonna
say I just kind of break down the
network and go into different components
when is the propagation delay right this
is just like a fixed value mm-hmm
the other is this delay noise which kind
of is what I plotted before right and
the third is basically caused by queuing
so this is this is due to actual
congestion okay and if you really want
to achieve full link utilization I mean
this component there has to be there in
some in some capacity okay
so basically this is kind of shows that
you know you look at congestion signals
you just wait and so how do existing
protocols perform on these types of
networks okay so here I have essentially
these these are lost base rate control
protocols so this is let's say standard
TCP where you see in you know default
windows TCP this is a TFR see which is
which is meant for media applications
but in essence it doesn't really do
anything for videoconferencing because
TF RC uses the exact same throughput
equation as this guy so essentially if
you look the only thing that gives you a
smoother rate but it doesn't help you in
terms of queuing delay and these are in
pink are the let's say delay based that
people actually use for video
conferencing so this is let's say this
is Vegas which is a delay based rate
control strategy and this is a googles
IETF submission web RTC so there's an
ongoing effort in the IETF to
standardize real-time applications like
video conferencing and they even one is
standardize the congestion control
aspect of it so if you look at the so
here's the average rate each of these
achieves so you see that the and this is
on the 4G upload direction the t-mobile
network so here you see like the
throughput is pretty good for these
networks but but the delay is very high
right for these lost based protocols and
if you look at these delay based
protocols you see low delay but the
throughput is low hey
and and so yes I mean there's no
existing protocol which gives you good
high trooper and low latency for these
types of noisy networks and I just kind
of given in intuition as to why this
happened without going into too many
details so it turns out all rate control
protocols have in essence what's an
operating point and this operating point
is is a function of protocol parameters
and the final operating rate and so this
kind of looks like this so here I have
let's say delay and let's say this is
the PDF of the delay this is the first
plot I showed and then let's say you
have an operating delay point here okay
so if you do something like this this is
on the clean network you actually get
full link utilization but if you have a
noisy Network and you're still trying to
achieve this operating delay point here
then what's gonna happen is the actual
operating delay is much higher right
because just simply because of the
distribution and therefore you're going
to confuse you know noise in the way for
congestion and this actually gets worse
as you know noisier the network is so
you can actually you know be far off and
why do you need a new protocol the need
for a new protocol essentially is that
you know networks are not the same so
you you want the same protocol to work
on all kinds of networks right so here's
a clean network here's a noisy Network
so let's say I just fixed the protocol I
I pick some protocol that works which i
think is gonna work good so you know it
has some operating point here right it's
this protocol is going to work good for
Network a but for network D it's going
to give you a link on the utilization
and then if I change the protocol let's
say move the operating point out then
you know you're gonna see full link
utilization on both the networks but
this network is going to see more delay
than it needs to you're going to incur
higher queuing delay so so noise is
essentially a short-term fluctuation in
certain capacity so it could be due to
actual capacity changes or there could
be some flow entering or leaving right
but but what I mean by so it is it
inherent delay noise essentially is is
is the portion of noise that's caused by
you know something that's not like a
rule it's not a competing flow actually
in the sense that it's not competing
over a long time scale
so so we do essentially is um you know
we start we started with essentially
utility maximization based rate control
scheme which uses delays a congestion
signal but then the main the main
contribution of you RSVP is that we
essentially do a three stage adaptation
okay so this is kind of working so you
don't have to look at these equations in
too much detail but most rate control
strategies does to this which is you
change the rate in response to
congestion so this Delta here is your
delay and these two are sort of like
parameters that you have and this is
your current rate so you know if this
guy is high so delay is high this
becomes negative so you're dropping your
rate right so this is like this is what
most rate control protocols do is just
this portion which is you know change
the rate the main difference between
this and your CPU RCP is that in
addition to just this rate adaptation we
also change these parameters in the rate
control framework so there's a another
adaptation which actually adjust these
to try to compensate for which network
you're operating on okay so if you're in
a clean network the operating point
moves lower if you're in a noisy Network
the operating point moves higher
see ya sir so there you can either do
some free calibration so you can either
probe the network when you start the
flow but then even after the initial
probing there is a background adaptation
process which operates at a slower time
scale to try to it continuously updates
the parameters k 0 and k 2 also so this
kind of just shows on that same 4G
network you know we get the we get the
rate that we want you know the higher
throughput but we get the delay levels
of the delay based protocols okay this
is proud people yeah right
so sprout actually uses it's it's an
available bandwidth estimation scheme so
it essentially has a model of the
network so it assumes that the network
is a model is modeled as a basically
accused modeled using a Poisson and via
rivals for the packets with a certain
escape rate for the packets being
dropped and it essentially tries to use
this network model to try to estimate so
like your observing delay like inter
arrival packet gaps and you're trying to
infer what state the network is in so
that the observations match what you
know the states you're estimating so
it's more like an available bandwidth
estimation scheme so just two things one
is available bandwidth estimation
schemes can't guarantee fairness in the
sense that you know the first guy comes
in he thinks the bandwidth is a certain
amount right the second guy comes in and
sees a much smaller segment of the pipe
and essentially the second guy will
never be able to fairly compete with the
first guy and the second issue is that
it's very specific to the network model
so if you get there if you can actually
watch that talk on ResNet Keith gave it
to me yeah yeah so what you do is you
move the operating point out so that
essentially on a noisy Network you push
the operating point further out so that
you don't confuse delay noise for
congestion so you're just the parameters
so that's kind of factor to the fact
that that the adaptations for the K 0
and K 2 are happening happening at a
slower time scale than the adaptation so
the time scale is not explicitly shown
here but so these are the formulas for K
0 and K 2 they have these R max in our
average our average is actually the
average rate in a 10 second sliding
window so that's where you're getting
the different time scales and same thing
with Delta average it's the it's the
average delay in the 10 second sliding
window right it doesn't in fact yet the
assumption is that if you yeah if you
have a lot of spikes you will
necessarily get confused where will
happen is you will keep much pushing
your operating point out until you know
those spikes don't further increase the
average delay
okay that's that's where the time scale
factors in so that's the plot and this
is essentially just yeah yes exactly so
yes yes so the way this deals with
fairness is is let's say you forget this
okay if you fix K 0 and K 2 you're
actually guaranteed fairness and the
reason for that is that every every
cycle of adaptation the guy with the
higher weight is going to decrease more
so the gap between the person with the
lower rate and the higher rate keeps
shrinking every every adaptation cycle
that that can be proved that with fixed
K 0 and K 2 it's going to be fair with
variable K 0 and K 2 the Fairness relies
on the fact that you essentially it
turns out like if you think of these uh
if you think of this Delta average in
long term Delta average is gonna
approach the Delta and our average is
going to approach R so the only thing
that really matters in this is is that
every flow converges to the same value
of these two parameters R Max and Delta
min if they don't converge to the same
values for these then fairness is not
guaranteed but the way these parameters
are actually updated there's a there's
very high probability that you will you
know what this essentially is doing is
that this is trying to figure out what
is the estimates of the long term
capacity okay
so if every flow somehow can get the
same estimate for long term link
capacity and this essentially is the
average background delay noise
okay and again if all the flows can try
to if you can say that these these two
parameters are all flows we'll get the
same value for these two parameters you
can guarantee fairness so there's a way
for there's a way for us to kind of
figure out what's gonna happen when you
have tcp is tcp is going to cause more
congestion right it's going to try to
push the operating delay higher and
higher okay until you see loss so what's
going to happen is that the way this k0
adaptation works is when this delta
average increases k0 increases okay
and once k0 increases this actually this
is flip this should be K 2 and K 0
should be on the inside but but once K 0
increases then you automatically push
the operating point further and further
out so that you can compete with TCP and
once TCP leaves because of the way this
beta is chosen there's always a downward
pressure to bring the operating point
back to low levels it becomes yes and
that's but you can't do that you can't
get delayed benefits anyways right so
then the tcp won't affect you right you
won't affect your delay or your
Hey because betting different servicing
all diffuse right right so then then
it's kind of like a single queue then
again right because if you're servicing
all the cues that if one of the queues
is full I think it's kind of same thing
yeah it's like it's kind of like a link
it would be I think it would be similar
in the sense that once your delay starts
increasing you will try to compete with
TCP right regardless of whether that
happens with this multiple queue or the
single queue okay so this is kind of
just the more results showing the
overall CDF of the way so this kind of
just shows that your CP is delay profile
not just the average claim and I get
percent out of the way but the entire
distribution of the way is close to what
the lay based protocols can give you and
this just shows you throughput versus
time so kind of just shows um and then
regarding the fairness thing so these
are just you know applied the algorithm
to video conferencing see how it
performs and basically what you're
showing here is that you know because of
the decoder reconstruction logic you
know you have missing frames and you
have different video coming out than the
original this is just one fairness
results showing you know you have flow
zero going from zero to 900 seconds flow
one comes in at 300 and leaves at 600
and we kind of show that they can fairly
share the link
and let's just show a demo of so
basically the rapid work the initial FEC
hybrid AF is here Q that wasn't
incorporated into RemoteFX for one which
is the which is essentially the protocol
that remote desktop users and as a
Windows 8 they're using this UDP based
protocol and so they have the hybrid FEC
RQ and they don't have the full
implementation of USC P that's being
incorporated for blue plus one so next
here but they have a pre you RCP
implementation of delay based rate
control and there's essentially a demo
they show that um maybe I should launch
it outside the PowerPoint study and this
is kind of from their advertising video
so I didn't make this video myself but
but they're just showing the comparison
between Windows 7 and Windows 8 for this
is like a transcontinental when about
one and a half megabits per second 200
millisecond OTT it's only for one no
it's our other detect yeah so I think
what they do so what they do do
essentially is that they start they
always initialize a TCP channel just for
the connection and then they open up a
UDP channel channel in parallel and then
if the conditions like RTT and estimated
damn both require it they move and they
can move traffic to the UDP link they
will they will put all the traffic on
the on the UDP link
yes how much networking stuff for the
rapid great so I would have to
instrument their code to get more
information
yeah you'll still see better than I so
you could do FEC Eric you but without
FEC optimization oh yeah so sorry
IG clarify RDP in Windows 7 requires
perfect lossless transmission as of
Windows 8 they actually segment some
other regions and so essentially they
have three things open one is TCP one is
UDP reliable which is 100% FEC + mq and
then they have UDP unreliable which is
just FEC without the Erik you portion so
they have and they can actually once
they do the segmentation they can code
some of the regions using - 64 type
video coding and those they can code
they can send over the UTC GDP
unreliable and so that's kind of thing
and I think I'll just stop there
yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>