<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Structures for Efficient Inference and Optimization in Expressive Continuous Domains | Coder Coacher - Coaching Coders</title><meta content="Data Structures for Efficient Inference and Optimization in Expressive Continuous Domains - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Structures for Efficient Inference and Optimization in Expressive Continuous Domains</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n6b5Bu6DgXk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so this is work with a lot of
collaborators my students at the ANU
clever dish from the your sis Apollo and
my one of my better students or best
students Jiang he went off to MIT for
his PhD before I get into the talk let
me just give you a nice morning
introduction if you ever in Canberra or
I invite you down to Canberra you would
know this is not Canberra actually but
this is just two hours from from Kippur
along the coast you get an entire mile
of beach to yourself so if you haven't
been Australia we do have funds for
visitors and I would encourage you to
consider coming down okay but back to
reality okay I'm going to assume this
audience is familiar with graphical
models okay not all ideas are but this
one is so I'm gonna jump right into a
continuous hmm and I'm gonna ask you how
you might do inference in it now this is
not your standard hmm it's not you know
it's not completely Gaussian so you
can't just do Kalman filtering but let
me define what I've got here so I've got
state variables B and X B for broken
also for binary so it's a it's a
discrete variable I have X for position
this is could be a continuous careful of
the changes over time and all them on
position i some some noisy observations
right so the observation is is a
function of the position and whether the
sense in whether the sensor is broken or
not
now to find this hmm I have to defines
for the prior distribution on the the
initial state right so I just choose a
uniform perhaps I need to choose a
sensor since they're opposite of
observation function so I'll say the
search was not broken maybe it just sort
of centered at the center of the true
position and sensor is broken maybe it's
a bit asymmetrical tends to
underestimate rather than overestimate
okay
and also I need to transitions I might
just sort of choose some some Gaussian
transition function so if I had a model
like this how would I do inference in it
and then specifically closed-form exact
inference okay so let me just show you
what these inferences might look like
here this is probability of observation
one given
since broken which is the sorry not
broken which is the blue line or broken
which is the red line okay so I mean if
it's not broken
you see if he's sort of convolve the
triangular with the uniform you can
serve a nice galaxy approximation but it
is broken you get sort of something very
non Gaussian here due to the a special
nature of the of the center of the
center model it's broken
okay might look at a different function
what's probability of the position at
time five given the observation at time
oh one so on the x-axis here is X five
on the y-axis going into the screen is a
1 and the z-axis is the probability okay
so here's the actual function that I get
ok and you know if you're like me you're
thinking you know what's that function
ok so I mean if you look at it it is not
really Gaussian right so sort of a
bimodal but I wouldn't use a mixture of
gaussians items that sort of has this
long peak if you look at it serve as a
contour plot you see it sort of you sort
of been at the edges due to battery
packs with very sharp very sharp by just
going to the initial uniform
distribution okay so the question again
is how would you do inference if I gave
you this
hmm scan this continue with hmm again if
the galaxy if it's galaxy and you could
do it but it's not Gaussian does anyone
do this in close form all I ask
ok no one said said yes so far and all
the talks I've given and so I want to
claim that no one previously did
inference like this in close form ok I
mean if you're curious this so that's
what this talk about is about today ok
now you might say but you know we work
with GTOs inference all the time ok and
so I I do want to cover some other ways
if you weren't exact that you might do
this inference ok so people all fin
discretize right so and then they know a
disguise you uniformly they may
adaptively discretize this is an
adaptive disquisition and generally
you're sort of bounded by the number of
disk ization points especially expansion
into the dimensionality so it's not
great in high dimensions but you you
can't do it totally in low dimensions if
you're adaptive
it's work but it's typically an art form
that's based on the problem you're
solving okay now one one group of
methods that are commonly used here I
know that are very good are sort of
projective message approximation methods
variational or ep so you have some some
some distribution and might not be given
to work with say cheese depression class
typically a priori often Gaussian okay
and well the steps work eventually in
practice I don't want to criticize this
especially with this audience I mean
it's always not clear that sort of you
yet you're passing class was the best
choice especially in a small data case
where you're you're where your posterior
or not to believe Gaussian and so I'm
not saying these are bad methods but
what I'll tell you bout today is perhaps
one way to choose the best part being
class on the fly with these methods and
I'll come back to that later okay and
finally sampling I mean sampling is sort
of staple of Canadian prints going back
to bugs it is effective it does work in
practice but there a few cases where it
does not work so we know if the
distributions are near deterministic or
deterministic we know it it's not going
to converge or not going to birds
quickly and if you have queries like
this where you might sort of say well I
don't instantiate my observation where
I'm on my evidence I sort of what I
leave it as a free variable well if
you're sampling I don't know how to do
sampling where you didn't stand she ate
your your evidence right you'd sort of
have to choose different sensations and
do the inference based on that so this
is one case for so you might you might
want take the derivative of this
expectation with respect to O right with
oh is a free variable and I wouldn't
know how to do this if I was sampling
okay so so III don't think youth in
yours inference is exact ompletely
solved and certainly there are lots of
good methods out there but I do n't tell
you about a different method that I
don't think anyone has used before okay
and so what is it different message what
everyone did missing haven't been doing
exact inference here and I want to say
it's the willingness to use symbolic
presentations and operations on
piecewise functions most of time we want
to choose a classic a gala scene which
has a nice closed form and mean and
covariance we don't want to sort of work
with something like Mathematica where we
just sort of do operations in equations
expand but I want to claim actually you
do that especially certain classes and
it can be very effective and very
expressive okay that's what this talk is
about okay so we need other we need a
symbolic form for general continues two
distributions of course you know it
should be able to express things like
gaussians that's a no-brainer but I also
claim the piecewise or two deterministic
distributions are quite quite common in
practice when you we need to support
these as well so if your work is to pick
a stick program do you often end up with
conditionals this gives you a piecewise
nature to your functions if you're like
me and you focus largely on sequential
decision theory then you've got
utilities which give step functions
often so this in some region you get a
reward outside the region you don't get
reward decision-making you have a max
okay that always makes a piecewise gives
you a piecewise function preferences I
before a to be things like like like
seeing true scalloped cetera introduced
in inequality so you want to be ahead to
handle these lame again for my work in
controlled system switching control not
well you know when you're still system
is can be in different modes and your
actions are discrete this is not well
solved in in in control but it's an
important problem and requires these
piecewise distributions okay and finally
you know when I was interning here in
2006 I was looking at risk Amato and
there's a delta function in there which
allows you to sort of some variables and
that's a very useful thing I mean it
goes back to bugs as well but you often
want deterministic equations in in your
models so I want some form of sort of
handle all of these things okay and
what's up for I'm gonna be well you know
if you've you've been to high school I
assume you have then you know you you
you've represented case notation I mean
in general this is all you're going to
need okay so here I have a function on
the domain X&amp;amp;Y okay in the blue I have
the constraints okay just tell me which
case partition that III fall into and
then say if X is two and Y is three then
I'll fall into this bottom partition
okay and then I died even value eight
this function with accident with X 2 and
Y three okay so it's just
it's an arcade case statement the Blues
constraints of undervalues I refer to
the entire company think straight and
value is a partition okay now when is
this a proper function its proper
function if the constraints are mutually
exclusive and exhaustive of the domain
okay and in this case the button the
bottom straighten is negation the top
one so that clearly holds okay this will
sign a unique value to every a nig Z
value to every value of x and y okay
satisfied general form I will specialize
it in in in in in a little bit but for
now I'll say general okay let's look at
what we can about what we can represent
with these functions so here's a
function I'll tell you it has linear
constraints and linear value okay this
function has linear constraints and you
know that the tops are flat so just
constant value it's even simpler okay
and this function I'm gonna look the
audience guess what sort of constraints
what I need for these concentric circle
point emulous TN and even perhaps more
constrained than just all plenty mules
just quadratics yeah so just x squared
plus y squared is less than a greater
than equal to R the value here might not
be obviously it's also quadratic okay so
I mean you know people often think of
piecewise functions as these sort of
Voronoi diagrams you know but pizzas can
also be considered concentric circles a
piece could be sine of X is greater than
cosine of of X right so you'd have an
infant number of disjoint pieces but
only two at two actual cases so so in
general don't restrict how you think
about the case thickness said they're
they're quite quite generic okay so this
is sort of the function of the functions
I'm going to work with okay and so if
this if I represented my graphical model
all factors with these case statements
okay then I just need a sort of a closed
form piecewise calculus on these cases I
need to add in in in multiply the cases
I need to take the max and min of them I
need to integrate over them especially
in continuous graphical models later
when I do some sequential decision
making I might skipped up today
something I can talk about later you
would actually need
it's out over a very boring function
okay
so in general for inference that you
need to do you need to do these things
and the question is okay if my functions
are case form how did I how I perform
these operations the closed form says
the result was a case statement okay so
let me go through that okay so initially
okay so plus and times are going to work
for any any case statement but because
the rest stock is going to be is going
to assume that the polynomial case you
can go to imagine that the constraints
were polynomial inequalities in there
and the function values were plenty mo
inequalities so if I want to do plus on
these two functions I want to case even
as a result first question for the
audience is how many how many K
partitions would be in the result at
least naively for good right so you sort
of just take all you know you could be
in five once I won and think in that
case your your your function values f1
plus g1 right or five five one side two
and so on so you just think it's quite
the cross part in some okay and if I if
I if I wanted to multiply right then I I
if this is multiplication here that I
just sort of changed this two times okay
so this works for a lot of the
arithmetic operation that it closed for
polynomials but what about the max right
so if I wanted the constraints that
constraints to be fundamental
inequalities and the values to be
polynomials then I couldn't max wouldn't
work out here max of f1 and g1 is not a
polynomial so the question is can I do
the max in case form okay so can I do it
okay often the answer's no but if you
think about it you can do an annoying
trick right I can say well if I'm in
five one side 1 and F ones greater than
G one rights is still in the inequality
polynomials then I get F 1 right or if
everyone's Lesley equal G when I did G 1
so I'm just exploiting the fact that
that that Max is a piecewise function
and I
getting up to those pieces easily as
plenty mule inequalities now others
typically said no to this function we're
a little bit annoyed and they said yeah
you can do this but it'll never you know
you know it you know it it'll blow up
you'll never be able do some practice
okay and I fully agree okay I mean what
I want to tell you des is just high
school calculus at most what kept people
from doing in practice was the fact they
didn't but it didn't have a data
structure to keep things compact and to
exploit the truck from these functions
okay I will get there so so far we've
done plus and times and Max and these
are closed form for polynomials okay if
I'm gonna grab the lot of interest I
just not need to do in for the the
integral right if I do the integral I've
given you everything that you need yeah
now you know the intervals kind of finds
pie my my most technical slide for the
next few slides and then I'll sort of
pop up a few levels okay so I went to
the definite integral from negative in T
to infinity right over some case
function where the flies and the
function values F may involve the
variable X okay so how would I do this
well the first thing I can do is just
rewrite in more key more convenient form
so I'll use the Iverson brackets to say
I I went the zero one indicator over
five sub I and I'll write we write the
case in this simple summation form I
think it's pretty easy to verify that's
equivalent okay and if once I do this I
I can know well shifted my shifted
though there was a little bit I can swap
the integral in the sum okay and what
what does this do it means now I only
have to work on the integral for a
single case partition and then this sum
here just becomes the circle plus right
that I showed you before so this is easy
I just need to show you how to do this
inner integral either simplify the
problem a little bit okay so it's best I
think you do this in the case in the of
an example so here's a definite integral
my constraints are Phi is it given here
and my function values f1 is here now if
I didn't have they did if I didn't have
the far if I didn't have the constraints
is be trivial right it's just a upon a
mule integral the problem it makes us
knowing is is the constraints and so how
do I deal with constraints if you think
about it the constraints are doing
nothing more than constraining the
bounds of
right so I know that X has to be greater
than negative one it greater than Y
minus one otherwise the value of fire
one times that point is zero and doesn't
contribute to the integral so in fact my
lower bounds gonna be the max of
negative 1 and y -1 but I thought I said
I can do the max earlier it's just a
piecewise function so here's my lower
bounds just a case statement okay
likewise for the bounds X is less than
or equal to Z and it's less than or
equal to y plus 1 so it's less than or
equal to obviously the min of both of
these and that's also case function okay
so I can I can pull the green sorry the
blue and the red constraints into the
integration balance and I'll just leaves
me with this constraint here and it says
it's independent of the integrand so I
can just factor out front okay and I get
this final form so I've gotten rid of
all the constraints right there there
are either put it into lower on the
upper bound or out in front now note
that I didn't just write y greater than
zero here because when I first did this
fight my student came back to me he said
can probabilities would be negative you
know this is always a always a scary
question for your students I said well
no and and but we like the operations we
realize well we didn't actually enforce
the devil rounds were greater than the
lower bounds and it's it's not always
clear that it won't always hold if you
don't constrain it to be that way so
example you don't know the Z is greater
than Y minus 1 okay but we never Brown's
happy grid in the loop in lower bounds
so we can just use pair wise constraints
we're gonna add in Z is greater than Y
minus 1 Z's grid and the negative 1 y
plus 1 is greater than Y minus 1 and
greater than negative 1 so in these
constraints here you get not only the
the castrates independent of the
integrand but also these pairwise
constraints on the upper ground critten
lower bound you do this everything works
out well and you get positive
probability which is a good thing okay
now I could do this very easily except
that I don't exactly know how to
evaluate the bounds of the integration
if they're key statements so so here's
here's the problem right this was the in
the integral is it's this result
evaluated they open the lower bound how
do I do this right if different lower
bound work on
to be trivial but they're not there case
statements so how can I evaluate this in
close form yes okay that that's gonna be
the effectively what happens very good
okay so you know we know that plus and
times so we needed that poem was like
invisible sometimes right and this is
this is closed form for case statements
so I can easily substitute in the upper
bound and X cubed it's a prevalence up
around times upper bound and once
one-half x squared times y it's one half
of bound up around times y which is
trivial single produced in case
statement okay okay and because plenum
there's only involved plus an x and in
in plus some time they're close form
even at the the entire partition
integral evaluates to a case statement
okay and so and then if that's the
partitioning they grow Leslie I need to
do is sum for all the partitions right
and that's also a case statement so
voila okay everything is closed form
integration marked mult multiplication
etc everything I need for inference in
this graphical model so general you know
your critical queries are just integrals
and mod in my multiplication that's
closed form okay
or the exact expectation of any
polynomial right the information of a
point mo is just some probability
distribution as a case statement that I
can infer times of polynomial which is a
trivial case statement so mean variance
Q kurtosis or your favorite polynomial
you can do that all these in closed form
and just like you do very little
elimination on a standard graphical
model I call this symbolic variable
elimination because you're working with
these symbolic case statements but you
can generalize the idea here and it's
exact and close forms this model so come
back to the Adina for example right if
I'm willing to approximate all these is
K statements now here's where the here's
where the something comes in right so
triangular is that in uniform most
trivial case statement Gaussian is not a
case statement well I obviously it's not
restricting to polynomials but I claim
that for any PDF you have you can get an
arbitrary approximation with the
piecewise
and you still might be annoyed that you
know I have to approximate a normal
distribution by a Gaussian but I mean
IIIi claim you get low error and the
Gaussian wasn't even appropriate to
begin with so we might all agree that
human heights are normally distributed
but I've never known a human less than
zero meters or greater than three meters
so I mean the normal itself was an
approximation most distributions for
real variables we see in practice are
actually truncated so in general you can
get you know you know you know an
arbitrary approximation with a piecewise
plan I don't tell not to worry about
that and once I do that then all these
functions here are just case statements
right resulting from the multiplication
and the integration so now right I can
do this inference in closed form which I
believe is the first time people
shouldn't do this okay and that's in
theory right so there's all this
voluntary so far so what what you know
this is this is nothing obvious it's
high school calculus with piecewise
functions right what kept people from
working with this in practice okay so I
get to in a second let's just look at
the complexity the best you'll note that
if my case statements had if I case if
it only had single partitions then in in
the best case I'd only have to do a
number of of operations equipment to the
number of variables okay or the number
of operations is then number variables
so i acting tears regret
graphical models there are some nice
lower bounds for the upper bound though
it's exponential number of operations so
on every case operation if you have at
least two partitions right index it
blows up you multiply two case with two
partitions you get four by another to
get eight sixteen so on so that special
number of operations the question is how
do you bend the number of operations in
the number of variables and i and i I
want to point out that it's not firstly
tree with dependent it's much worse so
the integral itself you'll recall
involves plus and times and max just to
the integral for the bounds and
substitutions right so they
neural itself involves hundreds of
operations residential in that so it
seems really quite bad or it seems like
we wouldn't ever be able to this in
practice this exponential blows up way
too quickly okay
and almost somewhat is a surprise I mean
I tried this out and it worked
decision diagram to really mitigate this
worst case worst case complexity and
keep things compact in practice ok so
what I'm going to do from here out is
I'm going to show you how to refer to
the case statement in a in a compact
form that exploited structure but before
I go into what I call the extended a the
extended algebraic decision diagrams to
represent this I firstly they asked do
PR people here familiar with binary
decision diagrams or algebraic decision
diagrams so guys see a show of hands if
you know the BDD and a to eat well well
enough to implement them in the next
hour ok so let me let me just let me
just cover some quick background then ok
so where I'm going is I'm going to I'm
going to represent the case statement as
a as a decision diagram before I get
there I'll cover the background the BDD
Nadd
ok so back to this greet world right
where things work out nice nice and and
cleanly so imagine this is a function of
a function of a B and C a B and here are
binary variables I know that I grab such
a function as a table the question is
can I do better than that how about a
tree right trees are well known so this
tree exactly represents this table and I
want to point out that it's compact when
you have things like context specific
independence so you'll note that in the
context of a being true the value of the
function does not depend on B right so
it's called CS I hate trees but Rupp
tonight compactly OOP but can we be more
compact in a tree what do we use a dag
in place of a tree right can scan any
one point out some inefficiency in this
tree representation that we could get
rid of right si si
see is redundant right we have we have
really exactly the same function see oh
my animation covered okay applause okay
so how are you detective the first thing
I eyed from bottom up I just III a
bottom-up merging procedure so said
before I had all the leaves were
separate now all the zero leaves pointed
the same leaf all the one least points
the same saying to the same leaf and it
was simple hashing right if every node
is represented by the decision variable
and the ID if it's true branch sorry I
didn't say earlier that solid is true
and dashed is false so if every node is
represented by the ID of the variable
and the the idea of the true branch the
idea of the false branch I could just
hash and I note that these two functions
are exactly the same function same
variable test same hiding like in what
branch right and so I can merge these
okay when I do that
when I do that and I had woop and I have
a strict ordering a variable test from
root to leaf I get something called the
ATD okay and so it can be exponentially
more compact than a tree not a so it
doesn't avoid CSI which trees it but it
also exploit shared sub structure so if
you look it's things like the XOR
function right it's not a compact tree
I'll show you in a few minutes but it is
a very compact decision diagram okay now
I want to point out that that ADT is not
only compact erupts and functions the
operations on then exploit their
structure so something click on the
apply operation in the ad D it allows
you to apply any binary operation to two
functions so here's two functions that I
want to multiply yeah how do i exploit
their structure when I multiply them
well ya de nieve thing to do that
wouldn't exploited like if you've
converts in both the tables do the
opportunity operation tables before the
table back to ATT right but but can can
I avoid that yes I can actually build
out the sort of the joint function
that's required to represent the
multiplication I can do this recursively
so I have a variable ordering and I
always want to branch on the first
variable in either diagram so that's
gonna be a they're both shared okay and
I can look at what's
function or predation when a is true
well here I get zero zero times anything
over here is gonna be zero so that's
easy okay so I say the true branch now I
do the false branch okay so a false I
get to this sub function here right and
at a is false here in system function I
I need to occur Civ Lee multiply these
two functions I can't do it exactly
sorry branch from the first favor which
is B okay
and then I get to the two functions with
C here
so B be true get to here right and I'm
still here in this first function and
then it's right in the second function
so in to recurse Kristin C okay you can
sort of see that I can just do this and
fill in the diagram and the nice thing
is this operation exploit the structure
is not gonna brush on anything it
doesn't have to branch on and the result
is provably minimal okay now you might
want to say what's what's the worst case
blow-up of such an operation so imagine
that I just just I had I had in the
nodes in nodes and this function and in
nodes in this function what's the
largest number of nodes I could have in
this function so you note that that I
get I can get one node in the result for
every recursion how many possible
recursions do i get well very naively
most you get recursion for every pair of
nodes okay so if this function I said
notice this house in I the worst case
blow-up of this is is n times n nodes
and that's worst case it's it typically
doesn't have doesn't happen in practice
but the key thing is if you have
structure in your operands you have
structure in your in your result
typically okay so I wanna point out be
the ADA T now not only exploits
compactness and the function of our
reputation but the operations it also
its place to fish efficiency of
operations that exploit the compact
structure okay
that was discrete now I want to go to
continuous case I want to convert a case
to the exit IDI
okay so I call this the extended a big
decision diagram XE TV was cooler than
CA de
okay so let me just show you what it
looks like so here's a case statement
okay and as next a TD right do you sing
something like this so the activity what
are the properties of it every node is
either an inequality okay if it could be
of polynomials in this case just linear
functions
so it's either an inequality that's a
true or false branch or a a discrete
variable and one don't have any discrete
variables in this case the leaves are
just expressions okay
and there is a strict ordering of
decisions from root to leaf if I just
had linear inequalities or polynomial
inequality to be trivial to sort of come
up with some ordering any any ordering
you wanted on those decisions okay so
that's the exit now you might if you
know the beauty and ADA T and you you
you you you know that sort of given it
ordering their minimal canonical you
might ask can I do this for the xat D
yes it's a lot less trivial because now
unlike the BDD and a DD the the
decisions out now interact and I'll show
you some issues that occurs this later
so it's actually much harder to reduce
this to a minimal canonical form you can
do it but it's in P hard I mean there's
basically an embedded Sat procedure in
it or a sat with with linear equation
solving so something called LP sat in it
and so my claim is you don't typically
want to do this in practice it's just
too expensive correctness of operations
on a TDI only requires the ordering it
doesn't require that you have minimal
decision diagrams and some practice
actually III III just an heuristic
minimization for the obvious cases
that's that's that's very very that's
very efficient but I don't always reduce
these to a minimal form and we have done
animation basically your your much
slower if you if you try to do that okay
so that's what the activity is just to
think about the compactness you know you
might be from the BDD you might
recognize this as the XOR function I
just wanna point out that this whatever
this function is okay it has a linear
number of nodes as an exit IDI but if
you expanded this into a case statement
it would have an exponential number of
case partitions right because every path
and it's exponential number of paths
is one case partition right so clearly
you know this can be exponentially more
compact than the case statement okay and
and just like the media t and the ATD
the operations exploit the structure so
imagine I wanted to do a max of these
two functions as XE d DS right so I
start with the earliest the earliest
decision so I'll say that's just
arbitrarily going to be this one like or
than zero okay branch true there okay
the next edition is x-ray than zero but
it's true there what's the max of X and
the max of Y I don't know right so I've
got to introduce a new inequality okay
in this points out that unlike the
beating A to D when you do operation
exit is you might actually introduce new
decisions right this can happen but
that's fine you can do it okay and you
can complete the result here this is the
max to the same max I should earlier but
I wanted to point out that as an X 80 it
maintains more structure than and what
if you if you had a case statement okay
okay and again the operations we exploit
the structure the same bound you got to
be DS and 80 DS that you serve you you
blow up max as the product of the size
of your f and g operands still holds
here for the X the X the X 80 DS that's
quite convenient now unlike P DS and ad
DS you may get issues with orderings so
even if you do correct operations you
make it variables out of order because
you introduced new decisions so what if
I knew that X is greater than Y occurred
before these two but we note that when I
did this max the X greater than Y got
into insert at the end it's out of order
right it should come before these up
into variables okay so that's one issue
I can correct it I'll show you in a
second I'd not get showed here but also
when you substitute variables you can
easily get things out of the out of
order and that's also a problem and how
would I correct that so here's a simple
trick imagine that I had Z out of order
with respect to sub diagrams one and
zero okay now I I'm going to assume that
these sub diagrams are we have been
recursively order themselves okay so I
so now I need to get Z in order how can
I do that
trivially okay
so I can create the left half the
function
Ram times the indicator if Z is true
because the one is the as false that
gives a zero and I can construct the
delivery of the function and then really
exclusive so I can sum them and so so so
automatically this is equivalent to the
left hand side okay but note now that
the diagrams this diagram is ordered
z is triply ordered right this is triply
order and this is ordered
apply preserves ordering right so the
right hand side is equivalent to the
left hand side if I do the apply
operations the plus some times to do
this and the result is ordered this is a
very very quick way to get a reorder
diagram that also exploits caching that
the apply operation does so it's just a
nice trick so in general the faculty the
fact that you do get decisions out of
order is not a problem in practice there
are other reordering methods this is the
one that found to be most efficient okay
now I did say earlier the decisions do
interact and III do want to point out
one unfortunate problem that happens
here so what if Y is greater than zero
and X is greater than zero
can X plus y be less than zero no right
you can you can never reach this left
most leaf so you want to print it out
and promote the right hand branch the Y
in place of this decision okay
now often in practice I work with what I
call linear activities so linear
constraints the leaf values may be
anything you want often but but the
constraints have it have to be or the
decision to have to be linear
why because I need to use the constraint
these the feasibility checker of an LP
solver to check the consistency of these
constraints you can do it for quadric
constraints and so on but it becomes a
lot less efficient okay
and so the question is how much how much
so there's no other prunings that I can
do as well here I haven't shown but the
question is you know how much savings do
I get from pruning out nodes and these
decision diagrams so it's working with
MVPs in one case we got a value function
at a million nodes and we did we just
couldn't compute further with that value
function for any larger horizons and so
I implement this printing operation and
went from million nodes down to a
thousand nodes right so so in general
the majority of this structure and these
in these
if you didn't if you didn't prune would
be unreachable so pruning is absolutely
crucial for efficient in practice okay
so that's the activity right so this is
a diagram that the lesson company
represent functions and do efficient
operations on them it supports all the
operators actually previously we won't
go into those details but it does that
exploit there the structure of this
hidden diagram and I just want to point
out that it makes possible all previous
inference for the for the canoes
variable graphical models and you
couldn't even do a single integral or
continuous maximization without the xcd
right things is I I'd tried this
initially and things just blew up so
that's that's really the secret that
makes all of this work okay so so well
ski can you do with this I'm gonna wrap
up I think soon here I won't keep you
guys too long
but just what exciting thing that I'm
working on a lot with a new PhD student
mind it's sort of very general Bayesian
inference so you know I don't think I
need to tell you guys what this equation
means your prior light your likelihood
your posterior for Bayesian inference if
the prior selectivity and it likely it's
not actually the posterior isn't in is
an exit Edie and I and I claim you no
longer have to choose your prior and lay
your likelihood for accommodation
convenience right you can choose these
really ugly piecewise functions that are
maybe appropriate for your task right
okay and then you can do the posture in
closed form okay now you might say well
yes you knew this for a little while
maybe for like ten data points
okay then things are gonna blow up okay
ready exit ad does blow up it's not as
much of the case statement but it does
blow up so at some point you're going to
have to head to proximate and this gets
back to the point I made earlier where
if you're doing if you want depressing
message passing how would you sort of
approximate in a more structured way I
want to show you sort of as the last
technical contribution an idea that we
had in a UI paper this year and I'm
gonna leave you with an open question if
you can help me solve the question then
you'll help me a lot in future work okay
so sometimes no decision diagrams
compact but hopefully there's a bad
approximation that is how would we do
that how would we find it lovely so I
was in doing this work I was not
motivated by prior work from around 2000
that was done in a TDS for approximating
ad DS so here's a DD it results from
this function I'm summing two to the I
times X I and then I add in this just
some small epsilon noise times x-4 okay
and what has what has that noise done if
you just look at the leads it's
basically just perturb the leaves by
small values right so 7.14 7.06 1.04
versus 1.01 now what if I went to partly
this diagram how would I approximate it
might be obvious give him the leaves I
could just merge them right I could just
murder them right okay I can merge these
with their average I can merge these
with their average until I'm okay and
once you large leaves right then you get
rid of need for X 4 and so on and it's
so if you if you if you do approximation
like this then you get a smaller diagram
with bounded error because I I knew
exactly the values of the leads I merged
so I knew how much error I introduced
this is a very nice within structure
approximation of an A to D okay and so
you you you could just keep track of the
air or here's a nice trick that they
would also use previously you could use
the leads to maintain bounds so here's
the exact function right and so imagine
that the leaves were labeled instead
with with with with a lower bound and
upper bound right so how would i
approximate this function i might merge
the 9 and 10 into a note of the lower
bound of 9 out of 10 and where's the 0
and the 1 until it was a little low
bound to 0 and therefore out of 1 right
so I might get sort of after I simplify
I'd get the dysfunction out and if
you're doing the operations are right we
know that top rate to do arithmetic
operations and song with it with with
intervals there's a thing called
interval arithmetic and it makes it
quite convenient to do it may it
maintains the balance so this is a nice
trick in practice for propagating bounds
when when you're approximating okay
that's 480 DS now how do I do this for X
80 DS so take the step function right
it's represent by this X this act this
simple XE DD so you might think well you
know this w is nicely approximated by a
nice linear function how would I
find the linear function approximates it
well okay I mean that way this is a
greedy way because greed is typically
the most efficient so I'm just gonna
look at pairs of nodes say the two nodes
4 4 4 these two leaves and I'm going to
merge them and now here's the question
when I merge what I want to think the
average of these 2 step functions well
the step function is the constant
function but no actually a linear
function is the best merge so the thing
is you note unlike the ATD you don't
want to average anymore right you want
some better way to fit the function you
know you know I'll come back to that in
a second but let's just say we found
this is the best merge and so we we we
greedily add another node and we find
this is this this linear function again
you can track the errors actually in the
nodes that's what the error here shows
node and eventually you would just get a
single linear function for this this XE
to D it has some error but it's it's
it's a smaller diagram and a small error
okay but I can't average leaves right
that's not always the best approximation
so here's the 0 problem you've got
leaves leaf values that live in two
polytopes and you want to say is there
some purple plane that best approximates
or or that well well approximates to the
two original leaves so we're looking for
an F that approximates F 1 to F 2 within
the region of both polytopes ok so
obviously what I want is the F that
minimizes the error and so here I'll
choose the absolute error between the
problem the the purple plane and the
blue and the green ones okay and if you
think about it was I mean annoyingly
it's sort of a bilinear opposition
problem I have to look at you know the
error every point in these polytopes
well actually I don't if they're linear
polytopes and I know that the error
always occurs at Matt Sarah always
occurs at the vertices ok
so in general actually I only need to
have a only need to solve a problem of
finding the the best F such that it
minimizes the error at all the vertices
so there's constraint here for the
absolute error at all the vertices
problem is there still no it's still an
exponential number of vertices here
right so in general this is not very
efficient to solve in practice but my
favorite trick is constraint generation
right you you you you can actually just
sort of it early adding constraints the
constraints that the most violate the
current solution most people in that
audience robably know this trick and
it's cool because how you know if you
have some setting of see here hey how
would you find the X's the vert the
vertex in those vibe I violated the
constraint it's actually a linear
program itself but that that linear
program can generate X is add the vertex
that maximally violates constraint you
can introduce those new constraints and
you iterate and guess in a cut rate
generation approach and typically you
only need you don't even need to
generate about ten constraint in
practice for for a problem that
typically has millions or billions of
constraints so my student I work this
very very nice method for linear
activity approximation okay and so
here's here as a result we working with
MVPs here and here's some value function
that we had sort of its represent by
this activity just know that it's large
and after applying these leaf merging
tricks to find the best approximation
you get as much smaller x EDD right
sorry they just made by merging and
compressing within this X this X this X
EDD and it's within a five percent error
of the original function and so you'll
knit the linear artifacts but the key
thing is is within five percent of the
air and so back to say very short u P
methods the questions can you come up
with a structured approximation right so
here's one way to shrink your your
reputation while incurring about an air
and preserving the shape of your
function okay but this is for linear
activities right and we know that for
graphical models we needed the
polynomial case not the linear case so
this a bit harder we have that point
enough to we need to find F star that
that sort of minimizes the error what
error function do you want to use what
approximating class as well but for the
error function well I mean the max of
the max error say between two
polynomials there's a pond animal
purring problem I don't want to solve
that problem that's a bit hard but I can
look at the I can take the difference of
the two functions and square it and then
right it those are all clues from kiss
operation so I can actually compute the
error the squid the development of
square error in closed form and I can
optimize that way but there are many
caveats and fortunately the integral is
not as efficient as the constraint
generation approach previously
constrained generation approach for the
linear case avoided looking at all the
vertices of the poet up because it was
doing constraint generation when you
integrate you can't avoid looking at all
the vertices of your polytopes right so
this integral in practice tends to be
quite expensive if you don't repeatedly
and mitai you need to do for for
approximation some practice
unfortunately this is not going to be
feasible and this is actually a huge it
open question for me how do I take this
linear exit ii technique of really
merging and apply it to the plenty okay
so that I can do nice download
approximations of these these functions
and sort of Bayesian inference ok
snipping question when I'm working on
this year with a student we have some
ideas I can talk you all fine about this
ok so I think I'm approaching 10 till if
I haven't come back later in the year I
will tell you all that sequential
decision making which is is if the
application that motivated all this work
we later realize that actually the
solutions that we gave here also applied
to graphical models that's why I talked
about today so I'm not gonna cover this
actually but we have some very nice
solutions for continuous mdps and I just
want to point out that we can actually
for the first time solve operators creat
action continuous MVPs canoe MVPs and
clean the exact policy in this is a lot
saw problems like multivariate inventory
control that were unsolved in practice
for 50 years it was a huge result in
triple era last year probably bees and
so on all these are these are all sort
of things I've done previously in the
sequential control area and like say if
I come back later in the year I might
get a chance to to tell you about these
things but it's just basically basically
noticing that in schedule decision
theory in addition to
integration and multiplication summation
you also need max if you can do the max
then then in fact you can do all all of
these things as well okay you can also
do some itching things because trained
opposition is the only one slide so I'll
tell you quickly get a conditional
constraints right so we don't patent
graphs are not just limited to
probability distributions we can also
encode constraint models say in factor
graphs so if you additionally show you
like there's actually a one-play it's
not
this is the slide trunk to you could not
encode this constraint in a mixed
integer lien program or QP unless you
use cycle the Big M trick and then if
you use the Big M tricky for you but
you've got a to nibs it does anyone know
what the big the Big M trick is no okay
sorry um in general you sort of have to
hack ease constraint into in my LPS name
I keep ease and people people do in
practice but I wanted to iiiii I
wouldn't point out that you you can do
that in case statements furthermore if I
had constraint system with sparse
constraints I could use dynamic
programming to sort of work with that
constraint system and I didn't show you
the sort of sorry I didn't show you the
max I skipped scriptural decision-making
so I didn't show you this operation max
over X but it allows you to seems like
parameterization so I can find if I max
out over X in this function I get the
function of Y the bub since the results
typically in an LP or QP solution you
wouldn't do such a thing but with
operational case statements you can do
what's called parameterize up up
optimization okay so I basically point
out that beyond graphical models
you-you-you can also do very expressive
things in sequential decision making
models and constrained optimization as
well okay so me recap so what I do when
did when I start off with I defined a
calculus for piecewise function this
shows you I showed you how to do plus
sometimes Max and min I showed you the
integral I didn't do the max and the min
but for certain cases that works out in
closed form
okay and I define the exit e to
efficiently compute with the cases okay
so I can do all these operations and
close for them fairly compactly and what
suppose does make possible I can now do
exactly for instance graphical models
where my factors are in this case form
and I got some the first exact solutions
to many planning control and operations
research problems okay and there's also
new paradigms for up for optimization
and so again the issue that
statisticians and operations researchers
and control theorists couldn't do high
school calculus no that's not the issue
the issue is that they didn't have this
data structure that could maintain these
these these large piecewise functions in
the compact form institutions and
operations researchers and control
theorists we're never happy to to use a
gigabyte of memory toward to represent a
function but it is possible now and
that's what you can do okay so in
summary I want to just point out that
this symbolic piecewise calculus plus
the ex EDD does this crucial data
structure for allowing you to maintain
things compact things point out
compactness and operations a lot is very
expressive to use inference operation
and control and if you've been loath to
think about doing symbolic operations in
the past I want you at the end of this
talk to actually reconsider that because
these things do actually work out often
in practice okay and what's up thank you
but it feels like things might I mean
for the BDD it didn't be hard to find
the best ordering when my previous work
with discrete models I did work a lot
with trying to find the best orderings
typically the amount of effort it took
the finding orderings exceeded the the
time that it would take if you just sort
of chose a decent ordering and worked
with it so
yes XO so you have the same issues you
could search for better ordering which
would give you a more compact exit IDI
but typically the time to search for it
might offset the the efficiency gained
the one thing with the STD is you add in
all these new decisions whenever you do
the max operation but it turns out
actually it is right to add those left
in the ordering because they're very
specific to certain parts of the state
space and you want the most specific
unique wise actually be to be near the
leaves so it turns out that when you're
adding decisions the way they're at in
practice said at the end tends to work
best but yes I mean if I have a very
efficient way to find a good ordering it
would help but I don't I don't have a
good efficient way to do that and that's
the problem are you getting sense that
you are in fact getting exempt upper
loge lands in the approximation so for
it ok for of him for discrete models for
sure
for the linear case we didn't actually
do the interval trick I want to point
out that you can dude I I think you
could easily Rebecca so I was there
I think you
easily change the constraints instead of
the absolute error to be a lot about
upper bounds of lower bounds and do the
same trick here so if I want to extend
this since it's instead taking the best
after sort of slices through the plains
I want to take the upper on the lower
bound I just changed the constraints to
do that and then I made maintain those
intervals at the leaves yep so I think I
could easily do that yes okay so yeah I
mean I often you know the exact
functions do blow up but me not as bad
as it would be the case statement but
they do blow up performance is is is an
issue so graphical model 20 continuous
variables is about the the it depends on
the structure of course but but I
typically can't do imprints in more than
20 verbal graphical models exactly it
because I don't yet know how to do this
pong you know I don't Matt head dude so
for exact inference
I hope for the governor model zai need
plenty mules and I don't know how to
approximate yet in a nice way for
graphical models so 20 20 20 variables
is where I'm stuck out right now however
if you're willing to give sampling right
one day I want to point out is that you
know you can do the conditional
integrals you know you can compute the
the the the additional probabilities and
the CDF's
in closed form and so you you can easily
do give sampling in these models you
know and to my one complain about bugs
is that you got to do metropolis
Hastings right so you need to tune your
proposal and if you do give you know you
you you have none of that so you can
easily do get sampling in any of these
models and avoid the need to do the
exact closed form inference and then you
you you you you you could also do many
things that bugs cannot do so doesn't
have any restrictions in practice so I
would say I mean I said why I start off
by saying how Jesus Emerson closed form
but at the you know at the end amount of
thinking with sample he's not such a bad
idea
I just don't have to to my proposals and
so on lately like like I wouldn't bugs
and this method allows you to derive
exactly
exact Gibbs hampers it symbolically
looking at which pruning strategies
might be optimal for certain
distributions of data for example if you
have set training set and you are said
you want to say okay I want to have I
have a bound on the size of the VB then
what is the best pruning strategy for so
strategy being greedy versus something
else yeah okay the only thing I know to
do efficiently is greedy right I mean as
soon as you look at merging multiple
leaves you get a sort of inches k
problem yes you could get smaller the
smaller the smaller decision diagram if
you if you did look at that but in
practice I don't think it's worth the
expense maybe what is the treasured that
you will allow I mean you can
parameterize even your greedy strategy
and at some point because the errors
accumulate right and sometimes you might
use up all your air to do a really hope
approximation and thread things later
there's another I I didn't we're going
to define DVDs for discrete models and
and and that that problem definitely
showed up there so it's a great point
and I I have looked at budgeting so you
know you you know you're gonna do in
merges so you you allow epsilon over in
air purge I can't say in practice that
actually has helped things but it's very
question don't a great answer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>