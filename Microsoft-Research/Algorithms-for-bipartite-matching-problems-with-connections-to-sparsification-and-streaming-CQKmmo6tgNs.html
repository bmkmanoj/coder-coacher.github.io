<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Algorithms for bipartite matching problems with connections to sparsification and streaming | Coder Coacher - Coaching Coders</title><meta content="Algorithms for bipartite matching problems with connections to sparsification and streaming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Algorithms for bipartite matching problems with connections to sparsification and streaming</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CQKmmo6tgNs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good morning everyone and we're
happy to have Michael from Stanford tell
us about algorithms for bipartite
matching problems and some applications
thank you all so I will tell you about
algorithms for bipartite matching
problems were the connections to
streaming and specification let me start
with the motivation well other need to
process modern massive data sets imposes
constraints on the types of algorithms
that we can use and very often we have
constraints on the space usage for the
algorithm and also very often on the
type of access that they algorithm can
have to the data so for example we can
no longer assume that the we can just
load the whole input into memory and
have random access to it all this value
is the need to design six and two
presentations of the input that
approximate the preserve and perhaps
approximately the properties of the
input that we care about so if a graph
algorithms or which is the main topic of
the talk a cut preserving graphs
versification has become an important
way to get a six inch representation of
the input and has become a fundamental
part of the algorithmic toolkit so since
its invention and 96 by bands are in
kharghar it has found the numerous
applications to undirected flow and cut
problems however the specification for
directed graphs is still a challenging
open problem so this talk is centered
around the following topics so first I
will talk about some algorithms for
bipartite matching problems that will
use specification and random walks and
novel ways and here we should note that
matchings are in a sense midway between
undirected and directed flow then I will
talk about the question of how we can
actually implement cut preserving graphs
versification and modern data models
then I will also talk about a new notion
of specification that we have for
bipartite matching problems and if time
permits I will say some words on some
new connections between different
notions of specification in
ocular between spectral specification
and spanners okay so more precisely the
stock will have three or four parts
depending on how much time I have I will
first present some sub linear time
algorithms for finding perfect matchings
and bipartite regular graphs then I will
take I will talk about the new notion of
specification related to matchings in
the streaming model and show some
applications well then I will mention
some work that we did on getting a
distributed streaming implementation of
cut specification and finally some
connections between spectral
specification and spanners leading to
effective algorithms for spectral
specification great so let me start and
in the first part I would like to talk
to talk about sub linear time algorithms
for finding perfect matchings and
regular bipartite graphs we will get an
algorithm that runs on time order n log
n so let me start with the background so
here we have a bipartite graph G the
slides of the by partition will be
denoted by P and Q and as a consequence
of regularity the sides the slices of P
and Q are the same which we denote by n
the number of edges is denoted by M
recall the graph g is d regular if the
degree of every vertex is equal to d so
in particular the number of edges is
just n times d a subset of edges m is a
matching if no two edges an M share an
endpoint and M is a perfect matching if
I'm is a matching and the size of em is
exactly n that is M matches all the
vertices in the graph it's easy to see
using halls theorem that every d regular
bipartite graph has a perfect matching
and finding one such matching in D
regular bipartite graph is the object of
our talk these graphs have been studied
extensively in the context of expanded
constructions routing scheduling and the
task assignment and have several
applications and combinatorial
optimization so in particular I will
also show application
two two problems the first is edge
coloring of bipartite multi graphs and
the second is Birkhoff on normandie
compositions of doubly stochastic
matrices right so this problem has
actually seen quite a bit of algorithmic
history which is close to a hundred
years the first algorithm can be dated
back to kearney in 1916 when can he gave
an algorithmic proof of existence at
that time of course people were not
thinking about algorithms but one can
see that connects proof runs an order MN
time in 1974 how Crafton carp gave their
famous algorithm for finding maximum
matchings and general bipartite graphs
that run runs in time order ever done in
82 galvin kariv considered the question
constrained restricted to regular graphs
and obtained a very beautiful linear
time algorithm for covering a perfect
matching when the degree of the graph is
a power of 2 this is a really nice
algorithm in fact it doesn't use
augmenting paths instead it does Euler
tours to decompose the graph into
regular graphs of smaller degrees ok
well after that there are three
improvements over about 20 years the
first by colon Hopcroft than by Shriver
and finally by Cole Austin Shara who in
two thousand obtained a linear time
algorithm that works for general degrees
so they removed the assumption that D is
a power of two ok so well this algorithm
is extremely efficient linear time was
just the time that we need to read the
input so what else can we hope for here
and the question that we ask is do we
actually need to read the whole limit so
can we go sub linear here for sub linear
algorithms it is of course important to
fix the format in which the data is
given so for the purposes of the stock
we're assuming that the graph is given
an adjacency area representation which
means that each vertex has an array of
incident edges so a natural conjecture
would be the following what if we take a
random sample of the edges of the graph
where each edge will be present
independently with the certain
probability and maybe we can prove that
for certain sampling rates but the
matching will be preserved a perfect
matching will be preserved and the
sample with high probability if we could
do that we could then run some standard
algorithm by cop craft carp for general
graphs and maybe get an improved okay
well this is a reasonable conjecture and
furthermore it turns out to be true and
this is something that we proved in 09
we show that it is sufficient to sample
a uniform sub graph of certain size so
the size is given by the following
expression that depends on n and the
degree but the main point is that this
is never bigger than n to the n roll m
and if we take such a sample of this
regular graph then we showed that the a
perfect matching will be preserved and
the sample with high probability now
using hopcroft carp and in the right
regime for the sampling gives us an
algorithm with the runtime m21 point 75
and this is sublinear for dense enough
graphs so this is the result great so we
do have a saloon here algorithm but well
m to one point 75 doesn't really look
like a natural stopping point seems that
this should be improvable and also seems
that if uniform sampling works then the
most probably non-uniform sampling can
help improve the run time that is also
correct we show that there is a
two-stage sampling scheme there is a
uniform sampling scheme uniform sampling
followed by a non-uniform sampling
process are together with a specialized
analysis of the runtime of whorecraft
carp on these subsample graphs i gives
us a runtime which is worst case n 2 1.5
and in fact equal to the in fact is
linear and the size of this uniform
sample well so this point n 2 1.5 was a
fairly natural runtime for by-product
matching
especially given given the hopper of
carp algorithm but then furthermore one
can see that this runtime is optimal if
we commit to the scheme that we're using
that is uniform something first and then
running a hawker carp algorithm however
the structure of worst case examples
here suggested to us that perhaps we can
get an improvement if we somehow managed
to combine the sampling process and the
process of augmentation so this in fact
can be done and this is the main result
of this part we show you that there
exists a randomized algorithm perfect
matching in a d regular bipartite graph
as long as the graph was given an
adjacency area representation that takes
order n log n time both an expectation
and with high probability ok so first
yes uh ok so stages yes yes the proof is
missing is i will show you the
expectation part and severability will
follow ezel so let me note the following
so first the wrong time of this
algorithm is independent and the degree
of thing of the degree of the graph so
basically we're independent of the size
of the input furthermore the wrong time
is within a logarithmic factor of output
complexity because we need Omega n time
to just output the night great so now I
will show you the algorithm which is in
fact quite simple and give the analysis
so the algorithm will use augmenting
paths to repeatedly increase the sizes
of the size of the currently constructed
matching this point let me remind you
that an augmenting off with respect to
partial matching as a path that starts
on one side of the graph and the P side
of the graph at an unmatched vertex and
then alternates between taking unmatched
and mashed edges until it comes to the Q
side of the
rough at an unmatched vertex well we
need a randomization of this process and
the very natural randomization is the
following instead of taking an arbitrary
step at on steps let's take a uniform
the random outgoing edge which is
unmatched at odd steps in this path and
just still take match match two edges at
even steps so this is something that we
refer to as the alternating random walk
let me give an example so here we have a
four regular graph and a matching of
size three so the green nodes are
unmatched the blue nodes are matched and
to the alternating random walk starts at
a uniformly random unmatched node on the
P side green node takes a uniformly
random outgoing edge and takes the
matched edge back and perceives them
this way so note for example that it can
easily visit certain vertices more than
once and eventually it arrives at an
unmatched node on the q side of the
graph great so it should be noted that
if we have a sequence of steps taken by
the alternating random walk from the p
to the q side then we can get an
augmenting path from this sequence of
steps simply by removing loops so here
we have a loop if we remove it or we get
a length three augmenting path and now
our algorithm looks as works as follows
we start with the empty matching and
then repeatedly for K from one to n we
run the alternating random walk with
respect to the mashing that we
constructed so far and wait until it
hits an unmatched verdicts on the q side
of the graph then we augment using the
augmenting path that we get from this
walk and proceed so I will now show that
the algorithm above finds a perfect
matching in order n log n time great
so to do that it will be convenient to
introduce the following the following
concept we defined the matching graph H
which depends on the graph g and a
partial matching m and the following way
so let me illustrate this so here again
we have our bipartite graph and the
matching m of size 3 so let me first
orient all edges from P to Q then I will
add a source and the sink so the source
is connected to unmatched vertices on
the left and the edges are drawn in
thick because in fact there rd parallel
edges for each thick edge and the sink
is connected to the nodes on the right
and now let's look at them matched edges
and we'll just contract all measures
matched edges into supra note so this is
our matching graph H well our algorithm
can be formulated in a very simple way
in terms of this matching graph so what
we're doing is the following we're
starting with the empty matching and
then repeatedly we run this simple
random walk from the source and this
match matching graph and wait until it
hits the sink once we have the sequence
of steps we have meant using the path
that we obtained from it so what we need
to show right so in the main lemma in
our analysis will say the following but
if we have our d regular bipartite graph
and a matching m that leaves K 2 K nodes
are matched so K nodes on each side then
the expected time until the simple
random walk in the matching graph that
we start from the source hits the sink
is at most 1 plus n over K so when we
start with a very small matching that
leaves a lot of nodes unmatched k is
large it will be extremely easy to find
and I've been augmenting paths with
respect to this matching it will get
progressively harder but the cumulative
effort will be small anyway good so now
let me prove the statement and the proof
will be very simple and it will be
convenient to modify the matching graph
a little bit let's look at the nodes of
the source and the sink
and let's merge them into one super node
okay so the process that we were running
on the matching graph was the simple
random walk from s to T now in this case
this directly corresponds to starting a
random walk at this new supra note s and
waiting until it gets back to us so what
we need to analyze them is the expected
return time to this vertex s great so
well now what really helps is the fact
that the graph that we are getting is a
balanced directed graph and it is most
probably obvious to most of you that
this will be easy to analyze and in fact
we know that for a balanced directed
graph of the distribution of the simple
random walk can be described in a simple
way so first let's check that it's
actually balanced so we have several
types of nodes here there is the supra
note then there are these blue lobes
that corresponded to matching edges that
we contracted well they have in this
case in degree three-and-out degree 3
and in general is d minus 1 and the
green nodes have out degree D and in
degree D it was these edges are thick so
they're balanced too good so we have a
balanced directed graph now we know that
the distribution of the simple random
walk on Sasha graph is proportional as
uniform over edges and so the mass at a
vertex is proportional to the vertices
out degree well at the same time what
we're interested in is the return time
to the special notice but the return
time was just the expected return time
was just the inverse of the stationary
distribution and now we can approve the
result that we want now the degree of
the node s of the super node is d times
K it's okay is the number of nodes that
the matching left unlatched so
intuitively when the matching is small a
lot of nodes are unmatched the random
walk will spend a lot of time at s and
that's good for us so now we can do the
calculation and the key calculation
shows that this quantity is at most a 1
plus n over K
great so this proved the main lemma and
now it's easy to get the runtime
analysis because we simply have n steps
and each step takes expected time at
most 1 plus n over K so the runtime is
bounded by the summation of these
quantities from 1 to N and that is
exactly n times 1 plus the harmonic
number of end that's all realm again now
this was the expected time analysis to
get the high probability result we can
just apply standard techniques truncate
random walks appropriately and use
concentration ok great so this was the
order n log n algorithm for recovering
one perfect machine now let me show some
applications so the first application
will be to edge coloring bipartite multi
graphs and here we get an extremely
simple order n log n algorithm now this
is slightly slower than the best known
the best known is order n log D or D is
the degree but our algorithm is so
simple that I want to state it so the
algorithm works in two steps of the
first step is standard we take the
bipartite multigraph and transform it
into a bipartite regular graph now in
the next step we can simply take out
matchings from this deregulate graph
that we get one by one the each matching
will take n log n time to find and we
will be done in order n log n time in
general now here I am skipping this
point that when we run the alternating
random walk it's important to be able to
sample a uniformly random outgoing edge
that is not matched efficiently and this
has to be verified but it can indeed be
shown that this sampling can be
implemented and constant amortized time
here ok ok what seems very nice about
this is that the fact that our algorithm
for recovering one perfect matching is
extremely efficient six and
log in time irrespective of the size of
the input now we can find such edge
colorings in a very simple manner just
by taking out matchings one by one so
another application is to find egg
matchings and doubly stochastic matrices
and so here if we're given m m by n
double stochastic matrix with M nonzero
entries then the burka formally moon
decomposition theorem says that every
such matrix can be represented as a
convex combination of at most n
permutation matrices so the question is
can we recover such a decomposition
efficiently let me just sketch how this
works and
right so d is the number of bits that
that we use to represent the numbers and
the matrix since it's doubly stochastic
matrix we need to specify what kind of
representation we use so there are some
known algorithms and that find setia
decomposition and so for example they
take order M times B time to find a
single matching in the support of a
double stochastic matrix and they take
order n B log n time to compute the
whole decomposition now I want to just
say that we have a very simple algorithm
with the very efficient runtime here
because we can view this matrix and with
a multi graph and essentially the same
analysis will go through so we can run
our algorithm as long as we can
implement the sampling stage that is
sampling a uniformly random outgoing
edge this is a little harder in this
case then in the edge coloring case but
we can in fact implement this and order
log n time and we get some efficient
algorithms let me skip this this they're
great so these were this was the main
algorithm and two applications now I
want to mention some lower bounds and we
proved to two statements here so first
we proved that randomization was crucial
to obtaining sub linear time algorithms
and particular any deterministic
algorithm has to take at least linear
time so the algorithm of coal Austin
shura which found a matching and a
linear time when the size of the input
was essentially optimal furthermore we
show that we cannot improve upon one
cannot improve upon the n log n runtime
if we want an algorithm that works with
high probability to essentially what the
show is that while we cannot rule out
the existence of an algorithm that finds
a matching in order n expected time and
terminates with probability one-half
let's see but if we want an algorithm
that terminates with high probability
then it has to
at least n log n time with the worst
case great so this completes the first
part of the talk mm so I talked about
sub linear time algorithms for finding
regular matchings in bipartite graphs
and some of them used versification at
least the first the first ones of them
so now I want to spend a few minutes
mentioning a different project that we
worked on and this is about graphs
versification and how we can implement
graphs versification in modern data
modern data access architectures in
particular in active dhts I will define
what that is in a few minutes okay so
even though we used cut specification
and the first part to obtain the first
series of linear time algorithms I
didn't define it so let me define it now
so if we have a graph G and unweighted
undirected graph G then a sub graph H is
a cut specifier and epsilon cut
specifier of G if all cuts in h are
within a 1 plus minus epsilon factor of
the corresponding cuts in G so this is a
great concept because of H is sparse
then we can use H instead of G and cut
or uncut based optimization problems
getting better run times the famous
theorem of bands are in kharghar proved
in 96 shows how to construct a such
specifiers in particular they show that
one can calculate probabilities for each
edge one can calculate a probability P
such that if edges of the graph are
sampled with these probabilities and
then we wait the sampled edges
appropriately the resulting weighted
graph H will be a specifier of G with
high probability furthermore Benson
kharghar also gave a nearly linear time
algorithm for finding these weights and
hence constructing this Parsa fire well
since 96 this has found numerous
applications and cut and flow problems
and in fact has become an integral part
of the algorithmic toolkit arguably
alongside such fundamental primitives as
BFS and DFS
this motivation they need to obtain
efficient implementations of cut
specification in modern data models so
the question that we ask here is can one
get an efficient implementation of cuts
versification in a distributed streaming
setting so ideally or we want an
algorithm that works in a single pass
and a distributed streaming setting to
put this in perspective one might think
of the situation where the nodes of the
graph do not fit into the memory of one
compute node and our architecture for
this will be active DHT which I will
define in a few moments it should be
noted that efficient implementation
implementations are known for random
access model and one pass streaming
model but we also want to be efficient
in the distributed setting okay so let
me say a few words about what activity
HTS are but before I do that I need to
remind you my producers so MapReduce is
this immensely successful paradigm that
transformed offline analytics and data
pros and both data processing recently
in my previous data is stored as key
value pairs in a distributed file system
and computations are the sequence of
iterations of certain MapReduce steps so
mappers and reducers are essentially
processes that are essentially compute
nodes and there is a programming
paradigm that specifies how they
interact well the main point here is
that produces great for offline data
processing so active DHT is on the other
hand may potentially become as important
for online data processing as MapReduce
is for for the offline problem an active
DHT here in fact stands for active
distributed hash table and the hash
table is active in the sense that
besides supporting lookups deletions and
insertions it also supports running
arbitrary functions on key value pairs
there are some examples of such systems
implemented a for example Twitter storm
system and Yahoo is for
and the main applications are
distributed stream processing and
continuous MapReduce so one might think
of this as MapReduce aware the mappers
and reducers do not interact according
to this rigid paradigm and iterations
but they have the ability to talk to
each other continues these are fairly
new systems and in fact there are
challenges in implementing them which
have not yet been fully solved such as
the inefficiency of small Network
requests and robustness but people are
working on that ok so that's all I will
all that i will say about activity HDS
now what we're interested in here is
constructing a spicy fire on active DHT
so let me sketch how does this will work
and to do that first I would like to
look at how standard efficient
algorithms for constructing sparsa fires
work in general there are two steps
first one needs to find these
probabilities PE of the sampling
probabilities and once we have the
probabilities we can just sample
independently using these rates and wait
edges appropriately so the most
important step here is of course how do
we find the probabilities and at a high
level the observation that we use is
that one can estimate these sampling
probabilities using a hierarchy of Union
find data structures the benefit of this
will be that union-find will be fairly
easy to distribute of course there are
some challenges that we need to overcome
to make this work on one of those is the
fact that we need to estimate a
connectivities and sample at the same
time we have to control the size of the
sampling when when this happens but this
can be done and another interesting
point is that we have to when we
distribute the union-find data structure
we have to ensure two things first that
the our distributed implementation does
not lead to excessive communication and
furthermore we need to
to ensure some load balancing properties
that is that not only do we have small
communication but also that the
communication is somewhat evenly spread
across computes and compute node okay
well these are some challenges that we
can overcome and let me just state what
we're getting so we get an efficient
distributed string processing algorithm
that computes a sparse afire on active
ehts and one path and it has some
favorable space usage properties and
good communication and load balance so i
have to skip most of the details here
but i'm happy to chat offline if
somebody is interested okay great okay
so so now I just assisted so far I've
been talking about sub linear algorithm
for matchings and cut and cut
specification now in the remaining time
I would like to talk a little bit about
a new notion of specification that is
related to buy protect matching problems
that we recently introduced and show
some applications to approximating
matchings in one pass in the streaming
model okay so let me now introduce this
definition suppose that we have a
bipartite graph G the sides of the by
partition R P and Q and for simplicity
we will assume that the sides are equal
so the size of P is the signs of Q and
equal to n now we call a sub graph H an
epsilon cover of G if H preserves sizes
of matchings between any pair of subsets
a and P and be in queue up to an epsilon
an additive error so here is an example
suppose that we have the graph G here so
this is the P side this is the Q side
and the condition that H is an epsilon
cover says that whichever two pairs of
sets a and B will look at and we compute
the maximum matching between the two in
G and then we compare it to the maximum
as you can H the next one match
an H should only be at most an epsilon
and additive term smaller so of course
the main question that we're interested
in here is what is the optimal size of
an epsilon cover for a graph on to end
nodes and nodes on each side so this
question asks for a general trade-off
we're givin em and we're given epsilon
or what is the size the optimal size of
a cover now we'll also be interested in
the following twist of this question
suppose that I want an efficient cover I
want to represent the matching Xin graph
using few edges so I constrain my cover
to have 0 tilde n edges and so am poly
log on edges this is a standard notion
of small let's say for streaming
algorithms and now the question becomes
what is the smallest epsilon for which
an epsilon cover with few edges that is
0 tilde and always exist so these are
the two questions that were interested a
to the best of our knowledge there is no
prior work on this so i will just go to
our results and here we prove the
following so on the positive side we
prove that we give an efficient
construction of a one-half cover of a
graph g that has a linear number of
edges furthermore we show that this is
in fact tight and the sense that if we
constrain the size of the cover to have
0 tilde n edges and poly log for any
poly log we cannot do we cannot have a
cover for epsilon smaller than one-half
if we want an epsilon cover for epsilon
smaller than one half then for some
graphs that will need to have at least n
2 power 1 plus Omega of 1 over log log n
edges which is significantly bigger than
any and poly log on ok so this
essentially completely characterizes the
second question that we asked about what
is the best approximation that we can
get with the few edges and for the
general case of the general trade-off we
show that the optimal size of an epsilon
cover is essentially equal to the
largest possible number of edges
and so called epsilon rushes in the
radiograph so this is a very interesting
family of graphs that come up and PCP
constructions properly testing in
additive convenor tarick's and i will
say a few words of about them at the at
the very end okay great so these are the
results but the question now an actual
question is how good is it so what does
it mean that we have a one-half cover so
to put this in perspective let me remind
you our main motivation is the main
motivation is finding approximating
matchings in the streaming model so here
we the edges of the graph are given to
us in a arbitrary order in a stream and
we can only use o tilde and memory the
question here is what is the best
approximation factor to the maximum
matching in the graph that we're given
can we obtain in a single parcel with
the data and sorry great so in the
context of this now one might think that
a one-half cover may not be useful
because a half seems like it seems that
the half is this half approximation that
we can always get by just keeping a
maximum matching that is in fact not
correct and to show that the one half
cover one half for covers in fact
roughly corresponds to an approximation
factor of two thirds for matching for
matchings so here our results related to
streaming the techniques that we use to
construct to construct our half cover
yield us the following first we get a
two thirds approximation to the natural
communication problem associated with
matchings in one pass and I will define
that in a few slides furthermore we get
a lower bound of two thirds for one pass
training algorithms that is we show that
no one pass training algorithm that uses
o tilde and space can get a better than
two-thirds approximation to maximum
matchings and finally this will also be
useful in the so this was for the
communication problem and got a lower
bound but will also show that our
techniques are useful in the stream in
the general streaming case as long as we
make this additional assumption that we
don't have any
rivals but very vertices arrived in the
stream I'll talk about this later great
so in the remaining time I will do the
following first I will show the
construction of what we call the
matching skeleton so this is the
matching sparsa fire that is our main
tool for these results and we show that
it is a half cover I will have to skip
the proof however then I will talk a bit
more about the applications to streaming
and also the connection between epsilon
covers and rushes I'm already graphs ok
so the matching skeleton and so the
matching skeleton will be a sparse
subgraph of g that in a sense preserves
some useful information about matchings
and now i will give the construction but
first i will make this one technical
assumption that in our graph g there is
a perfect matching of the p-side the
general construction will be very
similar but this will be easier to
describe so what this says in particular
is that the verdicts expansion of all
sets and the p-side is at least one so
one other thing that I will need is the
definition of an alpha matching so this
is a fractional matching that matches
each riddick's on the P side exactly
alpha times and each vertex in queue at
most once so alpha will be bigger than
one great so the construction of this
matching skeleton or will proceed in two
steps first i will take the graph and
come up with the decomposition of the
verdict set of the rough and to what we
call expanding pairs so these will be
pairs as jtj and these will be verdicts
induced subgraphs that have increasing
verdicts expansion so the j th sub graph
will have expansion denoted by alpha j
and this expansion will be the ratio of
the size of the graph
okay so once I have this decomposition I
will choose a fractional matching inside
each size sub graph and so the edges
that the fractional matching is
supported on will be the edges of the
skeleton okay so how does the
decomposition work the because the
decomposition works as follows we start
with a graph and we repeatedly find and
remove sets us from the P side of the
graph that have the smallest vertex
expansion for example here we find the
set s not that has the minimum possible
vertex expansion and we removed graph
now it might seem ill-defined and in
fact it is as I just stated it because
there could be a lot of such sets that
have the smallest possible vertex
expansion but one can show that there
will always be a maximal set to remove
and that's what we do so this gets
removed from the graph and we recurse on
the rest now again we find the smallest
expanding set in the P side and we're in
need great so this goes on until the
remaining part of the graph has
essentially the best possible expansion
for setia graph that is expansion equal
to the ratio of the of the size of the
vibration okay so this is the
decomposition now it can be shown in
fact that the verdicts expansion goes up
as we do this do this process and each
piece in the decomposition has vertex
expansion which is the ratio of the
sizes of the size of the sets in the by
partition so in particular there exists
a fractional alpha J matching or alpha J
is this expansion in each sub graph the
smashing can always be chosen to be a
forest just by canceling cycles and the
edges that this forest the edges of this
forest are exactly the edges of our
matching scout right so this is the
construction now I have to skip the
proof of the main property but the main
property is the following suppose that
we have two graphs vibrate at g1 and g2
and we're interested in the
so matching in the union of these two
graphs now if we instead replace the
first graph with its sparsa fire with a
matching skeleton then what we get is a
two-thirds approximation so this is the
main property and one can in fact derive
from this property that the matching
skeleton is a half cover so what this
means is we have a graph with n vertices
on each side then the matching skeleton
of the graph will preserve the sizes of
matchings between any pair of subsets up
to an additive n over 2 term ok well
again let me stress that it might seem
that something simple like a maximum
matching would have these properties but
in fact that is not true a maximum
matching does not give it better than
two-thirds cover ok fine so now let me
sketch the connections to streaming and
so far I defined this matching skeleton
and showed that this is a half cover now
let me show you the connections to
streaming and here I need to define the
following communication problem so I'm
covering oh sure you use language so if
we have a graph G then the graph H is an
epsilon cover those so and it's balanced
in the sense that the N vertices on each
side now H is an epsilon cover if the
following is true if we look at any pair
of sets a and one side and be on the
other side and calculate the maximum
matching between these sets ng and an H
and we compare them now the maximum
matching in H should only be an most an
epsilon an additive term smaller so in
this case so here we get this property
with the one half so we preserve these
matchings up to an over to edit return
ok so the communication problem is the
following and we have to communicating
parties Alice and Bob now Ellis has a
graph g 1 and bob has a graph g 2 on the
same set of vertices but the different
set of edges now Alice sends a message
to Bob after which bob was supposed to
output a 1 minus epsilon approximation
to the maximum matching in the union of
two graphs so maybe this magic now the
questions that we're interested in here
sound a lot like the ones that we asked
for matching covers first what is the
minimum size of the message that Alice
needs to send Bob that will always let
Bob output a 1 minus epsilon approximate
matching in the Union now this is again
asking for a general trade-off between m
and epsilon and I constraint a
restricted version of the question as
suppose we restrict the communication
between Alice and Bob to be 0 tilde n so
n pollywog and was a linear and the
sudden the number of vertices and
Alice's graph what is the best
approximation that they can achieve well
a natural approach to this problem is to
just ask alice to send a maximal
matching of her graph to bob this will
be very little communication it will
take Oh tilde and communication and give
a one-half approximation so now um yeah
this is a great problem but why do we
why do we care about this problem now
the motivation for this problem that
comes from the problem of approximating
maximum matchings in one pass in the
streaming setting and in fact a lower
bound for the communication problem will
immediately translate into a lower bound
for streaming algorithms and well an
upper bound will not really translate
directly into anything but nevertheless
there are the techniques that will work
for for the communication problem we'll
also let us get a result for streaming
with vertical arrivals ok so there is
some prior work on this problem there
has been significant progress on
approximating matchings in the streaming
model and k passes but for k greater
than greater than one for a single pass
the best known approximation is still
one half and achieved by the trivial
algorithm that just keeps a maximal
matching this was very recently improved
to one-half plus epsilon for a small
positive constant epsilon but this is
under an additional assumption that the
edges arrive
in a random order in the stream and
we're interested in the case when the
arrival orders are adversarial this is a
comrade Maggie and Matthew very recent
maybe a month or two ago so yeah so on
the lower bound side the only lower
bound known is omega n squared for one
pass but this is for computing exact
matching ok so let me state our results
so first it follows immediately using
the results that we proved for the
matching skeleton that the communication
complexity of obtaining a two-thirds
approximation to maximum matching is 0
tilde n in particular instead of sending
a maximal matching of her graph Ellis
can compute the matching skeleton which
is sparse and send it to Bob and so this
is for the communication problem but for
the general streaming case we show that
in fact so if we use the specification
procedure given by the matching skeleton
we can just use it repeatedly in the
steering model and as long as we have
the assumption that not edges but
vertices arrive in the stream we will
get a 1 minus 1 over e approximation to
the maximum matching this will take
linear space and we will only use a
single pass over the data so it should
be noted here that 1 minus 1 over e can
also be obtained in this setting using
the ktv algorithm for the online version
of the problem but that algorithm is
randomized and do so our algorithm will
be deterministic ok so so far I showed
that one half covers exist and that
communication complexity is quasi-linear
when we want a two-thirds approximation
but a natural question is what about
better covers and better better
approximation well here we show
connections to a family of graphs known
as epsilon whooshes and ready graphs
unfortunately I do not have enough time
to define them properly
but essentially would have a these
graphs are defined by the property that
they their edge set can be partitioned
into a union of induced matchings and
each such induced matching will have
sighs at least epsilon and in fact these
graphs come up in applications
improperly testing and in pcp
constructions and additive convenient
oryx and it is a major open problem to
determine the optimal size of these
graphs as a function of epsilon and then
the gaps between the best known bounds
are immense so for example the best
known upper bound for these graphs is N
squared over log star and the best known
constructions for constant epsilon
achieve this the number of edges which
is n 2 1 plus Omega of 1 over log log on
ok so and we show that for the general
question of bounding the optimal size of
epsilon covers this question is
essentially equivalent to bounding the
optimal size of epsilon wishes in
marathi graphs okay
okay now furthermore let me say how we
obtain the lower bounds for streaming
algorithms and lower bounds for the
communication complexity problem and so
this is done via an extension of a
beautiful result by Fisher and others
where they construct epsilon whooshes
and ready graphs that have constant
epsilon and they achieve other number of
edges n 2 1 plus Omega of 1 over log log
n but their construction works for
constant epsilon and here we extend this
construction to work for all epsilon
arbitrarily close to one half now this
immediately gives us lower bounds that
say that our bounds on the half covers
and the linear communication complexity
our best possible that is if we insist
on quasi-linear communication two-thirds
is the best we can do and if we insist
on quasi-linear number of edges one half
is the best we can do for coverage so
this also implies a streaming bound and
so this one half here is actually the
largest epsilon that we can possibly
hope for because our construction of a
one-half cover precludes the existence
of these graphs with a large number of
edges for larger epsilon great so so
this concludes the discussion of our
notions fortification for matching
problems and applications to streaming
now in the remaining minute let me say
two words about some other work that we
have been doing with the Rena panigrahi
and so this is others
so this is just a this shows some
connections between spectral
specification and spanners and in fact
we show that one can obtain efficient
algorithms for obtaining spectral
specifiers using spanners of random sub
graph okay and yeah so I've done some
other work on online matching and the
multi invented problems and
differentially private low-rank
approximation and I thank you for your
attention
for their
you know about that
but are there cases of further
assumption where
the faster for
yeah this algorithm definitely takes
time and login on the complete graph so
yeah I'm not really sure I don't know if
any assumptions that would make it
actually order end no that's a good
question yeah and the lower bond
actually the lower bond that we proved
in fact precludes odor n with high
probability only for dense graphs that
is if we have n squared close to N
squared edges Omega N squared so the if
the graphs of sparser it's not clear and
in fact it cannot be true for very
sparse graphs because there is a linear
algorithm if the degree is constant
there's nothing that I'm aware of
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>