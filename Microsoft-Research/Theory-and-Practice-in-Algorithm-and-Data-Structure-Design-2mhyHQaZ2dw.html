<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Theory and Practice in Algorithm and Data Structure Design | Coder Coacher - Coaching Coders</title><meta content="Theory and Practice in Algorithm and Data Structure Design - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Theory and Practice in Algorithm and Data Structure Design</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2mhyHQaZ2dw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
today I want to talk at kind of a high
level about some ideas in algorithm and
data structure design and how they
relate how theory and practice relates
and how theoreticians or how I think
theoretician should approach these areas
and I'll illustrate some of these
concepts with two problems I've worked
on over many years but there are still
things to be learned about these
problems so I'll talk about some new
work and but start out with a classical
work so let me begin with some general
observations you know that AVL tree
which was the first balanced search tree
was invented by Adelson belsky and
Landis two Russians in 1961 and actually
Addison belsky just died last month
and since that time and even since
earlier theoreticians computer
scientists mathematicians have developed
many beautiful asymptotically efficient
algorithms but many of these algorithms
are very complicated and have not yet
been used in practice and many of the
theoretically efficient algorithms when
used don't work so well in practice
because they have high constant factors
or people are ignoring logarithmic terms
or they're just too hard to implement
and sometimes the theory says the
algorithm should be good but simpler
methods work faster for somewhat
mysterious reasons so we'd like to
understand this phenomenon so first of
all of course implementers programmer
time is much more valuable than computer
time so implementers when faced with a
task will choose the simplest possible
way to get it done that meets the needs
of the problem so what is only in
situations where you have a basic task
that you want to solve repeatedly or you
have large instances many large
instances that you need to think about
sophisticated algorithms so Einstein
said something like make everything as
simple as
possible but not simpler this is our
goal to use the best possible algorithms
in the situations as required all right
now here's another interesting thing
which is that data instances are not
necessarily worst case some of us design
algorithms to meet worst-case needs but
the problem instances that come up or
not worst case they come from some kind
of distribution which we cannot
necessarily model probabilistically
either so average case analysis does not
necessarily reply apply although I'll
give you an instance where it does apply
practical instances sometimes have some
structure that we can exploit through
the algorithm so the goal then is to
instead of designing a general-purpose
worst-case efficient algorithm to design
an algorithm that's simple and exploits
the structure that actually arises in
practice all right now so my goal is an
algorithm designer to try to develop and
analyze simple methods apply theory to
analyze methods that actually get used
in practice and to improve those methods
take heuristic methods and turn them
into verifiably efficient methods take
complicated methods make them simple
find the structure in the data and
exploit that structure in the algorithms
so Paul Erdos who was a famous
mathematician who collaborated probably
with more people than anybody else had
this idea of proofs from the book this
is the book upstairs it's a beautiful
idea design a proof as simple as
possible it really captures what is
going on and in the same way I think
it's important to try to design
reference algorithms or algorithms from
the book they're as simple as possible
maybe they have some mysterious
properties that we can capture in the
analysis but the algorithm should be
really clean easy to code
efficient in practice and with provable
guarantee resource guarantees at least
on important input classes all right
this is all a little bit highfalutin so
let me talk about two one data structure
and one algorithm that are tied together
in interesting ways and on which there
has been work over several decades the
first data structure I'm going to
discuss is a data structure for
maintaining disjoint sets or
equivalently equivalence relations or
maintaining the connected components of
a graph when we're adding edges and the
data structure to solve this problem is
a compressed tree so in this problem we
want to maintain a partition of a fixed
set of elements we can think of adding
additional elements but let's for
simplicity just imagine that we have a
fixed set of elements we're going to be
combining disjoint sets and we want to
identify each set somehow and we'll
identify each set by an element in it
which I'll call the root so we can store
information about the set with the root
data structures for this problem as far
as I know first arose in Fortran
compilers in the old days computers had
very little memory and it was important
to reuse the memory by sharing variables
and overlapping arrays and so on and
that's naturally leads to this
equivalence problem and also the stateís
structure that I'm about to describe
comes up in kruskal's minimum spanning
tree algorithm another graph algorithms
and in percolation where we're trying to
find paths between sites in some kind of
a network where edges are getting added
okay so to be a little bit more precise
we want to maintain sets under three
operations we can create a new set
containing a single element that is in
no other set we want to be able to do a
lookup operation which we'll call a fine
which returns the root of the set
containing the element X so that's
essentially the name of the set and the
update the important update operation is
unite which given two elements if
they're in different sets puts the two
sets together into a single set and if
they're in the same set doesn't put them
together in the case and returns a
boolean value that gives you the outcome
so if x and y are in the same set this
is set up to return false otherwise if
they're in different sets we combine the
two sets containing x and y pick a new
root from among the items in the unified
set to name this set and then we return
true so if you prefer true to be X and y
are in the same set and falls there in
different sets feel free to change this
is natural and kruskal's algorithm where
this is the key operation all right now
there's a very simple implementation of
this abstract data type which is to just
represent each set by a rooted tree
whose nodes are the elements in the set
with every node having a pointer to its
parent in the tree and will make the
root point to itself so we can walk from
any element to the root of the tree
containing it and that root is the set
root the tree root is the set root so
it's simple to do a find operation we
just walk up the path in the tree the
shape of these trees is completely
arbitrary all we have to maintain is
that the beach each set has its own tree
and I'll denote by n the number of
elements and by M the number of finds
plus unites the total number of
operations and I'll assume that the
parameters are non-trivial so we got at
least two elements and we're doing say
at least one find on each element this
is just for simplicity in stating time
back
all right so to make a set we just make
X point to itself and then X as a
singleton set its its own root if we
want to do a fine operation we just
follow parent pointers from the node X
to the root and if we want to do a unite
we do a find on X and on Y which gives
us the two roots of the trees containing
x and y if they're equal we don't change
the data structure if they're different
we'll do what I call a link and return
true and a link just makes one of the
roots point to the other these are the
roots V and W and we have some
flexibility here we can make we can make
V point to W or W point to V and we'll
exploit this choice in a minute all
right now the trouble with this naive
implementation is that we can easily
create a long path and then we can do
repeated long finds so this costs us
order n per find which is a total of
order M in for M operations which is not
too happy our goal is to get much closer
to constant time per operation rather
than linear time per operation okay and
we're not concerned about the individual
time of operations
we're willing to allow some operations
to be expensive if that makes other
operations cheaper so I want to just
mention this notion of amortization
which captures this idea we're
interested in the total time of all the
operations which we can then divide by
the number of operations to get what
I'll call the amortized time per
operation and the links are constant
time to make sets are constant time
everything except the cut the fines are
constant time in order to improve this
data structure we need to reduce the
lengths of paths in these trees and
there are two ways of doing this both
exploiting the design flexibility we
have here that the shape of each tree is
arbitrary this
the most primitive use of a tree data
structure you can use trees to implement
priority queues that's the data
structure that was discussed in the
previous talk or you can use them as
search trees here all we need is a
parent pointer per node the shape of the
tree is arbitrary let's see what we can
do with this so the first thing we're
going to do is exploit the freedom in
the links to basically make the root of
a small set point to the root of a large
set now that's the idea of linking by
size which is the original version of
this improvement I'm going to talk about
a slightly different more efficient
version which is called linking by rank
where we measure the size of a tree by
the maximum path length the height and
then we'll talk about modifying the
trees during the finds to squash the
pass again exploiting this fact that the
shape of the tree is arbitrary okay so
linking by rank we add to each node a
non-negative integer value which we'll
call the rank which is a zero for a
single node and we always make the root
of smaller rank point to the root of
larger rank when we're doing a link the
only ambiguous case is if we're trying
to link to roots of the same rank and
then we'll increase the root of the rank
of one root by one and now link the
smaller to the larger so here's an
implementation of the link operation
with this kind of now this very simple
linking rule guarantees that the maximum
path length the height of the tree is
reduced from linear to logarithmic
because in order to build a path of
length one greater you have to put two
trees of roughly the same size together
that all by itself reduces the time for
fine from linear to logarithmic this is
a worst-case guarantee now now the other
thing we can do is
changed the tree when we do fines using
some kind of compression strategy this
strategy was invented by Alan tritter
was IBM many years ago in an
implementation of Fortran equivalence
and common statements so the idea here
is that when we walk up a path in the
tree we've got the answer we've got the
route not just for the original node but
for every node along the path so let's
make every node along the path point
directly to the route that will reduce
the time for later finds from whatever
the path length was to constant in fact
until we start adding more roots on top
so here's an implementation a recursive
implementation of find with path
compression this implementation walks up
the path to find the root and unrolls
the recursion walking back down the path
to set all the pointers you can also use
two passes over the fine path and
implement this iteratively if you want
to and just to make this clear here's an
example this is a tree representing 1 2
5 3 4 8 7 10 10 is the route it points
to itself if I want to do a find on 1 I
follow the parent pointers 10 is the
answer and if I want to do compression I
then make all of these nodes point
directly to 10 like so so this flattens
the tree and really improves things
later on I pay a factor of 2 more or
less in the find operation locally but I
compressed the tree and save time later
on so this is a situation where
amortization is very important we're
going to get cheap fines later on for
paying exit by paying extra on this
particular find but only a constant
factor ok now one can criticize the fact
that we're doing two paths over the fine
path in order to do compression we can
also do compression with only one pass
and there are various ways to do it
here's perhaps the simplest paths
when we walk up the fine path we make
each node point to its grandparents so
we only have to look ahead one node and
here's a purely iterative implementation
of find with splitting and here's the
illustration on the same tree we had
before so now if I do a find on one I'm
going to make every node point to its
grandparent along the fine path so one
points to five two points two eight five
points to ten eight doesn't change ten
doesn't change so this also compresses
the tree but not by so much it splits
the original fine path into two paths
each of about half the length all right
now how do these methods perform in
particular how much improvement do we
get out of path compression it turns out
to be perhaps surprisingly non-trivial
to analyze what's going on because you
have to analyze a long sequence of
operations if you use naive linking that
is without linking by rank but one of
these path compression strategies and
you reduce the time per find to again
logarithmic and it's actually
logarithmic which a bit with a base here
that depends upon the ratio of
operations to elements so as the density
of operations increases this this goes
down actually so this gives all by
itself path compression gives at least
as much benefit as linking by rank now
the question is what happens if you put
the two ideas together this is the
algorithm that
tritter used in his Fortran compiler and
Knuth back in the late 60s early 70s
posed the analysis of this data
structure as an open problem and it's
surprisingly non-trivial to analyze and
it's got a non-trivial answer so shortly
after Knuth posed the problem there were
a couple of famous theoretical computer
scientists who I won't name who gave a
constant
Tai's time bound for this data structure
which turns out to be invalid Mike
Fischer in 1972 got a log-log upper
bound hopcroft and ullman improved it to
log star and then I was able to show
that the real answer is inverse
Ackermann function and to illustrate the
fact that this is fairly non-trivial
this analysis there was at least one
later false proof of a lower bound of
log log n which couldn't be true if this
is the right answer I'm not going to go
through this analysis but I just wanted
to find what the Ackermann function is
here because it's kind of interesting my
proof uses a sort of a bottom-up
analysis there's a completely different
top-down analysis that was proposed
relatively recently all right here is
Ackerman's function this was originally
proposed back when logicians and
computer scientists pre computer
scientists touring were trying to define
computation and the logicians approach
was general recursive functions
recursive functions with multiple
variables and Ackerman designed his
function to show that single variable
recurrence recursion is not sufficient
to generate all general recursive
functions it's a two variable recursive
function that grows faster than any
single variable recursive function and
here's the definition the key part of it
is these are the boundary conditions and
this is the important thing
a KJ think of K as defining a the case
function in the family so we get the
case function in the family by taking
the K minus 1 function in the family and
applying it to itself J times
essentially that's what this is safe and
the zeroth function is just the
successor function this other piece of
the initial conditions gives that the
eight the first function is just the
plus two functions this doesn't look so
interesting but the second function here
is not plus three it's at least it's
really twice it's multiplying by two and
then the third function is
exponentiation and the fourth function
is essentially iterated exponentiation
power of two's
and this thing grows really rapidly if
you plug in small values here you get
giant numbers the inverse function the
inverse Ackermann function that I don't
know by alpha here is essentially well
the functional inverse of this this
function the details don't matter too
much because this thing grows so fast
that any reasonable inverse function all
possible inverse functions are related
to each other by an additive constant in
fact all right now this very simple
algorithm has this incredibly
complicated running time this is both a
lower bound and an upper bound the proof
on both sides is complicated good so
here's the story compressed trees with
path compression are splitting and union
by rank take just over constant time per
operation in theory Ackermann function
you can never detect experimentally it
grows too slowly
in fact Seidel and careers analysis they
did a very careful tight analysis and
they're bound on the number of pointer
changes is something like the number of
finds plus twice the number of elements
for any problem size that could be
solved at smaller than the number of
electrons end of universe so this is
this is a constant for all practical
purposes but theoretically non constant
and there's this esoteric function that
was defined in mathematical logic that
comes into play in the analysis of this
algorithm compression and splitting
takes something like log time and this
bound is tight if you ignore the base of
the logarithm these are the theoretical
results so we would say that if we want
to take the good algorithm in
practice we should probably take this
one and you can probably detect the
difference between constant and
logarithmic even though you're not going
to detect the difference between
constant and inverse Ackermann function
all right now let's turn to a graph
problem that starts out completely
different but turns out to be related to
this set Union problem this problem also
comes up in compilers it's the problem
of computing dominators in a flow graph
so a flow graph is just a directed graph
with a start vertex such that every
vertex is reachable from the start
vertex so imagine the a computer program
with a start point and a flow chart if
you will or text of the computer code
blocks of code correspond to vertices
and transitions from block to block
correspond to edges will say a vertex V
dominates another vertex W if V is on
every path from the start to W this
notion is important because if we want
to do global code optimization and we
have some computation going on inside an
inner loop that's repeated over and over
we can pull it out of the loop but we
have to move it to a point where we're
guaranteed that we will pass through
that point in the code no matter what
path we take to get into the loop so we
can move code from some place to some
other place that dominates it so global
code optimizers use this notion and
construct the Dominator tree the
dominators this is a relationship that
has a particularly nice structure it
defines a tree that is there is a tree
which I'll denote by D such that V
dominates W if and only if V is an
ancestor of W in this tree so the tree
is rooted at the start vertex and we
want to compute this tree so here's a
flow graph one is the start vertex if I
want to get to nine for example I have
to go through two so two dominates nine
and so on and for this particular flow
graph this is the Dominator tree so two
dominates sorry yes it dominates seven
eight and nine but it also dominates
five and six and six is dominated by
both five five two and one so to get to
six I have to go to five to get to five
I have to go to two to get to do I have
to go to one and that's the deepest path
in this tree and this is what I want to
construct now there's a naive algorithm
as an obvious one for constructing this
tree which is just to remove each vertex
in turn first applications I mentioned
global code optimization this is a key
algorithmic component in every global
code optimizer but there are other less
obvious interesting applications
including in circuit testing and in
theoretical biology analyzing food webs
where you're trying to figure out if
there are certain species whose removal
is going to starve a whole bunch of
other species so the dominance
relationship has to do with dependence
on each other as food and actually
whereas program graphs tend to be
reasonably well structured nature is
complicated on the graphs that arise
here have no useful structure in
computing Dominator trees okay now the
obvious algorithm is just delete one
vertex at a time
Oliver toises that become unreachable
once you remove a vertex are dominated
by the vertex that you removed so you
delete each vertex in turn do a graph
search which takes linear time the total
time for this algorithm is vertices
times edges which is certainly
polynomial which is good but it's not so
good in practice because among other
things the worst-case time bound is the
same as the best case time bound this
algorithm doesn't exploit any particular
structure
if you remove each vertex and turn the
only thing it might exploit is that you
can cut the search off early if there's
a lot of dominance in the graph some of
your searches aren't going to explore
the whole thing but essentially this
algorithm the best case is the worst
case now I did my PhD at Stanford
exploiting depth-first search to get a
fast algorithm for planarity testing and
when one has a good technique one tries
to hammer away at problems and this is
an obvious problem it's connectivity
problem and directed graphs where
depth-first search should help to solve
it but first back to our example so here
if you delete 2 5 6 7 8 &amp;amp; 9 become
unreachable so it dominated by 2 if you
delete 5 6 becomes unreachable and it's
dominated by 5 and if you delete all the
other vertices it doesn't create any non
reachable vertices so all the other
removals turn out to be useless but you
can't tell it ahead of time all right so
Tom languor was a student of mine at
Stanford we developed a three pass I'll
call it a three pass refinement
algorithm or the LT algorithm for
computing dominators based upon
depth-first search and based upon a data
structure which generalizes this
compressed trie data structure that I
started talking about previously so it's
three pass refinement a computes an
approximation to the dominators called
semi dominators and from semi dominators
that computes a better approximation
called relative dominators and from
relative dominators that computes
immediate dominators which are the
parent information in the tree structure
and I don't want to go into the details
except to note that step 0 and 3 are
very easy and steps 1 &amp;amp; 2 are the core
of the algorithm and
the real effect of this idea is to
reduce a graph problem via depth-first
search to a data structure problem a
data structure problem much like the
disjoint set Union problem except that
there are now numbers involved and the
specific abstract data structure that we
need solves this problem I'll call it
the path minima in trees problems so now
we have trees whose structure is forced
upon us by this Dominator algorithm but
they're all no disjoint and every vertex
has an integer value and rather than
walking up paths to the root and
returning the root we want to be able to
walk up pass and return the minimum
value along the path so we now have link
operations which allow us to hook two
roots together we can create new
vertices with predefined values and in
place of the find operation we have a
fine min operation which finds a note of
minimum value on the path from X to the
root of its tree and using an
implementation of this data structure we
can then plug it into the Dominator
algorithm and get a fast algorithm now
it's fairly straightforward to use path
compression here so we we get sort of
half the benefits of the disjoint set
Union algorithm but because of the fact
that we're dealing with numbers on these
paths and the link has to go in a
certain direction that is one of these
as specified as the root and the other
one is specified as the new child we
don't have the flexibility to do the
linking the way we would like to do it
to make the trees relatively balanced so
it seems like we can't get the benefits
of both simultaneously and get down to
inverse Ackermann function but we did
manage to in fact do it so we came up
with two versions of this algorithm one
of which uses path compression in a
straightforward way and has a running
time which is old ax m this is linear in
m if you ignore
log factors and then we also succeeded
in getting a almost linear linear for
all practical purposes algorithm Tom
actually implemented this both versions
of this algorithm and we were of course
prejudiced because we liked the
complicated algorithm but in his
experiments the complicated algorithm
proved to be slightly faster than the
simple algorithm on the examples that we
ran which weren't very big but not by
very much ten to twenty percent all
right and in fact because he implemented
this algorithm and put the code or
pseudocode into the paper this is the
algorithm that is used in a lot of
optimizing compilers in practice ok but
this algorithm it's got a lot of moving
parts and even though the code was there
it's hard to think about
so programmers raised various questions
about it an obvious one is can you come
up with a simpler algorithm but
similarly fast linear or M login or
something like that and a related
question about correctness is how do you
know that you've got this thing
implemented correctly is there some way
to verify that the tree you get out at
the end is the Dominator tree other than
by doing this naive approach of removing
each vertex in turn which is very
expensive that is can you make the
algorithm self-certifying can you
somehow easily verify a Dominator tree
we know from the theory of P and NP that
verification at least of certain kinds
of problems is much easier than
generating the solution in the first
place now here we've got a very fast
algorithm to generate the solution but
it's complicated is there a simple
verification test for Dominator trees
I'll return to this question later ok
lots of interesting theory in response
to this this question about a
significantly simpler algorithm Cooper
Harvey and Kennedy
2003 took a bit vector algorithm and
modified it in a nice way to give a new
simple algorithm the tree update
algorithm I'll call it you maintain an
approximation to the Dominator tree a
conservative approximation and then you
repeatedly improve it and the idea is
that if there is an arc in the graph
from V to W such that V is not a
descendent of the parent of W and you
can replace the parent of W by the
nearest common ancestor of V and W in
the tree so if we have it let's think
about this for a minute if we have well
I'll show an example in a minute but let
me just say what this algorithm does all
we need is a very simple representation
of the Dominator tree just parent
pointers as in our original data
structure we can find nearest common
ancestors by following parent pointers
simultaneously from that two ends of
each arc it's a simple iterative process
but it runs very slow in theory in fact
it runs even slower than this naive
vertex removal algorithm if you
implement it carefully whoops
you get a worst case bound of N squared
m but this algorithm though the best
case is not the worst case this thing
performs pretty well in practice and
these folks claim that it ran very fast
in practice
perhaps faster than the LT algorithm
that I described now here I've got an
example so if we start out with a what's
a good approximation well let's do a
breadth-first search of this graph and
use this to build our initial tree so
here's a breadth first search tree of
this graph which gives us a superset of
the Dominator information so and now we
notice this edge from 5 to 8 this tells
us that we can get to 8 avoiding 9
because there's this path here
so to encode that in the tree we replace
the parent of 8 by 2 and this edge 6 7
tells us that we can get to 7 by
avoiding 8 &amp;amp; 9 so we replace the parent
of 7 by 2 and this edge from 7 to 4
tells us we didn't get the 4 avoiding 3
so we replace the parent of 4 by 1 which
is the nearest common ancestor of 4 &amp;amp; 7
so by 3 update steps judiciously chosen
you can in fact get the correct
Dominator tree but of course you don't
know a priori what order to look at the
arcs and you have to process all the
arcs at least once and the question is
how many times do you have to go through
the arcs and is there some good order to
do the the processing all right so
another couple of my students and I did
some extensive experiments motivated by
this simple algorithm we experimented
with the simple tree improvement
algorithm and versions of the LT
algorithm and some hybrid algorithms
that use some ideas from the LT
algorithm but simplified it a little bit
with no time bound guarantees and we
discovered that the algorithm that's
really the right one to use in practice
is the simple version of the languor
Tarzan algorithm in our later
experiments it ran much faster not much
faster than 20 30 % faster than the
sophisticated version on small to
medium-size examples the the tree
algorithm does pretty well but even on
small examples it ran slower than this
algorithm and on large examples it can
be prohibitively slow in fact so this
algorithm scales extremely well and it's
it definitely is the algorithm of choice
our conclusion was basically that the
Cooper at all team didn't do the best
possible job of implementing this old
algorithm which is a experimental
algorithms is a very important field in
it
a giant can of worms because you have to
be very careful you can change the
results of the experiments by the
details of how you implement algorithms
anyway
now here's another set of experiments
more recently done by Patwari Blair and
man getting back to disjoint set Union
they implemented kruskal's minimum
spanning tree algorithm when the only
well you have to sort the edges in
increasing order by weight so you need
some sorting step or a priority queue or
something like that but the main part of
the algorithm is the disjoint set Union
data structures so you sort the edges by
weight or you have a way to get the
edges out and increasing order by weight
say with a priority queue the general
step is you start out with no trees add
the cheapest edge if it connect to
vertices not yet connected you add it to
your growing minimum spanning tree if it
connects two vertices already connected
in the same connected component you toss
it out so that's exactly the Uniting the
way I have defined it in the disjoint
set in the Union problem and they
discovered that in their experiments
naive linking with this one pass version
of compression with splitting beats
linking by rank with either compression
method okay so now we've seen two sets
of experiments where simple algorithms
with pretty good running times or
beating sophisticated algorithms in
practice and the question is is this a
random phenomenon or can we as
theoretician say something about this
why doesn't linking by rank or it's
equivalent in the LT algorithm help in
practice ok so here we come to this idea
of datasets having structure these
experiments that
Patwari Blair and
we're doing have an interesting
characteristic essentially the naming of
the vertices is completely independent
they were experimenting on random graphs
and grass drawn from the real world and
essentially permuting the names of the
vertices doesn't make any difference
that is to say take any graph instance
with vertices 1 through n permute the
names 1 through n that graph that you
got with the renamed vertices is is
equally likely to the original graph so
what does this do does this tell us
something about these algorithms can we
analyze these algorithms under this kind
of random graph model this is the
problem that we took a look at trying to
explain these experiments using now some
theory of random algorithms all right so
what is this renaming of vertices
correspond to it corresponds to
randomized linking we've got an instance
of this disjoint set Union problem will
identify the vertices by naming them 1
through n with all permutations equally
likely when we do a link operation we
put two roots together we make the
smaller root point to the larger root I
call that randomized linking now my
claim is that in effect this is the
linking rule that was used in the set
Union experiments it was not randomized
linking it was randomized linking it was
not naive linking was not the worst case
of linking it was this randomized
linking strategy just because of the
data sets that these experiments were
done with and I think something similar
happened in the experiments with the
dominators algorithm but it's more
obscure so the rigorous claims that I'm
going to make apply to set Union and
I'll wave my hands about the dominator
algorithm ok so here's an algorithm this
is the algorithm that was getting
experimented with can we say something
about it in theory
does this behave more like worst case
linking or more like best case linking
linking by rank so we set out to try to
analyze this now it's important to note
that there are other randomized linking
strategies and an obvious one is 50/50
linking flip a coin and if it comes up
heads make X point to Y and otherwise
make white point to X this one it's easy
to whoops it's not equivalent and it's
easy to construct examples where
fifty-fifty linking performs poorly
although we don't have theory to give a
tight bound on this one at the moment
we've got experiments that show that
this is bad
no theory on the other hand this
randomized linking strategy we've got
theory to explain that it's good
in fact now in expectation because it's
a randomized algorithm it's got
amortized inverse Ackermann function
time for fine so it's got the same bound
as the sophisticated algorithms you're
getting the effect of linking by rank
for free out of what the data is what's
out of the data sets this is what has
happened and I won't go into details
about how this the analysis works but I
got a few minutes left so I'll just
mention the non-trivial extra pieces we
need to do the analysis with randomized
linking the original inverse Ackermann
function analysis uses ranks very
heavily in properties of ranks and the
fact that the number of nodes of a given
rank is exponentially small in the rank
that's the key point so all the nodes
can have rank 0 but only half the nodes
can have rank 1 only a quarter of the
nodes can have rank 2 and so on and low
rank nodes point to high rank nodes and
then you get this bootstrapping
phenomenon that goes on with the path
compression which somehow produces
inverse Ackermann function so with
randomized linking there aren't any
ranks so we have to introduce ranks into
the analysis that we want to mimic that
kind of
analysis so we define ranks on the basis
of the numbering we give the so we've
got the vertices number from 1 to n
randomly we give vertices numbered 1
through n over to rank 0 we're gonna
cease numbered n over 2 plus 1 to n 3 n
over 4 rank 1 and the next 8 let's see
half quarter 8 the next 8 get rank 2 and
so on and so forth
so we get the right distribution of
ranks we got small nodes pointing to
large nodes now everything works except
for one problem we can now have parents
of the same rank we can have a node of
rank K pointing to another node of rank
K and this breaks the analysis except
that it doesn't happen very much there's
a negative binomial distribution so if
you have rank K the probability that
your parent also has rank K is at most
1/2 and the probability that that nodes
rank is also K is at most 1/2 so the
number of consecutive vertices you can
have of the same rank in expectation is
at most two so you don't have these long
strings of same rank nodes in order to
do that you have to do some additional
technical analysis that involves
converting a arbitrary sequence of links
into a special sequence where you just
add a note at a time and show some kind
of equivalence to the general case so
those are the ideas that we used in the
analysis and once you can handle the
same rank case and prove that it doesn't
happen too often
you can then call upon the old analysis
to get the inverse Ackermann function
bound all right we also went back and we
looked at the dominators problem to ask
whether it's possible to get rid of this
path finding on trees and just use the
vanilla district
set Union data structure itself and it
turns out that there is a way to do it
and it gives equivalently fast
algorithms we've done some theoretical
work in some recent experimental work
that's just coming out this year and we
also found a way to produce
self-certification of a Dominator
algorithm that is if you add a little
bit of extra computation you can compute
a little bit more information so that if
I hand you the tree plus the extra
information is very straightforward to
verify that you have the correct tree so
if you're suspicious of your algorithm
you can check the output with a simple
algorithm I think this is important and
this problem all by itself turned out to
be rather complicated although the
solution that we came up with in the end
is fairly straightforward ok so what are
my conclusions my conclusions are that
there's a rich interaction between
theory and practice
don't get stuck keep going around the
cycle and look at the structure of data
sets as well as the structure of
algorithms worst case for all possible
instances is maybe to course a measure
of what is useful in practice and
another important thing here is I've
talked about a couple of problems that I
and other people have worked on now for
a number of decades classic problems
important problems still have rich
secrets to yield and I haven't even
talked about for example concurrency and
some of these ideas that were in the
preceding talk which also have a serious
role to play here so let me thank you
very much and I'll be happy to answer
questions if you have some
you assumed some sort of random random
partition of the random stream oh do you
get anything well that's something we
want to look at I haven't had a chance
to do it yet but these results on
disjoint set Union strongly suggests
that with some of these graph algorithms
you could exploit the randomization
that's built into the data structure I
think it's something interesting to take
a look at with disjoint set Union
it's a beautiful problem it's more
complicated there are poly log time
algorithms I can send you some
references if you want yes something for
for the difference between what you saw
in practice and what you expected from
theory did you investigate the
possibility that the cost model that you
used in theory wasn't really accurate
with respect to what was used in
practice so that's a very important
question obviously that well there are
many ways to answer this question in
order to there's been a lot of work on
IO model and memory hierarchies and so
on which i think is very important it's
complicated enough to look at simple
models now I don't have a good answer
for you I think you've got to try to
look at the most realistic model that
you can but don't over complicate things
because for example memory hierarchies
change over time we're getting larger
and larger flat memories so some of this
early work comes back into play again I
think so I found enough rich
in the simplest possible memory models
but is important to look at memory
hierarchies thought from the
randomization of the note labels that
you can induce did you consider the fact
that the trees you are the graphs you
are considering we're themselves coming
from some particular distribution no
simple answer is no the answer is we
wanted to make minimalist assumptions
and since you can prove good behavior
just with this simple assumption which
seems to be true in the data sets I'm
happy I think you probabilistic models
are very dangerous if you have control
over the model that is you're running a
randomized algorithm or it's sort of
obvious in the data like certain sorting
situations you can assume a random
permutation maybe but you need to be
very careful doing that I think so I
prefer to make minimal assumptions about
the data sets other than the sort of
obvious respect to graph isomorphism so
if you trying to compute a result
doesn't respect graph isomorphism your
randomization argument yes yes which
gives it this or there's a property of a
problem that you're trying to solve the
problem you're trying to solve needs to
be invariant under isomorphism that's
true yes
for the joint Union problem
does it change completely if you
actually want to be able to remove nodes
so I think it's a bit more complicated
if you want to add things to the set
it says you run again but if you want to
remove things from the set of notes does
it just become a completely different
problem well good removing notes you're
also removing edges you're removing all
the edges incident on the note so that's
the same as the dynamic connectivity
problem where you can add and delete
edges essentially and that's the one for
which there are beautiful poly log time
algorithms I think the best bound
there's a deterministic algorithm which
is maybe log squared per find and a
randomized algorithm which is maybe log
in log log n per find by Mikkel Thorpe
it's a beautiful problem and it's not
well understood but there are strong
results there yeah generally speaking
undirected graph problems are much
easier than directed graph problems and
this becomes more so when you get into
dynamic graph problems and incremental
problems are easier than deletion
problems for example if you look at a
simpler problem for directed graphs
keeping track of strong components
strongly connected components there are
the the static problem you can solve in
linear time the incremental problem
they're the best algorithms depend upon
the graph density and it's roughly
square root of the number of edges time
per edge to do finds and there are
results on the dynamic problem where you
can both add and delete edges but
they're not very satisfying and I would
say again I can send you references if
you're interested in specifics</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>