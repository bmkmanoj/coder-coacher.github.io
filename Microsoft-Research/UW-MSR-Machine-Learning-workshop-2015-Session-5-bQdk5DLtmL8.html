<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UW - MSR Machine Learning workshop 2015 - Session 5 | Coder Coacher - Coaching Coders</title><meta content="UW - MSR Machine Learning workshop 2015 - Session 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>UW - MSR Machine Learning workshop 2015 - Session 5</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bQdk5DLtmL8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm
the first speaker is Thomas Joseph both
and with the topic constructive
discrepancy minimization for complex
it's ok thanks a lot yeah I I guess I'm
a little bit of an outsider here with
respect to the ml community I hope that
there there's still some not completely
uninteresting stuff for you in the talk
ok good I want to talk about what's
called discrepancy theory and it's it's
actually it's a classical subfield of
discrete mathematics or community oryx
and the setting is as follows you are
you have some set system over a finite
set of points actually not point let's
call them elements and what you want to
do is you want to find the coloring of
the elements with let's say red and blue
and what you want to do is you want to
find a balance coloring so you want if
possible that for every set you color
roughly half of the elements red and
roughly half of the elements blue and
the the maximum difference between the
red and the blue elements in one of the
sets let's call the discrepancy of the
coloring so if you look at this example
then you see that we have a set where we
colored both of the elements red so that
the discrepancy of this coloring would
be too and this is the best possible in
this little example and then the the
discrepancy of the whole set system is
actually defined as the discrepancy of
the best coloring there are some some
nice classical results like the theorem
of Spencer which says that if you have
an arbitrary set system of n sets over N
elements then there's always a coloring
where the difference between the red and
the blue elements in any set is at most
a constant times Rudin ah this is great
in the sense that if you just take take
a random coloring then the random
coloring would be a root log in fact or
worse so what actually has to do quite a
lot of work to get this this bound
another nice result that kind of is a
little bit surprising as the theorem of
back-end fiala that even a little older
and it says that if you have the special
property that every element is in at
most t of the sads then you can even
bound the discrepancy only linearly in
that so good frequency t so if you look
at the method that is used in particular
in in spencer's theorem then this is the
this is the somewhat standard technique
it's called the partial coloring method
and the idea is that we are relaxing our
goal a little bit and instead of finding
a coloring for all of the elements we
are happy if we can color a constant
fraction of the elements let's say half
of the elements I mathematically would
say that we're kind of allowing to use
the color 04 let's say half of the
elements and it turns out that this
actually makes the whole thing a little
bit easier so this gives us more
flexibility if you look into into the
proof of Spencer then what he actually
does is the following he doesn't
immediately show that this is a full
coloring for your elements with rude and
discrepancy he shows that they exist
such a partial coloring so he can color
let's say half of the elements with the
root and discrepancy and then what you
do is that you take the half of the
elements that you colored you throw them
away so you forget about them and then
you repeat with the remaining half and
the trick is that now you have a smaller
instance so the discrepancy guarantee
that you get for the partial coloring in
the second iteration is a little bit
better than the one that you had in the
first iteration and if you continue this
game then you can actually see that the
discrepancy that you get over the
iterations it gets smaller and smaller
and smaller and you get a nicely
convergent some so overall you're only
paying a constant times that original
route and discrepancy to color all of
the elements good
um let's let's have another look into
the proof of expenses theorem and let's
see what what what is actually doing and
let's let's take a more convex geometry
perspective ah you can imagine that this
is the hypercube in n dimensions and
each of the vertices of the hypercube
shows you one of the potential colorings
that you could choose to use for example
the point in the in the right upper
corner 11 that that corresponds to the
coloring where you would color let's say
both of you two elements with blue ok
now in this geometric setting how does
the set of the set of good colorings
look like well if you have n and set and
n elements then you can actually write
down some linear constraints and what
you actually would want to write down is
that for every set if you sum over the
elements then you're getting a value
that that's between minus some constant
times rude and and plus some constant
times rudin so actually the set of good
colorings in a fractional sense it's
defined by the intersection of n let's
condemn strips right so for every for
every said we get one strip so the whole
set of good colorings is the
intersection of n of those strips ok but
but how does this perspective actually
help us to find the coloring well you
could know ok so let's make some
observations so first of all what we
really would like to do is we really
would like to find one of the vertices
from the hypercube in that set K because
we know that the vertex corresponds to
coloring and the set K corresponds to
the point of good colorings or if we
relaxed all setting and we're happy with
the partial coloring then we would be
happy with a point that doesn't need to
be a vertex but it should be sued should
lie in some lower dimensional phase of
the hypercube so it should be a point
that has a lot of
coordinates either plus 1 or minus 1
okay now let's do a little bit of math
and let's try to use the fact that this
set K is the intersection of strips and
each of the strip's has a width of let's
say 100 to each direction what this
actually means is the following it has
the implication that if you if you count
what is the Gaussian measure of the set
K so Gaussian measure is the following
you take a random guy ocean with with
the standard deviation with variance 1
in every direction so you take a
standard Gaussian and you you check
what's the probability that the standard
couch ends up in k that's the Gaussian
measure gamma and ok and this gum a
Gaussian measure its lower bounded by
the product of the Gaussian measure of
all my end our strips I'll explain a
little later why this is the case but if
you have a strip it's easy to calculate
what's the Gaussian measure it should be
some constant that's very close to one
so you get some more about that is
exponentially small but it is violently
exponentially small and here's the trick
that any convex symmetric said that has
a gaucho measure that is that large will
definitely have a point in it that has a
lot of plus minus 1 coordinates I will
not show you that argument essentially
the argument would be that you take
those two to the end many hyper hyper
cube vertices and you put translates of
K you place translates of K it at each
of those hypercube vertices and then you
argue that well you must have a lot of
overlap going on and then you take some
of the some of the overlapping sets and
then you take the difference vectors
that will actually give you those
partial colorings what I want to give
you is actually an algorithm to do the
same more or less okay ah good I have
actually implicitly used on the last
slide nice and a nice lemma by she
talking
tree which goes as follows suppose you
have two sets k and s and what we what
you know is what's the Gaussian measure
of both of them and what you want to
estimate is what is what is the Gaussian
measure of the intersection of k and s
and the good news is there's an easy
lower bound you just take the product of
ka the goucher measure of k and s and
that gives you a lower bound for the
gaussian measure of the intersection the
assumptions that we're having here is
that one set K is convex and symmetric
and the other said it should actually be
a bit strip this is conjecture to work
for any convex symmetric sets but it's
only proven if one of the sets is
actually a strip okay good in other
words this is a very nice tool to get it
to get a lower bound on the intersection
of strips with with convex sets okay
good so let's compare a little bit what
is what is known on partial coloring
approaches there was the theorem of
Spencer with some kind of extensions by
glasken and journalist and all those
approaches they all based on a color on
an accounting argument we essentially
you use the pigeonhole principle with
exponentially many pigeons and pigeon
holes and the problem is that this does
not give you an algorithm ah but we
would really like to have an algorithm
but already for the original setting of
Spencer's theorem it took about 25 years
until somebody Nikhil van zile came up
with an algorithm that was a little bit
complicated it was based on iterative
iteratively solving a semi different
program and actually it was very custom
tailored to Spencer setting um but
fortunately two years later there was a
more general algorithm by la vedette
mika which works ah which is a very
elegant simple Brownian ocean and it
works in a much more general setting so
it actually works whenever you have a
pony to whenever k is a polytope
which which satisfies the property that
it's large enough so if you look at this
this quantity then here then it's may be
hard to digest what this means but if
you plug plug in this condition into the
lemma of shidduch and country then what
you get is that the the Gaussian measure
of this pulley to K is so large that
also the counting method would actually
give you the existence of a partial
coloring so whenever you have such a
pulley tube you can actually have an
algorithm to find those partial
colorings however the disadvantage is
that maybe you have some convex at k the
measure is large enough but you couldn't
use just that simple lower bound of
shiitake and country to to ensure that
and this is the main theorem the main
result that I'm going to shoot here it's
that actually you can get the the most
general case so this is the main result
that I want to show here and it goes as
follows you take any convex or matrix at
K so like like this at K and it's
possible to find a vector Y like like
this one which is in the intersection of
K and my n dimensional hypercube and the
property of the vector Y is that that
that a linear number of of coordinates
will be either plus or minus one and the
condition that I need for my set K is
that the Gaussian measure is at least
mildly exponentially large okay so this
is essentially the existence theorem and
now there's an algorithm for it we can
all we can do all the stuff and putting
them a time and here's the whole
algorithm step one is you take a random
Gaussian let's call it X star okay if
you take a random Gaussian then it's
extremely unlikely that this random
Gaussian will be in K or will be in the
hypercube so this this point is
definitely not going to do the job
but here's the trick you compute the
point y star that lies in the
intersection of k and the hypercube and
you take that point y star that is
closest to x star in terms of the the
euclidean distance ok so by definition y
star is in k intersect at the hypercube
but the claim is the claim of the
theorem is that actually this point y
star has the property with very high
probability that a linear number of
entries will be either plus or minus one
okay so good so essentially alone and
alone and Spencer in their famous book
that they actually conjecture for a very
long time that the thought this whole
partial coloring approach would be
impossible to make it to make it
algorithmic so it is actually surprising
that such a simple album actually can
give you this thing this partial
coloring okay so from a more abstract
perspective you what you would say is
that you you're kind of looking at a
random extreme point of X intersected
with a hypercube and the random extreme
point is actually going to have the
property that we need ok and I not only
there is the algorithm symbol also the
proof is actually simple so i hope that
i can go through the whole proof ok any
questions so far ok good thanks ok
there's actually there's one hammer
there's one hammer that we need for the
proof and that's the so-called
isoparametric inequality for for the
gaussian space and the ideas that is as
follows you have any set k and you we
want to consider another set k delta
which is all the points in k plus those
that are a distance at most Delta 2 K so
in other words we take a set K and we
make it a little larger so with we throw
in any point
of distance at most telecom and we're
wondering how does actually how does
actually the Goucher measure grow with
Delta and here's the here's the claim
the claim is the following even if you
have a set k where the Gaussian measure
is exponentially small you need to add
only points of this small constant times
root N and then the set will cover
almost all of the Gaussian measure so
this is essentially the fact that
Gaussian measure is most of the Gaussian
measure is not further away than a small
continent through them from each other
and if you where's the isoparametric
inequality well the Gaussian
isoparametric any quality actually tells
you that if you want to show such a
claim then the worst case are precisely
half spaces so the actually suffices to
look at the one-dimensional case so you
can imagine that you have a
one-dimensional interval that goes from
minus infinity to some point and that
has a cow should measure exponentially
small and you know just need to estimate
by how much do you need to shift the
boundary of that that interval until the
Gaussian measure is almost everything
and then you do one line calculation and
then you come up that well actually you
need to shift you need to shift the
boundary only by essentially the root of
that exponent that you have okay good so
far we learn the one the one thing that
we need which is that Gaussian measure
is not far from each other okay and yeah
now we have actually everything for the
for the for the analysis which fits on
one slide and here's our how the proof
goes okay so what did we do we did take
a random Gaussian X star and then we
calculated what is the closest point y
star there lies in k intersected the
hypercube so first of all what what what
do we know about two minutes
okay so I have to speed up a little bit
okay okay so we do know a couple of
things we know that the distance has to
be at least a fixed constant times Rudin
already because the distance of a random
garbage into the to the hypercube is
that large okay on the other hand we
learned that that XR would be close to
any set that's large enough in terms of
Gaussian measure okay so the only thing
we we need to do is to to find in this
picture is said that's large enough in
terms of calcium measure and and here's
the trick that actually what we are
solving is a convex optimization problem
and now for convex optimization problem
you can take constraints that are not
tired and you can throw them away and
that's not changing the value of your
convex optimization problem and that's
what we're doing so you look at your
point y star you throw away the
hypercube constraints that were not tied
you get a bigger set and now you can
estimate that this bigger said must have
a large enough Gaussian measure as it
has a large enough Gaussian measure from
the isoparametric inequality we know
that the random Gaussian point would be
close to it and that's a contradiction
because it would be closer than this one
fifth time through that okay ah good so
these are essentially all the arguments
we would probably need two or three
minutes more to digest this properly but
that that's it essentially okay good um
okay then let me let then let me close
with with some remarks that actually the
whole this whole discrepancy theory it's
not just an obscure area of discrete
math it's actually very useful and the
best approximation algorithms for
example for bin packing they actually
use those type of arguments
okay so let me close with my favorite
open problem which is aa which is the
following that if you are in this
so-called pack viala setting so you have
a set found only where every element
isn't at most tea sets can you get me
any better bound than just to tea on the
discrepancy thank you any question
is is big fat pyara constructive yes
immediately from yeah immediately from
the proof it's essentially you're
solving a linear program you're throwing
away constraints then you resolve the
linear program and so on John will give
me an exponential number of faces on
your polytope and then and then finding
that projection will be exponential oh
the number of phases is exponential but
your your central only solving a convex
optimization problem so k is convex the
hypercube is convex so notify trying to
find the closest point to some convex
set and that's essentially a convex
optimization problem okay okay you need
to be able to like solve the member like
separation problem for k okay that's
sure yeah which is not always possible
right ah in poly time okay okay that
that's an assumption okay well if you
don't ok then okay what what are you
doing
any other questions in terms of the
growth of K you have a bound on the
probability upper bound of 1 minus the
the thing why is it not it seems to be
the case that it was just the ball was D
dependent but the actual bound was not
no longer d dependent if you go back to
that slide ah I think so there's no
dependence on the dimension then I'm not
the dimension D this the metric D D of X
K Y is why is it the case that in this
lemma the right hand side the
implication is not dependent on d the
way in which your measure was this or
you just did maybe i missed it is this
Euclidean ah d ok actually this is ok it
would actually be independent from the
dimension this like minus some exponent
this is essentially three times the root
of the exponent and this is again mine
is that that same exponent not talking
about the dimension of the ambient
dimension tell me more about defects
comma K the distance in essence is why
is it no sorry the big Delta you would
choose it as being this three root delta
and that would be your your you you big
delta in this case you bleeding you
kisses cheek Lydia yes this question so
let's thank the speaker again
so our next speaker is Sebastian hot
topic 4ms r and title of his talk is
optimal discovery with populistic expert
advice thanks ok so I'm going to tell
you about program I worked on two years
ago so I will try to describe to you an
algorithm to that does something the
problem is that i don't know exactly
what are the good applications for this
algorithm but I things are actually many
and I will suggest two of them but I
things amo so essentially what the
program i'm i'm looking at is the
following i'm searching for something
and I have a few heuristics to do this
search and I want to combine those yer
istics so imagine for instance that I'm
trying to debug some code ok and I have
many heuristics to our server heuristics
to find inputs that will likely make my
code fail and what I want is to combine
these heuristics in a smart way that
will be one one of the two application
that I sort of it so here is a problem
slightly more formally so we have X
which is a countable set so think of X
as the the potential input to your bosom
and we have a subset a of interesting
items ok so what are these interesting
items these are the inputs that will
make your algorithm fail that will make
your algorithm bug now what will you
have is that you have access to X to the
input that you can give to the algorithm
through experts these experts so these
are the heuristics that you have and
they corresponds to probability
distribution over you set X ok so using
your Yuri stick you can generate at
random inputs to your algorithm and the
goal is to find as rapidly as possible
many elements of a ok so you just want
to use this realistic in a certain way
to rapidly find all the inputs that will
make your algorithm
bug and then decode debug it okay so
let's again do it one more time slightly
more formally so we have K experts okay
this this is known to us these other k
heuristics what is unknown to us is a
set let's say what is unknown is a set
of of potential input X the set of
interesting items is of course unknown
and the distribution is corresponding to
the experts are also unknown and what we
do is that we play sequencer games that
goes as follows so as T goes on at each
time step what I'm going to do that I'm
going to select one of those k
heuristics okay so i select one of those
k experts so let's say I select I sub T
at time T and then what the expert of
the heuristic does is that it generates
at random an item so we will use the
terminology item but you can think in
terms of input so it will generate a
night my tea in X at random from the
underlying probability distribution
which is new IT ok and this drawing is
independent of everything else now what
we we have this input so if you're
thinking if you think in terms of the
application to debugging an algorithm
then you test this input and you see if
the algorithm works so it doesn't work
ok so you learn whether YT is in a or
not whether YT is an interesting item
and what is your goal your goal is to
again find rapidly as many elements of a
as possible so what you do is you you
you have observed y 1 up to ym these are
the inputs that you have tried and you
look at the intersection of this with a
so that the number of the end you look
at the cardinality of this so that's the
number of interesting items that you
have found ok so of course if you if you
find two times the same interesting
items it was only interesting the first
time ok you don't care about finding it
twice ok so is the problem statement
clear so what I'm going to give to you
in this talk is is an algorithm that
solves this problem and that does really
well in practice and which is super fast
and could be useful but actually so let
me give you maybe another
tential application for this framework
just to give you an idea of where I
think it could be used this was actually
the original motivation this is a
problem of security analysis in power
system so imagine you have you have the
grid okay you had a power system so this
is a big network with many nodes and
what you want to do is you want to find
the 14 nodes in your network again what
you can do is you can actually go check
if you want to check is a note these 40
you have to actually go there check it
and then you can fix it may be potential
ok so this network is used and you
cannot go check every single node but
what you have is that you have a venture
so you have this network and you have 40
nodes in it so these are the important
notes so you can go check every node but
you have a bunch of engineers working
for you and they are smart and they came
up with different heuristics to explore
this network so engineer one is a
heuristic is let's say supported on this
subset of the network engineer to prefer
that one three etc for so you have these
different heuristics and what you can do
is that if you go to engineer one and
you ask him okay what should be the next
note that we check it will it will do
something which we model as it draws at
random realization from its owner I
underlying distribution and you will
give you a node and then you can go
check and I fix it if it's a 40 node ok
so that's that's another potential
application which is actually a real
application I didn't follow up with the
guys before my I did that but I think
they actually used it ok so let's try to
think about how to solve the problem it
will be also reasonably interesting
mathematically so here is what we want
to do imagine that we knew the
probability distribution new one up to
new key what could we do here is a very
simple strategy which sounds like a
pretty good idea so we have already
observed y 1 up to whitey these are the
items that we tried so now let's say we
know everything ok so we also know the
set of interesting items so the set of
interesting items that is left to be
discovered is
a minus the set y 1 up to whitey and now
for each expert I can look at what is
the probability that this expert will
give me a like an interesting items that
I have not seen yet okay so this is this
new I ok the probability of the set a
minus the set y 1 up to watch is that
the probability that experts i will give
me an interesting item that i have not
seen yet and what i can do is that I can
maybe I it smart to just query the
expert which maximizes the probability
of giving me an item which is
interesting and that I have not seen yet
that seems reasonable of course it's not
an algorithm I cannot do that but if I
could do that so first of all let me
tell you that this quantity a new I of s
at minus y1 up to yts is known as the
missing mass of interesting items for
expert I so what we proved ok I call it
a serum but it's more like a proposition
is something very simple under the
assumption that the experts have known
interesting non intersecting supports
meaning that if i look at the support on
the interesting items they are non
intersecting ok so this is a very strong
assumption which is not realized in
practice but the two theorems that i
will tell you hold only under this
assumption ok but i will show you
experiments where you will see that it
works even without it but ok if we have
this assumption then this strategy this
greedy strategy is actually optimal so
precisely this strategy of selecting at
each time step the expert that would
maximize the priority of giving me a new
interesting item that i have not seen
yet this strategy maximizes at any point
the expected number of interesting items
that i have found so far because this is
the optimal strategy if the assumption
is violated then this is not true it's a
much more complicated problem and the
greedy strategy will not solve it ok you
need to you have a big mdp basically to
solve and its most likely intractable ok
good but now more interestingly what we
want is a data-driven strategy ok we
want something that doesn't require to
know the probability distribution new
one or two
okay and that doesn't need to know the
settee etc so what we need is to find an
estimator for this mit the missing mass
and this is called the good during
estimator okay this is something very
well known so you know cheering during
the Second World War it was a at
Bletchley Park and he was trying to
crack the Enigma code and one question
that he was asking with his intern good
is a following look we have seen that
many code words for the Enigma machine
how many code words have we not seen yet
so the question is given what we have
seen so far how can we estimate what we
have not seen yet and this is a good
during problem and good and cheering
came up with with an estimator which is
really beautiful and really simple and
if you think about I mean if you that's
what anyone would come up with if you
think about this problem so here it is a
formerly so let's say that we have a
sequence x 1 up to xn I ID from some
probability distribution news reported
on X i'm interested in the random
variable M sub n which is the missing
mass on a so it's a probability that is
left on a once I remove the points x 1
up to xn because this is a missing mass
on a this is a random verbal and I want
to estimate it and here is the
estimators a good during estimator MN
hat so what I I look is the following so
so let's look at this indicator so for
any point X in a what I look at is
whether X appears exactly once in my
data set if if it appears exactly once
then I count it if it appears twice or
if it doesn't appear that I don't count
it okay and I look at the frequency okay
/ this one when i look at the frequency
of items that appears exactly once in my
data set and this is called a good
during estimate oh ok it's as simple as
that so this thing an element that
appears exactly once in my sequence is
called a packs in linguistic and here is
the serum ok so i should cite also good
and cheering 53 but this is a serum that
that will tell you the properties that
we need for the girls
concentration properties of this
estimator so the first thing is that
this is a good estimator so this was
proved by good and cheering in the sense
that it's unbiased meaning that if i
look at the expectation of MN minus MN
hat okay both are random babbles but if
i look at the expectation of the
difference then this is small it's
smaller than 1 over N okay so this is a
quite deep I mean it's not at all
obvious that you you you can have that
it's rather simple to prove it's
essentially four lines I mean it's for
good lines but it's it's only four lines
and then okay mcallister and ang appear
and the McAllister and Ortiz they prove
the following concentration done so with
probability at least one minus Delta you
can actually show that MN is sandwiched
between MN hat plus minus something that
looks like square root log 1 over delta
n so if you know about half dings bound
I mean this is exactly what you get in
in healthy but the situation not is much
more complicated okay we are not summing
just iid things so let's win just for a
minute how you would prove that it's
actually very very simple so we know by
the first thing that I said that in
expectation the difference is small now
I'm going to tell you that MN
concentrates around its expectation and
MN hat also concentrates okay so that
we'd implies this thing so y MN hat
concentrates around its expectation it's
just makya minik inequality look at MN
hats if i change one of the value in my
data one of the XE it doesn't change
much for MN hat okay so there is this
stability property which implies
concentration immediately so immediately
we know that this the last equation is
true if i replace em n by the
expectation of a man hat okay now what
about MN and then it's much more
complicated so if I had a bad day I
would write what would I write I would
write that M is sum over all the element
X in a of what of the new of 80 s new of
X ok so probability of x times the
indicator that X is not in my data so
indicator that X is not in my data this
is a binary distribution but now this
barrel is this different boundaries of
dependent ok so I cannot just apply
holding it in equality but the trick is
that they are negatively correlated so
if I know that one is one then the other
ones are more likely to be zero ok and
we know that have dings inequality and
more importantly in this scale down
Stein inequality generalizes immediately
to negative correlation ok so again it's
just a two lines and you get this serum
which if you think about it if you were
trying to you know if we didn't know
about Magda a middle and down Stein and
all this stuff it's amazing that you
have such a concentration property it's
really non-trivial ok good but we have
that so now what I'm going to do is
going to be very simple I have this
estimator for my missing mass for each
expert so what i could do is instead of
picking the one which maximizes the
probability i pick the one which
maximizes the estimated probability this
would be terrible ok because if I do
that then I mean I could estimate you
know just the first time I a pic expert
one I pick it two times and it just by
like a lack of luck I just got two times
the same interesting item so now I
believe that the missing mass is 0 ok so
it will take me a very very long time to
come back to this expert oh actually I
will never come back to this Expo
because now I have estimated the missing
must be 0 so I need to be careful and
boosted by some confidence because of
all those of you who know our bandage
this is an idea that has been around
since 85 since ryan robbins you don't
just trust your estimate you boost it by
confidence but so that's what we are
going to do now we have a confidence
interval given by the serum so we
designed this algorithm which we call
good UCB you see B stands for upper
confidence bound so we replace the
unknown missing mass MIT by its
estimator and we boosted by a confidence
interval so now the algorithm is to
sample to play okay to play to follow
the expert that maximizes the estimator
the good during estimator for the
missing mass of expert i plus this
confidence term okay and where TI t here
is a number of
that I tried expert I now I just want to
give you one actual serum and something
non trivial so let's do it so how do we
assess the performance of good UCB so
one thing that we could do is to look at
how many items it found at some time T
and we want to compare this to how many
items found by the optimal strategy okay
so that's what we want to do note that
we don't what we want to show you that
this difference goes to zero because
eventually any strategy we find all the
interesting items so it's not like in a
standard bandit setting where you'll
find with a regret that grows like
Square t here we want something a regret
accumulative regret that actually goes
to 0 okay so this is much more tricky
and the reason is that there is a sort
of restoring property meaning that if
you have an algorithm that takes a bad
decision at a given time step then it
will have more good options later on so
if I if I do badly now then in the
future I can do better because I didn't
find the items before and I need to
exploit this property in the analysis so
that makes things a little bit trickier
so let me just tell you the serum so I
define T of lambda to be this it's the
first time T at which the missing mass
of all the expert is smaller than lambda
and T style of lambda is the same time
but for the optimal strategy and now we
prove what we call a nonlinear regret
bound so we show that again under this
assumption of non intersecting support
so let s be larger than 1 think of s
let's say as 10 something like that then
with high probability we've probably
something like point 99 for any lambda
okay this is the key it's uniform in
lambda so you need for me at all scales
the time good UCB needs to lower all the
missing mass to lambda is of the same
order at the time it takes for the
optimal strategy to lower everything to
lambda minus a little bit ok so when
when I have discovered when I have put
everybody down to lambda i'm lagging a
little bit behind the optimal guy
everybody down to lambda minus 1.1
something like that ok so this theorem
is just telling you that you lag a
little bit behind the optimal strategy
so let's jump into the experiment and
you will see that it's actually doing
something so this is so for experiments
it's a following setting so the number
the total number of items the total
number of inputs is in the first 1 128
in the second one 500 third one 1000
force 1 10000 ok so I augment the size
of the input space now I have seven
expert 7 nearest exciting is interesting
the first expert as a probability the
first time you try it a 51-percent to
give me an interesting item ok so 50
basically if I have 100 item 51 of them
I mean the expert one puts mass on 51 of
them you see expert 225 etc now i tried
three different strategies one of them
is just a round robin ok so i have seven
new ristic so i try your wrist acquire
the new istic to etc ok so I just run
Robin Susan one of them is the Oracle
strategy z absolute optimal strategy
which we can compute in this case and
the third one is good you should be our
algorithm so so the top one so what I'm
plotting is as a function of time the
number of interesting item found ok so
the higher the better the top one is of
course she is a G Oracle ok the optimal
strategy the second one is our algorithm
VD CP and the third one is uniform
sampling and you see as I augment the
size of the space actually good UCB
becomes as good as your record it's not
even lagging is doing as well as if I
knew everything but it knows nothing so
yeah it's doing well so we call this
property this you see there is some kind
of limit we call this a macroscopic
limit and ok if I had more time I would
tell you what it means exactly but it's
a simple corollary of the nonlinear
regret bonds that I just showed you
before and now to conclude I want to
show to you a completely artificial
example
where the supports are intersecting and
having a series that explains this would
be very interesting but we don't have it
so here we have how many we have five
experts so the set of points are the set
of positive integers such a set of
points and the set of interesting points
are the the prime numbers and each
heuristic is just a geometric
distribution on the on the integers okay
with different mean okay the first one
has min 100 second one 300 etc and we
run good UCB and again we we compare and
we ran with different value of this
constant C and if you take see a little
bit smaller it's more aggressive so as
always in practice the more aggressive
you are the better things go and and you
see that it's doing actually really well
like it's almost it's lagging a little
bit bit behind Ozzie Oracle but it's
doing almost as well so these are the
references so the paper appeared in 2013
and you can also check out the survey
that I wrote on bandit which gives you
some intuition about where these ideas
come from thank you questions
four or the last experiment yes how did
you compute the Oracle strategy okay
excellent question so this is the Oracle
greedy strategy it's not the true I mean
it's what I locate it's what I call the
record in the talk but it's not the true
optimum I think you could probably do it
in this in this example but we didn't it
yeah it would be actually quite
interesting to see how big is a gap is
that I don't know good question any
other have you thought about what kind
of pathological cases might look like in
the case of overlapping experts yes yes
this is sweet so so things can go really
bad and I mean the simplest example that
we found is isn't it you need like with
three for instance with three experts I
could not find an example where where
the two would be different where the
greedy would be non optimal so we needed
to go to for expert and then if you go
to two large numbers and you can show
that really as the number of expert
growth the gap between greedy and on
greedy can also grow its unbounded yes
very reminiscent of set cover and huh
yeah and the nun and the non
intersecting right expert seems like set
cover where the sets don't intersect
which you can solve exactly that's up 30
and he looked at I mean there are many
approximations and set covers
approximately yes which would then relax
the requirement to have non intersecting
experts and I was wondering if that
could sort of compose with the other
approximation because in some sense it's
sort of a stochastic non intersecting
set cover which is absolutely absolutely
so I did not but I'm really hoping
someone will do
yeah I think this is exactly the way to
go all right let's take the speaker game
hi everybody so today I'll be talking on
a class of distributions defined we r
sub modular functions which we shall
call these submerged the point processes
and this is joint work with Jeff films
so basically we introduce a new class of
distributions which we call sub modular
point processes or spps and these are
distributions directly proportional to a
submersion of function and another class
of distributions which we shall kind of
relate to a lot is the class of log
sub-module distributions which we call
log spps and they are basically
proportional to the EXP or the exponent
of a submodular function and in this
talk i shall try to kind of compare
theoretical properties and results of
both of these distributions so some
motivations for both these class of
distributions essentially submodular
functions have very naturally been used
for a number of applications a number of
real world applications in the context
where one would want to define these
distributions with a submodular function
they are natural in applications where
one wants to do some like diversity and
coverage so sensor placement
summarization or diversified search our
natural applications where one one would
want to use these these distributions
with a submerged a function you can also
define these distributions with a super
modular function and and kind of natural
applications of these would be where one
wants to kind of model notions of
attraction or cooperation and
applications are like image segmentation
clustering etcetera so a question is
that since sub modular functions have
very naturally been used in many of
these
applications and in many cases certain
times these models have been
probabilistic a kind of natural question
then is can we define a general class of
probabilistic distributions which can be
defined via general submodular functions
and how hard is probably take inference
with these with these kind of
distributions and what kind of
guarantees can you give for
probabilistic inference so it's well
known that sub modular functions can
very easily do things like map inference
but probabilistic inference was kind of
open question so the first result is we
show that that for gender class of sub
module function is really hard to
compute these so basically it's it's
kind of this is indeed is in fact
independent of P is equal to NP so you
can kind of give information theoretic
proofs that computing the normalization
functions for both SPPs and log SPPs is
is hard and one can kind of give lower
bounds of order log n and order n
respectively on the other hand we can
also show approximation guarantee is
that for spps you can actually compute
an approximation which is order log in
and it was shown if in the context of
log spps bye-bye jalanga and crosser
that you can actually do get an order n
approximation and the idea is to use
seven Super gradients of a submodular
function so so basically though these
properties are known a number of sub
classes of sub modular functions
actually admit exact probabilistic
inference for both SPPs and log SPPs and
a kind of interesting open question is
what are the other such distributions
that can be characterized that admit
efficient distributions and finally I
would like to kind of compliment the two
so s VPS and log spp is both model
diversity cooperation map influences
efficient and probabilistic inference
can be done either approximately or or
or exactly in some special cases but
the complimentary teas are that SPPs
have kind of low dynamic range whereas
log spp through the EXP have a high
dynamic range so it's very interesting
that spp is actually our kind of closed
under some so they are very natural in
modeling mixtures whereas log spps are
closed under products so they are useful
as as priors so an application of this
kind of theory would be to give
alternate directions in learning
mixtures or other compositions of
submodular functions so thanks a lot and
if you have any questions you can come
to my poster so we are done with first
half of the event today and let's thank
all the speakers that did a great job
here today and we're going to start the
the poster session now the poster
session is going to be in the McKinley
room which is just the other side of the
building and we're also going to open
the pose now so we have awards for the
best best poster so make sure to vote
for your best poster note that we had
more than 20 posters so not all of them
had a chance to present their work as a
spotlight so make sure to check the
other posters as well there are as good
at the one that were presented over here
so thank you and see what the McKinley
room with a reception and some food free
food again
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>