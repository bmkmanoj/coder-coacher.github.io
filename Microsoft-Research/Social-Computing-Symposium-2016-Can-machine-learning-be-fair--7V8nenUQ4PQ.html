<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Social Computing Symposium 2016: Can machine learning be fair ? | Coder Coacher - Coaching Coders</title><meta content="Social Computing Symposium 2016: Can machine learning be fair ? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Social Computing Symposium 2016: Can machine learning be fair ?</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7V8nenUQ4PQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
I just want to thank it and Microsoft
for having me here today um my name is
Suresh Franken I'm the University of
Utah and I want to talk to you today
about algorithmic fairness so I'm a
computer scientist and as a computer
scientist the thing I do most of the day
is think about algorithms these
algorithms do many things I work in all
kinds of different problems but of
course many of the algorithms that I do
look at tend to find patterns they look
for patterns in all kinds of data and
the magical thing about studying
algorithms for finding patterns were
doing machine learning is that the same
algorithm can the same collection of
algorithms can find patterns as many
different kinds of data and and it's
kind of interesting to see why and how
that works the same algorithms that you
know might tell me that I really should
be buying this new first edition
remastered all new honda suit first star
wars edition is the same algorithm that
might actually decide whether i should
get a loan for a car or not as the case
may be it's an even more scarily the
same algorithms that will be used to
decide whether I'm a good citizen as his
rumored is is going to happen in China
over the next few years we have a credit
score for a citizenship and the reason
these the same algorithms can do all
these different things is basically a
bit of magic it's a magic tie like to
call the shape of information it's a
story that goes back in some sense about
500 years to Rene Descartes the idea is
very simple that we we what Descartes
said you know many hundreds of years ago
was that if we want to describe shape we
can assign numbers to shapes and by
assign numbers to shapes we can go from
geometry the kind of geometry that you
all learned in high school and Beyond
and to algebra and so by reasoning about
numbers we can reason about shape and
what we do which I like to call the
reversed a card trick service the thing
that underpins most of data mining which
is to say we take our objects and write
them down as numbers which is something
that's really easy to do because
everything is digitally encoded and then
we create a shape out of them we put
them in a
space which gives them shape and in
doing so we now say that our algorithms
that want to learn things about us our
shape Finers they're trying to find
patterns in the space that correspond to
things we want to learn so for example
you might have a collection of people
and you're trying to build a loan
pricker should you give someone alone or
you want to see you want to predict sort
of outcomes in life as we just heard so
you have you know a bunch of points and
you might say okay let me label you know
my training data with you know loans
that were successful own and we'll call
them along there warn't successful
Kellerman read loans that were
successful will call them in blue and of
course we do what we want is a rule some
kind of guideline to say when a new
person comes in are they going to be
successful not and of course this is
very binary and you know there are much
more sophisticated ways of thinking
about this but just for now bear with me
so your algorithm will think and think
and think and it might come up with a
rule a rule that can be expressed in
this geometry that we created from the
data as say a line or a high-dimensional
line if you wish a hundred or a thousand
or a million dimensional line that's
easy to think about after a few cups of
coffees and it says okay if I take a
point and I put it on one side of this
line when a new person comes in to apply
for loan then I'm knock i'm going to do
that alone and if they aren't the other
side i will grant them alone and at some
level is essentially how all machine
learning algorithms work they construct
these rules in these in these strange
spaces using strange geometry and and
that's how they make decisions and it
looks like magic to us because it looks
like they really know something about us
but that's what they're doing and of
course this is all well and good as long
as the world looks nice and lex and nice
and clean but of course no world is ever
as clean as all this and of course you
might have something like this you might
have points that are not that you
discover do not fit on your rule so you
have some more train later you got from
a different store so you say oh no this
is a wrong training rule that I've had
and I've maybe been misclassifying all
this time and so you someone have to
adjust what your rule is doing and
that's good because now you discover
there's another point that you should
have misclassified one way but you
actually classify the wrong way now when
you change your rule you've learned a
better way of doing
and so I guess what I'm trying to say is
that algorithms make mistakes and they
make mistakes that are hard to discover
because they're working in these very
high dimensional spaces and you know
this is probably not surprising to
anyone who has small children and you
know wonders why Netflix insists they
should be watching the third season of
pokemon black or white not that this
ever happened to me but but but there
are more insidious problems with these
algorithms that are harder to detect I
mean it's one thing to make
recommendations that are laughs will be
wrong it's another thing for example and
this is the thing that you may have
heard of came out a couple of years ago
it's one thing for the software and a
camera speaking of listening or seeing
devices to decide that because you're it
had it is trying to detect helpfully it
thinks whether your eyes are open or
closed when he's taking a picture and
clearly has not been trained on the
right kind of data to make a correct
determination it's not fun when a
browser ad placement algorithm and this
is again research that was done this
year decides that whether you're male or
female should decide what kind of ads
you get get shown and the ads can have
actual well consequences are not just
you know insignificant in their
differences it's a problem when again in
you know an atlas I'm not picking on
Google this is true for many of these
sort of companies that do this kind of
thing decides that because your name has
a certain ethnic flavor to it maybe we
should ask you if you have if you need
you know a bail bondsman or if you need
to go into drug rehab or even if you
happen to be you know working at the FTC
and a professor at Harvard and the when
I went with me when we worry about
algorithms making predictions that are
wrong in profound ways the usual
response it comes and well you know an
algorithm is after all just a recipe
it's a set of instructions this is the
can't that we've been told for years and
years and I've done this myself I'm
guilty of this you know so set of
instructions you open the recipe book
you read the instructions the algorithm
follows instructions so really the
problem is that if you have a whole lot
of code that's doing something you don't
like all you have to do is go find the
piece of code that says if person race
is called not Caucasian do something bad
and now all your problems will be solved
and sadly of course this is not the
and this is not the case for a very
fundamental reason that while in many
many algorithms we do use our recipes
the algorithms that run powerpoint are
well i hope are a recipe and not using
internal machine learning machine
learning algorithms themselves are
really recipes for making recipes and
what i mean is that these algorithms
work by searching a very large and
complex landscape this million
dimensional bumpy space that i mentioned
earlier for a single answer and this and
the answer they get is the recipe that
then becomes the procedure there you
should decide make decisions so imagine
you know taking a ball and throwing it
into a 50 dimensional roulette wheel not
this nice one here but a much one any
guys it goes bouncing around fairly
randomly and it stopped somewhere at
some numbers a 23 in the album say okay
23 what algorithm does 23 represent
that's the recipe I'm going to use to
make decisions and that's all very well
till you realize that you really have no
understanding of how the training
procedure the machine learning algorithm
the recipe for making recipes came to
this process it came to this conclusion
and even if you did have any
understanding of what it did the outcome
it produces is not necessarily
interpretable algorithms are inscrutable
especially deep learning algorithms that
we've been hearing what you know if I
give you a rule that says this is the
formula that decides whether amazon
should recommend new shoes to you and
it's a little bar thing we hear
something you might be listening yes
it's a rule yes it's well defined yes
you can look at it but does it tell you
anything no it doesn't tell you why and
that why is part of the problem because
we are assuming these algorithms have
decision-making power over our lives and
we want not to just to know that they
tell us what to do but we don't know why
they're saying so where they're doing
something wrong whether they're making
mistakes in the process it's can we be
bug fixing it's very hard to debug
something like this and that brings me
to the research that I work on the area
that is it's a growing area and I you
know I should say that you know it's I'm
not the neither the first nor the only
one working on this has been around for
a while in fact one of the original
inspirations for thinking about this was
in fact the article that caped and then
avoided written some years ago on
on throughout the name six after then
six provocations for a big data thank
you yes it's a very nice article and
these because algorithms have the power
not just to decide where we eat or sleep
or watch but also decide where we go to
school very good jobs what kind of world
we see through our through our phone
sighs devices how they filter
information through it was and whether
even our freedoms can be taken away from
us and that's a wrap much very serious
thing it's important to understand how
they work and whether they're being fair
in what they do in particular new
algorithms discriminate can we detect
whether they're discriminating
especially against groups on protected
groups minority groups groups that have
been distant historically disadvantaged
when you think of algorithms being
trained on past data we know a law that
passed data is problematic are the
algorithms merely reflecting and
amplifying those biases we talked about
we heard earlier from Tim about you know
the trade-off between moderation privacy
there's a similar tension here between
privacy and fairness you know you want
people's information be private but when
your privacy can be a shield for doing
unfair things how do you balance these
issues and it's something we don't only
understand how to do can we retrain
algorithms to be fair you know if you
want to build new out we want to wear
you know new weapons that we're being
using can we actually make them fairer
by instrumenting them in certain clever
ways and fundamentally since after all
algorithms are all mathematical
constructs can we translate the notions
of fairness and social justice that have
been you know handed down to us for
generations into a language a
mathematical language if you wish that
can be programmed into these algorithms
so that they can themselves going to can
learn what it means or even know what it
means for something to be fair it's just
a final note so there's a lot of talk
about our machine overlords and you know
the general conception of our robot
overlords the worst the most scary one
is of course Hal you know the utterly
indifferent supremely powerful machine
that just swats us away like a fly and
in many ways I think you know we try to
console ourselves by thinking Northern
the Machine overloads are not like hell
they're more like Skynet to whom we are
so important that they have to hunt us
down and kill us so what you know maybe
not much of a consolation but it makes
you feel a bit more important
what we'd like of course our algorithms
to be are you know mr. Spock right there
always logical always calm always
reasonable never prone to anger but
always losing the argument to be
emotional Captain Kirk that's how I want
them to be but as I said I'm a computer
scientist I work with data I work of
machine learning and to me I think when
I deal with these algorithms that the
image that the right image that comes to
mind is really a two-year-old their
inscrutable they're temperamental their
capricious you don't want to give you a
two-year-old arrange to power nah not
really but they can be trained they can
be educated and maybe one day even given
a modicum of responsibility to make them
a grown-up algorithm and that's my
research in what I hope to do with this
work in fails thank you project my voice
until we have a microphone working there
it is I know lots of people in this room
are working on these issues amen in
machine learning and also in broader
questions around discrimination so I'm
going to throw straight to you questions
from the audience here's to wonderful
dent so one categorization that's been
made is between algorithms that
integrate and weigh all available
information and those that precede one
piece of information at a time so they
try to make a decision on one feature if
that doesn't work they go to the next
and I go to the next and they generally
end up having to nice properties one
they use a third of all available
information whatever that is and to you
can write down what information they're
using and you can actually tell them to
guess if they run out of features so it
seems like in those cases where you know
what the features are you can at least
you know use algorithms that tend to be
as good as those that integrate all
available information but you can also
be transparent about what features are
being used and one one direction one
important direction in the larger space
of defendants exactly that how do you
build interpretable models that are as
powerful or maybe nearly as powerful as
methods that aren't interpretable
but then you can actually write down the
answer of what they're doing which is
very helpful one of the issues that
we've found and that sort of a common
problem in this area is that it's it's
not even sometimes sometimes it is the
default of the algorithm or or need to
make it more transparent but sometimes
even if you do have an effective
algorithm the training data you supply
to it and the metrics by which the
algorithm decides whether it's doing a
good job or not or what can introduce
bias into the system so for example you
know a simple example of this is that
most machine learning algorithms are
trained to say how many times do I get
the answer right on the training data
you know just accuracy training accuracy
and if you have a population that is not
well represented in the train data then
the algorithm can afford to make huge
mistakes on the small population and
will do so in order to optimize really
well on the rest of population now
imagine that small population is a
female applicants are a tech firm and
the larger population is male applicants
now you have an album that that
systematically discriminates against
women when it comes to looking doing
hiring frosting for tech jobs and so
while I think it's important to have
algorithms that you use that are
transparent you can look at the rules
this kind of problem is another problem
that you have to deal with when you're
dealing with albums as well so I think
they're both important but I'm glad you
brought the point of view so um so this
obviously sort of comes up a lot in my
work and going back to the work in the
workplace so reported for New York Times
said you know this is very scary you're
taking this out of the hands of the
recruiter which isn't actually true with
ours was a tool for them but recruiters
have been studied they look they give
you five seconds and look at three
details on your resume your name your
University in your last job all of which
to review are at best weekly predictive
and largely not predictive at all we
looked at 55,000 variables and then fed
that to them to make hopefully a better
decision and we've done a lot of other
things like that so I have this concern
that people have this idea and believe
me I'm not a believer that algorithms
are perfect or they should be allowed to
be autonomous at least not right now i'm
not a technique you tow pianist but
there's this idea that somehow magically
having a personal loop is inherently a
good thing in the sense it I think in
the well-meaning sense that I had one
great teacher and now you're going to
take away that teachers ability which
one while not being true also says well
that's great can that teacher handle the
next billion kids that come along
because that's all I really want to do
is give them that sort of chance so what
has been your experience in in dealing
with the issue of the ideal of sort of
human existence versus what what sort of
benefit can be provided just say
marginally we can improve things by
twenty percent Oh completely and and and
I should clarify yes they be the idea is
not to be absolutist and say that you
know our only predictions are bad the
question is and again from from where I
come from I see it more the techno
absolutism that maybe you've been
dealing with is that there is a tendency
to assume that everything can be done by
algorithm predictions and the problem I
think there is as you just said is is
that you know you that's also a problem
it's alright you want to augment you
don't want to replace so I think the
goal of the research that we're doing is
not to say that you know you should not
be using algorithms at all the ghosts
say how have we understand at least to a
degree the way humans have biases in the
way they look at things let us
understand the way in which algorithms
have biases we look at things and as we
move towards this augmented data society
can we make sure we don't forget about
all the fact ways in which albums can
produce bias and have ways of
instrumenting them so they don't do that
but I can we agree at that point so we
there has to be a balance here yes so
this is sort of a better question based
on a lot of the the entire panel so it
seems like a number of you invoke these
kind of early modern out modalities or
allegories so kercher with the listening
machines or Hobbes Leviathan or Day card
analytic geometry and I'm just wondering
like what is it about it seems like
there are just huge historical
intellectual issues that are being
touched here under the surface with
things like you know the deal edge
innovation and irrelevance of the state
as being like an actor that can sort
these things out for us the
impossibility of like public reasoning
about algorithms that we actually don't
understand this data well enough to sort
of talk and deliberate about it the the
social contract and the means of
production being replaced by the terms
of service Silicon Valley capitalists
and engineers or plating sort of like
the merchants and lawyers of the
Hanseatic League in the sense that like
it's their values and ideas and that are
kind of dictating what the the sort of
ideology is so I guess it's like one of
the things I'm wondering about is like
do we have to sort of throw that history
out and start over like is that no
longer relevant can we learn something
about how the people in those times
sixteenth seventeenth centuries like
dealt with the times of crisis is it
something where we have to as you were
saying like sort of teach the machines
how to recognize like these traditional
values or sort of lead into the ways
that they're disrupting those values
because some of those were not actually
like great paths for us to take a
society so I oh that is an enormous like
thing but it's something I see like oh
like all of these stocks kind of getting
at and yeah I'm just wondering what do
you guys think about like modernity slam
dunk shin woo I that it's so true I can
I just say as I complete second the more
work I do in a space the more we are
taken back to these early presumptions
around how modernity models the social
world because we are both replicating it
and creating enormous gulfs there so
Suresh bring us home modernity what do
we do answer answer this whole question
into second over okay let me think so
and I'm sure others will have many many
more things to say but I think what
we're seeing as always is that it's same
old story expressed in a different
platform it's about power it's about who
has power it's about
you know I'm mediating and changing the
way we think about sort of power the
problem with technology of course that
it has created amplification and many
ways I guess the Renaissance era was an
error amplification in the way people
talked about knowledge though you know
with the good with good and broke with
the printing press we're seeing the same
time amplification here and I think that
amplification combined with you know the
rise of computer science and a way to
understand amplification is what's
making us think I knew about these ideas
but yes they are not new ideas at all
and sometimes it can be frustrating to
sort of say you know yes people have
been talking about this for a hundred
years it is your new little idea is not
the newest thing in the world but but
yes I yeah I don't think it's I think
modernity is where we are right now it
was you know the Renaissance was more
than when they were doing it as well
it's just the question of understanding
how the old and the new interact and
what is specifically different which I
do think is data and amplification</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>