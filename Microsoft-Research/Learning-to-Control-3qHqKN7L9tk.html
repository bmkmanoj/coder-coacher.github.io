<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning to Control | Coder Coacher - Coaching Coders</title><meta content="Learning to Control - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning to Control</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3qHqKN7L9tk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
my name is the Karl Rasmussen I'm going
to talk about learning to control
something to quote from some early one
of our early speakers something I'm a
project I'm hugely enthusiastic about
and some of the students who are working
with me on this here Roger and Andrew
and Rowan and mark oral here now the the
project that we heard about initiative a
little while ago about the the automated
statistician I think it's sort of
fascinating stuff but I think it's a
little bit limited right because why
would you do a why would you do an
automated statistician what you really
want to do is you need to understand the
data because you're going to do
something right if you're not going to
do anything anyway then you know you
don't need to do the statistics right so
learning to control is actually closing
the loop completely right it's saying we
want to learn something on the data
because we're going to take some actions
right otherwise we could we should stop
right but otherwise it doesn't make any
sense right so I didn't bring any
algebra today I didn't actually bring
any slides either so what I'm going to
do is I'm going to show you a few videos
that'll sort of motivate the ideas that
are going on so what I'm interested in
is to how to how to use the ideas of
control within within ideas of learning
within control and so when I look out
there in the world I sort of see a big
problem in control right so we have
biological organisms that seem to be
extremely good at doing control so
they're much better than than artificial
systems so for example one of the David
wolpert in our group one of these
favorite examples is playing chess right
and actually the algorithm for playing
chess is very easy to do but actually
the algorithm for moving the pieces is
incredibly hard right and nobody in
robotics actually know how to how to
program that in the sense that they
would all be completely outperformed by
any four-year-old right so somehow they
control the control algorithms that are
being used on are not really very good
right and if you ask control people you
know how am I going to solve this
control problem
right then they're going to say well you
need a lot of prior information about
your domain right you need to encode
that into your model and into your
control algorithm and stuff I think
maybe that's wrong like maybe you just
need to look at the data right so what
I'm going to try to do here is I'm going
to show you some simple examples that
basically build models based on the data
that disease and nothing much else so
there's no domain specific prior
knowledge in there and basically try to
come up with a control algorithm based
on that so I'm going to show you a
number of examples of doing that so the
first one you may have seen before so I
apologize if that's the case so this is
a little inverted pendulum problem so
the idea is that you want the system to
learn how to balance the pendulum in the
in the middle layer and the state space
is four dimensional so there's a
position along the where the cart is
there's the speed of the cart there's
the position of the pendulum and the
angular velocity of the pendulum there's
one control action which is you can
apply a current to a motor that pulls
the cut back and forth and I actually
don't know what the dynamics of the
system is that the pendulum is 12 and a
half centimeters long but apart from
that I don't really know anything about
it all right and we tell the algorithm
that it should balance in the middle so
there's a there's a point where the
pendulum is swung up in the in the right
in the in the unstable configuration we
say that's a good state and states are
worse when they move away from that okay
and that's what did but the model has to
play with so we so there were first two
little sessions that were random actions
just to excite the system to get some
knowledge about what's going on and
after that we have these little sessions
where we are trying to where we then use
the model that we have to come up with
better control actions and that is then
executed all right and you see things
are getting starting to get a little bit
a little bit better now and the the
actions that we're taking our are
basically continuous continuously we are
at we're providing a current to the
system right so we've now done about
six sessions here so each session is two
and a half seconds long so it has about
15 seconds of experience they see what
happened to next okay so basically it
falls over but that's because the
session stops after two and a half
seconds there's a little bit longer a
session after that so this sort of flies
in the face of the of the conventional
wisdom which is to say you need a lot of
prior knowledge about what's going on in
the system right so the it won't be any
surprise to you that we're using
gaussian process to capture the
short-term dynamics so we basically
predict from a state and an action
what's going to be the next state and
the prior knowledge that we have in
there is that the dynamics are
continuing our smooth so if you change
the location a little bit then the next
configuration will change a little bit
and they are time invariant right if you
do the same thing a little bit later in
the day and the same thing is going to
happen right so that that's the prior
knowledge that we have in there it's
that's one example so how does it
actually how does it actually achieve
this how does it learn that quickly so
one of the things that we take into
account is when we make the predictions
about what's going to happen then we
make a probabilistic prediction like we
actually predict that this is the
distribution over things that might
happen initially when we start the
training then we don't know anything
about the dynamics right or we know very
little about the dynamics so and that
turns out this turns out to be a very
important aspect in the learning that
you have to not only know what you know
but also know the limits of your
knowledge right you can't just optimize
over something that includes uncertainty
so here there's include there's several
types of uncertainty that first of all
it could be that the system doesn't
respond deterministically it could also
be that there's uncertainty due to the
fact that you don't know what the
dynamics are there is a disarming
dynamics but you don't know what it is
so you treat it as being as being
stochastic so the next little video I'm
going to show you how that how that
uncertainty evolves okay let me just
stop that
I wasn't good okay okay so here we have
basically this is a simulation setup so
in the in the ala in the old task it was
it was a real example and a nice thing
about that example was that the video
that we obtained was basically the first
time we plugged it in right so there
wasn't any there was no free parameters
in there we basically plugged it in and
it learned to balance the pendulum it's
not quite true we didn't actually run
the camera the first time we did it
again and took they took the film right
but there's no there's no human
intervention in that process so here I
have a slightly more complicated process
it's basically the same setup but now I
have two pieces to the pendulum and
again you only have one one action that
you can apply you can only apply force
to the cart and the idea is that the
goal is to balance both pendula it
should be standing in the in the upright
position and the goal task is the one
that's up there in a in a plus now what
I show you here are yes no so there's a
there's only a reward for for for
getting the task as good as possible so
you basically the function that you're
minimizing is the average loss of course
if you have a huge uncertainty then your
average loss is going to be probably
going to be pretty large right at least
if you have uncertainty about the
properties of the system that the loss
function cares about right so there's no
there's no explicit penalty on that but
implicitly if you are very uncertain
about what's happening then you're your
cost isn't going to be that good yeah
but it's not something there's no
handcrafted information in there over
and above that ok so the ellipsis is
that that are being shown now are the
the system's own representation what it
thinks the uncertainty is going to be
about the position of the two joints ok
so watch that when it's while it's
learning
so initially it only keeps track of them
for a fraction of a second and then
basically loses all notion of what's
going on because its dynamics model is
still pretty weak tries a couple of
different things there okay so now it
actually confident that it can swing
above horizontal right it knows enough
about the dynamics later on it gets a
little bit better than that its wings it
up and you can see it loses the phase
information right so you get these very
elongated distributions it doesn't know
exactly where it's going to be but it
nodes going to be upright see in that
case and you can also see that sometimes
things happen that it thinks are going
to happen right so it's not going to be
particularly surprised at that so it
doesn't actually really learn very much
from that sometimes something happens
which it doesn't predict is happening
and of course those trials are very
informative it gains a lot of
information when when when that happens
okay so things get a bit boring by now
so in that case for example it thought
it was losing track of things but
actually things were fine right so
that's another way in which it could be
learning these things ok so again from
very little prior knowledge basically
just smoothness and time invariance we
can learn this we can learn this pretty
well so here's my last example which is
a which is a unicycle so it's a this is
my matlab rendering of a unicycle so it
consists of a wheel and a disk that it
can rotate at the top so the wheel you
can apply a torque if you're falling
forwards or backwards you can apply a
torque to do something about that if you
fall sideways then things are a little
bit more complicated now for real
unicyclist you can displace your weight
sideways when you're to counteract the
fact that you're falling sideways now
this unicycle can actually do that so
the disc at the top is is a uniform disc
I've only painted this way so you can
see what it's doing so it can't displace
its weight site sideways what they can
do is it can apply torque to the disc
which will rotate the disc which will
cause the unicycle itself to counter
rotate so that it's now no longer
falling sideways but either forwards or
backwards which it can do something
about okay so that is the idea that you
know perhaps this thing is controllable
by that by that means right but of
course we're not telling the system
anything about that all right so the
system again it's just a the
configuration of system it's just a
point in a I think it is a 14
dimensional space right you have x and y
you have x and y dot you have the pitch
and roll angle etc etc right so there
there are 14 coordinates I think we
throw away two of them so we don't for
the absolute orientation for example we
saw that way let's see what happens so
first to set off things we have 10
random trials and what happens in those
is of course that it falls over very
quickly right within a fraction of a
second it's in the ground with with
random forces and now we start using
policies that have been that have been
optimized you see the first thing it
learns pretty well is is is pitch so now
I was not falling forwards or backwards
anymore it's having trouble that it that
it runs off right so this kind of
mechanical system is actually much more
stable at speed than at rest rest is
actually the worst possible
configuration you could be in an even
real unicyclist they don't sit but
completely address but they always rock
back and forth right because you need a
little bit of motion to become more
stable okay so after you know this is a
fraction of a sec fraction of a minutes
experience it's it's clearly got quite a
bit of information about what's going on
here I don't know how many of you can
unicycle so I I tried learning
unicycling it's pretty hard so I've
gotten a little bit better I've trained
for maybe about five hours and I can
stay on for a little bit more than half
a second now where it used to be a
little bit less than half a second so
the annoying thing about this video is
that it looks pretty easy
II so I actually tried to handcraft a
controller before I ran this thing and I
didn't succeed so it's it's nebulous Lee
hard to to actually have a controller
that that worked well so I think it
doesn't fall over anymore it's not
particularly happy about staying staying
at rest it likes it likes movement right
so what it what it does is it so the so
the last function here is it's just a
distance of the top of the unicycle to
the point over the over the origin right
and so it's penalized indirectly for
falling sideways because because the top
would not have the right height anymore
and that's a small penalty of spinning
because otherwise it just discovers that
spinning really fast on the point
becomes super stable right that's not
really what was intended all right so I
think what I want to try to to to
persuade people is that there's actually
a very rich possibility of machine
learning open by machine learning I just
mean algorithms that look at
measurements that look at the data and
and problems in control and I think this
is for some reason these two domains
haven't really been connected up somehow
in the literature and the and the
communities that do control and do
machine learning also somehow very split
I think that's a pity because I think
there's a there's an excellent match
between some of the techniques that we
develop an hour in our communities and
some of the problems that that are out
there and I think this is actually what
machine owner should be doing like they
should be thinking about getting from
data over insight to action right that's
what machine learning is about and I
think sometimes for many other things we
do we kind of forget the big picture
right and we focus on these little
little tiny things so I'm happy to take
questions
yep between yeah so I so the videos have
been fudged a little bit so between the
trials so it doesn't it doesn't try to
learn during a trial although of course
if you had a lot of computational
resources obviously you know that would
be a good thing to do but the
computational requirement to run this
are actually quite large so there is it
takes about about maybe half an hour or
an hour on my laptop of computation
between each of these trials so that's a
that's a that's a problem that we are
that we're we're up against yeah this
example the cost is sort of the
proximity to the center diff of the
Delta this grade so then you see as some
component with respect to planning or
directly do you have to solve the
planning problem separately or is it
fold it into the single policy to just
as actions and ok so the policy the
policy is just it just minimizes the
expected cost over some long finite
horizon right in this case it's ten
seconds like so we just say well sorry
is it a trying to move on this any
problem no so we just do we just do
conjugate gradient search in the space
of the policy and the policy is just a
function that maps from state to action
all right so we're just searching in
this case actually the policy is the
linear function of the space although
the dynamics model of course is a
nonlinear function of the state yeah so
all in all their only you know 24
parameters that are really being learned
here although it has to learn a lot of
complicated stuff under the hood to
capture the dynamics another thing that
which is interesting is of course that
it doesn't in a in a in a 14 dimensional
space you can't explore the space right
the space is going to be way too large
right and it only knows about these very
small trajectories around in the space
but those are the things that it needs
to actually stay upright right that's
probably a characteristic of these
learning systems that you have to have
algorithms that are on policy that's
right because just just finding the
dynamics of a system you know that's way
too difficult but we may be able to do
that but finding the parts of the
dynamics which are relevant to solving
the control task is a much smaller
problem you have to focus on that
problem right and this is something
that's very natural in reinforcement
learning like people think about on
policy methods but that's not very
natural in control right because people
tend to split the control problem the
design of the control and the design of
the dynamics identification from the
actual from the actual controller right
and that's very unhelpful to do that
split yep be realistic to try and learn
safe from a raw camera image that's
pointed also have a student working on
that yeah so that's an excellent idea
right so instead of so one of the nice
things here is that the the the sensors
don't have to be calibrated like we
actually don't really know I don't know
in the in the pendulum example if I if I
put a positive current puts it move left
or right well I don't know it figured it
out right and so similarly we don't
really have to have measurements of that
of that kind but you would you would you
would like to have you know much more
generic input to the system right so
we're trying to do the pendulum now with
with just the camera feedback and study
yep this video games later problem I had
a look at it but I thought it might be
interesting yeah yeah
how good does it get okay so there's a
so they I think there's a limitation in
the in the way that the problem is is is
specified here so it's not really it's
not really clear what the real goal is
it's to it to stay upright without
moving around too much right but that's
a bit vague right so I can't really I
haven't tried to quantify you know how
how well can I achieve that task so from
a control perspective it's a sort of a
slightly odd task because a simple
controller for example a thermostat
basically what does is if the
temperature is a little bit too low then
it heats a little bit and if the
temperature is a lot too low then it
heats a lot right but this kind of this
kind of system you can't use that
because as soon as you start falling a
little bit sideways then you have to
actually respond by turning 90 degrees
but you can't turn a little bit to
compensate for that right so I don't
know what there's probably some fancy
terms for what systems like that mean
right but it's a it's a slightly more
involved control task and it's not you
can't sort of think about things around
the equilibrium state because it isn't
really an equilibrium state you can't
stay still around the state you have to
think about equilibrium orbits or
something so more complicated than that
right but but that's you know that's
that that's difficult stuff so I don't
really know you know how to evaluate
really how how how well it is except for
the fact that it stays up and that's
that's not easy yeah run as a task goes
maybe yeah that's right that's right
yeah so this is sort of it this is of a
phrase that's a regulation program a
problem but actually you know you should
you should be using this for two to go
from A to B right you want to achieve
something rather than just kind of
staying upright right and then that
would be a more natural way of phrasing
a wheel task yeah
you probably don't want to shake you
this way too much right yeah stir not
shaking right sir yeah yep the property
of systems that you cannot learn so why
can't you learn them you can learn
everything I give you any degree of
freedom you can learn you know you there
are some tasks some goals that you
cannot learn you if i give you figured
you a pen you can come have it on the
moon by itself or something a lot you
know okay so it so it so it is it
possible to balance the the unicycle
well we didn't know we kind of hoped it
was correct your eyes the fact that is
not possible no no I don't know about
that so that's so that's also i mean
many so many questions that are answered
within control within the control
literature are things like characterized
is something controllable and is it
optimal and these are things that i
don't really know how to think about but
I also think that's maybe a consequence
of this sort of style of biological
learning like you can learn to write the
bicycle but are you optimal I'm that
sort of a bizarre question right you're
good enough right so I think if you
focus on optimality then things get
really difficult right so maybe we
should back off from that question all
right
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>