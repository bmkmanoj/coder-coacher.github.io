<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Approximate Common Divisors via Lattices | Coder Coacher - Coaching Coders</title><meta content="Approximate Common Divisors via Lattices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Approximate Common Divisors via Lattices</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/edjIsWP_ICI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
the next talk I want to introduce our
next speaker nadia henninger nadia is a
an assistant professor at university of
pennsylvania now she was previously an
NSF for stock at UC san diego and a
visiting researcher at microsoft
research in new england she works in
cryptography applied cryptography
algorithms and security her most popular
work was in identifying entropy problems
in cryptographic keys on the internet
which got her two best paper awards at
use annex symposium she received her PhD
at Princeton and before that she was an
undergrad at Berkeley thank you very
much for inviting me it's exciting to be
here I am sad about the Warped slides I
just redid them in 69 trying to avoid
this problem that I guess computers are
hard so in this talk I'm going to talk
about I guess actually using lattice
basis reduction for cryptanalytic
purposes for the most part and I've
called this approximate common divisors
by lattices I'm going to actually
introduce the approximate common divisor
problem and sort of the framework of
using lattice basis reduction to solve
polynomial equations modulate divisors
and then talk about a number of
extensions of this idea to different
realms so sort of as a little bit of
background if you consider the general
problem of solving equations modulo n
where n is some integer if n is prime
then we know how to do this efficiently
you can compute roots of univariate
polynomial xin polynomial time modular
primes and if n is composite the best
thing that we know how to do in general
is to factor n into its prime factors
and then solve the equation Mahdi Prime
and then use the Chinese remainder
theorem to lift two solutions mod n so
this sort of general question is of
great interest to cryptography
in in general solving a quadratic
equation modulo n is actually as hard as
factoring and the Raven encryption
system is based off of this fact and of
course the RSA encryption system is also
based on the assumption that solving the
equation X to the e where e is the
encryption exponent of our SI minus the
ciphertext finding roots with that
equation mod n should be we hope as hard
as factoring the entire internet depends
on this fact and okay so we hope in
general that this equation the solving
equations mod n is hard if we don't know
how to factor n and then even if we do
know how to factor in there might be a
little bit of difficulty in enumerate
all of the solutions because it there
might be exponentially many solutions if
and has very many prime factors so this
problem might be hard in a few different
ways so sort of the the motivating
theorem starting this talk is a theorem
due to coppersmith which tells us that
in some situations we can actually find
roots of equations modulo n and so if
we're given some polynomial of degree D
and some integer n and we want to find
the roots of this polynomial we want to
find some root of this polynomial mod n
coppersmiths theorem tells us that we
can do this if the root is small where
small means of size n to the 1 over d so
ok and in some sense you can think maybe
this is a natural bound to have because
if you take your the RSA equation X to
the well call this D minus C if C were
has some rude over the integers so we
just removed this part then we know how
to solve this equation because it's just
factoring polynomials over the rationals
which we can do efficiently so if C has
some root of size less than and to the 1
over d
so this so this mod n just goes away
then we can solve this equation so what
coppersmiths theorem is telling us is
that even if there are some large
coefficients involved in this polynomial
we can still find its roots we can sort
of get rid of the large coefficients and
do the stupid thing so as I said why is
this theorem interesting so there's sort
of two effects by accepting some bound
on the size of the route that we're
looking for we can provably find
solutions to polynomial equations in
polynomial time without factoring n so
just forget about the fact that we don't
know how to factor and we can still
solve these find some solutions of the
equations the other implication is that
you know maybe and is not an RSA modulus
maybe n has very very many prime factors
and this actually is a sort of
constructible proof that there are only
polynomial e many small solutions to a
polynomial equation there might be
exponentially many solutions in general
but within this range of size where the
work coppersmiths theorem applies then
there are only polynomial many solutions
and we know there are because we can
enumerate all of them in polynomial time
so how does this sort of how do we
actually accomplish this goal so the big
sort of insight here is we're going to
take our input polynomial f and we're
going to take our modulus n and we're
going to try to construct some new
polynomial Q with the property that all
the routes that we're looking for of
Earth not in our actually roots of Q
over the integers and then once we've
constructed this magical Q then we can
just find the roots of Q over the
integers check whether they actually
vanish mod N and if they do then we
accept them as a solution to our problem
so the sort of main lemma here is the
fact that we know that ok so somehow
we're going to ensure the fact that Q
vanishes mod
and at the roots are that we're looking
for if q also has the property that its
coefficients are small where small means
that if we add up the size of the
coefficients oh there should be a to the
I power here so if there's if we add up
the size of the coefficients and sort of
the maximum size that the root could
have if that is strictly less than n and
we know that Q vanishes mod n then Q
actually must vanish at our over the
integers so then we're done so how are
we going to achieve the two properties
here that we want in our out of our Q
well to ensure that Q actually vanishes
mod n if we construct you as an integer
combination of our input polynomial and
multiples of n than any integer
combination of these two things will
also just vanish mod n when we plug in
our so that works and in order to ensure
that Q has small coefficients we're
actually going to take these polynomials
consider them as coefficient vectors
create construct a lattice generated by
the coefficient vectors of these
polynomials and find a small vector in
this lattice and that small vector will
correspond to a polynomial with small
coefficients and we just treat that as
our polynomial Q and factor it to find
its roots and I'm I've been lying to you
here slightly in that if you're
considering coppers mousse theorem this
vanishing mod n doesn't actually work
but if you take higher multiplicity so
instead of asking to vanish mod n you
asked to vanish mod n to the K which you
can achieve by taking powers of F in n
of this form then you can actually
achieve a theorem so this actually works
so here's sort of the complete outline
to make things concrete so we input our
F&amp;amp;N we're going to construct a matrix of
coefficients of powers of F and n so n
to the K k is some magic
value that we will that comes out of the
analysis then say X n to the K we start
including powers of F up to F to the K
some powers of F you know multiples of X
higher than that and so on so these are
all elements of the ideal generated by
F&amp;amp;N taken to the cave power and then
once we have this matrix then we're just
going to run a standard lattice basis
reduction algorithm we don't need any
special properties out of it so if we
just use the lll algorithm then as we
heard about in damiens talk just before
we have we get some exponential
approximation factor and we get the
promise that our vector is about of size
determinate to the one over dimension
then once we find a vector of this size
then we will construct a polynomial out
of that vector factor it find its roots
so you might be worried well what
actually happens in this exponential
approximation factor is this going to
destroy the theorem and one of the
magical things about this sort of
coppersmiths method is that in this case
the exponential approximation factor
actually disappears in the analysis so
it just becomes a constant which you can
get rid of in polynomial time which is
one of the sort of magical things about
coppersmiths method is that an
exponential approximation factor is okay
the other question you might have is
this sort of heuristic are we hoping
that we'll find like the shortest vector
and it turns out that this is completely
provable so once we find a vector with
the approximation guarantee then we're
set so sort of magical theorem here's a
concrete example just in case you were I
don't know feeling confused or if you
like me like practice this is not just a
theoretical method it totally works
really well in practice so here is let's
see I'm going to generate an RSA modulus
this is kind of an embarrassingly small
RSA modulus but I just wanted to like
have something that would fit on the
screen so ignore that part because it
still works if I take a 20
you know 2248 bit RSA modulus and then
I'm going to construct my message and my
message will be the password for today
is swordfish and this is just I'm
converting it into an integer so forget
that part and then I'm going to encrypt
my message with RSA exponent 3 so I
raise my message to the third power mod
n so imagine that for some reason I know
that my i am the attacker now and I know
that the ciphertext has the form the
password for today is something then I
can then try to apply coppersmiths
method to discover what this secret
password is so I construct some value
this is actually the bound on the size
of the route that we're looking for is
about the right size I construct some
matrix I didn't include that part in the
slides because it's kind of long but
essentially n is the coefficient vectors
of N squared and squared times X and
squared times x squared and so on and
then a bunch of multiples of the
polynomial a plus X minus the ciphertext
then I apply LOL to my matrix and then i
take the smallest vector in the lll
reduced basis I reconstruct a polynomial
out of that i factor that and then if i
subtract this value from a then i get
the actual message so this is actually
an example from coppersmiths original
paper it's the problem of stereotyped
messages so this is why our fa padding
is important ok the variant of this
theorem that I'm actually going to talk
about for most of this talk is actually
a version which is in this form due to
how grave Graham so the question that
I'm going to consider is instead of
trying to solve equations modulo n I
want to actually solve equations mod
some divisor of n so remember I still
don't know how to factor in so I don't
actually you know if I could factor in
then I could just I don't try to solve
ma teach divisor but here I'm just going
to say
I want to find solutions that happen to
vanish mod large divisors where I'm
measuring large by some parameter beta
which is between 0 and 1 and this
theorem tells me that i can find
solutions to the equation which is
essentially this is a degree one
equation X minus R or a minus X sorry
and I want to find a solution to the
advantage to this equation that vanishes
mod some divisor event and thus theorem
says that I can do so if my solution is
less than n to the beta squared in
polynomial time so as I said this is
sort of a generalization of copper Smith
here I'm specializing to the equation of
the form a minus X and I want to find
solutions mod this divisor but the
algorithm improve for essentially
identical to what I showed you and sort
of coppers miss the original theorem and
if you want to you can write this and
generally you can you get a pound of end
to the beta squared over D if you have a
degree 2 equation you can also say well
maybe I don't have an exact device all
right I don't have an exact multiple of
this divisor instead I just have to
approximate multiple multiples of the
divisor a1 and a2 and I can actually
still solve this i take a little hit in
the size of the equation that i can or
the size of the route that I can find
this one is actually heuristic okay so
what is this good for sort of the I
guess canonical useful application of
this theorem is for factoring with
partial knowledge so here I'm generating
some RSA modulus again i have 2 512 bit
Prime's P and Q so I have a 1024 bit RSA
modulus then I take this value a which
is some it's near p so I'm basically
just subtracting off a little bit so
that the least significant 86 bits of P
have been transformed to 0
and then I assume ok I have I as the
attacker have access to this value a
with 86 bits of P missing so what do I
do i construct a matrix which is
basically powers or its multiples of i
have n i have the polynomial a plus x
and then a plus x squared and then I run
LLL on this this basis of my lattice so
this is a three by three lattice I take
the polynomial generated by the smallest
vector I factor this the first route
that i get if i add it to a is equal to
P so this works actually if for those of
you who are going to Asia crypt I have a
paper with dan and Tanya who are sitting
in the back and we actually use this to
break real live RSA moduli that people
were actually using in practice so
coppers miss theorem practical okay no
so if you if it's the first 86 bits then
you construct the equation like you know
whatever piece you know plus like x
times you know to to the some power that
gives an offset and you just want to
find roots of that equation so you can
you can put the eighty six bits anywhere
in the middle and this is less powerful
than this theorem because if you want
sort of the exact end to the beta
squared then you need a much larger
lattice so I'm this example is taking a
very small latticed it gives you a a
worse guarantee so the if they're not
consecutive then you can take chunks and
you can try to solve okay I have you
know some variable representing this
chunk so I'm variable representing this
chunk and then you get you know a new
variable in your equation you have like
some linear equation Alex may has a
paper in Asia crypt several years ago
sort of doing that exact verte variant
and I guess I have a paper doing the
variant where you have non-consecutive
chunks you just have a bunch of errors
in your RSA modulus but you need extra
information for that in general sort of
extending to the multivariate case is
not as well understood so I want to talk
about a slightly different variant of
the multi multivariate case which is if
ya write on the board ok so that if you
have multiple if you have a bunch of
different chunks say like this is this
is P and say that here's one chunk you
don't know and here's another chunk you
don't know here's another chunk you
don't know call this x times 2 to the SE
t y times 2 to the s so here's like a
settee offset s this is offset you say Z
times 2 to the you then you could do
this by right you could solve this by
writing some equation of the form say a
plus x times 2 to the T plus y times 2
to the s plus Z times 2 to the U and
this should be equal to 0 mod p this
should actually just be equal to P so
that's one form of the equation or this
is one form of sort of a multivariate
extension of this the multivariate
extension that I want to talk about is
the case where we get a bunch of near
multiples
p so this is essentially that we have
like p plus r1 p plus r2 a1 equals a2
equals p plus r2 and so on and you can
add multiples here too if you want to so
the problem here is if you're given a
bunch of sort of near multiples of P can
you reconstruct Pete can you do any
better than if you just have one near
multiple of P which was sort of the the
original variant that I was talking
about so here if I subtract off sort of
a minus a 1 minus R 1 a 2 minus R 2 and
so on and I take GCD the GCD of all of
these things then I want that to be
equal to say P which should be larger
than its bounded by some power of n so
this problem actually arose recently in
the context of constructing fully
homomorphic encryption systems I don't
know how much we will hear about fully
homomorphic encryption today but in
general I mean fully homomorphic
encryption you want to do arbitrary
computation say in the cloud on
encrypted data and the original schemes
and most of the work has been
constructing fully homomorphic
encryption based off of say hard
problems over ideal lattices or lwe but
there's also a number of works trying to
construct sort of simplified fully
homomorphic encryption systems which use
assumptions over the integers and the
assumption used in all of these works is
essentially that the approximate GCD
problem is as hard if you're given a
whole bunch of inputs as if you're given
just one near multiple of P so already
we know we can do better than brute
force for this problem but this is a sub
exponential algorithm so the question is
exactly how hard is this problem so if
you
sort of take the inside of like what is
the what is coppersmiths method actually
tell us about this problem if we try to
just take the stupid as possible
extension of this to the multivariate
case basically all of the theorems here
the proofs look exactly the same the
question is to sort of did you how well
can you do the analysis to balance the
parameters so the natural thing to
consider is in you know we're going to
we now have a bunch of different
solutions that we're trying to find so
we have these r 1 and r and these are
all variables that we're trying to solve
for so instead of constructing a single
polynomial equation in one variable
we're going to try to construct a series
of polynomial equations in n variables
and then solve and and we're going to
construct them so that those all vanish
at the roots that we're looking for over
the integers or rationals and then we're
going to well how are we going to find
these we're going to look for these in
say the ideal generated by each of these
polynomials and our different variables
and N and we're going to ask for it to
vanish it at some high high power of our
of our approximate common divisor and
then we're going to solve the system of
equations to find the roots this is sort
of the cartoon version of what we would
expect an algorithm in this framework to
look for to look like to solve this
problem so what actually sort of what's
the difficulty here like why can't you
just do this one problem is that well ok
so we're going to construct some giant
lattice and we actually need m equations
rather than just one equation well it's
ok we actually know that we can find em
relatively short vectors in a lattice
just from the approximation guarantees
given by LOL so ok this becomes some one
plus little of one factor in the
exponent of the theorem statement and
actually in practice you can just ignore
having to do this complicated analysis
because a random lattice basically if
you run LLL on a random lattice all the
vectors have approximately the same size
and I'm using a slightly worse
approximation factor here then Don man
gave but this is this is what my
lattices look like when I just run a
fast LOL on them ok so approximately all
the vectors that will come out are about
the same size apparently LOL can't tell
the difference between my special
lattice and a random lattice the next
problem is ok so we have a system of em
equations in M variables and we need to
actually solve that system of M
equations and it might be that you can't
solve or you can't find a finite number
of solutions to the system of M
equations there might be algebraic
dependencies between the equations that
would give infinitely many solutions so
if you have a reduced basis of allowed
as we know that the coefficient vectors
of these equations are linearly
independent but we don't know that when
we turn these back into polynomials that
the polynomial equations are
algebraically independent and basically
in order to get a theorem you have to
make the heuristic assumption that you
will get algebraically independent
equations these are multivariate
polynomials yeah M so M could be 2 or M
could be 100 you can test so the
question is basically you have this sort
of black box thing where we know there's
a deterministic construction where we're
going to have some lattice and this will
be the coefficient vectors of like of a
bunch of polynomials of this form and
then we throw this into a basis
reduction algorithm and we will just get
out some essentially deterministic
function of this and if all of the if
all of the equations that come out of
this lattice are algebraically dependent
then we can't do anything so
yeah we're not guaranteed and in general
I mean this is a problem that comes up
in every in every application where you
try to have some kind of multivariate
extension of of coppersmiths theorem
that you have to if you're asking to get
em equations out of the out of your
lattice I don't know there's a there's a
few specialized cases where you can
prove something but in most cases like
basically all of these papers say
heuristic assumption you can get em
algebraically independent equations in
practice in nearly all cases you can
solve the system so the we did actually
run into algebraic dependencies but in a
lot of cases these resulted from some
obvious sub lattice and you could still
just sort of keep taking more equations
and then solve it and okay so once you
have these M equations you say okay well
how am I going to actually solve the
system of equations you can you know
sort of use resultant stew eliminate
variables or you can throw the whole
thing into a grosvenor basis reduction
algorithm and try to solve and you might
say well okay isn't that slow it'll be
exponential time in the number of
equations or like EE exponential or
whatever these things take but it
actually in practice if you have if you
take an overcurrent steering system so
you take more equations than you
actually need then it turns out to be
quite fast if you have a good
implementation so this is not actually a
problem the last part that you might
think about okay well we had an
exponential approximation factor from
LLL is that going to be a problem it
wasn't a problem in the univariate case
it turns out that it actually is a
problem in the multivariate case and
this actually results in a constraint
that gives you a lower bound on the size
of the approximate common divisor that
you can allow and essentially okay so we
have some lower bound on the size the
approximate common divisor and if you
can get a sub exponential approximation
factor or
for your lattice basis reduction then
you can improve the the lower bound on
the size of the approximate common
divisor so here if we just take an
approximation factor with 22 the
dimension also epsilon equals one that
gives this constraint here if epsilon
decreases then we can get a slightly
better constraint but this is somewhat I
can go through a disgusting a
calculation to show why this is
necessary in this algorithm but I don't
actually understand in a deep way why
this arises so but in any case sort of
here's the the theorem that results so
if you want to solve the approximate
common divisor problem when you have a
bunch of near multiples of P then you
can find you can solve the the system of
equations if you have sort of this lower
bound on the size of P and you can do so
if the size of the errors is essentially
the size of the multiples to the beta to
the n plus 1 over m so in the univariate
case we had M equals to 1 so we had beta
squared and so basically as M increases
then the size of the error that we can
tolerate increases approaching the size
of the common divisor but we have a
lower bound on the size of the common
divisor so if you wanted to keep
decreasing the size of the common
divisor and fix our then you couldn't do
that and this algorithm runs in time
polynomial in log of the inputs and
exponential in the number of inputs and
if you want to assume that you don't get
an exact multiple of P but you get only
approximate multiples of P then you pay
for this by changing the constant factor
in the exponent so if we try to apply
this to the integer fully homomorphic
encryption systems then we get sort of
an interesting phenomenon which is that
here it's the set of parameters
suggested by a paper by Van Dyke Gentry
halevi and by could ton of them so they
have a security parameter lambda and
they've set their parameters essentially
so that the size of the common the size
of the common divisor is just slightly
less than square root of log of the
inputs so this is violating the
constraint that was necessary for our
algorithm to work but it's just barely
violating this so if you could do a
better than exponential approximation
factor for LLL so you could do to to the
dimension to the two-thirds then you
could break this scheme in polynomial
time with three samples yeah so then you
can take it slightly smaller but then
you can increase the number of samples
so for any polynomial setting of these
parameters then there's a polynomial
number of samples and a an approximation
factor here where you would you know
two-thirds one-third you know keeping
keep decreasing so okay this is for sort
of a theoretical setting of parameters
there was a later paper by Quran Mendell
nakash and Taguchi that gave sort of
concrete settings of concrete parameters
for a related fully homomorphic
encryption system and if you assume an
ll approximation factor of 1 point 0 2
to the dimension then you can just
calculate the size of the lattice and
the size of the inputs that you would
have to just run LLL on in order to to
break the scheme and essentially the the
dimension of the lattice well okay the
the toys you know 100 dimensional
lattice we can do that but even in these
cases these are polynomial time
algorithms but there's no implementation
that can handle an eleven thousand
dimensional lattice and run LOL on this
but there's no reason that you couldn't
do this just that nobody has actually
scaled and implementation and gotten a
big enough computer to do it so okay
it's on my list but I haven't done it
yet I think there was an another was
sort of an analysis in the paper that
would that sort of went through just
applying this sort of directly to the
scheme in it you know they set the
parameters so that it doesn't work but I
haven't looked at it carefully yet okay
so I want to talk about sort of a
slightly different extension of these
results so you know okay so starting
from sort of the univariate how brave
Graham approximate common divisors we
went to a multivariate version well what
if we try to extend it in a different
way so instead of talking about integers
we talk about polynomials so there's the
sort of lovely analogy between the
integers and and polynomial so on one
side we have the ring of integers on the
other side we have the ring of
polynomials with coefficients in some
field and basically every concept that
you want to think about in the integers
you can come up with some analogous
concept for polynomial so we have some
integers or Prime's we can factor
integers and deprives we can factor
polynomials into irreducible polynomials
if we ask how big an integer is well the
natural measure of size is an absolute
value if we ask how big a polynomial is
the appropriate measure of size that I'm
going to think about is the degree of
the polynomial so if we sort of set
things up the way that we want here then
everything works the way we want it to
we can divide integers we can we have
unique factorization we can can be a
common divisors we can do the Chinese
remainder theorem which in the
polynomial side is just interpolation
I've been talking about integer lattices
you can define an analogous notion for
polynomials which is I mean you can
think of it as a polynomial lattice it's
an FC module and somehow there's all
these lists of sort of hard problems for
the integer as well we hope that
factoring is hard the best we can do is
sub exponential time algorithms the
shortest vector problem closest vector
problem
NP hard there's you know hard
mathematical problems like the Riemann
hypothesis if you take the analogous
problems for polynomials there's some
how much easier so you can factor
polynomials in polynomial time you can
actually solve the shortest vector
problem and closest vector problem in an
SD module in polynomial time the
algorithms are easy in a sense and you
know if you're sort of a serious number
theorist you can prove the Riemann
hypothesis for polynomials so what's
interesting about this is somehow if you
take these problems that are hard in the
integers these turn into sort of
cryptographic problems and what I'm
going to show you hopefully if I have
enough time is that if you take these
analogous problems for polynomials they
turn into problems in coding theory so I
mentioned polynomial lattices so what
actually is a polynomial lattice sort of
the concrete way to think about it is if
you in the integer case you have vectors
of integers or whatever and you can take
integer multiples of these vectors and
that generates elements in your lattice
in the polynomial case we have vectors
that are polynomials or rational
functions whatever you want and you take
polynomial multiples of these vectors of
polynomials and that generates our
polynomial lattice then if we ask ok if
you have a vector of polynomials what's
what's the length of the vector of
polynomials the definition that I'm
going to take is that the length of this
vector is the maximum degree of a
polynomial in this vector so using this
definition you can define an analogous
notion of lattice faces reduction I'm
not going to do this here but it's easy
in a sense and actually there are a
number of polynomial time algorithms to
find shortest vectors and the the
version that we care about is that we
can find a vector with the guarantee
that its length is 1 over the dimension
times the length or the size of the
determinant
so this is sort of the log analog of the
guarantee that you hope you can get in
the integer case so once you've sort of
set that up then you might hope to
translate the approximate GCD problem
into polynomial land and so here I've
done this Oh actually this is coppers
miss theorem no okay it's both so I have
some input polynomial here in the
integers here's my input polynomial with
coefficients that are polynomials and
some other variable then I have some
modulus integer here I have some modulus
polynomial and I'm looking for here in
the integer case integer roots in this
case I'm looking for polynomial roots
and I'm hoping that I have a large GCD
with my route and n here I'm hoping for
a large GCD measured by the size of the
degree and I can do that if the size of
my route is small and here it's if the
degree of the root is small you're
talking about regular teachers or
rainbow that features number regular
integers I'll talk about the ring of
managers in a number field in a couple
minutes so this is this is normal normal
integers this is normal polynomials and
what does the proof actually look like
here's basically the the steps we're
going to create some new polynomial it's
going to be a linear combination of our
input polynomials we're going to try to
bound the degree we're going to find
this polynomial by constructing a
polynomial lattice out of the
coefficients of our input polynomials
and apply lattice based the direction
this is the modulus polynomial so here
we have a modulus that's an integer here
we have some some polynomial that we're
hoping to find roots yeah okay so here's
the steps of what we hope this this
algorithm will will look like and
it checks out so everything just works
the way that you hope it does so I've
now done all this work but i actually
just said a couple slides ago that
factoring polynomials is easy and of
course you know the the reason that we
were interested in coppersmiths theorem
and how grave Graham's theorem is that
factoring integers is hard and so we're
interested in cases where we don't have
to factor integer as well if we can just
factor polynomials than why why did we
just go through all this work it turns
out that this is actually the analog of
the approximate GCD problem over
polynomials turns out to be exactly the
problem of decoding reed-solomon codes
so reed-solomon codes are one of the
most in-form important families of error
correcting codes they're based off of
the problem with noisy polynomial
interpolation so the decoding problem
for for a reed-solomon code you're given
your input is a bunch of points and you
want to find low degree polynomials that
match your input points on a large
number of values and the classical bound
for unique decoding of reed-solomon
codes gives you sort of the decoding
radiuses the number of points minus the
degree of your this is the dimension of
your code over 2 and guru Swami and
Sudan had sort of a very amazing and
famous results that if you are willing
to tolerate non-unique decoding so
you're willing to tolerate sort of
having a list of possible decoding
polynomials for your for your code then
you can actually increase your decoding
radius up to n minus square root of n L
so how does you know how is this
possibly the same thing as approximate
common divisors well you can actually
just set up the problem in a sort of
similar way so we're given these these
input points and you can actually sort
of translate the problem of well ok
we're trying to find some polynomial are
matches a bunch of these pairs you can
rewrite this polynomial matching this
pair as this polynomial is our of Z is
actually equal to Y I mod the polynomial
Z minus Z I so you can see that this is
true if you plug in zee i then this goes
away and we get r zi equals y I so this
is just a rewriting of this fact and
using polynomial interpolation we can
construct some polynomial G of Z that
goes through all of these points this is
going to be the analog of our value a
that was sort of near p and we're going
to construct our polynomial X minus G of
Z which is about it's the analog of X
minus a and our modulus ponnam is going
to be the product of z minus z I so all
of the evaluation points of our
polynomial and the degree of the GCD
here actually counts the number of
points where if we evaluate F of our
this will count the number of points
where are actually matches the
evaluation point or the the input points
so this will this will match the number
of non errors in our code word and the
this value measuring the size of the of
the common divisor that we're looking
for is actually telling us the number of
correct points in our code word divided
by the total number of points so
actually it turns out that somehow this
is almost the same as the Guru Swami
Sudan algorithm guru Swami in Sudan they
are also sort of their algorithm works
by constructing some interpolation
polynomial Q and instead of using
lattice basis reduction they use linear
algebra to over the coefficients of Q to
find a solution so in a sense the how
grave Graham algorithm is replacing this
linear algebra step with polynomial
lattice basis reduction and it turns out
that because polynomial lattice basis
reduction is so fast this gives you a
better running time than the Guru Swami
see down
for them so it's sort of interesting
that the coding theorists and the
cryptographers came up with the same
thing at almost the same time and sort
of knew about it but never actually
managed to like lay it out exactly in
parallel so I in the integer case was
talking about sort of multivariate
extensions of the approximate dcd
problem you can try to give a sort of
exactly analogous multi-varied extension
of polynomial approximate g cds and if
you do that the theorem looks something
like this so in the multivariate integer
case we had this lower bound on the size
of the common divisor that we were
looking for and we had some horrible
factors that arose here in the
polynomial case because we have no
exponential approximation factors for
LOL all of that horribleness goes away
and we can just get essentially that the
size of the roots that we're looking for
is about and the end times beta to the n
plus 1 over m so kind of ugly but it
turns out that it's nice the algorithm
is exactly the same now why do you
actually care about sort of a
multivariate extension of reed-solomon
codes so in this case instead of getting
one polynomial and we hope that we can
find some low degree polynomial that
passes through a bunch of a bunch of the
same number of points here we're getting
a bunch of different polynomials and we
hope that we can construct polynomials a
sequence of polynomials that matches a
large number of the same points for each
input polynomial and there's two
versions of this heuristic alee if you
take random if you take a random
sequence of polynomials and you take a
random sequence of errors
and this has already come up in
cryptography that if you have a
heuristic assumption then essentially
that the algebraic Independence of the
sequence of equations that are of the MM
variable equations that you get holds
then you can decode these this works if
you have random codes but in the coding
theory case you can actually sort of
define away this problem by defining
your code word to have a predefined set
of algebraic relationships so you don't
actually need to run into the algebraic
independence problem if you're in coding
theory land where you can just say ok my
code is defined to be this so there's a
number of constructions of codes based
off of this principle and somehow it's
amazing that the coding theory world
came up with these theorems and they
actually had the same result on the size
of sort of the multivariate approximate
GCD problem when the cryptographers were
doing almost the same thing and didn't
necessarily know about it so this is
interesting yeah parvez roading goswami
rudra so this is actually totally
practical I have a paper with in
Goldberg and Casey debit where we used
these multivariate reed-solomon codes to
essentially do private information
retrieval and this sort of gold line is
the decoding the running time of the
decoding algorithm based off of
polynomial lattice basis reduction so
this is the running time of the group a
gross Swami Sudan algorithm this is the
running time of a heuristic decoding
algorithm and this is the brute force
decoding algorithm this is a log scale
so basically the polynomial lattice
basis reduction is
credibly fast even in practice it works
very well so I guess to finish up I want
to talk about a final extension of sort
of these ideas to bring lwe so instead
of going from integers two polynomials
I'm going to go from integers to
algebraic extensions of the integers
essentially so the sort of setup for
ring lwe there's a lot of math here so
this would be in like a whole hour long
talk in itself but sort of waving my
hands very vigorously about the math the
setup here that we're going to think
about is we're going to take some ring
which is we can look at it as two ways
we can we can take essentially a ring of
polynomials in the variable X mod some
irreducible polynomial f of X and we can
view this ring in two ways so we can
view it as a polynomial ring which is
sort of represented as the coefficients
of degree n minus 1 polynomials or we
can view this as the ring of integers in
the number field hewo join alpha where
alpha is some root of this irreducible
polynomial f of X and you can think of
this ring of integers in it in a sort of
similar way that you have the elements
of this can be represented as
polynomials in alpha so you have
coefficients of these puppies elements
so why is this problem so why is this of
interest to cryptography it's because
you can define essentially the ring L to
be ring learning with errors problem and
usually you specialize to the case where
f of X our polynomial is X to the n plus
1 so our so the Alpha that we're joining
to the rationals is some root of unity
and we're going to fix some some element
in this ring uniformly at random then
the ring out of a problem is if you're
given a bunch of
tools of the form some multiple of s
plus some small error small defined by
some error distribution and we're given
AI and these bi sort of error-filled
samples of multiples of s then the goal
is to find s so that's ringing lwe and
this is used to construct efficient
public key cryptography it's used to
construct fully homomorphic encryption
and you can prove that I guess I should
have put a citation here but it's lou
machetty pie Kurt reg of so the ring LW
problem is as hard as polynomial
approximating short vectors in ideal
lattices okay so if I from the Crypt
analytic side want to take sort of all
of the coppersmith how great Graham
methods that I've been talking about and
translate them over from integers to the
ring of integers in a number field what
do I actually have to do well okay so
instead of talking about integers I'm
going to talk about ideals in this in my
ring instead of primes i'm going to have
prime ideals in the integer as well
there's really sort of one absolute
value that we care about the absolute
value in a number field you actually
have n absolute values and is the degree
of your number field each of these comes
from some just different embedding into
the reels and in order to get sort of a
useful theorem you have to bound all of
them instead of having a lattice over
the integers the analog of that is in is
an okay module so okay is the ring of
integers instead of having sort of a
basis you can try to find a reduced
pseudo basis of our okay module and in
this setting in most cases you can still
do efficient arithmetic so all the basic
calculations that we need to do even
though we're dealing with algebraic
numbers so you can still find roots of
polynomials and sort of represent these
elements in some efficient way
so okay so what kind of challenges do we
run into well what do we do about the N
absolute values really the only thing
that I know how to do is to keep track
of them all and this will take our
lattice and it will blow it up by a
factor of n how do you you know sort of
skipping some details but somehow we're
going to end up with a lattice where we
have ideals and we're going to try to
reduce this lattice filled with ideals
in our in our ring of integers how are
we going to find a reduced pseudo basis
well it turns out that there is a
polynomial time algorithm to do this but
in the case of in sort of the univariate
case all we care about is finding a
short vector in this lattice and so it
suffices to just embed each ideal as its
n by n canonical embedding which is an
ideal lattice as itself and then apply
LOL to this sort of gigantic lattice
that's blend blown up by this factor of
n so the N by n canonical embedding is
just um well I don't really have time to
talk about it but this is an ideal
lattice in itself so we have some
gigantic lattice and then what happens
you know once we've blown up this the
size of the lattice what actually
happens from to the approximation factor
from LOL in the theorem statement it
actually becomes a 2 to the N squared
due to two problems one of them is
blowing up the the size of the lattice
using by using the canonical embedding
the other one the other factor of n
comes from in the proof having to
convert an arithmetic mean to a
geometric mean so somehow these are both
frustrating
it's not better so if you were doing
this just to work out what the correct
analog of the approximate GCD problem is
for ring out to be so if you if you just
take the translation of the results so
instead of having to say well heuristic
alee if you just take the ideal lattice
and sort of find a short vector in it
that might not actually solve the
problem so this is a way of provably
solving the problem if you could reduce
the lattice it's not necessarily I mean
because it because it blows up the site
so this is this is worse than trying to
reduce the the ideal lattice it's the
natural analogue of the sort of
approximate GCD like how great grams
approach of the approximate GCD too but
it's clearly sort of not the way to
actually solve this problem if you
wanted to break the ring lwe based
schemes in practice so the theorem is ok
so we're given some number field of
degree n where n is cryptographically
huge and we're given some polynomial in
with coefficients in the ring of
integers and our number field and we
want to find roots of this polynomial
mod some divisor of an ideal in the ring
of integers and the theorem says that we
can do this if we have bounds on the
size of the root so we have to
individually bound each absolute value
and the product of the absolute value so
the norm of this route is two plus
little of one here's the N squared over
2 times the norm of the ideal and then
we get the beta squared over D that we
got all along so you can do this in
polynomial time probably not useful one
thing you can say is that using this
theorem if you can actually improve this
approximation factor here to a 2 to the
minus n times the square root of the
discriminant of your number
field then this would actually solve the
bounded distance decoding problem in
ideal lattices in polynomial time so in
some sense this is telling you that
there's not a whole lot of hope for
improving this theorem entirely or
improving the approximation factor of
this theorem entirely unless for some
reason this problem isn't hard so that
was all I had so in summary sort of i
went from how grave gram approximate
common divisors coppersmiths theorem and
took that in a bunch of different
directions going to multivariate
versions of this theorem going to
polynomial versions of this theorem and
going two versions of this theorem over
number number fields there's actually a
fourth version that I didn't talk about
the analog of number fields for integers
is function fields so a finite extension
of polynomials and you can sort of do
the analogous thing and it turns out
that this gives you an efficient
decoding algorithm for algebraic
geometry codes so all right thank you
that's all I have
crucial or for example if you use a
bigger exponent like 65,000 can we apply
the same attack if you use exponent 655
37 then you could find roots of size n
to the 1 over 65 537 yeah so in some
sense coppersmiths theorem is viewed as
a way of cryptographically understanding
small public exponent RSA
more questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>