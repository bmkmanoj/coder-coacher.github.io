<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR Theory  Day - Part 3 | Coder Coacher - Coaching Coders</title><meta content="MSR Theory  Day - Part 3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR Theory  Day - Part 3</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9O67zRzS98c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay yeah so the next talk would be by
naveen he's going to talk about some
algorithms for independent component
analysis Thanks this joint work with the
central Impala and being shy from
Georgia Tech so i'll talk about
independent component analysis or ica
it's a problem that was first formulated
in signal processing and medical imaging
community in the 1980s and since then it
has found connections and machine
learning statistics there's extensive
literature on it so before I describe
the general I see a problem i'll start
with a toy problem which gives the
flavor of it it's called the cocktail
party problem it's a standard way of
introducing it i ca so suppose you have
two people speaking simultaneously and
you have two microphones recording their
voices and so now you have these two
recordings and from these two recordings
you would like to pick up our what these
people say so you would like to extract
the voices of each person so to describe
a mathematical model for this this is a
possible mathematical model it's a bit
idealized but it will serve as a
prototype for the I see a problem so we
model the voices of these two people buy
signals s1 of T and s 2 of T the s 1 of
T is a random variable which takes
values at time step 1 2 3 and so on it
has a distribution associated with it
and at each time step e independently
sample from this distribution to get the
value of s 1 of T and similarly for s 2
of T so these are the voices of the
people the two people speaking but we
don't observe that what we observe is a
superposition linear superpositions of
these signals so one microphone records
1 superposition other recourse another
superposition and these are different
because the microphones are located in
or different locations so g + 1 i'll
talk to that yes so at each time step
you sample s 1 of t independently sit
from the same distribution so s1 has one
distribution as 2 has another
distribution and at each time step you
independently sample so we don't know a
IJ is here we don't know where the
speakers are located so we're going to
ajs we don't know distributions of s1 s2
and the problem is to recover a IJ s and
s given the samples of x1 and x2 right
so this seems like an ill-posed problem
there is not sufficient data to to solve
solve it and that's true so we have to
either get more data or make more
assumptions so the crucial assumption
will make is that s1 and s2 are
independent of each other so they don't
not only s 1 of 1 is independent of s 1
of 2 and so on but s1 s2 also don't
depend on each other which is reasonable
in many applications also when two
people are speaking simultaneously
presumably they don't have much to do
with each other so so that's this simple
version of IC problem we can describe
what's happening here pictorially so
here I am making further assumption that
s1 and s2 are uniform interval minus 11
so if we plot them then the situation
looks right click this uniform
distribution in this square and what we
are actually observing is a linear
transform of the squares parallelepiped
and uniform distribution with this and
from this we want to recover what we
just observe these points and we want to
recover what linear transform was used
so at least in this situation you can
see that if you use sufficiently many
points you can sort of learn the shape
of this curve
you can record the universe form so so
that lets us to the general I see a
problem the difference now is that the
number of microphones is N and the
number of speakers is also n so we have
this model x equal to a s X is what we
observe SV is the hidden signal which
components are independent of each other
is a matrix constant matrix non singular
and the problem is to recover a and
possibly the distribution of s given
distribution of X so even now the
problem is not actually well post and we
have to make some further restrictions
to make it well posed so one one thing
is that if suppose each component of s
is standard Gaussian and they are
independent of each other and if a is
the rotation matrix any rotation matrix
then what you get back is again
independent Gaussian components so any
rotation matrix gives the independent
component so the problem is not solvable
in that case so we'll just assume that
no any of the components of s no other
components of s is Gaussian another
thing is that if I scale a column if I
scale the first column by a factor of
two let's say and I scale s 1 x factor
of half then I get a different a in
different s and they still satisfy this
equation X equal to a s so I cannot
learn the scalings of these columns or
scaling of a s so we will allow for that
indeterminacy and similarly if I permute
the columns of a by some permutation I
permute s by some permutation again get
X equal to s for these new a and s so
again I cannot learn this permutation so
will allow for these two indeterminacy
but that's all there is a theorem in
probability theory from 1950s dar mas
Ghita which theorem that implies that
given this assumption and if we allow
the indeterminacy then the problem is
that posed you can solve it given the
distribution of X and let me just
remarked that the additional learning
problem is very similar to this it has
similar model x equal to a s but the
modeling assumption there is that the
components of s are not necessarily
independent but they are sparse so it's
useful for different set of applications
so I'm just restating the problem here
the so instead of we want to recover a
and s given polynomial number of
independent samples of X we don't get
the distribution away so that's the icy
problem so before I describe our
contributions I'll show you some
algorithms that are known for this
problem so so one can try to use PCA of
principal component analysis for this
which so in this special case suppose
your distribution is given points in RN
or in r2 in this case and suppose they
are uniform in this rectangle then you
can try to use principal component
analysis to find the direction of
maximum variance or the second moment
and that gives you this direction this
is the direction of maximum variance and
that is also direction of independent
component and similarly you can find
this other direction so in this
particular case you can just use PCA to
solve I see a problem which so always
useful if if your distribution both if
you'll rectangle over the square then
can moment looks the same in all
directions so it does not give you an
information about the independent
components the independent components
are these two but this does suggest
trying higher moments and interestingly
that works so if we take the fourth
movement in Direction V V is a unit
vector and the fourth moment is just the
expectation of V dot X to be four then
the local minima reform so in this
picture this this is showing the fourth
moment in this in this direction this is
the fourth moment so the local minima of
F of V decide we correspond to the
facets of the skill and they can be
efficiently found using a gradient
descent type of algorithm and we can
estimate the fourth moment quite well
using this empirical moment from the
samples that we have so in this case the
problem is reasonably well solved
although it doesn't handle all possible
distributions for s but it handles large
class of them so so what remains to be
done there is a more general version of
ICA which is called underdetermined ICA
the difference here is that the
dimension of X vector and s vector are
not necessarily the same so the
dimension of X can be smaller the
dimension of s and otherwise the same
thing and now the problem is again to
recover a s we cannot recover now
because there is no sufficient
information so we are given in some
sense even less data compared to what we
are given before because X is smaller
dimensional so the algorithm that I just
showed you doesn't seem to apply to this
okay so here here's a picture of a
special case suppose s1 s2 s3 suppose m
is equal to 3 and s1 s2 s3 are uniform
in intervals then the Joint Distribution
looks like a cube uniform distribution
on the cube and suppose n is 2 then what
the data that we get is some
distribution in this this geometric
shape and from this shape you want to
recover the orientation of the skill ok
so here's the main result about a lot
underdetermined icing it there was some
previous work but that applied only to
some special cases and our result is
pretty general but I only stayed it for
some special special cases for
simplicity so n is the number of X at
least two Emmys tanam dimension of s
more than n at most n squared over 10
and suppose these assumptions hold
columns of a have unit norms which is
without loss of generality because you
can't learn the scaling of columns any
two columns of a are linearly
independent and sis are far from being
Gaussian there is a way to quantify it
but I'll not right here and first eight
moments of each SI are bounded if these
things hold then I'll go in some custom
eight columns of a within let's say L to
additive error epsilon using number of
samples which is polynomial in N 1 over
epsilon and run our Sigma min of this
power away that I will shortly describe
so it's a polynomial time algorithm
except for I need to explain what the
sternness and the time is also the
similar order
so i'll describe out this power of a is
this is called a cathedral square of a
so if your matrix a looks like this
these are the columns then cut lead our
square is obtained by taking cathedral
square of each column individually and
that gives you the new metrics and what
is the cathedral square of individual
column it's basically just the tensor
squared put into a into a vector so it's
an N square dimensional vector we are
taking all possible pairs here so a 1
square a 1 a 2 and so on so so this is a
if this was an n-by-n matrix this is n
square by n matrix
so I've just restated the result here so
we have Sigma min of the cathedral
square of a here but it's still probably
not clear what this means this this is
hard to interpret quantity because it
involves a funny operation when the
columns away so we can we have one
result which I think gives some meaning
to this to what this means so you can
think of the matrix a as being generated
by smoother analysis type of
distribution so suppose you start with
any matrix a of these dimensions and I
squared over 10 and then you perturb it
by adding independent Gaussian noise and
0 Sigma square to each entry
independently so the so we let's assume
that this is actually in input to your
ignore then we can show that Sigma min
of the cathedral square of the spirit
herbed matrix is small with small
probability so so in general with high
probability it's not too small so so
that gives us that bit probably at least
1-2 over and the probability is over the
choice of n as well as the random
samples are algorithm finds columns of a
plus n within small additive error with
one over Sigma square here now it's
writing makes more sense
so in the remaining time I will talk
about the garam for this for under
determined I ca so as I said before the
local optimization method doesn't seem
to generalize to 1m greater than n case
so what we will do is will build upon
another method due to erode or which was
given for m equal to n and we will
generalize it to em greater than n gets
so I will first describe the algorithm
for gyarados algorithm for n equal to n
and then I'll describe a generalization
so these are standard notions in
probability theory so x and s and a are
just as before we can define for random
variable x its moment generating
function by expectation of X expectation
of e to the U transpose X and if you
take the logarithm of that then that
gives us what is called cgf give me the
generating function similarly we can
define these functions for s random
variable s so it's the same definitions
now the crucial point here is that the
moment generating function for s because
in the components of us are independent
it factor eyeses into moment generating
functions for individual components and
similarly the cgf decomposes into a sum
for individual components so this fact
will be crucial in what we do next
now assume that the variables T and you
are related by this linear relation to
equal to a transpose u then e to the U
transpose X is equal to e to the U
transpose a else this is just by our I
see a model and then this is equal to e
to the T transpose s bias or assumption
and that gives us that the CG f of X
evaluated at Q is equal to C GF of s
evaluated at T and so now I just basic
calculus we try to understand how the
derivative of C changes under the you
transform so we can write this relation
this is basic calculus so the Hessian
matrix of C with respect to u is equal
to matrix a times Hessian matrix of C
with respect to t times a transpose it's
just you just check how derivative
changes underling a transform the
variables and that gives you this
relation now we have paulino polynomial
number of samples of x and using that we
can estimate all the entries of this
reasonably accurately but we don't know
the right hand side anything in the
right hand side we do have one piece of
information about it which is that this
has a matrix is a diagonal matrix and
here we are using the fact that this CS
decomposes is the sum of individual cgf
some individual components of s and that
directly gives us that only these
diagonal entries survive okay
so we would like to exploit this
information to to compute a and here is
a very simple idea you to hear a door
you sample you and you prime in our end
uniform thee from the unit sphere and
you evaluate your Hessians at you and
you prime and take the ratio so we
already saw this relation and now since
a is invertible matrix a transpose
cancels with a transpose here and what
we are left with is this quantity and
now the quantity here is a diagonal
matrix so what we get is this thing that
we can compute is equal to a times a
diagonal matrix times a inverse and this
is just a I can value eigen vector
equation doing it I'll need maybe four
minutes so this is just I can value
eigen vector equation and so we can
compute a s columns of eigenvalues
eigenvectors of this matrix so so that
solves this problem
and eigen decomposition is unique if the
entries here are pairwise distinct and
that's where we use the randomness of
you and you kind so we need to show that
pairwise these things are distinct and
actually far apart from each other that
allows us to find the eigen
decomposition okay so that's for m equal
to n and i would like to generalize it
to underdetermined ica so we will try to
proceed in a similar way but things will
break down so let's see where they break
down so they still have to equal to a
transpose you and we can still write as
before the Hessian matrix of C with
respect to you as disturb a shin and
then we can try to take this ratio so it
is fine up till this point but this is
incorrect because is not a square matrix
now so it's not invertible matrix so a
transpose and a transpose inverse is not
canceled so this does not work we can
try to take pseudo inverse here but that
also won't work so so here is the main
new idea we'll use higher derivatives of
C so we can take the fourth derivative
so this is a tensor now it's n by n by
order for ten sir but we are writing it
as a matrix and square by n square
matrix so you just flatten the transfer
into to it or into a matrix just like
you can flatten a matrix n by n matrix
into an N squared dimensional vector
this is cathedral square of a the
diagonal matrix the fourth derivatives
and then cathedral square n transpose so
this is this is easy to obtain this
relation and now will again try to
proceed in a similar way so we can take
the inverse
and similar to what we are doing before
but again you have this problem of this
not being a square matrix but this time
if we take the pseudo-inverse for this
second thing then things go through so
then this is indeed an identity matrix
if this is zero inverse and so what we
get is very similar to what we got
before and now we can obtain the columns
of cathedral square of a as by I
convector computation and once you have
the columns of this matrix we can also
get cousins of a easily so that's
basically all the algorithm there are
several technical challenges in making
this work which is one of them is that
we have to show that these things are
far from each other in order for I
convector computation to be effective
and all these diagonal ization and all
these things only happen in approximate
setting so you have to take care of all
those errors so that's it let me finish
with an open problem so our algorithm is
polynomial time if say n is at most n
squared or any poly n but suppose n is
very small n is a constant and n is as a
given number then our algorithm can be
super constant it can take super
constant time we don't have a lower
bound for proving that that's essential
so that's an open problem here thank you
ask on it gets a little more interesting
when you consider that the matrix is not
a transfer function right because then
there is a fresh equation between mines
signal in the microphone and then the
correlation with in a signal offense the
result but you also have the same way
that you have a gain uncertainty you can
create have lightning of spectra I
wonder if you looked more into no
actually I don't know much about yeah
and see how I don't know much about the
there are these modeled what I want yet
there was a little bit of work here in
the speech goof but I think today they
are not looking much into that it might
be interesting to revisit just do a
interesting problem solved
good okay so the last stop would be buy
a new or who's coming from muscle ache I
guess he talked about lowball someone
okay thanks I recognize that it's the
last talk of the day and you're tired
I'm tired so I'm just going to show you
a bunch of animations that i made in
last few days and then we'll we'll call
it good so so before I start talking
about the complicated title i'm going to
tell you a little bit about why i care
about these questions which i think it's
kind of important because maybe a lot of
people don't know why those questions
are important so here's kind of for me
the biggest thorn in my side is the
complexity theorists which is that I
have no idea how to answer this kind of
question okay if someone asked me here's
here's a problem that i'm solving i have
an algorithm for it is my algorithm the
best you can do okay i have no idea how
to answer this question you can make
this concrete anyway you want to pick
any non trivial algorithm that you know
about you know is the algorithm the best
algorithm that we know for finding
matchings and graphs is that optimal we
don't know it's the best algorithm for
matrix multiplication that we have
optimal we don't know and the same for
almost all algorithms that we have and
of course the famous p versus NP
question is also of this format okay
that's just like asking is the best
algorithm that we can come up with force
at optimal and again we don't really
know okay so here's something we do know
here are some things that we know about
proving when algorithms are optimal we
know that for many tasks if you have a
linear time algorithm then that's about
as good as you can do because you need
to read the input right and that takes
linear time so you can't do better than
linear time that's one thing we know and
and then there's the technique of
diagonalization which is really clever
and that allows you to say that for
certain kinds of tasks which would
succumb to this technique the obvious
algorithm is optimal one for example if
you are given a program as input and you
want to know
what does this program stop in T steps
then essentially what you what you
should do is run the program 40 steps
and see if it stops and that's about all
you can do you can't do better than that
okay but that's those are that's about
all the techniques that we actually have
for proving lower bounds on the right
time so people have worked on this
question and the most of the work has
gone into studying all kinds of
restrictions of the concept of what an
algorithm is so you can restrict it like
this restrict it like that and you get
all kinds of different questions and
then you answer those questions and
there's a long list of such results okay
so first I want to start by convincing
you that actually this is this this
question about algorithms is essentially
a question about communication and
that's because of the the following
picture which took me a long time to
make you can see that yeah the internet
helped a lot I still had to do things so
what it says is that if you have an
algorithm that computes some function f
that map's n bits to one bit then you
actually obtain a communication protocol
suppose the algorithm runs in time T and
computes this function f then you can
show that there's a protocol that that
has about t players that participate in
the protocol each player knows one of
the bits of the input okay and each
player doing the protocol sends one bit
too to some set of players and every
player here receives at most two bits
okay so let me just show you an example
so so this guy might send based on his
bed might send something to these these
two people this person might send
something to these two people now these
players know some some information they
use it to compute some bit and send that
to some other people and in this way in
every step somebody speaks and send some
bit to somebody and eventually after all
the communication is done someone in the
protocol magically knows the value of
the function that we're trying to
compute
so that's something in sure if there's
an algorithm for F then you have a
protocol that looks like this question
it's a circuit this is the circuit this
is basically a circuit yeah so this is
essentially equivalent to the concept of
what an algorithm is but it's really
hard to prove lower bounds against this
this thing and one of the reasons why
it's really hard is involved here are
our private channels okay somehow the
reason that this protocol is weak is
that some of the parties know some parts
of information and other people know
other things and you need to exploit
this this fact that not everybody knows
everything to prove a look to prove that
it's the you know such a protocol can't
work quickly and and that's something
that we basically have no idea how to
exploit okay I don't know any way to
prove how to exploit that weakness okay
on the other hand if I allowed all the
players to communicate by broadcast to
just you know shout out a bit that
everybody can hear then you can compute
anything with the end players because
everybody just announces each person
just in the house is one bit and then
everybody someone knows all the bits
we're done ok so then becomes a
meaningless model ok so I need to
somehow understand what the weakness is
in the private channels to prove lower
bounds against this okay but there there
are settings where communicating by
broadcast can be something that's
meaningful to exploit for example here's
a result that was proved by valiant in
the 70s made more explicit by Ruby shin
myself and paper we did recently it says
that suppose s can be computed in
parallel time log N and total work n so
you have some number of processors that
are computing on it computing the
function and the total number of
instructions that are executed is order
n but the total time this taken is only
or login so this is a log depth circuit
of linear size then
you actually get from that a protocol
with little o of n players okay where
each player knows a very small fraction
of the bits n to the point one fraction
of the bits of the input and the players
communicate now by broadcast okay so the
first player looked at some small
fraction of the bits says something the
second says looks at some other fraction
of the bits says something and now that
all the communications by broadcast but
eventually one of the one of these
parties knows the value of the function
and outputs no quantifiers what are they
so like who decides which plays which
means so ahead of time out of time based
on the function the players decide these
are the bits that I know is a bit yeah
and then they start communicating okay
so all yeah everybody decides ahead of
time what good thing someone must be
things someone must one person besides
you are as well obviously someone of the
fence so
I'm not sure what do you mean someone
sides so someone knows effects but the
person is fixed that person is fixed yes
yeah so the last person in the sequence
these people all speak in this order and
the last person knows f of X yeah okay
so roughly speaking these players each
correspond to a line of the program
that's being run on in parallel but not
all the lines because there are n lines
and they're only n over log log n
players but there's a subset of the
lines that are kind of the important
ones and those are the ones that are
simulated by these players okay so so so
this is something that I can hope to try
and prove lower bounds against because
we have techniques that their work
against communication that happens with
broadcast so let me give you some idea
of what we know how to prove about
communication so here's some really easy
results that that we know how to prove
all these all the proofs or many the
proofs I'm gonna tell you about here I
won't tell you the books but they're all
really short okay so for one thing we
can proof example is suppose you have
just two parties and each of them has a
subset of 1 through n okay so that's an
N bit string and they want to know are
the sets the same okay that's something
you can do it obviously by having one
party send other party you said that
will take n bits and this is the best
you can do there's nothing better you
can do that's easy to prove it's an
exercise another thing that you can show
it requires n bits of communication is
to compute the generalized inner product
of these vectors mod 2 okay so that's
the same as counting you know is the
intersection size of x and y even that
also requires n bits of communication it
also requires n bits of communication to
tell whether x and y are disjoint or not
all these things are easy to prove but
those kinds of results seem quite
different from you know where we want to
get to eventually because we want
understand this situation the problem is
in this
iteration the players have a lot of
overlap in the input all right they see
a lot of there they say a lot of bits
that are the same and that's quite
different from the two-party situation
where they have kind of independent
inputs okay and let me show you why
that's different so suppose we have now
three parties and they each each party
has a set and they want to tell if all
the sets are equal then again it's
really easy to show that actually it
requires n bits of communication or
order n bits of communication to do this
Omega n it's the communication do this
but what if what if each party knows two
of the sets how many bits of
communication does it take to tell
whether the sets of the same or not two
bits and why is that
yeah yeah so you just you just need one
person to verify that x is equal to Y
another to verify that y is equal to Z
and then you know that all three must be
the same okay so suddenly the
communication that you need drops down
dramatically for this problem okay and
so it's hard to prove lower bonds when
when the parties start to have
information they can see in common
nevertheless we do have methods that
work against this kind of model the
really beautiful paper of bobby nissan
and sega d the early 90s showed that if
you want to compute the generalized
inner product of XY and z then that
still requires n bits of communication
in this model okay and the subject of
this talk is understanding for a long
time it seemed like that technique did
not work when you're when you're trying
to understand if the sets are disjoint
or not okay and that's exactly what we
worked on there there are several
reasons why this particular function is
interesting in theory understanding it
has applications to prove complexity and
basically other applications in
complexity theory that I won't get into
okay but for me the main motivation was
you know I want to find a technique that
works eventually for the model it's
related to algorithms and this seemed
like something that we have to
understand first okay this seems like an
easier problem so here's here's kind of
what the story is for this problem and
our result is at the bottom here but
basically there's been a law long line
of work that used sort of increasingly
complicated techniques until our work
which reduced which uses the basically
the same ideas is bns as this ball by
nissan density but yeah so eventually
what we managed to show is that if you
if you have K parties that have K sets
and each party knows
k minus one of the sets then and they
want to know whether the sets are
disjoint or not then they need to
communicate at least n over four to the
K bits of universes of size N and that's
that has more or less the right
dependence on N and K because there's an
upper bound there's a really clever
upper bound of grolsch who showed that
you can compute this in k squared n over
2 to the K bits of communication yeah
lower bones or be able to learn of mine
yes yes so all of these lower bonds also
apply to randomize communication and we
also reprove so our proof of the
deterministic lower bound is really
simple the proof of the randomized lower
bound is not so simple but we simplify
the the best proof before which was this
work of sure stuff that gave a lower
bound of roughly square root in ok so we
also reduce the complexity of that proof
but we don't improve the bound we get
exactly the same bar ok but this is
deterministic p this the upper bound is
deterministic yeah yeah the lower bound
is really interesting because there's a
quantum upper bound that matches this
lower bond okay and the proof works also
for quantum the lower bound proofs also
work for quantum so for quantum the
lower bound in this lower bound is tight
okay so somehow if you're going to get a
better lower bound you need to
distinguish quantum communication from
classical from randomized communication
what does night when did I start I
forget okay so now I'll quickly sort of
outline how the proof goes I won't
really show you many details I'll just
give you some starting starting point so
we're in the setting where we have K
parties there K sets each party knows k
minus one of the sets and I want to know
if the sets are disjoint or not roughly
the idea is usually the way the way
these kind of proof works is you you
find some hard distribution okay find a
hard distribution on the sets and argue
somehow that that on this distribution
players cannot succeed with with the
hyper booty so so I'll now define a
distribution on on these sets to do that
let's partition the universe into M
parts okay and each part is going to be
a size roughly for the king and then
within each part i'll tell you how to
sample all the the k sets you sample the
first k minus one of them uniformly
random randomly conditioned on the fact
that all came all of those k minus 1
sets intersect in exactly one point okay
so the first k minus 1 sets intersect in
exactly one point and the last set is
just a completely random set okay so
that's how you sample those sets in the
first part of the universe and you do
exactly the same in every part of the
universe
DC Universe so the universe is n
elements and now I'm just describing how
to sample the sets x 1 x 2 through xk as
substance of these and elements so you
take the n elements and break it up into
m parts ok and the first part the way
you sample it is you use sample the
first k minus 1 sets so they intersect
in exactly one of those elements and
then the last set is random so the
chance of having an intersection in the
first part is exactly fifty percent it's
the chance that the random set has this
one point of intersection of the rest ok
and you do this independently in all of
these m parts of the universe that's the
distribution now it's a really strange
distribution because with extremely high
probability there will be an
intersection of these k sets right the
chance of an intersection the first part
is half so with good probability there's
going to be intersection somewhere
nevertheless it's it's useful for the
analysis which is I think what makes the
whole proof really counterintuitive and
maybe it's why people missed this proof
before yes yes although later we'll also
use the same kind of distribution for
the randomized lower bond and they will
lower ok but then will mean I have to do
something more complicated ok so so now
for now we're not allowing errors so
let's let's set let's d let's allow di
to be the random variable that indicates
whether or not there's an intersection
in the ayah part of the universe ok so
that's a random bit then here's a
theorem that occurs somewhere one of the
older proofs it's due to shirts off he
showed that if pi is a protocol that's
computed using communication see if pies
the function is computed by protocol and
communication see then this this
expression holds ok the expected value
of pi on these sets times basically the
parody
these D is that the absolute value that
expectation is is small sponsored by 2
to the C minus 2n ok so in some sense pi
this this kind of says the protocol pi
is bad at computing the parody of the D
is but if you're trying to prove a lower
bond that's not exactly what PI is
trying to compute anyway it's trying to
compute the disjoint pneus which is just
like computing whether all the DS are
set to one so but but where we can prove
is that it's bad at computing the parody
of the DS but in the deterministic case
basically where we observe this that's
enough right so if pi is a deterministic
protocol and makes no errors then you
know when pies when when the sets are
disjoint then pi is equal to one but
then all the bees are equal to one so
whenever pi is equal to one this sum
here is exactly equal to m and when pi
is equal to 0 everything is 0 in this
product okay so that means that this
left hand side is PI computes disjoin
this is exactly equal to 2 to the minus
M it's just the probability that that
the sets are all destroyed okay so the
left hand side is to the minus M the
right hand side is this your rearranged
and you get that the communication must
be at least am okay so it's kind of
confusing that something like this can
happen but it's actually really simple
this is pretty much where is this thing
it's basically the same idea as Bobby
nissan and sega d which i guess i don't
have time to get into but you can it's
it's it's a two page proof and it just
you repeatedly apply cauchy-schwarz to
this expression until things become nice
well I can say
but I'm not I don't have time to to
explain it but it's just cauchy-schwarz
let me give you an outline of how the
randomized lower bond goes so this you
know we can't use this kind of thing now
anymore whoa what happened because here
I really use the fact that pie does not
make any errors okay because if pie
makes an error then it's no longer there
there could be places where I pi is
equal to 1 and this thing is not fixed
okay but nevertheless here's how the
proof goes you define this function f j
which takes an input j and it's simply
the probability that the protocol
outputs one when the number of places
where the the blocks are disjoint is j
that's the function f j and then you can
use the kind of bound that we saw in the
last slide to get this kind of algebraic
expression for f okay it's not too hard
to get here but it says that if F J is
defined as above then it must be the
case for all that for every artist
bigger than see this expression f of the
sum of these d 1 through dr times m / r
times this parody is small an
expectation ok this is kind of very
analogous to what I was seeing seeing
the what we were seeing the previous
slide this is the expression that
corresponds to PI but I'm not going to
tell you how we go out there okay and at
this point is this completely we forget
the fact that we're working with
protocols it turns out that you can use
this fact to show that F can be
approximated by a degree C polynomial
okay once you have this thing and that's
things something that takes a couple of
pages to proof but it's again a little
bit tricky it's not that straightforward
and once you know that fact you know if
F computes computes disjoint pneus then
f F F F came from a protocol that
computes disjoin this then f is supposed
to look something like this right f the
probability that the protocol outputs
one is supposed to be 0 when the sum of
the DI is less than m and only when all
the blocks are disjoint is it supposed
to accept so kind of f should look
something like this it should be close
to zero and all these points and then
suddenly spiked up to to something close
to one but we know it was proved by a
nice on integrity that any such
polynomial must have degree square root
m okay so we know that F can be
approximated by a degree C polynomial
and any such polynomial must have degree
squared M that kind of shows that C has
degree roughly square ok I guess I drop
the K terms miscalculation so that's
roughly what happens in the in the proof
okay so I'll stop stop there I'll just
spend a couple of minutes telling you
what I think should do next I think the
big open problem here is to prove all
the lower bounds you know how to prove
4k parties in the setup or the type and
over 2 to the K and the big open problem
is to prove a lower bond that's really
like N or n over K and over polynomial
in k and here's the candidate problem
that I think should be hard for this
model so imagine that you have the input
is K matchings ok so there's one
matching between these two sets of
vertices and another matching between
these two and other matching between
these two and so on and what we want to
know is if you started this designated
vertex and keep walking clockwise for n
over 100 steps and is this the number of
vertices in each graph in each part here
then do we end up at an odd vertex or
even vertex ok
so the trivial way to do it is you know
you know figure out where this edge is
going then figure out what the next edge
is going and so on and there should be
essentially no better way to do it than
that with communication ok that's a
candidate but but I will warn you that
there's a really clever particle they
can do do something really clever you
can try and figure it out for yourself
but actually it's pretty hard so it
turns out that if you if you want to
just do one round if you want to just do
three steps and there are three players
there's a protocol that can figure out
where the third step goes in little o of
n communication ok so so it is something
there's something non trivial you can do
that but then I don't believe that you
can you know that protocol can be
extended to get something that figures
out how a really long path goes ok and
that's also basically related to what I
think is the hard problem for this
question for parallel algorithms it
should be hard to tell when you're given
a directed graph where every vertex has
out degree one it should be hard to tell
if the vertex one is connected to vertex
two simultaneously in log depth and in
in long time and n work I'll stop there
as we wanted to ask a question can you
go back to the luck that second know
about the previous one so exactly so
everybody has not everybody has
everything except like the place i'm
excited claire has everything except
excited yeah sure so every player sees
like basically a matching from this the
first player see the matching from this
to this and it just doesn't know what
the matching is between these two layers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>