<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Small Image Sensors and Big Visual Data | Coder Coacher - Coaching Coders</title><meta content="Small Image Sensors and Big Visual Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Small Image Sensors and Big Visual Data</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0Xzp3sU5z0o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
morning my pleasure to welcome li Shang
here I've known him for almost 15 years
probably since he arrived at the
University of Washington across the lake
as a graduate student did a lot of great
work in computational photography back
then and continuing to this day I guess
about five or six years ago we lost him
to the other u-dub which is University
of Wisconsin and he's going to talk to
us today about some new image centers
that are coming out and how to use those
sensors both for understanding the world
and for producing great images I guess
okay yeah thank you Michael for the
introduction and thank you for having me
here so let me start the talk by showing
you a very interesting picture so this
is a picture of taking in 2005 where
when the Pope was announced that was
eight years ago and there's another
comparison picture like this so I guess
for researchers in this field when we
see this we got very excited because the
mobile devices are really a part of our
life and the way so we're using it take
pictures if we can do it do anything on
the camera or to the pictures that are
that are taken we either way very very
likely may you know impact our daily
life hey so as researchers I think I'm
interested in several questions in this
in this in this area so why is usually
when we want to take good pictures ways
to use a big camera so now most likely
we're going to use sort of small cameras
can we improve the imaging performance
of these small cameras to make it more
comparable to this okay so that's one
type of questions like at the end for
example dynamic range low light
performance and the second question is
okay once we are able to very easily
take questions we're going to take
pictures we're going to see a lot of we
we'll have a lot of pictures how do we
are browse and navigate through these
images okay so there's a third question
which is how do we extract some
knowledge is from these pictures like
doing face recognition or more generous
or object segmentation and the
recognition okay so I have done a little
bit work in these areas so I will
discuss this these works with you I will
start with the first one so high dynamic
range images so this is a high dynamic
range images are important so you can
use iPhones these days to take HDR
images and so as we all know it the way
it works is you take a succession of
pictures with different explorer times
so this learn longer wider bar means a
longer explorer and if we merge them we
get this nicer looking picture okay so
you can see that the reason we want we
want the longer Explorer is that this
part is so dark if we use for explorers
we cannot see too much but if we use the
long explorer then when the scenes is
moving okay so we're going to have
blurred here okay so then that becomes a
problem so not too many work have
addressed this issue and in that address
the blur issue when we reconstruct the
high dynamic range images okay so okay
so if the issue is you know we have a
lot of noise in the stark region ok so
maybe an alternative is this so we'll
just take a succession of short Explorer
pictures and maybe we can somehow remove
the noise by service somehow aggregating
all the measurements here okay so this
is another alternative so then the
question is okay which is better shall
we do this trying to remove the blur
here or shall we sort of it sort of it
take us to remove the noise I in this
sequence
so and because when we when you use a
short equal time within a certain period
of time you can take more pictures okay
so then the four there's this design
question okay which sort of a scheme
through the way use should we take a
very short and a sequence of the noisy
images where you should you know take
this type of sequences and I think in
two thousand in 2010 we had a papers
with either analysis comparing these two
schemes so our conclusion is that this
is tend to work better in practice so
you want to estimate the motion between
these frames and then remove the noise
and there are some more general analysis
from shreena our school which also
compare this type of image capture with
the other sort of a coded aperture
imaging for example flattered shutter
and Sora coded aperture imaging to see
okay which is better and in which
scenario okay so i will show you some
examples of my word so for example is
our way to dim the light but that's why
oh I say uh-huh okay so for example in
this low light condition we take a
picture of this birthday okay and if we
just blow up sort of amplified and the
pixel intense days we tend to get this
okay and so the idea is okay how about
we take a very short sequence of noisy
images and then you can and this is like
a total mapping result when you do this
there's a very likely your hand may be
shaking so you take a very short safe
and so you get you can recover all these
nice details so this is one example so
let me show you another example
so this is a noise level and
to this there's motion this motion
smoother okay okay so this is a sort of
a demonstrative concept okay we can
capture a sequence of images then we can
still reconstruct the high dynamic range
images ok the question is okay is it
this problem is the problem soft so we
talk about it it's not quite because
what does that means if you want to take
a high dynamic range videos on my cell
phone then your cell phone has to you
know work at this very high frame rate
then usually that means you're going to
consume a lot of power okay so so then
we thought about okay so if we do we
take high throughput high frame rate
because we want to avoid the motion then
sometimes maybe the scene is not moving
so maybe we want to do this image
sampling in a more adaptive way meaning
that if they're smooth no motion we want
to slow down what we can use a longer
Explorer time okay so that's the another
work we did so use the st motion
estimation to control the explorer time
so usually the explorer time is
controlled by the brightness if it's
bright enough you use short exposure
it's dark then use long explorer so but
we can use motion to control the x4 time
okay so I I think three I think maybe
three years ago I think there are some
cameras can do this but there's only
only for static shots so now we haven't
found sort of a video cameras that can
do this so we build a prototype 2 3 and
tweaks and to experiment with this
concept so i will show you this how it
works so this is like a regular video as
we're moving you can see that once we
move and this is a constant constant
next were then things get blurred okay
but here once you start moving think
things gets noisy you see the you see
the difference so here once it moves we
have an underlying through motion
detection you get a noisy reconstruction
and you get the noisy capture because we
reduce the shutter speed here the expert
on is constant so you get okay so the
idea is okay maybe from this type of
measurement we can get a better image
reconstruction yes we should estimate of
Linda in our shoes and so it's like a
shorter image-based like locusts kannada
type of motion estimation just be fast
enough to drive the camera and this is
already high speed capture oh right so
we did this we didn't do this on my
phone we have a laptop connected to a
point grey camera so the motion
estimation is done on the laptop yeah
any other question okay so okay so then
so you can see that talk at a particular
moment if this thing is moving fast
explore and in this capturing scheme is
noisy and in order to evaluate to this
with you let me evaluate to this scheme
with you some synthetic experiment
suppose they have a panoramic image and
the words will have a simulated camera
moving this way okay and in the first if
you look at the dashed red line that's
the signal noise ratio in the input
meaning that we use a constant shorty
Explorer meaning that every frame is
very noisy so the signal-to-noise ratio
is very low and then we can apply some
video de-noising method for example we
use the liver and three nines method I
think that's maybe three years ago yeah
you can pump up the signal noise ratio
to this okay so it works quite nicely
but if you use out through our server
adaptive sampling originally it's here
it's sort of a static so that means the
signal noise ratio is pretty high okay
and then you start Smoove so this is
moving fast then you reduce the export
I'm and suddenly the noise ratio
signal-to-noise ratio becomes low and
then once you come here the stops and
the move stuff so you see that if the
stash the black line is the
signal-to-noise ratio in this adaptive
captured in this motion based export
control capture ok and then based on
this you can do some video de-noising
them we can get a better sort of a
signal to noise ratio consider for the
school for those good input images we
get a very high signal-to-noise ratio
and we can we can get some improvement
like like this is all single frame so
well actually it's a sort of aggregating
multiple frames so
the solid lines are aggregated yeah yeah
the key benefit that we can get is we
have very good friends here sort of a
very good friend here so that we can
compute an optical flow and if we give
more ways here more ways here so when we
do the reconstruction we can get higher
improvement we can get better because
why not go all the way down where the
red dashed lines actually it depends on
the sea is here it touches the red line
yeah
who doesn't need to feel exposing longer
that means hear it move is really fast
so this is a sort of a conservative way
of setting is for her to make it very
short see if there's a motion machine ha
ha ha ha if if everything is moving then
you probably end up so that's the worst
case you probably end up with something
like this because we keep like right
right right right right
ok
so here are some comparison so this is
again so this is a synthetic examples so
that we can have ground truth so this is
the constants where home is it's a
blurry and this is noisy input so we can
apply this will have c PM 3d method on
each frame to get the noise removed and
this is another raloo and freemen method
to remove the noise and this is our
method you can see that this is a our
method is more similar or in terms of
the sharpness is for a more comparable
to this this is a this is a almost the
same slightly a little bit blurry so
this is the ground tools and if you if
we compare the signal-to-noise ratio so
this is the input the red solid line is
the input and we have two other server
BM 3d method and Leo and a free members
out there somewhere here and sort of our
method you can sort do better ok so
so this is another example you can in
this suitcase I think both the Leo and a
free man method in the BM 3d method sort
of over smooth the result a little bit
and this this sort of a in this result
the difference much is more clear so
this our result is more comparable to
the ground truth so the key reason is
that our methods were a switch between
the speciality noisy and the temporal
denoising if the motion estimation is
unreal I unreliable we use a single
frame otherwise we use sort of temporal
information so again so this is the
comparison so this is black line and
black line is our result ok ok so then
question is okay is this is this all so
not really so can you think of any
issues with this ok so there's a some
new shoes first of all when we use the
motion to control exporter we can only
measure the previous two frames right
then use the motion to predict the next
frame but this prediction is always
delayed so if you're studying moving
slowly suddenly you move faster then you
know the next frame is wrong ok so and
not right so that's the main problem in
the end we always have some sort of
blurry frame in the result in order to
enhance this one we have to somehow so
doing some sort of a registration
between a sharp one and blurry one I
mean I am use run it 200 Hertz or
something like that right soon and most
cell phones have some sort of inertial
okay mm-hmm right all right so but the
more they're even if the camera is
absolutely static there could be seen
motion so so that that kind of motion
need to be estimated from the from the
frames yeah so okay if we then you have
to do some registration between the
blurry and the sharp image that becomes
a tricky optical flow question because
the brighton is constancy is is violated
so what you if you just use a regular
flow that you're going to somehow move
these pixels to match with these blurred
intestine patterns the flow will be
wrong ok so we're addressing this
problem in the paper in lasta cvpr it
works quite nice for the interior so the
basic idea is we want to compute a flow
such that if you use that flow to blur
out this one and then the blurred
version will match up with this guy ok
so so that's the basic idea so it works
quite nice with these interior regions
but near the occlusion boundaries the
depths discontinuity it's very hard it's
very hard so so that I think we can
still keep improving writing papers but
in terms of having a real robust system
I think this is probably not the way to
go ok so then we sort of come up with
this idea and this sort of a randomized
extruder scheme so if you look at this
this is a yella stration or 10 pixel
camera you have a 10 pixel camera so and
so let me just say how it works so for
example at time I at time I you only
read three pixels these three pigs
horses pixel one ok you read pixel one
and the pics one has been exposed for
two frames so this is the Explorer time
ok and then you also read pixel six ok
so pixel six has been exploded
44 paid for frames okay that's thanks
for time you also read this pixel 9 and
it has a shorty sore huh ok for the
pixels that you're not reading at this
frame it's going to continue to expose
for example this white pixel to you
don't read it so it's being it continues
to expose okay and then you come to the
next time instant ok we're going to read
out at that at this frame we're going to
read out pixel one with the 64 time and
the pixel to with the 64 time does that
make sense and then and also picks 06
with the 64 time ok so the basic idea is
for another example at this frame we're
going to read out pixel three with the
64 time and I guess pixel 7 with the
slowest work ok so the basic idea is at
each time you're going to select a
subset of pixels and read them out and
yes don't need yeah the color doesn't
mean anything distinguish one segment
right right right right right
and so there are no gaps in any of the
horizontal rows writer X rather than the
black line indicates readout and then I
mean age you start exposing right right
right so I sort of a it's a color well
you can say that yellow means yeah
yellow means eight friends and the Scion
or the green means for corporation so
for pixel three it seems like sometimes
you suppose it for very long sometimes
very short uh-huh right
it's just random yes Brenda yes right
right now I surrender okay yeah
any
okay so so from this okay it's like we
measure the sum of photons in this
interval right and with sum the sum of
photons in within this interval right
we'll gather some sort of reduce the
number of photons then we want to
recover the number of photons in this to
resell and in this sale in this sale so
you can do that we can care we can
recover this high-speed frames and also
because we have this inter lay in her
leave the lungs for and shorts were
probably we can recover the high dynamic
range one okay so cohab so there are
some at least the potential advantages
of doing this why is it has one hundred
percent nearly one hundred percent
lights throughput we're not wasting any
life and it you can potentially we can
resemble taneously cover HDR and
high-speed videos and through we don't
just constantly doing something entity
has reduce the power consumption and
also I think that probably the very nice
thing is it can be implemented on a
single chip because it only requires
this partial rate of so I was talking to
some people at CVP our last week so this
one does not exist right now so
everything we do is simulation but for
the next generation CMOS sensors we
might be able to do this so right now we
can control parole not per pixel yet
okay so so this is like the first
experiment we did so we didn't design
what we didn't we haven't figure out
what's the optimum pattern yet so right
now we only just do a 4x4 time one two
four eight and randomly permute them
assign them to each pixel's okay
so okay so how do we do the
reconstruction so we have these
measurements those measurements are
constraints right but the cut number of
constraints is less than the number of
pixels we want to recover we need to
have some additional regularization okay
we do this sort of a block matching so
within the space-time volume is you so
let's say if we consider let's say this
is a four by four by four a sort of a
spacetime patch and if we can say okay
these two ones these two are very
similar so that gives us a
regularization right so and typically
when we work with regular videos this
sort of a spaced on volume is very is
not very robust because of the motion is
very fast or the template something
temporal sampling rate cannot compare
with the spatial one but here each frame
is this like a high frame is a this Fred
we have much higher through temple on
temporal sampling rate so each for
example we can be talking about like 200
frames per second so that's why this
space-time volume makes more sense in
this scenario ok so then if you look at
this we want to compare this volume with
this volume it's a little bit tricky
because the sampling we don't exactly
know the pixel value here right when we
compare this with this so how can we do
the comparison to say ok this block is
similar to this block so essentially the
problem is like this so we have so this
yellow bar ok so this yellow bar is is
this one ok and we consider here we have
this magenta magenta bar and the yellow
bar and a green one here so this user is
that right yeah i think this green
magenta a blue yellow so this is a this
row ok so we want to compare these four
pixels with these four pixels
okay how we're going to do this okay so
we cannot do it directly so instead we
created some sort of a virtual sample
create a virtual sample meaning that
what will be the measurement here we can
we reconstruct somehow approximate the
sample at the distal okay at this
location okay and when we construct this
we really need to because this this
measurement here this sample is for the
whole eight frames right so we have to
consider okay what would be the virtual
measurement for these whole eight frames
okay so what you can do is you can do
some sort of weighted blending so you
take half of this pixel it take so the
total value here and to have value here
okay you do a weighted average to create
a virtual sample for this location does
that make sense so then you can compare
these two values okay so if you do that
you can you can come you can compute a a
score sort between the difference with a
score to measure the difference between
these two blocks okay so if you do that
the cancer for you you can say that for
each block what are the most similar
blocks within this volume that will give
you some regularization for this whole
process so I will skip the details i
will show you some result so this is the
sort of the coke and our coded sampling
input ok this is the our reconstruction
result this is a video let me just first
go through each blocks this is the
ground truth and this is the so called a
4x4 time what does that mean is we have
a regular video and the Explorer time is
four frames is four frames on ok so this
is a regular video and the expert I'm is
only one frame long ok so this is the
but for this one it's going to be noisy
to apply denoising to this frame will
get the noise the one frame so let me
play the video here
so you can see that these motions are
sort of our turkey because the frame
rate is low okay so here we can
reconstruct this hi Marie high frame
rate output so why that's why it's it's
smooth so let me do at home mapping so
you can better see it
ok
okay so this is one thing so there's
some sort of close-up comparison between
for the details so i guess the result is
a little bit heart diseases at so that
word result is a little bit sharper than
sort the noise sorry denoise the short
weeks were sampling so this is another
result that shows that this method is
reasonably robust to complex motion
the white people are coming closer in
the bottom middle of moving faster oh
here is the thing so let me go back
so you can see that just for fair
comparison if we consider one two four
eight in total we have a length of 15
right we have a length of 15 frames and
for this 15 frames only sample four
times so essentially we're reducing the
sampling rate by a factor of four so to
do the affair of what to do a fair
comparison when we generated regular
videos we only sample every four frames
so that's why you see that for the
bottom row on that these are shorter and
long four frames videos they they are
there jerk here yeah any other questions
okay so
I guess I have a question just how
physically realizable is it that have a
different exposure in each pixel
actually if you think about it we don't
need to set it we don't need a set it we
just need to control when to read it so
we don't have a sorry what you just have
to read it how can you know the given
current silicon and the row column
architecture on sensors and so on have
you be different pixels at different
times that's a good question so I I
don't know circuit design but I ask that
you see faculties they think this is
certainly doable because it's a purely
of VLSI problem you can give a mask they
can read out whichever pixel you want so
right now you can we can already sort do
a per roll reading yeah we cannot do
this per pixel reading yet but I someone
told me as the repair that this per
pixel reading is also possible with
potentially I think it's possible that
in the next generation CMOS sensor this
one can be achieved yeah
so if it's puzzled by Rho could you
right now though they're sensitive the
first row by one in the second row
before yeah that's that's possible we
can do that I suspect the result will be
in principle the whole that all the
method in our paper can work with that
but my gut feeling is the result might
be a little bit worse so this is also
confirmed and I think in Columbia group
they try this if you have a per pixel
control you tend to get better result in
the nut for this project they did some
other project they get the same sort of
empirical result yeah yes so I guess
depending on like how far as your
readings are um I guess there's a range
where if you read like many many pixels
it might be just as efficient to just
read everything and anymore uh-huh right
okay after just setting like locations
and then reading those who you have a
feeling for like where that spot is
because then you don't
really need that extra logic to be able
to be from individuals that's a good
point so I think that then the special
case is you just read every pixel at
every high frame rate at every frame
right so but we're talking about it's a
300 frames per second then if you
operate at that mode it probably it will
consume a lot of energy so one I guess
one argument is compressive sensing guys
are making us if you do that if you do
we do if you reduce the sampling rate
you will save power consumption yeah we
also pixels versus single pixels mm-hmm
it's possible it's possible using the
same idea for mosques so in the mosque
if you have a 2 by 2 square mm-hmm
cheaper to read the whole square
together versus single pixels because
seems like there is possible to get some
more humanization I didn't hearing lots
of pixels in this diagram right all
right yeah that that's a possibility we
haven't takes for that that yet
it seems like sometimes the dynamic
range and seeing is going to be limited
you don't need the 128 X right right
right right you wanted to turn off the
8x ended it right you only the one tool
for once before yeah yeah yeah just
don't have that much time agrees right
right right what happens this dagger it
doesn't change the doesn't turn you're
right all right right right yeah in
conversely sometimes maybe just the
highlights
this is a very small range as well as
well area that requires if it's a very
different sort right right right short
exposure to front you're sacrificing
the rest of the scene to capture that
one right all right right right right
yeah right oh well we're generally you
have really talked about spatial an
education right we haven't talked about
this right right we haven't done that
yet but I know there there's one I see
severe submissions we're trying to do
this type of thing not for my group from
other groups yeah
so did I show this I probably show this
right so this is another example so this
is also
it's so dark i guess i will skip to
those is just that you have a set of
rolling dice those are all sort of a
hard examples if you want to do a
traditional optical flow based method to
do these oops sorry
so there's some artifacts here but it
doesn't serve it downgrade to
drastically so that's something that we
feel this approach might be a promising
to handle really robustly for fast
motion so this is shows that he'll
underlying this we have an iterative
method to do the reconstruction this we
use this in called alternating Direction
method of multipliers this myth right
now the method is slow it really
requires quite some iterations to
reconstruct the the highlight because
for the highlight a lot of pixels the
readings are sort of an invalid we have
to somehow feel we in these values ok so
this examples were shows that the
limitation of this method if you see
this
let me say this again
I stop here and when you compare these
method you can see that our method
cannot completely completely remove the
blur right it's sort of a removes blur
for for these these locations which are
closer to the center where the motion is
slower and it's better than the longest
were time but it for this very fast
motion we still cannot completely remove
the floor ok so that's right now the
limitation but one way I think one way
to address this is to somehow adaptively
you can throw in more shorter extra
frames to address this if you really
want to get everything sampled correctly
ok so I'll skip this some future work on
the optimum or adaptive Explorer
patterns and the more efficient
reconstruction and some real-time
preview right so right now we can
capture this this one doesn't look so
good so maybe we can have some fast way
of generating not not the perfect video
bizim a little bit blurry video but so
regularly looking so that we can preview
the content
okay so that's sort of a one part of my
talk to the I'll continue on this the
sort of the next thread which is using
multiple cameras okay so when we talk
about camera raise I think maybe 10 or
10 years ago all the arrays are big so
but now more recently we have seen this
much smaller erase this is a so you can
buy this from Point Grey and we have
camera raise up appearing on a cell
phone and also so this is a paper in
nature this year so we can really make
the camera very small so each of this
lens each of this little bar is the lens
behind the lens there's a photoreceptor
says you can see that this is a dome
shape the camera array so that this is a
two millimeter the whole thing is like a
fingernail sighs so okay so there's it
potentially then we have we can later in
the future we can have this type of rail
appearing on our mobile devices so one
question is okay what kind of image
processing can we do for these type of
exotic exotic devices okay so one thing
I did several years ago is ok if we have
multiple image measurements and because
of the camera is very small so the image
quality won't be very high so it's going
to be noisy can we aggregate these
images together to to get a higher
quality image give you a sense this is a
sim if you work in 3d reconstructions
you know that this is a synthetic is
that example um so this is the one sort
of a noisy image ok so in this work we
assume that the noise is is dependent
dependent on the image intensity has
this Poisson distribution
okay so you can we can compare sorry so
this this is the ground truth so this is
our reconstruction it's sort of it
there's a pretty reasonable job except
to see that for the textural region of
regions so i think i will sort of skip
the details and just to show you a cup
of result for this work so for example
in terms of the noise reduction we
compared with sort of this the best a
single image noise reduction ok so this
PM 3d method many people use it as a
benchmark and there's a pretty good job
for removing noise but it also has this
color array appearance and this is a
thinking markula voice group the
synthetic aperture denoising it's really
because it operates at each pixels or
independently in a sense it doesn't work
as well so this is our method it really
is we recover the details quite well
compared to the ground truth so this is
another synthetic is an example
so this is the real example all the
images are captured using Point Grey
cameras which the shortest to explore
time and highest again so this is quite
noisy and this is a single image
denoising and this one actually what we
did is we capture 25 images and we sort
of tweet this 24 these 25 images as a
video and the feeding the video these
noisy method so they have a video
denoising Marge as well surprisingly it
doesn't it doesn't just improve you just
don't get the better result if you if
you just if you have more data the
reason is when you do the UCM am a video
de-noising you still have to do the some
sort of a registration across the frames
and if you treat them as a video then
the degrees of freedom you're computing
optical flow the degrees of freedom is
much larger but when we treated them as
a multi-view denoising there's epipolar
constraint we can use so we can get
matching much better so that's why so
you know or result we can get quite
better result
we have 25 in this predictor yeah in
while there's the same camera we just
put other put it at different locations
we're agreed orlinda in line you know
one for the for this one I think it's a
great
well this is a fine example that shows
that we can handle the signal intensity
dependent the noise as well so when we
do it image denoising we usually have
this better provide a parameter which is
the noise noise standard deviation so
but if we have if we have sort of a
Poisson noise if you set this value too
low then you don't remove the big noise
so if you set this variance too large
then use your kill details as well so if
we provide that sort of correct model
then you can't remove the noise but also
keep the keep the details so that's one
sort of feature in this work ok so so
how many views do we need we did some
empirical evaluation this is the
horizontal axis is number of views the
vertical axis is the the signal-to-noise
ratio you can see that some after a
certain image after maybe 20 the
performance doesn't improve as rapidly
as we hope so i guess there are two
possible explanations why is as you have
adding more cameras so all these cameras
are front looking so the common error is
they can see it becomes less in the less
so that's one thing so the second thing
is we're not purely just they the noise
reduction is not purely dependent on is
it ok so the noise reduction needs some
redundant measurement and the
measurement redundancy does not
completely come from these multiple
views within a single image you can
still get redundant measurement because
self-similarity right so if we have
enough views too how about to help up
and to help us to find accurate self
similarities within a single image then
probably does that's enough we don't
need infinite number of videos to to
sort of a get a very high quality and
noise reduction
okay so that's that's for noise
reduction we also did some work on
exploiting camera raise for video
stabilization so this is these are some
professional solutions for videos to
blood relation you have this mechanical
devices to damp out the motion which is
not very comfortable to use for
consumers so and we have a many sort of
a algorithm based video stabilization
method and they tend to assume what you
they tend to assume distant the scenes
and the similarity based at camera
motion and we also have some hardware
based the solution lets you have a
floating sensor to compensate motion or
you have a floating lens to compensate
motion so all these works are have
limited degrees of freedom and the
motion cannot be too big so the basement
baseline is limited so this is a paper
in from siggraph 2009 so this is the
type of scenes that we typically apply
stabilization on ok and so usually we
have these assumptions so the background
needs to have a lot of features we can
do tracking and so the time dynamic
targets need to be small so this is even
in this cvpr I saw a video stabilization
method which sort of works in the
similar regime so the type of things
that will break the video stabilization
is something like this ok the motion is
big the background is sort of solid
white it doesn't have too much sift
features for us to track and and you
have a large depth variations of this
type of thing ok
nearby so these are the resort of
challenges and we're going to show that
if we have a camera array all these
challenges can be can be addressed in a
much in a much more easily assume you
have a camera array this array is
somehow vibrating in 3d space and we can
view this stabilization as the image
based a rendering problem you just want
to render a video along a smooth
trajectory so this concept is waiting to
come up with this concept this concept
is mentioned in this paper okay but they
didn't use array so we were arguing that
if you have a rate this we can we can
make this ideal work much better okay so
that's the that's the idea and the
reason the array helps is at each moment
in time you have a rate you can estimate
the depth and you can do image based
rendering to render all the scenes so
that a particular desired of viewpoint
you want to form a smooth video ok so
this video will really demonstrate the
concept so you have a camera read these
red pyramids are red can are five
cameras of the physical array and we
only use five cameras we for this
particular project and so this is the
one of the five input its River
vibrating a lot so then this is the
virtual camera the virtual camera is
removing around it's like a car
suspension right seats a seat is stable
but the there's motion between the seat
ended the frame if you sit down with it
and then you see everything is stable so
that's the basic idea
this is that camera so what's the yeah
the separation is um 40 millimeter right
so this is another earth yeah you can
see that this it's very hard to make the
traditional video civilization work on
this type of maybe a little bit too
contrived the scene to try to illustrate
to become concept in those two samples
the amount that you're copping out seems
to be somewhat minimal compared to some
the other consumer grade stabilization
techniques by cropping out the amount of
video that gets lost right right right
we try to minimize that you can sort of
a somehow optimize the position so that
the drop out region is somehow minimized
so you short the cell phone right where
the area was really small be like
centimeter or one cylinder into this
virtual view and go outside to create
the square and then it becomes like an
extrapolation right so in reality do you
think this this this algorithm would
work like basically the accuracy of
death estimation really becomes yeah if
you need okay if if these web cameras
are closer right this flu one used to go
outside way more yes yeah so then it
becomes extrapolation yet but that
really if you do that it will be more
it's certainly much hotter so the depth
needs to be more accurate yes stereo are
you staring from the light ignite campus
right right back yeah if you do
interpolation that's easier yeah
okay so so we compared with the result
with
so I movie so it has a hard time to
stabilize this
Oh
another result okay so there's a one key
thing in this method so if we do it if
we implement this idea in a more
straightforward way we would run sort of
some sort of a structure structure from
motion to get to the 3d trajectory and
then we sort of get a smooth from this
red trajectory with smooth it out to get
the blue one okay that's the more
straightforward way of implementing
those if you do that then we run into
problems because you can see that all
these things you don't have that many
features to track right so the keys or
the idea is when we do image based
rendering we don't really care the
absolute location absolute location of
the ready trajectory we only need to
know the relative posts between the
virtual camera and the physical camera
that's all we care about we just need to
know the relative post so then we can
formulate as optimization problem we
want to find a sequence of relative
posts such that if you generate this
virtual video or the city and the
features by salient features i mean the
edge map will move smoothly in the
virtual camera so that's why we can
completely avoid the structure for
motion so that that's why we can get to
the guest result for all these very and
difficult scenes
okay so if we can we can generate one
virtual vo we can also generate two
virtual views then if we have a goggle
we can you can imagine you see some 3d
movies out of this
so get the concept so if we have a depth
and underlying this technique we have
the depth you have if we have the depth
we can augment the video with some
virtual object like say this virtual bar
is rotating is the Chi
to them we can easily model the
occlusion relationship
okay so that's yes get the deficit you
write your own there you yeah yeah
whatevs whew maybe tours really depth
map gap estimation cvpr or papers yeah
we use those yeah other questions okay
so so then I always switch to another
topic in terms of the image okay maybe
just up to this out stop how cut off the
other stuff so okay we know that if we
can estimate the rigid seen 3d radius in
we can organize images okay so that's
what photo tourism photo scenes is doing
so I've been thinking is ok if we how
about we extend this idea to non rigid
structure motion non-religious scenes
it's soon so one thing I did in my
student is if we can capture a whole lot
of 3d shapes then we can very easily
navigate through the shapes and maybe
come up with new shapes so that the
interface is like this standard graphics
through interface where you manipulate
the object you get new shapes okay so
now I've been thinking okay can we use
the similar interface for 2d images
without getting into the 3d
reconstruction so that the one sort of a
scenario would be okay the sky Simpson
saw a person somewhere and he forgot
exactly shape but he can query faces
okay you get a whole bunch of face
shapes okay then he this person wants to
nail down okay from this millions on
faces how can we get the exactly shaping
my mind I don't have the specific
picture but I have some rough idea maybe
this person has a big mouth or smiling
face something like that so in order to
do this the difficult is how do we
refine the search result right so maybe
a particular smiling style a particular
pose he wants okay so but right now for
example why interface we may come up
with this okay give them a current
retrieve the face maybe you can just
drag the nose to the left you get a set
of your front looking faces okay another
example is you just do it this pinch on
your touch pad interface you get the set
of sore open mouth face images ok so
this type of thing to explores the
geometric attributes which are you know
hard to say in the world but it's easy
to manipulate hunger cell phone or
whatever touch touch pad interfaces so
it could be could be useful for criminal
profiling or photo management so again
so I'll probably skip the details just
show you a prototype we build
designer base images for example we can
drag a point on the nose to find cases
with a specific leftward pose the search
results are shown on the right by
clicking on each search result the full
image appears internally we rely on face
lineman to establish the shape of each
face in the database we can optionally
overlay the line shape for the current
image note that for this search pose was
selected and expression was lacked this
means that the system holds the
expression constant while allowing the
pose to change based on our edit in this
example we will use to edits to find
slightly left toward looking faces with
open mouth slats first we change the
pose using one edit
the point used for this edit is shown in
red we can keep this point as a shape
constraint in addition to a second edit
the intent of the second edit will be to
change only the expression not the pose
and so we deselect the pose option and
select the expression option instead
we pull the mouth open to find slightly
leftward looking faces with open mouth
smiles
we can also search for more subtle types
of facial expressions as this example
will show here we aim to find frontal
faces with closed mouth smiles to
accomplish this we will use 3 shape
edits first we adjust the pose
second we closed them out
third we pull the corner of the mouth
upward to generate a closed mouth smile
okay I guess you get the idea so that we
make it more useful we have to lock down
the identity so that you know the
identity doesn't just change the problem
in practice is we cannot find each actor
or each public figure it doesn't have
all the different expressions all the
poses you know in our database so we
sort of a we have to remove that
constraint okay okay so we have about
10,000 images and we do a pre-processing
to get out of a land line marks detected
that probably removed 55,000 because
there's something some alignment which
are not very reliable as we remove them
you know or fight final search database
that's you see you don't have the actor
and all expressions and that's because
you're going to
google image search or something that if
you actually started analyzing movies
from an actor over the course movie will
probably pose that's for sure that's all
yeah we didn't do that we just use a
some database maybe Columbia or some
other I think I'm good Columbia
collected this data set so yeah um okay
so i think i need a wrap-up just
although i have some other stuff so one
thing is the geometric attributes is not
enough so we really want to type in
maybe red hair or the guy with a
mustache so we need some appearance
attributes to make this thing more
useful that's one thing the other thing
is we really need more robust to face
alignment for this to be usable because
it's this is this is a little bit
different from photos things where you
can take a million pictures of Statue of
Liberty if somehow the teacher matching
doesn't work you can throw away maybe
half of them but you still have a lot
but here for faces maybe we're
interested in certain interesting smile
or if you if you throw that smile away
then you're really missing the point
that the pictures that people may be
fine what maybe they want to find okay
so we really need this and we have some
phase alignment paper last year and the
work on this problem we realize a very
very difficult problem in practice which
is all these different data set each
University has its own data set and each
data cells has the ink has different
definition of landmarks so for this one
the CML data has data set has like 50
points but for this data set has 20
points so there's it's very difficult to
merge these data set together to get a
very robust to get together more robust
to face alignment a model so how many
more minutes do I have
okay so because of this so more recently
I've been sort of thinking maybe we we
want to have a different representation
for faces instead of figuring out these
landmarks okay we can for each pixel
give it a label where there is a skin
and cheek or nose or teeth or hair right
so we're going to use the soft
segmentation as a representational four
phases which has probably handle the
hair in the future it's very hard to use
counter and landmarks to head to model
here as well so that's one motivation
and also we want to make this method to
our general so that we can use it for
Street View seen as well so I will just
show you I guess some the current result
we can get
okay so this is one sort of a result we
get in last you see cv this is a
frame-by-frame processing for example
based sir parsing of straight view
scenes if you pay attention usually we
miss smaller objects but for the big
objects like skies buildings road
surfaces we tend tend to work the method
tends to work better so that's for this
so on faces in this cvpr we have a
service similar but different example
based hub method which works quite well
we compared with other face segmentation
method just give you a sense show you a
demonstration
so one nice thing right now we cannot
handle air yet but was this sort of a
segment based a method it's possible you
can hand you can model teeth sometimes
the tease is important q if you want to
analyze the lip region you get the same
orientation with the labeling method
versus the contours so before you were
able to drag points to set orientation
or the gains of the face I'm still
confident accomplish that with this
method oh that's a good point this
method you can integrate the count or
the controversy in yes so you in order
to drag these points you have to have
some anchor point drag right so this
method for this segment based
representation we can integrate with the
counter as well yes we can't do it okay
that's that's I guess that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>