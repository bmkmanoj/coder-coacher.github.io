<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ParaSail: A Pointer-Free Path to Object-Oriented Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="ParaSail: A Pointer-Free Path to Object-Oriented Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ParaSail: A Pointer-Free Path to Object-Oriented Parallel Programming</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bNWTRdAlwUs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon everyone I'm rooster Lane
oh it's my pleasure to introduce Tucker
Taft who will talk to us about his new
language design so Tucker has been
involved in languages and and different
program analysis techniques in for
decades and starting at intra metrics in
Boston in the 80s and 90s and then
during his own company soft check in
2002 which more recently was was
acquired by by ADA ADA core which I
guess seems to have collected a number
of companies so today he'll tell us not
about ADA even though he's been quite
involved in 8 and 95 and 2012 but about
parasail welcome thank you so I thought
I might also say you know why am I here
well I met Rustin at a very nice little
conference in Maine of all things and I
brought along my main hat just in case
there was any doubt that I've been Maine
and anyway it was I think we sat in each
other's presentations and there was a
tremendous amount of overlap and what we
were having to say about what we
believed in and what we were trying to
accomplish and so on and so Daphne and
parasail I think are trying to you know
address some of the same issues and this
morning I was walking over here from my
nice little B&amp;amp;B and I was thinking well
I'd see parents sale Plus Daphne Parham
Daphne paradox
so that was my official name for the
attempt to combine these two anyway this
is about parasail not about paradox but
I think you will see some some
similarities and overlap at least we get
into these stuff about assertions I'm
going to start talking to you more about
the parallelization attempt and so I set
out a while ago to start designing a new
language I've been involved with ADA
since 1979 roughly which was before ADA
was
even a standard and got very involved
well pretty soon thereafter but in 1990
we won the contract to do the first
revision of ADA and I ate a 95 as it's
now called at the time it was 8 and 9 X
because we didn't know what it would be
and I was the lead technical person on
that but well before that in the in the
70s I started getting interested in
programming language design I don't know
why but it was sort of one of those
things you know that hits you and I've
always been interested in it and as I
said about three before years ago
interested in starting over you know
that's just blank sheet of paper what
could we do if we really wanted to start
from scratch and one of the reasons to
do that was the the whole multi-core
thing that is we're now dealing with a
world where the number of processors is
going to be growing or doubling perhaps
every two years we used to be having
twice as fast processors but now it's
gonna be twice as many and that's a
fundamentally different problem to solve
the old days we could sit on our Duff's
and our software would get twice as fast
now we actually have to get a little
more clever to take advantage of all
those extra processors so anyway this
language is called parasail for parallel
specification and implementation
language it's intended to be a very
formal oriented language has built-in
assertion syntax and so on and
pervasively parallel that is it it's
easier to do things in parallel than
sequentially in some ways starting from
scratch it doesn't really happen so I
have to admit that I've studied a few
languages in my time and and you will
probably see plenty of them coming
through on some of this but there are
few things that I think are unusual and
it may be the combination which is
perhaps unique anyway so and this is a
little bit of insult on C and C++ which
I'm probably insulting at least some of
you here but my belief is that C and C++
or two of the least safe languages
design the last four years and they are
winning the battle in the
safety-critical world I'm very involved
with safety critical systems and it used
to be almost 8020 eighty percent were
written in Ada and 20 percent were
written in C but as the number of safety
critical systems
grown and they've sort of branched out
from the military-industrial complex
into you know sort of just commercial
world ada has not followed that path and
so now instead of a t28 it to see it's
more like 8020 C to Ada and C++ is used
not at the highest level of criticality
but it's certainly growing in use in
some of the safety critical systems so
that's kind of you know I understand why
ADA
did not make the jump is that me I don't
know who that is
it's not me anyway I don't think that'll
bothers too much
what's the beeping oh battery backup is
that for my pacemaker you heard about
the guy who hacked his own pacemaker
that's happened recently that's kind of
cool that's just to prove that you can
hack into anything I guess especially if
you're a little crazy and you have a
pacemaker anyway so one of the things
that that I think most of us in this
room recognized but not a lot of people
elsewhere recognized the computers
actually stopped getting faster in about
2005 which is longer ago than I would
have guessed but my laptop is you know
looking a little old here but it's still
three gigahertz and you gotten by laptop
today it's kind of still three gigahertz
you know and if things that kept
doubling which is what they were doing
for 25 years or so this would be like in
50 gigahertz zone if I got a new one
that is anyway and so chips are gonna
start having more and more cores the
other thing that I think has happened is
static analysis and that's represented
well within this group has actually come
of age and it's time to to get it into
the language you know stop beating
around the bush where you have a
separate tool or whatever you know let's
get her into the language this is just a
slightly enlarged version of that I
Triple E slide and it's pretty dramatic
what happened in 2005
they just started cooking their chips
and they decided that was not a good
idea and maybe we should try to figure
out how to make them you know higher
performance in other ways and that's
where all this multi-core stuff started
seems to go back and look at articles
written in that zone 2004 or 2005 2006
where they realized that things really
were gonna change Intel calls this the
right turn because this is a huge deal
for them they were having no trouble
selling a new chip every couple of years
to people with laptops and it was twice
as fast you know you got a 1 gigahertz
my goodness come on 3 gigas where it's
all about but but now it's kind of hard
to convince people say oh yeah I'd have
a new laptop because it's sort of
prettier or in a thinner or something
anyway I mentioned I drew from various
languages I ml it was certainly a source
of much inspiration ADA certainly
Cyclone and if you know Greg Morissette
but this is something he worked on with
region-based storage management CUDA
silk silk a lot from silk anyway a lot
of these influences will show up
probably so I had this idea new
programming language and I knew I wanted
to have lots of good parallelism and
lots of good formalism and I really knew
that I wasn't start adding things it had
to start from a pretty simple core so I
start ripping things out from languages
that I loved to try to get down to the
absolute bare minimum and I think if you
can do that that's generally a good
thing if you're trying to do parallelism
and formalism well one thing was to make
conceptual room in people's minds you
know if the language is already complex
and then you start adding more into it
it's going to be really hard to to deal
with but the other reason was I wanted
to be pervasively parallel and safe and
easy to verify
maybe easy to verify as overstatement
straightforward that's my usual word
when I can't say easy but easier to
verify and getting rid of things is
definitely a good place to start when
you're trying to do that parallelization
parallel by default every expression is
parallelizable so if you write f of X
plus G
why f of X and G of Y can be evaluated
in parallel and the programmer doesn't
have to worry about it it happens
automatically if you look at languages
familiar with silk it's a language that
was done came out of MIT charles
leiserson i believe and then was bought
by intel and they've put more energy
into it it it has very easy paralyzation
but you've got to say spawn if you want
an expression evaluated in parallel you
say spawn this or spawn that and it's
caveat emptor if you say that they're
assuming well then I must be safe to do
that there's no real guarantee that
that's the safe thing to do I wanted a
language where there wasn't and of that
issue that the paralyzation was always
safe and if it wasn't the compiler would
catch it and make it so easy and and
sort of baked in that it's easier to
write parallel than sequential because I
think if we get our programmers to all
write it this way we have to make it
really easy and then from the
formalization point of view there's been
a you know growing acceptance of pre and
post conditions type invariance that
sort of thing getting them right into
the program and in my view the compiler
ought to be the one that's enforcing
these now it may be a smarter compiler
than your average bear maybe it has
boogie and z3 even its back pocket but I
think from the programmers point of view
it ought to be part of the same tool
every time you compile it that gets
checked and and if you can do all that
at compile time then do we really need
runtime exceptions and when the overhead
runtime exception handling and the
conceptual overhead of dealing with
exceptions which make testing thoroughly
testing a system much harder so I'm
hoping no so here's my little list of
simplifications and actually it is more
than just these but this is already
enough to have something to chew on
so no pointers that was actually kind of
came last it was not where I started at
all I knew global variables were gonna
be trouble if you're trying to paralyze
I have expose GUI and you're not allowed
to look inside them you're just looking
at their spec well either you've got to
be very explicit about what what
modifies what which is a pain in the
neck for programmers to maintain or
you've got to just say well no global
variables why do we need them well you
know we're used to them or it's painful
to pass around parameters and so on but
I was willing to give it a shot anyway
no hidden aliasing that is when you get
into a procedure if you've got two
parameters X and y you can rely on the
fact and one or the other them you can
do an update on you can rely that
they're not the same parameter so if
there's any aliasing it's things like a
sub I versus a sub J those might be the
same and that's determined by whether I
equals J but X versus Y well there's no
way those could be aliased unless you
pass them as a parameter for example
where one was a global and one wasn't so
getting rid of that sort of aliasing and
then various other simplifications this
one is I think relevant to people
thinking about parallelization a lot of
languages are have explicit threads or
adding them as C++ just did recently and
I think that's not the way to achieve
massive parallelization or pervasive
parallelization it's it's like asking
programmers in the old days in C you had
to say I want this variable in a
register so you'd name like you could
have picked three of your local
variables and you could say register and
then the compiler would know okay
that'll be register 3 that'll be 4 and
that'll be 5 and at some point we said
you know that's not really the
programmers concern there are too many
registers you know we got 16 now we got
32 registers I can't have them trying to
figure out which one of these local
variables should be in register since
probably it's some of the temporaries
that should be in registers so as the
number of cores begins to grow expecting
the programmer to say well I think this
should be a thread and that should be a
thread and and you know one core for
this all of this code and one core for
all this code well they're too many you
know they're maybe they're 100 threads
how are you going to be able to 100
cores it just gets out of hand
same thing with locking and unlocking
you know if you have to kind of worried
about you geez I guess I better lock cuz
might there be two threads that's again
this just not gonna work and again the
idea of waiting and signaling doing it
explicitly we're okay I'm waiting on
this condition variable and I'm hoping
somebody's gonna signal me at some point
and I'm hoping I do it because of the
right reason and so on trying to get rid
of some of those things no race
conditions that is the compiler ensures
that there are no undesirable
simultaneous access to shared data if
that's if that is possible that would
either complain
or in some cases perhaps it would insert
additional synchronization but the the
general rule is that the language is
designed so that the compiler can
complain about any such possibilities so
why those simplifications and particular
why pointer free I mentioned f of X plus
G of Y and a lot of it comes down to
thinking about that problem we're just
trying to safely evaluate f of X and G
of Y in parallel without looking inside
of F and G so what does that mean well
let's assume x and y we're parameters
coming in to us and now we're calling F
of X plus G of Y well we probably ought
to be able to say that those aren't
aliased so no global variable is clearly
helpful if FRG if they both manipulate
the same global variable they might be
stepping on the same object no parameter
aliasing is important which I already
said several times and what do we do if
x and y are pointers well I'm not even
sure what that means anymore if I'm
saying no global variables what can they
do with a pointer well is dereferencing
a global a pointer referencing a global
variable depends on your model of how
global variables work but but let's
assume it's not we have this fundamental
problem can I get X&amp;amp;Y
is there a danger that F or G could
follow the pointers I give it and
ultimately arrive at some common object
Z and the answer is without a lot of
analysis that's probably quite possible
you don't really know where pointers
ultimately lead you can do you know
various kinds of static analysis to
prove that x and y are separate
you know separation logic and so on but
if we're not expecting the programmers
to do a lot of annotations that are
somehow going to allow you to prove that
we're not allowed to look inside F or G
pointers make life a lot harder and
furthermore if we have parameter modes
like in out or in or var versus non var
how does that relate to the things they
point at in a language like ADA which
has parameter modes you can declare
pointers that have only give you
read-only access to the object they
designate but
but then that it only ever gives you
read-only access if you pass a pointer
as an in parameter and it's an access to
a variable then the caller can update
that variable so parameter modes don't
really help very much here you've got to
be able to know well what are they going
to do with things that they reach
through levels of indirection so after
almost a year or two of doing language
design I finally arrived at the answer
well let's just get rid of pointers
completely and started trying to write
code that didn't use pointers pretty
early on I I realized that I needed to
represent things like trees without
pointers and a way to do that is
essentially to add to every type an
additional value this is like maybe
types in many functional languages and
so on and pointers for that matter have
null values but don't think of null as a
pointer I think of it as just like a
very small value or it might be
equivalent to sort of uninitialized but
it's it's got a name and you can check
for it so it's not quite as bad as just
a random bit pattern anyway so for every
type be it a record type or a array type
or an integer type or a boolean type
we're going to add an extra value to
that type so it has all of its usual
values true false and no and you can use
null when you declare an object or a
component or parameter to be optional
and that basically means okay it's that
might be a null value if you don't say
optional it can't be it's got to be one
of the normal values of the veil of the
type but if you say optional then that
allows you to put null into that object
and my conclusion was that a lot of uses
of pointers are for that purpose that is
the purpose of starting off with nothing
and then putting something there and a
pointer is sort of the only way to do
that in in many existing languages but
if you just sort of say well let's add
another value called null so I can start
off with nothing and then I can put a
tree there or I can start off with
nothing and put an array there
and in that that's one of the big uses
for pointers now if you're not gonna
pointers you don't want to then
assignment had better not be pointer
assignment because then suddenly you've
got pointers again so assignment is
going to be by copy but that's kind of
painful do we always want to do it that
way suppose I've got a big tree here and
a big object here I'm trying to put it
into that tree or I'm trying to put an
object into a set or into some other
kind of container I don't want to make a
big copy of it and then delete this
whole dock why don't I just actually be
able to move it in so if you're not kind
of pointers then you may need additional
operators that are similar to assignment
but are for the purpose of fundamentally
moving a piece of data into a container
or swap is the same thing if assignment
does copying you're trying to you know
let's say switch two halves of a tree to
do balancing or something if I have to
do it by assigning to a temporary that
makes a copy then I sign this over to
here and and that makes a copy then I
sign over here makes another copy that's
a lot of copying if I can just swap the
left and right halves of a tree then
then that's much nicer so that gets you
a big portion of the use of pointers and
then let's generalize the notion of a
race
Fortran head arrays it didn't have
pointers didn't have records for that
matter when it started off and people's
managed to survive but it was painful
and pretty quickly we got away from that
if we could but if you have containers
that allow the user to find what
indexing means so it might be a hash
table or it might be a set or it might
be a it might be an array it might be
some kind of expandable vector but it's
got the notion of an index so you can I
want to get the nth element or I want to
change the enth element or maybe it's
the element whose key is ABC there's a
general notion of an index or a key and
there's a general notion of a container
and if we if we allow the programmer to
implement these sort of containers and
then use indexing syntax to refer to
them perhaps we don't really need
pointers even for cyclic data structures
we know for tree-like structures we can
do without them but for cyclic data
structures it's pretty hard to do
without them unless you have some
alternative and so let's just make
indexing very general the individual
elements in your in your container can
grow and shrink because they can be null
and they can become non null values so
we can write things like for each n in
directed graphs of I dot successors loop
and the successors are not pointers
they're a set of node IDs and we're now
iterating through all of the successors
of some graph and we can add edges to
the graph we can add nodes to the graph
and so on but we don't really need
pointers for that purpose in fact in my
experience when I started looking at
code I'd written in the past using a
language of head pointers to do directed
graphs I frequently didn't use the
pointers to do the directed graphs I
frequently did it this way anyway for
other reasons like the nodes often have
existence independent of whose pointing
at them you create a graph with a bunch
of nodes and then maybe you add edges
and your move edges and so on but you
don't want the nodes to just disappear
because no one's pointing at them
because they have some existence
independent of the of the edges
connecting them so it's not obvious that
pointers were the right solution for a
directed graph anyway so my hope was
well given this I can I can do anything
that I could have done with pointers and
with this additional stuff i minimized
an actual need for them the other thing
that came along with this was objects
are now sort of self-contained that is
they grow and shrink but they don't they
don't dangle if you will you don't you
can't have a dangling reference to an
object because they're no references at
all objects live in a conceptually in a
stack frame where they're declared and
they can grow and shrink but they don't
no one's going to have a pointer to them
that might outlive their their lifetime
so that allows you to change your whole
storage management approach so no global
heat because you don't need to create
objects in places where they can live
indefinitely they can only live as long
as the scope in which they exist so we
now move all of our storage management
to
the local region based storage where the
objects are local and there there's
essentially a local heap which is
essentially what a region is and their
growth and their shrinkage is all within
that local heat yeah well you're not
gonna have cycles it's really explicit
cycles because you don't have pointers
so but you're going to simulate that
because you're going to say well you
know this this this the index of this
thing is over there in that collection
at index J right you're gonna just a
value it's just a key of some type you
can use it to some degree as you see fit
it's just a you know it's it's a it's a
scalar value it's a string you know and
you can store it in other I mean here
I'm exam I'm imagining I've got a set of
those indices stored in the successors
set but I could have come up with I mean
where did I come from maybe was passed
in as a parameter or maybe it was result
right so the there are preconditions on
using indices under certain
circumstances and for example it might
be that it's got to be in the range 1 to
100 if it's a hundred element array and
so those preconditions are things that
have to be actually checked at compile
time and you have to prove that you're
within the range or the alternative is
you might have a data structure like a
map where it returns a null if there's
no value at that index so those are the
two ways you would deal with them does
that answer your question ok other
questions and one thing that that you
often or that you do bump into is you've
now got a tree let's say and it's it's
maybe fairly elaborate and you've now
asked to have this sub sub sub component
replaced with its
currently null is so it's sort of like a
little stub on the end of a tree
somewhere I'm now gonna put a branch
there and let's suppose I do this by
passing the passing it as a parameter as
a var or you know parameter that
particular stub I want you to stuff
something into it
well currently it's null and I want to
be sure that you put that new piece of
the tree in the right region so either
have to pass into every procedure an
additional indicator of what region I
want you to put things in if you
allocate them and add them my tree or
the tree itself has to identify where
should if you add something to the tree
where should that addition go does that
make sense you're trying to make certain
that all of the pieces of this tree live
in the same region and I've now asked
you to plug something into this null
part of my tree put a non null piece
into that so what we do is we make each
null identify what region it is for so
null is not just the value 0 it actually
identifies it says I know but by the way
if you want to replace me with something
non null you've got to put that in
region Q for example yeah if you have a
tree with holes like that that are
associated with a particular
storage regions what happens when you
use your copy operator to copy the whole
tree into some other region and now
these guys are still talking about the
old
correct the nulls when you when you
create a copy the null in the in their
new value identifies the new region in
which its living although in the
original tree they could they they they
could be referring to read as other than
the tree oh okay they have to be they're
all referring to the same region that is
the tree its heritage that the null
carries the region of the container is
living exactly exactly
and it's the place where that object was
declared this fully determines
everything about all the storage within
it now it is possible to have what you
would probably call a pointer shortly of
references to existing objects for
example when you pass a parameter we
don't make a copy we just pass a
reference to the object that you're
referring to but it is a hand off if the
object is being updated by that function
you're calling when you pass it you're
essentially logically handing it off and
no one else is allowed to update it
while it's being manipulated by that
function and you can also return
references but you can only return
references to things that are passed to
you and that's how indexing is
implemented you have to be allow users
to implement their own indexing what can
they do you pass them a map they can
pass you back in index to an element of
that map I mean a reference in all my
men so trees I've already talked about
this but it's a little bit more specific
how is it tree represented so here's an
example of in parasail every module has
an interface and a class that implements
it the interface gives you the external
view and the class gives you the
implementation this is this is an
external view of a non opaque type we're
essentially exporting how is this type
represented but you could put this all
in the hidden part of the in the class
itself and then just provide operations
you know setting and getting these
things but at this point this is just
indicating how would you represent a
tree or a node of a tree in parasail you
have a payload which is of the type of
the parameterize type here and left and
right which are optional trees default
initialized to null so that's what I
know of a tree looks like and then they
grow by having trees plugged into the
left or right component does that make
sense so here's a few little examples of
creating a tree we declare a variable
root of type tree node over strings and
we initialize it to payload with the
payload having the value top and left
and right or defaulting to null then we
can assign into the left tree a new tree
node with payload l and with the right
tree being payload LR and this that
conceptually these are values as opposed
to things that are pointed out these are
just values whose size is more dynamic
than the average value but they're
conceptually it's of value semantics and
then if I wanted to move some part of
the tree from one place to another I can
use this move operator which has a side
effect of leaving the right-hand side
null so it essentially at a semantic
level it's copying the value the
right-hand side into the left-hand side
and setting the right-hand side to null
if these two happen to be from the same
region you can do that without
physically copying you can just
essentially move the underlying pointers
but from a semantic point of view this
is an assignment followed by setting the
right-hand side to no that makes sense
okay
anyway a little bit about region-based
storage management cyclone this is greg
Morissette and a few others yep you did
it the other way round route doc left up
right gets rubes right if you then first
to move and then as null in the route
right you'll cut off the thing that you
just right so does one have to be
contained with port you have to make
sure that one is not contained in the
other when you do the know it is smart
enough to not lose the data so you can
for example remove an element from a
linked list by replacing the linked list
with its second element essentially and
it'll essentially then remove that
intermediate element that make sense
so we can make search management you may
be all familiar with it this is a little
bit of a twist on it in that in cyclone
there are pointers but the pointers have
an implicit region associated with them
and so what they really you can think of
him as being indices into that region
they're not general-purpose pointers
they can't point arbitrarily in
arbitrary places they can only
essentially they can give you a
reference into that region but the other
thing about it is that that regions live
in a certain scope and they're local to
those scopes and when you leave the
scope you reclaim the whole thing which
simplifies at least storage D allocation
so we're getting the same advantage of
that but it's simplified even further
because there's only one pointer to
every piece of data and so when that
pointer is set to null you can
immediately reclaim that storage so this
so you have these local regions which
are essentially local heaps and the
space for an object declared in that
scope is allocated from that region and
when this object shrinks because you
have a non nil part of it that you said
to no that is immediately returned to
this region so the region has its own
little you know storage manager if you
will and it's very simple initially if
there hasn't been any things returned
but if you start returning stuff then
it'll do the usual you know list of
reusable pieces or something but then
when you leave the scope you just flush
the whole thing because you know all the
objects that are living in that region
were declared in the same scope they
can't be from an outer scope and as I
mentioned move and swap are very cheap
when you're staying within the region
analogous if you think about a file
system like on UNIX maybe or these days
you have a USB card and I can move a
file within the USB card it's very fast
oh that's no problem it's just sort of
changing some pointers and so on if I
move it on to my laptop it's actually
got a copy it same idea here each region
is like a separate disc if you want to
think about it that way and in the UNIX
world are separate Drive and if you move
within the same region it's very cheap
or swap if you move across then there's
some extra actual copying to help in
this process is if you know you're
creating an object that is going to be
moved into a particular container
so I'm creating this object I'm going to
move it into this tree or I'm cleaning
this object I'm going to move it into
this set then when you declare it you
can say it's going to be if it's being
created for the purpose of becoming part
of that object and so here it will use
it'll declare this object and allocated
space out of this region which you know
will outlive this object so essentially
this is like a late declaration of an
object that's going to be living in a
outer region and then at some point we
can move X into the root object here
which is what I'm showing route left
that payload we move X into that and now
we know that'll be cheap this is just a
hint it could be completely ignored it
wouldn't change the semantics but it
affects the performance that makes sense
the other thing about using local heaps
is that they're generally better behaved
for parallelism because now objects are
generally not scattered all over the
global heap indicating at what point
they grew you know if you're adding
something to a object and then quite a
while later you had another thing to the
object and that another thing then the
object is spread all over your global
heap the other hand each object lives in
its own region then you know that the
space you're allocating forward is going
to be coming out of that that same
region the other thing is that when you
have multiple threads that are all
allocating space in parallel then
they're generally each working in a
separate region when they're doing that
if you've doing region if you're doing
global heap they're probably allocating
out of the same global heap and you end
up with unrelated threads who are
allocating storage right next to each
other and if they're going to be
updating that storage that's really bad
news because you're going to get into
cache false sharing kinds of things
where they're essentially knocking each
other's data out of the cache so region
based origin is essentially better
behaved when you start getting into
multiple cores yeah
would you know Valu carried state if I
have a long long line that's carrying
state how many generation the next and I
have to declare a region that encloses
the entire loop in order for the for the
object to be life right in which case
that would not reclaim it until the
entire exits exits loops are implemented
by having each iteration be its own
thread and when you start another
iteration you can pass it some
parameters and those might be what it
needs to do the next iteration for
example is a separate thread so how do
you do boot karat stage at the end of
one iteration can pass a primer to the
next I know if that's answering your
question I guess you could reuse right
yes the Train try this now music I have
to see an example that you know exactly
answer your one thing that was the
problem was that
regions were not sufficient to be able
to write arbitration programs that you
had enough having to do things like
having linear types and so on so that
you could escape the lexical scoping of
a region based storage management
systems so that you could do things like
you know Kevin stated because otherwise
you would just run out of memory if you
were sitting in the bathroom and you you
know well
why is the loop Kerry State growing
that's what I'm not certain material Oh
everything that was in it may grow in
some cases but it would all be allocated
into a region that was that contained
the entire loop so you wouldn't read
that region until the entire mix of it
yeah I guess I have to see the example
because each iteration loop has its own
region potentially if it needs one and
then you have the outer region but the
the loop carried state is I mean I'm not
certain what makes it grow I'm trying to
think of examples where it would be
growing and that would be helpful to see
I do have some examples of loops and
maybe in there you'll be as they all
look at your loop carried State and
that's gonna kill you so maybe you can
save it for those questions okay so I've
been kind of diving down into the
underlying guts of parasail and no
pointers and so on but at a higher level
what does it look like well it's an
object-oriented parallel programming
language it has interfaces and classes
one thing a little different about it is
that every class has an interface so in
Java and I believe in c-sharp you either
you have an interface which has no
implementation and you have a class
which has no interface and classes can
implement interfaces if they want to in
parasail every class is an interface
which is its external visibility and
that's the only way you get to a class
is through its interface but you can say
that that I want this class or this
interface implement other interfaces so
you can define a new module which is the
name of a combination of an interface in
a class and I want this to implement
this other interface as well so it's my
own little super amazing hash set but it
implements the more abstract notion of
set and you can implement interfaces
whether an
they themselves have a default
implementation some like I mean in Java
you often see where there's a nice
interface and there's a default
implementation which is sometimes called
an adapter or various names for it here
every interface can have a default
implementation if you don't want to have
any implementation for interface you say
abstract interface so that just means
there is no class there's no default
implementation but you don't have to you
can implement non abstract interfaces
elsewhere so anyway the other thing is
that every module is parameterized so
there's no notion of generic versus non
generic everything is parameterize you
can have no parameters but the default
is parameterization and that that sort
of right from the beginning set your
mind on saying most things will be
parameterized and it supports
inheritance and the usual pretty much in
the usual way a type is an instance of a
module there's exactly one syntax for
declaring a type and that is by
instantiating a module all types have
this syntax including arrays records
enumerations integers you you think it
up it uses this syntax so it identifies
the module that you're instantiated it
gives the actual parameters and this
little new here is optional and what
that determines is if you say new I want
this to be considered a distinct type I
don't care if the actuals and the
modules are identical to some other
instantiation over here when I say new
this is in modular three terms its
branded its it's got its own identity
and it allows me to make distinctions
that might otherwise be be blurred by
the compiler so if I declare type T as
integer one to 100 that's the equivalent
to every other time I say type T is or
type R is integer one 200 but if I say
type T is new integer 1 to 100 then
that's a different integer type you
can't inter operate on them without
conversion
it's a fairly standard so that our
precipitation would be the same yes
and unlike because there no pointers
there's a pretty big distinction between
an object that stores exactly one type
versus an object that stores a type or
anything that looks like it so if you
want to have actual polymorphic objects
so you have an array of hashed sets or
you have an array of any old kind of set
those are considered different things so
if you want to have a polymorphic array
for example or just a polymorphic object
then you add a little plus sign at the
end of the name of the module and that
becomes anything implements that for the
end of the type in the name of type so
set plus would be or set integer plus
would be any anything that implements
the interface set instantiate with
integers and you could have an array of
them and so on because you've got this
because objects can grow and shrink
there isn't this problem where once you
create it it's sort of locked into a
particular type you can store a
different kind of set into a polymorphic
set type during its lifetime and that's
just fine or you can set it to no but
it's because of this expandable storage
model it means that you can do things
that that would be a little awkward or
would somehow be inefficient in other
languages so a type is an instance of a
module and object is an instance of a
type and you can either Avars or
constants and you can initialize the
VARs you must initialize the constants
if you say optional T here then that
default initialized to null if you don't
say optional T then it's not initialized
from the point of view of the semantics
then you must initialize it before you
use it so that becomes another
verification condition if you will to be
proved and that one's a little easier
than most so we've got module type
object and then operations which are the
things declared in modules that are
operate on objects so the code all the
code lives in operations
so I said most of this all modules of
parameterize all modules in your face
unless abstract a class blah blah blah
so everything they type use okay so in
type you structural equivalents unless
mark news so if they have identical
module identical parameters then they're
equivalent unless you say new here's
just a little bit of syntax we've
already seen one before but this shows
you an interface and queens the mata
parameters and then here are some local
types that have been declared and they
here we have one we used branding here
to get a new integer type which we're
gonna use it's going to have this bounds
minus eight times two to eight times two
whatever these are used for numbering
the diagonals on a chessboard this is
for numbering the rows is remembering
the columns and this is used for we're
trying to do the N Queens problem one
solution for the N Queens problem is a
list of column numbers indexed by row
essentially saying for this row where is
the Queen so if it's you know first row
the Queen is in column one the second
row the queen is in column three the
third row the queen is in column 5 and
so on so a solution to the N Queens
problem is an array of columns
I wrote optional here just to sort of
show the syntax but also to show some a
post condition so when you start off
trying to come up with a solution you
don't have any Queens on the board so
they're all there column number is null
so in row what's in Row one null no
Queen what's in road to where they in
Row two no so you start off where the
the empty or the incomplete solution has
nulls for all of these guys and then we
define this function place Queens which
returns you a vector of solutions the
intent is that returns you a vector of
all possible solutions and here is a
post condition where we're saying that
for all solutions of place Queens place
Queens when used here is talking about
the result of calling it so it sort of
place Queens results for all of those
solutions for all of the columns of the
solution the column is not null so we're
all we're claiming is that we're
returning a vector of these guys but
when you get it all of these column
numbers are
filled in so that's the that's the the
promise that make sense so a little bits
of syntax here things that are assertion
ish like constraints we have got some
constraints here on these row and column
numbers post conditions preconditions on
that all a uniformly use this brace
notation sort of borrowed from for logic
where preconditions would come before
the vector the arrow here post
conditions would come after and then
there type in variance which appear kind
of in the middle of a or a class and
range the middle of a class definition
but generally any place you see one of
these guys that's a that's some sort of
assertion like thing in terms of calling
operations you can either put the object
out the first parameter can be put out
front sort of like you know a lot of
object or any languages or you can put
all the parameters inside if you need to
identify the operator you can use this
the type double colon operator but in
general the the namespace is
automatically searched the namespace of
each parameter type so if you've got X
plus y it'll look for a plus operator in
the module that defines the type of X
and the module that defines the type of
Y so you rarely have to qualify
operators or any any kind of an
operation name it all pretty much finds
it by doing overload resolution it uses
the context to do that so it it it's
pretty rich and its ability to find the
operators yeah
the capital end of your type is that's
something that's static a compile time
where can you actually instantiate these
interfaces on different runtime thinking
I'm gonna write a program that lets the
user tell me right so this is a kind of
a weird way to do it it would make more
sense to pass it as a parameter at
runtime
if you do it here the requirement is
that it be known ultimately be known it
doesn't need to be known here obviously
cuz we don't know what it is but when
you when you instantiate a type all of
the opera and all the instantiate a
module all the parameters must be either
other parameters of an enclosing module
or be literal value so thankfully so so
what there's nothing it's almost more
methodological if you're gonna prove
things sometimes you want to be able to
expand them out and so it's sort of
saying well let's not make the provers
life too hard there's nothing really in
the language that couldn't deal with
these dynamically since most the time
you don't know what they are it's more
just the overall there's kind of a
guarantee that if you go back to the
very you know initial initial and Stan
Shi ation you'll know everything it's
just sort of nice in some cases but it's
really more of a methodological thing in
some ways that's the difference between
the parameters to a module and the
parameters to a operation okay the thing
that's a little different is that there
are five types that each correspond to
each kind of literal so unlike and see
where you've got to say well this is a
you know a long integer literal or a
very long float integer literal or
something that's just one integer
literal and it's you know it's universal
it's infinite precision same thing with
real string Universal string is an
arbitrary long vector of Universal
characters Universal characters are
unicode you know you get Klingon and
everything
so you could imagine and then the one
that's little odd here is universal
enumeration types it's not odd if you're
familiar with how things in Lisp work
but essentially we've given them a
distinct syntax so when you want to
define an enumeration type you're not
introducing new literals the literals
are out there you're saying that these
literals are legal values of my type or
can be converted to my type the way
literals work is that the two allow to
use a literal with a given type you have
to declare the from you neva / a ssin
and that from you neva / a ssin is a
conversion from the one of these
universal literal types to your type and
so if you declare one of these from you
neva paraders then you get to use those
literals that's the rule and the
precondition on that conversion
determines what subset of those literals
you can use with this type so if it's a
32-bit integer then the precondition on
the from Univ would be it's got to be in
the range minus 2 to the 31st - plus -
231st minus 1 or something if it's an
enumeration type the precondition is
critical it identifies which enumeration
literals can be used with this type so
if it's red green blue then that
precondition says it can only be red
green or blue and that makes sense and
if you want to do 32 you know if you
want to do conventional 8-bit characters
well then it would have to be the
character could have to be in the in the
range for that purpose so I talked a
little bit about the generalized
indexing basically all these different
kinds of things can all be seen as
either kind of key is value or key goes
to I'm here kind of like a set and
they're homogeneous at compile time at
least they might be conceivably
polymorphic at runtime but from a
compiler point of view they all look the
same and you want to have things like
iterators indexing slicing combining
merging concatenating and so on you
might like a representation for empty
containers some way of creating a
literal container they can grow and
shrink over time automatic storage
management so
the idea is there's some nice syntax I
like to be able to use with my type in
the same way that if you define that
from you neva crater you get literals if
you define a certain set of particular
operators like indexing and the empty
bracket and or equals and a few others
then you can use the nice syntax and
here's an example of the syntax
container sub indexes for indexing
containers of a dot I'd B is for slicing
empty brackets for empty container you
can do literal both positional and named
literals and so on and these just our
syntactic sugar for a call on one or
more operators building up a thing like
this it it calls the empty container
operator then it calls something to add
more elements into the container so the
compiler generates those calls for you
and then if it finds a bunch of calls
which depend only on values that are
known at compile time then it
essentially evaluates them at compile
time and replaces it with a reference to
that constant so it does make these
operations to implement things like
containers but if it's one two three it
will actually do the computation at
compile time to evaluate that aggregate
okay so the whole point of a lot of this
was to make it possible to have
pervasive parallelism and I've already
talked about why the underlying model is
there are thousands of threads it's too
many to worry about individually so it's
gonna be we're just gonna use them like
water use them like registers using like
virtual memory they're just a resource
they're not something that you should be
getting too hung up about individually
there your goal is used as many as
possible and the language should make
that easy and it should also prevent
some of these nasty situations
automatically I think if you're familiar
with how register allocation works it's
actually a pretty similar problem you're
trying to avoid you know you're trying
to use cores
and the same we trying to use registers
trying to avoid stepping on the you know
using the same register for two
different variables that have
overlapping lives and things like that
parallel by default syntactically f of X
plus G of Y tennis well I'm missing my
tennis date how bad is that so f of X
plus G of Y is evaluated parallel or at
least it can be the the compiler decides
whether to it but any of the donee work
that sees f of X plus G of Y it will say
well that's that can be valued in
parallel in fact you can't write
something that cannot be valued in
parallel that would be illegal
you can't say f of X plus G of X if f of
X and G of X can both manipulate can
both update or if either one of them can
update X that would be illegal in
parasail so everything you were allowed
to write can be evaluated parallel you
can also the semicolon kind of is you
know a separation between statements do
you really want to say I want to do all
these things in parallel you can put
double vertical bars and then you're
limited to the same rules that you can't
write that unless they can be safely
valued in parallel if you use a
semicolon then that says well if you can
do it in parallel please do if you can't
don't complain and then there's another
operator which says then process X then
process Y then process Z and that says
don't do them in parallel you know I you
may think it's possible but don't do it
of course the compiler can always say
well I'm so smart I can do it anyway but
that's not recommended
and there's this whole book span in the
sort of communication person this
computation cost
are you leaving that up I mean if you do
to fine grained parallelism then then
you're gonna be slower than super that's
just yeah I mean it has a certain tape
because I mean there's a lot of
experience of people taking pure
functional languages where you have it
amazing amounts of parallelism right but
you know the overheads really kill you
and it's still very hard to sort of do
the right Frank authorities store yeah I
use it might sort of but my time was
more time sorry um yes there are there's
clearly tuning involved in coming up
with the right granularity there and
right now it's it's heuristics very
simple it must involve at least one out
of line call it's gonna if it's gonna
you know if it's f of X plus G of ym and
those are both out of line calls then it
will create pico threads but these are
not actually run on different CPUs
they're simply candidates for being run
and it uses a work-stealing model if
you're familiar with that model and so
you know whether that's too small that's
a good question the intent is to make it
such low overhead these threads don't
have any context they're really minimal
overhead but not you know you're not
going to do X plus y yeah so that's the
idea
so I should probably wrap up here so I
don't you know blow out completely I was
somehow thinking I was going for five
o'clock but 4:30 is where I was headed
so there's lots more to say but you know
there's always time another time to say
it but I'm happy to answer more
questions if you have some at this point
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>