<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dynamic PDP vs. Dynamic POR: Are they really different? | Coder Coacher - Coaching Coders</title><meta content="Dynamic PDP vs. Dynamic POR: Are they really different? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dynamic PDP vs. Dynamic POR: Are they really different?</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZocoqtPJjeQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
nothing in particular one disclaimer
that I have is even though today's talk
is titled dynamic PDP versus dynamic
pure unfortunately I don't have a
coherent set of slides so what I am
going to do is the following using a
first set of slides I'm going to talk
about dynamic Pro Bowl data position so
one way of achieving cloud storage
integrity and using a different set of
slides I am going to talk about dynamic
proofs of retrieve ability and now my me
switch to the third part I'm going to
talk about some of the newest results
but without slides so I apologise for
any inconvenience so let's start talking
about dynamic global data position now
what is it exactly
so consider secure cloud storage so
forget about the rest of these slides
I'm going to only focus on the storage
what's this is outside outsource storage
setting we have a client and a storage
server and we have some files the client
wants to outsource so the client is
going to send this to the server the
server is going to store it on behalf of
the client one of the security
properties we want is integrity so if
the server modifies let's say some of
these files can we detect it or if the
server removes some of these files can
be detected so that's the integrity
problem we need to address another
security problem is confidentiality can
the server understand the contents of
these files maybe obtain some meaningful
information to use a switch so those are
in general the two main problems
associated with outsourcing our storage
to an untrusted search so that's the
scenario now to address confidentiality
what we can do is we can encrypt these
files keep the key at the client
samp the ciphertext to the server the
server stores them and this encryption
prevents the server from obtaining any
meaningful data so for the
confidentiality part we already can use
existing standard cryptographic
solutions now what's the interesting
part here for the integrity again there
are standard solutions that exists for
examples I can hash the file before
outsourcing obtain some digest keep this
digest locally again send the file to
the server and then at some later point
if I want to verify the integrity of
this file
what I can do is I can ask the server to
send me back the file once I receive it
I hash it back compare the output with
the digest I was holding make sense this
is a very standard approach for checking
the integrity alternatively I can
instead of a hash I can use a Mac keep
the key locally now the advantage here
is that in the hash based solution I may
need to keep one hash per file whereas
here I can keep one key for all the
files that I am going to outsource or I
can do the a symmetric version and I can
sign them and maybe send the signatures
to the server as well and later on I can
request give me back the file together
with its signature or Mac and I have no
idea what happened
so then once the server sends me back
the file together with its Mac or
signature I can again run the
verification and if it accepts now I
know the file integrity is protect now
what's the problem in these standard
approaches the client only stores a
single key for let's say lots of files
there's no problem - the client storage
is very low the server storage it's the
file associated let's say an Associated
signature or Mac that's acceptable I
mean if I have a van gigabyte file and
then let's say a few hundred bits extra
for the Mac or the signature that's
still okay the problem is the
communication if to check the integrity
I need to receive the files from the
storage then that's not possible
realistically so that's a very
inefficient operation if I am storing 10
gigabyte data actually my current
Dropbox account stores more than 20
gigabytes so if I need to retrieve all
that data to check the integrity then
that's very costly so the whole problem
we are trying to solve in both dynamic
probable data position and dynamic
proofs of retrieval the type of works is
to be able to verify this integrity with
very very small communication and
computation that's the whole goal now as
I said there were these seemingly two
different approaches PDP type of
approaches Pio our type of approaches so
let's first see them independently and
at the end we are going to talk about
the relationship between them so for
efficient solutions the first work is by
a 10 years at all in 2007 provable data
position the idea is the following we
consider a file split into blocks we
have a key and using that key we create
a tag per block so for each block we
create attack now we only outsource the
file we also outsource these tags and
the server stores the blocks
together with the techs at some later
point when we want to verify the
integrity the client sends a challenge
this challenge in some sense picks
random blocks so let's say 1 4 &amp;amp; 7 now
the server creates a combined tag out of
tag 1 tag for end tag 7 creates one tank
so out of let's say multiple tags it
creates one take
similarly for from multiple blocks it
creates one single block and then sends
these back to the client so for
communication the only thing that sent
back is one combined block one combined
so you are saying for example what if
all these three one four and seven were
sent separately that would still be
better than sending the whole file but
we can even improve it to oven
communication essentially constant
communication by combining them in a
clever way and this combination can be
verified against this key and of course
we know what we challenged so this
verification procedure can run if it
accepts we are confident with high
probability that the files kept stored
intact now what's the problem here the
problem is that if let's say I want to
change one of the blocks here so I want
to change some of the blocks what needs
to happen is that now the server has
essential two copies the old version and
the new version and when I challenge
there is no way to understand whether
the response was based on the old
version or the new version because both
versions have valid tags so that's the
main problem if I update the data both
versions we
have valid text and therefore there is
no way for the client to verify whether
the old version is used or the new
version is used now to make it dynamic
we are going to require a data structure
we can use a Merkle tree or as array a
tile described we can use a rank based
authenticated skip list what does first
of all rank based means the following at
each node of this Skip list Skip list is
a tree like structure a binary tree like
structure each node contains how many
blocks
how many leaf level nodes can be reached
from that node
so there are seven blocks in total from
root I can reach seven of those here I
can reach two of those so I can reach
this one or I can go right and reach
this one so I can only reach two blocks
from this node let's say I can reach
three blocks which three four five and
six so those are the ranks of the blocks
so now if I want to search in a rank
based structure how did I do it let's
search for five so I started root as
usual now when I look at the root the
root says if I go down I can reach sorry
if I I can reach seven blocks from the
root if I go down I can reach six of
those if I go right I can reach one of
those now I am looking for seven sorry
five five is less than six okay which
means I should go down so what I did
here was if I go down I can reach the
first six blocks I'm searching for the
fifth block it is within those first six
blocks so I should go down now what
should I do actually
so if I go down I can reach two blocks
which block I am searching five
no it's the number of blocks that are
reachable so for example five is the
block ID the fifth block this one no it
is consecutive here so for rank based
data structures the block numbers are
consecutive so starting from let's say 1
or 0 whatever your consensus is and then
increment I will get to it just let me
finish this search I will answer that so
now if I go down I can only reach two
blocks whereas I am looking for block
five which means I shouldn't go down so
now I go right
ok now the moment I go right remember I
skipped over two blocks so I need to
change what I am searching for I was
searching for five but I skipped over
two blocks so I now need to search for
three in the remaining part of this data
structure now what should I do
so if I go down how many blocks can I
reach one which block I am searching for
now three right no longer five it's just
three since 3 is greater than 1 I need
to go right again when I go right I am
skipping over one block so I need to now
search for 3 minus 1 that's 2 now again
if I go down how many blocks can I reach
2 it says to here so which block I am
looking for number 2 which means now I
can go down so I go down now since I
went down I didn't change the block I am
searching for I am not skipping over
anything so I still continue searching
for 2 now if I go down this is a leaf
node which means there is one block
there but I am searching for 2 this
means I need to go one more right and
now I am search I skipped over one block
so I am searching for 2 minus 1 which is
index 1 I am here if I go down I see one
block should I go down yes because I am
searching for 1 done any questions about
this rank based property here yes
now consider any other let's say type of
data structure where these blocks have
explicit indices meaning for example as
all I was suggesting let's say this is
block index 1 this is 5 10 25 50 etc so
not necessarily consecutive and not
based on actual order of the blocks now
there are two main problems one is that
whenever I try to insert some new block
in between now all the
look indices must change to the right of
that block so that's a very big problem
now suddenly the insertion in between
becomes actually a linear time operation
same for deletion if I try to delete
let's say block number four here now I
need to call this for this five this six
so I need to essentially shift all these
indices again in linear time operation
what can happen here is that the moment
let's say I remove block five I also
removed this one okay now this rank
becomes one this rank becomes two three
five and six
okay so all the ranks up to the root
word Ikram entered by one that's it I
don't need to change anything else in
the status director so with only
logarithmic effort now I can do these
shifts because these indices are not
explicit they are implicit from the
ranks so the next time I search for
index five this one was removed remember
it will find this block which is the new
fifth block make sense
now what does authenticated mean most of
you already know this the value let's
say the hash value stored at this yellow
block is computed using the hash values
of its children the green ones so that's
true for any node in the structure for
example for this yellow note the hash
values of the green ones are used to
compute the hash value of the yellow
which means each node depends on its
children so if I change any node between
this structure all its ancestors need to
be changed and because we are going to
use a collision resistant hash function
this means if anything changes within
the structure the root value must change
so if someone changes the file it will
be detected because the root value will
be changed that's the authenticated
structure so this hash of this root we
call the digest or the metadata now what
the client will do is it will keep this
digest locally it's going to send the
blocks with their PDP type of tags
together with this data structure to the
server alternatively the the server can
construct the same structure if they
agree on some random seeds because it's
a randomized data structure but in any
case now the server stores this this
authentic ated ranked based skip list as
well now when we challenge let's say 1
and 3 again random blocks something
similar to PDP will be done so the
server will combine these blocks to
obtain a single block there is no need
to combine the text it will only copy
the one the first and third tag to the
proof and then to verify the first and
third text remember what we said each
node depends on its children
so consider the pet from this note up to
the root we have this parent it has two
children this one and this one this
orange ones hash value need to be
included in the proof so it is one of
the children missing along the path to
the root for the third one we go here
these children is missing we can combine
these and then this children is missing
so the proof yes so the proof is this
actually the combined block the tags
that were challenged and then the
missing parts of the data structure for
us to be able to recompute the root
value and the client receives this proof
first of all we will do the PDP
verification of the block and the tags
now if it verifies now we are going to
do the rank based authenticated skip
list verification so we will start with
this empty structure we will start
computing hashes so here this is as you
said the hash of the tag now I can
combine these two hashes to get the hash
of this and I can also combine their
ranks then I can do the hash of this I
can combine these two to get the hash of
this and the rank will be four here I
can combine this and this to obtain the
rank six and the hash value here finally
I can combine this hash and this hash
and the ranks to obtain the root now I
have obtained both a rank for the root
and the digest value for the root the
ranks tell me which blocks are these
texts for so through these ranks I can
easily compute this as the first tag
this is the third tag and then with the
hash of the root I was storing that
digest so I compare if it is the same as
the one I was - any questions here this
is basically how the dynamic probable
data position works
so if I want to modify let's say the
fifth block I send the new block
together with a Neve tag now this
modification remember the block will be
changed the tag will be changed and then
the pet to the root will be changed so
there will be new hashes and Neve rank
values possibly in this case the rank
builders don't change because it's the
number of blocks stay the same but
possibly they change the server sends
the orange things that are necessary for
verifying for reconstructing these so
what will be sent is the old proof for
the fifth block that I changed so first
I verify this old proof using the same
strategy and then at the end it should
be the same as the old digest value I
have then I know what the new tag look
looks like I put it here instead of the
fifth block and then I recompute again
now I obtain a different digest because
this is a modification operation so now
I just update my digest with this
exactly so all these proofs these
verifications etc are logarithmic time
with high probability we say with high
probability because skip lists are
randomized data structures so their
worst case is not logarithmic but they
are logarithmic with high probability I
mean in terms of the height
some very quick notes about this some
further work that can be done or that
has already been done now distributing
this to multiple servers was a challenge
or replicating the data with multiple
servers without of course increasing the
complexity now for resiliency against
failures this will be more clear in this
pure type of line of work but
essentially proofs of retrieve ability
use erasure codes so error correcting
codes or erasure codes so that even if
some part of the data is modified or
deleted we can still recover the
original now another thing is in all
these works they don't talk about what
happens if the proof fails so the server
sends this proof the client verifies and
it's rejected
what should the client to them we have
developed some actually official
arbitration mechanism so both the client
and the server can go talk to a judge to
an official let's say Court and we can
prove which one is actually doing the
trade is the client trying to blame the
server or is the server actually corrupt
the data there are works that are called
public verifiability it does not cover
official arbitration because in public
verifiability works a client can
actually produce some fake let's say
values for verification such that it can
blame an an honest server so public
verifiability does not do the job for
official arbitration some of the
important works in this area for static
pruple data position and proofs of
retrieval to the two very important
works are the attorneys at all probable
data position work and Tehama motors
compact proofs of retrieval tea
Charmian waters have a journal version
that's much better actually I recommend
reading that one for dynamic Pro bill
data position we have this array at all
work that's the first in 2009 then we
created a distributed and replicated
version of this where the client does
not have to be aware of how many servers
are used and the clients let's say
workload is independent of the number of
servers employed later on we have
created some optimizations mainly the
optimization of a flex list or flex DP
DP is that now one problem with ranked
based structure is it assumes all the
block sizes are constant they are the
same now in practice if I want to modify
a file my modification may not be an
exact block size and in those cases the
existing DP DP types of works have some
problems in the Flex DP DP the idea is
that instead of indexing block numbers
we can index let's say byte indices so
now any number of bytes can be inserted
anywhere the blocks can have different
number of bytes and so for dynamic
proofs of retrieval teaches the next
part of this talk I will change the set
of slides the first one was by cassia
Tov in 2013 this uses oblivious Ram as
the underlying structure later on she at
all and Chandra natal they constructed
more efficient schemes realizing that a
full oblivious Ram is not necessary it
will be very clear in the next talk and
this is the surprise result this is
currently under submission we know now
how to create dynamic proofs of
retrieval tea using only dynamic
provable data position and static
probable data position so in essence
even though dynamic pure was first
invented
in 2013 it is near result in 2009 we
could have already had dynamic pure yes
that's the overall scheme presented in
this construction and finally for the
official arbitration we have a
publication on how to do it properly
such that showing public verifiability
is not enough and how to do it such that
an official court can actually decide
which party is the right now I'm going
to switch to the next talk so if there
are any questions I can answer now about
this or I can switch to the dynamic PR
works
so exactly that sexually what we
realized in this flex list flex DP DP
line of work those lips are not constant
size so we needed to modify the rank
based structure instead of ranks now we
have lengths of blocks so essentially
each node specifies how many bytes you
can reach from that node and once that's
done that way we can easily handle
variable length blocks one interesting
result we have there is we have
implemented this flex DP DP and we also
implemented the static P DP P DP is
currently the most efficient known
construction for the static case we
implemented pot we try it using for
example planetlab
we try it both locally and on planetlab
on in the on the Internet
what we realized was the following if
you implement static P DP as it was
described in the original a 10 years
adult paper in terms of the server
workload it's worse than flex DP DP it's
actually worse than the dynamic case now
what you can do is you can modify the
PDP structure slightly such that the
exponentiation that are the hardest
workload you outsource the
exponentiation stood aligned so the
server doesn't perform exponentiation
the client does them and this increases
communication bandwidth but decreases
the server load then it can be faster
than the dynamic version but even in
those cases as far as the client is
concerned so the time you sent the
challenge until you get back it and
verify it there was actually no
noticeable difference between the static
PD P and flexible dynamic provable data
position
so that's a very interesting result in
practice we can use flex DP DP almost as
efficient as its static version so the
other thing as I promised is now the
dynamic proofs of retrieval this
scenario so what's the difference as I
said now we are going to employ erasure
codes this work is initially presented
by David in Eurogroup 2013 later on
they'll modified some of these slides
finally I modified some of them but
mainly it's their slides now these parts
we already know so I am going to skip so
the idea is the following the proofs of
retrieval T the first work on this was
by Jules ankle tsuki but as I said the
suggested work is by Shyam and waters
now what happens is that the grantee a
proof of retrieval to work tries to
provide is that once the challenge is
verified it means it should mean that
the server must know of all of the
client data now what does it mean to
know of all of the client data we know
actually how to formulate those type of
things in zero knowledge proof of
knowledge what do we do there so we have
an extractor algorithm that can extract
that data by possibly rewinding maybe
asking multiple questions and so on so
we do the same idea here now what we
know is that essentially we want
logarithmic or poly Logan complexity and
is known to be optimal in terms of
online memory checking works there are
lower bounds proven there and this is
the optimal also known in this global
data position world
now the original let's say pure our type
of scheme the static version roughly
works as follows so we have the original
data at the client the client first
encodes this data through an erasure
code now the data gets larger
now what this erasure code enables us is
let's say it depends of course on the
parameter of the code but let's say even
if half of this encoded data is deleted
and it needs to be of course random
deletions even if half of this data is
randomly deleted we can still recover
all of the original data that's what the
erasure code buys us it increases the
storage size but then it lets us to be
able to recover the whole original data
even if we have some loss the of course
stupidest way would be to duplicate the
same data advice so even if the first
half is removed I still have one copy of
the data but of course this is not
random erasure erasure codes protect
against random ratios now mostly in code
this similar to a PDP type of tag we
create these tags let's say with a Mac
or a signature to be able to verify the
integrity of each one of these blocks
individually so we first encode the data
and then for each encoded block we
create a tag to be able to verify the
integrity of that block now for auditing
for challenging again as usual we pick a
random set of blocks and then for this
the server
the server sends us something similar to
the PDP so it can somehow let's say
combine these blocks and The Associated
text and then we've now what this tells
us is the following so these are one of
our tools I will talk about it
is there a there's no okay
so if let's assume okay sure
so assume that the server knows less
than half of these encoded blocks okay
so this is the encoded data and let's
say out of these blocks some of them the
server corrupted or erased some others
remain so if the server only has less
than half of these encoded blocks now as
the original ones now the issue is that
we are challenging randomly and the
probability that we don't hit any of the
erased blocks in that case will be 1
minus 1 over 2 to the security parameter
so if you challenge let's say 80 blocks
the probability that we don't hit a
changed block will be 1 over 2 to the 80
and the probability that we will detect
a problem is mam - not make sense so if
the server removed a large portion of
these large meaning something that we
cannot recover using the erasure code
then we detected with very very high
probabilities now consider the other way
around let's say the server still has
more than half of these blocks so the
server has most of them now even if I
don't
catch that the server deleted some of
these bro blocks I don't care because
the erasure code will enable me to
recover back my original data so that's
the high level idea of a pure are scheme
if the server has less than what the
erasure code can recover
I will detect it if the server has more
than what the original code can recover
even if I don't detect it
I can still recover my original data so
this the original constructions were
static and the reason that static is I
will explain in just a second now why is
it difficult to make these constructions
dynamic so first of all because of the
erasure codes if I update let's say one
bit of this original data M it will
change linear number of keywords in the
encoded data so this erasure code is
very bad in terms of efficiency of the
updates if I change something very small
in the original data lots of things need
to change in the encoded data now think
of the other way round let's consider
encoding blocks of data individually so
I only encode this chunk here this chunk
is encoded here and so on which means if
I change one of these chunks the change
will not propagate to other chunks in
the encoded data now one problem is that
so that's good for efficiency but the
security problem here is the following
now remember to lose actual data the
server now needs to erase or modify more
than half of the corresponding erasure
coded data the main problem is the
following remember how we were
challenging we were challenging randomly
now
the more than half of this particular
encoded theta is actually constant size
so as compared to the whole data size
it's constant which means they I have a
very very low probability of hitting one
of these to make that probability high
then I need to challenge each one of
these code blocks let's say individually
but then this makes the challenge
complex the linear so if I if I keep my
challenge complexity constant then my
probability of catching will be very low
to keep that probability high then I
need to go to linear complexity so what
we want as we said this poly logarithm
now I can try to hide this association
between these individually encoded
blocks by permitting them so the client
knows a pseudo-random permutation let's
say so first we encode each block
individually and then parameters so that
it's hard for the server to erase a
particular block without erasing many
blocks in the total so if the server
does not povich blocks are the blue ones
his attack is now much worse but the
moment we switch their dynamic setting
let's say I am going to update I'm going
to update let's say one of these blue
blocks it is going to affect this blue
code vert which in turn I will have to
say to the server update these locations
the moment I tell the server to update
these particular locations now he knows
all these locations are tied together
for the blue let's say code word now he
can erase those so update of the data
gives a lot of information to the server
to
perform the same attack as we discussed
if I try to hide it I need to update a
much larger data so that not only the
blue blocks but maybe I also Rhian crypt
some of these other blocks and perform
unnecessary updates but then now the
update gets inefficient so in all these
approaches either it's inefficient or
insecure that's the main problem that it
took us let's say four years after the
dynamic PDP to create a dynamic pure
these erasure codes were the main drop
so our main idea was to use oblivious
Ram why oblivious Ram hides the
association between the client request
of the update so I want to update a
particular block and what the server
does so the server updates that location
but without knowing which location it
updated so by hiding this access pattern
what we can do is so get this data and
code each block individually and then
put this after a permutation although
the Oram construction already performed
the permutation so we don't need to
explicitly do it and then store it in an
or M structure for reading or writing to
a particular location for example to
read this location I need to read the
whole code word and then decode it so I
sent read request for each one of these
through oblivious Ram but remember these
sizes are constant so efficiency is not
a problem so I do constant number of
oblivious Ram reads for writing the same
I need to first get it modify it
Riaan code and then write those
locations but again each time one part
of data is associated with a Const
number of encoded let's say blocks so
always constant number of oblivious RAM
reads and writes again for audit I am
going to check let's say security
parameter number of random locations and
we require all Ram to provide us potent
isset e so for reads we require the one
m to provide us also to essentially the
same thing as PDP tags to be able to
verify that what we read was indeed the
correct value and we can achieve this
again through for example other
mechanisms through max and so on so for
each one we do a random read here and
then if all those reads verify
essentially if all the tags match in
some sense then we know we achieve it
for audits for audits you don't need to
actually because it is random locations
it doesn't tell the server anything I
will actually get to some of the Neva
results after this maybe we can talk
more about them in detail and again the
intuition for this security is the
following so let's say we have an
adversary who can answer these audits
with non-negligible probability now we
want to be able to extract the whole
data from the set versa so what we are
going to do is as usual our extractor
will keep rewinding and auditing at
different random locations so at each
remind we will audit a different random
locations now if the audit is successful
which happens with non-negligible
probability by our assumption now we are
going to fill the code words in our
empty structure so we start with an
empty let's say file we start filling in
locations each time it we receive a
verifying
suppose we rewind again ask fill in more
locations now what we proved in the
paper is that the expected number of
fielding locations here will be large it
will be enough for us to essentially
decode back the original data and
because of the obliviousness of the Orem
structure because the server has no idea
which parts we are reading then this
tells us these locations will be
distributed randomly throughout the file
throughout the encoded file now what
this tells us is that with very good
probability for each one of these
individual code words we will have more
than half of the codeword Fielden so if
you have a large fraction of the whole
encoded data that is randomly
distributed then for each particular
code word with high probability we will
have more than half of the blocks
already filled in if we have more than
half of the blocks for an encode that
think we can decode back to the original
again these half etc these depend on the
erasure code you use but you can tune
the parameters as you wish computer
science people love to write so one over
two to the something is our favorite
that's the reason we use these numbers
now one of course issue is that the
standard oblivious Ram security does not
include rewinds so standard pure or PDP
definitions include tree vines standards
in aknowledge proofs of knowledge the
extractor there can rewind but standard
oblivious Ram definitions don't allow
rewinds of the adversary so there we
need to create a modified Neve
definition we call that next read
pattern hiding to define what
type of oblivious RAM event so that's
the NIV definition we provide it's
essentially the oblivious RAM definition
but it allows rewinds and we showed that
actually some of the already existing
oblivious Rams already satisfy this
definition so it's not a completely
unrelated realistic definition even
though it wasn't shown before these
previous schemes already achieve this
definition now this was the first
so think about the standard oblivious
room definition so let me talk about
these offline and use the board let me
finish this we are almost at the end and
we can talk about more details but the
main idea is the following we are going
to perform oblivious RAM reads to fill
in these locations right so we are
trying to hide the access pattern of
these reads so we perform reads and we
want the adversary not to be able to
distinguish different sequences of reads
and we want this to hold even for the
cases where we keep rewinding the
adversary I will get very good let's say
observation now these I am going to skip
because this no longer matters much
there are more efficient constructions
so one open problem we had in this paper
was is the use of oblivious RAM inherent
in this dynamic view our type of
solutions so do we need these oblivious
reads oblivious rights etc it turns out
in Chandra metal and sheetal
constructions we don't need a full habla
visa now what using oblivious Ram buys
us in dynamic PR is the following if I
use oblivious Ram with in dynamic pure
even the access pattern of the client is
hidden from the adversary so the server
has no idea what the client is doing
where is he reading where is he writing
etc so this is an additional benefit in
a dynamic pure scheme but the original
dynamic pure definition does not capture
this it doesn't require the dynamic
viewer to hide the access pattern so if
you don't want to hide the access
pattern it turns out you still need some
obliviousness for
security but not a full Orem
construction so I will going to briefly
discuss this after this talk so this is
a very interesting point we try to prove
any possibility result it turns out it's
not impossible now so what we know for
dynamic PD P is there is nothing secret
at the client site essentially in the
dynamic PD P construction there is a
public key that the client keeps and the
digest value so those values can be
revealed the client does not need to
hide those public key is for the tags
and essentially it's the definition of
an RSA group that's it one small but
important difference between original
static PD P tags and the dynamic PD P
tags is that static PD P relies on the
RSA assumption dynamic PD P only relies
on the factoring assumption so the tags
are simpler in their case because this
data structure on top takes care of
other things so the text can be simpler
you can think of it that way now for
dynamic PD P we know how to do this
without any secret state at the client
but Orem inherently requires a secret
state at the client so for the dynamic P
are constructions today we don't know
how to do them without any secret keys
so that's still an open problem and yet
another open problem we talked about
this next rate pattern hiding security
we defined does it have other
applications any questions about this
except once the neighbor results etc
that I will talk soon
okay impossibility or impossibility
visage
okay so then let me stop the recording
here and then we can talk about it on
the board in more detail</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>