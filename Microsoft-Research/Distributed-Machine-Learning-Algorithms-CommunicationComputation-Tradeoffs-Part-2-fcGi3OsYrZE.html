<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Machine Learning Algorithms: Communication-Computation Trade-offs - Part 2 | Coder Coacher - Coaching Coders</title><meta content="Distributed Machine Learning Algorithms: Communication-Computation Trade-offs - Part 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Machine Learning Algorithms: Communication-Computation Trade-offs - Part 2</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fcGi3OsYrZE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so we will get started so just to
quickly recap see we basically looked at
how the data gets the distributed in a
big data scenario and we also briefly
looked at what sort of computer under
communication model that it is useful
for solving these machine learning
problems in a distributed setting and we
need some basic cost analysis for the
loss function competition one can carry
out similar analysis for every step of
the basic learning algorithm that we so
and we also briefly covered few
disappear computing strategies which you
saw as parameter mixing under the
nutritive parameter mixing so now
because our goal is to design an
official distributor machine learning
algorithm let us do this cost analysis
in little more details which will
motivate us to that topic so this is
just a quick reminder which you already
saw so I'll just skip so the main thing
is we'll be focusing on this part of it
just to quickly recap on the notations n
refers to the number of total number of
examples emmys number of examples per
node I use the notation in an underscore
p just to remove that dependence p i
just introduce this new notation em here
and p is number of nodes d is featured
on dimension and you know in that basic
learning algorithm we said that there is
an i mean it is an iterative algorithm
so these m refers to
the number of iterations so here I will
call it global or outer iterations I
also introduced a new variable called
IAM underscore p which is the number of
iterations that one may have to do
locally for example let's say you want
to run a stochastic gradient descent
algorithm locally then you may have to
pass through many examples or something
they are right so so there are several
distributed machine learning algorithms
you know which carry out this I mean
which require several iterations in
order to produce good model parameter or
direction for the optimization algorithm
and so on so let's I mean in some sense
you have seen all this before but it is
good to go or it so let us assume that
we are in that instance wise
partitioning space and there are two
quantities that we want to compute one
is this total computation costs and
other one is the total communication
costs so the notation see comp refers to
you know the complete this comp refers
to the computation costs and Here Come
refers to the communication costs so the
here you can see that this is called a
total because another term P appears
here although I may do the computations
paralee if you want to look at what is
the total computation cost you have to
add them right so the factor P appears
here and then we see this factor MP
coming here which I mentioned before
which is if in each node there are
multiple iterations involved then the
factor MP comes here and between these
two you see this product M times D
because as you have
before in the instance which
partitioning in order to compute
quantities quantities like function the
computational cost at that time we saw
an underscore p times d which is what is
written here as M times D here so MP
appears because of you know local
learning algorithm run over multiple
iterations example is stochastic
gradient descent turning over multiple
examples and M is needed because you may
do one pass over all the instances and D
is needed because you may look at all
the features in general even just one
look of all that entire matrix X will
cause to that of course you know this
cost may be slightly reduced if you have
a sparse data but for our analysis let's
just keep it this way now let us look at
this communication cost so here again i
know this p comes in because imagine a
situation where each worker node has to
communicate a d-dimensional vector to
the master and vice versa for example
the master may update the weight vector
and send it back to each one of these
nodes so that is why you get this term
and this factory M comes because you
have the number of outer iterations of
the global iterations so your total cost
is going to be P times D times M here so
that is on the cost part now let us
introduce some notation to capture this
time part where TP refers to local
computation time taken per node so you
are doing certain operations now you
look at commerce time it will take to
complete that computations and similarly
you can define there is I mean the
global computations happened in the
master node so you can ask how much time
an order takes and now we said that
there is a communication phase where
worker nodes communicate something to
the master and vice versa so there is
some communication time involved to
transfer information per iteration and
depending on the programming framework
that you use there may be a set of time
I mean it may not be there also so then
it is you know it's very simple thing to
see that the total computation cost is
nothing but the local computation x +
global computation times x g because the
one you see within the parenthesis is
the per iteration cost now we introduce
it is the set-up time also as part of
communication time here but it's ok they
say i already mentioned so you know this
may not arise in all situation but if it
arises then you need to take into
account and the TC is the main thing
that you know maybe we'll have to worry
about because this actual communication
costs that you cannot avoid as long as
you transfer some data back and forth
now the main comment is amount of course
the total time is the sum of these two
and then the main thing is in the
distributed settings this T
communication time can be significant it
is particularly happened in the
commodity clusters situation so we
looked at the costs under then we looked
at the time now we can try to look at
what happens let us say if we were to
run a distributed batch gradient descent
algorithm so in this case I have assumed
that there is a sparse data so these n
is that by P is the per computation cost
and you can you know work it out why
this factor to comes when you look at
the batch gradient descent algorithm
now the company's communication cost is
defined by this term beta times P times
D P you know why it comes d you know why
it comes because if I want to pass D
dimensional vector that much cost you
need to incur but now what is this
factor beta see our goal is to compare
the computation and written
communication costs so we want to
understand what is the relative role
played by each one of these so that is
why you know this factor beta is
important so typically in order to do
one floating point operation the time
taken this lot less whereas if you want
to transfer some smaller chunk like one
byte of data or something the cost could
be different so we want to factor that
into account so in other words
computation speed is typically high
communication speed is slow so now this
factor could be flying the range 10 200
depending on the actual communication
infrastructure that you use one thing I
want to make here is that you know we
considered a very name model where the
workers directly communicate to the
master but that is not necessarily the
only model that one needs to work with
so one can look at in a sort of a tree
topology where you know you connect to
two nodes together and then they in turn
get connected and so on which can help
in reducing this factor P to log p these
are all standard tricks that people
losing networking community and so on so
but that's fine but as long as you keep
in mind that you know it need not be
always p you can make it efficient by
taking something into account on the
system site are constructing a tree
topology that's good now we would like
to consider in a two regimes where what
we call a small D regime and then large
de regime now the reason why we are
doing this because if you look at this
communication cost that is dependent on
D because once let us say you are chosen
the number of nodes that is P is fixed
and the beta is also
once you have the system so now the
communication cost is completely
dominated by this variable D so let's
see if your D is small now what it means
is that this cost is not going to be
much it will basically means your
communication cost is going to be far
less than the computation cost which
basically means you will get nearly
linear speed-up in P because effectively
all computations you are parallelized so
therefore you get this speed up of P but
the situation is not so good when this D
becomes very large okay that is where
the problematic thing now comes the
question that how do we trade off this
computation versus communication cost to
speed up the algorithm so that is the
main team here so this is a important
thing to note and understand all right
so whatever highlighted in blue we have
already covered so let's get into that
no last but one part of this so
efficient distributed machine learning
algorithms so we will look at the three
algorithms there are few things to keep
in mind one is our goal is to trade of
computation and communication costs so
we will understand how we each one of
these are will I know will try to
highlight of each one of these
algorithms handles that part second
thing is these are not the only methods
that you will see in the literature
there are many different methods and
what I have chosen is one particular
case of instance which partitioning
under than what are some of the popular
methods that we know of and similar
situation exists in the feature wise
partitioning scenario also which I am
not covering in today due to lack of
time but those references are given at
the end of this PDF so you can look at
that so just to come back before I get
into the details of these algorithms I
just want to hi
late so four key ideas that are used to
do this no trade-off so we saw that you
know that total communication time is no
given by this factor now as I said
before this TS is the set-up time 100 TT
TC is the actual that data transfer time
or communication costs associated with
that so before they always park under
the Hadoop beyond all these versions
came the problem was still difficult to
solve in that Hadoop MapReduce prion
version of that so so one thing that
people wanted to do was ok this TS is
really killing this speed up that one
will get to make the algorithm efficient
so these are grown at all they came up
with a nice Hadoop compatible already
use framework idea I will briefly talk
about it later in order to reduce this
time completely I may not be completely
but mostly yeah and of course you know
there are these new frameworks that have
come up so if you really look at it in
this so this TS part is to a large
extent at rest so this is one basic idea
and the other one is which I briefly
mentioned before which is the cost is
this beta times that p times d now if
you use a proper some sort of optimal
topology you may be able to reduce this
costly to log B log P times DSM in other
words this P gets reduced to log P D
still remains do not read it as log time
log of P D bit T still remains and the
other one is that even when you have
such a tree structure when you have a
large amount of data you can implement
it in a pipeline to way so that you can
even reduce this factor log P even
further okay there will be still some
cost
but it will not be as is log P or
something okay so this is another trick
that people use to you know reduce the
mean in this case now you can see one is
when there are only three terms here
right so if you reduce one of that you
hope to get some improvement out of it
so this part talks about reducing TS and
this parts some extent address reducing
TC now the third one that is left out is
M so
yeah yeah this all covered as part of
that yeah yeah on it ok so the third
thing that we see the CM which is the
number of outer iterations in that
iterative algorithm and of course you
know it's obvious that if you reduce the
number of outer iterations your
communication cost goes down now you
know there are several ways to do that
so one is now from optimization you know
talks you learn to that SGD has better
initial convergence ok this is some
experimental result i'm not sure where
that cheese and Schroeder sewage sort
and L bfgs like second-order method they
have better convergence near the
solution so the one obvious thing that
comes to the mind is that now can I make
use of these observation and use it in
some way to reduce these number of
iterations m so that is the basic idea
so again these are little they propose
to hybrid method so what they do is in
step one let us say that the data is
distributed in P notes now what do you
do is that you first run this stochastic
gradient descent algorithm parallely in
each one of these nodes okay so you just
to do your few passes typically even
less than fine number of passes is
sufficient to get a good solution but
they are all computed for individual
nodes now you combine them and you get
one weight vector similar to what we saw
in that parameter mixing method no
yeah correct so that cast is fixed right
we are not now correct yeah I mean
ideally you should include it so yeah
but that is fixed now once the data is
distributed now I am going to run the
algorithm now how do I reduce that part
that is what here yeah
typically its pipeline
example of a framework these already use
framework what I mentioned here right so
that is something that people use yeah I
mean are you asking that it is natively
supported is that your no no this is
your specialized to think that they
developed this Hadoop compatible or
reduce framework which the these are
grown literally developed I mean if you
ask whether it is available as part of
native Hadoop no okay all right so this
is a simple trick that they use to based
on the observation from that
optimization lytic surrender this one
but it is very effective actually yeah
the point is that when you don't have to
think that we have to do really very
complicated thing to make this algorithm
efficient even mean such observations
really help you to design effective
algorithm I mean that's the main point
right and yeah that is a nice thing they
did in this work littering yeah then
this is another example so where we want
to again reduce this number of outer
iterations so if you look at you know we
are solving an optimization problem
which iteratively under then so what we
are trying to do is we are at the
current to iterate w of tea or something
you want to take the next step so one of
the important thing is that you need to
find out which direction to move okay if
you can find a good direction then the
progress that you will make will be good
and the hope is that you know you will
be able to solve the problem much more
quickly in other words the number of
iterations that you need to
outer iterations that you need to take
will be lot lesser in that process you
reduce this communication costs so the
idea that is proposed in this work is
basically no I want to find a better
descent direction but then finding but a
decent direction does not come for free
so you may have to increase the
computation costs okay so the main
observation our suggestion from these
work is that make communication costs
Commons rate with computation costs and
this is achieved for example you know
you have a better local learning
algorithm suppose you imagine that you
know there is a local learning algorithm
which will do multiple passes over the
data locally okay let us say it runs the
K times now each time you run you incur
a cost of the sinzi over p so the total
cost computation cost per node it turns
out to be just a product of that now the
idea is that ok now if I increase K now
my this total local computation time
goes up now if i can adjust make a then
you can sort of make it to comment rate
and of course the question is that no
why should i do this view your goal is
to get bit descent direction so that's
where the trade-off comes into picture
now we will see how that can be done now
one thing is it of course we are coming
up with this new algorithm and then they
all look like a bunch of tricks but
ultimately you know we have to make sure
that whatever recipient machine learning
algorithm you come out wait we have to
make sure that the converse because my
goal is to solve that original
optimization problem and if I for
example if I take a convex optimization
problem I want to get to the global
solution not stop somewhere under them
get some sub optimal solution so there
is something one has to ensure and other
thing is that ok so when you are doing
it also in all this batch gradient
descent is an exact algorithm
we know that so which means climates a
better rate the same accuracy which is
equal to getting that global solution so
the now the question we are asking is
that whatever new algorithm that we come
up with can we do much faster than the
name distributed by the gradient descent
now in a distributed setting if you ask
the question is it always possible
probably not okay and that is the little
bit of a tricky part okay so because we
already saw that for when number of
features is very small D communication
cost is quite less so even simple this
distributor bgd can give you the linear
speed-up okay so you may not really gain
so it depends on the real scenario and
also the communication cost of the
underlying infrastructure that you are
working with so so now the best so there
is one algorithm at least we know that
this this fadl stands for functional
approximates and based distributed
learning algorithm which can meet all
these expectations so justly to give a
quick idea about that method so as
earlier we assume that you know data is
distributed instance wise and so this
paper this work assumes the my words
with linear models and eat salsa up
Porsche in the primal space so here is
the last function that we have is the
standard regularizer and you have the
last function return as sum over the
nodes because we are dealing with the
instance ways partitioning and our goal
is to get better descent direction right
for that is the goal here now what this
method does is the core idea is the
following now so each node wants to find
the weight vector W let us say
now the best way to do that is to have
all the data in that node but that is
not feasible okay so how do I get that
so only way you can do is that sort of
approximate the function of the last
function that represent the other nodes
okay so which basically means if your
approximation is good basically what you
are saying is that individual node is
optimizing the whole objective function
but this may not be always true but the
idea is just to make that approximation
now when you do that approximation then
because you have information about the
data present in the other node the hope
is you will be able to make you know
better judgment of how to move that's
the basic idea so which is what is
pictorially shown here so here this if
one of Omega are the W so that is the
function that you have at node 1 and DF
2 is what you have with node 2 under the
f of w what you see here is the total
objective function now suppose if i am
using only this information may be hard
to find the appropriate descent
direction on this total function f of w
on the other hand here we pictorially
show an approximation to this function f
20 of Omega now this f2 of if to hat if
you look at it it is a sum of these two
which means f1 of Omega plus the sum of
this now if you look at this green
function that looks closer to f of w in
other words if you look at the minimum
of these two function they look very
close it basically means with this
approximation i may be able to find a
better direction to move in that process
i will be able to converge faster there
is a basic idea now of course you know
since you are going to add this term it
comes with some extra cost and after you
have added these now we will do a local
learning to
with the white actor and so on so the
here is the whole algorithm but I will
just point out some key steps here
without getting too much into the detail
so this far loop that you are seeing is
that outer loop and this WR that you see
is sort of the global weight vector and
basically what happens is you compute
the gradient at this current iterate at
the global level and now you ask the
question you know whether i satisfy the
termination criterion or not if yes you
just got if not then you pass that
information to all the worker nodes
under then do the parallel computation
now this gradient formation that gets
passed to all the north straight so and
that is in needed to make that
approximation that you saw here in other
words in order to make this in order to
get an estimate of this you need to the
gradient information ok so that is the
basic idea now once you do that you know
this is step three what you see here is
basically the parallel computations that
happens in the P nodes and you see an
inner loop here see that the K hat
appearing here so this is where you have
the local learning algorithm and do you
run through multiple iterations so now
the hope is that by controlling this
parameter K hat we will be able to get
better direction so that is the now so
the algorithm basically the local local
competition algorithm is basically runs
this k times and it produces an output
and then what you do is you form a
convex combination of that one can show
that if these individual direction is
decent even the convex combination will
also be decent direction so so you can
look at the paper for more details
button that's the idea and then the
master now you can do a line sets in
that direction
and then decide what is this step to
take under then update to the model
parameter so the key thing to note here
is that the various control parameter
key k hat that we have now which can be
used to make the trade of the
computation in the communication cost
now some key observations about the
algorithm so so we are able to prove
that this algorithm has global linear
rate of convergence I mean this globally
narrator you know what it means to be
because you've written to Chichen
already talked about it and it is a
general method because the function
approximation that you make here mean
you can choose which one to use so for
example a simple linear approximation
has this form what is shown here and you
can also have you know quadratic
approximation and so on and the in the
experiments that I will show later we
used to this quadratic approximation
form and we found to get the quadratic
approximation form gives better
performance and this linear
approximation but of course that comes
with a little more cause but the
advantage is that the number of
iterations that you need get reduced so
in effect you see good improvement in
the efficiency on the algorithm and
there are another nice thing about that
algorithm is that since there is a local
learning algorithm involved you could
use any algorithm it could be stochastic
gradient descent or retron or anything
and the nice thing about these
theoretical result is that the proof
goes through for any of these methods
okay and the other important thing that
you know because we talked about it on
the algorithm fruit converts so in order
to prove certain conversion property
then one may ask the question that a nor
do I need to solve this problem locally
exactly or can I do approximate solution
it turns out that you know even you can
do at least stopping under certain
condition you can show that the
algorithm will converge
and there are other no results so
earlier I mentioned that iterative
parameter mixing that we saw earlier
does it have a proper convergence proof
and in this particular case we are able
to prove convergence because if you
really look at it what you are doing is
this convex combination of this which
can be seen as an iterative parameter
boxing algorithm therefore we have a
provable convergence recent result for a
IP a method and this is also a good
parallel is GD method and with some
convergence proof so I will not get in
details of that but the main point I
want to make is that it is a very
general method which is powerful as you
will see from the results and it also
has some nice properties okay so our
goal was to you know decide efficient
algorithm compared to the basic
distributed batch agree orientation so
this slide shows you know the analysis
of comparing distributed bgd method with
the photo algorithm it a just a
described so this is not true overall
time actually there is a factor am
missing here is a typo there but this is
sort of a per iteration cost and so here
you see different components playing
this beta D we already saw which is the
communication costs and the first term
is basically the computation cost and as
you can see here this MP is the number
of iterations that you see here and
these n is that by P is the per
iteration passed under D is depending on
the method that you use for example if
you are using tron there may be some dot
products involved due to which this cost
comes in and so depending on whether you
are using both it on our distributor bgd
algorithm and Ford algorithm can be
recommend the cost of that can be
written in this form so this table gives
you what are all this parameter c1 c2 c3
in these two different
settings e so as you can see here this
dissipated batch gradient it's because
it just computes the partial gradient so
there is really nothing like inner
iterations here whereas this photo
algorithm has this inner loop and then
this K hat comes here and under there is
this factor too because of the what is
the information that will you send back
to the nodes so now suppose if you
assume that the number of outer
iterations is greater than three times
that is needed for this then one can
show that you know this relation I mean
there is a proof in that paper if you
look at it but the main point to notice
you know for order to give good
performance you will see that suppose if
d is very very large right then you can
show that this value will be very small
which means that this relation will be
easily satisfied so you will get good
improvement with this photo London and
in fact if you look at it earlier we
talked about distributor be GD is not
good for large city and there is exactly
the scenario that you know that comes in
here so for large d for which we wanted
to address the problem that gets
addressed effectively by this for a
logarithm and because I mean of course
this the factor n Easy appears here but
that is completely determined so if you
have highly sparse data this yinz will
be small so again you get improvement
with this be something that you will
observe the results in the experiments
now moving on to this terascale learning
work now so as I said that they were
trying to address that problem of setup
time so what they said was that in some
sense you take control of all the worker
nodes and then construct a topology and
how do they communicate with each other
and so on
so basically so that when you are going
to run your entire algorithm you know
there is nothing like intermediately you
store some data in the file system but
then bring it back under nothing of that
sort anxious so they / they propose to
the spanning tree server model with the
each node so this is I mean simple tree
you have and imagine that each node has
this apparent IP parents raised IP
addresses and imagine that you can pass
the information between these nodes by
establishing TCP connections so once
this information is available on to TCP
connections are established then you can
run your algorithm until you finish your
completed you know completely though so
these already is basically does the
following so here is a simple example
that I over time here so let's say that
these are the numbers that the one that
are highlighted in red that are the one
that represent in them node and now you
simply want to aggregate and then pass
it back to all the nodes so this
intermediate I will know to adds this
number 14 plus 27 to get the answer 41
and similarly we read right hand side
sell to the same thing and then d pass
information higher up where it gets
added to produce the result 64 and then
they send it back so at the end what you
see is that all the nodes have educated
information 64 now this is something
that is useful if you relate it to the
descent algorithm in the parcel grading
competition now you can imagine that the
parcel gradient information is what is
available in each one of these nodes and
then they get pushed to the next level
node where they get aggregated and so on
and then finally at this node the entire
gradient I mean all the partial
gradients get aggregated and now you can
push it back to all the notes so that
all of them are of the same information
now as you can see that if you use this
architecture or the topology the
communication costs will be that factor
P will go from
go to log P that is the basic idea but
one catch is there you know there is a
limited fault tolerance because if any
of these nodes because you have taken
control of this entire tree and then if
something goes bad then there is no one
to fix the problem unlike what is
available in and hadoop mapreduce
framework who takes care of that so in
order to address that problem what they
you know they worked wishes some idea
called a speculative execution so what
they do do is that you know I said that
in the first step they run this parallel
SD dialga so at the end of it you know
there will be some nodes that will
survive so when they move to the next
step they make sure that they work only
with that survivor notes and then
construct the tree and work with that
the hope is that after that those nodes
won't fail them in and this is something
that experimentally they found and then
found it to be very effective and then
they have some statistics which tells
you even in several thousands of
machines the probability of a node
failing is so small so in the sense that
within the time interval that requires
to run the algorithm this problem does
not exist so in that sense you do not
have to really worry about that problem
but the idea itself is very nice and
then 90 the pipeline implementation one
can imagine for the tree structure and
then this we already talked about and
this go the last main another method
that we talked about this communication
efficient anticipated dual code in a
decent this I just took it for two
reasons one is so now it is a dual
coordinate method under this is also
very effective and it is the iterative
primal dual algorithm it again works in
that instance wise partition scenario so
as you know that in the dual way problem
formulation you have this coefficient
alpha and you know that with each
example there is a coefficient golf
associated with that so now we are
dealing with this instance which
partitioning what it means is that the
alpha
bitter sort of local variables for that
particular for each partition okay so so
this BP that I indicate here is
corresponding to the set of examples
that is present in the node and alpha BP
represents the dual variables associated
with those examples that what it is yeah
now in this a algorithm what they do is
that they do this block maximization of
these dual variables in each one of this
node we have recall Jesus presentation
he talked about optimizing the objective
function on a subset of variables
keeping rest of the things fixed means I
mean essentially knows similar thing you
do here and we also saw that that w-wait
vector can be represented as some of
this you know dual variable times the
input feature vector so using that
relation you know they update to this
they find this weight vector update and
using which the primal variable is
updated and then again that the same
iterative loop is set now one question
to ask is that a no why's this how is
this algorithm trading of the
computation and communication costs here
again the thing is that in the when you
are doing the local competition how many
iterations do you want to run so there
is a control knob here in terms of you
know how much time you want to spend in
the not doing the local competition so
that you can get better update here
yeah this is you know this i mentioned
enough kind of wondering i think all the
experimental results they reported in
data spark programming framework under t
is in significantly so there is one
method which you must know i don't
remember cheesin are sweet talking about
it but this is one important
optimization method that you need it is
good to know from distributed
optimization perspective it is a very
good paper written by boy a little and
determine anybody who is interested and
distributed optimization must read this
paper and so on very nicely written
paper so basically this all the problem
of the following form shown here so you
have this function f of X and de venir
so there are two sets of variables you
have x and dizzy and there are some
constraints so basically their idea is
to solve this optimization problem using
augmented lagrangian method using method
or multipliers so due to lack of time i
will not get into the details of that
but the main point is that using this
method we can we can solve that the loss
minimization problem in that instance
ways partitioning and this way so for
example you can imagine this if mean as
earlier we have this loss function
defined for individual nodes that is
what you see here and each node
maintains its own weight vector which
what is indicated as WP and there is
this global parameter w because
ultimately you need a global solution w
and these constraints are placed because
as you solve this algorithm iteratively
at the end you want all the weights to
be you know synonym same right and the
nice thing about these algorithm is that
you know there are three key steps here
one is given the global model w
t and the Lagrangian variables
associated with these constraints you
can solve the problem locally in
parallel so this is the parallel
learning step and in the global learning
basically given your current iterates
from all the nodes and the lagrangian
variables how do you get this global
weight and there is a dual action step
where you want to update the electrons
in variables and this can be solved
iteratively so this in some sense as the
flavor what we already saw so it is a
very nice algorithm 11 tricky thing is
that there is a parameter called a row
which is a penalty parameter and it is a
key quantity that we need to set it
properly in order to get to good
performance but it's a nice method to
try it out so here are some experimental
results so these are some of the data
sets that you know we experimented and
so these data sets are selected because
of the diversity as you can see here we
are dealing with this big data scenario
one is that only one thing at least we
want to make sure the number of examples
is very large and this since we are
interested in trading min we are
interested in understanding how this
computation of the communication trade
of words we wanted to vary the number of
features so you can see that it goes
from Ueno 784 to all the way up in
millions of features and of course you
can see that if not the sparsity is
quite high in four of these data sets
whereas it is different in this mls the
data set and so this table basically
gives the computation to communication
cost ratio and see whenever you are
solving this machine learning problem
now you want to
so there are two things one is you can
observe how your objective function is
moving right and other one is you can
also look at how your performance is
improving when I say performance I am
talking about accuracy our area under
precision recall curve and so on so the
numbers what you see here is when the
algorithm was terminated within point
one percent of the best value that you
can get on 128 notes that is what this
is this table is giving which compares
all these four different try algorithms
on different data sets now as you can
see here this algorithm for Dell gives a
value close to what most all data sets
whereas if you look at the cost they are
very quiet I mean they all vary quite a
bit across different algorithms and you
know by trading of this communication
with a common computation cost making
the comments rate the algorithm is able
to become a lot more efficient plus the
main point so here are some results so
so this x-axis shows the time taken in
seconds and y-axis is the relative
function value difference basically you
know if you run the algorithm to run for
long time it will settle at some
objective function value now with
respect to that you measure this
quantity now so this is for the kdd data
set where you know the number of
features is very large right so man very
high so the left a plot shows the
performance with the p equal to 8 and
the right blood shows the performance
with p is equal to 1 28 and there this
fertile algorithm is given by this red
line and the green is this cocoa
algorithm and so on so as you can see
that the photo algorithm you know drop
squared sharply
compared to other methods in both the
scenario and this is on the amnesty data
set now as you can see here compared to
the earlier one there are many methods
that are coming closer to here because
the dimensionality of this featured set
is small it is just 784 so the photo
algorithm really has advantage when the
number of features is very high and of
course we assume that the number of
examples is large and so here so this
figure basically shows that how does I
mean terascale learning is a very good
you know algorithm it's a state of the
one of the state-of-the-art methods so
we wanted to compare how the other
methods perform in terms of the time
taken with reference to that particular
method so this is again showing for two
different data set one is M nest under
them the right one is a right hand side
you see for kdd data set so if it is
exactly same as terascale evening then
you expect you a flat line here okay my
not one if you see high value here which
means that that our algorithm is better
so as you can see here the improvement
given by this fertile algorithm is
roughly five times under they sort of
uniform across the number of nodes on
the other hand on the amnesty data set
you do not see that much improvement it
is still good but as the number of nodes
increases this becomes really large
because as you can see when P becomes a
large the communication costs also goes
up in some sense you know that explains
why this happens so that completes the
you know whole story so I don't have
time to go through this but pretty much
we have covered all the topics in the
distributed machine learning setting so
there are you know many other things
that we didn't cover of course as I
mentioned before we also have feature
waste
auditioning version of this functional
approximation method and there are this
parallel and distributed coordination
methods so particularly you know there
are some papers by peter is tariq and
metal that you may want to look at and
there is also a hybrid method okay which
basically means you will not only have
the distributor setting but even in each
machine you can prove you can do some
multi-threaded programming so you get a
hybrid method and of course we didn't
talk about the non linear kernel methods
and if you recall cheese and talked
about the problem of storing this kernel
matrix and how do we handle so we have
not talked about it and there are few
other topics how do we learn a decision
tree models deep neural nets and so on
so you come to the last part so of
course there are some basic questions
that keep coming right so why not
subsample big data so that it can fit in
a single machine and then work with that
yes it is possible in some applications
in other words you may not need the
entire data to get the maximum accuracy
achievable but then the trick is how do
we subsample the data how do I know that
the subsample data can give me the
maximum accuracy achievable so that's
bit tricky and you know in some cases
it's not possible because if you
subsample it you are going to lose on
accuracy these displays I trainees and
data set is something that grow little
showed that the performance difference
can be you know quite significant so it
doesn't make sense to do subsampling and
of course you know when you are looking
at subsampling algorithm then that needs
to be fishing because otherwise you will
defeat the purpose of having an
efficient algorithm enemy but still it
is cost ray so if you look at the
overall time you still incur loss but
you do not do learning multiple times
anyway right so if you are talking about
repeated
training that is a difference of area
but then if it is repeated training that
means that new data has come in that
means again you have to do subsampling
so it is not that straightforward then
other basic questions that you know when
do we use the parallel algorithms
optimized for this of course you know
the obvious answers are here data can
fit in a single machine and we already
saw that most of the machine learning
algorithm freighter the computations can
be parallelized you can expand at and
your communication infrastructure is
pretty bad and you do not want to use it
and you know these are some of the
numbers which shows that about this
communication time this I am and this we
already talked about we can't mean you
need disability machine learning
algorithms when data cannot fit in a
single machine we cannot do subsampling
the other thing is the data itself
naturally arrives in that computer so is
another thing data load time from this
to memory say so far as I gave you an
example earlier that if you have one
terabyte of data and if you are going to
read it sequence lee that is going to
take lot of time when you split it into
100 gb reading each one is faster so if
you include that cost also you know this
may still be a winner in some cases so
one thing that you have to watch out for
is that although we talked about
distributed machine learning algorithm
this PDF may not be always linear or
something okay so in fact there is a
nice article on this which you can read
here which is say is basically that you
know speed of improvement that you get
is not what it is okay so it's actually
the data itself is like that under then
you have no other way but to process it
to that way but did you eat in some
sense it is overstated here in my
opinion because there are scenarios
where it can be really beneficial and so
we talked about two things so it is a
multi-core shared subsystems versus
distributed computing is they come
hybrid
method yes it is possible and and of
course and this is the famous thing
people use GPU systems for learning deep
neural Nets but then people also
recently asked the question is GPU
systems only useful for deep neural nets
are can I also use it for building let's
say Colonel models or even linear model
toward and they recently there is some
article which came which actually shows
that GPU systems are very good even in
those problems so you can read more
about it here so finally yeah just to
conclude so basically if the data can
fit in memory you know you can just use
multi-core systems so that you don't
have to worry about in all this trading
your computation communication costs and
so on use the system's match due to the
models okay so currently at least we
don't know any good solution where deep
neural Nets can be learnt in a
distributed setting for example in a
Hadoop MapReduce framework or something
use better and appropriate programming
frameworks and we already talked about
some base of making this computation and
communication times in comments rate and
of course you can always continue to
design faster learning algorithms which
better convergence rate and finally
don't always expect the linear speed-up
with the number of nodes so there are
you know few references these are just a
representative but you can get more
references from these papers so that
will stop thank
here these slides will be made available
so I think are now any questions
sorry multithreading on a node will be
more efficient or many mappers only node
if there are 10 nodes and we are giving
the salon spark that there will be 30
mappers so 10 members three mappers on
every node who if that will be more
efficient for there are three threads
running one is single single node how
many notes how many cores are there then
note 10 cores are this and 30 peppers we
have assigned so ready members on every
code correct so that will be more accent
or there will be three threads on a
single por el alone distribute you said
oh distributor and it's a distributed I
meant that there are multiple machines
right so that is what there are multiple
cores which are in each machine you have
multiple yeah coach in every core there
are three threads okay means you can I
mean the main message is that you know
wherever communication cast is involved
right just to be aware of it okay
understand that part that will tell you
to make this design choice is what I
knew your questions so I have a brief
question one eye level question so any
comment how many of these distributed
optimization methods can be transferred
to sort of sequential tasks of
sequential nature's like online
prediction or something toss of sequin
we're basically data keeps coming in
sequential eating some estimate oh man
this data arrives in not in a batch
necessarily but or streaming scenario
and I don't have a good answer because I
don't have a good experience with that
okay so yeah but it's an interesting
question I mean the thing is what is not
clear is what is the underlying data
distribution model that you have so how
the data itself arrives see in some
sense here you have sort of
set of nodes assigned to you and then
you are running jobs on them right now
they say that the data is coming so
typically you know from your file system
data gets loaded into these machines or
something when they arrive now you need
to look at how this data is going to get
distributed to these worker nodes for
example so there is a distribution time
cost involved there so it is not clear
to me how you can control under then
manage that so you need to take that a
distribution time also into account
which you probably you may not think too
much in this particular scenario so
maybe you'll have to take that into
account yeah yeah correct but the thing
is that they take control of the entire
nodes with you right isn't it so then
you have some flexibility of when the
data comes out we are going to
distribute eight o'clock think there's
an additional scheduling yeah exactly
correct so through which you can control
under then do an accelerator to exactly
correct that's under your control yeah
yeah that is possible yeah okay so there
are no questions let's anchor dr sanjay
run again present him with a small token
of our appreciation thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>