<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Theory Day Session 2 | Coder Coacher - Coaching Coders</title><meta content="Theory Day Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Theory Day Session 2</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pU4mrSeCmR4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
all right till welcome so you should pay
attention this is the only talk of the
day from someone who does not report to
Jennifer offer decade will tell us about
bandit convex optimization thanks yeah
so this is a joint work with sustained
go back and with the you Val and with
the total Quran who's a student at the
Technion it's going to be about learning
theory I'm going to talk about one of
the you know most important still open
problems in learning theory really one
of the kind of embarrassing gaping holes
in understeer in our understanding of
online learning so here's the problem
it's just jump right into it okay so the
problem is adversarial banded convex
optimization will parse what this means
so this is going to be a tea round
repeated game between a randomized
player and an oblivious adversary so the
player has access to random bits and the
adversary does not adapt from round to
round so he's oblivious to the player's
actions the player has an action set and
this is going to be some convex set C
which is known in advance the number of
rounds in the game is also going to be
known in advance and here's how the game
proceeds so the adversary privately
chooses some sequence of functions okay
f1 through ft and each one of them is
convex so which one maps each one of the
actions that the player can play to the
interval 0 1 so they're also bounded
they're bounded in convex but otherwise
there's no relation between f1 f2 f3
they can change arbitrarily from round
to round and he chooses them privately
so the player doesn't know these
functions at all and then the player
starts to iterate right so the adversary
is oblivious so he can make all his
decisions before the game begins another
player plays the game so T rounds of the
game on each round he chooses a point in
the convex set and plays that point he
can do this using some random bits he
incurs the loss which is the value of
the function so if he's on round T and
he chooses the point XD then he pays a
loss which is f t evaluated X T and this
is the number that he sees and he sees
only this number and then he has to go
on to the next round okay so he's
collecting this loss as he's going on 40
rounds let's see a little picture that
shows this so again this game starts out
by the adversary choosing the entire
sequence of functions so here the the
action said that the domain of the
functions is just the two-dimensional
square and you choose as arbitrary funk
they have to be convex but that's it and
they have to of course have a bounded
you know so they have a bounded range as
well but you can see they have no
specific form other than being convex
and then the player starts to iterate so
on round one he chooses a point again he
could do this using some randomization
and he plays this point and he incurs
the loss the loss is just the value of
the first function at that point but he
doesn't get to see the function he just
gets to see this one number 0.3 okay and
then he goes on to round number two and
chooses another point and suffers
another incurs another loss and so on
and so forth okay never gets to see any
of these functions but it get collects
this loss okay is that clear right this
is so so this is a convex function so so
you're the best fingers are actually not
going to be on the boundary of the body
but somehow inside but it could be they
could all be linear it could be okay but
generally speaking they the adversary
can do whatever he wants they could be
inside they can be on Dean under they
not necessarily on the boundary because
because he's lost about two okay yes
okay so let's define a few definitions
the expected cute of loss of the player
is just the expectation of the some of
the losses that he incurs as he plays
the game this is his loss this is the
thing that he wants to be he wants this
to be small but you know since the loss
functions are arbitrary just wanting
this to be small is not enough we have
to compare to some benchmark it's
meaningless to just look at this loss
because the adversary could just choose
all the laws fun just to be the constant
one right so you can always incur a big
loss that's not the point here we have
to compare this expected cumulative loss
to some benchmark the benchmark what is
it it's just the value of these same
functions evaluated at the best fixed
point in hindsight so if you take the
took the offline problem if you knew all
these f's you just solve this convex
minimization problem just find the point
that you know minimizes the average
function that's what I want to compare
to so the difference between what the
algorithm accumulates in expectation and
the loss of the best explained in
hindsight this is called the regret okay
so this is how the player penalizes
himself just to define our notation so
row will denote the policy that the
player plays so the player plays on
policy row this is the loss functions of
the adversary and this is the regret
with respect to those two things
so this is how we about a player
evaluates himself and now we can measure
the difficulty of the game is this
minimax regret it's just there it simply
they regret when both the player and the
adversary play optimally okay so its
minimum over all possible policies of a
player maximum over all possible loss
sequences of the regret played by the
player okay now just note that the
adversary knows the policy when he
chooses choosing these loss functions
this is important because later on we'll
see that the roles will be reversed so
he just who knows what the policy is he
doesn't know the players random bits and
I would say that this game is learnable
if this minimax regret is sub linear in
T all right so if in the worst case when
the player when the adversary is doing
his worse still the the rate at which
you accumulate regret is sub linear
means that on each round you're
accumulating something that has to 10 20
so you're getting better and better as
the game gets longer okay so you're
learning so this for learning just means
the thing is sub linear okay so how do
we know if a gay if this games is
learnable or not so let's take a step
back let's make some assumptions so
first let's assume that the fact that
the functions are also lifschitz but
this is a technical assumption will get
rid of this in a minute and now let's
pretend for a minute that instead of
getting the value of the function i
actually get something much more
informative i get the value of the
gradient of the function at that point
okay so just pretend that for a minute
and now in 2003 syncovich sowed show
that just simple Grady descent will
already guarantee a reversible in your
regret of square root T so this is
already kind of an amazing thing I mean
the function I'm taking is Greg and step
based on everything I've seen in the
past or the current function and
tomorrow's function has absolutely
nothing to do with it yet still I can
guarantee this learning you know if I
just follow the gradient path and the
idea here the idea of the proof is just
to show that okay I don't know what the
adversaries gonna do he can do whatever
he wants but if indeed the adversary
chooses functions that whose minimum is
very very different so he somehow mixes
it up I take a great in step in this
direction and he puts the minimum down
there and then I take a great in step in
that direction it puts them in am over
there then in fact you'll be hurting all
the points uniformly so this best point
in hindsight will also get worse right
what the adversary would want to do in
fact is not that but you would want to
hide a really good point hope that I
will never find it but make it
consistently good across many of these
rounds and if I do great into said I
will actually find that point
okay so that's the idea of why I can
learn even though the past in the future
have nothing to do with each other okay
so this is just something we have to get
used to there's a matching lower bound
so square root teased as good as it gets
okay but now let's get back to this
problem that we're that we do care about
not the one where we get gradients with
the one where we can all we only get the
valuations of the function it's for this
vendor for no so they're so for all for
any algorithm the worst-case adversary
for that algorithm will in fact you know
can flick this much damage any other
seize the day in age yeah for any great
either sale yet for any algorithm well
in fact it's for any algorithm they can
see the entire function so if you can
even assume that he received the entire
function okay so here's just a depiction
of this so again you know I get the
gradient I think the step for the next
loss i play this point and i take a step
for the next lost i pay that point and
it take a step and so on and so forth
this gradient descent the first real
progress on the on the bennett version
of the problem so Bennett means that i
only get the value of the function was
that in 2005 by flaxman Callie and
Brandon McMahon and here's the idea I
can estimate these gradients using only
one only evaluation of the actual
function at only one point so I can have
a one point estimate of this gradient
and I can run Grady descent with this
estimate so here's the estimate so i
estimate the gradient at the point that
i want to play by the value of the
function at a point that's nearby so let
me show you what these things are so i
start with this point this is the point
that I you know this is my state the
point that i have in my head but rather
than playing this point what i do is i
choose a uniform point on a sphere
around around the point that i want to
play so this is XD you is going to be
some uniformly chosen unit vector Delta
is going to be some scale the radius of
this of this circle and I choose a
uniform point in this circle and then
I'm going to take a step in the
direction opposite to this point the
size of the step is going to be
proportional to the value of the
function at the point that I played ok
so this kind of magically turns out to
be an estimator of the gradient so this
is how it goes so you have some point in
your head you have to pay a different
point but then you take a
estimated gradient step now the next
function comes along again I choose
another random point this time the value
is very low so I take a small step now
the values may be bigger so I take a
bigger step and you can show that this
is estimating a gradient step except for
a few things you know add some noise so
there's this estimator has some bias it
has some variants also I'm not playing
the point that gradient descent is
telling me to play and playing a point
that's nearby all these things
accumulate to some additional noise and
my regret bound from square root II
jumps up to t to the three quarters okay
so have to pay for all these estimations
yes x values values yes if you get to if
you can query the function at two points
then you can get squirty it's exactly a
question so so I mean look at the radius
of this ball right so this Delta it
turns out that if Delta is very very big
then the variance goes down if Delta is
it's very very small the variance goes
up and the bias behaves the other way
around when you can have a two point
estimator then somehow variance is taken
care of in your and you're good ok so
our bound is deteriorated to t to the
three quarters the lower bound is still
square root T so now we already have a
gap another interesting observation is I
can now remove lifschitz so i don't need
the function to believe and this is
simply a due to the observation that
when you have a convex function with a
bounded range it's effectively lifschitz
i mean if you know think about it you
have you have a bounded range you have a
bounded domain you want the few you're
trying to build the function that's as
non lifschitz as possible it can only
shoot up very very close to the boundary
of the set I mean it can't shoot up in
the middle because you know if it's if
the slope starts being very very big it
has to keep being very big because its
convex but it's just told you that the
range is bounded so you just can't can't
construct a function like that so
basically if you take your your domain
and you shrink it a little bit you're
already lifschitz in that area the non
Lipschitz part anyway is going to have
very very high losses so these are not
when you're gonna play you know in any
case so the range is 0-1 are saying that
the losses are always between 0 and so
using that trick you lose a little bit
more you get T to the 56 ok so it's
learnable so we can learn a creation and
one day they take
about it enzymes you're bonded pneus of
the function and yeah so I'll show you
the theorem in a minute i'll show you i
show that whole thing in it okay so now
we have you know it's learnable it's a
sub linear regret it's great but it's
very distant than a lower bottle squirts
okay so they did this this was beautiful
there you know we knew that it's
learnable we want to closed okay so
that's what this talk is about after
that a sequence of papers were published
that try to do better and no one was
able to do better in the general case
but in special cases people were able to
make improvements sometimes with the
same algorithm this estimated gradient
algorithm and sometimes with small
tweaks to that idea but still the idea
of estimating a gradient and doing
gradient descent so in a paper with Alec
agarwal and Lin Xiao we showed that if
the function is strongly convex you can
get the tea to the three quarters down
to the two-thirds another paper show
that if the function has it's a smooth
function again you can go down to two
thirds recently we prove this to t to
the 7-eleven so you see we're kind of
inching down words if the functions are
linear in fact and lift shits then you
can get the square root T the type thing
and we recently generalize this to
general quadratic forms so again in the
special case we actually have the tight
characterization also very recently we
showed that if the function is both
strongly convex and smooth you can get
square root see the idea being that
people looked at the you know the same
in dimensional problem and just said you
know if I restrict the adversary to
choose functions from us more specific
family i can get slightly better rounds
and sometimes even type out okay so this
was the game for a long time but
progress on the main thing was very very
elusive this is what we're going to talk
about today so we're going to talk about
the general case where we're just
assuming convexity and boundedness no
leaf sheaths no smoothness no strong
convexity none of these other
assumptions but we're going to talk
about the problem in the one-dimensional
case so surprisingly even in the
one-dimensional case we really knew
nothing better than this T to the five
sixth regret upper bound with a lower
bound being still square root ease okay
so our kind of first step into solving
this very very basic problem and all in
learning is just to close this gap in
the one-dimensional case so the theorem
that we're going to talk about is if
each one of these loss functions is just
the mapping from the interval 0 1 to the
range 0 1 and it's just convex that's
all I need then the regard is going to
be on the order of square root T up to
some blog where the block terms yep
question you just about the strategy but
it's not issue of computation correct so
so well all of these are algorithms that
you can compute and and it's another
it's easy to compute them and when we
can we run them our proof is going to be
non-constructive I'll get to that in a
minute okay but it's not but it's not
open even so particular issue the book
is not exactly with it that is
non-destructive in two dimensions is
still babe there will be a surprise at
the end versus but for her maybe let's
see ok so again we want to lower this
guy all the way down to square root T we
want to show a tight bound in one
dimension and this is how we do it ok so
first observation is the following we
can discretize so when we're in one
dimension or in fact also an arbitrary
dimension we can restrict ourselves
instead of playing the entire interval 0
1 to a grid ok so we can find an epsilon
squared spaced grid X 1 through X K and
restrict the player only two points in
this set ok and this is this is goes
back to a shot of your question so how
much do I lose so simple demo shows that
the best fixed point in hindsight within
my set is not much worse than the best
fixed point overall this is the penalty
I pay for this discretization so if I
this if I discretize finally enough then
you know I'm fine playing in this
discrete set and now my analysis becomes
easier because I'm just playing some
finite set of actions this exactly has
to do with the fact that a convex
bounded function is in fact kind of
lifshitz already wonder this also
generalizes to echo the images this is
this is hide the dishes not restricted
to one dimension no but then I mean I
don't know what k here but k might grow
exponentially in the dimension if
right and you you need pictures yeah but
that's not gonna be that's no no there's
no legit so there's no lifshitz in the
think this is exactly because a ballad
convex function is effectively kind of
lesions okay so sku shrimp see yes yes
we will see this in a minute it has to
depend on T so you're right k can be
exponential that's not going to be the
reason why we are a one-dimensional
proof we have some other technical
reason why our proof is only restrict to
one dimension so the exponential the
fact that you will your grid will grow
spiritually is not going to be a problem
for us we'll see this in a minute okay
so with this observation we can already
solve the problem using machinery that
we already have so a que armed bandit
problem is the same problem where we
just have K discreet actions it's not
some convex function or some structured
space are just k actions each one has
some loss you could think of the
functions that we talked about before
just being arbitrary bounded functions
not convex functions right so I have a
grid each at each point the function has
some lost value and we know how to solve
these problems with regret that scales
like square root TK with a finite number
of actions k so if I discretize and then
forget about convexity altogether just
treat each point as an action I you know
I can avail I can choose one of these
actions and see its loss now it's just a
care and Bennett problem I pay this
regret I pay this for the discretization
epsilon and K are related through this
so you know if epsilon square space then
k is one over epsilon squared i optimize
over epsilon and it comes out to be T to
the minus a quarter which gives me your
grits that's T to the three-quarter so
that's already better than the five over
six that we had before just by
discretization and another little
comment we can also do a non-uniform
discritization and get a little bit
better still so again as I said if a
function is going to be bounded in
convex it may be non Lipschitz but only
very near the boundary if we make our
grid a little bit more dense towards the
boundary and more sparse or the interior
of the set then we can do even a little
bit better ok so just forgetting the
structure of the of the of the functions
and just treating it as a discrete
problem this is how far we can get so
it's better than 5 over 6 but it's still
not square routine ok so how do we get
square root ed this is we have to work a
little bit better a little bit harder
so this is where the non-constructive
and constructive nuh scans comes into
play so we're going to use the minimax
principle and we're going to say that
the min the minimax regret the things
that we're after is equal due to the
minimax to the 10 man's MIDI machs
principle to this maximum regret so this
is called that'll be called the maxim
invasion regret so what's this here the
adversary chooses a distribution over
the entire sequence of loss functions ok
so the loss functions are going to be
drawn from some prior distribution that
he chooses adverse airily the player
knowing this distribution is going to
play his policy and now we look at the
regret which is just going to be the
mean regret as defined before over this
distribution of your losses ok so this
is going to be called the maximum bajan
regret and now this is a different
setting so before we talked about the
adversarial basis convex banded convex
optimization setting this is the base
and banded convex optimization setting
so again the adversary chooses some
prior distribution this is a
distribution over the entire six women
sequence of loss functions not over one
of the loss functions he reveals this
distribution to the player but then he
privately draws a concrete instantiation
of the losses and then the game is
played as before ok so it's important to
note that this sequence of loss
functions are not independent they're
not identically distributed and this is
important to know because if you know
this literature then a very popular kind
of cousin of the adversarial problem is
what's called the stochastic branded
problem there so stochastic in this
world is synonymous with independent so
when people talk about stochastic Bandit
they talk about loss functions that are
all drawn Ind so they're all this is the
stuff that actually makes money for my
crust ok so we've shown that the minimax
regret that we care about or what I mean
we just used straight straight forward
many machs principle to conclude that
the thing that we're interested in is
the same as this maximum Beijing regret
so now we can think about the Beijing
setting and our strategy is going to be
a
let's upper bound this Maxim invasion
regret let's think now only the beijing
city even though what we started caring
about with the adversarial setting and
and we'll use this to get a non non
constructive bound on the minimax regret
in the adversarial city okay so a little
bit of notation so now we're in the
basing setting so where the player we
get this prior we know the distribution
from which the losses are going to be
drawn okay at each point in time we have
this HD this is going to be the
information that we have at the end of
round t okay so formally it's just the
sigma field generated by all our actions
and all the losses that we've seen so
this is the history given this history
that we've seen where round t we can
actually compute the posterior
distribution right so we can apply Bayes
rule we can rule out all the functions
that are not consistent with what we've
seen and we can have some posterior
distribution so this is going to be an
interesting way that now the past and
the future are in fact related all right
so the things I've seen in the past do
tell me do narrow the possibilities for
the future so this is a much more
structured setting than what we had
before it's going to be much easier to
work with we're going to have this
little shorthand so we're going to take
conditional expectations e sub T is just
going to be expectations conditioned on
this history so this is we're going to
work with and then we're not going to
worry about computation because it's not
constructive to begin with okay so let's
try something this is going to fail but
let's just think of something I mean if
I have a posterior so at this point I
know the distribution the posterior
distribution for which lost functions
are going to be sampled I can take the
mean of I can take the average loss
function for today's round so I know an
average what the adversary is going to
do today maybe just play the minimum
with that function okay so that's a bad
strategy that's not going to work so
let's see why that doesn't work so this
is just to show that even when I know
these bus tears it's still a hard
problem so here's the example that shows
that that fails so imagine the following
World assume that the prior is such that
the adversary chooses between two
functions one is a parabola with a
minimum at 0.3 the other is a parabola
with a minimum at 0.7 so chooses one of
these two functions and just chooses
that function consistently for the
entire game so it draws once between the
two with equal probability and just
sticks with that one function forever so
if I just you know
do one round of exploration just to see
which of the two he's played that I'll
know what the function is going to be if
you know for all the rounds i'll just
play the minimum of that in my regret
will be zero playing the minimum of the
beam just means you'll play the minimum
of the average of the two which is just
the point in the middle where exactly
the two values of the functions are
equal so if I play if I minimize the
average of the two I can get zero
information I'm going to get this number
and i get no information about whether
we're in the red world or in the blue
world and therefore in the next round
again you know all i have is is the mean
being this and I and this goes on
forever I'll keep playing this point
I'll keep getting zero information about
whether we're in the red world or the
blue world if we're in the red world
this point is definitely lower than that
and we're going to suffer this constant
regret times teen that it's going to be
linear regret aren't you conditioning
under history so after we've seen one
second visiting all the 16 to this point
so i played this all i only get the
value of the function hey hey you want
to do some kind of gradient descent on
these things like if you could you this
value by little bit you would learn but
you if you're not still good that's
exactly the exploration that i must do
what i'm doing here is just pure
exploitation I'm not exploring at all so
I'm not thinking about if I you know if
I know that this is the world so I just
play this point one time it if I get
this function I know we're in the blue
world if I get this value I know we're
in the red world so if I sacrifice one
round for exploration I could exploit
from that but you know so doing this is
what we're going to be calling a pure
exploitation and that's always bad so
there's still this exploration
explanation trade-off even though we
have a model of what the guys going to
do to me today and tomorrow and so on
okay so this is just a show that I can a
zero probability event if you will get
the expression this one or again these
are zero problem it's sort of like a
zero probability and it seems like it
was so unlucky that I think exactly
crosses that point it just adds a little
bit of brandon Moore you win this piss
accepting right yeah but in general
there are other examples of conveyance
to like the treeview gets
information ya une you can't can just
know it makes the snow human we get
it's because usually it's like a
distribution of an infinite number of
functions so i can say something similar
that will work in a minute I mean that
the text is there is hope 212 is a
strategy where you play the minimum and
you had a little bit of randomness oh so
you don't know whether this is 11 ok so
we have to work a little bit harder so
let me define a little bit more
temptation so X star is going to be the
best point in hindsight this is the
minimizer of this random sequence of
functions so X star is the thing that
I'm trying to discover so remember we
discretized it's one of these discrete
points ok so it's it's the minimum
within our discrete set and I'm going to
define two very important quantities
that we're going to work with one is the
instantaneous regret so this is the
expected regret due to playing X on this
round conditioned on the past so r sub T
of X is at time T if I play x how much
regret we expect to pay right so just
the difference between value of the
function at the point that I play and
the value of the function at this best
point in hindsight ok so this is the the
thing that's added to regret at this
round at this point in time the total
regret the total maximum Basin regret is
just the expected some of these values
ok so in words r sub T of X is given
what I know again all expectations are
conditions on the pass given what I know
how much do I expect to pay for playing
the point X ok so that's the first thing
I want you to remember the second thing
is these contains information so we'll
see what it is in a minute but in word
that's going to be given what I know how
much information do I expect to get
about X star by playing x ok so for each
point in my domain I want to be able to
say how much do I expect to pay for it
and how much information do I expect to
get by playing it and it's all going to
be a question of balancing exploration
exploitation ok so let's see what this
definition is so the information at time
T by playing x is just going to be the
conditional variance of this random
variable so this is a random variable
that removes all the noise that's
independent of X star and just keeps the
randomness that's due to X star so
perhaps maybe all the functions are all
polluted with some independent noise on
each round just average that out just
keep the randomization just in these
functions which is due to the identity
of x star the best point in hindsight
okay so now at some point X at some
point X I can say look at this point as
X as my guest for what X star is is that
thing varies this value of this function
is going to change if this thing has a
big variance then knowing that value
will tell me a lot about the identity of
X star if you think about the previous
example where there's two parabolas met
right that was a point where X star
could have been here or here but the
variance was zero I played I got exactly
knew exactly the number that i would get
in case i'm doing the value what's your
changing its traditional knowing who x
star is out of this grid so x star is
one of the guys in the grid so the
variance you is over the choice of x
star and the expectation is everywhere
everything is in some sense this is a
random variable that averages out
everything except for the random
innocent x star and now the variance is
over the different photos of a star so
this is in words it's a lil bit
complicated but it totally it's very you
know it you'll see you'll see it it's
very very simple you're just saying if I
have high variance it means that knowing
that value is going to give me a lot of
information and now we have a lemma and
this is lemma adapted by from a paper by
Rousseau in Van Roy and this says that
the sum of all the information you can
collect throughout the game is upper
bounded there's some finite amount of
entropy about who X star is and I can't
experience more variance than that I
mean once I've you know played points
with sufficiently high variance I know
what X star is and there's no more
variance so the sum of the square root
of this information term is upper
bounded by some sum total amount this is
using some some information theoretic
arguments magical dimension and so from
no but look at the square root T that's
suspiciously hiding here so we're going
to use the fact that this is going to be
square root tea ok so the total amount
of information I can collect throughout
the game is bounded so this mediately
gives me a very simple recipe
for getting the type of bound that I
want easy yeah process so get wine
explain is it weds xt anything give it a
Polish any policy yet for any policy
that I play for any adversarial
distribution of F's the total amount of
information so how much entropy could
there be about the identity of X star
it's this much hey what is k k is the
size of the grid the number of points
okay that's actually yes so x star
depends only on a scripted right patent
on my policy x star is a random variable
which is the minimizer yeah it doesn't
depend on the post it's the minimizer
orly actually lets the issues yes really
good okay so here's the very simple
recipe that i get for proving regret
bounds okay so if i can find an
algorithm that guarantees an upper bound
on this quantity this is going to be
called the information ratio so this is
the instantaneous regret divided by
square root the instantaneous
information if this is bounded by a
constant then just my regret is going to
be upper bounded by that constant times
total information that I get which we
already said is upper bounded by square
root T so the algorithm is controlling X
of T and what we've basically done here
or what initially rusev Android that we
we extended to our case is to break it
down into a sufficient condition which
looks only one round at a time so if I
can prove that on each iteration the
amount of regret that I paid is
controlled by the amount of information
than I get then I'm golden i can get my
square root okay so i don't know if i'm
going to pay a big regret or a smaller
grid but if i pay a big regret I'm
guaranteed to also have collected a lot
of information if I pay a cent if I you
know only got a small amount of
information I'm guaranteed to have
suffered only a small regret okay so
somehow the two things are proportional
to each other then I can immediately get
my rhythm okay so is that clear does
everyone see that okay good so here's
strategy number two and this is
something that does work we saw a temp
number one to play the minimum of the
expected loss function that doesn't work
here's something that does work so this
is something called Thompson sampling
this is something that we
using Microsoft product and it's a very
simple and you know intuitive concept
and here's what it is so we have this
posterior we can compute the posterior
draw a concrete loss function from that
posterior okay so we know that the real
function was drawn from this posterior
but we draw independently our own
version of the last one for this
posterior and play the minimum of that
thing so pretend that that's the real
loss function and play the minimum of
that so again we draw some F prime T
from our posterior and we play the
minimum of that okay so that's Thompson
sampling this is what rusev an roi were
mainly interested in they cared about
this in the IID case but this also holds
in our case more generally and what they
said is that if these functions are
bounded and not necessarily convicts so
again in this krm setting where there's
no structure like convexity and we
choose this X of T according to this
very simple Thompson sampling rule then
this constant which we saw here is just
going to be square root K okay so now if
we go look at the previous slide for
that case for functions for grid has K
points and we don't even use convexity
we already have a bound which looks like
square root K square rutile okay which
is exactly the bound that we have with
that we know it's now an alternative
proof of the bound for the caribana
problem and this is what they were
interested in but in our case that's not
going to work precisely because the
comment that you made before that in our
case because we are doing this
discretization of this continuous
problem and we pay for that the
discretization has to be fine enough
specifically the number of points in our
grid is going to have to be some have
some dependence on T so if we use that
lemma that what I showed you before then
k is actually going to be equal to t if
we do a non uniform grid maybe it'll
shrink down to something like square
root tea but in any case it's going to
be something which is point you know
how's that point on with dependence on T
so our bound will be this thing or
square root this thing sorry yeah score
this thing time square routine will be
bigger than squirty so this theorem is
not powerful enough but he didn't use
convexity of course it's like to be
powerful enough right so we're gonna use
convexity to get a stronger theorem and
that's our strategy so in order to prove
our theta of square root evasion org
redbone we're going to define a slight
variant we're going to have to tweak
comes on sampling a little bit and we're
going to get this kind of result so
we're going to show that the
antennas regret is controlled by the
instantaneous information times some
poly log of K naught square root of K so
convexity is going to turn this from
square root k to poly low key and that's
exactly what we need and that's why
we're not going to hurt even if we have
exponential grids and so on there's also
going to be a small little term over
there that's we will yes so we're get
we're getting we're getting to the point
where you eventually I mentioned you
would get invited yes so I think I think
we know how to generalize this part to
higher dimensions the part that we don't
know how to generalize the higher
dimension I'll point at it in a minute
but still I mean so far everything is is
is either already works the high
dimension or we think we know how to do
it so here's a proof sketch this is
where it gets a little bit technical so
we're going to look at two functions so
we're at time T we're going to look at
the X the mean function so this is the
average of the posterior so we know what
the posterior is we're gonna just say
what's the mean function that the
average sir is gonna play and then we're
going to look at the main function
conditioned on knowing what the optimal
value in hindsight is so this is going
to be f bar and f bar of x so assuming
the optimal visit x what's the minimum
function so we're going to look at these
two guys and it turns out that are
instantaneous regret is almost equal to
the expected value of the difference
between the main function in the main
function where you're playing the point
that you know is going to be off to one
the end okay so this is going to be
equal tour in sinister grid the
instantaneous information is going to be
almost equal to the expected value of
the l2 norm between these two functions
so somehow this is an expectation over a
local point-wise difference between
these two functions and this is an
expectation over a global distance an l2
distance between them okay so the
variance is going to be proportional to
the other distance and and and the
regret is going to be professional to
the point west distance so this is just
some technical stuff that we can prove
but now let's compare this random
variable with risk this random variable
for each X so now the question is so
okay again I want to show that this is
upper bounded by that time some what's
that
vision / voice machine it's the Christie
almost a posterior so I'm hiding a
little bit of yuckiness but so we just
the posterior so there's just one
distribution the distribution of X star
and the distribution of the point that I
play is the same because it's Thompson
sampling so that's exactly what I'm so
simply does so again I want to show that
this is upper bounded by that times the
small thing and I'm going to compare now
each one of the terms inside so ideally
I mean it would be great if I could show
you that this term the regret the thing
that the expectation which is the regret
is upper bounded by just dl two times
our poly log k but that's not exactly
correct but here's the intuition that we
that we use so we haven't used convexity
anywhere before and this is where
convexity comes into play so again the
adversary has two goals he has this f
bar that he controls he has this f bar
assuming that I know what the optimum is
so we're looking at we're comparing
these two functions making the
difference between them at the optimum
big means the regret will be large right
because I regret is kind of proportional
to the expectation of this function so
he wants to find a point X star such
that if you knew that this is the best
point it'll actually be a very very low
value right but not knowing that just
taking the posterior it's it's somehow
hidden for you ok so the gap the
difference between these two numbers is
going to be proportional to R regret the
l2 distance is going to be proportional
to the variance of the information that
we get so the adversary has two goals he
wants to regret to be large once
information to be small so he's going to
take the reference function which is
just the expected loss at this round and
he's going to want to try to hide a
really really good value from us by
pulling the function down to make a
point that's much much better than I can
see but keep the distance between the
functions very very small ok so you can
see that it's very easy to do this if
there are no restrictions on the
functions right if I want to take two
functions and find make their point wise
this is big but they're l2 distance
small that I can do it very very easily
not when they're required to be convex
ok so that's where convexity kicks in so
this is our local to global lemma which
says that a local change in the funk
if you take a convex function and pull
it down at some point to a point that's
below its optimum so you hide something
really really good you're going to have
to change it globally in the sense that
the l2 distance between the original
function the new function is going to
change your life so any local change to
the function which is significant in our
setting is going to give us information
because it's going to make them the
energy between two functions very very
big this is where most of the work in
the paper is spent I'm not going to talk
about this anymore because I'm out of
time but but this is the intuition
really this is the property of convexity
that makes that square root tea into a
Polliwog equality between the Infinity
norm and now what we get is a we show
that the ratio between this guy and that
guy is always going to be bounded by
some term which has to do with the
energy of the function f with the energy
of the function f independent of the
right number and this term when we take
expect it could actually be big for some
cases so actually there could be points
where you very for example in this case
if you pull down very close to the
optimum you can actually get a very
unfavorable ratio you can move the
function by a little bit and change the
energy between the function is also be a
little bit let me take expectation then
the term that we lower bound this by is
going to look like a like a harmonic
some and it's going to turn into this
log so there's some math magic that
happens there on average if you change
the function locally you're going to
have to change a goal in just one
sentence which is a DX into distance is
not your perspective egg but it's
respect to the 40 discreet I'm actually
clear expectation well there's there's
more there's more cheese baby it's this
is a distribution only supported on the
interval between this and that I mean
there's some details this is this is a
kind of a hairy messy little so it's
beautiful math and also kind of
disgusting at the same time
okay so this is the idea this is the
property of convexity that makes us this
is the Kosovo you on Monday this is this
is the real showstopper for
high-dimensional proof to show that the
local change induces a must induce a
global change our proof is very very
manual it's very much picture I mean you
say this line has to be lower bounded by
different line is you know if it's 20 19
they say two dimensions yes in a minute
I'll in the next level I'll tell you so
anyway that's the end yes that's all I
have time for and let me conclude so
what we have is a non constructive upper
bound on the minimax regret of the
adversarial Bennett convex optimization
problem in one dimension we use the
minimax principle to reduce the
adversarial setting to the vision
setting this has been done before but as
far as we know not for bandit problems
so when you have what's called full
information problems people have used
this trick but not for Bennett learning
and then we have this local change
induces global change property of convex
functions this is somehow independent of
our work this is you know some some so
something that I don't know I would have
expected to find in books on on convex
function that we exploit and the
combination of all these things gives us
our non-constructive uh program this is
the answer to your question so this is
breaking news from just a few weeks ago
so say Ben Ronan Eldon were able to
generalize this to arbitrary dimension
I'll be it with an exponential
dependence on a dimension so before I
didn't explicitly tell you how all these
regrets that we had before depend on
dimension but they were all a small
polynomial so something like n squared
and cubed here if you're willing to pay
exponentially in the dimension then the
new result which is builds on the same
principles but uses a different type of
algorithm different for the beige in
case it's still able to get square root
II regret but with an exponential
dependence and that's a topic for
perhaps the future talk bye bye centers
with outages
this is without if it is not important i
mean we get away with from delicious
just using that discretization trick and
now we're in a finite problem it's no
longer if an you know it is also no I'm
stuck yes now imagine so nobody knows
it's even it's even worth less and less
constructive instead of Tempe we use a
probabilistic argument of every semester
to show that there exists two points
that will give you enough information I
mean no difficult so our algorithm if
you care about the Beijing setting as a
first-order thing then it's it's
constructive perhaps very hard i mean so
for some families of functions you could
maybe actually compute this strategy
right sit atop some sampling with some
small modifications so i told you what
it is you can solve it we just don't
know how to map that back into an answer
to the adversarial case this is thanks
you go to adopt edition is crazy yeah
these do any kind of dimension reduction
is he suppose I mentioned reduction so
what is the atom I'm trying like
joseline so like do is either way to
somehow argue that somehow you could
reduce any space in price I don't think
so why don't you take victory I don't
know I don't we I mean don't anyway
water consumption you say there is it so
far
if the amino for convex optimization
right that's really great with a very
tricky so if anyone knows something like
this in high dimension I think we're
done I mean we have almost all the other
parts but just this idea of how much of
the function change globally will you
change it locally we use for any norm so
you had so the norm is arbitrary it's
governed as set set by the posterior but
if you can show that for any norm this
thing holds a local change is going to
make a global change you cannot hide a
little good point here without making
the functions very very far away we'll
be done with this big problem still we
have the problem of finding an actual
algorithms that achieves this but we
will have proven it one exists as you
don't prove something like that we
essentially do something like that that
we are you going to do it reef respect
to go back instead of you respect to the
posterior and then we we show that there
is a small discrepancy between low back
in the passer distribution so it's it's
in two stages so if you had done instrum
a convex function together multivariate
polynomial bounded heightened degree
it's known princess something called
Markov inequality which says more or
less things like this that local change
if there's a drastic variation in a
small neighborhood then there's a
drastic variation overall what is that
to be I don't think it's called Markov
inequality it's not t marco v now it's
or something but for polynomials are
depending on a heightened degree but
multivariate for moment but they have to
believe she's as much where you are you
have to be leave sheets yeah well they
have to have bounded height and bounded
coefficients lies in degree you know I
suspect something like this isn't true I
mean you get the iie case for this
problem the way nesterov
really constructing very complicated
region that's for what types of aspects
of the kind of body to Baba it's not
like you look at a few points in love of
some common you really construct very
complicated regions to chop up what's
been your think this reflects what that
I suspect they the analog you want
multiple dimensions is going to be a
good bit more subtle yes if I be it
might be real that exponential you have
your elevator they got one question if
you think you are good in it it is
actually exponential an end the current
that was a me yes it is exponential
right but I believe that the one where
you place a minimum and you just
randomize a little bit around the
minimum yeah that should be give you
putting on melanin that sedition if it
is it like you even know this is true in
the Easter cast exciting this would be
wonderful no we know you're saying at
door bomb she's even the other dance mr.
test except people okay I think we are
out of time so we'll have another break
where further questions can be discussed
in private but we're going to continue
the next up in four minutes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>