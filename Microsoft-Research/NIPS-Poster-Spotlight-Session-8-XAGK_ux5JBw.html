<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 8 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 8 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 8</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XAGK_ux5JBw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay next up we have some more poster
spotlights and as always I hope you find
these very interesting and come ask them
lots of questions tonight at their
poster so I'm going to about next system
for real world development evaluation
and application of active learning my
name is Kevin Jamison this is joint work
with Weller Jane Christopher Fernandez
Nick clattered and Robin Omaha so active
learners about data collection and
specifically and talk about data
collection from human laborers so the
idea here is that we're going to find
the same classifier regression or
embedding or ranking but we're going to
ask questions and adapt awake let this
data adaptive way so we can get away
with asking fewer questions in total
meaning if your time and your money last
morning but in practice a lot of
challenges diplomatic learning in
practice either for empirical
evaluations or for just in practice to
you you know exploit these sort of
advantages you need all this
infrastructure univer tissy pants but
you know mechanical turk makes it kind
of easy but you need you know a GUI for
participants to be answering questions
you need to experiment for database but
most importantly use computational back
end because you're choosing these
adaptive queries so when you net when
you want to ask the next query us do
some computation to choose which query
or just like which created a snacks and
that's what next take care of tariff so
next is computational real time system
that runs in the cloud and helps you so
that all you have to worry about is
codeine for algorithm putting up on the
server and then pointing towards
Mechanical Turk or whatever you want to
do and start answering questions so
literally a system that makes active
learning reproducible so who uses next
Nexus use like used by NL researchers
like myself so for instance the Air
Force Research Lab use the next to
develop new active horny aliens for
image classification for binary
classification and they run you know
simultaneous studies to figure out which
algorithms will actually work
experimental excuse next so you double
university wisconsin psychologists use
next to find out what algorithms work
best for their application but you're on
multiple same time they're not they're
not coding up algorithms or just using
algorithms that are already implemented
for their application
and finally nexus for practitioners The
New Yorker has this cartoon caption
contest where they have a cartoon and
they invite thousands of captions to be
put in by their readers and the idea
here is they want to use active learning
they are active learning techniques to
find which is the caption that should
win this contest at the end of the
contest and so they've been using
naturally been working them for the past
several weeks and would continue to work
with them and on every week we collect
hundreds of thousand responses from
thousands of people and really show is
this sort of like real world capability
of next for your applications thank you
hello my name is Peter fluff and this is
joint work with my postdoc middle school
this work is relevant if you use
precision Rico and F score to evaluate
classifiers in situations where two
negatives don't add value if you've ever
done some kind of experiment like that
chances are that you will have made one
or two mistakes which will have bias
your results one common mistake is to
use the arithmetic mean to average
multiple F scores for instance coming
from cross-validation this makes you
overconfident about your model because
you should use the harmonic meanie
instead which is always lower than the
arithmetic mean there are several ways
to mitigate this but in this work we
mitigate this by transforming all the
quantities to linear scales relative to
a baseline model which which is the
always positive classifier so the
quantities are called a precision gain
recall gain and Afghan and by doing this
you inherit all the advantages of roc
analysis the second mistake that you
might have made occurs when you use area
on the position rico curve as a model
selection criterion so for instance you
might look at the middle panel here and
conclude that the dashed line has higher
area than the solid line and therefore
you would choose the dashed model
actually if you look at the precision
recall gain curve on the right it is the
solid line which has higher area under
the precision recall gain
earth and we proved formally that that
means that you your mother has higher
expected F gain score in a very precise
sense finally you may want to switch
from f1 score to FB to score by using a
weight harmonic mean of your precision
and recall current methods mean that if
you switch to a different value of beta
you have to retune your decision
threshold using the precision recall
gain curves we can derive a calibration
procedure which gives you in one shot
all optimal thresholds for any value of
beta for the FB to score and if you want
to know more details come to my poster
which is posted number 25 which is
towards the back of the poster room
thank you hi everyone my name is John
Doe fan and I'm going to talk to you
about this work on equilibrated adaptive
learning rates for non convex
optimization and this is joint work with
armed devries and yoshua bengio so the
key problem that we're tackling with
this work is the question of whether we
can make training deep neural networks
easier in a principled way and the
approach that we're taking is using
preconditioning so and the two figures
on the right illustrate what the idea is
so what makes what can make optimization
hard is when the contour lines around a
minima or a critical point or uneven so
that you make progress slowly in certain
directions and preconditioning is the
geometrical transformation where we make
sure that the contour lines are even are
as even as possible so you make fast
progress and as many directions as
possible so in this paper represent new
theoretical and empirical evidence that
the so-called equilibration
preconditioner is comparatively better
suited to non convex problems when
compared to what has been proposed
before and we show also that
equilibration is related to our ms prop
and that can help explain some of the
success of our mess
for turning dinner deep networks so
equilibration methods are a family that
was proposed by vendors Lewis in 1969
and the simplest kind is Roku libration
where we scale the rows of a matrix by
the norms so the rows are equilibrated
because now they have the same norm now
we propose a new theoretical
justification that is more general while
we show that this simple method is able
to reduce an upper bound on the
condition number and we also give a
concrete theoretical justification for
the use of equilibration in the non
convex setting so finally in terms of
implementation what's really nice as you
can see in the bottom right the
algorithm itself is very simple and
basically it's it's an upgrade on
stochastic gradient descent we also find
experimentally that this method produces
very good results and is comparable and
better than RMS prop in some of the
settings that we've considered if you
want to learn more we'd be happy to talk
to you at poster 27 thank
hi the title of our paper is a
structured transform small footprint
deep learning this is a joint work with
the terrace I Nathanson GF Kumar my name
is Vikas and money we are at Google
research New York so in this paper we
consider the problem of building compact
deep learning pipelines that are
suitable for deployment on mobile
devices available computers mobile
robotics and there are a number of
approaches available to build compact
architectures ranging from quantization
of weights to imposing sparsity using l0
l1 regular risers to low rank assumption
on parameter matrices or sophisticated
forms of weight sharing of which
convolution is a prime example we
propose a unified framework to learn
structured parameter matrices and their
generalizations so structured matrices
are matrices that can be characterized
by much fewer than they have much fewer
degrees of freedom so an n-by-n
structured matrix that can be described
in much fewer than n square parameters
is a structured matrix and examples
include opal it's van der mond koshi
matrices that support fast linear
algebra fast matrix vector products fast
linear system solution to linear systems
so we tap into a previously ignored
literature on numerical analysis on
displacement operators that turn these
structured matrices into low rank
matrices and give a very nice
characterization of a broad family of
structure in matrices which includes
which actually includes generating more
general structured matrices by taking
products of let's say two plates
matrices or in verses in linear
combinations and so on so forth so our
approach leads to fast training
procedures because a fast forward
propagation backward propagation and it
gives us a nice way to essentially span
an entire continuum of parameter sharing
schemes from highly structured one
structure we show in mobile speech and
image classification problems dramatic
acceleration in training time as well as
a very strong compression a relative to
state-of-the-art models so if you would
like to learn more about this please
come and see us at poster number 34
thank you hi I'm Oreo Phineas and I'm
gonna present the work with it with
maida Fortunato and navdeep jelly name
pointer networks so last year we present
that Nipsey sequence to sequence
learning paradigm in which we show that
we could do machine translation by
mapping from a sequence of French words
to a sequence of English so while we
tried to answer in this work is whether
we are kind of ready to try to tackle
the following problem can we take the
instance of a complicated problem such
finding the Traveling Salesman problem
which is the path that goes through all
the points minimizing the length and
pose it in a supervised setting in the
same with the same framework that
sequence to sequin proposals so in this
example for example I I show the points
which would be the input set or sequence
that the algorithm consumes and the
desire output which is shown by
supervised learning to the attitude to
the learning algorithm would be just the
set basically the one that minimizes the
length we study a bunch of problems such
as tsp as I mentioned or simpler ones
like finding convex hulls and we tried a
bunch of approaches starting from
sequence to sequence and going through
attention models like the ones proposed
by Montreal the cold Erin and search but
in the end it turns out that the best
approach is to actually think of this
problem as the instant the output
instance should point to the input
points rather than have a distribution
over a fixed vocabulary and that was
very important to both achieve very good
results on a limited set of tasks
algorithmic tasks and also it was
important to achieve generally
beyond the number of and like the size
of the problem that we trained on so if
you want to learn more about the
architecture and what we tried and
problem such underfitting overfitting
and so on we are at poster 22 and here
you can see some examples of the
mistakes that the network neural network
does one for example tsp thank you very
much okay the second afternoon session
was over so please join me to thank all
the speakers in the session
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>