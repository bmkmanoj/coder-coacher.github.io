<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Variational Gaussian Process State-Space Models. | Coder Coacher - Coaching Coders</title><meta content="Variational Gaussian Process State-Space Models. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Variational Gaussian Process State-Space Models.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/f2fnE-xjEpE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello I'm UTM postdoc with sweeping
ceremony and this is joint work with
Roger purikura and professor Karl
Rasmussen
so I actually give the man credit to
Roger he can order all the experiments I
write in the paper so so this is a new
inference a great variation or inference
algorithm for the Gaussian process
state-space model the so I will first
talk about like why we need this model
so the nonlinear dynamic systems were
like with the time series data generated
from this model is all like happens
almost where and most people are like a
lot of you are eating like studying the
properties of time series because like
you have seen like robotics or control
theories and also like in finance where
people want to look at the underlying
dynamics of like the volatilities also
in neuroscience and other a lot of other
areas so most in most cases like people
issue in learning the actual nonlinear
dynamics that drives the latent space
for others latest state from like one
has stepped with another time staff the
the function so one example is the
robotic unicycle people from Professor
Karl Rasmussen group is trying to apply
to apply measurement techniques to
teacher a unicycle to learn how to keep
balance by itself so there there are two
spinning two spinning sticks like one
horizontal and one vertical to try to
like to learn how to keep it and sliding
some a small region there so the latest
pace of this dynamic systems will be
like those locations the the angle of
the the disks and also the select
angular speaks speed so the main goal of
the of the problem is to study like the
nonlinear dynamics and also most cases
the states are unobserved
or like observed with some noise so we
want working further like true latent
process so the state space model is one
probability model to describe on dynamic
systems the in the most general cases
like we have the X index valve time T is
the like true like latent unobserved
states and we may have some external
input which is usually given and we are
actually in and we what we observe are
the Y's so we are usually in two
function where this is the transition
function from one time step to another
step and G is kinda observation function
you can if you are familiar with the
hidden Markov models it's kind of a
generalization to the continuously space
so you know in this project I would just
talk about the simplified version where
we don't have external control but it's
quite straightforward to cheer like that
there's two the case with you so a
common way to describe the model is that
we can specify some parametric model for
this district for these two
distributions so f and what G our
probability distributions for the X
conditions are given some parameters and
the goal of learning this dynamic system
will be inferring the state process or
the latent state and also the unlike
unknown functions F and G that due to
the nonlinear light transition and
observation however like with the
parametric models sometimes we just
don't know what the function value like
the type of function should be or even
we know like in some cases for example
the like the system of the autumn like
the plane piloting like we may specify
every like dynamics is like equations
exactly but the problem is too
complicated and we were involved a lot
of parameters and by just observing like
very few like but given very few
observations it might be very easy to
get outfitted to the estimate parameters
we asked
I did alternately we can consider a
Bayesian approach which K in this case
like we don't have a specific parametric
distribution for the a parametric form
for the function have instead we assume
F itself is some random function that is
drawn with some from some prior
distribution the cutting process is one
example very like commonly used prior
for those random function else so in
this graph we can have the every F F is
some like unknown function that is drawn
from GP and the GP is parameterize by a
mean function and the kernel function
where a covariance function well here I
just for simplicity eyes that I assume
the mean function is always zero line so
the mean is always zero for any input
and the K kernel function specify like
what's the covariance between the
function output at two different
locations the cursor like these two
points are in to each other in the input
space like more likely the output
function will have like similar values
so I can multiplies out the random
function itself then what we end up with
is that all the function output at the
given location from x0 to like xn then
they the Joint Distribution of all that
function output becomes a very big joint
Gaussian distribution where the mean is
zero and the the covariance matrix is
provided by the kernel function the
benefit of using the Gaussian process
instead of parametric model is that when
we observe more and more data on the
function we infer like the nonlinear
function we infer here can be arbitrary
complex like learn more and more details
with more data and and also like when
the data is very few then we can learn
some simple smooth function and it is
very effective against like overfitting
on the very small size of data and when
the data grows we can in automatically
in like to add the complexity there okay
so if we want to use the gaussian
process as a prior for the transition
then we end up with a like a probability
description of the entire systems so
here I still assume G the G fund
observation function is some instant
parametric form but I like we can also
extend that with the Gaussian prior so
now I just talked about like how to you
assume a Gaussian prior for the for the
function of the transition function in
that case like every function f is drawn
from some Gaussian process oh yeah for
some thousand process I use F I here
because there can the latest space can
be some like D dimensional space so we
have need to have like the functions
corresponding to each dimension then
once once we have this function the
output at a particular location is just
doing the like look up from their
function then the X for next time step
is some noisy of the ratio of F so this
it becomes a new like rough model
representation of the state space model
we can so virtual eyes out the random
function f itself well and the we work
at a joint so for the connect to the
group graph for all the FS now we notice
that this is not Markov process anymore
I mean like when we observed F like the
distribution of the next F or next acts
does not depend on the current state
anymore
it also depends on all the RF in the
past so so we can also just write out
the Joint Distribution of the entire
system including both the X and the
function value F so the first part is
the prior over the first time step this
part corresponds to the problems model
for the observation this is the G
function and that part is similar to the
cut the original like calcium constant
process regression problem like the
original definition Gaussian process
where you can treat every like this
shape as just one line here so this is
input to F the F output and don't know
it's observation then the Alpha here
becomes the input at the next time step
so with the with the Gaussian process
state-space model the inference or the
learning process become is very usual
very difficult and the is intractable to
compute the like the joint the marginal
likelihood of the observation
analytically so there are quite a rich
literature about how to run this this
system and to the inference on top of
there so there are a few approaches like
assume probability model with actual
parameters then we can turn the Gaussian
process into some actually progress
model but in that case we need to
include a lot more parameters and
there's a summary of fitting again the
similar which the second approach is
also similar it's optimized both the
latest process and parameters and when
the latent process is large there's also
some problem with all fitting and so the
last approach is current like the state
of art for learning like doing learn and
inference for Gaussian space Gazza
process state space model it's a pure
beaut it's a pure facial approach where
we basically do sampling for all the
unknown random variables including the
latest space any states and also some
hyper and also the the hyper parameters
in the in the dynamic systems so it's
use of particular particle gifts with
the ancestor sampling problem algorithm
because in this case when we multiplies
out F all the acts are correlated with
each other so the right the usual
sequential Monte Carlo method for
drawing samples in the Markov process
doesn't apply here and if we take like
the regular computational complexity is
TQ where T is the number of time steps
but we can take some space approximation
and bring it down to T times M squared
where m is number of like some
approximate input we should come in the
sparse approximation forecasting process
and as if the number of samples we have
to draw the so the precision of this
method is the accuracy of this method is
quite good but the problem is is courage
like to slow because we need to draw a
lot of samples together some accurate
predict
and do prediction we need to use every
step sample to do prediction separately
and then computer March the average so
our project the motivation is that we
want to use some more efficient way to
do to do inference in this model and get
a faster like speed in birth training
and the test phase the method we use is
called virtual inference so the method
the particular method is first
introduced by teachers paper in 2009
that matter applies to the regular
Gaussian process the regression problem
where we were given post the x and y and
we want to infer the unknown function f
the method start with introducing some
like called pseudo input here or pure
usual call inducing points where that
that is in the same space of the input
space and you in the in the service
space of F I should pay for the joint
diffusion or F and you follow the same a
prior distribution from Gaussian process
so we can get the original the exact
Marshall likelihood of why conditional
as the conditional distribution of Y
given X by marginalizing out old F and
you so that introducing you doesn't
change the distribution of Y team X then
the change the approximation comes in
this press weather where we can apply
the Jensen's inequality and get some
lower bound of the likelihood of the
conditional distribution where the in
this case we introduce the new
distribution kill for F and you where
this Q can be arbitrary distribution but
no matter what you choose like it will
always provide a lower bound to this to
the conditional distribution if Q equals
the posterior distribution over F and
you condition on all the other variables
we get a equality here otherwise it's
always in equality it's always a lower
bound because the conditional
distribution is in terms of compute in
order to make it a tractable we take a
particular form of the variation
distribution where we can have an
arbitrary distribution for kill view but
for the F part
we assume it's still the same as a prior
distribution when we don't know why this
is a conditioned distribution from the
prior given the particular this
factorization of the queue
the version distribution we can compute
the lower bound and track fully and what
the effect of this approximation is that
now we don't have a fully packed row I
poor connected all the F and views but
all the use are conditional expected but
all elf's are kind of like conditional
independence can be given or the use
with this virtually first method we
reduce the time complexity from like n
cube to and and M squared where Emily
the lamb will use and we can get optimal
like analytical form for the optimal
distribution kill you that's very nice
and also we can show like with so
although we introduced a lot more
parameters here but like it always
improved the low power here and in the
in some case where if does that the
status that include all the data phones
here we will get exactly the same
conditional distribution in that case we
don't need to worry about the Oh fitting
problem when we learned how parameters
of the Gaussian process okay so so to
apply the variation inference for our
model it's kind of similar we also
introduced some you for the input space
and function space then we provide the
same jason inequality the difference is
only that we need to include x in the
distribution because exiting our case
then we introduced a similar
factorization of the variation
distribution so now like now the problem
because we want to optimize the version
distribution for both the X which is the
latent state process and also the U
which is the inducing points we
introduced for our version inference
method
fortunately we can still compute on
analytical analytical form for the
optimal distribution of Q you
which is sum over all the time steps and
there's some function we can compute
easily but for the optimal distribution
of QX it is still intractable
unfortunately but what we observe is
that in this case the distribution
forward here the the correlation between
different acts are Markovian but arma
coping I mean in this distribution the
like when we conditional X that the
future and the past are independent with
each other so this is very different
from the full credit counseling process
states force model in the original form
that gives us a lot more flexibility to
choose what the sequential Monte Carlo
method we can use so our algorithm
becomes like this form where we won't
optimize actually the comb xu and also
the hypo pronto the dynamic systems the
doesn't process cause in process so it
will be iterative algorithm to
optimizing all of these terms like
iteratively and then this actually
provides a very flexible way like we
don't have to do the optimization for
every term for kill the ads we can use
sampling method to draw a few samples
and then for qu and theta we actually do
optimization the reason we use we want
to do sampling because it provides a
more accurate as made to the posterior
distribution of the latent latent
trajectory and also when we do predict
you actually we don't need QX we are in
new near qu and theta so compared to the
the original like the hybrid inference
method I've just managed the particle
MCMC method I mentioned like we get a
most synchro structure so that we have
apply more like state order something
algorithms in this case and also during
the special during the prediction time
for the particle MCFC method it has to
do prediction for every sample drawn
from the training system and the
computer average but in our case we only
need to compute a one like function
analytically so that time the time
complexity is very different in this
part like it request it is scaled as a
number of samples but you know case
there's no such thing okay we can also
talk about like how to do like the kite
variation in difference this basically
is down the computational complexity
from or linear to the time step to just
a constant which is a short segment of
the entire sequence I think I'd better
just skip this part due to time
constraint okay let's just see some
experiment so this is a wall synthetic
data where the Green Line is the true
nonlinear dynamic systems it's just a
piecewise linear function and these dot
dot points are I think there are the
inducing points we used so the blue line
and the red curves are learned that
nonlinear function because we infer the
positive distribution of the nonlinear
function so it's not just a single
function but a distribution functions
the the blue lines are tolerant mean
function mean of the distribution and
the red lines provide uncertainty about
our land function so we generate here
some data and apply different method to
compare the prediction accuracy you can
see like oh this method used the same
cousin process mod state-space model and
all the others use some parametric
models the accuracy from all these like
these model use just use different
inference method well this is our first
method this is the matter with this the
test version universe and this is the
part where I'm CMC method oh don't give
up very similar like prediction error in
the error in the prediction rate such
but when you look at the test time our
new version inference method is much
faster than the sampling method because
like we don't need to deal with a lot of
samples during the test another example
is that we applied our method to the
story the latent process of the neural
spike training record like that this is
the one time mission data where each
value is the numbers neural spikes
in a unit time and we want to learn like
the nonlinear dynamics of this data when
we do it that we have we assume a two
dimensional latent space and apply a
possum observation model for this data
so this the first figure is the original
data
the second figure either inferred like
latent the trajectory of two dimensional
latent space a latent States and we we
want to do it some sanity check a sense
is check to see like if our will some
sensible dynamics and this is the result
if we use the learn dynamics to simulate
some new sequence you can see like it's
not exactly the same as the training
data but it captures the main like
frequency in the data yeah and this is
some like another example to show the
same problem this the same dynamics
just a quick thumb rule like we
introduced like we use a a new librarian
inference method that combines per
sampling and version inference to do a
more efficient inference for state space
a constant process state space model and
it's very easy to generalize to other
like learning settings like online
learning and the cousin success for all
the functions like including a purse F
and G okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>