<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards High Performance Networked Systems | Coder Coacher - Coaching Coders</title><meta content="Towards High Performance Networked Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards High Performance Networked Systems</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PMcM8W_4_Ag" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so thank you all for coming it's an
absolute pleasure to introduce a sauna
jacuzzi who's an assistant professor at
lumps before that he did his PhD from
gina's to Pittsburgh so a San and I have
been kind of friends for more than a
decade now as we did our undergraduate
together at lums and then we were
Bartman waves in Pittsburgh as well and
more recently we have also been
collaborating which is which is going
great so I saan will talk about some of
the stuff that that's going on in Islam
okay over to your son thanks a lot fat
so i'll be talking about a couple of
recent projects that my research group
has taken has has worked on and before
that i'll give you a quick background
about my research group so i had the
networks and systems group at lums and
we work on three broad areas one is
cloud computing and data centers the
other is wireless networks and the third
one is smart grids that's that's quite a
lot of areas to actually look into so
I'll describe briefly sort of the key
themes within these areas that we
looking into so in the context of cloud
can do in data centers we've been
looking into how do we design protocols
and frameworks for meeting low latency
requirements of large skin cloud
applications and secondly we have looked
into recently on data center
architectures for maximizing efficiency
in resilience so our recent work looks
at how do we realize virtual network
instructions which are resilient thirdly
we have worked on efficient routing
mechanisms topology the design and
management frameworks for data centers
in the context of wireless networks our
work has focused on three broad themes
one is gigabits per second Wi-Fi so
there are lots of macular challenges
involved when you go to hundreds of
megabytes per second gigabits per second
how do you address them secondly we have
also looked into specific design
considerations for developing regions so
our recent work has focused on enabling
real-time applications using long
distance Wi-Fi and so this brings in a
very interesting cost
aspect of the design and finally we have
worked on supporting real-time
applications and both transfers and
Wi-Fi networks our third focus has been
on smart grids and this is and in
particular we have worked in this space
because a Pakistan like many developing
regions has been hit by a lot of
loadshedding it's not uncommon to have
eight hours of low trading in Pakistan
so we have worked on three spaces in
this in this particular area one is
peacoat reduction so the infrastructure
provisioning in smart grids basically is
is based on peak load so if you reduce
peak load you have opportunity for
reduction load shedding so we have
looked into appliance scheduling
algorithms in this space for demand
shifting secondly we have looked into
purse ammonius modeled for demand
prediction so cal so gathering doing a
lot of survey and and collecting a lot
of inputs from from people is hard and
so so prior work and demand prediction
assumes a lot of input from the
individual homes so how many people are
there what kinds of appliances are there
so the focus here is can be used very
small number of input to be able to
predict demand reasonably accurately and
that has implications for lots of
applications and finally you know we
have looked into models for enabling
reliable power through renewables in
this talk I'll be focusing on two recent
projects one on data center networking
and the other on wireless networks hence
the sort of Jamal general title of my
talk today that was the reason for this
so let's dive into the recent work on
cloud computing and data centers so so
we have recently worked on a low latency
data center networking and the context
so you can reduce latency on many fronts
so the context of our work has been
essentially transport frameworks and
buffer management frameworks so that's
the context and the first two works have
given have been geared towards private
data centers of production data centers
owned by companies like Google like you
know web search applications and such
like and then the the context of the
final work has been both and private
data centers in cloud data centers
so if you look at transport that hard to
deploy in public clouds where the end
whole stack is not only confronted the
control to the operator completely so in
those context by our management's
framework come into play so I'll focus
on pace which will be which will appear
in sitcom this year this is a joint work
with several of my students and
colleagues so I'll immunity so subset of
this work was done while adding whinnies
was an Internet max of research working
with fahad so I'll start with that so if
you look at you user facing online
services in the last couple of decades
we've seen the emergence of these online
services like web social networking and
advertising systems and a key goal of in
such services has been to minimize user
response times and response time matters
because it impacts offered to revenue
and user experience so for instance
amazon found that every 100 millisecond
latency cost them 1% in business revenue
similarly yahoo found that an extra four
hundred millisecond reduce traffic by
nine percent and so this begs the
question why is the network important
here and if you look at typical data
center applications they have many
distributed components and it's not
uncommon to have those components
interact via the internal data center
network so for instance a big Bing
search query you know oftentimes touches
more than hundred machines it's not
uncommon for that to do so similarly
measurement results have shown that
network can impact oppression
performance so this study which is
conducted with the big Bing data center
show that ten percent of research
responses observe 12 14 milliseconds of
nap and queuing delays that's orders of
magnitude larger than the round-trip
propagation delays you have in data
centers which is in the order of 100
microseconds or so and if you look at
data center network resource allocation
categories so there are essentially two
broad categories one is fair sharing and
so the idea is that on a single link you
divide that link equally among flows
given enough demand from the flows
and it's been known that fair sharing
leads to suboptimal flow completion
times and therefore if the goal is to
minimize completion times the
traditional fairness met metrics are
less relevant so transport protocols
like TCP data center tcp they focus on
fear sharing the second category is
quality of service aware or scheduling
approach as I call it so the idea is to
prioritize some flows which are
typically short over other flows in
order to minimize flow completion times
so on a single service system for
instance SRP TVs shortage remaining
processing time versus known to minimize
the mean flow kamina completion time
similarly the the goal might be to
maximize the number of deadlines met and
this is what d cube which was work done
at Microsoft Research sitcom doesn't 11
i think that tries that aim set and
along with these challenges data centers
also have their inherent challenges as
you might be aware of them one is the
way the applications are architected so
partition aggregate structure is fairly
common which leads to synchronize loss
responses leading to issues like tcp in
cast and you have traffic workloads
which with diverse requirements so you
have short flows which are typically
latency sensitive so in the search
contacts this might be the the query
traffic and the small control control
traffic and then you have long flows
which are typically needed for data
updates maintaining consistency
replication and these are typically
through throughput sensitive and it's
well known that tcp based deployments
are unable to meet these requirements of
these applications tcp has this buffer
feeling nature which leads to large keys
and impacts the latency of sharp flows
so as a result in the last few years we
have seen a lot of work on data center
transports and and these are excellent
pieces of work so one one category is of
works like d cube p fabric and PDQ and
these works achieve near optimal
performance but are not deployment
friendly because they require changes in
the data plane so they required changes
in the switches the other category
protocols include data center TC
p that line of where tcp low latency it
is dct and these are deployment friendly
but are suboptimal in terms of
performance so in this block what we do
is that we take a step back and ask this
question that Walden you know right away
proposing a new protocol we ask the
question how can we what design insights
can be leveraged from existing proposals
to come up with a transport which is
both deployment friendly that is it does
not require changes in the data plane as
well as the chief in the adoptable
performance so that's the target and
towards this end what we did was that we
analyze existing transports and we were
and we distilled what were the
underlying transferred strategies that
these protocols used and they turned out
to be three transport strategies so the
first transfer strategy we identified
was that of self existing endpoints
essentially these stratus natural
strategy is essentially where each flow
independently makes adjusts its rate
based on network conditions so data
center tcp dsquared tcp or these
protocols which essentially leverage the
self-adjusting endpoint strategy the
second strategy is that of arbitration
so there is a global arbitrator which
decides the share relative share of the
flows and so protocols like DQ PDQ broad
which is another work to appearance calm
this year follows this transport
strategy similarly the third transport
strategy is that of in network
prioritization so particles like P
fabric you have in the switches you have
priority queues so each switch
essentially independently schedules or
drops packets within the switches so
that's be fabric for instance does that
unfortunately if you look at transports
they only use one of these transmit
strategies all the transfers strategies
only use one of them and these transfers
strategies have their strengths they
also have the limitations so for
instance it's well known that if the
protocols which employ self-adjusting
endpoint strategy they are easier to
deploy but they do not support strict
priority scheduling as a result you have
flows which might be low prior
with the continued descent so
essentially they do not support
preemption of flows which degrades
performance similarly if you have
arbitration like approaches like D cube
and PDQ the support strict priority
scheduling because the sign explicit
rates two floors but the challenge is
high flow searching overhead so it takes
about one to two round-trip times for
flows to change their priorities from
low priority to high priority and if you
look at data center floors it's not
uncommon to have a large fraction of
data center flows which span only one to
two round-trip times and so this can be
a sizable portion fraction of the
overall response time of short flows and
we showed in our paper why at medium to
high loads how this becomes this floor
switching overhead translates into
increases in response times and if you
look at an in-network prioritization it
helps in reducing the flow switching
overhead because you have multiple
multiple keys and so when the top Q
isn't isn't full you can serve the later
queues and it's but but the challenge
which is there two challenges within a
proprietor ization one is that you have
limited number of priority queues and
switches which leads to degraded
performance and also the fact that the
decisions are switched local so you
might be signed a particular rate but
your packets might get dropped later
later on and so the decision a switch
switch local as a result your
performance is suboptimal in certain
cases and we make the case that all
these strategies must be used in unison
or in synergistically because they
complement each other they help
addressing the challenges in individual
strategies so I'll give you two examples
of how that might the challenges might
be addressed in the in the ones that I
outline so the first one challenge with
net within network privatization is
limited number of keys and why is that
problematic so suppose you have to queue
is a high priority given a low priority
queue in the system and you have four
flows so now you have flows with these
priorities one through four so you have
more flow than the number of cubes or
priorities in the number queues you have
and and if you come up with a static
mapping of priorities to q's it leads to
multiplexing and in any priority
scheduling system you would like the
highest priority flow to be receiving
the entire shoot if it's if it is able
to saturate the link but in this
particular case it leads to it can lead
to multiplexing which degrades
performance and if you combine this with
arbitration approach so if you have in
network prioritization along with
arbitration you can address this
challenge of limited excuse in existing
citrus and the week and addresses you
can dynamically map flows to cues so for
instance the highest priority guy can
get map to the top cue the rest are map
to the low priority queue and as this
guy finishes you dynamically change the
mapping of lost accuse so you can you
you are able to address the issue that
you get off of only in network
prioritization purchase similarly if you
look at arbitration or no new approaches
there are there are two key challenges
with that one is high flow Stewart is
switching overhead the other thing is
that it's very hard for such approaches
to compute precise rates so for instance
suppose you have two flows one of the
priority 10 and the other flow with
priority 20 suppose that the arbitrator
decided to give one gigabits per second
to this guy and paused the other flow
and the question is what if the high
priority for can only send it point
eight eight gigabits per second so if
you have non-network bottlenecks which
might be receive a socket buffer or the
seconds or a center socket buffer it's
very hard for arbitration raise to
purchase to actually know of that and so
they employ different kinds of things to
actually overcome that which leads to
other issues so so the key problem is
that when you have more network
bottlenecks calculating precise rates is
hard and so with if you combine
arbitration reproach with self adjusting
endpoints what you can do is that since
self-adjusting endpoint has this nature
of probings the network so it can grab
the the remaining point2 gigabits per
second so you are able to maintain high
network throughput by by combining
arbitration approach with self adjusting
endpoints so similarly we have had we
have several examples and were in a
paper on how in network privatization in
arbitration
again can be combined to reduce the flow
searching overhead that you have with
with arbitration only approaches so base
essential you uses these insights in its
design and so the key design principle
of pace is that each transport strategy
should focus on what it is doing what
what what it is best is doing so the app
if you look at the arbitrator's they
should focus on inter inter flow
prioritization at course time scales or
at RTT time skills and endpoints should
probe for any extra link capacity which
might might be in the system because of
non for instance non-network bottlenecks
and in network privatization should
focus on per packet timescales
prioritization or sub RTT prioritization
and so here's is his head is the
overview of pace so we have an
arbitration which is a mechanism which
is implemented as a separate control
plane this is not part of the data plane
so data plane approaches require changes
in this fortress so this is unlike PP DQ
+ D cube so it's a separate control
plane which is implemented and the
output of the arbitration is essentially
a reference rate and a priority queue so
flows obtain that and based on that so
the so the end hosts essentially use
that reference rate and and and then
they essentially use a more guided rate
control so they use that reference rate
as a pivot and then they can increase
and decrease their rates around that
reference rate which is which is given
by the arbitration and finally for
inappropriate ization we leverage the
existing cues in in switches so let's
now talk about pace arbitrations so
every link essentially has a dedicated
arbitrator with pace and that arbitrator
essentially decides the reference rate
and the priority queue on that
particular links among all the competing
flows and what the source does is that
the source picks the minimum of that the
minimum reference rate of the priority
queue and so if whenever you have an
arbitration mechanism which is part of
the control plane
the three key challenges that arise one
is of arbitration latency so every floor
has to sort of potentially go to the to
an arbitrator and if these are really
short smooth flow flow spanning one or
two round-trip times that this this
latency of going to the arbitrator can
add up can be a significant fraction of
the order response time the second
challenge is processing overhead so if
you if every flow has to go to the
arbitrator and requires a immeasurable
processing that then that might be a
scale of scalability bottleneck and the
third is network overload what fraction
of licking / capacity is actually being
used by these arbitration messages or
control overhead so the way we address
this these challenges is by leveraging
the tree structure of typical data
center topologies and this leads us to
follow what we call a bottom of
arbitration strategy so on the right
hand side we have a in illustration to
explain that so the end when part
between a sender and receiver is
essentially divided into two halves so
so the left half is essentially taken
care of by the Centers of the sender or
the leaf nodes they initiate arbitration
so and on the left half and on the right
half the arbitration is is essentially
initiated by the receiver and so so the
center of tains arbitration feedback
from from here and it retains
arbitration feedback from here and it
picks the minimum rate and the priority
queue to use up now a question that
comes to mind is why you such an
approach so it turns out that if you
look at intra rack scenario you don't
need to go to the arbitrator at all so
essentially the sender has has a local
arbitrator which is responsible for
reservations on its link to the tour
switch the receiver is responsible has
it has an instance of arbitrator running
locally which is responsible for
reservations on this particular link so
for intra rack communication you don't
need to go to the arbitrator so you
avoid any network latency overhead
arbitration latency overhead and this is
important because intra rack traffic is
a size of a portion of data center
traffic because there is an explicit
notion of rack affinity in data centers
and for into rack communication
this kind of an approach bottom-up
approach lends itself to several
optimizations which I'll describe so
I'll focus on the delegation
optimization for inter rack scenarios
but it but early pruning is also one of
the optimizations which which helps in
reducing the overhead with with the
control plane arbitration so in
delegation essentially what happens is
that we reduce the arbitration latency
significantly by making decisions
arbitration decisions ask those two
sources as possible so here is the high
level structure so what with delegation
what we do is that a link with capacity
C is essentially divided into n virtual
links and the parent arbitrator
essentially what does is that it
delegates the virtual link to the child
arbitrator to load arbitrators so it
allows them to make reservations on its
behalf and the child arbitrated as
arbitration for the virtual link and
this virtual link capacity is updated
periodically based on the flows arriving
in different parts of the race and data
center network and so that was the
delegation optimization but reduces the
arbitration latency significantly and
therefore health short flows in the
inter rack communication scenario then
we have self adjusting endpoints so
based on the arbitration decision which
is the reference rate and the priority
queue we have a rate control protocol
which uses them the feedback from the
arbitrator's to to decide the the rate
so essentially the top Q uses
essentially the reference rate and then
you have the middle Q using the data
center TBT CV control laws and then the
bottom Q essentially uses a base rate
which is equivalent to I think one
packet per round trip time or less loss
recovery becomes complicated because you
have low priority queues where packets
might be buffered but they might not get
an opportunity for transmission so we so
we use large round trip time large a
retransmission time out for for lower
priority queues in order to avoid you
know timeouts and we also use small
probes to reduce the overhead foolish
timeouts in this particular case so now
quickly talk about evaluation so we
carried out evaluation of based on a
small-scale test bed in ns2 simulations
we considered web search in data mining
workloads we compared our protocol with
deployment friendly protocols as well as
with best performing protocol in terms
of completion times so here is the
structure of the topology comprising of
10 g + 1 g links and we we had you know
the round trip times and buffer sizes
which reflect what we have in typical
searches so here is a graph of the
application throughput or the number of
deadlines met as a function of the
offered network load so so the blue
curve is that of data center tcp and the
green curve is that of d square t CP
which is deadline where tcp protocol and
then you have pace which is red so
essentially as you increase the offered
load increasingly the application
throughput degrades for these deployment
for new transports and with pace this
it's strict priority scheduling approach
improves performance by making sure that
more fraction higher fraction of flows
actually meet their deadlines here is a
graph of the average flow completion
time as a function of the offered load
for comparison with best performing
scheme which is pays which is P fabric
so relative to P fabric up till forty
fifty percent load pace performs
compared ibly but as load increases
based significantly outperforms be
fabric so in conclusion pace synthesizes
several transport strategies while being
deployment friendly and its performance
is comparable to a better than the
state-of-the-art protocol so it required
changes in the data plane or network
switches and one of the aspect
interesting aspects of pace is that it's
a modular design which which helps in
enabling innovation a bit because the
reason for that is that you have in
depth of prioritization which is
decoupled from arbitration which is
decoupled from self adjusting endpoints
so if you have if you come up with the
newer arbitration algorithms you can
actually do almost like a plug-and-play
in
the transport framework of pace and that
would that would be that would be good
so which which helps in evolution so
that was our work and pace so if I'll
take any questions before moving on to
the wireless port so anything which is
unclear or questions okay so I think I
should move to the to the next part so
the next problem a project that we
described described is that of loss
differentiation in high-speed wireless
LANs so this is joint work work with
several of my students and this work
appeared in info confident 14 so packet
losses in Wi-Fi networks occur for a
variety of reasons so one is one may be
due to attenuation channel fading or
shedding effects and we call this noise
and the second cause might be collisions
so two or more nodes and transmitting at
the same time will lead to collisions
and packet losses and in this particular
scenario you have two nodes which can
actually carrier sense each other so
they can sense transmissions but they're
back off counters become 0 at the same
time so they lead to collisions and the
third packet loss can occur due to
hidden nodes so you have uncoordinated
nodes which can't carry a sense each
other when they transmit the
transmissions will happen time
collisions can happen so lost
differentiation is the problem of
inferring the cause of packet loss and
why is this important this is important
because different kinds of packet losses
men date different kind of reactions of
the mac layer so suppose the cause of
packet loss is due to noise or
essentially the channel condition was
was was not good in that case the the
approach the action should be to reduce
your bit potentially reduce your bit
rate physically arbitrate and if the
packet loss was due to collision you
need to increase your back of time so
the back of protocols which will use my
Wi-Fi needs to increase in order to D
synchronize the sources and reduce the
likelihood of successive collisions
similarly if the packet loss was due to
hidden
Oates you need to employ coordination
mechanisms like RTS ideas right and what
happens today is that packet losses are
not distinguished as a result based on
packet loss due to for instance noise
you would increase your back of which
would degrade performance due to for
instance if there was a packet loss due
to hidden nodes you would reduce a bit
rate which will increase the packet
transmission time and in fact increase
the collision probability in the neck in
the next round so if you are if you do
not distinguish between losses leads to
performance degradation and this becomes
increasingly important as you move to
tens of megabytes per second gigabits
per second Wi-Fi so for instance if you
have look at n it supports up to hundred
six hundred megabytes per second and a
state e relevant AC can support more
than one g 1 gigabit per second and so
lack of loss differentiation essentially
leads to higher loss of throughput and
such in such high data rate wireless
LANs so the key the key thing that our
work leverages is some of the features
which are part of it Logan n in AC like
standards so one of the features to
achieve high Mackley efficiency is frame
aggregation so with frame aggregation
multiple frames are transmitted on every
channel axis so in the traditional a
eight or 11 ABG networks on every
channel access you would only send one
one packet in this multiple frames are
transmitted per channel access which
amortizes the overhead and the second
thing is that when you have multiple
frames they leverage what we would the
the block acknowledgments so they have a
bitmap in this block block ack which
contains which contains information
about which frame was received
successfully which which is a frame
wasn't received successfully and so we
are our work for loss differentiation
and the key observation that we make in
our work is that if you look at noise
collision and hidden nodes they result
in a distinct structure in losses within
aggregate frames so an AM pdu or
aggregate Mac protocol data unit
is essentially a bunch of it's
essentially combination of multiple MPD
use so I'll use the term impede you to
describe the aggregate frame and so it
results in a distinct structure in
losses within the MPD use and MPD
retries so at a higher level essentially
if you look at packet losses due to
noise and due to collision hidden node
you can tell apart noise from collisions
and hidden nodes based on the
consecutive number of consecutive losses
within aggregate frames and you can tell
apart between collisions and hidden
nodes based on the retries the X retries
which I will describe in a bit so in
this work we proposed BL Mon the best
loss monitor loss transition scheme that
leverages the distinct structure in
losses without requiring any protocol
changes or customized hardware support
or out of band communication and so as a
result bin one can be deployed in
commodity devices using only driver
level changes at the center side so we
conducted a measurement study on a real
a two-run devanand testbed to validate
or our observation that that losses show
distinct patterns we design and analyze
BL mon we propose metrics to capture
quantitatively capture the distinctness
and patterns and then we and then we
propose BL one which uses a joint
distribution of these metrics in order
to infer the cause of loss and finally
we implemented this in the ath9k driver
and conducted evaluation on on the
terranan testbed so I'll quickly go over
firstly the measurement study that we
carried out so this was an indoor test
bet that we had on our campus and we had
a variety of links with with a wide
range of packet error rates so this
particular link for instance the minimum
packet array that we observed was 0
point 0 1 percent and the maximum was
twenty-eight percent similarly we have
several other links so we also had very
lossy highly lossy links like with the
minimum p or a cadet rate of forty-one
percent in a maximum
nine percent so these variations were
essentially due to shadowing effects
different objects that would be and n
due to multipath fading all nodes used
the 8th Ross ath9k driver and the
maximum retry limit was set to for this
is and when this is this retry limit is
exceeded or a frame as we transmit four
times then it is counted as an x3 try so
this is important because I'm going to
talk about x three tries in a bit we
used a variety of MCS indices bit rates
and multiplexing modes and here are the
results for the noise nozzle analysis
though of consecutive loss so if you
look at this particular graph this is
the CDF of consecutive losses within an
aggregate frame and this is a link which
with the minimum of 0 point 0 1 and
maximum 28-percent packet error rate so
the key key key thing to take away from
this graph is that more than ninety
percent of the ampd use they had less
than six consecutive packet losses so
and if you look on the right hand side
this is a graph this is a seed of CDF of
consecutive losses on the most lossing
link that we had which had a maximum
Pakatan rate of ninety-nine percent
maximum off by the standard maximum 64
but in our implementation in ath9k the
maximum 32 frames you isn't really doing
this no ok so we we did not change the
number of frames actually we sent it was
if it's dynamically adapted by the 888
Ryan key driver yeah and so in this
particular case we had at least sixty
percent of the losses which which had
conduct the sixty percent of the impede
us had less than 15 consecutive losses
in this particular case so the key
takeaway is that if you look at
consecutive losses they tend to remain
small across a wide range of packet
error rates then this is the CDF of the
x ray tries this is the MPD retries and
so we explicitly label x three tries
differently so for more than four
retries believe in it as extra time so
this is on link one
so the key takeaway in this particular
case is that almost all of the impede
use did not experience any weak
transmissions so this is in the absence
of collisions this is only due to noise
on this particular side we have the more
lossy link at various MCS values so in
this particular case only this guy which
is MCS 13 experiences a significant
fraction of retries but two key points
are that they are you zero or negligible
X retries more than four retransmissions
are very rare which is which is this X
retry part of the site and the reason
for that is that if you look at retries
when do retries happen retry happen when
you have the plc p header preamble
getting corrupted or the block back gets
corrupted now the plc p hadron preamble
are centered the basic rate which is
much lower than the data rates that you
use for the MPD use so as a result
they're more robust to channel
conditions so so that's the analysis for
the noise lost case then we now we move
on to the correlation norse analysis in
this particular case we only had
collisions and and so here is a graph or
here's a cdf of the consecutive losses
so observe that the loss pattern here is
much more bestie in the collision case
and then we have this is the a rat or
the MPD retries that we have so less
than sixty percent or retried one or
less times and so majority of em videos
are retried only to x and x three tries
are also negligible and the reason for
why XD tries are negligible is because
you have back off mechanism so if every
time a collision happens you increase
your back of window which d synchronizes
or reduces the probability of successive
collisions and so the x ray tries are
also rare and collision case then this
is the hidden mode now analysis in this
particular case we have huge leap st
losses almost all losses experience
where of 15 sighs and this and on the
right hand side we have the cdf of the
NPD retry subs
that more than eighty percent of the
packets experienced x-rayed rise and the
reason for that is that because of frame
aggregation presents an opportunity and
it's also presents a challenge because
now you have large number of frames that
you send at the same time so the amount
of overlap could also be significant so
because the nodes are uncoordinated in
hidden nodes this increases the
likelihood of successive collisions in
with with aggregation frame aggregation
so now we design I will talk about some
of the metrics that we designed to
actually capture the degree of
distinctness in these patterns so the
first metric that we designed is we call
that the best isolation index that
characterizes the decree of isolation
between losses so this is simply the
fraction of the number of 01 or 10
transitions where 0 is a a a lost MPD or
failed MPD you and one is a successful
frame received this is MPD you and
divided by the total number of Fairfield
frames as a result if you have
alternating sequence of losses and
successes in your in your aggregate
frame the BIA is to this is the maximum
value that you get and if you have
bursty losses then bii tends towards 0
so the more closer to 0 you are the more
the bursty losses are and if you have
all successful successful frames receive
the bi is undefined so here is a CDF of
bi I for in case of noise laws at ten
percent forty percent and fifty percent
so then you have a packet are eight of
ten percent in twenty percent a
significant fraction more than more than
seventy percent of the impede use
actually had a bi of more than point
five the packet error rate due to noise
increases then you start getting more
bestie losses so for instance the bi I
for about fifty percent of the MPD use
was close to zero and here is the CDF of
PII due to collisions and you to hidden
note separately so observe that now for
about seventy percent
or more mkay dues you had VII which is
much less than point five to read this
one this is not monotone this is not
Madonna CLE increasing so if you have
more if you ever have some such you're
not pushing the timer see what it was
where did those lines cross oh why do
you think these eggs pass yeah oh right
when will the life not in the same order
right right so this is this is
essentially VII calculated on each block
acknowledgement so there is due to you
know randomness because of shading
effects and such like you have these
variations increases so like I showed
there is a minimum PR there's a maxim PR
so there's a lot of you know variation
due to noise that you are you encounter
in this mythical it is not distinguish
between tenant for citizens between ten
and forty percent at very high VII
values yes so essentially the key
takeaways you need to if you look at the
ticket this graph you if you look at
particular this particular point then
you have a majority of em ampd us having
larger than 0.5% yeah we'll expect
because I the ten percent line is in
between the forums online and the same
section where right so here is the cross
and and in my understanding is that it's
because of the variations that we have
in the link characteristics so this is
point five why why is it's in percent
line in between seventy percent line and
why is the ten percent line in between
okay hello or is it labeling just yeah
it is lately right so I think one thing
is that we are I did not I should have I
think mentioned the MCS index that we
used which we did not use in my
conjecture is that based on different
mcss
you have different multiplexing modes
and that essentially leads to different
kinds of behavior based on so you can
either send two streams same streams on
two antennas or you can send a single
stream on both Antinous so that has
different properties but I need to
double-check with this you're right I I
think some information is missing here
as far as the MCS index is concerned
yeah and if you look at the CDF of bi
for hidden conditions and the collisions
we have higher values for majority of
frames which are less than 0.5% that
particular match so this is the CDF of a
rat for noise loss at 10 14 seventy
percent and HCL ncl and in the hidden
collision case we have large number of
extra trials that we observed and we did
not observe large number of XT tries in
both the noise case of the collision
case the third metric that we use is
that of history based packet error rate
so on the so this is this if you look at
so ampd you PR is the per ampd you
packet error rate which was in one
aggregate frame and PR is the short term
history of the package rate which is
which is smooth based on a weighted
moving average so the blue curve is
essentially the history p or history
based PR and the red spikes are the EMP
dup oars and so essentially you can
observe the sharp increases in the
history as well as the instantaneous
packet error rates and we leverage this
actually detecting collisions so bl1
essentially uses all these three metrics
together in order to maximize the
detection accuracy while maintaining low
for false positive rate so essentially
we use a very simple way of combining
these strategies there are several
strategies which are possible which we
we haven't looked at so essentially our
with BL mon if a bii indicates a highly
bestie link and x three tries the last
few bets are high if we observe extra
tries the last you burst then we
conclude the
to be a head-on collision if bii
indicates a bursty link and the EMP do
PR is greater than the history based PR
by some threshold we conclude this to be
a collision Louis otherwise and in all
the remaining ones if the first two
conditions are not true then we conclude
this to be a noise loss and we only
apply burst oz monitor up till a certain
PR so if the packet error rate if the
link is really noisy then it's very hard
to tell apart what was the reason for
loss and so and in our testbed this came
out to be roughly between 75 to 80
percent so beyond 75 to 80 percent back
at a rate we would not actually use PL
mon yeah the idea did you you you figure
out these thresholds for a given Network
and then you apply them or are the
threshold sort of robust it was so so we
tried various set of thresholds this was
an indoor test bet and these thresholds
were specific to the environment that we
used but our understanding is that they
fought for indoor environments they work
fairly robustly that would be our inside
but but you moving around Posey's a
wireless knows a little bit more enemy
right so we also carried out experiments
and mobile settings so i will show you
the results in a bit but but yes they're
for different kinds of environment like
outdoor settings and such like you may
need to reconfigure the thresholds but
with the environment that we looked into
which was typically indoor with the
variety of links these were fairly
fairly this give us the these thresholds
gave us fairly they were fairly robust
in different settings when your network
is deployed you're not going to have
ground truth on so you can dynamically
right learn what thresholds right you
have to do it in a controlled experiment
where you know the cause of the right so
then that has to be robust absence I
agree with that and and that is why in
our evaluation we we tried we ran be
evaluated by l1 with the threshold with
very special then you also look we also
evaluated in the wild when we when we
had uncoordinated devices that's a very
good point so we implemented bl1 in the
ath9k driver and so the PR is already
maintained and updated on every block ax
retries and x 3 tries are already
reported and the only computation we add
is that of bi I which requires
essentially linear scan of the 64-bit
block add bitmap so these were the some
of the scenarios that we evaluated BL
months we be considered static clients
UDP with collisions UDP with sources
with hidden nodes we tried with UDP with
varying sending rates with TCP clients
we looked into mobile clients ok so a
quick result on this is UDP with
collisions and hidden oh so this is only
for the this is the y-axis is the
throughput on the x axis we have the
number of nodes in the network so this
is with bl1 off and this is with bl1 on
so as you increase the number of nodes
what we observe is that we get gain a
larger improvement in throughput so 2.7
times improvement in throughput when you
have six nodes so the reason for that is
the larger number of nodes you have the
more opportunities you have for
collision and telling apart between
collision and you to know is on the
right hand side we have the throughput
as a function of the number of nodes
with hidden collisions so this is
without BL mon this is BL mon on and
what we do here is when you have when we
detect a loss due to hidden node we do
not change the our rate or physical
editor it and in this particular bar
what we have is that we envy Envy in
addition to not changing the rate we
also enable RTS EDS so we gain a larger
improvement in throughput then we had
TCP with the static clients and here the
improvement was approximately one point
nine times so TCPS congestion control
algorithm kicks in which reduces the
rate if you have large number of
successive tries and we had so this was
the in the mobile setting and this was
an aesthetic setting and yeah and so
there's a lot of excellent work which
has taken place in the in this in this
space one body of work is it requires a
customized hardware support so there's
soft rate which essentially uses
physical layer physical layer
confidences and to to compute the
interference-free bit error rate and
then you have zigzag for hidden
collisions then their protocols like
coolie which requires changes in the
protocol or an out-of-band communication
and then you have purchase which use the
RTS CTS dynamically to actually
differentiate between losses BL mon does
not require any of these things it
simply uses it looks at the block at
bitmap and tries to infer the cause of
loss and in conclusion bien Mon
differentiates losses while encoding low
overhead uses features which are part of
emerging standards in existence that is
like n and AC frame aggregation block
acts and it only requires driver level
changes and it can improve throughput
significantly so this is all possible
because of a fantastic group of students
i have had i have in the industry and
these are these these students have
worked exceedingly hard and the
brilliant i have two PhD students also
so one of my pc student who was part of
the pace work which represented in the
first it he attended the summer MSR
summer school and had had a lot of fun
camron so the wireless work that i
presented he worked on that that work
and there's a considerable focus on
undergraduate research at lumps so if
you look at these body of works papers
which appeared recently in venues like
in for comment sitcom the the
red-colored names are those of
undergraduate students and they all have
received a range of offers from leading
universities in the US so we had
we have received offers students have
you see offers from MIT Stanford CMU
Cornell and suchlike and that's all I
had thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>