<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generalization Bounds and Consistency for Latent-Structural Probit and Ramp Loss | Coder Coacher - Coaching Coders</title><meta content="Generalization Bounds and Consistency for Latent-Structural Probit and Ramp Loss - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generalization Bounds and Consistency for Latent-Structural Probit and Ramp Loss</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_kMwKHfipZM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let's get started so it's my pleasure
to introduce David McAllister with the
professor and the chief academic officer
at the Toyota technological institute of
chicago prior to that he was a faculty
member successively at Cornell and MIT
and then it was at AT&amp;amp;T a researcher at
bell labs from AT&amp;amp;T Labs and he is a
true playa fellow in a hospital another
lot of different areas machine learning
of course but also natural language
processing a vision di planning and
automatic reasoning and the view of
Earth but I have missed and so I just
want to mention also that also options
to to meet with David so then it sent
the signup sheet around and so ever
Phillip Phillip the sheet or talk to me
send me an email and we can certainly
arrange something so now I let David
speak okay it occurs to me that we may
have a problem with this talk in that
there may be people in the audience who
have seen it before so there was a
workshop on speech and language a year
ago in bellevue and i gave this talk
there have how many people have seen
this talk before any has anybody seen
this doctor okay at least the people who
seen it before it didn't come okay so
we'll go ahead with this this is joint
work with Joseph kesha at TTI this is a
theoretical talk just how many people
have ever published a paper with whose
point was a generalization bound or some
cult-like comfortably okay so this is a
theoretical talk but not a theoretical
necessarily theoretical audience so I
often give this talk to people who don't
do natural language processing I know
there a lot of NLP people here but my
favorite example
that I should try to use as a running
example in this talk is actually machine
translation so in machine learning we're
often interested in binary
classification because a patient have
cancer or not alright so this often a
typically cult the conference on
learning theory studies the problem
theoretically of binary classification
but what I'm going to be interested in
here is the study of what we call
structured labeling and my favorite
example of structured labeling is
machine translation we have some input
which is a structured object like a
sentence and we're interested in not
necessarily labeling it but but decoding
it into a sentence in some other
language so we can think of a decoding
problem as a problem where we're given
some input X and we want to produce some
output Y where the output Y we're in the
input X is a structured object like a
sentence and the output Y is a
structured object like a sentence and
the way we're going to do the decoding
is by optimizing a linear score so we're
going to say find the output the decode
which maximizes a linear score which is
an inner product of a weight vector and
a feature vector and if you're if you're
familiar with machine learning theory
and support vector machines and kernels
you realize that almost any kind of any
kind of scoring function can be
represented this way by making the
feature vector sufficiently elaborate
you can make it have all kinds of
nonlinear features so even though this
is a linear score the feature map can be
nonlinear so you can represent
arbitrarily complicated things this way
okay so this is our decoder it's
maximizing its producing an output
maximizing this linear score we would
like so so what I'm gonna be talking
about is training the weight vector
we're going to hold this feature map
fixed and we're going to look at the
training problem the training problem is
the problem of setting the weight vector
we would like to set the weight vector
such that it minimizes the expectation
over input-output pairs so I'm going to
assume here we've got a corpus of
translation pairs and we've got
input/output pairs and we've got
something like the blue score that we're
interested in minimizing
or the or we're interested minimizing a
loss I'm going to use a loss that we're
minimizing and what we would like to do
is minimize the expectation over drawing
a new reference pair a new translation
pair of the loss between the reference
translation and the translation produced
by this decoder yet what is it
reasonable to assume that you have
distribution or otherwise I'm stare by
so over all the inputs you have this
efficient web why is it what is that you
have distribution over the out it's just
a standard called assumption that you
have a distribution over XY pairs
intuitively intuitively there are large
corpora of translation pairs and they
were produced somehow the coolest you
know classification
or you have targets here the Y's are
translations right right so it's
well the way I think about it is it's
sort of a cynical way to think about it
is we want to win the competition right
and we know that the competition is
going to have translation pairs that
we're gonna get scored on and and our
training data is probably a it's
probably reasonable to assume that the
training pairs are drawn from the same
trance aim distribution that our
evaluation pairs are going to be drawn
from and we want to win listen what me
just say that though is that otherwise
you have the traditions of exes because
that's what you normally encounter all
right you don't have the random possible
completions we've got all of the
pleasure zones but all you need all I'm
going to need here is there's some
distribution of XY pairs I don't need
that there's a true blood other is an
e-check should any funny situation is
that the white here is not sampled from
the cross the entire distribution of
whites it's a very pious then why part
is the one that's really vice X is going
to be a random sample of your language
utterances but why is going to be
specific to reasonable translations for
closeness with
drums tuition assistance
so this is this is a pair sampled from
trent from some corpus of translation
pairs so i built a big corpus of
translation pairs i randomly select some
is trained and some his test the this is
a whole new translations whatever your
whatever your translation care data is
like right so this is a gold standard
translation a reference translation okay
so so okay we're interested in what we
would like to do is find the w that that
produces the best performance on the
evaluation at the evaluation now the
problem is that the typical way of
approximating this that there's a
problem with overfitting right if my
corpus is not large enough I can make my
training performance very good but my
test performance is going to be bad
because I over fit and the way over
fitting is typically controlled is by
adding a regularization term so the
training algorithm minimizes a loss a
sum over the training data of a loss on
the training data plus a regularization
to try to drive the the norm of the
weight vector to be down and what we're
going to be doing is is giving theorems
that justify this kind of structure that
give us generalization properties okay
now there's a fundamental problem I've
written this as L sub s this is a the S
stands for surrogate if I literally use
the training Los this decoding is
insensitive to scaling w so I could
scale w I could take W and scale it down
just multiply it by epsilon and it
doesn't change the decoding at all but
it completely eliminates this
regularization term so regularization of
the task like I'm going to call this the
task loss like the blue score the thing
we're going to get evaluated at at test
time if I simply use the task loss as
the empirical measure I become
insensitive to scaling w and the
regularization is meaningless
so we need a surrogate loss we needed
that we need the surrogate loss to be
scale sensitive right so as we scale w
up this lost changes its sensitive to
the norm of W okay so an SVM and so this
is a this is this is the loss the
surrogate loss function underlying a
structural SVM yeah since I would
reverse the victor vaporization salsa
scanning system
check it out
well you can make the regular I mean an
SVM is going to use I mean you could
work you could say W has unit norm and
then work with a margin but then the
margin becomes scale sensitive so the
standard approaches work with this
standard approach to this an SVM
approach works with a scale sensitive
regularization the people also don't
always use l2 they use some other norm
but that norm also has a scaling issue
okay so here is how many people have
seen structural svms this is the general
is okay well this is the structural
hinge loss this is the binary hinge loss
and it's it's easy this is a standard
mapping from the structured case to the
binary case in the structured case the
feature map takes two inputs in the
binary case the the label is either
minus 1 or 1 and this is the standard
mapping from the between the two but i
can define the feature mat on XY to be
this way and i get i can define the
margin to be this and then the standard
hinge loss looks like this so under this
standard mapping between the structured
case in the binary case this agrees with
the hinge loss okay I'm not going to say
i'll come back to the structure of this
I think but this is a difference this
hinge loss is the difference between
something called the loss adjusted
inference this is maximizing the score
plus the loss relative to the reference
translation so this is a loss of a
weight vector on an input X and a
reference translation Y and this is
saying consider the decoding which
decodes in favor of bad blue scores it's
a loss adjusted inference and you're
saying take a bad label favoring bad
translations and and take the difference
between that score adjusted by the loss
and this and that's the structural hinge
loss
okay so I'm just going to go through
surrogate loss functions this is log
loss this is we define the probability
of an out of a decode given X if we have
our log linear model we can define this
probability in a log linear way we take
it to be proportional to the exponential
of the score and then normalize with a
partition function but then if we take
that in the binary case we get this
smooth curve that looks a lot like is
qualitatively similar to the hinge loss
so in the binary case the log loss and
the hinge loss are similar ones a smooth
version of the other one thing I want to
point out though is that in the
structured case this this max is over an
exponentially large set right it's all
possible decodes in the binary case
that's over only a two element set so
it's not too so it's it's the fact that
these are similar in the binary case can
be very misleading in the structured
case these are I believe these are quite
different loss functions okay what I'm
going to be talking about in this talk
is consistency so I'm going to give you
a learning algorithm I'm going to do is
define two more surrogate loss functions
we're going to talk about ramp loss and
probit loss that both are also
meaningful in both the structured and
the binary case and I'm gonna be showing
that those loss functions are consistent
in a predictive sense that means that in
the limit of infinite training data the
weight vector will converge to the
weight vector that's optimal with
respect to your loss function and I'm
not there's no notion of estimation or
truth here there's just optimal
performance relative to the blue score
these functions are convex so there's a
fundamental convexity consistency
tension any convex loss function if
you've got an outlier especially in
machine translation you're going to have
reference translations which your
decoder are not going to get right so
you've got these hopeless reference
translations and they're going to have
bad blue scores
there's nothing you can do about it
right your systems not going to get it
so you're going to have outliers you're
going to have margins that are bad so
when you have outliers they have large
the loss is large especially for a
convex loss function the convex loss
function has to assign them large loss
and that can't because its convex it
also has to be sensitive to how bad
these terrible translations are so your
your ultimate and training algorithm
becomes sensitive to the outliers and
that's going to block consistency so a
convex loss function is not you're not
going is not going to be consistent if
you're familiar with SVM's if you have a
universal colonel something else happens
people claim that as binary svms are
consistent and it's because they're
talking about a universal Colonel but we
can talk about that if there are
questions okay here's the ramp loss so
if you remember the structured hinge
loss it was a difference between a loss
adjusted inference and the score of the
reference translation here's the hinge
loss this is a loss adjusted decode the
score of a loss adjusted decode it's the
score plus the loss minus the score of
the reference translation okay now what
we're going to do is replace the score
of the reference translation by this the
unadjusted decode the score of the
unadjusted decode so this is the score
of the adjusted decode minus the score
of the unadjusted decode and if we go to
the binary case via the standard
translation we get a ramp that is not
convex right at the the outliers
eventually have constant loss because if
these two things once these two things
agree have the same decode you were just
left with this loss so you get this
plateau
and you can see that it's different from
the hinge loss so it's not convex it has
this other aspect to it here's the
probit loss one more loss function the
probit loss what it does is it takes the
weight vector W and adds a Gaussian
noise so if we're in D dimensions we're
going to take a d-dimensional unit
variance Gaussian noise add it to our
weight vector decode with that take the
loss and then we're taking the
expectation over the noise of the decode
loss right so that's the probit loss if
we take that to the binary case we get a
smooth a smooth ramp right it becomes a
continuous this is going to be a nice
continuous function of W right so we get
this but again the fact that this looks
like this qualitatively is misleading
because the binary case is misleading
relative to the structured case okay so
here are some basic properties that hold
in this both in the binary case and in
the structured case so the structure
case is different for the binary case
but these qualitative properties hold
the ramp and the probit are both bounded
201 so even in the structured KO I I
should back up I'm assuming that the
task loss itself is bounded 201 all
right so assuming the task loss is
bounded 201 these loss functions are
bounded 201 so that's going to mean
they're not going to have high loss
outliers it may it's a robustness it's a
robustness property no individual data
point can have an enormous effect
another property is that the the ramp
loss is a tighter upper bound on task
loss than is hinge loss and all of these
properties are kind of immediate
properties for example you get uh you
get that this is an upper bound on the
10
gloss by sticking the decode so if I
stick the decode value in to hear this
thing goes down right because i'm taking
a value that's not optimal right if I
stick the decoder that optimizes this
into here these cancel because it's the
same score and I'm just left with the
loss and all of the properties that I
just showed you are derived it's just by
findings one of these Max's and sticking
in a value and realizing that this goes
up or down and okay so we have these
properties this property was used as the
original so there was a 2008 paper
introducing the structured ramp loss and
their motivation was simply this
property but it's a tighter upper bound
on the task loss yeah the show though
probably love so if hope so the red loss
is a profound that public loss is not
enough about you
the profit loss is not right but but I'm
going to argue ultimately the probit
loss is the is the best thing yes so for
the binary patient's house boss is you
alone
right so so the next slide
previously
in fact is going by the right time with
a set of whatever shape it is that would
be the house was the pitcher
that's why it's a better approximation
is outside so if you look at hinge sorry
hinge here so task loss is going to go
is a step function right there just ok
as close to the stuff function right
so in this regime it's closer to the
step function than it would be if it
went up here
okay so this is just a slide on the
history of some of these ideas so
there's a question of where these is
there a reference for each of these
structured versions of the standard loss
functions so this is the structured
hinge loss oh no I'm sorry I'm sorry
this is talking about sub gradient
descent on unregular eyes DRAM plus if
you look if you do sub gradient descent
on this right so what is what wouldn't
what what does it mean to be sub
gradient descent on this so great the
scent means you you find this Maximizer
you find this Maximizer and then you
take the gradient of the that function
of W with respect to W so your take your
finding a bad label and a better label
and the gradient with respect to W is
the difference in their feature vectors
so there's been working natural language
which says look at it they can invest
list so I'm going to argue that the
following hack is approximately sub
gradient descent on the ramp loss take
an end best list of your decodes measure
the blue score on all of them relative
to the reference translation distinguish
the good ones from the bad ones take the
feature vector of the good one and and
the difference between add in the
feature vector of the bad one and that's
a direction that's moving you toward the
good one and update your weight vector
in that direction toward the good one
and away from the bad one and the
argument is that this if you do sub
gradient descent on ramp loss it's it's
a version of that you're taking a bad
one and a good one and you end up moving
toward the good one and away from the
bad one you're actually better in
practice finding finding the decode and
a better decode doing loss of Justin in
the other direction but this is a
theoretical talk
and the theorem is easier this the
theorem actually I don't know how to
prove the other theorem the theorem
works this way okay so so several peeps
everal groups in machine translation
have done things like this that are like
sub gradient descent on the ramp loss
and we call that direct loss and we had
an earlier theorem relating the sub
gradient ignoring the regularization to
the gradient of the task loss yeah
happily abandon convexity my
understanding is that the machine
translation communities function is
convex it's not happy we would love
comics today we can't find a way to
bring it here but 01 laws in that kind
of x the theoretician still love
convexity okay and we've we've recently
disposes the slide about empirical
results we've recently experimented with
probit loss directly and shown that
probit loss shows improvements over
hinge loss there are two ways we've got
an improvement we've gotten improvement
with the direct loss update sub gradient
descent on ramp loss using early
stopping instead of regularization in
practice and we've got improvement using
the probit loss with a normal
regularization these these are
improvements over the structured hinge
loss okay so now i'm going to start
proving theorems so I'm going to traduce
some notation so if I have a weight
vector W the loss of W is just defined
to be the expectation over drawing fresh
data the expectation of my test time
loss all right so this is the expected
test time loss of w l star is the best
test expected less test time loss that I
can achieve the info / w of the test
time loss of W what's the best loss I
could achieve with any W
okay now i'm going to build empirical
this is an empirical loss measure so I'm
going to prove consistency the way to
prove consistency is I'm going to zoom
there's an infinite sequence of training
data and I'm going to look at what
happens when I trained on the first end
and then I'm going to let n go to
infinity so so we're always training on
the first end we're letting n go to
infinity so this is the loss the average
loss on the first end training points
okay that's what this out loss estimated
loss based on the first end training
points of W okay this is going to be our
learning rule so we have so what we're
going to do is we're going this is the
learning rule I had before this is the
surrogate loss function this is the
surrogate loss the average loss on the
first end training points we're going to
optimize the score which is the the
measured surrogate loss the probit loss
in this case plus a regularization of
the norm of plus or regular riser okay
and what's going to happen is that
lambda n is going to grow within so
we're going to regularize somewhat
harder somewhat harder than 1 over n as
n increases okay so here's our theorem
as long as lambda n increases without
bound it could increase very slowly like
log n but lambda n log n over n
converges to 0 so this is this is
another way to think about this is
lambda n could be any power of n
strictly between 0 and 1 so it's it's it
grows at some rate between these two
bounds it increases but this goes to 0
then the limit as n goes to infinity of
the generalization probit loss so this
is the the generalization probit loss
equals L star
what was Pro good it actually
probability 1 over the sequence right
over the choice of the sequence okay so
here's the here's the theorem I'm a pact
Bayesian person so that's just what i
like to use and I think these theorems
are much more awkward than any other
framework because especially this
theorem because there's a tight
connection between the pack Bayesian
framework and this particular loss
function so there's a general Pak
bayesian law I'm going to do here is
state the general a general Pak Bayesian
theorem there are a couple recent
references that that put the pack
Bayesian theorem in this form so the so
this says that if I have a training loss
okay so what's going on here I've got a
space of W weight vectors the space of
weight vectors is continuum is
continuous right so so anything that
talks about discrete sets of predictors
isn't going to work here so in the pack
Bayesian framework we assume a prior
over the weight vectors and we're using
l2 regularization and the natural prior
corresponding to an l2 regularization is
a Gaussian prior so I'm putting a
Gaussian prior isotropic Gaussian prior
on the weight vectors so the pack
Bayesian theorem however is completely
general it says for any set of
predictors and for any prior on those
predictors what I'm going to learn is a
posterior on its kind of pazi and it's
going to learn a quote posterior on the
predictors and the way I'm going to use
that posterior at test time is I'm going
to randomly draw a predictor from the
posterior and use it ok so the loss of
the posterior is the expected loss over
drawing a predictor from the posterior
on predictors so this says that the
generalization so with high probability
over the draw of the training data for
all simultaneously for all possible
posteriors q
for all possible waits on the space the
generalization loss of that posterior is
bounded by this expression in terms of
the training loss of the posterior now
this is the training task loss but we're
drawing from the posterior so it's going
to actually become scale sensitive Plus
this thing that depends on the KL
divergence between the posterior and the
prior and a confidence parameter and the
number of points of your training data
and this so all I'd have to do here is
pick my regularization parameter before
I look at the data right this is to make
this exactly hold so it's a very nice
simple statement and you can see that
the regularization parameter can't get
too large before this term doesn't
matter before this term becomes close to
one so this is sort of saying this is
sort of predicting the regularizer in
some sense the Stearman sort of saying
your regular Iser should be roughly
order one in this formulation and in
this formulation we're getting exists
something that looks very much like
regularize minimizing exactly what our
learning rule is minimizing so just as
so now I'm going to do is I'm going to
say ok the what I'm going to want to do
is bound a certain loss of a weight
vector W that I'm learning so my prior
is centered around zero right of my
prior is a Gaussian prior centered
around zero I want to say something
about some w that's highly non zero so
I'm going to take my posterior to be
centered around w with the same
isotropic Gaussian distribution so that
distribution is exactly the distribution
i get when i add Gaussian noise to W
right so adding Gaussian noise the W
defines a posterior over the
distribution I simply plug that
posterior into this formula and this
becomes the probit loss of W right that
is the probit loss of W this is the
empirical probit loss of W right and I
get this equation this this bound so now
basically this is going to give you the
theorem
right so the theorem says so help of Q
is the expectation of them also the
drawers from cute right and reason this
works it seems is like because you have
expectation of a program without huge
taken Carol dance petition of the noise
right it's in a reappear great right
yeah this is an expectation over noise
so it's drawing from cute essentially
and the same is true here this is is the
empirical performance when I draw from Q
this is also the empirical performance
when I draw from Q okay now too now
we're interested in proving consistency
so we're interested in taking n to
infinity and the conditions I gave on
lambda n are going to be such that this
term is going to go to 0 as n goes to
infinity and the reason I need there's a
log term that's actually coming from
this the other thing I'm going to do is
let my Delta term go as 1 over N squared
to get probability 1 over the the
sequence and and then the if this term
is going to 0 as n goes to infinity but
I still have this inequality and lambda
n is going to infinity which means this
is going to 1 i'm getting that this term
is dominating this term and then i have
to argue that i can take w of i can
consider particular w's of increasing
norm and add 4 w's of sufficiently large
norm this this term is going to converge
to l star that's a little like the paper
has a little bit of a careful argument
there that basically what you want to do
is say for any so L star is defined to
be the info / w of the performance of W
in the proof I have to say well consider
any reference w all i have to prove is
that my performance gets at least as
good as any reference w if i hold a
reference w fixed and take into infinity
the algorithm is going to minimize this
right so what I'm going to get is that I
have a better slide with this no so all
I have to do is prove that I do as well
as any reference w I pick a w I have to
prove that I'm doing as well as that w
my learning algorithm is going to be
minimizing this expression the learning
algorithm is going to pick something
whose probit loss here as n is going to
infinity right
I also only have to look at the limit as
n goes to infinity the learning
algorithm is going to pick something
whose empirical probit loss is doing at
least as well as w and i can also at the
same time consider scalings of my
reference w i can scale it up to be
larger and larger and as I scale it up
to be larger and larger we can prove
that this empirical probit loss becomes
a valid estimate of the true
generalization loss for that particular
W as that as its norm goes to infinity
it's a countenance of some what I've
given this talk before I've really
cheated at this point in the talk and
said this is a straightforward argument
now there's one issue in this theorem
that some people get upset about in that
what I've proven here where's my
consistency theorem
what I've proven here is that the probit
loss of my estimator is converging to l
star I'm i have not proved that the loss
of my estimated w is converging to l
star and I justify that by saying well
this probit loss can be realized I can
actually just implement the process that
adds Gaussian noise make predictions so
it's giving me a prediction algorithm
whose performance is approaching L star
because I can achieve this loss the
reason I can't get that L approaches
that L approaches L star has to do with
the fact that in infinite dimension and
with latent variables even though the
performance is converging you can
construct this weird example where the
vector W is rotating in an infinite
dimensional space forever and it's not
actually the direction of W is never
converging okay so here we're going to
do the analogous thing for ramp loss
this becomes much more this the proof of
this is much trickier but now we're
going to replace the surrogate loss with
ramp loss so this is the empirical ramp
loss we're going to minimize the
empirical ramp loss plus a regular eyes
term and the theorem is very similar
there's like a logarithmic factor
difference in the theorem right so it's
still going to be true that for any if
the regularization is a power of n where
where that power is strictly between 0
and 1 we're still going to get
consistency so this theorem looks
deceptively like ramp loss is similar to
probit loss I'll say why that's not
going to work or why it's deceptive
later but it's essentially the same
theorem up to logarithmic factor okay
how does this theorem work we have this
we have this inequality we know that
ramp is an upper bound on task right and
we know that the limit as as as the
weight vector goes to infinity or
equivalently as if we take a if we think
of adding
boys with variance Sigma if we take the
variance 20 that's equivalent to taking
this weight vector to infinity that we
have this system of inequalities so what
we want to do is find a finite rate as a
function of Sigma that relates the
probit I'm sorry that relates that
relates the probate to the ramp loss so
we're going to get so we know that the
limit as as Sigma goes to 0 of this
expression is less than the ramp loss
and what I'm going to do is I'm going to
give a finite rate for that in terms of
Sigma okay so this is our theorem that
the probit loss is bounded by the ramp
loss the probit loss at a finite sigma
is bounded by the ramp loss plus a
penalty that depends on Sigma okay so
that's what I'm going to prove I'm
prutte I'm taking this inequality in a
limit and giving it a rate so at a top
at the top level what I'm doing is I'm
room I've got a bound in terms of the
probit loss I just argued for that based
on this pack bayesian theorem and what
I'm going to do is I'm going to relate
i'm going to give inequalities relating
ramp to probe it and then use the bound
on the probit loss and i've got this so
should i skip this slide I can see
people fading away yeah the last term
that we should live is the busty of the
West base
and get oh yes sorry this this is the
number of possible decodes right and
that's bad but at least it's in a log so
this is you think of this as the length
of the sentence that's still bad we
believe I believe that using Johnson
Lyndon Strauss we can actually prove
that we can get that down to log log in
the original paper kohler guest Ron and
tasker colon guest Ron on the structural
hinge loss they proved the theorem with
a log log term here basically in their
particular setting I think you can get a
log log in general but yes this is the
number of possibility coatings and
that's a that's a troublesome term let
me just give you the the essence of the
idea i'm not going to go through all of
this i'm just going to let's just look
at this one line what's going to happen
here is I'm going to say for every
possible so we've got the space of
possible decodes I've got an input
sentence on the space of possible
decodes for every possible decode
there's a margin what I mean by the
margin I mean take the decode that
systems actually producing that's the
best scoring decode every other decode
is going to have a score worse than that
take the the gap between the two scores
and that's the margin of a potential
competing decode sometimes these
competing decodes are called distractors
the biological community like it likes
to calm distracters so every distractor
every alternate decode has a margin the
idea is that there's a certain threshold
on that margin which provably such
provably that anything that whose margin
exceeds that threshold can be ignored
right it's it's just not the decode is
not going to be that when i add noise to
the weight vector so the idea is that
the probit loss is less than this thing
is going to be some something handling
the face that the fact that it's not all
it's not completely true that i can
ignore the bad guys the things with
large margin but i can basically look at
the maximum that the probit loss is less
than or equal to
Sigma we're going to take Sigma 20 plus
the max overall the plausible decodes of
the loss of that plausible decode and
that's the fundamental proof method in
rule in getting something for ramp loss
right because this quantity the max over
plausible decodes is something that I
can relate to ram plus and this
threshold is just picked such you know
to make this theorem true especially
when I do a Union bound and a high
probability deviation bound and then
this is just saying this is just
finishing the proof it's taking that
first thing and doing a sequence of
steps that relate it to the ramp loss
this max can be replaced in here by a
max over everything and then this
becomes the decode this becomes a lot
equivalent to a loss adjusted inference
and you get it related to the ramp loss
without going through a lot of details
there and then using this inequality
this main lemma this is as the probit
loss is less than or equal to the ramp
loss plus this I get this generalization
bound for the ramp loss this is saying
the generalization probit loss of W over
Sigma is less than and all I've done
here is I've taken the probit loss and
replaced it by an upper bound by the
upper bound I just proved right so the
probit loss was upper bounded in terms
of the ramp loss and I've replaced the
probit loss in the original theorem by
the ramp loss here and now I've got a
generalization bound on the got a bound
on the generalization probit loss in
terms of the empirical ramp loss and now
we can just take schedules for Sigma and
lambda and get our theorem get our
consistency theorem
okay now the other thing we can do
rather than just taking schedules for
these to get our theorem the other thing
we can do is actually sort of optimized
away Sigma and get a generalization
bound directly in terms of the ramp loss
so it turns out that this is an
approximately optimal value for Sigma to
minimize that bound and now we get a
finite sample generalization bound in
terms of the ramp loss so this is saying
that the generalization probit loss is
bounded by this thing in terms of the
ramp loss and really I think that these
consistency theorems these asymptotic
consistency theorems are not so
interesting what's much more interesting
to me are these generalization bounds
because this is providing a concrete
finite sample generalization guarantee
it's telling you more than any kind of
asymptotic infinite limit statement so
this is the this is the the finite
sample guarantee we had for
generalization with respect to using the
probit loss as the surrogate loss when
we optimize away Sigma here's the
analogous guarantee in terms of ramp
loss but now look at the look at the
differences between what I want to focus
on is this regularization term and this
regularization term so this term is
linear in lambda in in this part of it
lambda times W squared 2 over N this is
lambda times W squared over N to the one
third so what this says is that for the
bounds we've been able to prove the ramp
loss bound is significantly worse than
the probit lost bound and it could be an
artifact of the proof technique because
the proof technique was natural and
immediate for the probit loss but i
think my feeling is this is real that
you're much you're better off using this
probit loss okay so I'm basically done
I've skipped some of the glossed over
some of the technical details and the
summer is the summer
as we know we need surrogate loss
functions if we're going to regularize
and we have all of these standard
surrogate loss functions from the binary
case that generalize to the structured
case I haven't talked about it but all
the theorems in the paper also are
actually written for the structured
latent case so in the structured late in
case i optimize not only over the decode
but i also optimize over latent
information like parse trees or what
have you and all of all of this analysis
generalizes to that case as well and we
have probit entrap that are both
provably consistent but i believe
they're significantly different in the
structural setting yeah so you're in
tradition is why the car back software
is not consistent is that they are on
now that they just keep going out which
makes them sensitive to outliers so that
would suggest that if you consider for
example the past and one of my functions
nava like the truth
time maybe we could this generalized to
all such functions you pick two
particular functions have rocks in the
shape but also found it onto the
functions
well that that's almost certainly true
in the binary case and and there are lot
there's lots of theory about it uses a
Lipschitz bound that they have to have a
Lipschitz constant associated with them
and then the bound comes out in terms of
the Lipschitz constant I would have to
think about what properties you might
need for the I'm probably there's
something like that the structured case
so the fewer theoreticians talked</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>