<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficiently Verifying Outsourced Cloud Computation: from Theory to Practice | Coder Coacher - Coaching Coders</title><meta content="Efficiently Verifying Outsourced Cloud Computation: from Theory to Practice - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficiently Verifying Outsourced Cloud Computation: from Theory to Practice</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Fff-u4Tm4CM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright everybody looks like we've
finally reached a critical mass here so
I'd like to welcome you all to this
session on verifiable computation which
is a topic that has moved from esoteric
theory to practically near something we
will do be doing in practice any time
though and it's been making this
transition in a short less than five
year period so it's a pretty exciting
area to be working in i'll be your host
on brian par know i'll be hosting you
for the next 75 minutes during a recent
graduate from Carnegie Mellon and I
graduated about three years ago I've
been at the security and privacy group
here in Microsoft Research since that
time and I'll be giving you a brief
overview of the field of verifiable
computation as well as touching on some
of my recent work in this area which has
been focusing primarily on cryptographic
techniques following that will have
professor Michael mitza mocker whose
group has taken almost the exact
opposite approach they've avoided
cryptography entirely and instead
focused on what are called interactive
proofs pressor mitza mocker recently
finished a stint as the area Dean of
computer science at Harvard University
during his career he's published over
150 journal and conference publications
on topics ranging from hash based data
structures error correcting codes and
data compression techniques he's also
published a textbook on randomized
algorithms and his paper on low-density
parity-check codes has recently won the
ACM sigcomm test of time award following
what Professor emits mocker will have
for a sort of Mike walfish from
University of Texas at Austin his group
has also been working in this area and
they've actually been combining in a
very elegant fashion oh yes here is also
right there the restaurant wall fishes
group has combined both proofs and
cryptography in a very elegant way his
interest cover things from networking
systems and security and so naturally
he's worked on topics that include
untrusted networking untrusted storage
and security against denial of service
attacks he's also won numerous awards
including the NSF Career Award a slow
and research fellowship and Air Force
Young Investigator award as well as
multiple teaching awards from the
University of Texas now from these
introductions you might have noticed
that there's a correlation between
working in
computation and being named Michael but
turns out there's an even stronger
correlation and that all three of us did
our undergraduate work at Harvard
College where the the school motto of
Veritas appears to have had some effect
so without further ado let's turn to the
topic of outsourcing computation in
particular we're going to look at a
scientist named Alice who hasn't had so
much luck with her grants lately and so
instead of doing the computation locally
she's going to outsource some very
important computation to the cloud and
based on the results she gets back from
that she might do some further thinking
and run the computation again with some
new data hoping to get a different
answer but unfortunately she gets the
same answer back and of course
eventually it might occur to Alice to
wonder is the cloud doing the
computation that she asked it to or is
it doing something a little simpler and
probably cheaper from the clouds
perspective now this seems like kind of
a contrived example but if you look at
the customer agreement that you signed
when you sign up with Amazon's web
services or with address for that matter
you see there's basically no guarantee
that you'll get anything back or that
the things that you get back have any
correctness or good properties
associated with them and of course it's
not just scientist that are outsourcing
computation today it's fairly typical to
take some tasks that would be to power
intensive to run locally and outsource
that to the cloud and it might be nice
to know that the results coming back
from that are in fact correct and even
from the perspective of legitimate
providers who we like to think are not
trying to actively change the results
they're giving you having verifiable
results can help encourage confidence in
the cloud and hence move more customers
to the cloud platform it may also
command a premium over those services
that don't offer this kind of guarantee
and we look you can look at it as a way
of shedding liability in the sense that
if the customer is unhappy with the
results they get back the provider can
prove to them that the results that they
get back our came from the inputs and
the computation that the client
specified and are not the result of
mistakes or other changes that the
provider might have made and so before
going into the details of this area I
thought it would be nice to have a sense
of just how quickly this area is
changing so starting about five six
years ago the state of the art was
something called probabilistically
checkable proofs or pcps which is
elegant and beautiful theory but if you
apply it naively to a computation such
as matrix multiplication you find that
the verification takes something like 72
trillion years and that's pretty bad in
general but it's even worse when you
consider that the base calculation only
takes about 15 milliseconds so nobody's
going to outsource under this regime now
a few years ago we developed a technique
that is based purely on cryptography and
not on pcps and we're able to reduce
these costs by approximately ten orders
of magnitude now in many areas of
computer science if you do that you're
done you just walk away no more work to
be done but unfortunately we were still
left at about thirty seventh centuries
of verification time so still plenty of
room for improvement Mike wall fishes
group has recently started working on a
improving the PCP based line of work so
taking that original system and reducing
compared to the crypto system by six
orders of magnitude and by the original
system by over 16 orders of magnitude
furthermore you can take this and
improve it even further by batching so
if you're doing a whole lot of
computations all at once you can reduce
this verification load below the point
at which it's to the point where it's
cheaper than doing it locally so you can
actually win here unfortunately with
these early systems you often required a
large bats eyes so you need hundreds of
thousands or millions of instances to
make this worthwhile fortunately as
Michael tell you during his talk they've
continued to work along these lines and
reduce the significantly to the point
where it actually makes sense in a cloud
computing setting and then our work we
continue to work on the cryptographic
side of things and reduce another seven
orders of magnitude on the individual
instance verification to the point where
we're actually beating local local
computation for individual verifications
and I think it's the most exciting part
here is not the individual bars or the
milestones along the way but the fact
that overall the cost in this area
fallen by over twenty three orders of
magnitude in the last six years and
we've seen similar improvements on the
worker side as well where the cost
initially were just exorbitant you could
never even produce the proof in the
first place to the point where it's
starting to look much more reasonable
still costly so it's going to cost you
more to generate this proof but if
you're in a cloud computing regime where
you might actually have extra resources
sitting around then maybe it's not too
crazy to spend extra resources to
provide this extra guarantee to your
customers and again here we've seen 18
orders of magnitude fall and in fact
we've also started combining some of our
efforts so Mike and I recently
collaborated on a paper where we were
able to reduce the overhead on the
approver even more while still staying
in the batching regime for the
verification so talk about these systems
a little bit more qualitatively the
early systems were based on something
called fully homomorphic encryption
which is a very recent innovation and
has the nice property of providing
privacy of all of your all the
individual inputs and the computations
that you're working on unfortunately the
cost of that encryption is so high that
those systems just aren't practical yet
the PCP line of work doesn't provide
privacy but can achieve very good
efficiency when you're when you can
batch lots of computations together and
also is nice in general purpose and
makes the simplest cryptographic
assumptions of all the current systems
out there the interactive proof system
that Professor miss mocker will tell you
about has even better practical
performance but gives up on generality
for some computations though recently
they've started increasing the class of
general computations that can handle but
for things like matrix multiplication we
can't even show that on the graph that I
just showed you because they've gotten
the performance so good for for those
specialized computations we also did
some work last year on again looking at
other cryptographic techniques we might
use using something called a b e or
attribute based encryption which is much
more efficient than fhe but and gives
you nice properties like security
against quantum cryptography or quantum
computers and allows you to do public
verification but unfortunately the the
parameters are such that they again not
that practical yet and finally in some
recent work we were able to achieve
practical efficiency using cryptographic
techniques which gives us this public
verification property and allows us to
do other fancy cryptography
cryptographic techniques like zero
knowledge proofs and so just do step
back for a moment say well well really
how do we formalize this notion of
verifiable computation you can think of
it in terms of three algorithms that
we're going to run the first algorithm
is a key generation algorithm which
takes in a function you can think of as
the program that describes what it is
you want to outsource to the cloud and
it generates some key material so in
this case say a public evaluation key
describing the computation and a public
verification key describing how you're
going to check the results once you've
done that you can outsource your inputs
to the cloud and the cloud can do the
computation as it normally would but
this time it's going to run a second
algorithm called proof that takes in
that evaluation key and that puts a
proof that it this particular output
corresponds to this particular input on
this computation
it can return that proof back to the
Alice and Alice can run the third
algorithm which is verify using that
verification key that she produced
earlier and that's going to tell her if
the cloud is cheated or if the result is
good and of course the key challenge
here is to make sure that checking work
is less than doing the computation
herself or there wasn't much point to
outsourcing in the first place now you
might want to play more so this is kind
of the basic setting but you might want
something more general for example you
might want what we think of as a
signature of computation and since that
any other party can come along and say
hey I have a computation I want to do as
well and he can receive a proof he can
check that proof using the verifiable
using the verify algorithm and he can
decide whether he got a good result
furthermore he can show this proof to
Alice and she can independently verify
that result as well so she doesn't have
to worry about the fact that Bob had
access to this key possibly allowing him
to cheat her so it really behaves like a
signature for a message but now we're
signing entire computations so in the
rest of the talk I'm going to focus on
my work on the cryptographic side of
things and then later we'll hear about
the various techniques that my
co-panelists of developed for avoiding
cryptography or at least minimizing the
math cryptography you have to use and so
my portion of the work was joint work
with some folks here at Microsoft as
well as some folks at IBM research so
the main thing that we contributed here
was to develop a new protocol for
general purpose publicly verifiable
computation and also happens to support
worker called zero knowledge arguments
and I'll get into a little bit more
detail what that means in a bit and we
were able to do this by coming up with a
new encoding that we call quadratic
programs that is a new encoding of
computations that really lends itself to
this type of cryptographic application
and the nice thing is that it gives us
extremely good asymptotics so the amount
of work to set up that key in the first
place is proportional linearly to the
amount of work you would have done to do
a single computation in the first place
the worker does quasi-linear work and
what he would have done any way to
evaluate that computation the
verification depends only on the size of
the inputs and the outputs and not on
the computation at all and the signature
is constant size it depends only on the
security parameter no matter how big the
inputs no matter how complicated the
computation
that you can think of n as the size of
the original computation so whenever a
cryptographer shows you a bunch of
asymptotic like this you should always
be a little suspicious about what's
happening underneath the covers for
example the early work one fhe had
really good asymptotics in fact better
asymptotic from this and yet it's
completely impractical run in the real
world fortunately in terms of concrete
performance we did pretty well on this
front as well so we improved on the
verification by 60 to 100 times we
improved on the verification by seven
orders of magnitude and that signature
really is small it's about 280 eight
bytes about what you'd expect for an RSA
signature and so this gave us one of the
first verification systems where we
could actually beats native see
execution on a per instance basis and so
in the rest of the talk I'll tell you a
little bit about how we are able to do
this by walking through the pipeline of
computation so we start from a program
written in a subset of the C language we
compiled that to an intermediate
language it's based on a circuit
representation we compile those circuits
into what we call a quadratic program
and then we compiled that program into
those cryptographic protocols that I
mentioned earlier so in the first step
we need to do this compilation of C and
to do that we wrote a compiler that
understands a large subset of C so it
knows about variables function calls it
knows about type defs all kinds of
compiler pre-processing languages and
essentially is going to take all that
and turn it into one giant expression so
it's going to unroll all the loops it's
going to inline all the functions and
turn that into a giant expression
similar to the secure multi-party
computation literature if you're
familiar with that work and so at the
end of the day we're going to convert
that into an arithmetic circuit so you
can think of that similar to a normal
circuit but rather than taking bits on
the wires it's going to take large
values say integers mod some large prime
P and it's going to do basic operations
that consist of adding and multiplying
those numbers modulo that prime we also
developed some custom gates that allow
you to test for things like have I do I
have a value of equals 0 which can be
useful for doing loops and we have a
gate that splits a value into its
constituent bits so that allows you to
do sort of bitwise operations like XOR
bitwise and bitwise or is that brought
into the space of computations that we
can cover so once we've represented the
computation as the circuit we then need
to convert it into a form that's more
amenable to cryptographic verification
and so that's going to be these
quadratic programs
so you can think of the quadratic
program as an efficient encoding of the
original computation and when I say
efficient what I mean is that we can
take any original circuit an arithmetic
circuit in this case convert it into a
equivalently size quadratic program that
computes the same function and so what
that means is that we can take any
polynomial time computation or even any
in peak computation that can be verified
in polynomial time and verify that using
quadratic program and we have a similar
theorem that takes boolean circuits and
converts it into a related idea which is
called quadratic span programs when this
talk I'll focus on quadric arithmetic
programs to try and give you at least
some flavor as to how this might work in
particular if you look at this example
circuit we have here it's a very basic
circuit takes four inputs produces one
output and you can think of it as sort
of combining a series of very small
equations for each of those
multiplication gates so particularly
that first multiplication gate says that
C 3 times C 4 should be equal to C 5
similarly the next multiplication gate
says that if you add C 1 plus C 2
multiplied by C five you should get c 6
and so on for the entire circuit and so
at a high level we're going to construct
a pair of polynomials D&amp;amp;P that are going
to encode these gate relationships in a
particular fashion such that if you
evaluate the circuit correctly and wind
up with a correct set of C values then
the D polynomial will divide the pea
polynomial so what does that mean well
it just means that there exists some
polynomial H such that we can multiply
H&amp;amp;D and get back to pee or to put it yet
another way it means that anywhere that
D becomes zero it must be the case that
P becomes zero and so by constructing
these polym is cleverly will ensure that
this condition only holds if you do the
circuit evaluation correctly and then
inside of the cryptographic protocol
what we're going to do is check that
divisibility property at a random point
and with high probability if you've done
it correctly and the divisibility holds
then the divisibility will hold at that
random point and if you've done it
incorrectly then the visibility will not
hold so let's take one step deeper and
look at how we actually do this
construction so again looking at this
example circuit we're going to pick a
route value so an arbitrary value from
the field to represent each of these
multiplication gates so in this case
there's to multiplication gates so we
end up with r5 and r6 we're going to
define the divisibility polynomial d to
simply be the polymer becomes 0 at both
of those points so Z
srr 5z minus r 6 and then we're going to
find the p polynomial that needs to be
divided by D in terms of three sets of
polynomials VW and y and in particular
we're going to create AV polynomial for
each of the original inputs and each of
the outputs each of the outputs of a
multiplication gate in the circuit so we
wind up with four for the inputs and two
for the multiplication gates and we're
going to define these polynomials in
terms of the values they take on at
these root values that we chose
initially so r5 and r6 and we're going
to do so based on the left inputs to all
the multiplication gates in the circuit
so for example we look at the first
multiplication gate and we see that it's
left input is c3 so what that means is
we're going to put a 14 v 3 and 0 for
everything else because nothing else is
a left input to this particular gate
then we're going to look at the left
input to the second multiplication gate
here we see that two inputs c1 and c2
both feed into the left input of this
gate and so we're going to set v1 and v2
to be 1 and everything else to be zero
fine we're going to do the same thing
for the w's but the w's are going to
represent right inputs to these
multiplication gates so we look at the
first gate we see that c4 is the only
one feeding into the right hand side and
so w4 is the only one of those takes on
a 1 everything else is 0 and similarly
for the second one c5 is the only one
going in so w5 is the only one that's 21
and finally as you may have guessed the
Y's are going to represent outputs so
the first one we look at the output it's
obviously just one c5w or y5 gets one
and the second multiplication c6 is a
output so again why six is the one
getting a one okay so if you've stuck
with me this far and now I need to
convince you why this makes sense why is
this useful well remember we're going to
define or in particular we're going to
define this polynomial to be the sum of
all of the wire inputs applied to the
v's times all the wire input supply to
the w's and we're going to subtract off
the wise which looks like kind of a
hairy equation but remember our
condition here is that we need D to
divide P and in particular that means
any time d becomes 0 we need P to become
0 and we define D to be 0 at both r5 and
r6 so that means we need to look at what
happens to p @ r 5 well if we look at
the first term we see that all the v's
are going to be 0 at r 5 except for
v3 which is going to become one so that
entire summation over the v's is going
to boil down to just be the term c 3
similarly for the w's we see that all of
them are zero except at w 4 and so that
entire summation is going to boil down
to being just c4 and similarly for the
wise everything is going to go to zero
except for y5 and so we're going to wind
up with c5 and so what does it mean for
P to be 0 at r 5 it just means that C 4
times crc32 x 24 has to equal c5 and
that's exactly the equation that that
first multiplication gate is committed
to encode and we see the same thing
happens at our six if we evaluate P at
our six we see that the v's boil down to
be just c1 plus c2 the w's boil down to
be c5 and the Y's boil down to be c6 and
so again for P to be 0 it means that c1
plus c2 times C 5 has to equal c6 and
again we've just checked that
multiplication gate so essentially by
checking this divisibility property
we're checking all those individual
equations in one fell swoop so now that
we've built up all these polynomials we
have to turn that into a cryptographic
protocol and in particular I'm not going
to go into much detail here but the
high-level intuition is that alice is
going to generate all those polynomials
she's going to pick a random point s and
then she's going to evaluate all those
polynomials at that secret point and
stick it up in the exponent so that it's
hard for the worker to see what what
values you chose and she's going to send
that over to the worker who's going to
evaluate the circuit just like he
normally would that's going to tell him
all those C values that he needs to know
he's going to apply those linear
combinations up in the exponent of the
group to compute those summations but he
won't actually know the values of the
underlying polynomials and finally he's
going to solve for that extra polynomial
H that proves that that divisibility
holds and he's going to stick that up in
the exponent as well so finally he's
going to end up with a few terms that
he's going to return back to the the
verifier and the verifier is essentially
going to do that multiplication up in
the exponent using a cryptographic tool
called a pairing that lets her get away
with that once and so at a very high
level we're just checking the visibility
and if that divisibility holds then the
verifier will know that the computation
was done correctly now we can take it
another step and add some some more
fanciness on it to make it zero
knowledge so why would you care about
zero knowledge well there's a movement
afoot to add smart meters to people's
houses so that the meter will send how
much electricity you're using back to
the electric pump
a very fine granularity so they can bill
you more during the day and less during
the evening or bill you more when other
people are using electricity and less
when other people aren't so this is
great from a resource allocation
standpoint but it might reveal some
information that you're not comfortable
sharing with the world and so we can use
zero-knowledge techniques to help
prevent this kind of scenario in
particular we can have the smart meter
output the results to Alice directly
rather than to the building company and
the billing company can provide the
building function directly to Alice
along with one of these evaluation keys
Allison can decide how much she owes by
evaluating this function on her inputs
and she can generate a proof that she's
done this evaluation correctly the zero
knowledge aspect is that she can then re
randomized that proof such as the wind
she sends a result back the billing
company can tell that she did the
computation correctly but learns nothing
about the individual inputs and so both
parties get what they want out of this
computation and so we're living in a
better world and the nice thing is I'm
not gonna go into how this works but it
only adds about less than a tenth of a
percent to the overall protocol overhead
and so it comes essentially for free on
top of our existing protocol so talk
very briefly about the implementation
we've implemented six or seven sample
applications things like matrix
multiplication or a lattice gas
simulator just say how gas molecules are
behaving we've implemented compiler that
takes that into circuits and another
compile it takes that into the
cryptographic protocols the protocols
themselves operate over B and elliptic
curves if you're interested in that they
provide about 128 bits of security the
entire system is single threaded but
Mike's group and my Michaels group have
both done some work on parallelization I
think our system would be equally
amenable to that and the source code for
this is available so if you want to try
it out you can play with it on your own
system the key takeaway for the
evaluation is to look at are we beating
native execution so you can see on the
x-axis how long it would take if you
compiled that C program directly and ran
it on your own computer versus on the
y-axis how much it cost to verify the
results to come back and so you can see
for three of our applications we're
actually for large enough parameters so
large enough matrices are running the
simulation long enough we're starting to
cross that threshold where you're
actually beating local execution so
you're better off verifying the cloud
compute
than doing it yourself now it's a little
hard to see but there's three
applications that are kind of stuck
overland y-axis if we zoom in we can see
that they're all trending in the right
direction so if we push them far enough
we'd eventually cross that why that y
equals x line unfortunately these
applications don't translate as well
into the circuit representation and so
we pay more to compute them and so we
can't push the application parameters
quite as large on the laptop where we
were running these experiments so just
go into a little bit more detail for two
applications in this case evaluating
polynomial and simulating gas particles
when we translate that into a circuit
you get about 800,000 arithmetic circuit
or arithmetic gates that translates into
a few hundred thousand polynomials those
are used to generate the keys which
takes a minute or two which is not too
crazy the verification key which you
might put on your phone to verify the
results is typically about a kilobyte
the evaluation key which you would
upload to the cloud is larger a few
hundred megabytes but and the time to
generate the proof is reasonable but not
not as what low as we would like there's
still some fair bit of room for
improvement here fortunately when you
finish this computation you wind up with
a proof that's only two hundred eighty
eight bytes regardless of the
computation so the network overhead for
sending that back is very low and of
course the verification takes on the
order of 10 milliseconds which compares
quite favorably for native execution for
the polynomial evaluation not so
favorably for the gas stimulating like
we saw on the previous gasps of previous
graph and that's again because of the
translation to the circuit model so you
can see here that translate into circuit
takes about 20 times as long for the
circuit on the polynomial as it did for
the native but for the gas simulator
takes over 400 times as much that's just
because of the inefficiency of the
translation so in future work obviously
a big step that we're working on now is
ways in which to move beyond the circuit
model so we can avoid some of those
inefficiencies ways to do more efficient
computations over sine data to support
scenarios like the smart meter that we
talked about and of course additional
applications of this quadratic program
since it seems like a very nice encoding
and we think it can be applied to other
areas so just to quickly summarize my
portion of the talk we've developed this
encoding for computations this seems to
lend itself very well to both verifiable
computation zero-knowledge programs and
possibly even things like after based
encryption the we've built a tool called
Pinocchio that compile C programs all
the way into verify
computation protocols and that gave us
orders of magnitude of improvement and
gave us one of the first systems where
we were beating native execution on a
per instance basis so with that I'd like
to thank you for all of your attention
and we have time for one or two
questions and then we'll save all the
rest of the questions for for the end
when you can have that all three of us
so any quick questions yes no no the
moment where we aren't counting that in
part because the approver cost is so
expensive so in theory you can bring
that down in fact both of these
gentlemen have shown that you can bring
that down by paralyzing the effort so
that improves latency so in theory you
could you know if you through enough
machines at it you could bring the
latency down significantly and then
maybe it would make sense to count the
total time at the moment though the well
first you're still paying for all that
computation even if you're throwing a
lot of machines out it and second you
still have the issue of uploading it to
the cloud and then getting the results
back so I think at the moment that's
best suited for things where it's just
computationally or maybe power
inefficient to run on your local device
and so it still makes sense to do this
outsourcing even though you might have
to spend longer waiting for the result
to come back
wouldn't you better than if i use all
the schemes and how would you compare
say can do it what's the occasion yes so
you're asking could we take advantage of
the improvements in in-home or
encryption to to do better for some of
these particular applications it's a
really good question I think it would
still be more efficient to use one of
the the existing verifiable computation
systems than to use that just because
you'd be paying a lot for that extra
hiding now for applications where you
care about the privacy then sure you
should definitely look into that but I
still suspect that fundamentally any
time you're adding that privacy layer
you're going to be losing something on
the efficiency I think we should
probably cut it off there and professor
miss mockery take over Thanks I'm
Michael mitza mocker I'm going to be
presenting really the work of my student
Justin thayler who is just about to turn
in his thesis and I should point out
will be on the job market next year is
we've already heard from the beginning
but just a quick review there are
certain goals for verifiable computation
you want to offload your stuff to the
cloud but have some sort of correctness
guarantee when it comes back maybe this
isn't because you assume the cloud is
malicious but there could be bugs or
other sorts of smaller problems so you
want something that will work against
malicious clouds but also lightweight
enough for benign settings you want to
make things hopefully faster for the
verifier but also minimize the effort
required for the cloud to produce this
correctness guarantee one of the ways
that our work is a bit different than
the other pieces of work is that we want
ours to work or ours was designed to
work in a streaming setting so ideally
the user won't even have to maintain a
local copy of the data and I'll discuss
more about the street streaming model as
I go okay so as Brian pointed out we
start from the realm of interactive
proofs right where you have you know you
on your little laptop and a much bigger
cloud provider who can do the work for
you so you send over your data to the
cloud provider and again in our model
you may not even have to keep the data
after you send it you may just keep some
fingerprint of it as I'll describe later
and in the interactive proof model then
after they do the computation you sort
of get challenge them and say hey okay
well if you did the computation
correctly you should be able to answer
this question and you get back an answer
and you know after the question answer
you send these sort of challenges
responses can you know challenge
response challenge response and this may
go over some number of rounds you might
have sort of a back-and-forth
conversation but eventually you become
convinced that they did the computation
correctly and you'll decide to hopefully
accept because they've answered your
challenges in the way that you would
have expected okay so this is a
conversation based approach although in
some cases you can develop one round
versions where it doesn't even have to
be a conversation for certain problems
okay all right so modeling this in the
the theoretical way we talk about this
we have a prove or P and a verifier V
the prover solves the problem and then
just tells you the answer you have a
conversation and the goal for the
provers to convince the verifier that
the answer is correct the sort of
properties we want in the theoretical
sense or completeness that if you have
an honest prover it will always convince
to accept and soundness that you know a
verifier will catch a lying prove ER
with high probability right now
interactive proofs have been around for
a long time they're nothing new to
complexity theory we didn't invent
interactive proofs by any means they've
been part of some of the great seminal
results in theoretical computer science
from I p equals P space zero knowledge
proofs PCP theorems but they've actually
up to this point have little impact in
the sort of delegation of computation
scenarios and our claim was that you
know we went into this thinking well why
is this the case how can we make these
more practical because certainly there
seemed to be applications and if you
look at the original theoretical work it
was really trying to deal with the
situation where you had a set of hard
problems like I was saying oh well so
you had a prover that could solve NP
complete problems or sharp peak complete
problems and of course we don't actually
have proved errs that do that and in
practical scenarios that may not be the
real situation where we actually want to
offload this sort of computation but in
2008 there was a paper a key paper that
we rely on quite a bit for our work by
Goldwasser kalai and roth bloom labeled
interactive proofs for muggles and had
some of the harry potter pictures you're
seeing and the goal here was to allow
the verifier to try and run quickly so
that you could outsource for useful easy
type problems not necessarily
np-complete type problems and also that
the prover did not need a whole lot more
time to prove correctness than to
actually solve the problem now why is it
that this result didn't immediately
yield an out-of-the-box solution well
it's because you know to a theorist not
a whole lot of extra time for the
original paper meant a cubic blow up and
essentially the computation time the
size of the circuit which was a bit
difficult and also in the original
version the verifier had to have the
full input so one of the ways that we've
been pushing on this is to actually say
ok you don't actually need to keep the
whole input we can have this work in the
streaming setting as well so the idea is
that the data itself you're working with
a big data set that's why you're
offloading the computation you don't
actually have to store the data you have
to see the data so that you can keep
some sort of fingerprint do some sort of
processing that you'll use later in
these challenge response scenarios but
you don't actually have to keep the data
itself okay so you can look at the data
in a one-pass dream matter manner our
claim is that this is a very useful sort
of property to have in the context of
dealing with these big data computations
that you want to offload to the cloud ok
so again just a reminder many of you
will have seen this before in the data
stream model we think of having element
swim by from some sort of universe and
you see them sort of one at a time and
your goal as you see them as you can
you know well at the end of they are
trying to compute some sort of function
on this stream but the actual verifier
isn't necessarily going to keep the
whole stream they're going to work with
an amount of memory that sub linear in
the actual size of the stream and the
size of the universe and their go it's
going to see the data as it comes to
them and have to deal with it and then
move on okay and the results that we
give actually work in this setting we
have results for again for different
settings for different sorts of problems
a model where you have one message sort
of the non-interactive the proofer just
sends the verifier say an email with the
answer and a proof attach the cost of
having less interaction of sending it
this way is that you actually of course
have to have a sort of a larger bit of
data to send what I'll be talking about
more in this talk is the case where we
actually do the multiple rounds of
interaction and you have that
conversation some of the costs that
should be considered or our main cost
that we consider is going to be the
amount of communication that actually
takes place in the amount of memory that
V needs to keep to as seeing the data
stream there are other important cost to
such as the actual running times and the
number of messages that I'll be talking
about so again this is justin's thesis
work he's been working on this with
other people myself included for a long
period of time and there's sort of two
prongs i'll be talking more about the
first prong or the first prong is really
what we're talking about here in in in
this panel which is to do general
purpose implementation you're trying to
verify you know essentially arbitrary
computations and there we're going to
build on that GK our protocol I talked
about but he's also done a lot of work
on developing highly optimized protocols
for very specific problems and the goal
here would be that if we could develop a
rich enough class of building blocks
with very efficient implementations then
you may want to you know cast your
computation into those sorts of
computations and by working with very
specific computations you can
potentially get much much more efficient
sorts of schemes
then working in the general purpose
model so in particular I'm going to look
at sort of a specific example of one of
these sorts of second prong type
approaches just to give an idea because
it sets up a lot of the technology that
we actually use in both settings okay as
Brian talked about one of the key ideas
is this new student of arithmetic right
so instead of thinking of things as
being over just the 01 binary alphabet
you sort of work with you know in you
arithmetica dition and multiplication
and one way to think of that is that
you're going to be working you know now
with polynomials again sort of a way
that Brian described and you can work
with them over a larger field you don't
have to be thinking in terms of just
zeros and ones you can be thinking of
numbers even though you know the actual
inputs to your circuit are going to be
zeros and ones and a trick here is when
you're dealing with this sort of way of
thinking about for polynomials is to
view what we call low degree extension
right so you look at this polynomial you
say it's defined over field you know how
it has to behave in binary but you can
use the fact that you're working over a
larger space of numbers all right so
here's an example problem that lets you
see the power of this sorts of technique
okay so the second frequently frequency
moment says you're going to see seeing
letters and an alphabet go by in a
stream what you want to do is keep track
of you know the squares of the
frequencies of these items okay so you
can see this sort of example here at the
bottom a be a CBA you've seen a three
times B twice G once so the second
moment here would be 14 because it's 3
squared plus 2 squared plus 1 squared
there's associated frequency vector and
you can think of the frequency vector
sort of being the the representation
that we're going to end up using after
the stream goes by okay so what we have
for this scheme is something that uses
square root of n amount of communication
and square root of n space so even if
you have terabytes worth of data going
by you've turned this down into a few
megabytes of speed
in communication and this is you know
essentially tight there's a lower bound
on the product of the communication and
space required so what's the sort of
trick that we use for this okay we're
trying to come up with the sum of the
squares for this frequency vector so we
can think of this frequency vector we'll
split it up into the surf frequency
square right if they're n elements we're
trying to keep track of we'll break it
up into a square root of n by square
root of n square all right and then the
prover is going to do all these
calculations to figure out the second
frequency moment and we'll just send the
answer in pieces instead of sending us a
single number it'll send us square root
of n different numbers okay one for each
row in this square and what the verifier
can do to try and check that the
calculation is being done correctly is
pick a row at random and check that the
provers lie okay and if the prover is
lying then there's a chance of being
caught in fact a 1 over square root of n
chance of being caught so the verifier
only requires subway near Mount to space
square root of n amount of space and a
chance of probability of 1 over square
root of n well that's not a very good
probability of catching that a problem
has occurred so we need to be able to
beef that up so what we'd like is the
setting that says we'll wait even if the
prover lies even about just one thing
going on they'll have to lie about many
of the pieces going on and the idea is
by phrasing this in terms of these low
degree extensions we can essentially
build an aircraft in code on top of this
actual input okay right so we think of
these values as being on a polynomial
and the point is is that we have this
frequency square and now we the verifier
can pick a random row and the point is
is that if the prover tries to lie the
provers going to have to lie on most of
the actual things and so we'll be able
to use this to catch things catch these
sorts of problems with high probability
right so here's just some sort of
experimental results with one round vs
multi round essentially what we see is
that you know
there was room for improvement even the
one round scheme using efficient fast
Fourier transforms and the multi round
schemes are even more efficient they all
sort of are linear in this log log scale
but we're getting to actual efficient
times with very large runnings very
large problems all right so now let me
say now you have this sort of idea how
do we think of this for general-purpose
IPS okay so now the point is is that so
we Brian pointed out we're working over
circuits right this is what an f2
circuit would look like we have the
frequency vectors input we sort of
square each of these values and then sum
them up ok so the prover starts by
saying hey here's the output here's the
answer and the verifier makes some sort
of challenge and the point of this
challenge the way you set up this
challenge is the prover has to respond
with something that tells you about
information about the circuit at the
next level down and then you can
continue this process recursively the
challenges go back and forth and the way
you can think of each challenge response
is saying ok you've committed to
something about this level of the
circuit I'm going to force you now to
commit to something about the next level
down in the circuit and eventually the
prover is going to get down to the
inputs and the inputs is what the
verifier has actually seen in dealt with
right so the verifier by performing
proper sort of computations fingerprints
on the input can check the prover
statement and I should point out that
these fingerprints are not computation
dependent it doesn't depend on which
actual circuit you're trying to verify
you can develop these fingerprints just
from the beginning and then later decide
what sort of computation you want to do
on the data stream right so this saves
the verifier significant time and space
in particular you know even when
multiplying 512 by 512 matrices we're
already much faster than the verifier
doing itself ok and this will only get
better as we deal with large input sizes
and we've done this in a way that
minimizes the provers overhead
right we the theoretically we brought
the provers runtime down from cubic in
the size of the circuit s log s whereas
as the circuit size by doing and we've
done a lot of additional engineering
things like picking the right finite
fields to work for using the right kind
of circuits and so on okay I'll show
some more results further in fact Justin
just recently and this is work that will
be appearing in crypto shortly I was
able to improve things for regular
circuits get things down to linear time
and you might think that the log s
factor isn't so big but we're talking
about circuits right so these circuits
are potentially millions of gates so a
log s factor actually is orders of
magnitude and in fact Justin's been able
to get programs that are less than ten
times slower than a C++ program for the
prover right it's obviously going to be
faster for the verifier but the point is
is that worth in a striking distance
within an order of magnitude to actually
develop the proof then that takes the
time for the prover to actually solve
the problem right so here are some
sample applications distinct the number
of distinct things matrix multiplication
may be the key thing to note is the gap
between the prover time p time and the
circuit evaluation time again you can
see there within an order of magnitude
of each other so you're not necessarily
paying a whole lot even on the prover
side to get this sort of verification
like in two more minutes Justin's also
expanded this work to deal with
irregular circuits by using the idea of
data parallelism and sub computations
the idea is if you have repeated sub
computations you can actually set it up
that you essentially just have to deal
with that circuit once so that really
the cost is from the size of the sub
computation not from the size of the
total circuits itself and we've also
done as Brian pointed out a great deal
of work taking advantage of parallelism
and in particular in this area showing
that we can deal with these sorts of
parallel data parallel type computation
specifically one of the last things in
this paper that's just coming out as
justin has specifically
very good result for matrix
multiplication again looking back he was
able to take this general work use it to
look back at a specific problem and come
up with a new matrix multiplication
protocol that takes essentially no
additional time and space over just
regular matrix multiplication that it
sort of ends up being in lower order
terms and we think this will be a great
primitive for other sorts of routines in
this space all right so future
directions we'd really like to avoid the
circuit model which is what we use and
what we've depended on for this work
we'd like this to work for more general
classes of C programs there's certainly
more room to continue pushing the speed
of and functionality of these sorts of
systems another sort of way of thinking
about this is we've been talking about
this in terms of big data and big data
cloud problems but another way of
thinking about it is actually going to
the very small right so you think you
have your computer but you might want to
put in some sort of high-speed
attachable device maybe you have a GPU
that you just plug in that handle
certain specific computations and you'd
like it to not only do that computation
but to prove to your main home trusted
machine that it's actually done it
correctly right that when you bought
this special piece of hardware that it's
actually doing the job that you decided
to offload to it and so these are some
of the things that we're looking at in
terms of future directions so thank you
I think we have time for one quick
question well Mike sets up
sorry can you repeat it I think so I
didn't get the question no I what we're
what we're saying is that the data is so
big I mean if you look at a lot of the
applications of the cloud you have these
like giant astronomy datasets or giant
you know retail transaction data sets or
stock data sets where the point is is
that these things are so big that you're
not actually storing them say on your
laptop that you're offloading the data
to the cloud to begin with so there's
already some notion of trust in the
sense that you're trusting the cloud to
hold your data now you're extending that
notion of trust so you're saying okay
not only am I going to have you hold the
data but because it's so big it's not
even clear i can download it back to
myself to compute on it I'm going to
allow you to compute on the data as well
but I'd like to be able to trust not
only that you're holding the data
correctly but that you're doing the
computations correctly so it's not
saying that this is public information
or anything like that it's just saying
that would you we have the potential to
deal with data sets that are so large
that we're not easy holding the data
ourselves but we're offloading that
aspect is sort of a a priori and now
we're trying to extend that ok so can
save the rest of the questions for after
our final speaker okay can this thing on
Yeah right and can you hear me okay
awesome so thank you and Brian thanks
for the invitation to speak here I'm
going to be talking about some of the
work that my group has done in this area
over the last few years and also present
a framework for organizing this work and
some of the challenges this work is
collaborative with folks at UT and
outside I want to recognize my senior
students tree not steady who's driving
the project
my faculty colleague andrew blomberg
who's here raise your hand okay so
you've heard the problem set up in the
excellent talks of Bryan and Michael but
I just want to emphasize how exciting
all of this is from a systems
perspective when my collaborators and I
started working in this area no one had
implemented any of the theory and in
fact my systems colleagues thought that
I was crazy for spending time and
graduate students cycles on this since
then though there's been a lot of
activity resulting in a rich design
space of works that can properly be
labeled systems to organize this design
space I find the following framework
helpful it's organized around a
trade-off between performance and
expressiveness vertically we have
qualitative costs mainly for the
verifier horizontally we have the class
of computations that the system's apply
to higher means lower costs right word
means more expressive so higher into the
right is better I'm going to slot a
system in here if and only if it is
implemented with published experimental
results so the first two systems in the
area were CMT which you heard about in
Michaels talk and pepper in which my
collaborators and I refined an efficient
argument system a kind of interactive
protocol and pepper was in principle
general purpose but it only achieved
reasonable performance for a restricted
class of computations we followed pepper
with ginger which expanded the
applicability of pepper but the
situation there was similar as far as
performance reasonable performance for
restricted set of computations
ridiculous performance on the order that
Brian mentioned for computations outside
that set we followed ginger with zatar
which expanded the reasonable
performance to a much larger set of
computations xoch hard drives a lot of
its power from the beautiful quadratic
arithmetic programs formalism that Brian
mentioned of Gennaro Gentry parno and
Ray cova the next publication was
Pinocchio which also uses that same
formalism but in a way that achieves
more cryptographic properties in
exchange for higher expense we were able
to fill in the gap in the frontier line
between CMT and zatar with our work on
allspice which achieved very good
performance but at the cost of an
inexpensive setup phase following
allspice Justin thayler borrowed some of
its techniques introduced a raft of
other
and produce that excellent performance
that Michael described this is the best
performing system the literature
although its applicability of course is
restricted at this point none of the
depicted systems can handle computations
that have side effects they can't handle
storage they can't handle state we have
been able to remove that restriction and
work that will be appearing at SOS p in
a few months pantry allow extends the
verification machinery to handle things
like verifying queries on remote
databases verifying MapReduce
computations verifying computations that
use indirect memory references prior to
pantry pointer values had to be known at
compile time speaking of compilers all
of the shaded systems with
compilers Brian gave an excellent
example of what those pipelines
generally look like in these systems one
important axis that the framework here
doesn't captures cryptographic
properties the systems in the bottom row
achieve things like zero knowledge non
interactivity meaning that their setup
costs can be amortized indefinitely I
should say that the way that pantry
achieves these cryptographic properties
is by leveraging Pinocchio which it can
do because pantry applies to a number of
the systems in the literature to put all
of this in context Brian talked about
Pinocchio Michael talked about the
systems in the upper left the rest of my
talk will be in two parts in the first
part I'll do a quick performance study
of the shaded systems so you can see
what the issues are in the area I'm at
we handle integrity not availability in
this research area and then in the
second part of the talk I'll describe
our work on pantry which I'm very
excited about which concerns extending
the verification machinery to real
computations that people use the cloud
for now we're doing this performance
study there's three questions we need to
ask the first is what is the verifiers
per instance that is per input output
pair cost of doing a verification and
how does that compare to simply
executing the computation locally second
what is the verifiers costs or what are
the verifiers costs from participating
in the protocol in the first place and
third what are the workers overheads and
in doing this performance that I have a
few ground rules the first is that I'm
going to
the system if and only if it's
implemented with published experimental
results the second is that the data here
is going to be from all of our
re-implementation of all of the systems
that appear I insist that my graduate
students recover performance as good as
or better than what's reported in the
original papers of course going to run
these experiments on the same systems
turns out there's not a lot of
experimental variations so I'll just
report the averages from three runs I
should say that for a few of the systems
namely those whose experiments don't
terminate in our lifetimes we're going
to extrapolate from detailed micro
benchmarks we're going to measure two
classes of systems the general-purpose
ones and the special purpose ones and I
should say a little bit about the
general-purpose ones Brian gave an
excellent presentation of Pinocchio the
line of works from my group pepper
ginger zatar is best explained by
comparison with Pinocchio so in
Pinocchio well let me just say
notationally what the superscripts
represent are different instances of the
same computation different input-output
pairs and the rectangles didn't note the
types of proofs that the worker sends to
the verifier to convince the verifier
but the worker computed correctly now in
the world of Pinocchio the verifier can
get back a verification or a proof after
a given output is supplied by the worker
to the verifier Pinocchio is a setup
costs that's per computation and then
that setup cost is amortized
indefinitely over multiple instances of
the same computation in pepper ginger
and zatar by contrast verification can
only happen that's what these stacked
rectangles denote at the end of a batch
and as those costs are amortized only
within the batch so qualitatively
speaking Pinocchio's amortization
behavior is superior of course we have
to ask about the qualitative and
relative costs of these two things so
returning to the setup here I'm going to
look at two benchmarks matrix
multiplication and a clustering
algorithm and so the first question
concerns / instance verification costs
what we see here is that several of the
systems on a per instance basis I'm
ignoring setup costs for a second beat
native execution there's only two
problems though or there are there are
two problems with this
the first is that that 50 milliseconds
denoting the cost of local computation
is quite optimistic if you turn on the
optimization flags on your compiler it
turns out that native execution is
actually 5 milliseconds unfortunately
none of the systems are beating that
however optimistic baselines and I am as
guilty of this as anybody else are
endemic in our research area are so I'm
just going to imagine that the baseline
is 50 milliseconds the second bummer but
would you sort it you know you can
imagine that there's lots of page faults
or something but the second issue here
is that none of this actually takes
setup costs into account and the
question you have to answer in analyzing
setup costs is what's the crossover
point how many instances of a
computation have to be outsourced before
it's worthwhile to use the verification
machinery versus computing locally and
to analyze that I'm going to take each
of those Y values there and those are
going to be the slopes of the total cost
lines that are depicted here so the
slope of the black line is the cost of
computing locally the slopes of the red
lines are the y axis values that we saw
before but this graph actually has the y
intercept representing the setup costs
and what this graph is showing is that
some of the general-purpose systems
actually do have a crossover point
that's reasonable tens of thousands of
computations what this graph is not
showing is that Pinocchio's amortization
behavior is superior in that once
Pinocchio passes that 50,000 mark all of
the other instances of that same
computation are gravy whereas for zatar
all it's only the instances within the
batch that count as gravies otter then
has to incur that set-up cost on the y
intercept for each batch now what I'm
going to do is take each of those
crossover points on the x-axis put them
on the y-axis and compare them to the
special purpose approaches and this
graph is showing you two things first if
you're willing to use special purpose
approaches your crossover points are far
superior second the special purpose
approaches don't always apply but it is
nice that's something like allspice or
even CMT as Michael said one run of it
is better than executing natively the
third question we have to answer
concerns the workers costs in all of
these ski
and unfortunately what this graph is
showing is that the workers coughs or
the prover as michael called it is
unfortunately orders of magnitude 10 to
the 4 10 to the 5 etc more than
computing locally I don't have justins
most recent work on here because it came
out to recently but it looks like for
some computations he's going to be able
to do way better than this to quickly
summarize the performance in this area
we've come a long way none of the
systems as a true practicality the
biggest disaster of course is the
workers costs the verifier can get close
to practicality either if we're willing
to make assumptions about
special-purpose pneus or if there are
multiple instances of the same
computation to verify now you might ask
yourself when are there tens of
thousands of instances of a computation
to verify and when do we have an
abundance of CPU cycles for the worker
and the answer is cloud computing data
parallel cloud computations exactly
match this requirement for a crossover
point because they involve the same
computation on different inputs and with
that let me turn to the second part of
this talk which concerns broadening the
expressiveness of the verification
machinery in the context of our upcoming
work on pantry and to motivate pantry
let me just observe that prior to it
computations could not have side effects
all inputs had to travel from the
verifier to the worker all outputs had
to travel back from the worker to the
verifier now matrix multiplication is an
excellent example of this pattern and it
was an important test for our area but
if we want this area to truly succeed
we're going to have to handle real
computations that people use the cloud
for under pantry we can handle something
like verifying queries on remote
databases starting assuming that the
verifier starts with a digest of the
database or we can handle outsourcing
computations that work with indirect
memory references prior to pantry
pointer values had to be known at
compile time ruling out any notion of a
ram abstraction in the outsourced
computation or the verifier can
outsource a MapReduce computation
getting back and assurance that the
named output files truly contain the
data that is the correct data that would
be output by the MapReduce computation
so
how pantry does this I'm going to review
the pipeline that Brian mentioned that
these systems have it starts with a
high-level language compiles down into a
set of constraints on circuit execution
into a quadratic earth medic program
into binaries that implement the
verifier in the worker at which point
the worker can prove that it computed
correctly to the verifier the
differences in the system's ginger is
our Pinocchio and so forth concerned the
last stage of the pipeline how this qap
encoding turns into a proof what's in
the binaries that implement the verifier
in the worker but I'm going to focus on
the front end of the pipeline because
that's where pantries innovations are
and I'm going to explain this front end
slightly differently from what you heard
because it explains what's going on in
pantry so one way to understand the
front end of these pipelines is that
they take as input a high-level language
for example in a subset of C Pinocchio i
should say is the first to to work with
see in particular they compile them down
to a set of constraints on circuit
execution meaning a system of equations
with the distinguished input variable X
and a distinguished output variable Y
and that system of equations is set up
in such a way that when that input
variable X takes on the value foo the
system of equations admits a solution if
and only if the output variable Y is
equal to bar where bars the correct
output of the computation applied to foo
as an example the decrement by three
computation is in the upper left of the
slide that's equivalent to the
constraints in the upper right because
for example if we take X and set it
equal to seven then the output variable
sorry the system of equations is
consistent if and only if the output
variable is set to four which is exactly
what we want since four is the correct
output of this computation when its
input is seven now I should say
returning to the pipeline then it
decomposes into two phases in the first
phase of the pipeline a high level
computation gets compiled into a set of
constraints in such a way that correct
program execution is tantamount to
satisfy ability of the constraints when
the distinguished input variable X is
bound to the verifiers desired input and
the distinguished output variable is
bound
the purported output Y in the second
stage of the pipeline the worker proves
to the verifier that indeed it knows a
satisfying assignment how this is done
is way outside the scope of this talk
the question I want to answer is how can
we leverage the second stage of this
pipeline and so our design question
becomes how can we encode the
interaction with storage and constraints
in such a way that correct interaction
with storage is implied by the
satisfiability of the constraints and
I'm going to answer that question by
telling you about a very bad idea an
idea that we're not going to follow so
the bad idea would be to have one
constraint variable for each cell of
memory we could then faithfully
represent interactions with ram or
storage or state like a load operation
would be equal to those constraints that
you see on the right the problem is for
every single load operation we now need
to constraint variables and one
constraint which is a lot if you imagine
two to the 32 cells of memory so instead
we're going to turn to a different idea
and in order to explain that idea I need
to review the concept of self certifying
blocks from the world of untrusted
storage in that world imagine forget
about pantry for a second and imagine
that there's some storage server that a
client doesn't trust if the client
starts with digests that it's presumed
to have retrieved through trustworthy
means it sends those digests to the
untrusted storage server and then when
the block travels back to the client the
client can check that the blocks are
correct because the block is expected a
hash of it is expected to equal the
digest and then the client performs that
check self certifying blocks are
interesting for pantry for two reasons
first they remove the exponential
association between the possible values
in a memory store and the names of that
store given a set of data there's only
one possible name for it a collision
resistant hash of it and second existing
work has shown how to use self
certifying blocks as a substrate for
building higher level abstractions like
verifiable ram or untrusted file systems
note though that in this picture the
client is actually retrieving the blocks
and handling them what pantry will
do will be to represent the interaction
just the sorry pantry will represent
just the checks of the blocks themselves
in constraints without the verifier
needing to handle the blocks turns out
this can be done reasonably efficiently
and I should say that the high-level
idea here was known in theory that this
could be done pantries contribution
consists of demonstrating that in fact
it can be done and of being the first to
actually do it which we do by extending
the subset of C with two primitives V
get and V put in a correct execution of
the program v get which takes as input a
digest a hash of some data must return a
set of data such that the hash of that
data is the input to V get the situation
with Vee put is similar using this
primitive then we can implement a
computation in the subset of C extended
that expresses the notion of adding an
input value X to a value that's
referenced in directly using the V get
it compiles the thing on the Left
compiles to a set of constraints that
express the hash function and then the
key point is that the only way to
satisfy these constraints is if the
worker chooses a value Z such that the
hash of Z in fact equals the digest this
is equivalent to doing a verifiable load
at which point that value Z is available
to the rest of the computation and I
assert that the constraints on your
right are equal to the program on the
left putting the whole pipeline together
it looks like this recall that the
second phase of the pipeline proves from
worker to verify that a satisfying
assignment to the constraints is known
meanwhile the work of pantry is to take
the various cryptographic checks of this
hash function encapsulate them in V put
V get and turn them into constraints and
finally existing work not ours work with
the long history shows how correct
interaction with storage can be
represented as V put and V get and if
you compose those three things together
what you get is that if the verifier
gets convinced by the worker that it
knows a satisfying assignment then the
interaction by the worker with storage
is correct we're not out of the woods
yet because we'd like to build higher
level abstractions on top of V put and V
get things like ram or sir
trees existing work shows how to do that
so I'm not going to take up too much
time with it except to say that you
write your computation using actual
interactions with Ram that gets compiled
into a subset of C that uses V put and
Viet which gets compiled into
constraints I will take a minute to
explain how we use MapReduce or how
MapReduce works in this context so in
the world MapReduce is going to use V
put and V get as follows the verifier is
going to outsource a map function in a
reduced function and supply a set of
input digests to the MapReduce
infrastructure and what the verifier is
going to be given back or a set of
digests such that the data that is the
correct output of the MapReduce function
better have as its hash these digests
the cool thing about this is that the
verifier can then take those output
digests supply them to future MapReduce
computations or else use the digest to
verifiably retrieve the actual data
meanwhile the verifier in the actual
verification process never needs to
handle the data itself which is one of
the coolest things about this the way we
do this is going to take more time than
i have so i will just very have been
talking for 18 minutes so i am just
going to quickly show you a graph what
this graph is showing is that the
verifier breaks even in the sense of
saving CPU cycles from using this
framework again the verifier never needs
to handle the actual data pantry applies
pretty widely not only to MapReduce but
also to verifiable queries on remote
databases we can even have the state
that the workers holding be kept private
using and leveraging the machinery of
Pinocchio our code base for pantry works
without modification for both the sitar
and Pinocchio to quickly just sum up the
performance problems in the area we have
high setup costs for the general-purpose
systems but those can be amortized in
the cloud computing context there are
high verification costs if our baselines
are not optimistic unfortunately none of
the systems are beating native execution
but that's okay because saving CPU
cycles us in everything there's also
saving network costs and saving storage
costs and using something like pantry we
can in fact save storage and network
costs even if the CPU costs are a little
bit more expensive than we
like worker overhead is still a little
bit of a bummer in this area and the
computational model despite many
improvements is still a toy so to just
there's three takeaways I'd like you to
have from this the first is that one way
to organize the exciting work in this
area of this performance versus
expressiveness the second is that our
work on pantry extends the verification
machinery to real computations that
people use the cloud for and the third
is that there are a number of open
problems for all of the systems that
would be juicy research problems and
these juicy research problems actually
span Systems Theory parallel computing
and so forth so it's an exciting time
for the area and I'd like to conclude
after 20 minutes exactly with that all
right so while we start with a question
or two for Mike and then we'll open up
to all three of the panelists and try
and speed things along here so about
these batch operations is it possible
that multiple users share the same key
and use the same computation and if so
what will be like security implications
for those users so that's a great
question is in the batched model can we
have it be the case that multiple
clients could be submitting their
computations into the batch does that
work or what are the security
implications and the answer is if you
use what's known as the designated
verifier schemes which is pepper ginger
zatar or Pinocchio and designated
verifier mode then multiple clients can
submit into the batch but they all need
to trust each other they all have to be
part of the same trust domain if you
want the clients to be mutually
distrustful you want a designator yeah
well but public verify yeah then you
could use Pinocchio's public
verifiability and then they don't the
clients don't have to be mutually just
the clients don't have to be mutually
trusting but they have to believe that
the proving entity or the worker didn't
have access to the cryptographic
material and the private part of the key
yes they have to trust the key
generation process but not each other
more questions and since we're getting
close to lunch maybe we should just open
up to general questions I think Tom Tom
what you guys speak more about like
exactly a threat model and how the
nuances here and what you expect in
terms of deployment scenarios where
people are really going to be concerned
that outsource storage providers are
outsourced to competative providers are
deliciously modifying code and if
there's been thoughts about benign
failures in this type of brian said I I
I personally don't like the word
malicious because it kind of raises the
bar and when you'd want this
verifiability is about protecting
against bugs miss configuration I mean
lots of things can go wrong that I
wouldn't call malicious so if the
question is when does that happen I mean
I don't know when people use the cloud
today there's bugs there's buggy
hardware there's lots of situations in
which we might like a guarantee you know
if we take a robot and send it off into
outer space you might like to know that
it's computing properly without
necessarily believing that nASA has all
these malicious employees embedded in it
or that the Martians have hacked into
your Rover I think it's a question I've
talked with various csos who come to
visit Microsoft and very few of them are
worried about cloud surprisingly if you
are worried about cloud security in
general and those that aren't that
worried about integrity issues but I
think if I if you present it to them in
this framework they aren't that
interested if you present this to of
them as a way of checking to make sure
that the results coming back are correct
they're much more interested so they see
it more as a making sure everything
hasn't gone wrong or as an auditing tool
they make it use occasionally to check
just to say okay and general things look
good but I don't think they're worried
about Microsoft actively getting in
there and tampering with with the data I
think the only case where you might be
worried about that would be if in the
peer-to-peer setting right which was
what so started me off in this area was
thinking about things like steady at
home or folding at home work you just
have random people on the internet
contributing computation you know
nothing about them and you want to check
the results that come back wait right
and the same is talk about you could
imagine doing randomized checking so you
won't actually check every computation
but they'll use
the auditing tool so you may check you
know every hundred computations and just
to check if there seem to be errors
going on yeah so this is sort of a naive
question you mentioned that everything
that has happened so far has been four
straight line programs is that true what
happens with programs where there's a
lot of branching does the vocal learn
based on the branches that when this way
worse as the other is that kind of
primacy already taken care of in this
city so in general none of the
approaches we've discussed at least in
terms of implementation provide privacy
so the worker knows everything about the
computation I see and so for dealing
with loops and that kind of thing or
branches everything just gets unrolled
so you execute both sides of the FLC
execute the maximum number possible
iterations of the while loop so that's
where that's why all three of us you you
heard us say we want to move away from
the circuit model because that really
sucks for a lot of computations just as
a technical point the worker can
short-circuit but the actual key lengths
and setup costs are proportional to the
sum of the code in the branches as a
minor point thanks Brian so in your
answer to the previous question though
you said well what we've talked about
today doesn't these schemes don't
provide any privacy that is the worker
can see the computation but there has
been work some of the original work on
home orphic encryption was geared at
providing that kind of privacy so I'm
just curious have you guys or others
also been working on trying to make
those systems with the stronger privacy
guarantees more more practical uh-oh
specifically I personally have not i
mean my group and i have really been
focused on the question of integrity
since it seems like would be really cool
and it's required a lot of work to do I
think Brian might be the most natural
person to answer that being as he was
the P in the ggp paper that applied
fully homomorphic encryption to this
problem providing privacy
the main answer is there's a ton of work
going into making fully homomorphic
encryption more practical and there's
certain there was certainly a lot of
room to begin with and there's still a
lot of room for continued improvement so
I think at a high level vision it hasn't
put it this way the fhe schemes have not
improved enough that I am excited about
going off and implementing a scheme
based on fhe I think it's going to be
several more years if not a decade
before those become competitive now
there are alternate schemes like the one
that we built on attribute based
encryption it might provide some degree
of hiding but even those at the moment
at least if you want general-purpose
systems just are not competitive in one
of the things they think comes out is
that we've all approached this at
slightly different angles you can see
the systems have different properties
and fully homomorphic encryption it's
not quite clear when you get it to a
practical level what those systems will
look like and exactly what properties
that they'll have but my guess is that
it will prove to be another point in the
tradeoff space and so one of the ways
that or one of the things that's
exciting about having these multiple
groups working on it is we seem to be
hitting different points in the
trade-off space now there's some hope
that you know as we continue to progress
we'll get things that improve in all
dimensions or that combine the best of
the possible ideas of all these
different techniques but my guess is
still at the end of the day there's
going to be a multiple sets of
techniques that you could conceivably
use that are going to offer different
sorts of trade-offs in terms of you know
the speed versus the Express ability
versus yeah other sorts of things you
might look for in the system and so as
fully homolka encryption becomes more
practical they'll be interesting to see
what it gives you you know on this
trade-off space and what that what are
the values it provides I guess one final
note is that even though this has been
entirely a they talk about proofs based
on cryptography or inactive proofs there
are of course other techniques you can
use right there auditing there are good
hardware isn't team fault tolerance I
trusted hardware so if you're willing to
make the assumptions that fit
of those categories so if you're willing
to say okay Microsoft probably not going
to tamper with the hardware or you're
willing to say all right Microsoft and
Google both might try and screw me but
they might screw me in different ways
then then you might be able to do better
than some of the the techniques that
we've been presenting so our techniques
are really focused at the case where you
don't want to make any assumptions you
just want to outsource your date and
have a strong guarantee on the result so
I think we're running a little bit late
for lunch I don't want to keep you any
longer so thank you all for your
attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>