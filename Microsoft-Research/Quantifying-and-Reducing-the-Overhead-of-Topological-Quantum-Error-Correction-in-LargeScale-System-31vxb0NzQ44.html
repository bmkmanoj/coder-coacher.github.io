<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Quantifying and Reducing the Overhead of Topological Quantum Error Correction in Large-Scale System | Coder Coacher - Coaching Coders</title><meta content="Quantifying and Reducing the Overhead of Topological Quantum Error Correction in Large-Scale System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Quantifying and Reducing the Overhead of Topological Quantum Error Correction in Large-Scale System</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/31vxb0NzQ44" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon Martin chikara is a
postdoctoral scholar at UC Berkeley his
research interests in quantum
computation including quantum error
correction and quantum algorithms he has
been working on the development of new
quantum error correcting codes that have
a high error correction threshold and
can be efficiently decoded there's a
long bio he's done a lot of good stuff
and were thrilled to have him here
Martin thank you for the introduction
today I am going to talk about
quantifying the resources needed to do
real world computations on quantum
computer using a d topological and
concatenated error correcting codes
quantum error correction is of course
very important problem we cannot build
quantum computer without it and also
it's a challenging problem because it's
it's different than a classical error
correction where if you have classical
information we have just bits of zeros
and ones and the only area that can
occur is a bit flip but quantum
information is continuous and we have a
new range of partial errors but also
face flips and phase shifts that we need
to error correct the two main families
of quantum error correcting codes that
can address these errors are the
concatenated codes which were developed
starting with the work of peter shor in
1995 and topological error correcting
codes which started with the work of
electric-type in 1997 both of these code
families have some advantages and
disadvantages and these are summarized
in this table the topological codes have
much higher error correction threshold
which means they can tolerate much
higher error rates and also computation
lists the topological codes can be done
using local operations for concatenated
codes if we for example wants to do the
two qubit controlled not operation we
have to typically swap qubits before we
can perform a local control
not operation but for topological codes
we can use braiding which can be done
purely by using local operations and the
no swapping is needed but topological
codes they have to solve a difficult
problem to decode the errors typically a
minimum weight matching problem is used
to guess which error occurred given
syndrome information but for
concatenated codes the decoding is much
simpler so the classical controller that
controls these error correcting codes as
a bit more involved for topological
codes also um it is not clear which code
is better in terms of number of qubits
number of gates and internal resources
and this is the focus of this talk I'm
going to address the issue of
quantifying the resources needed to do
error correction with these two code
families and also I will look at some
ideas i have about simplifying the
decoding for the topological code so
this is the structure of the talk first
just some background about concatenated
and topological codes then I will lay
out the methodology that I used to
quantify the resources needed to do
error correction is the two code
families and finally I will speak about
building a faster decoder for decoding
errors in topological error correction
the stabilizer formalism is very useful
to characterize error correcting codes
the stabilizers are basically the
syndromes we need to measure to diagnose
errors they are generated by the
stabilizer group which the elements of
the stabilizer group / vice commute and
the stabilizers act trivially on the
goat space so if we have a state sigh
which is in the goat space then the
stabilizer doesn't change the state
which means we can safely measure these
syndromes without affecting the encoded
information and we can learn about
errors from this
syndrome measurements concatenated codes
perhaps the simplest example of a
concatenated code as the bacon short
code which is the quantum analogue of
the classical repetition code now we
encode each bits three times and then we
use a metra to vote to decide whether be
encoded 0 or 1 so to encode a logical
states 0 and the bacon sure code we need
to also protect against the the face
flip not own a bit foot but also face
flip so to encode 0 we use the three
plus state and to encode one we use 3
minus state initially and then to
protect these pluses and minuses against
bit flaps we use the repetition code so
0 would become triple zero and one would
become strip triple 1 so as the name
suggests concatenated codes have
concatenated structure so we can repeat
using smaller building blocks we can
build the code from ground up and up to
a certain limit this will improve the
error-correcting properties of the code
decoding of errors which concatenated
codes is very simple in case of the
bacon sure code it's basically just
simple majority voting as the
stabilizers what is often cited as
advantage of this concatenated codes is
transfer zone nature of many of the
gates so what does this mean if a gate
is transversal or operation is
transversal in the code then we simply
needs to apply the gate n times for each
of the N building blocks in the code so
for example the controlled not operation
is transversal so we have these two
blocks of 9 cubits each which which
represents two logical qubit and to
perform the controlled not operation we
need to do control notes between the
corresponding pairs of qubits but
actually from the resource estimation
perspective
even though this gate is transversal it
is fairly expensive because we cannot
generally do controlled not operations
on a physical quantum computer between
any pair of qubits they have to be
located at the same location that they
have to be neighboring qubits so that
means we have to use the slow operation
to move the qubits around and this is
this is one reason why using
concatenated coats is expensive and then
of course we also have gates that are
not transversal so it is possible to
show that we need at least some known
transversal gates to do universal
quantum computation there's the
concatenated codes so here's example of
the t gate which is nonsense rizal in
the bacon sure code so the gate uses an
ancillary state that needs to be
distilled this is the distillation
circuit which takes 15 copies of the
insulated state t plus and it distills a
single copy of the insulated state with
higher precision and this distillation
process needs to be repeated sufficient
number of times to achieve the target
fidelity of the ancillary state and then
the insulated state is used in this
circuit to apply the decayed so the
original states I was here here was our
insula and after the circuit is applied
the t gate is applied to the state side
which appears here
topological quantum error correcting
codes have a very different structure
they consist of qubits that live in a
regular grid in this picture this is the
surface code of dennis kita press kill
and Lundell the the qubits live at the
edges of the grid and the stabilizers
which are shown in this picture we have
to type two types of stabilizers the
stars here and the plug at and by
measuring these stabilizers we can
obtain error syndromes which gives us
information about the errors that
occurred there are other examples of of
topological quantum error correcting
codes this is the five square a coach
that I developed at IBM d had the each
code has a little bit different
properties one of the key advantages of
this code is that in order to
reconstruct the syndromes the only needs
to do two qubit measurements so the the
red object in this figure are the
stabilizers but they decompose into
smaller gauges and by measuring gauges
of weight too we can reconstruct the
syndrome information and this can be
beneficial in situations where wait for
entangling measurements are too
expensive to perform on certain quantum
technologies in addition to locality
another advantage of the topological
codes is the way they perform
computation computation can be done
vibrating control node operations can be
done by braiding so how does this work
exactly well the poly operators accent
logical action which equals e are
strings in the lattice so here's the
basic lattice which doesn't contain any
defects which means that the syndrome
measurements are enforced in the entire
surface of the lattice
and the flattest in this state and goes
to logical qubit and this picture shows
the logical x and z operator is
corresponding to these two logical qubit
now let Simon wants to encode more
Cupid's and we want to perform control
node operations in this system so what
do we have to do well it turns out that
if we puncture a hole in the surface we
will increase the number of logical
qubit that are encoded so first of all
what do I mean by puncturing a whole we
have to change the physical layout of
the system and the answer is no to
puncture a hole they simply stop
enforcing the measurements of the
stabilizers in the region where the hole
appears so this is the hole and we don't
measure any of the stabilizers in that
region and if we introduced to such
shows a pair of holes that will
represent one logical qubit and this is
easier to see because the logical x and
z operations will be strings that
connect the pairs of defect and strings
that loop around the holes now which
which string is X and which drink is Z
depends on the exact location of the
hole into lettuce because there are two
kinds of holes Reb holes and smooth
holes but regardless um to perform
controlled not operations in this system
we simply take one of the holes which
corresponds to the controlled qubit of
the cenote operation and we will move
the whole to the target qubit then we
braid the the hole around the whole of
the target and we move it back now this
works if the control is smooth and
target is a rough oh and there is a
simple conversion circuit that can
adjust the problem when we have lost
control and target qubit is represented
by pairs of smooth holes
so this is very convenient because this
operation can be done simply by changing
the location of the syndrome measurement
so there is no physical movement of
information needed to do the control to
not operations and how do we decode
errors that occur in these topological
codes well when we do our syndrome
measurements we will detect locations
with non-trivial syndromes which are
shown in red color in this figure so
these forests in Jones will measure it
as non-trivial syndromes and then it is
easy to see that these syndromes are
here at endpoints of strings of errors
so if we have a string of X errors then
at the end point of the string of errors
there is going to be a non-trivial
syndrome the same for forzieri there is
this star syndrome here and here which
detect that there is this string of
errors consisting of a single Z error
here so once we measure our syndromes we
can use minimum weight perfect matching
to guess how these syndromes should be
connected can use the add ones plus an
algorithm for example to do this and
then once we connect the pairs of
matching syndromes we will correct so
this is this is the placket syndrome
this is also placket syndrome so we know
that if these syndromes are matched we
need to do X Corrections bit flips on a
string connecting the the syndromes so
the red axis represent the correction
that is being done and the black x marks
they show the actual error that occurred
so in this case after we apply error
correction we just apply to this loop
operator consisting of bit flips on this
on this loop here but it turns out this
is this is fine because this this loop
is a loop that is in the stabilizer
group so it doesn't affect the included
state of this
stone now in reality this this matching
problem is actually a three-dimensional
problem because our syndrome measurement
itself will be faulted it needs to use a
quantum circuit to measure the syndromes
and that that's that's going to be prone
to errors so some of the syndromes are
going to show as faults even though
there was no error that occurred and
vice versa so to address this you can
use basically an analog of the 2d
problem by introducing the three
dimension and here in the third
dimension each of these lines represents
a single syndrome that is measured over
and over again and we will mark a red
point if the syndrome measurement
outcome is different than the
measurement outcome in the previous
round and this way we will obtain a set
of points in three dimensions that we
can match and now depending on the error
rate of measurement versus the error
rate in the memory we can adjust debates
of these edges in the temporal dimension
compared to the weight of the HS in the
space dimensions and if we solve the
minimum weight matching problem then
this high probability we will correct
the errors that occurred but this
problem is so this is a classical
problem that the classical decoder has
the self but it turns out if we have
millions of qubits and we need to repeat
the syndrome measurements certain number
of times this is going to be a large
problem that is going to be expensive on
a classical computer the surface code of
course exhibits threshold behavior so
there is certain threshold if the error
rate is below the threshold we can
correct the errors that occur at this
rate perfectly well as long as the code
distance is big enough so as long as we
have enough physical qubits encoded the
information you can correct the errors
if we exceed the threshold we cannot
recover the information so i did a
simple simulation that estimates the
threshold for various topological codes
I used C++ and I did Monte Carlo
simulation so I will inject a random
error into the system according to the
probability model that is described here
and then I will try to decode the error
corrected and I will record the frequent
service which I can correct the errors
of the given probability level after
student number of Monte Carlo
repetitions and this is the this is the
result that will come out of the
simulation tool so here I varied the
error rate with which i injected errors
into the system and on the y-axis is the
percentage of the time that i can
successfully recover from the error and
you can see that as i increase the
distance of the ghost as i increase the
number of physical qubits that angle the
information the curve in this picture
will get sharper and sharper which means
if I am just below the threshold the
percentage of failures is going to be
very small so almost always I will be
able to recover and if I'm above the
threshold then the percentage of
failures is large so I cannot recover
but if I choose any yes you mean as if
the ones that were not recover
personal chef Elias is the number of
Monte Carlo iterations that resulted in
failure so I could not recover the
original encoded state and you can see
that we need to choose code of
sufficient distance if he wants certain
let's say we want at least five percent
probability of succeeding and our
physical error rate is three percent
well then he needs to choose code
distance at least eight to guarantee
this performance so the the key
properties of topological codes it is
easy to increase in this decrease code
distance and that way influence the
reliability of the resulting code local
operations are sufficient to do control
node operations and basically
computation with the codes and they have
error correction threshold that is
significantly higher than threshold of
concatenated codes but the drawback is
that the classical processing for error
decoding is time-consuming so next I'm
going to move to the actual resource
estimation there's the topological and
concatenated codes first I'm going to
give a brief overview of the properties
of the quantum technologies that we
considered as part of the quantum
computer science project in which which
form database of this resource
estimation then i'm going to show you
the methodology that i used to quantify
the resources needed to perform
operations with the error-correction
codes and the numeric results so this is
the structure of the resource estimation
task we will consider a certain set of
quantum algorithms and for each of these
algorithms we will express the number of
logical cubits we need to perform the
task the number of logical gates to do
the computation
also information about parallelization
factor and the length of the two qubit
operations in the system for the quantum
technologies we will consider the gate
times and fidelity's and the memory
error rate on a cubist at sit idle in
the system and we will also consider the
properties of the four families of four
basic families of error correcting codes
they can show steen c4 c6 and the
surface code and all this information
feeds into our resource estimation tool
that produces a qubit layout and it
estimates the circuit delay gate count
and the fidelity of the entire
computation yes you see a number of
logical qubits for this number probably
eventually
yes so people choose a specific problem
of certain size and then we will express
this so right now we finished phase one
of the qcs project where the problem
sizes were hard coded but now we are
moving to Phase two and we are going to
parameterize to problem sizes so that we
can we can adjust a simple parameter
which describes the size of the problem
and that way we can obtain gate counts
for example let's say we want to run
quantum computation for one year what
size of problem can be solved so we can
solve the inverse problem if we
parameterize this is what we're working
on right now the worst the upper bounds
for a given instance of the problem are
we considering we forgiven size of an
incident she said
as a parameter is the size of being
present in itself
the premature would be this so forth
let's say we want to solve some gas
problem you want to lastly the triangle
finding problem you want to decide if
there is a triangle inside of of
estrogen graph then the perimeter would
be the number of vertices of the graph
so the of course the more vertices the
harder the problem is going to be in the
higher the gate count is going to be so
this is a collaborative project I have
been coordinating the break on the
resource estimate on behalf of the USC
team this is typo project the resource
estimation is currently being done by
four teams independently and I have a
number of collaborators and at number of
universities who all that great work to
analyze the properties of the algorithms
and quantum technologies in our project
you are studying for families of error
correcting codes bacon Shorstein c4 c6
and the surface codes we have seven
algorithms which all of these algorithms
have very different qualitative
properties they use different different
quantum primitives some of them use
quantum simulation some of them use
quantum random walk fast Fourier
transform so that's a very diverse group
of algorithms and we are considering
also six technologies and six quantum
control techniques that enable us to
reduce the errors in these technologies
and in this talk rather than presenting
the cross product of these results which
we have to obtain in the project I'm
just going to show the highlights of our
results
so for the quantum technologies our goal
is to obtain for a range of quantum
realistic quantum technologies to obtain
the gate times and gate errors to
perform physical quantum gates in the
system and in the qcs program we studied
the effects of quantum control protocols
on the gate errors basically which which
control techniques are effective at
reducing the gate errors in which are
not regarding our methodology we used
Monte Carlo simulations to insert random
noise and study the effective control
protocols on this noise we used
optimization tools to optimize choices
of control parameters and also we used
gate constructions because not all
quantum technologies support all
elementary quantum gates and even if
they do sometimes like smart
decomposition of the gates from a
different set of gates can result in a
smaller error so this is one technique
used as well and this technique was
actually very helpful in reducing some
of the errors for some technologies the
two of our favorite technologies are
superconducting qubits and iron traps
superconducting qubits have errors that
there are due to markovia noise and
therefore it is difficult to reduce
these errors by using control techniques
the error of the gate is basically
proportional to the duration of the gate
and if we use sophisticated control
techniques that increase the duration of
the gate that will actually lead to
increase of the error so primitive
control is the most successful with
superconducting qubits and the errors
would be roughly 10 to the minus 5 for a
quantum gate on average
Iron Chefs have extremely low error
rates except the measurement error but
we were able to use a circuit
decomposition that uses three
measurements to bring down the error of
the measurement down to roughly 10 to
the minus 9 which is in line with the
the low errors of the other gates for
iron traps so these are our two favorite
technologies because they have they have
low errors and they can be used both
with the topological codes and
concatenated codes neutral atoms also
work reasonably well or well sort of
well this topological codes but the
errors there are about 10 to the minus 3
so that's just below the threshold for
topological codes but they cannot be
used as concatenated codes and for a
quantum dots and photonics our results
currently do not allow us to use these
with any of the existing quantum error
correcting codes so here's an overview
of the numbers that feed into my
resource estimation tool so here are the
domain I selected some of the
technologies superconducting qubits Iron
Chefs and neutral atoms and I'm showing
the average gate time for the average
operation our resource estimation to
actually takes the the gate count for
all the individual gate so the
information that fits into the tools
more detailed than one what this table
shows so this this is the average gate
time then we have gate error for each
gate operation and then we have memory
error per unit of time per nanosecond so
the key observation here is that Iron
Chefs have the lowest error rates by far
but also the gates times of iron on
shelves they're not they're not very
short they're roughly three orders three
orders of magnitude slower then the gate
times for superconductors
so superconductors are faster but
they're a little bit more error-prone
the earth would be 10 to the minus 5 so
it is certainly interesting to compare
these two technologies because it's just
by looking at this table it's not clear
which one is better and then we have
mutual atoms which they are both slow
and error-prone so they're not going to
be certainly not going to be the winner
the money that is right but I'm not
convinced that this is unphysical aight
i think this is art effective of our
model
so to do the estimation I picked three
quantum algorithms I picked algorithms
that will give us reasonable running
time so in our project we are
considering algorithms that have just
quadratic speedups also and these result
in huge gate counts you're considering
running these algorithms on problem
instances that are very ambitious again
this leads to a big gate Council here
I'm trying to handpick the algorithms
and problem instances that will result
in in results that are human readable so
if we actually build the quantum
computer you will be able to see the the
result of the computation in couple of
months so one such problem is estimating
the ground state for a molecule I picked
a molecule that it's not too difficult
to analyze why I picked this glisan
molecular small organic molecule which
only requires 50 basis functions to
describe the ground state and I
calculated i decided to calculate the
ground state in a fairly crude way with
only five bits of accuracy so this is
the first problem
which of the podcast country basis to
choose um I'm not sure about this a 3g
4g i'm not sure about it i'm in cubes 50
cubits for the bases and then 10 cubits
additional the second problem is the
binary velvet tree algorithm so the
problem formulation is as false I will
take two binary trees and weld them
together in the middle so here the top
part is one binary tree here at the
bottom is the second tree the two roots
vertices are special one of them is
marked as the start vertex and the other
node the other route is the finished
vertex and the goal is to start at the
start vertex and find the finish vertex
by relying on an Oracle and the Oracle
will give us give us information about
the graph if we supply the Oracle with a
label a name of a vertex in the tree it
will give us the names of the
neighboring vertices and each each
vertex s actually exponential number of
labels assigned to it and the Oracle
goal return just one of the labels and
we have to decide once we clarity the
Oracle at a certain vertex view have to
decide which of the three neighbors we
will move to next and this way we have
to find the finish vertex and using
classical computation it was shown that
it is impossible to find the finish
vertex and sub exponential time but
there is a polynomial time quantum
algorithm that uses the continuous
quantum random walk and we chose this
problem for three depth 300 which is an
instant size that would be very
difficult to do on classical computer
yes so three tips is M equals few
hundred three days there's a depth so
I'll leave a lot of the number of nodes
is going to be huge this going to be
like great exponential Troost to today
well it gets bigger and get smaller so
it's perfect yes well they are not
exactly perfect there's this almost
perfect i think we can assume that they
are perfect for this purpose there
they're not perfect in the middle
because it turns out there is this this
strange feature of the problem if we
have exact binary trees and we've all
done in the middle we will have vertices
of degree 2 in the middle and then there
exists a classical algorithm that can
exploit this information it knows
basically when it hits the middle and it
becomes easy to guess at which point in
the in the past we make the wrong turn
so they are screamed a little bit but
they're basically binary trees so since
you scrambler little bit now there are
many possible please
they've other paths we think that this
you mean the center is a random second
grade yet yes so now so in addition to a
number of knowledge there is a which
particular trees pair of threes
so what they do to the stand the first
time around how gives a number which
depends only on the size of the program
and it also depends on particularities
of rocker mutation of the joining of the
right is this is this problem available
in public literature it is said there at
least two papers signed and they're off
that studied this look upland ir but ok
binary multi tree benchmark curses okay
fine
and then the search algorithm I studied
is the triangle finding problem so again
it's a graph problem where we have graph
is n vertices and there is our task is
to decide if there is a triangle in this
in this graph so this is the the
instance of the problem is set up to be
this pathological instance where the
graph is dense but there are very few
triangles there's actually there's going
to be each of one triangle or no
triangle at all and the task of the
algorithm is to decide if you are given
instance of a graph that has one
triangle or no triangle so the triangle
here is in the middle and then we have
these components which contain each of
them has an over six nodes and these
edges mean that all the all the vertices
in the individual components are all the
pairs of vertices are connected by edges
so there is no no triangle in this
region but a single triangle in here so
we consider it a graph with 2 to the 15
nodes
so what's the challenge here was so
what's the classical way of showing if
there is one
well I guess classical way of solving it
oh well there is an Oracle also that
tells you how the structure of the graph
looks like so i guess you have to query
it you have to tell it the name of the
the vertex again and it tells you which
are the neighbors and then you have to
scramble this inflation
I believe that for this problem 32,000
nose is actually pushing the boundary
because this is uh this is the original
perimeter given by Ayyappa and from
phase 1 and we try to or they try to
come up with parameters that that are
pushing the boundary yes like quadratic
road like speed up here or going to stay
home I believe the quantum walk here
gives exponential speed-up 2 i'm not
sure about probably i know that for
bella tree it's provable for the
triangle problem i'm not willing to back
number of tables of those and also jesus
knows
just exhausted
right but i believe the oracle well i
don't mind
I will have to check the the speedup
claim for this algorithm so this for the
welded tree its exponential and 40 depth
300 this would not be solvable
classically for the ground state
estimation this more like so we
originally studied molecule that was
more complicated but actually the
numbers the gate counts coming out of
that are huge so I decided to analyze
the simple molecule and for this
molecule this would be I believe
classically solvable problem so this
table summarizes the the gate counts
that we obtained so again I I picked the
valid three and triangle finding problem
they use the quantum random walk which
is fairly efficient so the the gate
Gavin's coming out are not too bad and
also for a ground state estimation
you're using a simple molecule so we
have shown state estimation 10 to the
12th gate the number of qubits needed to
do the calculation at 60 that's these
are the logical qubit and then we have
paralyzation factors which which tell us
on average how many of the gates can be
done in parallel in the circuits in my
opinion there is scope for improvement
in these parallelization factories we
don't know if we can lay out the
circuits in a better way that is more
parallel
and for each of the algorithms we not
only have the total gate gun but we also
have a breakdown by gate type so this is
the ground state estimation algorithm so
we know how many state preparations we
need how many harder martes cenotes sze
gate measurements and this is the
information that feeds into yes you're
able to do ground state without any
rotations
who you ride the circuit
oh so I think I think actually the these
Z rotations are arbitrary rotations I
think that's eating Oh guess it's
arbitrary and Z that's fine there are
some scenes really yes that's a docent
at misleading choice of okay so you
can't do that politicians so for the
actual analysis of the error-correcting
codes what is the overhead so so far we
only saw numbers with logical cubits and
logical operations what what is the
number of physical operations we need to
do so we looked separately at the three
concatenated codes and at the surface
code so for both code families we
assumed a tiled layout so we have a two
dimensional structure that contains the
physical qubit and the structure is
divided into tiles where each tile is in
charge of one logical qubit the tile has
enough space to include NLCS so if we
need to error correct the information in
that logical qubit we can do so within
the tile and the third dimension in the
space is reserved for a classical
control so how exactly looks the layout
of the qubits inside of the tile we had
to determine that separately for each of
the error-correcting codes because they
encode information differently
operations are done differently and the
number of qubits is going to different
side of each tail too so here is the
tile structure for the steam code at the
second level of concatenation so the
first level tile is here so it contains
six by eight physical qubit and then at
the second level to use six by eight of
these level 1 blocks to construct the
tile and we have to choose a sufficient
level of concatenation to guarantee high
probability of success of the
calculation so I chose a cut of fifty
percent success probability for the
calculation and i used the equation
I believe it was originally shown by go
Desmond in a paper that shows how many
concatenation levels do you need and
from there we can calculate the number
of physical qubits so what is going on
inside of the tile during computation
well the size of the tile depends on on
the code there is existing literature
that shows us the location the optimal
location of the qubits inside of the
tile and the sequence of operations we
need to do to perform error correction
as well as all other logical operations
so existing literature tells us what to
do for steen and bacon short code but
for the c4 c6 code we have to come up
with our tile design so here's a
specific example from the paper of swore
that shows the operations inside of the
tile for the sting code so the tile
sizes six by eight and the sting code
that uses seven physical qubits to
encode one logical qubit so these red
Cupid's here the red locations there are
the locations where the data lives and
then here on the interior of the tile we
have some answers that are needed in
order to do syndrome extraction and
error correction for this team code so
this determines the location of the data
qubits the NLCS and also a sequence of
operations so the paper of swore
actually shows the exact gate sequence
of all all the gates that need to be
performed to do error correction as well
as all other operations this is a
snapshot of the tile at the particular
moment in time during the error
correction and these arrows they
represent physical gates that are being
done to deserts wop gate and control not
the layout has to be optimized to
minimize the number of operations that
need to be done to
ensure good reliability of the circuit
and to minimize the amount of movement
so these swaps are done because
controlled not operations can be only
done on Cuba's that are next to each
other so this is actually a very
expensive thing for the for the
concatenated ghosts the movement our
tools we use three cursive equations to
express the gate counts for the desired
level of concatenation so we counts the
number of elementary gates and at the
time it needs to be nice to do these
operations taking parallelism into the
account we also estimated the additional
space just needed to do and so a state
generation up to desired level of
fidelity we used similar methodology for
the topological codes so as I said
earlier in topological codes pair of
holes represents a logical qubit so we
assumed this layout we have to ensure
sufficient spacing of the holes because
loops that connect the host our logical
operators if we have holes that are too
close to each other there's going to be
a low weight logical operator and the
code is going to be error prone so we
calculated a code distance that is
sufficient to again guarantee that the
calculation succeeds with high
probability and we also took into
account the movement of the horse during
braiding so we left enough space in
between the host so that another hole
can be braided without affecting
negatively the Arab properties of the
code so to obtain the physical qubit
count that is very simple we just
obtained the go-to distance and then we
multiply from Co distance we can get
size of one tile and we x number of
logical cubits and that gives us number
of physical key bits for gate counts we
started by first calculating the
running time of the entire computation
and then from there we were able to
calculate the number of gates that are
needed to do the error correction which
is done all the time on all the
essential elder cubits so this is the
major major component of the total gate
count and then finally we edit the small
number of additional gates needs to do
gates such as measurements Hadamard and
so on so numeric results these are the
results for superconducting qubits which
so if you recall this is the technology
that has very fast gates times on
average gate only takes 25 nanoseconds
and as far as errors go this is
somewhere in the middle of the road
among the technologies I showed you it's
ten to the minus five I repair logical
gate for the brigade and this table
shows the resources for the three
algorithms for the instant sizes we
discussed so you can see that the
surface code will result in computation
times from months two years for these
instances here are the gate counts and
then this is the physical qubit account
so we need a few million cubits for the
computation the bacon short code which
is representative of the concatenated
codes request much higher computation
time and I think the reason is we need
several levels of concatenation three or
four levels of concatenation to address
the gate errors and that will translate
in a huge gate count and huge h10 need
is to do the computation and also the
number of physical qubits is going to be
high because the folders concatenation
we need to store the information
somewhere yes
to this number like if you push then de-
with everything was like yours of
magnitude that is a good question it is
it is sensitive to it but prep oh yes
the question is a sensitivity to the
gate errors if we change the gay
terrible the the resource requirements
change a lot and the answer is yes it is
sensitive because for concatenated coats
for example there is this very sharp
transition if you go from three
concatenations 240 concatenations the
resource requirements will blow up by
three four orders of magnitude perhaps
and that's by changing a single number
if you are just at the boundary then
this can cost you a lot but also what is
very interesting i'm going to discuss
this in the next slide there okay so in
the ground state estimation we sort of
agreed hear that that you need to have
various Z rotations as as the gates
right yes did you count did you count
them each one as a single gate or didn't
until you estimate
be kind of the lengths of decomposition
into
into the lamentable swifty
was that doesn't represent 80-count
involved in implementation of a single
at 800 the gates one gay over a hundred
gate over I pencil dick ty include one
but we used to like a diet to do the
decomposition of the rotation you do yes
oh ok ok so that is the real number of
guests yes right so what and ask what
flavor of saturated I afraid
yeah I have you seen as there yet course
recent was this question we kind of
improved so awake it airworthy others of
magnetization and I was wondering how
much it would have affected the ground
state party possibly I i I'm here there
could be improvement big no-no but he
had Z up there was the full rotation
Kate so the way goes into here is to
multiply it by 10 to the second type of
third to get the accuracy in us even by
the bid x so she didn't actually have
right so the point is you could probably
drop three digits the tenth of the
twenty second might drop down to 10 to
the 19th instead if you didn't have if
you didn't have to do the work if you
could get three orders might do better
on the rotations is that what you're
getting at yes yeah yes it I would be
certainly interested in learning new
techniques to improve the decompositions
it's still a protected on teeth I mean
right still you know it's this simple
molecule is still a hell of a lot
engaged right but we have lots of we
have lots of problem instances that have
much higher numbers i'm just showing
numbers that are human readable they're
just get along these lines is it clear
that if you drop so you said you took
fifty percent success rate for their
companies all rhythm did you clear that
if you drove that to something like
attention to my
you just repeated the experience like
many times which is it clear it windy
better than waiting for house of years
for once and stuff that's a great
question we haven't done this
calculation and it's possible that we
can obtain lower running time by by
targeting lower fidelity of the
algorithm and repeating it a few times
also there are basically many
optimizations that could be done and we
haven't taken into into account one such
other optimization is distillation of
the enso us perhaps if you distill them
with lower your target fidelity for the
Angelus is lower perhaps that will
introduce inaccuracies but since the
distillation again is this concatenated
process if you use one fewer
concatenation in the distillation
process that saves you a lot of time and
a lot of gates so it's possible that
these trade-offs can improve the
research system is significantly in the
earlier slide did you have the logical
gates and so on and how many suppose
there was no errors was it distant yeah
that Z is really wrote agency and must
be expanded out this whole thing I
understand so it might be some big
number writer that gets much bigger yes
all the rest of them are sort of what
they are which says you have something
under worked in the 14th or something
plus whatever the Z County whatever the
T cabins he is yep so in each episode
yeah so the the next question that I
wanted to ask was if there is some
retching so we saw that the topological
codes outperform the concatenated codes
for the superconductors so the next
question I wanted to ask is is there
some regime where actually the surface
codes are not performing as well and the
answer is yes so I considered these
three technologies which we have neutral
atoms which have huge error rates on the
gates and then we have superconductors
which have lower errors and then we have
Iron Chefs which have even lower errors
and okay so the result here is very
interesting for the high areas of
neutral atoms we can only use the
surface code because the surface code
meets the threshold 10 to the minus 3
but the concatenated codes they just
cannot deliver in the regime where we
had 0 and at the time to do the
calculation is huge because the gate
time for neutral atoms is three orders
of magnitude longer slower than 40
superconductors superconductors we
already know that the surface coat will
win but then if we decrease the error
rate even fritter for the iron chaps the
interesting observation is that the
concatenated codes actually do better
than the surface code and the reason is
with these very small error rates we
only need one level of concatenation so
error correction is the concatenated
code is super cheap but the the surface
code the duration of the computation is
almost independent of the code distance
because all the
erations in the surface code are done in
parallel all the central measurements
are parallelized all the operations to
braiding is highly parallel so what
actually determines the the running time
for the surface code is the duration of
the gate so some key observations the
the surface coats are better in most
regimes unless you are dealing this very
low error rate tianjin and see not gates
are the dominant gates for logical
circuits but as far as physical gates
are concerned see note was the most
frequently used physical gate for
topological codes and swap is the most
frequent gate for concatenated codes and
the reason is that for a concatenated
codes we need to use swaps to move
information around so that control node
operations can be done locally there as
surface codes do not need the swapping
so finally I will just use the last five
minutes to describe some thoughts about
building a faster decoder for
topological codes so so far we are only
concerned with the quantum resources but
we have to keep in mind that for the
surface code we will also need a
classical controller that decodes the
areas that are cure so we know that you
need to be able to solve problem that
has millions of physical qubits we need
also we know that syndrome measurements
will be inaccurate so we will need to
decode errors after several rounds of
our measurements syndrome measurements
and we know that we like technologies
with low gate times because the surface
code works well as technologies with low
gate gate times such as superconductors
so we would really love to have a
decoder that works in real time and this
is this is break that I'd like to do in
the coming months I started looking at
the decoding problem
the to decode errors for the surface
code we need to solve the minimum
matching problem too much pairs of
syndromes and one key observation is so
even though this this problem is this
fundamental time solvable we would
typically use the Edmonds matching
algorithm to solve it it is going to be
too slow for the problem sizes here
considering so one observation that is
due to faller is that we don't need to
solve the most general minimum rate
matching problem but we can prune
certain edges from consideration so
here's an example we have a vertex V and
we want to match it to some algebra tax
and follow observations all those that
points in space cast shadows and if
there's two point number one it will
cast shadow here in points3 will be
shadowed by it so vertex V we only need
to consider the edge from V to one but
we can ignore the edge from V to vertex
number three and the reason being we can
show that assuming a minimum of eight
matching would use the edge from V to
point number three we could find an
alternate matching that would have lower
weight so now this is a two-dimensional
picture and of course we need to solve
the problem in three dimensions it turns
out that in two dimensions there is a
very simple linear time algorithm that
can prune the edges prune the shadowed
edges but I did not manage to find a
counterpart in three dimensions and in
fact I believe it's unlikely that such
algorithm exists but i found a heuristic
that that allows us to to prune the
edges in linear time this very favorable
results and the heuristic works as
follows so we are given the vertex V and
we want to find the candidate edges so
we will look in in the for geographic
dimensions for the closest point
and then you will create a bounding box
in these two dimensions and we know that
any point that vertex V well connected
is going to be inside of this bounding
box and then we look at points in the
the street 3rd dimension perpendicular
to the screen and we find the closest
such point in each direction and we
connect vertex V to these points so i
looked at the resulting average degree
of the vertex in the three-dimensional
gap did I obtained by simulating the the
surface code and so here's the number of
qubits in the surface code so I went up
from 100 cubits 2 1,000,000 cubed and I
looked at the average degree of the
vertex after do pruning and it seems
that the number approaches about 180 so
we have in the limits we will have a
constant number of of edges per vertex
and I try to solve the the minimum
weight matching problem on just my
laptop computer and see how how long it
takes to do the matching and these are
the results and you can see that the
scaling is also linear so 4 1,000,000
cubed it takes a bit over at minutes to
construct the graph generate the errors
and prune the edges and then it takes
about seven or eight minutes to do the
matching itself yes there's a question
let's do the number of qubits but what
is
our eight years in place because that
has a big effect on number of vertices
you need to pass to put crap i was
running fairly close to the threshold at
about two thirds of the threshold if the
error rate was much lower than I think
the result would be we could we could
decode errors for higher number of
qubits because there would be just fewer
syndromes but probably I think the
statistical distribution of the points
in the space would be very similar so I
think the degree of the of the node
would actually be the same um yes
another question using this ristic to
paralyze wrong I understand so this is
not parallel yet this is candy but it
can be and I'd like to look at as
possible
so it is certainly possible I i have
actually parallel implementation
similarly have it in in the next slide I
have a parallel implementation for the
pruning which is easier to do I mean the
pruning is just a very simple heuristic
I don't have paralyzation for the
matching itself which uses the edmonds
algorithm it is this primal and dual
updates for switches just does this
trick of creating these box around the
very season having these small crafts
does that affect the performance of
course oh yes it's a very significant I
would not be able to do I would probably
have to stop at somewhere between
thousand and ten thousand cubits if I
didn't do the pruning
universe protiviti coming here
it doesn't affect the probability of
errors because we are still solving the
exact same problem here you can show
that the only prune edges that must not
be in the minimum of eight matching so
in conclusion I did some work on
topological quantum error correction I
helped to develop to a new quantum error
correcting codes in this space the five
squares code and the triangular code and
then the past year I've worked on the
quantum computer science project of
firepower to estimate the resources
required to run a variety of algorithms
on realistic quantum machines using four
families of quantum error correcting
codes and currently I'm looking at the
problem of parallels paralyzing a
decoder for topological quantum error
correcting codes thank you
well yes I don't know there's a simple
answer this if not we can take this
offline but what about the bottom into
laundries when you decode the surface
over twelve amendments can you take into
account preparation errors and what
about the last layers okay so I guess
the point is that you measure many times
to be more sure about the silver
mountain but the less measurements who
did might have errors which you're not
sure are good so you want to decode what
is in the past substance so all you deal
with this so I think there are two or
perhaps three schools of thought how to
do list is one of them says but just
regardless how you solve this problem
you could just do a dummy round of
terrific measurements and then run your
algorithm on this instance and then is
the knowledge that in reality you would
do something else such as do the error
correction maybe ten rounds behind
schedule behind it the actual
measurements the second school of
thought would be obviously simulate
exactly what would be going on in the
real system which is perhaps this you
would it would do the decoding with some
lag and the third school of thought is
to use a periodic boundary condition in
the in the time dimension so you would
connect essentially points to the
boundary
yes I've been so just a comment this I
think you collaborated with on this
project but yes on the surface notice we
had some members for some of the
distillation procedures
we assumed a very conservative model of
how we were distilling magic states and
so some of those numbers I think you
show could be reduced fairly
significantly with a penalty area I
changed it already actually right so we
had earlier results that were more
pessimistic because we used a convert
conservative assumption that only a
single senior would be done in the
surface code at any given time but the
numbers i showed right now I try to
paralyze all the other see notes that I
could including in the state
distillation in our state distillation
and a yes we need a larger number of
qubits than originally predicted that is
correct</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>