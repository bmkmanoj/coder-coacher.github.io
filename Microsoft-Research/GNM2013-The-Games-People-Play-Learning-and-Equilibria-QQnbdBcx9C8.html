<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GNM2013: The Games People Play: Learning and Equilibria | Coder Coacher - Coaching Coders</title><meta content="GNM2013: The Games People Play: Learning and Equilibria - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GNM2013: The Games People Play: Learning and Equilibria</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QQnbdBcx9C8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
well this is a slight change of
direction but again it's trying to
elicit I guess how people then really
play and how that how can they learn in
certain situations and I guess I'm
really interested in if you have sort of
a situation of a large number of
interacting people in for example if
you're trying to drive a car or
something to get into work as I am
you've got a lot of choices and and how
had you out what's the best kind of
decision to make in an online market
again what's the what hap was the best
way how do people learn in such a
situation and ultimately I like to know
well how can you design a system that
works really well given the way that
people behave and so this is a talk
really with two main elements first of
all I give a bit of context but then
talk about some it empirical evidence of
how people behave in real systems if you
if you give them pats a slightly
artificial game and ask them look at how
they behave what do they do and then a
slight change of tech saying a rather
prescriptive approach saying well okay
what happens if you if if you look at if
you if you say if you under what
conditions would people come up with a
plausible equilibrium and then just say
give an example of a sponsored search
auction so previous speakers talked
about display ads I'm gonna talk a
little bit about a sponsored search
auction okay so some background just
stirred to get everybody on the same
page but everybody see as a game
theorists so the classical game is
something like the prisoner's dilemma so
if Bonnie and Clyde for example agree
that if they're captured they won't
they'll stay silent and they are
captured interrogated by the police and
if then if they both cooperate in other
words they stay silent they'll perhaps
get off with one year in jail because
there's not enough to cook to to convict
them whereas if they both them if one of
them spills the beans against the other
then one will get off free but the other
will get 20 years and if they both you
know if they both since trying to shop
each other they get 10 years in jail ok
so this is a classic game and you can
say and so in terms of nash and Nash
equilibrium it's just a situation where
it's in your best interest not to
deviate from a particular strategy so I
got a set of actions I take a strategy
as a probability distribution over there
Zach
then my best response internationalism
is not to deviate so for this game here
and the baseness takes probability
distributions for this game here
reneging or if you like defecting is a
dominant strategy and also is the only
Nash equilibrium renege renege when they
both be better off of course cooperating
so that's a very simple situation of a
1-1 shot game and a thing to take away
another way to think about that is to
say well if I look if a player looks at
for example their best response and you
look at mapping best responses in
strategy space then a Nash equilibrium
is just a fixed point in that space and
the Nash equilibrium exists if and only
if there is a fixed point and well
that's a very simple game a one-shot
gain and in practice you want to look at
more complex games i'm particularly
interested in kind of repeated settings
and most general game you can think of
us is a stochastic game where at any
point you can move to another game
dependent on the actions of somebody so
this just shows it pictorially where you
can see that Yoram you're going
potentially from this payoff to this
payoff depending on the actions of the
players so this is a very general
setting which includes the basically
it's a game we allowed to make
transitions that depend on the state and
it includes things like repeated games
where you play the same game of a number
of times and also Markov decision
processes and again I guess the
interesting thing to note so if you
imagine playing this this situation
repeatedly and you discount utilities
okay then then certainly a markov
strategy is something that only depends
on your current state there's no history
over time then then a Nash equilibrium
markov perfect equilibrium always exists
provided you discount okay so that's the
kind of result that again their
existence the trouble is finding it can
be very complicated and it's not sort of
clear if you can really do it in
practice so that's the kind that's at
the end of the theory overview but just
to kind of hold in mind this thing that
to think about a stochastic game where
you can move over it where you play
repeated over time and in some sense in
nash equilibria exist but what does it
mean ok so now let's but what happens in
large systems this really is impractical
system if I'm driving a car I can't
really know what everybody else is doing
around me and make predictions about
them I can't use very limited
information about it and people had so
how do people behave in practice well so
what you what would you like in
equilibria what would I like an
equilibrium to be well I like it to be
stable and computable if you can't
compute it in finite time the ones what
does it mean and four players are in
some sense bounded Lee rational you
don't have to be perfectly rational I
need to in some sense have some some
limited rationality that's what I'd like
to look at and of course the question is
well what kind of equilibria stats
satisfy that and perhaps one example of
something that is sort of semi plausible
is something like fictitious play so
that says well I just observe what other
people do in terms of distribute their
the distribution of their actions
characterize what they do I'll have some
prior beliefs about what they what they
do and then I'll just play a best
response given my beliefs then I'll
observe what happens and I update it and
I play it again there's a very
relatively simple algorithm bear and and
you can show them some sense it will
converge in some senses but it converges
to outcomes that might not be the same
as a Nash equilibrium or a certain
technical points but I mean it's a kind
of plausible plausible strategy where
and sort of it mimics in some sense what
you might be able to do if you kind of
localized your knowledge so but what do
people really do okay so let's now look
at something some very stylized games
and just see what happens so the first
game is a classical them strategy game
called Colonel blotto so those of you
who know risk it's is kind of a
precursor risk and the basic idea is
you're you're given a set of troops and
a set of territories and so it's a I
play against you for example I assign my
troops to the territory you do then we
compare them and whoever has the most
treats on that territory wins the
territory and you and and the 12 mins
most territories wins okay so this was a
2-1
Stan Shih ation of that is this example
here which was done as a facebook games
we launched it as a facebook game and
the I in this particular setting there
are a hundred troops you had 100 troops
and you could see there's five
territories and and so as I said the
most the person with the most troops on
the territory winds and the Nash
equilibrium is sort of relatively
complex what's the ideal thing to do if
you're playing Nash well it turns out
you'd allocate randomly between naught
and 40 troops the marginally marginal
distribution will be to allocate
randomly between naught and 40 troops /
territory but with this constraint the
complicated thing need constraint that
your title should equal 100 otherwise
you're kind of giving away something and
so then question is what does this mean
this kind of Nash equilibrium that's so
you know if you're playing against
somebody you have to play gets an awful
long time to sort of take the expected
games to come up with this being a Nash
equilibrium and so what do people really
do okay well them so this is this was
now some results of looking at how
people actually played this Facebook
game so one interesting thing to notice
people not always very rational so
there's this is this a strategy here so
these are strategies where work sort of
looking at equivalence classes so this
means so this class means any time
somebody put 100 on one for example on
one territory and 0 on the others so
this is a strategy which is bound to
lose unless you meet somebody who's
equally stupid and also puts 100
characters on the same thing glitch case
you know you might win with probability
half so some people just put 100 all the
troops on one territory bound to lose so
well it might be there sub rational or
it might be that they just haven't
understood the game and these are kind
of list of types of strategies with with
the number of times people played them
so for example this one here if you
equally distribute your troops across
all five territories then this was this
has played quite a reasonable number of
times and this is a kind of a strategy
where people
just essentially pick three territories
and put equal numbers roughly equal
numbers on those and you can see various
other strategies down here so there's
kind of a you know people did tend to to
concentrate it on a certain number of
strategies but on the other hand if you
look at the kind of number of times
people played a particular strategy then
they just tended to often play the
strategy just one so they didn't play
the same strategy many times so the
question is okay what what should you
play or so this is a graph now that
plots what people actually did end the
so each point here for example is this
point here is them is this kind of
strategy where you just put hundred on
one territory and the arrows represent
errors joining different strategies
represent the probability that you'd
beat that strategy if you were to
randomize over all possible instances so
that as you can see this has got no
arrow going anywhere because you're
bound bound to lose this is the most
popular strategy which is to put this
you know this roughly equal amounts just
on three territories and you can so you
can see it dominates so so this strategy
but he's always dominated by another one
so you look at that if you look closely
you can see these arrows have these
lines have got arrows on them which says
whether that will beat that strategy and
of course you can always beat any
strategy in this game if you know what
somebody is playing you can always beat
it and say but but so how do people
actually use these strategies in
practice well they seem to do some kind
of consumer of limited learning so if
they played against somebody and a
strategy beat and they perhaps mirror
that strategy okay so the people seem to
sort of relatively limited kind of
learning you know very limited reasoning
in the in the players we saw they didn't
do deep reasoning perhaps it was one
step reasoning this person did this
therefore I will do this to try and beat
them there are some caveats and that
sort of people that even within this
setting there are certain features that
that didn't come out that aren't obvious
for example there's a left side
perhaps because of the way that the game
was designed or you had to allocate
troops people tended to put more troops
on the left and territories and then
right where do you expect it to be
random and people also the probability
of winning against some of they knew was
rather difference from playing against a
stranger whether that was because they
knew they could win against somebody or
or isn't quite clear where they were
playing deliberately different
strategies and so there's other games we
can look at online auctions later but
this is an example of another game the
issue of the last game Colonel blotter
it's actually quite complex to to kind
of rationalize about you've got five
territories hundred troops but and
you're playing repeatedly so let's look
think of a really simple game an all pay
auction and this isn't your arms going
to talk about a bit this later you can
think of this as a model of
crowdsourcing when you put effort in or
putting in an effort into a programming
competition where only the best person
wins so the general ideas if there's end
players and they bid or compete for some
item and we'll assume it's got a common
worth em ok so there's just one prize
and you both agree everybody agrees on
the value and the mechanism is this
auction is that everybody bids the high
speed winds but everybody pays so even
if you lose you still basically lose
your your bid your steak so what's the
simplest example this will think of two
players so in that case in the Nash
equilibrium or there's symmetric Nash
equilibrium there are others asymmetric
ones it's just a bit uniformly between
Norton however much you value the good
for and if you do that you can see well
the expected revenue to the person
running the auction is just m and the
expected player utility is zero they
expect actually not to win to make any
net net gain on average ok so is that
clear ok so again the course to get
people to play this games you have to
disguise it so this is a this is an
instant again another example of a
Facebook going dressed up as a pirate
game called double in dash which again
was we tried out on that on the
onlive people and the basic idea is that
you had essentially you could bid
between up to 10,000 so m is 10,000 in
the ideas you had to say in about how
much you're going to pay for a for a
ship to reach the island and the more
you paid the faster was the ship and you
had to EndNote the prize was was 10,000
and so think of this as an alt
assimilating an all pay auction but
disguised and each player had to submit
a bid in the range of nought to 10,000
ok and so then so then we recruited them
people on Mechanical Turk to play this
game and and and of course the trouble
is mechanical turk you have to play them
participation fee but then we also pay
them a bonus and the bonus depended on
how well you did in this game in
particular you are paid a bonus
depending on your average utility you so
use the average utility if you paid at
least 30 games she had to pay 30 games
because then to kind of ensure that
people would go and go sumwer nning
which means you could earn a bonus of
roughly between zero if you could have
always lost the maximum amount or two if
you've got if you did the maximum
possible win if you won the maximum
possible amount at every possible game
ok that's just a quick quick picture of
the game wish you can play if you want
you can play both these games you want
to go on facebook if you kind of if you
want something a diversionary tactic
during the lecture so what do people do
ok so we fair we looked at five hundred
odd players and they paid about 13,000
games well the first thing people do of
course as they cheat so we said
specifically you can't bid on a small
number of simple bids and we and we call
people who do that spammers and roughly
fifty percent of the people ignore that
they did they did do these relatively
simple bits apps just to earn income the
other people did was well the smarter
they basically created Sybil's so the
issue was they could create they created
multiple identities and played against
themselves will they be guaranteed to
win and roughly something like twenty
five percent of the people who didn't
you know incense spam did that kind of
behavior so if you take all those people
out what are the rest
well one thing they did was to have
focal point in other words they tended
to bid on very similar around certain
specific numbers for example there was a
biased a strong bars to bid it in
multiples of a thousand and to go again
to sort of bidding multiples of ten
people certainly didn't play Nash we'll
look at that in a bit in a bit in a
minute and they typically over bid this
is what other people find this is what
happens in these type of all pay
auctions people typically over bid and
as a consequence the average revenue was
was roughly it was thirteen thousand
that's one point three seven times this
amount em where's you'd expect it to be
em if people were playing Nash and they
people received relatively low returns
so for example the expected utility to
play with zero in fact there is the
average they received a negative amount
so this is again a consequence of them
of over bidding this is a graphical
description of what they actually did so
this is neck on the axis here is the
amount that the people bid and here's
the cumulative density this is like a
cumulative distribution function so if
they bid a Nash equilibrium you just
have a straight it would just be
linearly increasing whereas this is the
actual cumulative distribution and you
can see it's got these jumps at a
thousand that reflects the fact that
they focus on those values and you can
see here the sort of grat the a slightly
over bid at the lower end and then and a
bit at the lower end and then gradually
over bit as time goes on so there's very
little weight in the middle yeah so and
that's the content that that translates
to the difference in in expected return
well let's look at a try look a bit
detailed or what people actually did so
this is now a player's bid distribution
so this is what people did if you tried
to cluster so if you look at people
divided the bids into bins and then just
cluster them on the basis of similarity
using a simple k-means clustering
algorithm and these this just shows
average beads and players bid for two
typical clusters and this is a cluster
where people sort of bid you know they
bid a low bid
with relatively high possibility not
much in the middle and then a bit at the
end and this is the red line is the
average distribution and the and the
blue lines represent players
distributions and this is a slightly
different distribution here so you can
you can do this and think about
separating people into classes and they
do majed in it's already two different
ways if you just ignore the the time
dimension but what we really like to do
is do people learn so if you see what
what happens now people look over x you
look at people's bids over four
consecutive games and you see what
happens here this is the time period
here and this is their average bid so it
rapidly increases then it sort of
wanders around a bit and decreases later
on towards the end I mean there's a lot
of variation but you can see there's
certainly a noticeable increase at the
beginning and then it stays constant and
then fluctuates yes because in boy for
peace I don't think it particularly by
four case yeah that's good question i'm
not sure i mean i guess we're gonna look
at a minute what trying to disambiguate
these are very the Germans these don't
tell you how people behave they just
sell it just kind of an indicate what we
really want to do is say what do people
really do sequentially and this is just
this is just purely sort of looking at
it yeah overtime yeah I don't think it
by for kicks I mean that you've got a
mix of behaviors but it's a good
question so what do people really do
well them people seem to be definitely
playing affected by winning or losing so
perhaps people are less less bothered
less concerned about getting their net
income they're more they're more
affected by whether they win or lose
because we would behave differently
according to whether they win or lose so
for example less their bid is small then
we it was statistically significant as
their behavior following a win or a loss
so what people did was after after him
as they lost against an opponent they
typically increased their bid and what
that means is that so roughly almost
everybody two-thirds of the time
increased their bid after
after winning if you control on them
actually conditional on them actually
changing their bid and the reverse
effects apart ply as well so if they win
then they would decrease their bid and
so that's kind of plausible you could
say well if if a player wins again then
then perhaps you try and increase her
utility by by lowering her bed while
still trying to win so it's a kind of a
plausible simple behavior the
consequences if you kind of look beyond
that try to say well it did people do
something rather more sophisticated for
example did they have a limited recall
the facts they will just look over the
past four games or three games and try
and sort of maximize try and use that
information to to to develop a better
bit did they do that well it seems as
though they didn't particularly mean you
could fit some players who did that but
you didn't get but not not very many and
there's some people you could plausibly
say did that but typically not many so
in fact the only thing if you're trying
to mimic leap let the behavior of an
average player the only thing that
fitted it well with of really a kind of
really dumb kind of heuristic which is
the following so in each game you say
well if I if I if I won the previous
game then I'll just decrease my bid and
if I lost the previous game I'll just
increase my bid and just by we tried
various amounts and if you just pick a
number like a thousand ten thousand a
thousand it should be rather than 10,000
then you get very close to you mimic
affected that average pair for what
people did because they this isn't a
great strategy so then what we did we
play this against the empirical
distribution if you look what people
actually did play this stretch against
them see what it comes out with you get
a return of about minus two thousand
which was the average play off to people
remember this isn't a good strategy you
should be able to get you know should be
aiming to get at least 0 so this is
really dumb strategy seems to do well
who seems to impossibly mimic what
people do so this is what this is a
limited recall best dynamic which is say
okay I'll look over the last a window of
history of length D perhaps tears one
two or four then we looked at infinity
for the
all thing and then we'll choose the bed
that maximizes my average return if I
were to have used it against all those
those bids okay so be a relatively
simple loan recall dynamics where you're
kind of using a limited history and
trying to do the best conditional on the
past so that was the that was one thing
we tried which fitted a few people but
and a better thing you could do is say
well actually a stand-alone approach is
to do something like a multiplicative
awaits update so what that does it says
well I start from some bit distribution
discreet set i'll assign uniform weights
to them and then if i win i'll increase
the weight bye-bye well I decrease the
weight by some amount 1 minus lambda
where lambda some learning rate and if I
lose all I'll increase it okay so this
is kind of your you're trying to you're
decreasing rather so essentially you're
trying to adjust the weights according
to whether you win or lose okay so this
is a really simplest kind of learning
thing you can do just increase or
decrease your weights multiplicatively
and what happens if you do that well if
you play that in practice that does
really well against against the
empirical distribution you know you
dominate you get a positive return and
do much better than anybody anybody did
in practice moreover the previous one
this limited recalled dynamics also did
very well we did better than people
didn't it on average you come up with a
positive return so that sort of says
well look so kind of to summarize people
appear not to use for anything very
clever they don't seem to use standard
learning techniques they don't seem to
use limited recall perhaps they do some
limited best response but really dumb
but if you did something more more
clever if you gave them some hints said
we'll try look at the past or try using
this learning algorithm you do a lot
better we suggest well okay so first of
all it says people aren't very clever in
this setting and also said you could do
a lot better if you kind of gave them an
agent to help them okay so now it's just
as a flipped to say well this is a
setting in a very specialized what
happen till you try to say how people
should behave okay and if people did
react to this empirical distribution
what would happen well I'm going to
think about a mean-field approach which
says suppose now I'm in a stochastic
game and I just want to maximize my
expected return then the Conklin
strategy would mean I'd have to know
everything that was going on which is
kind of you know implausible but a
mean-field approach just says well I
assume there's a large number of agents
and I'll just only react to the average
of the edge for average state space and
the average strategy ok so then you can
come up with a strategy in a sense the
only depends on the average behavior of
others so why is that a mean-field
approach well it's a mean-field
essentially because you need the number
of states to be large and if you do that
then it says if you've got some
distribution of states such as of F if I
think of the distribution of states as f
then if I given that that distribution I
can define a strategy and then if that
strategy that I so define it is a steady
state distribution then I'll got a
mean-field equilibrium so I mean in
pictures at saying if you start from
some distribution across states I then
find an optimal strategy given that
distribution to produce a new
distribution ok if everybody plays it a
producer a new distribution and if
that's the fixed point then I'll call it
a mean-field equilibrium ok so that's
remember we talked about equilibrium is
fixed points this is another way of
getting a fixed point equilibrium so
let's apply this again to an auction
setting but rather slightly more complex
than the all p auction and motivated by
a sponsored search auction so in
particular I'm now thinking about
sponsored ad auctions which is we're not
display it so for example if you type
something like organic food into being
you get these examples of ads which are
which people are bidding to have shown
to so there you've got some on the main
line and some on the sidebar now this is
when people bid on these they
advertisers have budgets the question is
well if they knew their budgets or if
you do them for them how should you
altitude they bid optimally and what
should you do and this is just two say
people behave very differently so these
are real key words such as coupons
blackberry
and this just shows the probability of
building an amount if you take the bid /
the budgets of various advertisers I
mean a thing to notice is that they're
very different know these they don't
have a common shape people behave
differently the markets behave very
differently so the traditional game
theory approach would be to assume that
your number of bidders is known you know
everything and so then you could work
out this big Nash equilibrium work at
what to do in all these situations that
clearly just unrealistic I mean for
start the system its systems very noisy
and also you just cannot possibly know
you cut this too much information to
kind of de passe so let's try and look
at this kind of stylized model now
suppose we look at budgets bidders who
are constrained by budgets and they want
to maximize the return a repeated
auction okay so that's equivalent to a
an online stochastic knapsack problem
and I'm going to soon there's just a
symbol single good and the user has some
budget be some value V and can bid some
amount a so think of a is your action
okay and I'm going to shooter the winner
plays the second highest price it
doesn't matter too much they pay some
amount p so that means my private
valuation which i know you but other
people don't is V and I'm assume there's
some competing bid that's not observed
okay p in this case so then my decision
variable is what's a bid which will
depend on the boast my value of my
budget and my how much do i gain well if
I wind a good then I'll my net return is
just the value minus what I paid in
other words V minus P and remember I pay
that the second highest price in other
words the highest bid I a competing bid
okay and so if i want to express this as
a problem i can say well i'll know just
maximize my expected discounted return
and now i'm going to make a big
assumption i'm just going to assume for
the minute the bids are actually
identically distributed with a common
distribution okay that's a big
assumption but i can perhaps a appeal to
mean few limits to justify that so then
the trick is is to actually do a kind of
joint space and time scaling
if I scale both time and space now those
I scale the discount factor by letting
the beta go to zero at the same time
divided the budget so essentially I let
be to go to 20 and of oops and divide
the budget by meter so this is this
joint scaling if I do that terms that I
can produce a really nice result that
says if as soon as the system gets large
then the best thing for me to do is just
a bid it's to shade my bid according to
this factor 1 plus V prime B where B is
actually the value function and that
holds true regardless of what kind of
auction I'm dealing with so if I'm
dealing with a second price generalized
second price or first price I should
always shade my bid bye-bye according by
what I left by this amount and I should
bid what I should have do with that
value dependent on the auction so for
example if I knew what how to bid in a
generalized second price auction with
value V I would just shave my bid by V
by that amount okay so there's a nice
characterization and you can show and
you can show actually that to show that
you get a mean-field equilibrium at this
you have to do kind of a kind of lot of
dancing if you like you have to dance
around a bit create something slightly
strange you have to design divide the
budgets then the bidders into groups of
budgets in two classes of budgets and
essentially allow a resetting of budgets
to get the machinery to go through which
is kind of a bit of you know it's a bit
of there's a bit of kind of a merc unis
there so I'll skip over that but
essentially if you if you imagine that
you restrict yourself to policies that
are the same modulo restarts in other
words if I can randomly reset my budget
then you can show a renewal equilibrium
is something that does that and what you
can show that one exists in this setting
as you get the number of players go
large this is a mean-field equilibrium
so in other words what's that saying if
I that's saying if I look at the
distribution of my highest opponents
bids then a min feel bid such that i can
bid according to that and so this works
if there's a lot of auctions a period ok
so my
well strategy in other words it's just
is such that I bid against this maximal
bit of my opponents that turns active
for everybody does that that will create
the same distribution so then the
question is well okay that's that's very
nice in theory suppose I'm an advertiser
could I ever possibly implement this and
I suppose now I'm an advertiser rather
than that the platform so the problem is
then is that I don't know my opponents
distribution book so what could I do and
they face an interesting dilemma because
if I if I bid in an auction I get a
censored observation as if I win with
bid be okay then I know that the price
I've paid I know exactly what the assume
in this case I know what the price is
paid okay if I lose then I don't know
all i know is the price was above what I
bid so i would have to increase my bid
by some amount to win okay and of course
in this in practice your only charged if
an ad is actually clicked so how can you
deal with this one way around this is to
use essentially the kaplan-meier
estimator which is which deals with
these censored observation this is a
sensitive servation because you've only
essentially got information on one side
and if you do the kaplan-meier estimates
you can think of that as something like
a product limit algorithm so what i
could do is imagine you plug that into
an algorithm to estimate my distribution
function f and then use f you use this
thing that I know my characterization of
the true control given F to calculate
what I should do and then you can do
some long share some work with long 10
shown if you do that you can show as a
regret algun that that will achieve
something like that will achieve a root
t regret if you have seen that the kate
the km kaplan-meier estimator converges
suitably so this greedy probably product
limit algorithm is to say well I'll
estimate a maintain an estimate of F and
then I'll bid according to maximizing my
you know essentially maxima choose the
map of the bid that will be the optimal
bid for the setting were for the value
function if I knew that distribution
with remaining time T left and budget be
that I then receive us
sensitization of this kind of
distribution F and update that and I go
round okay so that's a relatively simple
algorithm the chompers it can be quite
expensive so a simple thing to do is say
well I could I could separate my
exploitation and exploration phase so
suppose instead I'll just use a bid
randomly for a period of time to see
what happens and then I'll just use that
I'll use that then to estimate the
distribution and from then on I'll just
fix my knowledge at that time and just
use the bids from then on okay so for
the first period of time I just randomly
bid and then ever after after this
period of time epsilon T I estimate the
distribution and then just calculate the
optimal bid using that estimate are not
updating it okay so the range of the
time I just use this to bid so I don't
update the distribution so that actually
achieves slightly worse which regrets
something like ordered tea to the
two-thirds but you win in terms of
complexity so that's the kind of
trade-off you'd you might be able to
make in practice we try various
experiments looking at first of all way
where we constructed the distribution
from adcenter data being ads and
simulating the model and the second one
was just violating all the modeling
assumptions just replaying historical
bait data to see how this / firm
performed sequentially and it does
pretty well I mean it's surprisingly
well so this is just this is the
theoretical ones i'm going to show you
now if you compare it to an offline
optimal algorithm which is more than you
possibly ever do you couldn't an offline
algorithm has to know what you would bid
you have to know how to bid given
essentially history of the future that
you can't see but you can see that the
competitive ratio of something like this
greedy product limit is pretty high it's
about about 90% she do pretty well
compared to just doing something like
q-learning which actually does worse
even so the standard machine learned
technique does less one in this case
because you're not making enough you're
not making use of the characterization
is simple characterization of the
optimal control and simple budget
smoothing we
should be to smooth your budget out of
the constant rate does pretty poorly and
lastly just to talk about sort of again
this performance complexity trade-off
which might relate to how can people you
know if you think about people learn
into situations you need to solve your
compute time problems so this
generalized product limit performs
reasonably well it has a kind of good
competitive ratio and this epsilon first
thing we're essentially you kind of only
learn for a period of time so it's got
larger a gret but it runs a lot faster
so that's the trade-off essentially it
can run five times faster but you're
losing ten percent of the accuracy so
that's the trade-off so this is just an
example here this is an example of a
distribution of market price as a
percentage of the maximum budget and
these are just various algorithms
playing out over time where this out
this is the competitive ratio so one
would be ideal and this greedy product
limit you can see is achieving about
ninety percent of the optimal where
something like this epsilon first is
going losing another is temp sent down
on that but if you look at the time it
takes the cpu running time in seconds
then the problem is this greedy product
limit as you as your percentage of
budget remains just really shoots up
where's the epsilon first is kind of
constant cost so it's it's way down
there so you've got this trade-off we
know what the issue there i think is
what happens in practice and if you're
trying to think about how people do what
do they do did they do this trade off or
are they doing something different so
this is a kind of i guess i've tried to
sort of say address this problem of how
do people learn in practice and hadley
learning automated systems and there's
basically admitted as a rationality gap
or the big gap these are just kind of
steps along the road in one sense we
know how people learn in simple systems
and they clearly don't behave according
to well-known Nash equilibria and
complex situations they seem to do some
limited learning but very often very
naive so in these large systems you've
got the question then world could should
you actually help people can you give
them a
kind of hint as to push them more in the
direction of doing some like best
response and if you do that you can
certainly show that me if people did
that that they an equilibrium would
exist but i guess the challenge is to
come up with computable incredible
strategies and and really what i'd like
to do is to complete the loop to link
these models to strategies that people
actually do because a minute there's a
gap as a gap between the models of what
people do and what they really do so
this is just steps on the road and I'll
think I'll stop there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>