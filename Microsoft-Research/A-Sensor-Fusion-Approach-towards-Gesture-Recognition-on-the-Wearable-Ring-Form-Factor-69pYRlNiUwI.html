<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Sensor Fusion Approach towards Gesture Recognition on the Wearable Ring Form Factor | Coder Coacher - Coaching Coders</title><meta content="A Sensor Fusion Approach towards Gesture Recognition on the Wearable Ring Form Factor - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Sensor Fusion Approach towards Gesture Recognition on the Wearable Ring Form Factor</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/69pYRlNiUwI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so good afternoon everybody it's my
pleasure to introduce Jeremy Jamison a
fifth-year PhD student from UMass
Amherst during his summer with us this
summer he built this audio and
accelerometer sensor based user input
device on that work on a ring platform
so Jeremy thanks for the introduction
Bodie so before I came from my
internship this summer I knew I was
going to be working on some kind of ring
platform we weren't exactly sure what
the application was going to be and what
we converged on was doing gesture
recognition and we do that specifically
by doing something called sensor fusion
so I'll go more into what that means
later now so over the last 50 years or
so I mean we've seen this rather obvious
trend where we have computers that are
kind of very far away from us that are
you know coming common closer to our
bodies right so in the 60s and 70s we're
using dumb terminals and maybe you know
remotely accessing computers that are in
a remote data center personal computers
kind of emerged in the 1980s where
people actually owned their own computer
laptops we finally had computers we can
carry with us smartphones have been
really popular over the last decade and
then as we all know kind of the big
trend over the next ten years is
wearable computing where we can think of
devices like Google glass like there was
someone wearing that around the building
today there's the pebble watch which got
a lot of exposure through a Kickstarter
and then augmented reality so that's the
oculus rift platform that you can
actually wear and experience augmented
sorry not augment it's like virtual
reality for playing video games right so
what are so we want to push this a
little bit further and have a platform
that's even less obtrusive so you
something that you could wear every day
and maybe not even notice that it's
there so why not a ring right so in
popular culture this has been kind of a
very popular thing that you have this
ring that kind of gives you some kind of
magical powers right so in the 80s there
is this television show where these kids
had these rings and they could summon a
superhero there's this guy here the
Green Lantern he can create force fields
using a ring and then of course we're
all familiar with Lord of the Rings you
know you have this magic
lets you disappear unfortunately I
learned very early in the internship
super powers aren't going to be feasible
the big reason I can think why are eight
is the energy bottleneck I mean if you
wanted to do all of these things you
need to store a lot of energy in a small
ring so on the left here this is a ring
prototype that bode had actually built
in it has a small area where you can use
buttons to enter inputs and this is a
picture of the battery that's actually
part that's in the ring is underneath
the board here and it only stores about
1 milli amp hour of energy which is a
pretty small amount so the trick that
you use to keep this ring powered so
here if you can see it there's a some
copper windings which is actually an
inductive charging coil and the idea is
that you can recharge this small battery
kind of opportunistically so if you're
wearing the ring on your ring finger and
you're holding your phone a lot of
phones now have NFC and you can kind of
opportunistically recharge the Rings
battery when you use your phone normally
throughout the day so actually this year
at a movie sisto bode and i had a paper
that explored that idea in a little bit
more detail so we were looking there at
a security application the big takeaway
that's kind of relevant to this work is
that we found that using like a very
aggressive harvesting strategy we can
harvest up to 30 milliwatts of power
from a phone that's a lot of power when
we compared kind of the low power
consumption costs of sensors now so this
is an opportunity right so we have this
remote charging source that we can use
to replenish the small battery on a ring
platform ok so what do we do with the
ring there are a few ideas that we
explored early in the internship so the
first was continuous health monitoring
so one thing that we thought about right
was monitoring you know something like a
like a pulse ox sensor or maybe doing
galvanic skin response to understand
someone's emotional state unfortunately
so we found that a lot of at least so we
found that the signal integrity that you
need you know for those different health
health related things for various
reasons I mean because the surface area
on the on the ring is so small for
example forget
vonyc skin response that would mean the
electrodes would need to be very close
together and you get a very weak signal
which isn't necessarily good for
detecting health and so that also kind
of we have this idea of doing a motion
based HD is maybe you can detect that a
user is frustrated and then maybe
somehow you know change web web search
results is something something of that
nature right but but we couldn't do that
because we couldn't implement those
sensors so instead we focused on this
idea of a gesture the ring as a gesture
input device they're actually a you know
a couple of ring like things out there
that do this they're a little bit clunky
right so the thing on the left there
that's basically a miniature trackpad
that you can wear on your finger so if I
was giving this talk and I wanted to be
able to flip through my slides I could
do that from a ring but we think that we
could do better and have something
that's much more you know seamless and
something that you might want to wear
you might be willing to wear all day so
yeah our project goal right it's
implement a ring based always available
data input device so we can think of
data input in a variety of ways so first
right there's you I actions so say
you're in a web browser you want to
navigate back and forth between pages
that you visited you know you could you
could implement those gestures maybe
scrolling up and down in a page that
would be another another to gesture is
another way you can into enter input is
kind of like a virtual keypad like we
have on our phones see of a virtual
keyboard and you can enter in individual
letters there's more advanced ways of
doing this with it's called shape
writing as the general yep doing
there are there are some there is
actually a couple of applications out
there were I've seen there's a ring that
kind of has just a passive NFC tag in it
right and basically when it sees that
tag ID it does some single action on the
phone right but we're looking at doing
you know more than just one action kind
of you know a richer set of inputs to it
to a device this upon that we all have a
microsoft yeah relate to that actually
there's another project all the events
to me
security part of effort
yeah so what we're going to be focusing
on in this talk is kind of kind of the
two last cases here so character input
and shape writing so basically looking
at characters as shapes right and trying
to detect those those shapes accurately
so we can emulate characters so there's
obviously a variety of challenges and
getting something like this to work
effectively so the first challenge is
related to energy so how do we keep the
ring always available for input when
we're only relying and you know this
harvested power from a phone and a small
battery so the second is sensing so if
you have sensors that are located you
know in this segment of maybe the index
finger or the ring finger you know you
might not be able to get good good
gesture just you know accurate sensor
readings to have good to be accurate
gesture recognition the third is
computation so how should the ring
process the raw sensor inputs should you
do all the processing on the ring should
you do some processing on the ring and
then and then push those results and
have you know something that has more
computational facilities do the rest so
I'm this trade-off between computation
and communication is also important
right otherwise you'll end up killing
the battery on the rain yeah what if I'm
fully before the rain is ages we just
refer to my
or just my top of the phone touch screen
because it's already there how about the
perception of military lives and your
body capable device odd or different
place there is not a problem how do you
think we did for such a life you look
for comfortable so you're saying you
have some kind of remote display and
wearing the ring and I want to be able
to interact with it your father's mo
display the stream PC it has too loud
okay
I think another word is here would be
the using perception how does the moon
sure yeah that would be a fifth
challenge right yep why tossing the DA
screen we were an actual finger that is
not yeah it's not yeah I would require
you know a user user study to understand
what that is yeah right okay so there
are several different approaches towards
entering symbols on different types of
devices so kind of so blackberry is kind
of on its way out but it was really
popular because it had this very
accurate tactile method of input where
you have these keys that you actually
press so a few advantages of pressing
buttons right you can do this at really
low power you're basically processing
interrupts and instead of you know
having to continuously process sensor
data it's very accurate right I get nice
tactile feedback when I want to push a
button you might be able to be faster
with buttons as well and key and entry a
couple of limitations are form factor
right so if I want to have a lot of
buttons I need a lot of space if you're
thinking of something like a ring you
don't have a lot of space to work with
and then and then how do you map a rich
set of symbols if you are kind of
constrained in terms of space how do you
have like a rich set of inputs right
another thing that became popular I
think starting with the Nintendo Wii a
few years ago is doing gestures in 3d
space one of the advantages here is it's
very verbose so you can move your arm in
very large areas and do a lot of a lot
of different types of gestures and these
motions might be really natural to
people but one of the issues here right
so there's there's lack of intent so say
um if I'm wearing a ring and I wanted
and I wanted to process things in 3d you
might you don't want the user to have to
press a button right so I'm moving my
hands around all during the day and I
don't want these spurious movements to
be misinterpreted as gesture inputs
right so this is a problem and these can
also be low accuracy right so it's kind
of hard for me to kind of perceive where
I am in this 3d space in front of me
like if I wanted to accurately reproduce
the gesture it's not exactly clear where
the exact boundaries are you also to
deal with things like like variable
orientations because you're in
3d space and then it's can also be
higher power because you have to stream
out maybe a lot of accelerometer
readings in order to localize the thing
in 3d space so third way that we that
you can consider is doing something in
2d space so one of the advantages is so
this is one of the first things that we
learn in school right so handwriting we
learn how to write letters on a flat
surface uh so some of the reasons why
why this is this is attractive right so
you get haptic feedback so when I'm
sliding a pencil on a surface i can feel
the vibration through the pencil into my
hand and I know maybe how fast the
objects moving if I'm thinking if even
just writing doing finger painting right
I can feel my finger on moving along the
paper this is something natural because
I people have been doing this you know
for ages you look at like cave paintings
and tablets I mean this has been
happening for a while and surfaces for
doing this are available all around us
right so if you aren't restricting
yourselves like capacitive touch
surfaces you know maybe if you're
considering tables whiteboards maybe
even it could be your your trousers
right could could be as a 2d surface
that you would use to enter input I mean
it's always readily available so a
couple of the challenges here right or
surface detection right so how do I just
a bag just in big you with my hand is in
the air and when it's actually on on a
surface right that's an open question so
it's less verbose than 3d so I'm kind of
constrained maybe do a smaller space
where I can where I can move my finger
in a comfortable way and then power is a
question right so we don't really know
how much power it's going to cost to do
the surface detection okay so how does
how does a user input 2d gestures on a
ring so this is our idea of how you
would do this right so step one so the
user will initially tap on the desired
surface right that's when you first put
your finger down on the surface that you
want to interact with step 2 so this is
actually an optional step this is this
is just so you have this challenge right
that you need to understand what
coordinate space you're talking about so
the user might first have to enter some
reference gestures to let the system
know kind of how the user is oriented
relative to the surface and then thirdly
the user enters a series of strokes on
the surface to interact with some other
device
okay and then the fourth step which the
user doesn't do and it's implicit right
is they stop entering strokes and then
do bring we'll go it back into a
low-power state where it's not
interpreting gestures anymore so with
the one milliamp hour battery that will
actually fit in a ring-like platform so
based on some back-of-the-envelope
calculations we did on hardware that's
available on the market that we actually
used to do some prototyping up so we
found that given 3700 micro Joule
battery capacity you can do around 4,000
gestures on a full battery and given our
previous results on NFC harvesting you
can recharge all that energy from the
phone in about 20 minutes right so the
idea is that you're using your phone
periodically during the day and that
battery will kind of keep being topped
off and then when you want to do another
type of interaction without the phone
there will be energy available for you
to do that so we haven't actually
evaluated how effective that is would
require like a longer term you'd have to
have people wear these things and
understand how often they use their
phone but that's another another topic
okay so the first challenge that I want
to talk about right is segmenting
gesture data right so we need to know
when one gesture starts and another
stops right so this is just me sitting
at my computer and I'm sliding in my
trackpad and I'm actually in putting an
up gesture right now right but it
doesn't clear from the video at least
whether I'm moving up or down right you
can't really tell when my finger is
raised off the surface or when it's down
on the surface it's kind of this this
fuzzy notion right so how do we how do
we do this well doing this there's
there's two main challenges so the first
of which is detecting whether or not the
finger is on the surface right so first
we need we need to know whether it's
upwards down and then and then if it's
on that or if it's on the surface and
the second is distinguishing between
different gestures well the finger is
moving on the surface so we know that
the finger came in contact now what is
it doing well it's on the surface okay
so the first part of the talk is going
to be how we detect a surface so related
to those steps that I showed earlier we
want to first detect when a finger
and the surface so a great way to do
this so you use an accelerometer right
so what you look for is a sudden
deceleration in the z-axis right so when
the finger stops when it hits the
surface you'll see a spike and one of
the the great parts about currently
off-the-shelf available accelerometers
is you can do a threshold-based wake up
for very low power costs so for less
than a micro ampere you can you can
determine you know when these when these
spikes occur and then the second part is
continuing to detect the finger as it
moves across the surface so surface
friction it's actually it's it's emitted
it it's emitted from most surfaces as
audible band-limited noise right so what
we're doing here is we're using a
low-power microphone and some signal
processing techniques to observe the
surface friction noise and we're able to
do this at reasonably low power
consumption less than a million within
with an optimized circuit design and
what this lets us do so if you combine
this with the accelerometer another
added benefit is it reduces false
positives from spurious tap so say if
I'm wearing this ring and I'm nervous
and I'm tapping you know on the side of
my trousers you'll have a lot of false
wake up's and waste a lot of power so in
order to keep the system up in an active
state what you look for is a tap
followed by the surface friction noise
and if you don't hear both you quickly
turn the system back off
smoke up before I don't know I think I
know that there isn't one of the
prototype now but you could envision you
know maybe quickly pulsing an LED
something something like that to get / I
see
I have something that Senator Robert
r-tx is is always like vertical we can
actually 55 with csa you can actually
use you don't just have to use the
z-axis you can't and i think it looks
for actually any of the axes I just said
z axis here because I'm assuming you
have a horizontal surface and maybe
that's the one you're looking at in
particular but power is it for the the
wake up power so that is just for the
just for the accelerometer right so I'll
have a slide on number for this a little
bit later but basically you're in this
low power state where you have
everything to sleep the accelerometer is
using only point 27 micro amperes of
current and it's just it's just listen
it's just waiting for these these spikes
I'm gonna have more slides on that
uh-huh yeah all right go on okay so this
is our initial experimental setup so I
have to be honest it's not a ring yet
that's kind of a work in progress but
but what we built here is this is a
bicycle glove i bought at the commons
and this is a it's a commodity
accelerometer on an evaluation board and
it's done it's in the location on the
glow of wear a ring would be located so
the sensor data that you get from it
will be reasonably close to what you
would get from a ring and then it's
connected to a microcontroller that's
that basically outputs the accelerometer
readings over a serial port we also have
a low-power MEMS accelerometer basically
on the other side of the finger on the
opposite side of the finger from the
accelerometer and we basically are
outputting all of the data from that
microphone into a PC sound card so that
we can analyze you know what these
surfaces sound like so the index finger
is easier right
yeah but nobody wears wings of
hey mine they might I mean you you yeah
it's possible that you could do it on
the ring finger as well maybe maybe the
motion within the two are correlated
that we started with the index finger
alright so the first part right of
detecting the surface right is is the
finger impact and what this is is this
is just the time series trace of data
that I get from the 3-axis accelerometer
and these spikes that you see you know
around around 2 g's here that's actually
when the finger is striking the surface
right so I mean that it's reasonably
easy to detect that right but what about
the other part right so I said there are
two parts so first there's the impact
then there's the sound that the surface
makes so let's do an evaluation kind of
right now here what is the sound like so
the first is really loud sounds this is
a piece of styrofoam right and this is
what it sounds like right it's kind of
like almost like nails on a chalkboard
very loud and easy to hear this is going
to play a little with you okay and then
the next is a surface like wood right
you might not even be able to hear this
right we are actually able to pick it up
with the microphone and then the other
scenario you can think about right is
when there's external noise that might
be swamp the signal that you're looking
for so here's some children playing on a
playground and i was playing this sound
from a laptop next to where i was
performing you know a series of gestures
with the ring and i'll show in one slide
that we're actually able to disambiguate
the two the two signals so i actually
evaluated 12 different types of surfaces
and they all produced some some common
frequency band limited noise and in
addition to those evaluations we did I
there is an inner at this journal paper
from the acoustic Society of America and
they kind of you know confirmed our
suspicions right that a lot of surfaces
have these common frequencies okay so
these are actually results the results
that I referred to in the last slide
about you know what this looks like when
we have the noise of these children
playing on a playground and then me
performing
series of gestures right and so on the
left here so these red regions so okay
first I should explain the plot here so
what we have is a spectrogram the x-axis
is the frequency components of the
signal that we're looking at the y axis
is the time that that spectral content
was was present and so what we see here
all this red is the children playing and
these yellow bands here that are
separated by blue bands this is me
dragging my finger across the surface
for about a second picking it up
dragging it again for a second right and
so there's there's a lot of space here
right where they don't overlap one place
where they do right so they're in the in
the lab rows in the experiments there's
actually some servers on and I generated
this yellow band that's kind of their
present through the whole trace but
there's plenty of frequency space around
that that doesn't overlap so if we
wanted to be even more immune to noise
there's a couple of other techniques we
could use so we could use something like
dynamic filtering where we have a
programmable filter that can look at
different different regions of the
frequency content and then the second
might be time domain analysis you have
really short lived noise you could
filter that out because that doesn't
look anything like these surface
movements right it's much shorter all
right so to do this audio processing
there's there's three steps that we need
to do so we need to do processing right
because the ring isn't going to be able
to look at raw audio samples you'd have
to sample audio at 44 kilohertz you very
quickly drain that one milliamp hour
battery I showed earlier so we use a
hardware filter so first you apply a
band pass filter you know that that's
constructed to look at that region
that's usually separated from say human
speech then we apply some gain right so
we need the signal to be bigger in order
to be interpreted by a microcontroller
say with analog to digital conversion
and then we also want to use something
like an envelope detector so we can
actually sample that signal at a
relatively low frequency right and
understand when that noise is present
and when it isn't present so what does
this look like after band pass filtering
so what we see here this is another
spectrogram plot this time I was drawing
the letter L on a surface so these red
bands that you see here that are very
close together those are the individual
strokes of the letter L right
yes so we're so we're able we're able to
do this you know with filtering and gain
and so after the envelope detector we
can actually see the two peaks that
correspond to the strokes of the letter
L right so this is a promising results
this is all implemented in MATLAB this
isn't from an actual Hardware
implementation of the filters so we were
kind of guided by these results to do an
actual filter design and this is what it
looks like it's a little bit of a mess
right now but what we have here is so
these two boards here are the envelope
detector this one does some filtering
and some gain we have a microcontroller
the same accelerometer from before and a
similar microphone from the previous
setup so we chose the particular
components that we use to be so that the
op amps that are used for the filtering
are very low power so all of them
together use 620 microgram piers the
accelerometer even when it's active uses
only three and the microcontroller I
mean I think that there are more
efficient ones in the market the one
that we used is around 270 but if you
sum all of those up you you have less
than our milliamp a budget right and
then in standby right so this is with
the accelerometer in its low power mode
and when the microcontroller is also
asleep we consume around only one micro
ampere of power when everything is kind
of off and waiting for a user to
interact with the surface okay so let's
revisit writing the letter L but this
time with our actual hardware so these
are traces that were actually output
from our our serial port right so
basically our microcontroller was just
reporting the ADC samples that it got
from our audio filter and we're actually
able to distinguish the letter L so
these are spaced a little bit further
apart in the previous plot so this was
actually from an actual user so I had a
few people enter gestures for me and
this person happened to enter it more
slowly than I did in the previous but
because of the filter characteristics
even if the strokes were closer together
it would still work yeah
if I write a tea will be recognized as
oh yeah I'm gonna have a few results on
that later so this was just kind of to
prove that the envelope detector is
doing the right thing so this is just
the fact so L contains two strokes right
I move down and I move right all this is
showing is that I can get two strokes
per L right they might not be the right
strokes but there are two that that's
what this plot is showing ok so to get
ground truth about about how accurate
this surface detection is in the
movement right we actually used a
off-the-shelf capacitive touchscreen so
what we did right we recorded the
coordinates of the finger on the
capacitive touchscreen over time so we
knew whether or not the finger was
actually moving we also recorded audio
from the finger from the finger motion
right and so then what we did we
compared timestamps of the audio and
also the timestamps of the touch screen
coordinates we found that they
correlated pretty well I don't have a
plot here to show that but it proved
that our our audio detection scheme is
doing the right thing ok so I've just
described how we detect that a finger is
moving on a surface rights i'm going to
go to the second part which is so how do
we construct a symbol so based on you
know maybe movement on a surface right
so one thing to note right as symbols
can be arbitrarily complex you think of
you know maybe not maybe not American
English where the characters are simple
but maybe like so it's a Japanese or
Chinese like you can think of very
complicated symbols that a user a user
might answer so one naive strategy you
might do is you might try and put all
the computation on the ring and compute
the entire symbol and then tell the end
device this is this is the character
that they entered this could be
computationally challenging so you might
need to use advanced the machine
learning techniques things like language
models might have to update vocabulary
specific to a user may be one user
writes a little bit differently than
another and you might have to do
customization there so I'm instead of
trying to compute the entire symbol on
the ring we break it up into a series of
strokes right so for example if i write
the letter A that's going to be two
diagonal lines in different directions
followed by a horizontal line
so I would send each of those segments
to the end of ice and let it figure out
that that's the letter A so kind of an
architectural view of what we're doing
here so at the bottom level right we
have fingers on a surface and we're
detecting some signals right and so the
ring is detecting those signals
converting them into strokes and then
those individual strokes are sent to an
end device maybe it's a windows phone
and it's being converted into symbols
and words and so yeah an example that
I've been referring to is handwriting so
this is the letter B the letter L in the
letter W this is how it's decomposed
into strokes and kind of the order the
ordering of those strokes and then the
way that you might report this to the
end device maybe you have different IDs
corresponding to the different stroke
primitives you support some timing
information right it might be important
how quickly how the strokes are grouped
together that might help disambiguate
one symbol from another and maybe even
in a character like if I write the
letter A for example the two diagonal
lines might be while I'm on the surface
and be close together and the horizontal
might be further apart so that timing
information could be helpful in
interpreting it later and so this is
great because we can send a few bytes of
data instead of sending out 400 Hertz
accelerometer data which would
completely kill our battery so the two
core system challenges here right again
so it's identifying the beginning and
the end of a stroke reliably and then
the second is using sensor fusion
between the microphone and an audio
circuit and the accelerometer to
understand the relative directional
properties of an individual stroke okay
so this is a user that I was actually
collecting data from wearing our
prototype and what we have here so the x
and y coordinates or oriented so they're
facing a table and a piece of paper so X
is positive in the right direction and
then y is positive in the direction
facing away from the user right and so
what beret actually sees is something a
little bit different right so you're
going to have some tilt in the X and Y
so basically what will happen is the
part of so gravity is detected by the
accelerometer and then it's going to be
found in some set of these axes you have
to normalize you know kind of the
coordinate space of the ring to the
surface and then the other thing that
can happen right is the user can
actually have their have their
finger rotated right so that will
actually confuse your x and y axes so so
combining the microphone and
accelerometer how does it work so so
step one so after that tap happens and
the finger is first touching the surface
you can compute the finger angle
relative to the surface during idle
periods that's the get rid of that Z
component from gravity and then the
second step right is identifying that
the finger is moving on the surface
right so you get that audio envelope and
that lets you know when the stroke is
actually being performed and then step
three you can observe the finger
accelerating and decelerating in
different directions depending on what
gesture the user is is inputting and
then you can use some physics based
heuristics to figure out what that
direction is right so I mean if you if
you want to move a finger you have to
accelerate if you want to stop you have
to decelerate so the accelerometer is
going to definitely pick up those
signals so kind of a laundry list of
different stroke primitives that we that
we want to deal with so first there's
the easy ones right so there's up down
left and right we're basically you're
just looking at the signs of the
different axes of the accelerometer then
kind of a medium difficulty i call it
medium because you need to actually look
at combinations of the of the x and y
axes to detect what type of diagonal
motion you're talking about and then the
third is hard i call it hard right
because now we're actually we actually
care about kind of the shape of the
accelerometer malaysian right so you can
detect things like centripetal motion in
order to understand that a that a curve
is happening as opposed to a straight
line so during the course we want to do
all of these that that's the end goal
I'm during the internship internship we
focused on the easy and the medium
difficulty strokes so now I'm going to
go into kind of how this works so this
is data from an actual user of
performing an up gesture so the red plot
here is the output of our envelope
detector right and so what that lets us
do right is so say if you set a
threshold when you look at your analog
to digital converter and you see that
the voltage went above say point 2 volts
and goes back below point 2 volts you
decide okay that's the boundary of when
the finger was moving and then I can
draw a line down the middle
so there's the first half of the stroke
and the second half of the stroke and so
what we see in black and green so we
have the x-axis in green and the y axis
in black we actually see the finger
accelerating and decelerating so we have
zero so because the axes are actually
backwards here some negative means
acceleration and positive means
deceleration but if we look at the two
halves of the finger movement right we
can we can clearly see this by looking
at the y axis and that signal is much
larger than what we're getting on the x
axis right so we can probably figure out
that that is a vertical motion as
opposed to horizontal so let's look a
little bit more at this right so I just
took away the envelope detector plot and
this is just the accelerometer and so
basically what we do it's very simple we
look at both halves of the of those
intervals t1 and t2 and we do an
integration right so we integrate the
first half of the x-axis second half of
the x-axis and do the same thing for the
y right and so based on the relative
signs of those integrations and then
which one where the total integral is
larger than the other right we can
determine which access had the motion
and what direction that was whether it
was forward or backward or left or right
and in this case we did up so we see the
the prominent access is the y-axis right
yeah so this is what we did we compute
we compared the integrals that the
dominant axis during the two halves of
the movement period so what is the base
yeah that's computing this this this
integral rights with negative so
basically I'm just I'm just summing over
the whole thing right and then
representing that entire sum as kind of
the sort of the you think of it as like
the average I think it's the average
speed right over the over the first half
the total integral value as what I've
computed over that my you and your
gesture is from bottom to top and then
return or just stop vomited on just
bottom the top and then we define down
as top to bottom
okay so so it okay did these are actual
this is the raw data right that is from
the ring but because I know how the
accelerometer is actually mounted on the
ring itself I actually so in software
you can you can reverse the axes and
figure out the sign you'll grab sure I
do on Tomasulo you guys are going
hitting the peak mmm and then coming
back down this is just the magnitude of
the audio signal right oh so getting to
people such a transitioning yes yes
exactly so I'm speeding up during the
first half of the finger motion and then
I'm slowing down when your fingers at
the top you don't bring a favor bread
you do not bring a finger banged up
right so then in order to do up down
left and right it's rather simple
process so i just filled out a table
here for the signs you're looking for so
say if you correct for the axes well so
what you're looking for is an initial
acceleration and the y axis then a
deceleration in the y axis and you're
looking for basically no activity in the
X the opposite is true for down and then
you look at the other the x axis for
horizontal movements and you can come up
with a very simple algorithm right that
that does the integration and then and
then compares which axis is dominant and
what the sign is so this isn't this
isn't too hard so one of the advantages
right I'm only looking at these four
gestures is that you're relatively
immune to rotational drift so there's 90
degrees of difference right between say
up versus right or right versus down so
if the person kind of drifted when they
were entering the gesture and maybe
entered something that looked like a
slight diagonal line you would get what
they intended and actually get up so one
of the cons those are limited to four
features so that's not to say that we
couldn't say string a bunch of these
horizontal and vertical primitives
together to do something more complex so
you can do that but you might want to
have a richer set of strokes to begin
with yeah
so it's not actually that sensitive to
that threshold so basically the
important part is picking it to be the
same on both sides so as long as you
pick point 2 volts on both sides you're
going to have a symmetric view of the
motion and when you do the integration
the math works out so you want something
that's wide enough right to be able to
see as much of the signal as possible
but not so wide that you're actually you
know getting some of the accelerometer
noise as part of your competition right
now it is there's no simple heuristics
and observations but the workers to
these people
okay so to understand how well this
works so first we looked at four gesture
classes up down left right so I had five
helpful people enter gestures for me and
I looked at these four gesture classes
so I asked the participant to basically
enter one of these gestures 10 10 times
in a row for for each of for each of the
gestures I collected all the data with
the glove I outputted it over the server
laporte and then I analyzed that offline
using MATLAB so basically what I did as
I implemented that simple algorithm and
matlab and saw if I could you know
reliably determine which gesture was
which so for one user this user did
particularly well we calculate so we
compute the correct gesture among among
the set of among the set of gesture
gestures that are available a hundred
percent of the time except one time we
computed we falsely interpreted the down
gesture as a left gesture right that's
not perfect I mean not not so now but
just to look at if the user the input
how are they actually that's actually a
great idea and I should have done that
so what I did do so after each user was
done entering the gestures I took
pictures of their hand and kind of what
kind of orientation they were using
during the whole session but to
understand the dynamics while this was
happening that would have been really
valuable and the next time I collect
data that will definitely be something I
will do and then when we look across all
five users of course things degrade a
little bit but not by much so again we
confused d down and the left gestures a
little bit a little bit more so we went
down to an eighty-six percent accuracy
there they were all right so the global
it's a right-handed glove so I didn't
actually check but based on the yeah
okay so if we add a little bit more
complexity right we can think of doing
diagonals and the way that you do this
so first right we have up down left
right and they look exactly the same as
they did before but then we kind of have
this fuzzy notion right of where if
you're doing a diagonal line you're
going to see some amount of activity in
the X and then also some amount of
activity in the Y and if you're asking
people to do diagonal lines if they were
drawing a 45 degree angle and if your
calibration is correct you should see
the same magnitude right but in reality
you know users as they input the
gestures they're going to drift a little
bit that might actually even rotate
their rotate their finger so they might
be entering the angle correctly on the
table but then you misinterpret it
because your axes aren't aligned anymore
and so in this case the user was
entering a downright store this is after
I do the integration you see you know
comparable amount of activity in both
the X and Y axis but the signs are
opposite so that's how you figure out
kind of the directionality and yeah as I
mentioned it's it's very susceptible to
individual user variations and also to
finger rotational drift so so the first
you know could be fixed if you had a
scheme that kind of adapted to different
users if you used one of these more
advanced machine learning techniques and
also that type of approach could also
help you handle more of the rotational
drift if you actually wanted to
completely solve the rotational drift
problem if you wanted to completely
solve it you would have to
a gyro but that it turns out that gyro
is right now costs significantly more
power than an accelerometer so I mean
we're trying to avoid using that it's
certainly something that you could add
if you were willing to deal with the
bigger battery so first I'm going to
look at what I what I call the best user
so they actually got almost a hundred
percent accuracy across all 8 gestures
right so in one case uh so it is the
down gesture it was misinterpreted as a
down left and that's you know that's a
reasonable mix-up right so maybe when
they went down that time it looked more
like a 45 degree angle than like they
were going to down yes yes it was it was
neither myself or bode I might add yeah
um so when we add all users to the mix
of things actually degrade significantly
right so in the worst case so when we're
entering its the up right gesture right
so we only get it right fifty-four
percent of the time twenty percent of
the time we think that it's right and
then twenty-six percent of the time we
think that it's up right so they were
some of the users we're probably
entering an angle that looked more like
right than than a diagonal right or down
then it done a diagonal that's in
between so this is something that we
want we want to address in the future
using some lightweight machine learning
approach SPM was actually one of one of
the suggestions that they came to us
from a machine learning expert but I
mean this is an encouraging result this
that this is possible right so this is
good enough that Navy machine learning
can help push it up to maybe eighty
percent plus accuracy I don't know
constructed before the experiment too
trying to draw straight lines yeah it
wasn't specific I didn't say draw 45
degree line I said draw go gonna draw an
upright gesture right so maybe if I was
more clear maybe I would have gotten
more accurate results but I wanted to
kind of I mean I wanted to observe
variation and users how do people
actually use this thing yeah yeah I'm
good they weren't yet there were no
reference lines drawn on the table
either so what I had was a blank white
piece of paper and they use their
imagination haha Oh slight math mistake
there ain't I yoksa wrong with her
access to that rotary table
yeah i mean i can i could sort of i can
sort of compute that for each individual
each individual stroke that users
interpreted so i can actually adjust for
it after the fact so when i do the data
analysis right at the beginning I I
calibrate the axes and then I leave it
alone for four when I look across the
whole trace but if I wanted to determine
how much it drifted I mean because I
know the ground truth of the gesture
that they entered I could actually you
know adjust adjust how that angle is so
that communities a truth versus a
question bias for example one thing i
know this is when I don't throw would
good night I will always keep my fingers
like this one yeah but that's a nice to
finish me you're not doing gravity
vector extraction we became I am you are
okay and where are you doing that also
I'm doing this when the finger first
touches the surface and it's not moving
yes you have enough holiday to do right
now I'm doing an offline in matlab
priced yes what you're doing is it's
pretty much be baptized this angular
select four buckets and then usage table
lookup yes we're not computing sine sine
cosine yeah but then that transit are
ready to you their protection yeah sure
sure that doesn't help that's for sure
yeah okay so now that I've kind of shown
you what 8 gestures look like I want to
show you what combining strokes together
looks like so I talked earlier about
combining strokes into doing letters and
so this is actually a user entering the
letter Z that's a combination of
diagonal lines and horizontal lines and
the four peaks you see here in red are
individual strokes of Z so this is
someone that druzy you know with the
line in the middle so you see these are
the three first strokes of the Z right
and then the fourth is the line in the
middle alright and so this is just an
example of three instances of this
particular user entering Z when you look
across so there there were two users so
in the general case it didn't do that
well right but across to users I was
able to get seventy percent accuracy and
detecting the lead
z just using these simple heuristics
that we've developed so far so future
work for this right is doing what i call
advanced gesture detection so if you
have something like a left circle up
gesture so say I'm writing the letter B
right and Abby will consist of a
vertical line and then kind of two half
circles to the right and maybe the
direction that those half circles are
made is important to how the letter is
constructed so we can actually see some
interesting features of those motions on
the accelerometer right now we're just
not completely sure how to deal with the
signals right so for example I have two
entries here from one of my users where
they did a left circle up and a left
circle down and so for example the
y-axis here so you see that it's concave
and convex on different parts of the
curve during the first and second half
right and then for example another thing
that we noticed rates so in the x-axis
if you look at kind of the the energy of
the signal it's more there's more energy
in the signal on the left half but on
the right so that might be able to give
you your directionality so one of those
two axes will give you directionally and
then the other might tell you that
you're drawing a circle right because of
the characteristics of the centripetal
force I'm not going to say much more
here because this is all fairly
speculative but I think a machine
learning approach might be able to deal
with more complex signals like this yeah
slide with the zebra were you dissing
figuring disambiguating that from other
characters like an S or a name not not
in this case so this is basically the
user entering the letter Z ten times in
a row and even that is a little bit so
I'm not doing any time domain analysis
right now either so I think the the key
thing right to just to distinguish
different characters from each other
would be you know the gap between
groupings of strokes but this was just
knowing beforehand the letter the letter
Z and then trying to figure it out based
on the sequence you get that's right
okay so that that's all i have for kind
of current work so so to kind of
conclude i showed you the sensor fusion
approach that we use for detecting
gestures on a ring using an
accelerometer and a microphone so our
next steps right so we want to add more
audio noise robustness so there are a
couple things i mentioned about
this kind of time domain analysis of the
envelope signature to know you know what
to spurious noise and what's a stroke
and the other way might be having this
adjustable filter right so you could
look at the region of frequencies that
don't overlap with things that you
detected as noise maybe you can change
the characteristics of that filter as
the user is using the ring okay so next
you know being able to adjust for finger
rotations using using reference gestures
so the idea there right is so maybe you
could even enforce this right so maybe
like once every ten strokes you have the
user draw a vertical line and then a
horizontal line and if they've changed
their finger orientation a little bit
you could use kind of that reference
gesture as a waiter to rely on your
coordinate space the third thing so
machine learning so you can use an svm
classifier to actually look at all 12
gestures including the curves so so far
i just showed heuristic based results
for doing eight we'd like to be able to
get up to 12 and to do complete letters
and sequences of letters we need to do a
more extensive NFC harvesting evaluation
right so say someone is wearing a form
factor ring and they're using their
phone during the day how much do we
actually get in practice so in our
mobius paper this year we actually did
some analysis so there is this live labs
project at Rice University where they
had lock/unlock traces from phone usage
right and that is indicative of an
opportunity that you have the harvest
energy from NFC because when the phone
is unlocked you're able to harvest power
right so by looking at those kinds of
characteristics you get an idea of how
often you could recharge the ring next
really important thing right building a
form factor platform so the components
that we've chosen thus far right are
amenable to miniaturization right so
these are all just simple off-the-shelf
op amps that are available in smaller
packages there's nothing that would
prevent us from putting this in a ring
sized object because we're we've we've
designed it around a small battery right
it's it's a power efficient
implementation we need to do more user
studies doing things you know like
different different characters in the
same session looking at you know having
a having a camera to be able to better
understand how people move their finger
around while they're performing gestures
and then finally doing an end-to-end
evaluation with the ring as an actual
you I device right
so maybe I'm in my living room I'm
sitting at the coffee table now I'm
playing games on my xbox and then I want
to decide I want to be able to navigate
around that dashboard and select
different media maybe a different game I
happen to be wearing the ring so instead
of me I could use I could use the table
right so maybe I'm not even playing a
game and I don't want to use the
controller right this might be like a
more a more seamless way to interact
with things that are in your living room
um so of course I'm a bunch of it
acknowledgments so first won't
acknowledge Bodie has been a great
mentor a lot of really valuable guidance
and steering the project in the
direction we took of course thanks to G
and the rest of the center's an energy
group for for having me as an intern at
a lot of really valuable discussions
with different people in the lab that
helped mature the project I had a couple
of discussions with metallo posen and
Tim Peck so one of them as a machine
learning guy and the other does stuff
with with you I so they had a lot of you
know nice ideas that we incorporated of
course all the people that contributed
gesture data and finally my fellow
interns add a lot of nice discussions
with people i mean sometimes the things
that help the project the most are you
know random ideas that people have over
a dinner conversation so thanks to them
and and thank you all for attending if
you want to get in touch with me after
my internships over this is my email and
at this point I'd be happy to take any
questions yeah you use the X and the y
axis for
we provide a defined the symbols do
something the z-axis with their
television um so sort it could write so
if the finger tilt changes along the
z-axis right I mean maybe that would
give you additional hints that that
might be able to let you tell one
character from another for example it
could also let you more accurately
choose the beginning and end of the
audio envelope I mean yeah yep logistics
basically acceleration so we detect that
the movement starts just based on the
sound and then after the fact right
based on when that envelope went above
and below a threshold that's where we
know to look at the accelerometer data
like that throughout the day people
death surfaces all the time it gets
acceleration us all the time how to
differentiate the drama pathos surface
the edge of my glasses nice crossing my
head mean that's my legs I could
appreciate that there's an actual just
ring the best be great yeah well so so
personally have a strong you can you can
enforce like a strong user tap to start
interacting the device right that might
help eliminate some of the some of the
tapping type things maybe I'm not
tapping really hard throughout the day
that maybe it might happen sometimes but
less often so maybe you can do more
careful frequency analysis right maybe
not all surfaces look exactly the same
on the other bit too right is to say if
you're trying to identify different
different gesture inputs and all the
time I'm just getting garbage right I
mean obviously you would probably want
to turn the things I thought in a slide
yes I just as happy right dimitrios was
saying that maybe I tapped my face and
then I slide slice its tap twice in his
life yeah and if that's not good enough
three times and two slides regardless of
medication
sensors
be sensitive to the kind of us
and greet touch I mean in speech
recognition is neither can recognize you
common mistake is the human subject will
speak louder or a slower that's actually
making sports so in this case if I touch
say harder and so what codecs of
characteristics you seriously Sosa based
on oh so you're saying the users
emotional state the characteristics of
the way that they enter strokes
characters might change that might yeah
may have a chance to learn adapt to the
device so that even made mistakes the
next time they comedian how did I just
that compensate that so so either so do
you see this offer I think this device
basically everybody will have a learning
curve to adapt Cheryl yeah yeah so you
do need morning in both ways right so
you can have the ring learn what a user
does and also you can have the user
learn when their gestures aren't being
input properly right so one way you can
do that right is so maybe you have like
a plug-in or something on the device
that you're interacting with maybe you
have some non-obtrusive something like I
like a colored region or something that
lets you know whether your inputs are
good or bad you could also think of
having something like you know a
multicolor LED that turns on very
briefly on the rain to let you know kind
of how you're doing but because they're
moving really fast yeah a little bit
slowly sure that isolation could be
slimmer yeah there's literally I think
we'll be as a reducer learning just a
really most likely all of these things I
will be if I the visual or whatever
yep and you do segmentation hmm people
writing multiple letters in a row any
you know which stroke which shows go to
get it if I motor right so okay we don't
actually have a technique developed to
do that but our intuition right is that
strokes that correspond to one letter
should usually be grouped more closely
together than ones that are part of
different characters so I don't know
whether or not that's true it's probably
true you know maybe eighty percent of
the time and then twenty percent of the
time you have to do something right
right so the context of the use case
matters right so maybe if i'm doing it
depends on what type of texture entering
or i mean if you're doing simpler
gestures that might not matter so much
but character systems like Palm Pilot
and grants brain predefined characters
you have to use because they couldn't
recognize the answer letters but I
relied on the fact that they could
detect what you picked your finger up
yeah but you can't actually even detect
when the person just got a finger off
versus down as long as it's still right
so if I do it up an end to the side
I might think up in between or not
necessarily
well that's not necessarily true so you
might be able to detect something from
the accelerometer right now we don't we
don't depend on that right so you might
see a change in the z-axis to detect the
finger is moving up but I mean we do
know that those are two distinct strokes
but we don't necessarily know right
weather right now whether or not the
finger has been lifted yeah vertical and
horizontal mcatee plaza now right well
so again you can look at the gap between
the strokes so I looked at a lot of the
user data right and say for example
you're writing the letter A and you have
those two diagonal lines those two are
spaced very very close together let's
say if you're writing the letter T
you're drawing a vertical line lifting
your finger moving over to draw the
horizontal you see a lot more space
between the two movies given that
writing that does give your hand is the
most possible to do in that it may not
be the killer scenario for this
application then manipulate some you
guys right yet maybe maybe I'm drawing
exodus triangles circles squares you
know that that kind of thing rely on the
gap time between the strokes better than
angry angry strokes might start to
affect some things right because people
will do things very slowly and
deliberately
and then I can't use that direction it's
true happy relative that's why you put
all that to the my family and to the boy
that's when you go to be lazy once again
we did it the GSR sensor to work so we
know how angry they are ya</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>