<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Collaborative Computing and the Physical World | Coder Coacher - Coaching Coders</title><meta content="Collaborative Computing and the Physical World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Collaborative Computing and the Physical World</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TUvOLaQ5iuc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so let's get started it's really my
great pleasure to introduce to Greg
hugger Greg is professor and also chill
of computer science department at john
hopkins university John often you know
is a very well-known for medical thing
Greg used to work in computer vision for
gender Lola bodies but a little more
evolved into vision for surgical
robotics I guess and he has been in
church now in a lot of other things like
interactive large display and today he
would talk about collaborative computing
in the physical world all right thanks
in you it's a pleasure thanks for all of
you for for coming today and listening
so when I was putting this talk together
I you know I kind of chose the title
intentionally to change a little bit the
the thought pattern of the usual talk i
give which tends to revolve around
collaborative systems specifically in
robotics and i wanted to kind of broaden
it a little bit and think more about
collaborative computing beyond robotics
including robotics but really computing
in the physical world and a part of what
i started to think about as i was
putting it together as you know what's
the difference between where we are now
in computing research in this area and
where were we roughly a decade or a
decade and a half ago so this is
actually this is a slide for Rick so
this is work that you know comments came
from my lab really around I guess the
the mid-1990s to 2000 timeframe and I
guess the interesting thing to me really
is a lot of the things that I see today
we're going on back then we were talking
about them you know so we've got you
know this looks suspiciously like what I
guess used to be called the Microsoft
Surface that we were developing in the
lab gesture-based interaction you know
real-time feedback from robotics if any
of you remember Kintaro toyama he
actually built that video game that you
saw go by there so interactive video
games so certainly both human machine
interaction using perceptive devices and
robotic interact with the physical world
using perceptive devices was out there
so why is today different than what we
were doing back then and I think there's
a really simple answer which is
computing has won the war if you look at
what we had in terms of computing
platforms and in terms of sensing and
robotics 10 or 15 years ago it's wildly
different so you know as you all know
we've now got computers you know winning
jeopardy we're all carrying
supercomputers in our pocket I actually
checked so that that's about equal to a
Cray two for anybody who actually cares
and your computing is just showing up
anywhere this is actually a shot from
Johns Hopkins sitting outside the o.r
that's what people are doing using their
computers and of course obviously people
think about computer-aided diagnosis
this I think might actually also be from
kintaro its third world education yep
MSR India and of course now in
entertainment practically everything
lives on computing it's just it's it's
pervasive in fact just to check it out I
look to see how many computing devices
either in the world and I'll just you
find that as micro processors and so as
close as i can guess there are somewhere
between 50 and 100 billion active micro
processors in the world right now so
like what does that work out to be 10 to
20 micro processors per person on the
face of the earth so you know if this
were a virus we'd be looking for an
antidote or for a form of life we'd be
worried about it taking over and it's
just continuing to grow at remarkable
speed the other thing that's different
is your platforms for input so of course
I'm at Microsoft and so i have to give
Microsoft credit for really changing my
life over the last two or three years by
introducing the Kinect because suddenly
you've got a low-cost easy-to-use
platform that gives you direct 3d
perception of the physical world and of
course originally you know introduced
for gaming which is a lot of fun my kids
certainly enjoyed it but in my life
just about everything I knew suddenly
grew a connect so you know robots
sprouted connects is actually connects
in the ICU at Hopkins right now
observing patient care and this is
actually a video that that almost went
viral from our web where we're running a
da Vinci surgical robot from a connect
and again this continues to grow there
are probably 20 connects in my lab at
this point doing just about anything
that that you can imagine so we now have
a rich source of input to connect all
this computation and in fact it's not
just the connecting you know there's
every form of input known to man so you
know we accelerometers gloves every
laptop now has a camera you wear a fuel
band and decide how much you move today
so the world is just now connected to
computing in a way that it was never
connected before and the same on the
output side so you know this is a set of
platforms that I can now get in my lab
and again ten years ago none of these
existed but now you can get the Willow
Garage pr2 that's the deal are Justin
robotic just a crazy mechanism in terms
of what it can do NASA robonaut's
actually up in a space station right now
and many of you probably saw in the new
york times that the discussion rethink
robotics which is a new low-cost
manufacturing platform that's actually
just being introduced by well rethink
robotics a company founded by rod brooks
so we have not only this the the input
in the computation but the output is
there as well and so it really makes for
an interesting mix and and furthermore
it's actually having practical impact in
the real world so for those of you who
don't know this this is the intuitive
surgical da Vinci system it's a
commercially available robot that does
surgery right now there is about roughly
speaking 2,500 of them installed around
the world they do in excess of 300,000
procedures a year and for example if you
go to get a radical prostatectomy odds
are better than eighty percent that it's
going to be done through a robotic
surgery as opposed to traditional open
or laparoscopic surgery and those
numbers continue to go up so it's it's
very rapidly becoming a standard of care
in certain areas now so that's all great
and I think this new really exemplifies
the opportunities that are out there in
connecting people with really
computational systems what's missing
here I'll get rid of the the bloody
slide what's really missing though is is
kind of the level of interaction that's
going on there so you know everyone's
familiar with traditional robotics which
grew out of automation research so it's
really it's a programmable controlled
machine it's writing a program just like
you're right Microsoft Excel it just
happens to take place in the physical
world the other extreme is the the da
Vinci robot we just saw which is you
know strongly human interactive but it's
direct control there's really no
intelligence in the machine whatsoever
it's there just to replicate hand motion
into the patient and to transmit light
back to your eyes so you can see what's
going on and it really promises to do
nothing else in between there is
effectively no force feedback so it's
really and that's actually not unusual
and laparoscopic surgery that you don't
get a lot of force feedback in the
system
this scaling of the surgeon no it's
actually it's it's more subtle so so the
longer story is this so we started out
with open surgery where you can put your
hands inside the patient you could do
anything you wanted then in the 80s we
went to laparoscopic surgery and what do
we do we we separated the surgeon from
the patient using tools but those tools
are pretty much straight chopstick tools
and so when you introduce the robot you
introduce two things the first is you
get stereo visualization which you don't
get in laparoscopic surgery and the
second is you get a wrist inside the
patient and so suddenly you can do all
types of reconstruction surgery or
reconstructive surgery should say that
you can't really do comfortably with
traditional laparoscopic tools so you
can sit down ergonomically you feel like
your hands are in the patient again you
can see in 3d it's just a completely
different platform again really for
people to think about surgery and part
of the the game has been for people
really to relearn and to rethink how you
can do surgery when you've got that kind
of a platform available this is just few
times
well chuckles do there is a believer it
doesn't save labor but for any so to
begin with it's minimally invasive so
the big thing is with minimally invasive
surgery you're reducing blood loss
reducing morbidity of the patient
reducing scarring so it has all the
advantages of traditional laparoscopic
minimally invasive surgery and then it
actually has two really key advantages I
think first is you know as we said you
have additional dexterity in the
patients are the things that are just
much easier to do with it and the second
is actually it's a lot more ergonomic
for the doctor so if you look at certain
areas of the medical profession they
have some of the highest rates of back
and neck injury of any industry and it's
because the I know surgeons who have 18
our procedures they just stand there for
18 hours and usually something that's
not particularly ergonomic so it
provides a lot of interesting
possibilities yeah exactly dee dee dum
yeah and it's actually you know it's
called intuitive and it is intuitive
when you sit down you could sit down and
within 10 minutes you would very quickly
understand what's going on because it
feels like your natural hand-eye
coordination I feel like I'm an ad for
intuitive surgical here um but so the
key thing though is so we're
transmitting the surgeon into the
patient but we're not really augmenting
the surgeon in a real way except perhaps
worse emotion scaling and so a lot of
what we've been thinking about is well
how do you operate in kind of this mid
space where you have which you can
imagine is in structured environments we
can do a lot of autonomy in unstructured
environments like surgery we have the
humans strongly in the loop are the
places where we can kind of find a
middle space so for example if you're
trying to do remote tele manipulation
you can actually scan the scene you've
got the robot has a model of the scene
and now you can phrase the interaction
at the level of moving objects around in
the scene not just raw emotion on in
terms of the robot or if you're doing
this particular task it's a task which
has extremely poor ergonomics you can
basically do it for a week or two and
then you have to go out for a few days
because you've basically got repetitive
motion injuries so could we bring a
robot into this task which is hard to
automate but if we could have a robot
doing at least some of the more
injurious parts of it person can still
be in the loop but they can do it
without injuring themselves or I guess
my other video collapse but retinal
surgery is another area where now it's
about scales how do we do really micro
force sensing and and interaction at
scales that people can't perceive and
it's not just these are any number of
places where we can talk about really
augmenting human capabilities so not
replacing human capability now doing 12
1 but augmenting human capability
difficult for
robust essentially tie knots that have
the dexterity to it's late rope yeah so
there are two things the first is that
it's a it's a complex task from a
robotics point of view it's hard to
model what's going on it's hard to you
know solve the computer vision problem
for example just to find the components
of the task also it's a small lot
problem so that actually is they're
doing cable ties for refurbishment of
Boeing aircraft and so you do you know
maybe a few thousand of these cable ties
and then you're done with that task and
maybe a month later you pull it out and
you do it again and so it's just not
effective to think about completely
programming a robot we're building a
robot that's going to do just that task
and this is actually very typical right
now this is actually where in fact it
turns out the u.s. is is still
relatively competitive in a funny way
which is in these kind of small lot
manufacturing cases because it's not big
enough to offshore it and there's not
really an automation solution either
this one is described as a submarine but
you may have limits on wed equipment can
get where that's exactly the skeptics
darity just
right so you can project basically you
can project people in whatever format
makes sense for that particular task
exactly right so I mentioned it's not
just you know these tasks another so one
of the things I happened to look up the
other day was the prediction for the
most rapidly growing profession in the
United States and its in-home assistance
that's expected to grow think over ten
years by seventy percent or something
like that and again if you think of em
home assistance it's not about
automation it's not like you're going to
give somebody a one-to-one telling
manipulation environments about
interaction with people to help them in
their daily lives so how do you develop
these sorts of interactive systems that
can replace or augment certain functions
as needed and I happen to find this too
so I guess maybe they're even some third
world applications of human augmentation
of robotics I think that's made out of
wood so it's also recyclable apparently
you can throw it in the fireplace and
burn it and then make a new one really I
don't think you that I don't think it is
I can't tell Ashley but I just found it
I couldn't resist anyway so so I think
there are a number of opportunities
where because of where we are now in
just in the technology curve in terms of
sensing computing and robotics we can
start to think about these interesting
mixes of people devices in the physical
world sensing of the physical world to
do interesting things so a lot of our
work has been in medicine so you guys
asked a lot of questions about the The
Da Vinci robot so you kind of get how it
works so we started to look at it and we
realize that it's really a unique
opportunity and the reason it's a unique
opportunity is because for the first
time you're really interposing between
the doctor and the patient a
computational system and what it means
when you've got a computational system
is not just that you can program that
system but in fact that becomes a data
collection device as well we can build a
complete record of everything that
happened and procedure down to the
minutest motions that most minut motions
that the doctor made inside the patient
and so we actually have developed a data
recording system
now allows us to make those records and
if you think about it we can start to
study how humans perform interesting
manipulation tasks in the real world now
at scale you know hundreds of thousands
of procedures in principle and you can
immediately see interesting things so
for example this is an expert surgeon
doing a simple fourth row suturing task
you can actually see one two three four
throws this is someone who knows surgery
they've actually used the robot before
they're just still in their training
cycle and they're doing exactly the same
task and so you can start to realize
that what you'd really like to do is to
take somebody who looks like this and
make them look like that as quickly as
possible through whatever types of
augmentation teaching or training that
you'd like to have so you can start to
think about a space of task execution
and skill in ways that you couldn't
think about before well this is a
training task so it's not inside the
body but in principle yes exactly and
i'll show you in fact in a little bit
what that task is the other thing that's
interesting about surgery though or
medicine in general is that it is a
highly structured activity and it's
actually fairly repetitive so this
happens to be the flowchart for a
laparoscopic cholecystectomy so taking a
gallbladder out and you can see it's
it's highly stylized there are major
major areas of the operation and then
you can break that down into sub areas
and all the way down to dissection of
specific anatomic structures and so it's
not just that we can get a lot of data
but we can get it in a case where
there's actually a fairly known and
taught structure to the procedure so it
really lets a study of both from the
data side and from the side of
understanding semantics or structure and
so we call this the language of surgery
how can I take data from a robot and
parse it into this sort of a breakdown
of a task so just to give you a concrete
sense of this here's in fact that fourth
row a suturing task is a training task
so you can see you can sit down and you
can identify in this case roughly eight
basic gestures that
take place and this is the way that if a
doctor we're teaching that they tend to
break it down and to teach it and then
you can say well how do we model a task
like that well language of surgery kind
of suggests a natural thing to do would
be to start with some of the ideas from
speech and language and say look we know
that we've got a set of hidden states
which are gestures we've got a set of
observables which are the video and the
kinematic data from the robot can we
train something like an hmm to actually
recognize what's going on and if
presumably if we can recognize what's
going on we can now start to analyze
what's going on in interesting ways so
such robots would be different from the
wrong language of the human
because of the three kokin-chan on the
robot well so that's an interesting
question disley you cannot do keep
everything
no you can't no you can't you can
recognize I would argue recognized
almost everything the human is doing
whether you can replicate it is a
slightly different case so the
interesting thing I think in what you're
saying is suppose that I were to give
all of you video from surgery I'll bet
whether it's open surgery laparoscopic
surgery or robotic surgery you could
very quickly recognize when they're
suturing even though you've not really
seen it before because you have a kind
of a high-level semantic model in your
head of what suturing means you could
probably even break it down into kind of
sub pieces of what they're doing so in
some sense this part is invariant I
would argue the language is invariant
what what changes is how that language
gets expressed in the device or in the
domain that you're actually operating in
and we've actually seen this so we've
we've applied this not just in robotic
surgery but in laparoscopic surgery and
now in retinal surgery as well so we
also by the way had some belief that
this would work which was about ten
years ago we did a very simple test in
in retinal surgery in fact and we found
we could actually use HMMs to parse in
some sense what was going on so we
actually just to kind of again give you
a sense of how this goes so what what
you can do is you can have people sit
down you can have them do a training
task like this you can record it so you
know everything again that happened and
in fact you get a lot of data out of the
robot it's not just the tool tip
position it's the entire kinematic chain
both master and slave so it's really a
lot of data and you can do this for
multiple individuals you can repeat it
several times so you can get a kind of
repetitive data set of people doing this
just like if you're training speech you
would get a lot of speakers saying the
same sentence for example so you can
start to learn the language model so we
do the same thing and in fact if you do
just experts and you confine them to a
fairly constraint ask you can do
extremely well at recognizing what's
going on so this is the the difference
between a manual labeling and an hmm
segmentation of that for through
suturing task into
gestures and the the main thing you can
see is obviously we've nailed it in
terms of the sequence of actions the
only real difference is these slight
discrepancies at the boundaries trying
to decide when they transition from one
gesture to the to the next so when you
see me give accuracies like this 92%
what I'm really measuring is how often
the blue line and the red line are
perfectly lined up so here eight percent
of the time they don't line up and
that's what we consider the error so
that's good so what if you try to do
this a little bit more in the wild which
is get a wider range of individuals and
you get a wider range of task and you do
the same thing and these are again
pretty standard training tasks in in
robotic surgery so this is a data set we
collected a few years ago and the first
thing you find out is if you just take
standard HMMs and you try to apply them
here they don't go they just don't
generalize well as the data gets more
diverse which is probably not a big
surprise in the end but at least it was
the starting point for looking at this
so then we started to figure out what
what's going wrong here so part of
what's going wrong is we've got really
very high-dimensional data relative to
what we're trying to recognize and in
fact at different times different pieces
of that data matter sometimes knowing
where the tool is moving matters
sometimes knowing how the wrist is being
used matters for example so we
embroidered the HMMs a little bit so
these are factor analyzed HMMs where
there's basically dimensional reduction
step that's taking place off of the the
hidden discrete state and in fact once
you do that you start to see your
numbers pop up so we've gone you know
from 54 to 71 percent but there's still
a little bit of a Bugaboo and that's
this that you might wonder what is set
up one and what is set up to set up one
is more or less standard leave-one-out
training and testing so I just take one
trial and I throw it away train
everything and then I test but there's a
problem with that anybody guess what the
problem is see now i'll go into lecture
mode here
oh it already says there you go yeah you
you need to this happens to me in class
all the time too by the way and
sometimes they still get it wrong that's
the amazing thing so so yeah we didn't
leave one user out so what it's getting
to do is it's getting to see the entire
user population and then basically
recognizing as soon as you take one user
out you see things drop pretty quickly
and so it's over training in some sense
it's it's learning to identify people
for all practical purposes which in
itself is interesting just is a useful
thing to keep in your head so this is
what the the parsis look like again this
is a good one so you can see again
things over way pretty well this is a
bad one but notice that if I look at the
sequence the sequence is actually just
about perfect really all again that's
happening is it's it's kind of arguing
about where the gesture changes take
place so that's something useful to kind
of keep in the back your mind for later
so so we weren't happy with this so we
started to look at other ways that we
could improve this and so there are two
things that we've done both of which
right now seem to give pretty good and
about the same results one is to go to a
switching linear dynamical system and so
the only difference is we're going to
take this this hidden state and we're
going to add some dynamics to it or
another way to think of it is I instead
of saying that we're going to view each
discrete state is essentially having a
stationary distribution associated with
it now it can have a little dynamical
system that changes a little bit over
time associated with it and that that
has its own dynamics somewhat
independent of the discrete state so if
we do that things get a little bit
better in the leave-one-out case but
they get a lot better in the leave one
user out case and so it seems to be
capturing more the essential aspects the
data and not overtraining into a single
user and then on another bit of work
this is really Renee vidales group did
this is looking more at a dictionary
based approach so instead of just trying
to train straight from the data first
develop a dictionary of motions and then
build the the entire training appt
on top of the dictionary and in fact if
you do that you get really a roughly
about the the same performance so you
these two kind of are about equal from
the kinematic data what's interesting
about this method is if you look at the
video data so this is some work that
that appeared in Makai last spring where
we took this we took the video data not
the kinematic data we did spatial
temporal features on the video data and
then we trained up these models using
the the video spatial temporal features
and so if you take the something like
this dictionary based hmmm on the
kinematic data these were the the sorts
of numbers we had but if you now go to
the visual data and you do a little bit
more embroidery on the dictionary based
methods you can actually start to get
numbers which are getting back to that
first set of trials that we had we're
now looking at kind of 90 percentage
eighty to ninety percent recognition
rates interestingly if you add the
kinematic data in it goes down a little
bit again which probably kind of makes
sense because the kinematic data had a
lower recognition rate to start with and
so it's confusing things just a little
bit to put the two together yeah yeah
well it's so it's the all the tooltip
positions velocities the rotational
velocities orientation of the tools it's
about 190 different variables includes
things like their the gripper opening
and closing as well so when I say
kinematics I just mean anything to do
with any joint on the robot moving one
way or another but we're getting pretty
happy with this now so we've actually
managed to scale it up at least to a
more diverse group yeah
so the visual data I'm sorry you don't
have it's going to get the video from
Renee and I didn't get a chance the
visual data so we're taking the video
we're doing spatial-temporal features on
the video so in every frame your
computing something like a sift feature
but sift in space and time at the same
time so you get that a group of features
together you learn a dictionary on those
features and then you build a dynamical
system around that dictionary position
these men you have all the information
for the kinematics wiki
I wish I knew what I gained in the
videos so what I probably gained in the
video you're right is maybe the best way
to think of it is this way so when I
learned from the kinematic data it's
like learning Panama right I'm just
looking at hands moving and trying to
guess what they're doing based in the
hands are moving when I get the video
data some of the features are from the
surface as well until i get contextual
information i now know that the tool is
moving to the suture site as opposed to
the tool is moving in a straight line i
can only infer that it's moving to the
suture site so that's my hypothesis but
you know it's it's machine learning
right so there's a black box and i
haven't figured out how to open the top
of it yet but yeah this really is
starting to put us as i said in the
ballpark of really being able to use
this data to recognize what's going on
in diverse individuals so what can we do
with that so i mentioned skill before so
what is skill well right now skill means
that you stand next to a doctor an
expert an attending physician is doing
surgery you learn through apprenticeship
they watch what you're doing they give
you feedback and eventually you know
after roughly a year a fellowship for
example you they'll say okay you're now
a surgeon go to surgery the today
there's getting to be a lot more
interest and saying well how do we can
we define that a little more formally
how do we assess scale how do we know
someone's qualified to be a surgeon or
or how do we do reassessment so you know
how do we recertify someone as a surgeon
so for example this is something called
Oh SATs where what you do is you go
through a set of stations and you have a
set of basically skills you have to
practice and there's a checklist and
somebody can more or less grade you on
your skills and you get an overall score
but this is still you know this is
problematic you got it got to set all
these things up you've got to have
somebody who's basically licensed to to
do this sort of evaluation we'd like to
say well could we get it from the data
itself and you can actually see stuff in
the data that that correlates with skill
so for example if you just look at the
sequence of gestures that an expert uses
they look very clean right there linear
they stay
at the beginning they go to the end they
don't go back and forward using kind of
intermediate gestures and this is what a
novice looks like if you take those
basic gestures and you just plot how
they use the gestures so clearly there's
something here and in fact if you just
do you take that and you just project it
down into a 2d space you can actually
see pretty quickly that experts experts
intermediates analysis start to separate
in the space so we can actually start to
see where you are in the learning curve
of surgery from these models now that's
using what we call a self-declared
expertise it's just based on how long
have you been using the robot how much
training have you had you can also do
something like Oh SATs on the data you
can have someone actually grade the the
task that they did interestingly it's
much harder for us to replicate Oh SATs
using these techniques so we drop them
about ninety six percent to seventy two
percent chances thirty-three percent and
you can see if you do the same sort of
collapsing that thing's kind of cluster
together more when you look at the the
by trial performance as opposed to buy
self-declared expertise performance so
we've been thinking about how to think
about capturing skill a little bit more
precisely in these models and we think
we have something now that's starting to
work with this this is still in in
evolution but what we've done is we've
adopted a different coding scheme for
the data so one of the problems that
I've been complaining about to my group
for a couple years now is that we're
working from the raw data itself so it's
a little bit like saying I'm going to
take the raw recorded speech signal and
I'm going to try to parse it into
phonemes well you don't do that in
speech right you take the raw signal and
you do vector quantization of some sort
on it typically and then you start to do
you know phonetic recognition so what's
the version of vector quantization for
this well think of this suppose that I
take the just the curve you saw and I
build a little for name at a point so i
can compute the tangent i can compute
the basically the plane of curvature
gives me two directions the third is now
defined relative to those two and now I
can say well what's the direction of
motion over a small time unit relative
to that local frame now this has a lot
of advantages one of which is now on
coordinate independent so if I'm
suturing you know up here or down there
it doesn't matter the code is the same
and it also abstracts the motion a
little bit more and so we've actually
found that using even very simple sort
of dictionaries based on these for an
a-frame discretizations we can push our
recognition up much further and the way
we do it is we discretize and we
actually look for the substring
occurrences in that data so you'll get a
long string of symbols you say what are
the repeated substrings experts tend to
have long repeated substrings novices
much shorter and different repeated
substrings and so we can build very
simple string kernels string comparisons
that actually get us now again kind of
in this ninety percent plus range which
is where we start to feel like we're
actually understanding what's going on
and in fact we can do both gesture and
expertise which is interesting because
now we can actually break it down and
say well what gestures are you doing
poorly in which gestures are you doing
well and I can illustrate why you might
care about that I'll show you a little
illustration this kind of goes now
towards teaching which is what I've done
here is I've got two videos as you can
see both of these are experts and what's
going on is we're taking the video we're
stringify it and then what we're doing
is we're playing the videos together
when the same symbol is occurring okay
so you can kind of think of it like if
they're speaking in unison the videos
play together if they're saying
something different one stops and the
other advances so it's a almost a
visualization of edit distance if you
will and what you can see as experts
really do converge to a pretty
consistent model they're running at
about the same pace they're doing about
the same gestures there's not a lot of
pausing except this guy likes to
readjust the needle a little bit and so
you you'll get a little bit of pause
there
so that's cool so it shows we can align
a couple of experts what about if I take
you and tell you to do this task and sit
you down at the robot what can I do now
now so there goes the expert there's a
novice actually probably could have told
could have told me that already right so
you can see it's not it's not so easy
right well so there are a couple of
things interesting I mean obviously yeah
it's a little scary isn't it um luckily
that's a grad student it's not a surgeon
well so here's the interesting thing
there are two things here so so first
just to make a quick point here so this
person is doing things completely
differently right I mean they're holding
the needle differently the the motions
are different but we can still align
them and that goes back to that diagram
I showed you before that we're getting
the gestures right in the right order
and we can even do it when there's this
wild style variation the other thing is
and this goes to the teaching hospital
point that one of the problems with
robotic surgery and and I think this is
going to be generally the case as we go
to these more collaborative technologies
is it's really hard to teach people
because you can't stand next to the
person and say here's what you should do
you're sitting in a cockpit in this
particular case doing something and it
best the expert can be outside saying oh
you're holding the needle wrong there
you should fix it but they can't reach
down and say okay here's how you should
do it at least not when you're inside
the patient they can so they can show
what's going on but here what i can do
is i can basically just put the expert
in the bottle right i can record that
video you can start to do the task and i
can immediately detect when you start to
diverge from what an expert would do and
you dare i say have a little paper clip
that could pop up and say are you trying
to drive a needle right now could I help
you that may not be the best example but
but you see what I mean you know we can
you can build now basically a dictionary
of what everybody ever did you know
inside a patient in a train
task or whatever you can start to detect
the variation and you can offer
assistance if nothing else by saying
here's what an expert did at this
particular point in the procedure and so
it becomes basically a way to do
bootstrap training and also it you can
do it at this level now of gesture even
so I can detect it's the needle driving
that's the problem because you're
holding the needle wrong and you don't
have good control the rest of the tasks
are actually doing just fine you know
they're passing the needle as well as
the expert did and so you can actually
do now really what I think of as
interactive training for these tasks
i'll give you one other thing you can do
with it which is i can also learn to do
pieces of the task autonomously if i
want to I've got a discrete breakdown of
the task again certain parts of the task
are pretty easy to do like passing the
needle so why don't I just automate that
so here is a little system we built this
Nicola Pedroia a former post ah so you
can see this switch probability so
there's a little hmm running behind here
which detects when you're in an
automated will gesture and then it
switches control over and effectively
plays it back and the trajectory is
actually a learned trajectory too so you
sit down you do the task for a couple of
dozen times the system learns the model
both in terms of the discrete States but
also it does a regression to compute
trajectories for these automated
trajectories so so you learn the model
you break it down into things you want
to automate orange the trajectory and
now again it pretty reliably recognizes
in this case when you're done driving
the needle and then it says okay I could
do that automatically if you wanted me
to again I'm not advocating this is the
way you want to do it but at least
illustrates the concept that the system
is capable of now changing how the robot
acts or reacts based on the context of
what's happening within the procedure so
you know putting it all together we
actually can start to build these
systems which work together with people
and they actually have measurable impact
one of the nice things about this is
it's not just that I can build it but I
can say well did it change anything so
in this particular case it reduces the
amount of master motion
that you need to do or the amount of
clutching that that you that you have to
do which actually turns out to be
important in terms of efficiency we can
go one step further with this and we
talked about the fact that we don't have
perception in the loop so here's a you
know a couple of examples of building
perception components that that we've
picked up and we can start to look at
well how could we use perception to
provide even more context and make these
things even smarter so I'm going to
actually use the 3d object recognition
side of this and so we did actually some
experiments this summer this is what we
call perceptive teleoperation so again
da Vinci console in this case it's
wrapped in Ross its trance it's
connected to a robot which is over in
over 15 home from Germany and the
robotics and mechatronics lab there and
so we were interested in saying well
suppose you're trying to do these tasks
and for whatever reason you've got a
poor communication channel you know you
you can't really get the information
across either with a bandwidth or the
delay that you need to do the task
efficiently can we use perception to
augment the performance and the idea is
to say well again if I can recognize
what you're trying to do and I've got a
perceptual grounding for the task I can
automate or augment some part of the
task around the perceptual cues that are
there so this is a little is actually
work that's going to appear at the
robotics conference in in May so this is
the the robot in Germany it's being
teleoperated from Johns Hopkins on a
DaVinci council in fact and this is
without any sort of augmentation and
there's about probably a quarter to half
a second time delay just depending on
what the internet happens to be doing at
any point in time in this and so you can
see you end up in this moving weight
sort of mode where you move as far as
you think you feel safe and then you
wait for a little bit this is what they
do with the Mars rovers on Mars right
they move them a couple of meters and
they wait 20 minutes until they see what
happened and they move
again and you know in this case you know
so the the operator grabbed it but in
fact they didn't have very good
orientation so this thing kind of
swiveled and moved and in fact in some
cases it will actually fall apart so now
what we do is we turn on assistance it
actually parses this structure this
little orange line says oh I see a grass
point it recognizes you moving to grasp
and it simply automates the grasp and so
now you've got it in the right sort of
configuration it happens relatively
quickly and now you can go and go back
to whatever task you want to do and it
simply automated that that little bit of
it and again you can measure the
difference that it makes so in this case
it just about doubles the time or has
the time it takes to perform the task
and in fact you know the same idea now
is what we'd like to do in that little
knot tying test so we talked about you
know breaking this down into component
pieces so one of my grad students I said
just as a benchmark use the da Vinci and
see if you can do this task so you can
see he's using all three tools on the da
Vinci in this case you've got a switch
between tools to do it so this tip you
know we could time this it probably
takes you know 20 seconds or something
like that for human and this right now
takes about three or four minutes to do
with the robot and so I said okay your
thesis is now defined so how do you go
from three or four minutes to 20 seconds
with a robot I don't know if he's going
to achieve that but at least we can
start to see the sort of benchmarks we
have to achieve to be in this sort of
space with these techniques
so just a last thing so I've been
talking a lot about robotics and you saw
in the the beginning of my talk I had
also human-computer interaction using
vision and so I wanted to talk a little
bit about that in fact that's part of
the reason I wanted to stop by today is
talk about something else we're doing
which is now still I think of it as as
collaborative computing in physical
space but the physics is much more about
people interacting with virtual
environments than robots interacting the
physical environment is a display wall
project so in just about a year ago
little over a year ago the library came
to us and they were building a new wing
for the library and to make a long story
short they wanted to have some
technology focus to the library just by
by way of background Johns Hopkins
library at one point in time actually
had more digitally stored data than the
Library of Congress we host the Sloan
Digital Sky Survey we host connect omics
data from biology this happens to be
scans of some medieval French
manuscripts that are airing the archives
so terabytes and terabytes of data and
they said no is there a way that we
could start to think about ways that
people can interact naturally with data
in the library space and so we built a
display well forms my grad student Kel
Garin with his back to you who built the
basically display wall from scratch you
know starting in about December you know
come up with a physical design the
electrical design wrote all the software
and we opened it in August and we
dedicated it in October so basically 10
months from conception to to completely
running the reason again is that what
our library really thinks that this is a
way to start to think about how people
will interact with the information in
the future and how research will be done
around these large data archives and so
this is what it looks like so it's about
12 feet wide you can see it's running
three connects for a gesture interaction
this is the kind of the opening screen
that you walk up to
it's just a montage of images from
Hopkins so you walk up it sees you it
builds a little avatar out of the images
and you know you can kind of play around
with that if you want to and then we use
this curious put your hands in your head
gesture and you get a you know a fairly
rudimentary but still workable user
interface into a set of applications
that you can now drive using a gestural
input and part of what we're after here
is not that these are the best
applications or the user interfaces done
as well as it could be but it's really
interesting that it's out in the middle
of the University and about 10,000
people get to see it you know at various
times during their academic careers and
so our goal here really is to think of
this as a lab it's a lab to learn about
how people might want to use
technologies like this and then try to
run after them and figure out what do we
need to make this technology useful for
them we've course had to have a video
game so when we went live we've got
basically an image viewer video game and
some couple of other kind of image
modalities I was hoping I would have
this to show you we actually figured out
how to load CAD files of buildings in
that so you can start to do walkthroughs
of buildings basically at scale using
spatial interaction so it really is a
completely a work in progress but it's
it's now kind of flipping around and
letting us use a lot of the same ideas
and technologies but thinking of it more
from people interacting again in virtual
space so for example if you're doing
something in biology there's a natural
task if I'm doing you know molecular
design there are things that I want to
be able to do manipulations I want to
take place that's my task I want to
parse that and then I want to do
anticipatory computing to say well if
they're trying to move this particular
binding site over here how can I
actually run a simulation that will
check and see if that's a reasonable
thing to be doing at this point so part
of it is just exploring that level of
interaction we've had people already
come to us and say they want to do
art and dance so they want to make the
system react to people doing like a
dance performance in front of it and
make that part of the display
crowdsource science so we're trying to
put connect omics data on it and let
undergrads be labels for biology data we
need you know it's it's basically like
mechanical turk except now with with
undergrads and the payoff for them is we
say look if if we publish a paper on
this year a co-author so i say my goal
is if i could publish a science paper
with a large fraction of the
undergraduate population of johns
hopkins then yeah that's my windings
well so you know there are physics
papers with over a thousand authors on
them yep so I I was actually talking to
these these huge experiments right I
mean it's a horse
yeah bama Hopkins is a research
university we we advertise the idea that
undergrads you get involved in research
so what the heck here they go and
actually we've got I see I can't spell
can I the this these French medieval
manuscripts are actually motivated by an
art historian who wants to do research
on those manuscripts and needs something
that allows them to interact with the
data with the sort of visual scale that
the wall can give them so I really think
of it as it's really just this this lab
sitting in the middle of the library and
it's just trying to create the idea that
people can do this as a project to
create new ways to interact with with
data in their field just to give you
some anecdotes about it so we're getting
about right now ten to fifty sessions a
day it started out at about 500 a day in
fact I've got about ten students a
semester right now that actually come up
and say I'd like to do something with a
display wall we get about it on the
average about one query or a week from
somebody in the Johns Hopkins community
saying I'd like to build something that
could run on the wall for my research or
just for whatever personal personal
amusement I have I'm we're actually
starting to get sites that other
libraries in fact that are calling us up
and saying hey could we put this in our
library as well and would you be willing
to share the the code I don't think that
most libraries don't quite understand
that this is not quite as much of a
turnkey operation as they might like to
think but it's interesting that a lot of
people are latching on to this now so
let me just kind of close by saying so I
started out by saying that you know I
think the world is different today than
it was 10 or 15 years ago and it's
really different because of the
platforms we have the computational
tools we have the output devices you
know that display wall could not have
been built easily even five years ago
probably the way I can now and so we
really have to think of what does this
mean as you know the world has become
porous to computing you know just about
every aspect of life can cross the
barrier from the physical world into the
computational world and back again so
how does that change how we want to
think about computing so one of the
things that I'm thinking
about these days is you know what does
it mean to program these sorts of
collaborative systems you've got
hundreds thousands of different things
that you would like these robots or
these display walls to do and you can't
have a computer science students sitting
there next to somebody every time saying
okay here's the code that's going to
make this application work so how do you
think about combining what I think of as
programming in a schematic sense saying
look here's the thing i want to do with
probably some learning and adaptation
that then says okay now here's you how
it gets customized to what i'm trying to
do so in the robot case for example if
i'm doing small at manufacturing i know
the manufacturing steps i know roughly
the handling that has to take place
build the schema and then run through it
a few times manually in a training
cockpit like you saw with the da vinci
have the robot kind of fill in or the
system i should say fill in some of the
details and now I've got a workable
program but what's the what's the
paradigm around that and can we make
that something that's accessible to a
broader group shared context is
important if we want these systems to
work with people there has to be a
shared notion of task and context I
think I already said what's key also is
that we we have to have these platforms
in the wild because we have to see what
people do it's you know i always said i
used to have things like the display
wall inside the lab and we bring people
in and they'd be really excited and i
play with it and they go away having
that display wall out in the world we
you know we spent two weeks we had a
soft opening so we had the wall up for
two weeks before classes started and it
was incredible how many Rebs we went
through in that two weeks is we just
watch people use it kind of fix things
change things until it got to a stable
stable point so we really have to test
these things in the wild and then I
think the key thing is once we have this
creating interfaces that now really do
focus on interaction as opposed to input
output so thinking about how do I
anticipate what's going on and adapt to
it essentially thinking about interfaces
in terms of a dialogue as opposed to you
know I type something in and something
comes back to me there's state there's
history there's intent around it
I think the great thing is there there's
a real reason to do it right there are
real tasks or real industries there's
economics that can actually drive this
so I'll just close by saying you know I
think we're living in such a cool time
yeah and I'm so glad that I was here
actually to see it i think you know
computing keeps reinventing itself but
at least relative to what I do it's now
suddenly in a state where we can do all
sorts of things that I just couldn't
have imagined even ten years ago and so
with that I will thank a probably a
subset of my collaborators at this point
in all these different projects i will
also thank the people that support me to
do it and maybe i'll just leave you with
that final thought about where we may be
going someday all right thank you very
much for your attention I'm still
wondering why they device
side work you know this is just a vision
so reminds me of so we'll look who's
connect and so how this skate right was
on the desk lamp or so
even as well but also
pencil so that your skin economically
that's kind of scary thats right exactly
so but connect connects off the notice
i'll usually at the connector pretty
scared it's still have joy prison so
that's why sometimes custom apps us the
funny divider brightness prime just get
it
but I wonder you guys there is no the
use of bone connects could ask the
resulting in the Magdalen's the same
reason
no it is absolutely i mean well so i
should say so the kinematic data has
super high fidelity you know I know yo
yeah I know where the tools are two
fractions of a millimeter at least in
relative motion but what they're missing
is the context so you know the analogy
would be if I had a skeleton the
skeleton doesn't tell me if you're
holding something and I might be able
within the context of a task to infer
that like if I see you reach down and
pick see a pick up motion I say okay
they probably just picked up the
briefcase that's part of whatever
they're doing but with the video data I
can confirm that and I know that and I
know whether you succeeded or not for
example so in fact like one of the one
of the projects that one of my students
is playing with just to kind of get
warmed up is can you detect whether or
not in a fairly automated way what the
gripper is holding so needles suture if
it's in proximity to tissue but do it
without hand can tuning something just
get a few million snapshots and then
correlate kinematic data to what you see
in the image and then learn something
about the the set of states that the
gripper can be in and just that little
bit of information can make a huge
difference and we already know that from
some some manual tests
taking the word in Hindi at moving from
this pure 1121 manual tasks
and
just curious where your gall bladder
removal procedure fairly standard they
yes
said that that my robot
in
20 years from now
look like that I mean I don't know where
no yeah I know for a couple reasons one
is no I think technically that would be
incredibly challenging to do and no
socially because I just don't think it's
an acceptable solution so it's always
going to be a human in the loop system
and I think what I imagine more
happening so we actually so to give you
an example the sorts of things that we
think about doing so one of the
procedures that is still on the cutting
edge of robotic surgery is laparoscopic
partial nephrectomy so it's it's
removing a tumor from the kidney
laparoscopically with a robot and it's
it's challenging for a number of reasons
one is it's the way you do it is you
burrow into the kidney you expose
everything and then you clamp the feeder
valve to the kidney and then you go and
excise the tumor and when they do that
they start a clock and the reason I
started clock is because it's well known
that after about half an hour kidney
function just drops off like crazy and
the reason you have to clamp it is
because if you try to cut the kidney
when it's fully when you haven't done
that it bleeds like stink I mean you
just can't do anything with it at all so
you've now got this really constrained
thing you got to go in you got cut out
the tumor you've got to tamponade it
you've got a suture it and so we look at
places like that and say okay what
pieces of that could we provide some
sort of augmentation for and it's not
just physical so we provide a dissection
line so we can actually I've got a video
where I can show you we register and we
show you here's where you should dissect
you get a reasonable tumor margin we can
actually start to you know in the
suturing case we can start to automate
so you have to really use 3 hands to
suture and so you've got a switch
between hands and that takes time so if
you can automate the movement retraction
any piece of it you can basically
shorten that time and that improves
outcome and reduces the the difficulty
of the procedure also few so with
airplanes right
like automatic flight like they don't
let you for instance could you put the
newest all like are you thinking is that
other possibilities for gun cubbies
shoulder saying stop here again dude I
bored so my dream is if you had these
three hundred thousand procedures and
you had them all basically in a
dictionary as it were and you're going
along you can always be comparing what's
going on now to what happened in other
cases and you know the outcomes of all
those cases now so you can start to
predict okay given where you are and
what you're doing this isn't looking
good anymore and by the way here's you
know the closed its example to what
you're doing and here's what happened
and so you really could start to have
you know again the paper clips not the
right right model to think of but
something that at least could have a
little icon that says you know there's a
you've deviated from standard procedure
here you may want to check what you're
doing and think about it the other thing
with like thinking again of airplanes
with Blake semi-automatic and
research it's the transition points
right yep where you go from back you
know
that's showing me yep it is so
situational awareness you know the the
thing I've been thinking a lot about
recently is automated driving because
everybody saying automated driving is is
going and you know you can imagine
having an auto driving car which does
great until it gets into a situation
that it doesn't understand says well I
don't know what to do here it's your
problem now and so suddenly you know in
the middle of barreling over a cliff
it's your job to suddenly take over
driving and now that's unacceptable I
think that goes though to this this
contextual awareness and a task and
anticipation is saying it's not just can
the system do it but should it do it
right now because doing it right now
might put the human in a position that
later on they don't know how to recover
from
Oh
I guess you're in computer science yes
do you know what's going on in he could
take mechanical engineering control
theory you guys in terms of jello
operation especially with delay because
that's really speaks to I would think
having doesn't delay you have to do
something locally low delay means some
kind of intelligence and no are they
doing anything that will address sorts
of problems you're looking at well so so
that's another one of my hobbies right
now in fact so there with a lot of the
science around this in that space right
now is looking at things like how do you
actually prove properties of systems
with time delays for example so i can
prove passivity of a system and if i
have a passive system even with time
delay the system will not go unstable
which if you're doing surgery you
obviously would like to be able to prove
that your robot won't go unstable inside
the patient what is missing i think and
this is something that this is kind of
my glass of wine conversations is if you
think about doing those tasks there
there's a information a certain amount
of information you need to perform a
particular task what are the objects in
the environment you know what are their
orientations where am i trying to put on
what how much do they weigh who knows
what it is and if you're doing full
teleoperation you're assuming that all
of that information lives in my head
essentially and I have enough capability
to get movement information to the robot
to say here's how you perform the task
using what's in my head if you want to
start to automate pieces of it what
you're doing is you're moving
information into the remote side so now
the remote side plus you have to have
enough information to perform the task
so I think there's really this notion of
information invariance around tasks
which says you know I could fully
automate it if the robot had every piece
of information relative to the task it
probably doesn't so a piece lives with
me apiece lives with it and how do we
prove properties to think about
properties of systems where information
lives in both places and the sum of the
two pieces is enough to perform the task
that's not something that I as far as i
know the control theory people are
thinking about right now the the
practical solutions right now I'll tell
you our mostly anticipatory so you try
to predict what's going on and you feed
forward the robot but you try to do it
in such a way that you can maintain
safety ya betta fish exhibition that you
have played on beta there's a video and
say to it whether you feel good with the
business data beautiful
I can't make the robot data public
because that's actually governed by
Intuitive Surgical I have an agreement
not to share their data with people
other surgery data sure modulo IRB
concerns so privacy concerns sure we
could share that data publish your
savings not until now and and that's
largely because again I have to to do
these sorts of studies involve human so
I have to get institutional review board
approval to do it and if I put it out on
the website i would go to jail so I I
kind of try to avoid that but I can
actually ask them for permission to do
it and we've done it in the past and so
he probably will season you will do
little buys a visit it's all as a fun
integer to Italy funded by government
that's right that's right come so
that'll create an interesting decision
well the other thing that you're
something you know this data is gold
right so if you have data and you have
not yet kind of mind that data to so
it's really a question what's a
reasonable period of time that you have
to exploit a data archive before it has
to be made public sphere or lost
I was gonna say lastly good
if you watch they can work
oh I stopped by ricks house just to see
the the piece de resistance we worked at
the house Michelle Obama's on the screen
so that's the park yourself so there was
a teddy bear show which I had no idea
whether computer graphics or a robot it
was too small a little person I probably
tell the operator hmm probably
teleoperated would be my guest walk down
okay i love the cheered up a civil yeah
I am it was around up there he mom no
okay no then I am it was I'll go check
what's got to be on YouTube by now I'm
sure yes was it relates that would be
yeah yeah the mocap
I wanted to see it again to try to start
prepping this computer graphic cream
hmm because it
I haven't been invented that large
display
which is very cool and a question about
the first one and wondering about the
users usage so most students you that
for pain game or user to get information
about the you know and my second
question is have you observed any
frustration from the sooner you pebble
from student reusing system and what
that frustration if there is what the
frustration comes from from the
interaction I'll come from the how to
navigate the library well so these are
the usage statistics by the way this is
the dedication day so that just about
got the highest highest usage so we're
usage right now is running at the guy
was saying 20 kind of 20 30 session
today most people play the video game
about a third of them use the the visual
is a the image visualization app and
about a small number of them just kind
of go through some of the other how to
stuff on the wall and that that's kind
of act actually I'm kind of surprised
this isn't bigger so I'm pleased that
there at least looking at images we've
got some really nice images so it
actually is fun fun to do I'm sorry was
your second question also any operation
yeah my frustration is the gesture
recognition works not as well as I'd
like but I will tell you that so when we
started the wall project we look at at
both windows and then at open source for
a variety of reasons we end up going
open source so we're using night as the
the gesture recognition software we're
not using Microsoft so my firm belief is
that if we were to run Microsoft
software it would all work perfectly
there any so because it's a large space
do you guys police or multi participant
interactions yeah most of my interest
quite honestly is in multi participant
collaborative interaction so how do you
have three people solving a problem
together with computation behind it and
in fact nothing nothing for the wall is
one-to-one it's all multi 21 so I'm a
question regarding evaluation of the
training session and see this means
there's no absolute conscience so it's a
buncha to use user self-assessment of
yep that's really people a little
confident apology excuse well that's
already other guys so we don't do self
assessment so people do tell us what
experience I have and we do compared
with that as well but we actually have a
some clinical collaborators that go
through and they do an objective
assessment of the the data so that Oh
SAT score is done by someone who's not
not part of the trial
alright ok so we exhausted everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>