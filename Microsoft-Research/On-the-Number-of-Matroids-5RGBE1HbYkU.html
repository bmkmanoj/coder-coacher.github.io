<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On the Number of Matroids | Coder Coacher - Coaching Coders</title><meta content="On the Number of Matroids - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On the Number of Matroids</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5RGBE1HbYkU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's a great pleasure to have NICU
bansal here at Microsoft Research I knee
kills a professor at University
Technical University I toy then I before
that he was the restoration manager at
IBM Research Nikki works on various
types of algorithms online algorithm
approximation algorithms and today he
will talk tell us about the number of my
droids hey Thank You Costa and thanks
for inviting me yes I'll talk about this
work on the number of Metroid's and its
joint work with Rudy pan driving who is
a colleague at T window one and a master
student yan wonderful ok so probably
most of you know what mate rights are
have used them extensively but these are
like central objects and optimization
and one nice thing about them is you
know they combine like graph theory and
linear algebra together but just to
remind you so metroid is basically a set
collection a collection so you have a
universe of elements let us say one
through n and there is a collection of
subsets of you which will call as
independent sets which satisfies very
basic axioms so the first axiom is that
these sets are subsets close so if I is
independent that every subset of Y is
also independent and a more interesting
property is what is called the extension
property so if you have two independent
sets say I and I prime and I strictly
bigger than I prime then there is a way
to extend I prime to a bigger
independent set by including some
element which is not in I prime but ni
right so in other words there exists
some X in i minus i prime so that this
is also independent ok and actually most
of the terminology for matrix like comes
from linear algebra so a typical example
to keep in mind is that so your universe
is a collection of vectors and
independent really means they are like
independent over some field and so then
you should see that ok some set is
independent that it's subset is also
independent and extension property also
holds tight because and always extend or
set add some another linearly
independent vector if some set is
smaller
okay so one big program in Metroid
theory is like to understand how do
these Metroid's look like because notice
unlike graph theory where you know you
have some kind of structure these
networks are defined by property so it
is not really clear what the structure
is and this is of course a whole
industry in itself and so the typical
question is like how does it random
Metroid or a typical Metroid look and
there is also been attempts to define
like a certain notion of random matrix
theory and again it's much much less
developed than a random graph theory for
example and we do not even know like
very basic questions and in fact one of
the most basic questions that you can
ask right is like how many metrics are
there on n elements because if you want
to define any kind of probability or
something you better will be able to at
least to answer this and notice when
trivial boundaries of course 2 to the 2
to the N because you have elements we
could have up to two to the N subsets
and it's any collection of subsets right
so yeah this is the cleevil upper bound
now already in 95 74 yes almost 40 years
ago cruel showed the following lower
bound that the number of metrics is at
least two to the this number so it's 1
over n times the central binomial
coefficient and since n choose n over 2
is roughly like 2 to the n over root n
so this is roughly like this right and
the way he actually did it and we will
see this proof in a few slides is he
constructed an explicit class of
Metroid's what are known as sparse
paving Metroid's and he showed that
their number is this much so let's see
so if you summarize these two things
let's see what is already known so since
we will be dealing with such big numbers
it's useful to look at these things of
the log log scales we look at log log
number of Metroid's yeah so so this
result health is between n minus three
halves log n minus some constants and
this after taking log log will become n
right so so the number lies somewhere
here now you might ask ok why do we
bother about you know this tiny three
half slog and I mean usually an
approximation you just care about
Constance so this is negligible so
depends I mean you can give several
answered so one thing is
like this log log scale is a bit
deceptive right i mean because even if
you have X versus X square once you take
the log-log scale it just translates to
an additive one error between X and X
square and the slope more important
reason is that it has been widely
conjectured in Metroid literature that
actually most Metroid's are sparse
paving so people believe that connotes
lower bound is actually the right answer
for the number of Metroid's and so there
are various quantitative versions of
what that means but I wouldn't give you
sort of bother you with the precise
conjectures there are several of them
yeah but morally they mean that
northbound is sort of close to the right
answer okay good so just recall truth at
this three half slog em right now we saw
an upper naive upper bound of 2 to the 2
2 VN notice it can be trivially improved
and we can shave off this half log in as
follows by just looking at basically
metroids of a given rank so again what
is the rank of a matrix re call because
of this again we all know this so the
maximal independent set in a metroid is
called a basis and all of them have the
same size right because of this exchange
properties did not then you could extend
one of them and the site so the size of
each base is kind of unique and it's
called the rank of a metro and one way
to specify a metroid is of rank R is I
just tell you which sets of size are the
bases right because if I tell you that
you know everything else because every
subset of that is a independent set and
every other our set is not a base right
okay so this tells you that the number
of Metroid's on n elements and a frank r
is at most this so good so so if you
take log log of this you will get log so
we have first log shapes of this two
then you take log of this and this is
maximized when r is n over 2 which is
right n over 2 to the n over root n
right so if you take the log it like n
minus half log n good okay so this is
for a Metroid of particular rank and now
notice that a total of number
Metroid's is just like at most n plus 1
times matrix of a given rank and you
know like this n plus 1 is a negligible
factor right once you take two log log
sit like disappears don't know like
badly okay good so this shows that the
log log of M n is at most this and this
also tells like okay so two bound MN we
can essentially focus on the number of
meters of a given rank like we don't
have to worry it's just going to add
another n plus one factor okay so this
was a trivial way to get this half log
and improvement and again like almost 40
years ago piff had a little bit stronger
upper bound where he could shave off
another half log n so he could show n
minus log and plus order one so again
Ruth had three halves and this was Logan
and this was sort of the best state of
affairs until now and so what do we show
so we show so we tighten this upper
bound and show that basically we can
recover sort of closed lower bound up to
this adaptive term and in fact more
precisely our additive term is just one
plus order one so basically what we show
is like Loblaw Gehman is what growth had
plus one plus zero of one as opposed to
pay half log and gap or if you remove
the log logs more precisely it looks
like this so know that this lower bound
and we have like a factor two extra here
it which translates to the plus one when
you take too long
and the interesting thing is that to
prove this we actually do not need to
know much about Metroid's in fact like I
am not really Metroid guy so sort of
very very basic facts and in fact for
like almost ninety percent of the talks
we will not even talk of networks will
just talk about sort of independent sets
in a graph or stables so I will actually
call these stable sets using the war
technology because independent like in
Metroid means something else but I might
confuse myself anyway because you are
usually used to calling these
independent sets but yeah so this will
be our main tool is smaller in the real
thing yeah so that is ok so i will
actually describe exactly yeah so it's
believed that the number of metroid is
close to what are called sparse paving
Metroid's which is the class truth
constructed but presumably like sparse
paving matrix could be more than this so
someone doesn't know right like there
could be a gap booth so so I don't trust
is know much about my trade but yeah
yeah I can give definitely like sparse
paving is a special class will see
actually what they mean it's a very very
special class but somehow assam yeah so
you can like spanning tree metroid for
example is not sparse paving right but
the point is these are like very tiny
fraction of this huge space of Metroid's
yeah exactly in fact like yeah the most
explicit examples we know are not sparse
wave metroids but yeah but we'll see
what these are in a moment or me be
clear ok ok so the outline of the top
will be roughly the following so first i
will tell you this Kreutz lower bound
construction of and what are these
persuade rights and then for a while
i'll talk about sort of some technology
to count the number of stable sets in a
graph and then we will see how this
connects to Metroid's in the end okay so
next couple of slides I'll tell you how
booth came up with this lower bound on
the number of metroids okay so the first
observation is if you want to specify a
metroid of rank are you could also just
specify
the sets of size are and i will call
these are sets which are non basis
because if you tell me which sets of
size are non basis so the other sets of
size are the bases and all of their
subsets are independent sets so so this
will give you entire everything you need
to know about a Metroid so it's like a
complete description and it is
convenient to define the Johnson graph
which can probably many of you know so
this graph is has two parameters n NR
and it has n choose R vertices
corresponding to each subset of size are
on these n elements and you put an edge
between these two vertices if they
basically are common on r minus 1
elements right so have a picture yeah so
in other words here like so these are
two sets of size are and if they just
differ in like one element then if there
is an edge between them so more
convenient way to think of them is like
you have a 01 vector of n dimensions
where you have exactly are once and
there is an edge between two vectors U
and V if they you know you make a 1 to 0
and a 0 to 1 somewhere else right so you
just saw now notice one thing is a very
structured graph so one thing is like
it's a regular graph with degree r times
n minus R and why is that because yeah
how do you get to a neighbor you make
one of these once to a zero so there are
our choices and when there was a zero
you put a one right that's a swap so
there are our times n minus our options
say okay now here is a actually a very
simple theorem it's very easy to prove
so it says the following so so if you
pick any stable set in this Johnson
graph or in other words you pick these
are subsets which do not differ by one
swap all right so there is no edge
between them so pick any sort of stable
set in this Johnson graph and if you
call those as your non basis then that
gives you a mate road ok so is the
yeah yeah so if I just pick some stable
thread in the chosen graph those are my
normal is every other vertex in this
Johnson graph is a basis and that will
have yeah that will always form a
Metroid and actually for those of you
who know what the base exchange property
is it is very easy to see because the
point is like whenever you have two
bases you can always you won't get
blocked you can always find a path but
again if you do not know what basics
changes its let ya do not worry about it
it again it's like a one-line proof
though so but we can take this result on
faith yeah and these are precisely
called the sparse paving Metroid's so
this class and again there are sort of
various other characterizations of this
but this is one useful way to think
about it for our purposes these are
extensively studied also like a Metroid
theory okay good so stable set is
sufficient because right so what could
go wrong if it's not a Metroid if you
take two bases you cannot find a path
right and like and what is the path you
try to like do a one swap right so if
you can't okay so suppose I just take
two bases which differ in two elements
like if it's just one swap there is a
trivial path so let's say they differ in
two elements but then if I cannot find a
path this way and I can't find a part
this way then these both guys are non
basis because they are both blocking me
but then there there is an edge between
them right so it's it's just not right
yeah so it's sufficient I mean of course
there are other Metroid's which drawn
satisfy this but that is the whole proof
you know so in a stable set is always a
Metroid but of course it's not if and
only if this is equally it was partially
PIL so that's the definition of sparse
babe like one definition of sports
paving yeah the yeah there are also
various other characterization in terms
of dual matrix in Albert okay
but this is a but again as I said we'll
just mostly talk of stable sets right
okay so as I said sparse paying matrix
are precisely the stable sets of this
Johnson graph right so let us look at
these for a while and we are doing the
lower bound so one thing is let me
denote given a graph G alpha G as the
sides of the maximum stable set and I am
interested in the number of stable sets
of a graph right let us call it IG so
clearly I G's at least to do the alpha G
if you have was independent set of
allergy every subset is an independent
set so what does this give us right and
again because we want to look so one way
to lower bound the number of metrics is
to show lower bound the number of sparse
paying Metroid's so that is what we are
going to do so let us see what this
knife yeah so if you can lower bound
alpha G then you have a good lower bound
on number of independent sets using this
so already yeah so mantri well lower
bound is just the following so we saw
this Johnson graph jnr is regular with
degree our times n minus R because it's
one swap neighborhood and when r is like
n over 2 it's roughly n square over 4
and we know that any D regular graph has
an independent set of size at least you
know what d plus 4 meters the greedy
algorithm will give you so this yeah so
this Johnson graph with our is n over 2
has these many vertices and the degree
is roughly and square over 4 so it did
give you something like 2 to the N over
N to the 2.5 I am so yeah so that's
alpha G and so I G's at least 2 to the 2
to the dot so that's like a trivial or
bound and what crwth did to show you
should actually the Alpha geez at least
1 over N instead of this one over n
square a 4 over n square thing here and
the way he did it was actually just to
give a very explicit coloring n coloring
of the johnson grove right and actually
this coloring is very sort of cute so
and it is very simple to describe so
again John a vertex in the Johnson graph
was just 01 vector with r1 right so
associate at the following number with
the vertex V like so you look at the
iata coefficient multiplied it by eye
and then
overall the coefficients so I VI now
yeah so let us focus on this number now
I claim that if you have two neighboring
vertices this number can differ by at
most n between these two vertices right
because what is the swap right you drop
something and you add something and
these coefficients are always between
one and n so if i look at this number
modulo n all neighbors will be colored
yeah so for any edge like no to both the
end points will be colored with
different guys right you feel so right
so it is our very simple explicit
coloring so it tells you that the
independent set is at least this now one
thing actually I will also tell this in
the end so presumably it could be the
maximum independent set could be bigger
right but nobody knows of anak lower way
to lower bounded so so that is sort of
one trouble so if one could push this
somehow maybe 22 / n for example then
actually it would give a tight result
right like I haven't told you the upper
bound prove but we have because you also
proved an upper bound of like 2 to the 2
over n times n choose n so that's not no
no so that's not correctly and there are
four small values so people have done
experiments and there seems to be like
1.3 or something yeah so there is no
escape yeah that might be a key element
yeah I'm not sure yeah
now there is at least a clique of size n
over 2 I do not know about n but yeah
okay good okay so any questions so far
because that proves the lower bound
right so if there are alpha is at least
this the number of stable sets is at
least that and each table set is a smart
spam I tried okay so that was the whole
lower bound so now we will sort of come
to our upper bound so again Lotus recall
our goal was to show this upper bound
that the number of log log number of
Metroid's is at least lower bound plus
one plus order one so actually when we
first started thinking about this
problem the first idea was okay if you
want to upper bound the number of all
matrix you better be able to at least
upper bound the number of sparse paving
Metroid's by something that is close to
cry if you can't even do sparse waiting
there is no hope you can per pound
everything and yeah so so that's the
step we focused on and this was actually
nice because you know this is a very
clean object right is just the number of
stable sets in the charts and graphs so
it's a purely combinatorial problem and
luckily like the ideas we developed here
turned out to be directly extending to
Metroid's which was kind of fortunate
yeah so sort of most of the talk I will
just talk about yeah so I denote SN as
the number of sparse waving mate rights
and how to bound this yeah and then
there will be the extension to matrix
okay good so in so we'll try to upper
bound the number of spots playing
Metroid's okay so the first claim will
use is the following that the maximum
stable set in the Johnson graph is at
most 2 over N times the number of
vertices so I'll denoted by capital n
this n choose n over 2 instead of
writing every time the number of
vertices and notice Knuth already showed
that the lower bound of 1 over n times n
and now we are saying that okay the
upper bound on the maximum independent
set is at most twice of that okay
actually this follows from what is known
as Hoffman's bound and probably many of
you know which just says the following
so if you look at the adjacency matrix
of a graph and if minus lambda is the
smallest eigenvalue
so then smallest eigenvalue will always
be negative because I can value sum up
to 0 then this lambda n over degree plus
lambda is a upper bound on the size of
any independence and I actually will see
a proof of this in a couple of lines and
probably many of you have seen this
before okay now for the Johnson graph
this spectrum is sort of very well
understood they are very widely studied
graphs algebraic graph theory yeah and
it's known that so we saw the degrees
like n square over 4 and it's known at
the smallest eigenvalue is like minus n
over 2 so lambda is n over 2 in our
terminology and if you just plug these
numbers here you will get this upper
bound okay okay good so we can upper
bound alpha G by this now let's see what
this gives us right like on the number
of or at least naively so if you know
that the maximum stable set in your
graph was alpha G then the total number
of stable sets cannot you know it's
either n choose 0 plus n choose one or
all possible sub sub up to this right
and so in our case like alpha J is
bounded by this 2 over n times n and so
this term will sort of dominate so it
will roughly look like this and if you
just look you Stirling's approximation
or whatever the binomial approximation
it will give you some constant n which
is the ratio of n over K right to the 2n
over n right ok so this is like a knife
way just using Hoffman's bound together
with the naive enumeration gives you
this ok and what lower bounded we have
it was two to the alpha G so notice
there are two things that differ between
this upper bound and lower body so one
is the base of the exponent here that is
the crucial so here it was to hear it's
like n and then there is this factor to
sit in here also which we won't be able
to remove because that is the best upper
bound we know on the stable set but this
n is sort of problematic right like
because if you take n it will become
login in the exponent here which is not
quite what you want right so so the
point is you have this naive way of
counting independent sets number of
Independence earth is kind of lossy
so what we will show actually that let
me go back so instead of n to the two n
over n the number of independent sets in
the Chancellor of subtly to to the twice
n over N so we can shave off this base
from n22 down and morally you should
think of this bound is saying that most
independent sets in your Johnson graph
are subsets of this large one large
independent set right because it is not
like they are smeared out everywhere but
yeah so actually this phenomena happens
more often you know all around or so so
the most sort of natural example is to
look at the hypercube so say you have a
n dimensional hypercube with 2 to the N
vertices let us call it capital n then
it is a bipartite graph so we know that
alpha G is n over 2 so again what the
knife way of counting all possible
subsets of size up to n over 2 would
give you the following upper bound on
the number of independence a thread
which is like almost half of 2 to the N
it's pretty much useless now what is the
right answer and actually for hypercube
people have nailed it down exactly like
two constants it's quite an amazing
result so the right answer for the
hypercube for instance instead of 2 to
the N it's like 2 to the N over 2 yeah
so again you should think of any fact
this is a very small constant weight
like three or something so what this is
saying is that yeah so there are in the
hypercube there are these two
independent sets of size n over 2 n over
2 and essentially every independent set
is coming from one of these guys right
here so maybe this is the right picture
so hyper cube is your this graph and
essentially everything either comes from
here or here and intuitively this makes
sense for the hypercube right because
point is the moment you try to pick a
few vertices here it will sort of block
a lot of vertices in your independence
set on the other side so if you want to
pick really lots of independence that
you better just stick to one piece and
this will be sort of the idea that we
will try to use and this example order
tells you that to be able to do this you
have to so the reason why this is
happening is
because of the expansion in the graph
right like if you try to use something
from here it blocks a lot of stuff here
and somehow you're bound should take
into account expansion so otherwise if
you do something knife it won't be
strong enough to do it yeah good and as
I said like yeah this kind of phenomena
happens more often so in fact there is a
very interesting result of Jeff con
which says that for any D regular
bipartite graph like not just hyper cube
or whatever so the independent set is at
most 2 to the N over 2 plus n over 2 D
so again this 2 to the N over 2 is the
right thing which morally says that most
independent sets come from one piece and
actually what's nice about this result
is it's exactly tight because if you
have say n over 2 D copies of this kdd
complete bipartite on D by D so there
are n vertices right this graph has to
revert as's and I take n over 2 D copies
so it looks like this then for each of
these pieces there are two to the D plus
1 choices for my stables at either i can
pick any subset here or any subset here
and then if I raise this number to the N
over 2 D I will get exactly that so it
is like yeah and there was a nice result
of Zhao recently which showed that this
is even holds for jungle graphs
d is the degree okay so he proved so
Jeff man proved it for d regular but
maybe these ideas are okay so they're
also general d regular but not necessary
bipartite but I think there might be a
simple argument which shows that these
are extremal cases when it's regular or
you can do some swapping argument and
make it regular okay okay so what do we
show so our result is another bound on
the number of independent sets and it's
okay so if you have any D regular graph
with minimum value minus I can value
minus lambda then we can Brown the
number of independent sets by so let us
ignore this term for now this is the
main term so recall this was the
Hoffmans bound right the size of the
largest independent that so basically we
say it's two to the alpha G so
essentially everything is coming from
just this guy and this is supposed to be
like a negligible term it's like yeah
two to the N over D roughly with some
log square factors to Kenyon okay okay
yeah and notice it's not such a bad back
because if we look at say the setting
where of a bipartite graph like any
arbitrary bipartite graph then we know
that the smallest eigen value is minus D
and corresponding to this eigenvector ok
so lambda is d so if you plug it in our
bound this will become like 2 to the N
over 2 because lambda is d so this is
half yo so they will give you to the two
to the n OT plus this small term rate so
cons argument he had a plus n over 2 D
so we lose this extra log square
additive but we sort of get the
dominating term right right and actually
this also holds for J so this was for
any general graph right and so this
proves also this bound for a general
drop because lambda again is only minus
D or higher sorry
young yeah yeah so I'm point is like so
we will give a much more general
argument in terms of lambda so why is
this useful because if your eigen value
was smaller it was away from minus T
then this gives you a much tighter bound
right because for on Johnson graph we
don't want something like n over two we
wanted something like this capital n
over small n right and that's the whole
yeah so he cannot use of off-the-shelf
is ours bound or whatever we yeah yeah
so this so my point is like this theorem
kind of essentially recovers close to
these khan and all which was a long line
of work but also a sort of much more
general okay yeah so this is i will try
to give you a flavor of this result in
the next couple of slides okay so the
whole idea of this proof will be you
know given I independent set will give
some kind of encoding scheme so it's
like a computer science a way of
counting so we'll show that you can
always encoder independence at using a
few number of bits so it cannot be more
than two to the number of bricks that
you use okay and this approach is sort
of again there is a been a long line of
work in this work starting from flight
men in the 60s but our approach is sort
of closest in spirit to a recent paper
of alone at all but sort of the bounds
they get are weaker and were not useful
for our purposes and also as I mentioned
earlier this encoding idea that will
develop for stable sets will be useful
for Metroid's okay so the rest of the
talk will be described these two things
okay and before I give the proof so here
is a useful lemma which is useful by
itself I think also it says the
following so suppose you have a d
regular graph with minimum i can value
minus lambda then the following holds if
you look at any subset of vertices call
it a then the number of edges within a
right so let me denote g of s the graph
reduced on this a so the number of edges
is at least this number and again this
so this is sort of the dominant term and
so what this means like suppose now you
look at a random subset of vertices
capital a
a of G of a yeah yeah so gfa is the
graph induced on a and E is the number
of edges so it says that no matter which
subset of vertices you pick the number
of edges is at least something so it is
giving a lower bound and how do we the
right way to think about this lower
bound so suppose you pick some random
set a in your graph right so let us say
this was the right number of now if you
look at a typical vertex you will expect
a degree of so it has degree D in the
original graph and you are picking a as
a random shots we will expect its degree
to be roughly like this way over n
fraction of its neighbors will be here
yeah and you are picking a such vertices
so so the total sum of degrees will be
this in expectation right and thats
twice number of edges so basically what
this theorem is saying is that yes twice
the number of edges order some of
degrees is essentially what you would
expect for our random set- sort of some
error term which depends on how your
minimum eigenvalue looks like so if this
was a much smaller than compared to D
then you would so yeah then every set
behaves like a random set ok and this
actually yeah so let me rewrite this
again okay and how do we prove this
again it's a standard result but it's
sort of you do the usual thing that you
do right like so you want to find the
number of edges in a so if G denotes the
adjacency matrix of this graph then it
is nothing but you know you just sum up
over all these pairs this will count
exactly what you want and this you can
write it as this guy is the incidence
matrix of a and then you do the usual
thing you expand this guy in terms of
the eigen basis of G right and we know
that the largest so one of the
eigenvectors is 1 1111 so so i write
this guy as a over n times 1 1 1 plus
other vectors which are orthogonal to
this one vector because so this is
removing the bias and sort of i expand
out to the usual thing and
think pops out rates again probably many
of you have used these arguments I don't
need to elaborate okay so one corollary
of this is this Hoffman bound just pops
directly because right because if you're
set a is independent then the number of
edges within it should be zero so you
just asked like what is the largest a I
can push so that this stays 0 and if you
just do the algebra it will come out to
be this ok good ok so the useful
corollary of this result for our
purposes will be the following if I look
at any set a of size epsilon epsilon is
some small constant plus some little
term that depends on the spectrum think
of this as a negligible term because
lambda is much smaller for us than D
then essentially yeah so what this
theorem says is that this guy will have
lot of edges right this set will behave
like a random set of size epsilon n so
in other words then G a has a vertex of
degree at least epsilon D so this will
be a useful curve yes us ok so here is
based on this how we do the encoding and
again the idea will be the yeah so let
this slide I described the idea so
suppose yeah so again there is some
stable set I in our graph I want to
encode it somehow so one way to encode
is just to write it down exactly right
this vertex this vertex results so that
will exactly give you the naive bound
that we had because the size could be up
to alpha G and you write log in bits for
each of them ok but here is a
potentially more useful way to do this
yeah so suppose so maybe this picture is
useful it is not the most accurate most
exact picture but it is a useful analogy
so suppose there was this independent
set that you would like to encode using
a few number of bits if you could do the
following suppose you could find a small
set s so this s is supposed to be like
very tiny right formerly you think of it
as this size so this set s is contained
in my independent set suppose I could
find such a set but
this set had a large neighborhood like a
very very large neighborhood suppose I
could do that then then you know that
your independence set must lie in this
remaining region right because this set
s rules out all of this red part like
this cannot lie in your stable set so
for these remaining guys in a I just
need to say like I use need to use one
bit 0 or 1 whether it's in my
independent set or not right or in other
words this independent state is
completely specified if i could find
this sort of seed set and in the
remainder which which of these guys lies
in my a yeah okay so the number of
possibilities for I is yeah so if you
could give such an encoding its n choose
s because for the whole possible ways of
choosing the set but once I choose my
set it fixes my a and then I only need
to specify to do the a need like two to
the air combinations right like which
subset of that is there okay yeah so
this yeah so this will be the idea to
come up with an encoding scheme with
that's why these properties and if you
just plug these bounds for s and a this
this recovers exactly a result okay so
why should such an encoding scheme exist
so let me sort of for try to give a
sense we are almost exact proof ok so
here is a way to encode the independent
set so initially we said yeah so there
is some independent set given by these
red vertices that I want to encode right
and I just one what to list all these
vertices that's too many bits so I do
the following I will start with all my
vertices and let us say I'll arrange
them in the order of decreasing degree
so this is the maximum degree the next
one and so on and ties I break in some
fixed order and let us consider the
following procedure so I look at the
first vertex maybe I have an animation
for this so if it's in my if it is not
in my independent set like this blue one
I just discard it and yeah so the first
guy is not in my dependence that are
discarded second
i discard now this guy's in my
independence set so i choose it and if I
choose this guy it rules out lot of
other guys from being right all its
neighbors like in the remainder it will
rule out okay so I keep doing this okay
so I now these guys are ruled out so I
removed them now in this remainder graph
maybe the ordering of the vertices
changes because it's per degree and then
I keep continuing this procedure until
my remainder vertices are this Hoffman's
bound on the independent set because
then I cannot guarantee you any edge
anywhere that maybe it's table set so
the procedure sort of doesn't kill any
vertices when I pick something okay so
the whole crucial observation is that so
at the end of the day right when I
stopped when I am left with so many
vertices I have picked some vertices
like s so one thing is that this s that
I have picked like these red vertices it
completely determines which set is left
over why because this was a completely
deterministic procedure right so for
example like if you go in the very
beginning right so in the beginning like
you knew what was the ordering on the
vertices now at the end of the day you
are left with some set s you see oh the
earliest vertex in my set s is this in
this ordering so I must have rejected
these two rights you keep applying this
argument so so if I tell you this set is
it exactly tells you what is your
remainder set at the end of the day all
right
kind of yeah yeah so this is just a
small seed that sort of determines
what's left over here you can think of
it good and there is just some
calculation to show that okay and what
is the property right so the point is
that initially if we pick some vertices
then we had arranged these guys in the
order of degrees right and we had this
argument that if I look at say some big
set then a large sets have lot of edges
between them right this was that lower
bound that spectral lower bound so it
means the highest vertex should kill a
lot of vertices there if i pick so you
can sort of do some math and show that
because of that the number of vertices
that you pick up cannot be too much
because every time you pick something
you kill a lot of guys and you can only
kill so much okay and yeah so that
basically tells you that the set s that
you pick is roughly like log D over D
times the number of vertices okay so
yeah just to recap maybe it was a bit
quick the idea was again like you want
to encode some independent set here you
find a subset s which sort of rules out
a lot of things and this set itself is
not too large so your independence that
can only be in this region and this a is
also completely determined by this set s
so you only need like zero or one
indicator and not the whole log in bits
okay good all right so this was the part
for how to capture independent sets now
how do we encode sort of how do we
extend it to Metroid's because now we
are trying to count general Metroid's
right not just parse bearing metroids so
these are not necessarily independent
sets I mean they could have non basis
could be all spread all over the place
okay so we will use one more idea but
the main idea will be the following yeah
so again a Metroid could be specified by
saying which of the our sets were non
basis but I will efficiently encode
these bases instead of listing like each
one out there and again the use picture
to keep in mind is that
right so if I will find a small set s
okay and again like if I think of the
Johnson graph maybe these are all the
neighbors of this graph so in the
previous setting i could say in my
independence that none of this can lie
and my independence that can only lie
here but now because we do not have any
pendants that maybe you also include
some guys from here as you are non base
is right that is the whole complication
but what we will show so this is the
next key lemma for Metroid's that all
the non bases in this neighborhood of s
can actually be specified by essentially
twice the size of this guy okay and I
will it'll be clear what I mean by this
so so even though your Metroid could
have lots or lots of guys here lots of
guys as non bases here you could just
there is a very small sort of witness
for them and let me explain you what I
mean by this yeah okay so once I prove
this lemma the encoding is clear right
so because I just store my set s and the
non bases in a like previously and the
only difference here is that for the non
bases in this neighborhood it could be
many but I will just have a very small
witness which will tell me which one so
the total number of bits i use is not
too much i just have three times s log n
bits plus 0 1 bits for this ok now what
do I mean so so what is the idea that
right so what do we want to show is that
a lots of non bases here can be
specified by a few number of sets and
the idea will be the following if I look
at any set and I look at its
neighborhood right in the Johnson drop
there are these n square over 4 guys and
maybe lots of these vertices are also
non base is right but instead of so one
way to specify them is to list each of
them but we can actually give a very
concrete representation so instead of
listing all of these which are not basis
I can i just described two sets which
will encode which of them are non basis
and and the reason is and this is the
following structural property that yeah
so if X is a
on base it is our set which is the non
base and there are many other non bases
in annex then there is some kind of
underlying reason for these guys to be
known basis so so let's look at the
following is example right so say you
have a graph and your metroid is the
spanning tree ha metroid ok so a set
like this is an on-base because it has
this circuit now look at all so look at
the possible neighbors of this guy right
so a neighbor was just when you flip one
thing and add something so this is a
neighbor this is the neighbor you will
also have other neighbors when you break
this cycle and add some other edge but
for all but there can be several
neighbors like this right now in for all
these neighbors I can just list the
reason that oh there is the cycle there
and that is why they are always so
instead of specifying each of them using
a lot of bits I can just say oh here is
a cycle and anything that contains its
cycle is like so there is a very
concrete just this one cycle it tells
you the whole information instead of
storing everything so so this is sort of
the intuitive idea that yeah there is
like inherent reason why things in the
neighborhood of something or non basis
ok so here is a way to make thing in
precisely and again it is a little this
is the only place where we use some
properties of Metroid's but just this
one slide so let me go over it in case
those of you are more familiar with mate
right so the key lemma is the following
so if you have a dependent are set X or
it is an on-base then we can associate
two sets that are witnesses for all the
non bases in the neighborhood of X
instead of like there could be up to n
square over for non basis but i will
just exhibit two sets which give you all
of a witness ok so here is the whole
proof of this ok so look at some
neighbor y of x it means that wise of
this form where you swap throw some
element X from this and add some element
Y and let us look at two cases ok yeah
so this is case one so suppose you are X
to begin with it was a dependency right
so it cannot have wrong car suppose it's
rank was ar- towards
smaller then actually I can just take my
witness as the set X because what is why
it's just you are adding at most one
element to X so it can't have rank more
than one so this is just a very concrete
witness for such guys right everything
in the neighborhood is dependent so this
is a trivial case so the more
interesting cases when you are x has
size are a rank R minus 1 okay now in
that case recall the basic property that
X will have a unit circuit like spanning
trees maybe they exist so that is one
unique cycle and what I will show in a
sec so the witness will associate with
this X is this unique cycle and what is
called the closure of X and a closure of
a set is just if I add all those extra
elements which do not increase the rank
of X that is the closure okay so why is
this witness enough so here is the proof
so here are suppose there are two sets x
and y which are non bases and why is
obtained is a neighbor of x which looks
like this then first claim is and again
this is just a one line calculation as
well skip that you know so this was an
on-base of rank R minus 1 and Y is also
a dependent set so either it must be
that when you remove this element X from
X its rank fell down to r minus 2 and
even when you added up it could not go
all the way up to our or it could be
that or it must be that X plus y has
rank R minus 1 yeah so this can only
have less rank and you can show that
yeah this my sub modularity that either
of these cases must hold good till it
good so if rank of X plus y is r minus 1
this means X lies in the closure of
vilas in the closure of x rayed because
X had and carbonation so if I give you
this as a witness then everything that
is in that closure of X tells you yeah
just this once it tells you all these
other sets that will be expanding and if
rank of X minus X was r minus 2 yeah and
notice r minus two is pretty less than
the cardinality of x
minus X right because this was
cardinality are so this is r minus 1 so
it means X minus X must contain some
circuit but that circuit also lies in X
right and but X only as a unique circuit
because it was a frank r minus 1 yeah so
if i just specify this circuit I that
tells you i mean thats the reason why or
rank is r minus 2 so if you had any why
it will still increase so that's
basically it right so yeah for each let
me give a concrete thing yeah so just to
maybe recap we kind of do essentially
what we did in the stable set procedure
but instead of yeah we find these sets
sna but now we also store witnesses of
the neighborhood in each x which is not
too large and that gives the efficient
encoding ok so just to conclude so we
showed this gap yes so we sure this
light much stronger upper bound on the
number of Metroid's but we still have
this one plus order one gap right every
nice if it can be reduced to little o of
one but that is sort of a inherent
bottleneck at least why this approach
fails and probably why maybe any
approach based on sparse pairing
metroids would fail and the reason is
that we don't quite understand how the
max table set in this johnson graph
looks right I this is it was asking
because I cannot give this explicit
construction of this much size and the
best way we can upper bound is only this
and as long as this remains you will
always have to to the alpha G this too
will stick up in your exponent term
won't go away when you take log log see
given that 1 plus 0 of 1 no matter how
you clever you are on the other things
so sort of one natural way to break this
gap would be you know to understand what
really is the truth and actually this
question has but this might be a hard
question i think because this actually
problem of stable sets in the Johnson
graph there are lots of papers in the
coding literature and people have also
done like simulations of up to n is 24
or something I mean it's already a
humongous graph so they use a lot of
symmetries and all kinds of things and
the simulation seemed to suggest the
answer is like
closer to this but it is hard to judge
because you know it's something like
1.07 n over n up to 1.3 ln presumably it
is converging to n over n who knows
right these are small numbers but yeah
like so if this is the right answer then
may presumably ok so maybe even if this
is the right answer one problem is we do
not know of any other general techniques
to upper bound stables at other than
sort of these eigen value methods or you
know this is the res del sorts method
which is also sort of similar but maybe
one could use stronger Lasserre
hierarchies or something to upper
bounded that is the only technique I
know and perhaps if we so if we got a
method for certifying that upper bound
was actually this then maybe one could
on top of it combined with metroid
techniques to actually do the same
machinery that we didn't get the right
bound on MN yeah but that's probably a
long program ok so that's it thank you
for your attention yeah
he also assuming ok so again who knows
how this machine we will look but you
would have to kind of carry it out right
because for us we use the Hoffmans bound
right with 2 n over N and then on top we
built all this eigenvalue to actually
show that most independent sets come
from just this guy right it's not like n
choose this but more like 2 to the N
over N word so we use eigenvalue
arguments in that way to show such a
thing so this was the right but
hopefully like yeah so that would be one
piece one has to do all this yeah
exactly right and the actually the
theorem we can show is only this and the
reason why this is good is because this
is like to to the Alpha you but maybe
one tighten this it's not clear whether
this technology to push that goes but
presumably it should if that's right
thing a black box reduction
question he also theta function is
exactly gives you Hoffman bound yeah so
there is a paper from scriber back to
the seventies all these s yeah so all
these things else arts method theta
function and eigen value zero apparently
all the same yeah I do not understand
why it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>