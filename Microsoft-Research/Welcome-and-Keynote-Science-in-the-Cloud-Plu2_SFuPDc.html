<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Welcome and Keynote: Science in the Cloud | Coder Coacher - Coaching Coders</title><meta content="Welcome and Keynote: Science in the Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Welcome and Keynote: Science in the Cloud</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Plu2_SFuPDc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
miss Dennis cannon I'm one of the
organizers of the event and it is my
pleasure to introduce one of the
co-chairs of the conference Tony hey
Tony is a corporate vice president for
microsoft research connections tony is a
person who's been in the in the academic
community for a very long time he was
dean of engineering and tariff computing
at Southampton he I would credit him
with basically inventing the term East
science because he ran the e science
program in the UK an incredibly
successful program that's still going on
today at Microsoft he's been doing an
amazing job and pulling together a lot
of different university research
projects and connecting them with the
microsoft research organization but i'm
not going to say anymore Tony it's yours
thanks very much Dennis okay no it's
great to be here and a nice day in
Berkeley when I was last here for my
son's graduation in June I think it was
it was raining never rains but it does
occasionally so it's great to be here
and I'm just here to say a few words of
welcome so first of all I think we're
all aware of the data explosion big data
is everywhere and almost every part of
science is now engaged in data intensive
research and increasingly we're seeing
that federal agencies are taking an
interest in what you do with your data
where do you save it where do archive it
and so on so technology to publish and
share data in the cloud is clearly going
to be relevant and then data analytics
tools to analyze the data you've got are
coming important and things like machine
learning as a service and finally in the
UK the equivalent of NSF has just asked
universities
to be responsible for the data that
their researchers produce for 10 years
and keeping data over 10 years costs
money and so the moment universities in
the UK are panicking about how they're
going to do that in the u.s. we're
panicking about what a data management
plan is and so on so there are many
issues that we need to address in the
cloud I believe can address some of them
so I do believe that cloud data services
from commercial providers like Microsoft
Amazon Google and so on will play a role
in this and condemn up cut eyes access
to some of the scientific data that we
have we have to have research data
services built on top of the basic cloud
so they have to be open and extensible
there have to be easily accessible and
so on but above all they can actually
encourage scientific collaboration so I
think what I'm looking to find our
examples and examples of best practice
and so on but instead of having to have
a massive amounts of data storage
massive amounts of support in order to
maintain this maintaining data in the
cloud has a number of advantages but
again we need to experiment to find the
right sweet spot in terms of local
versus cloud and sustainability so what
I'm excited about is the cloud futures
papers here present some of the ideas of
what's possible and I'm hoping to learn
a lot from what's happening today maybe
one of these days we'll publish a sequel
to the fourth paradigm when we've all
figured it out but I think it's a very
exciting time and I'm grateful to you
all for coming and helping us figure out
where the cloud can play a role so this
is the third workshop in the steerage
series that started in 2010 and I'm very
grateful to hosting it at UC Berkeley
which is obviously one of my favorite
places so thank you to Michael Franklin
our co-chair and we have attendees from
18 countries apparently and oversee
the institutions so I think that's and
now when they started this was one of
the few cloud conferences now there are
many so it's a testament that people are
still interested to come to our
conference and tell us about what
they've been doing so I'm grateful that
you're here so the program overview in a
moment I'll hand over to Dennis to
introduce our first keynote Joe hella
stain from the Google's big science
effort and I'm looking forward to
hearing that that science in the cloud
at Google and then we have Yosef kalady
the Microsoft looking at opportunities
and challenges for large-scale cloud
computing there's lots of plenary
sessions at parallel sessions and the
poster session and the end of the day
tomorrow we have a panel chaired by
Michael who's going to talk about Big
Data on campus addressing challenges and
opportunities across the domains so it
sounds like to me like a very exciting
program and what I'd like to do is and
just hand over to Dennis who might want
to say something about the housekeeping
and introduce one right I'll stand down
here so first of all I think some of you
already know this is the most popular
question of the morning so far where's
the wireless so i think that's that's
SSID is that is rescomp re s co MP cloud
12 password lower case may I've been
overruled yeah I think you should turn
off the wireless during the sessions but
so it is there but please don't use it
ok if you want to tweet during the
meeting there's a there's a twitter
hashtag the speakers please submit your
your slides to to us or to the you know
to the front desk because we want to
make sure we have all the slides that
are a form i think you need to sign
because some of this is being webcast it
as it is right now for those of you that
drove your car up here and you want to
go to the dinner we advise you not take
your car to the den
your location leave it here and the
reason is is that the parking is four
dollars but they only take one dollar
bills so if you've got for one dollar
bills you want to do it that's fine
otherwise we're going to run the shuttle
back and forth here so if you have your
car here you go to the dinner you can
come back here and because the shuttle
will just circulate so so don't worry
about that there was some other
announcements that will that will give
you after the keynote just before the
break so right now I'm going to hand it
over to one and one is going to really
make the next my name is Juan Juan
Vargas and with Dennis and Tony I am one
of the organizers of this conference and
it is my great pleasure to introduce yo
Hellerstein who is going to be doing the
first keynote Joe had his PhD from the
University of California in Los Angeles
and from there he went to work for
20-some years at IBM Research In jerked
on Heights New York then he joined
Microsoft where he was in the developers
division working on dotnet and then from
there he went to Google where he became
the manager of big science and he's
going to tell us a his presentation
which is entitled science in the cloud
he's going to tell us about his new
program which is discovery computational
discovery Joe so I want to share with
you some work I've been doing over about
the last nine months on about nine
months ago I was chatting with some of
our senior leadership and Google and
they were asking about well now that we
have this planetary scale distributed
computing system basically that
advertising built for us so what's next
what are the next interesting workloads
and so I started down this path trying
to ask the question well I don't want to
go to commercial companies and and say
well gee we're sort of doing some
exploration when okay all right enough
technology interludes so I wanted to do
something where it could be exploratory
where I could talk with people sort of
freely they had no illusions about the
this
what I was doing no promises we're going
to be made and so if we want to look at
big data and big compute scientific
codes are great for that because most
scientists are today are putting their
codes and open source at least some form
and there's a lot of public data devil
so in the process of doing this and
talking with scientists domain
scientists you initially started out
with the usual kinds of stuff you know
how much data rehydrates you know ratio
between data access and computation all
the usual stuff we do as CS folks but in
the process of doing that and getting
more detail about what the scientists
were actually doing I came to learn that
there's a lot of there's something
that's really transformational that's
going on with the way the conduct of
science today and whether using
computing and these words almost sounds
sort of you know platitudinous right now
because they're you know everybody
saying it but it was a really
interesting experience now you know
needless to say I would have time to
share with you the entire nine months
but I I can share with you at least a
couple of case studies and go through
and so actually this talk today will be
mostly chemistry and biology there will
not be that much computer science I hope
that through the process of doing this
you'll get more of a feeling for some of
the issues over the last nine months
I've gone from that's interesting let's
see okay over the last nine months in
order to be able to talk with these
scientists i found that what's really
important for me to educate myself more
on the sciences too and so that's become
sort of an avid hobby for me at this
point trying to acquire this information
there's some amazing resources on the
web to do this in a self-study form so
what I'm going to do I'm going to sort
of take you through a couple of case
studies going to start out with a little
tribute to some of the supercomputing is
going on the cloud today and focus on
one particular piece of work we've done
it at Google at least you know briefly
on molecular dynamics I having to do
with transmembrane proteins and my goal
here is to give you enough of the
science background so you appreciate
some of the questions that are being
asked and then seized or this interplay
that takes place between being of domain
scientists and
having to understand the nature of the
computation that needs to be done as
well there's sort of the third dimension
to this whole space to that I'm not
going to have a chance to touch on but
that's really important and that is to
be effective not only do you have to
understand your domain science and you
have to understand sort of you know
computing its scale you also have to
have sort of a understanding of
statistical modeling too and you see
that coming through time and time again
so that's another dimension to this
field as well so okay so I'll start out
with the chemical dynamic stuff and then
then I want to go into a little bit
deeper into one particular area imagine
many of you I know have have been doing
work in genomic so i'm going to pick a
whole different area of biology and
that's that's brain biology and sort of
interesting we've been working with the
Allen Institute for brain science or
about a hundred feet from my office in
Seattle and I can share with you some of
the challenges that they have with some
of the studies that they're doing and
then so that the computer science apart
will really be more the last part
they'll be about you know after having
done all this what have I learned about
the needs of scientists and to some
degree sort of like how science really
needs to change to include more concepts
from computer science part of this is a
matter of simple stuff that you know
it's just a matter of how you do
computing but there's actually sort of a
deeper understanding as well for example
when you look at at studies of the brain
one of the things that's interesting is
a lot of questions that are being asked
or actually computer science questions
biologists are asking how is computation
distributed how do we get robustness how
do we do how his memory stored these are
really computational questions that you
know you need a computer science
involved I think that's sort of exciting
to terms of how we might blend
disciplines so I know I imagine many of
you had seen this piece of work that was
done and on Amazon there was this was a
work that was done really it's a
supercomputing effort done with drug
discovery and the that the big opt
opportunity here was to be able to do
supercomputing at a much lower cost so
these folks were able to get 50,000
cores on largeness was done on on ec2
and and that was the price involvement
basically what they were doing is they
were looking
at molecular dynamics I'll give you more
of an example what that means and seeing
how how small molecules on some of these
are actually weather called ligands bind
to the proteins but the big thing here
was to be able to do a kind of discovery
at scale that was not possible before
and so this play right here is all about
just providing more cycles that's that's
what this was about so we've also done
this at Google as well we have a program
called exercise I'll on I won't go into
a lot of details about the the
particular system that we've built
there's actually a paper coming out an
Internet computing that describe this a
bit more but the idea behind it is that
if you're able to structure your job as
a large number of fairly small tasks
that basically take one core one get one
gig for no more than an hour then we can
give you hundreds of thousands of course
and this is not something it's available
externally it's something that right now
was built internally and so what we've
done our model is that we hire visiting
faculty and they come in for a year and
that they'll learn about our tools and
be able to use do this work but as a
result of this particular piece of work
we're able to do some science that
wasn't possible in many of these cases
i'll give you an example of one project
so now we're going to get into a little
bit of chemistry so before i start
always like to ask how many people here
have a background in chemistry or
biochemistry okay okay that's actually
not bad for this kind of so so that's
good actually I was recently giving this
talk at at University of Michigan and I
asked that question everybody raised
their hand because I was talking to a
bunch of biochemist so that was sort of
a scary thought on any case I an
undergraduate came up to me afterwards
and she told me that it I did a great
job of explaining these transmembrane
proteins so it was from an undergraduate
though not from any of the pie so we'll
see you know how many case I give you
this the way background they get I'm I'm
an amateur chemist very amateur at this
point but let me share with you from my
perspective what the significant issue
is here so everybody's familiar with
cells all
organisms are composed of cells milk
argue that maybe viruses aren't but but
you know certainly large organisms like
us are and the key thing and one of the
key things about cells is how do you get
molecules into the cell nutrients for
example and how do you get rid of
molecules or produce molecules that have
to go outside the cell like into our
bloodstream and the way we do this where
our cells are made up of a have a cell
membrane around them it's made up of
what's called a lipid it's a kind of
large molecule and the way this is done
is actually very interesting there are
other kinds of molecules proteins which
undoubtedly you've heard of proteins we
we make many many proteins that's a
purpose of a lot of our DNA is to make
proteins and these proteins actually are
structured in such a way that they go
through the cell membrane and the way
they do this is actually very clever
proteins consist of many of media Nino
acids amino acids some of which are some
are positively charged some are
negatively charged and as a result of
those charged characteristics they're
able to embed themselves through this by
lipid layer now the next challenge is
how do you select which molecules will
go through the protein and the way that
works is really about the way the
molecule configures itself whether it's
called what's called the confirmation it
has to actually orient itself into a
three-dimensional shape and that shape
is structured in such a way so that the
size of a molecule that can go through
that protein is determined by the shape
the pores that are actually left
available inside the the structure of
these bodies circular structures in the
protein in addition the protein will
consist of amino acids that have the
right charges and therefore will attract
the right small molecules as they go
through and these proteins are very
specific as to which molecules can go
through so now this doesn't this flow of
molecules through the protein does not
happen in a passive way actually
ourselves and you know most all you know
big organisms do this control this
mechanism by controlling that the
confirmation or the three-dimensional
shape of the protein and this can be
done a few ways in the brain it's done
by by control of voltage there
actually bolted gate gated channels for
many cells what happens is that in the
inter in the cellular side the
intracellular space certain chemical
reactions will happen in the this would
be the the inside of the cell certain
chemical reactions will happen down here
which causes the orientation of the
confirmation of the protein to change
and so what molecules can go through
will change and that's the way the cell
essentially determines what's going to
come in and what's going to go out to
these proteins so a very interesting set
of mechanisms now what I gave you was
you know sort of you know my my
simplistic understanding about how it
works the fact is that the actual
mechanisms that are taking place are not
well understood the a lot of these
mechanisms are taking place as a result
of a very fine grain time very fine
timescale of actions that are happening
a lot of them are taking place in
nanoseconds or even from at 0 seconds
which are 10 to the minus 15 seconds and
their dynamics there's actually the
molecules are not stationary they're
vibrating a bit so we understand we can
use Newtonian physics and that and the
nature of the molecules to understand
those very fine-grain kinds of
interactions but the chemistry that
happens the actual transport of the
molecule what has to happen over a
period of time to get molecules either
in or out that takes place on the order
of milliseconds so we've got this huge
timescale difference between 10 to the
minus 15 seconds and 10 to the minus 3rd
seconds we understand the rules it will
take us from the size of this very fine
you know fine grained time constant to
the coarser grain time constant but that
the time scales are vastly different and
the way we bridge that gap is
computation we just compute a lot of
random two directories over a long
period of time now why is this so
important it's certainly interesting
from an academic point of view but the
reason why this is important is getting
stuff into cells is one of the main ways
we do drug treatments we target these
these receptors to try and get molecules
that are going to be therapeutic into
the into cells or things that we want to
get out of cells I mean those are those
are the things we do it turns out that
this particular
class of proteins these g-protein
coupled receptors account for forty
forty percent of the pharmaceuticals on
the market today cup a target this class
of protein so it's a very interesting
protein to study so let me tell you a
little bit about what it is that we
ended up doing so I here I show you the
but the dynamics that are going on here
and the way this study was done I
mentioned before on this tool this
exercycle tool that we've developed it
requires that you structure your
computation since the small ball pieces
one core one gig for one hour but to do
this kind of simulation from you know 10
to the minus fifteen through ten to the
minus nine seconds to ten to the minus
3rd you're obviously going to take much
much more time than just one hour
although the computations Chloe fit
within one Corran one gig and so the way
this is done is that we're essentially
computing what are called trajectories
we compute the molecule of one point in
time we look at we use random numbers to
sort of do perturbations and and we look
at the molecule over time look at that
position of each one of its atoms in
space once a trajectory has been
computed for about an hour then that
becomes then a new computation that can
be passed on to another task and we
continue on in that manner this
particular set of computations went on
for several months on and the results of
it were many many terabytes of these
trajectory data and so now we got to the
interesting part we largely focus in
sort of computational discovery on
producing the data turns out we have big
problems with analyzing the data to its
now we're in a situation where really
what you want to be able to do is do
some sort of interactive exploratory
analysis of this data and what we found
to be extremely effective was to come up
with a summary statistics for each step
in the trajectory in our case what it
was is was the x y&amp;amp;z position for each
one of the atoms because based on those
positions we could compute bonding
angles and bonding angles are going to
our going to determine the shape of the
molecule and that's going to determine
what get
transported so that's so we wanted to
compete have the x y&amp;amp;z positions for
these molecule for the molecule each of
them in the molecule over the entire
period of time and we found that
basically putting that into a sequel
like database that could operate on
billions of rows was a very powerful
analysis tool so so what I hope I've
given you so far is some sense of you
know sort of the relationship between
the problem a chemistry problem and the
the computations that we were doing let
me also give you a feeling for on this
particular project sort of the profile
of how things went over time so what I'm
showing you here are days into the
project at this point it's been a bit
over a year so this just shows basically
the earlier parts of it and the y-axis
is the number of seconds of simulation
that we study remember the interesting
chemistry is happening in milliseconds
so you know the more these milliseconds
we compute the better off we are on the
the y-axis actually is in microseconds
so we're up to you know five point five
milliseconds so you know we're in the
range where interesting chemistry is
happening now when the project first
started we had some initial data so
that's why you see it's not necessarily
at zero from from some previous work but
we're well under a millisecond and so
you see this gradual ramp up and so what
we found was we were debugging a big
system not surprisingly we had stability
problems so our rate of progress in
terms of producing useful director
trajectories were a little bit slow then
we got to the point where up about until
here now we're getting to the point
where we can actually look at observable
data that we can compare with laboratory
experiments because now we're actually
seeing the results that are the time
scale where chemistry is happening that
that so we made you can see that there's
sort of this linear progress here but it
wasn't until here that we could start
checking the models we didn't have
enough data to check the models until
then we just had this hypothesis about
how it worked during this period of time
we realize there were all sorts of
issues with the correspondence between
what we expect it to happen based on
laboratory data that happened in a
course time scale and what we're
actually
serving from the computations once we
finally got back past all of this area
here then we got into sort of a
production mode where we could then
continue on to the best of our knowledge
about what was actually happening so now
what we're doing is we're doing this
analysis i mentioned before we have the
the many terabytes of data and we're
trying to understand how these models in
detail work and we're looking for
internal consistencies there as well as
correspondence with other literature so
what I fully expect to happen so now
we've produced you know over five
milliseconds of data is it we will find
other issues out here and that will be
will be back into this space before too
long we realize that our models need to
be refined further so I mention this
because it's always useful to get a
sense of the profile that a project goes
through because then this can understand
once we sort of understand that that
becomes a kind of workflow and then we
understand the kinds of tools that we
need as well okay so that's the first
case study i'll come back to this a
little later when i talk about you know
what the requirements are in terms of
cloud computing being able to do science
in the cloud alright so that's not there
all right so let me move on the second
case study I mentioned before some work
that we've been doing with the Allen
Institute for brain science I don't know
how many people saw this in the papers
so the Allen Institute it's largely
funded by paul allen's on Vulcan company
and recently on they announced a new
project there's a fairly famous
neurobiologist his name is christoph
cook he's now the chief science officer
at allen and christoph he's written he
was co-author of many papers with with
francis crick among other things he very
well-regarded Christoph's project is
something called mine scope and Paul
Allen recently announced that for that
one project he's investing three hundred
million dollars over the next three
years and the assumption is that the
project would actually continue on for
10 so quite an investment so those are
the folks that we're starting to work
with what I want to do in this study is
talk a little bit about the kinds of
problems that they're looking at on and
we're
small piece of a very big effort to do
this I'm gonna have to give you a little
bit of background in brain science and
the project itself has to do with
sleep-wake cycles and cats this is
actually based on a paper that this was
a starting point because there was some
well-established science done in 2005 on
looking at at cats at sleep-wake cycles
and sleep-wake cycles are interesting as
a starting point to understanding what
makes us conscious and so that's an area
of ongoing interest that christoph and
others has had okay so um how many I'm
sure everybody's heard of neurons on
neurons are the basic cell that exists
inside the brain we have as humans I
guess the guess is something over a
hundred different types of neurons they
will vary in many ways I'll discuss that
in a little bit but let me show you the
basic architecture of a neuron so we've
got the cell body it looks like a cell
like any other cell it has these
additional structures going out called
dendrites and these dendrites have
places where basically they can receive
signals and the structure of these
dendrites it's referred to as a forest
or Ann Arbor more commonly it's actually
pretty crucial in distinguishing the
types of neurons and the kinds of
interconnections that they can take
place when neurons connect to one
another they connect to these dendrites
and points called synapses and an
electrical current will be generated on
occasion that electrical current gets
propagated across the cell body to this
very critical point right here which is
not well illustrated by the original
diagram called the axon hillock and
that's the point at which if a threshold
current is exceeded there's a current
then that gets propagated all along this
structure over here called the axon on
and the axon is basically the output of
the the neuron and the axon then has
these various output pieces over here
these axon terminals that then can
connect with other neurons so what you
see here you may notice right away is
that we have a whole bunch of potential
input sources over here on these
dendrites and the AO
puts over here these and from the axons
can go to a whole bunch of different
cells so it's not uncommon to see inter
connects on the level of 1 to 1,000
that's pretty common so you have a
massively interconnected structure not
random but definitely massively
interconnected a just a couple of points
of reference to sort of appreciate some
of the the second-order subtlety but it
becomes important terms of understanding
neurons so the way the electrical
current to get propagate I'll give that
a little more detail on that in the next
slide but what sort of interesting about
this is overtime on there it was it was
deterrent the the way evolution work is
there are these insulating segments here
called the myelin sheath it's actually
created by another kind of cell that
wraps around the axon and what this does
is it increases the rate of propagation
across the axon by a factor of a hundred
on and you can imagine if you're working
at a rate of you know one one-hundredth
of your normal thinking rate this is a
big deal so obviously that's a big
advance in evolution there there are
diseases that will actually cause the
mile on to what's called demyelination
of the axon and it's horribly
devastating you can imagine as a result
of being able to lose that competitive
advantage with transmission that said
even though we get this big speed up by
having the mile on cheeves the
propagation delays are on the order of a
millisecond across the axon so we're
talking about a system that operates you
know much slower than what we're used to
electronically but a system that has
massive interconnects far beyond what we
would normally do okay so here's some
frame of references in terms of in the
kinds of you know density of neurons
that you see so sort of going from you
know quote lower level species to higher
level species so you know Cecil I did
this in units of millions so let's take
a mouse which is actually the focus of
study at Allen is about four million
neurons cat has 300 million neurons so
you can see cats should have a
competitive advantage there in general
predators do better on have more neurons
associated with them and human beings
were at a hundred million neurons so and
then the
these are really considerable our brains
are a bit over a kilogram so you know
our densities are in the range of you
know one gram of brain tissue has 71
million neurons in it very impressive
and that doesn't include all these
interconnects as well which are also
there too all right so let me tell you a
little bit i'm not going to go into a
lot of detail here now you give you a
little feeling about how this
propagation works the reason for all
this is I want you to understand enough
about the brain and the brain operation
so that when we get to the point of
trying to do brain science you
understand what it is that the
simulations don't include because a lot
that they don't include and you have to
make a decision about what's important
what's not so the way the that the way
the signals get propagated across the
neurons it's a combination of electrical
forces and osmotic pressure and so
basically there are ions sodium ions and
calcium ions that remember the g-protein
coupled receptors that go across the
membrane and we'll let molecules go
through there are similar kinds of
channels here that go across the
membrane these are what are called
voltage-gated so remember I said before
that these axons will conduct a current
to another neuron and then that neuron
if it gets enough of a voltage
difference around that axon hillock well
then what will happen is these these ion
channels will open up the ions will flow
across and this will create a potential
difference and what that does is then it
propagates the signal across the length
of the neuron now that this is where
this axon the mile on chief comes in
because the way the propagation works it
turns out it jumps you see the blue area
here that's where the myelin sheath is
it's able to jump across that fairly
rapidly because of that insulation and
then it goes along over here and the
reason for the gaps between it is that
actually the axons have additional
structures that can go off and to
provide additional connectivity again
some of the way that the brain has
developed over time to do computation so
so the key things here are that the way
this propagation works you actually have
to take into account some of these ion
channels and that becomes another level
of detail in a simulation of brain
operation
so the last piece of this this
architecture at the micro level is on
are the synapses so the synapse is where
the the axon endings over here these
terminals connect to another another
neurons dendrites now there's some
interesting subtlety s that take place
here the way this transmission works
this is not an electrical transmission
this is a chemical transmission there
are various chemicals on neuro
neurotransmitters that are used five or
six common ones they have different
characteristics some of them are used to
excite or neuron some are used to
inhibit them and then there are men one
of the other interesting subtleties
around this is that it's not just a
matter of so what happens is that the
sending neuron with on the axon the axon
side has structures inside that will
release these transmitters across the
cell wall and those transmitters then
get picked up by a transmembrane protein
on the receptor side and then that's the
cause of stimulation on the other and
that's where some of these voltages then
can be created on the other end so the
transmitters if you want this to work
right we're spraying chemicals into this
gap but in this exonic gap between the
presynaptic and the postsynaptic
complexes and so one of the tricks to
this whole process is you want that
transmission to happen and then you want
it to stop because if it just kept on
pounding away we would not get the same
information being encoded and so part of
the trick to all this is that the neuro
transmitters have to actually be taken
up again by this same ending and again
it goes through the same mechanism if
you have a transmembrane protein it
picks up molecules and brings them back
in it turns out that one of the effect
of narcotic and narcotics is that they
block the uptake of neurotransmitters
and so you get this constantly
stimulative effect and you can see how
that becomes cumulative because now then
you need more neurotransmitter to have
something that's a differentiated effect
so it's interesting it's a to some
degree if you look at this
entire mechanism about how the neuron
works you know you and I as engineers
would not have designed it like this
there's so many ways this can break down
and it is so complicated but this is an
evolved system and so that's part of
understanding the evolved system is to
understand the pieces and when the other
challenges is and to figure out what it
is that you can sort of ignore because
it's part of this massively redundant
system and what is it you really have to
pay attention to to actually be able to
get the right biology when you're doing
a model all right let's go out a little
higher level this is just an example of
brain wiring I'm not going to go into
much detail the only thing I want to
point out is that you seem that you'll
see that there actually is some sort of
structure to the way the these are
really neuron I'm sorry axons from the
neurons that are being interconnected
you see if there's some sort of
structure to this those are grouping and
it tends to be somewhat hierarchical
where you have more local connections
than remote connections and that's that
is certainly fairly consistent across
the brain there's some macro structures
that go around this on okay so let me
take a break here and and give you sort
of an interesting anecdote so um
needless to say people have been
studying the brain for a long time it
was a long time when there wasn't a good
understanding it's hard for us to
believe this but in ancient Egypt there
wasn't really a good understanding of
what the brain did they didn't think the
brain did anything actually when you got
on when you're being embalmed in ancient
Egypt they actually removed your brain
because they didn't think he needed it
um so obviously you're dead so you
didn't need it anyway but but it was
interesting they did they didn't think I
was necessary I'm over a period of time
and you know starting to wear you know
sort of modern science began to take a
hold and there was observing and
reporting of information that became to
come a better understanding of the brain
and the original idea was that the brain
was you know we knew that thinking took
place there that was really important
but it it was this a more civ mass
undifferentiated and a major turning
point actually was in the mid-1800s
there was a report of an incident where
railroad worker his name was Phineas
Gage received a horrible injury he had a
metal rod that actually went through his
prefrontal lobe now it turns out amazing
the man survived he with the rod was
removed but the observation was that his
behavior changed radically the initial
observation because the prefrontal robe
is actually where we as humans that's
one of our most evolved part of the
brains that differentiates us a lot from
primates we don't understand all that it
does but we know that it's important the
the comment from physicians at the time
in the late in the mid-1800s was it was
so fortunate that this injury occurred
in an unused part of the brain so they
subsequently discovered that was not the
case this fellow who had been a leader
very gregarious became really a narrow
do well and had all sorts of
psychological problems but he did
survive another 11 years now I mentioned
this only because this led to an
understanding that on being able to
appreciate what different areas of the
brain did might lead to some fundamental
understanding of the brain and that was
sort of a revolution so over time what
happened was this is a this is a human
brain not a cat brain over time what was
happening was that there's a recognition
that you could actually if if scientists
and physicians sort of connected
examples of injuries that occurred they
could come up with sort of a map of what
parts of the brain did what based on
injuries they observe now it's a little
bit of a challenge because an injury is
not necessarily localized to one
particular area so you have to draw some
degree of inference they can do you know
more systematic studies obviously on
animals where you can actually you know
damage an area cause elysian be more
specific but if you want to understand
the human that you sort of take what you
got but it developed this idea of what's
called bromans map of the brain this is
the pre from the anterior of prefrontal
cortex where phineas gage was damaged
but there are other areas as well for
example of visual areas here then
typically there is a primary area where
signals are received and their
interpretation areas that are secondary
on top of that we have up here sensory
areas and motor areas as well that have
also been identified you can taking this
a step further it's also sort of
interesting to see how in our evolution
neurons have been allocated to
action so this is what Scott this is a
mapping of our cortex as to the sensory
areas and you see that we have a lot of
sensory area devoted to our face our
lips also with our hands a lot of
sensory they're not so much for a feeder
much of their other parts of our body
and for motor the same thing we have you
know a lot of motor areas for our hands
and our tongue so again it is
interesting to see that well there isn't
a relationship between the size of an
animal and the organization of the brain
that one-to-one mapping depends much
more on the evolutionary value all right
so that's my tour of brain science for
you so you have some sort of context and
have some feeling for sort of the
complexity of the system that now we
want to model so I'm going to take you
now through this this work that we've
been doing and sort of a starting point
with the folks at Allen and this is
modeling sleep and wakefulness cycles in
cats and I showed you a cat brain at the
beginning so on what we're going to do
is we're going to ask the question we
have these macro level observations
where we can see sleep-wake cycles and
this is actually one of the examples of
the imaging and the kinds of things that
you're looking for is a cycle where
there is that the it's a it's a heat map
so this is basically low-voltage no
firing these are higher voltages and if
it gets wide it's actually being fired
and you do see this sort of oscillatory
behavior in sleep-wake cycles so that
means you've got some sort of macro
level scale feedback going on and we've
got again these micro level rules right
we understand we understand a lot about
how the chemistry of axons firing these
ion channels we understand that we
understand propagation delays across of
the axons we understand a lot of detail
there the synapses but we don't
necessarily understand how the whole
thing once wired together produces
sleep-wake cycles because that depends a
lot of the details about how much of the
neurotransmitter is spread across what
are some of the places at which synapses
actually occur and therefore these
voltage differentials take place it's a
lot of that detail that actually brings
a
all together so we get a macro level
system that actually produces something
of interest to us so that's the question
can we actually reproduce these
sleep-wake cycles so the way it's going
to do this is it going to take this very
complicated neuron its eyes described it
before and reduce it to a single point
so it's just a single point and what
it's going to do is it's and I'll show
you a little bit of the equations behind
this but it it's going to take his input
some of this data about on current
that's coming a voltage differentials
from from other sources and a little bit
about the ion channels but it's going to
greatly simplify what is here the other
big challenge here are the neural
circuits you know so cats have 10
million ten at ten million neurons I'm
sorry 100 million neurons mice have have
500 million neurons and we're going to
take a small subset of that we actually
don't need the entire brain so the
cortical part is is more like 10 million
um we're going to take a small part of
that and we're going to then try and
figure out what a representative
circuits so this is a huge challenge
first of all trying to track down these
axons and dendrites and the
interconnections it's very complicated
very fine scale you know micro level
kinds of observations and stainings the
second thing is there are individual
differences on you know based on what we
learned and how we develop so ultimately
the the models for interconnects are our
statistical models so this is another
point of essentially variation and study
when you're doing this kind of
biological investigation is you've got
to come up with some sort of model about
what the interconnects are and the last
part is in general we're trying to
calibrate with empirical data and so
that becomes an ongoing challenge
because at each step you want to come up
with with values for parameters that are
representative for what the actual
biology is doing okay so here's sort of
a summary of the kinds of parameters I
mentioned before on there there's no
there's a relatively simple model for
propagation delay i mentioned before
there are many different types of
neurons we're not even considering the
details of neuron and certainly not
different subtypes do do a little bit
with ion channels
and a little bit on some of the synaptic
stuff on so the net of this is if you
want to do the simulation you have
hundreds of parameters so this is a huge
challenge if you think that you're doing
science because you've got a large
number of these parameters you're doing
the best you can to calibrate to a
limited amount of laboratory data but
it's all approximation and what you're
looking for is do you match up the
empirical results okay so go on to a
little more i'm not going to go through
a lot of detail here there's a set of
differential equations that are used to
describe how the neuron behaves this is
basically how neuron behaves you've
gotta this is the threshold and that the
threshold here is Theta and it's it's
dynamically varying depending on this
kind constant and here's how the
voltages are done remember the the
voltages are actually it's a spatula
distribution across the neuron cell but
in fact what this is doing is saying
that well for each one of these um ion
channels so we've got a I've got a
sodium channel here we've got a
potassium channel here we'll have just
simple equations to try and express
those so greatly simplified model okay
let's talk about so I mentioned before
one the other challenges is the neural
circuitry the way this was done remember
there are different areas of the brain
and so what they did here was they
developed a model about the layers of
neurons that could be present their
interconnects and had a statistical
model for what those interconnects could
be and they're both excitatory neurons
and then there are also inhibitory
neurons and and they indicate they
inhibit inhibitions here by the black
and the rather they of where things are
getting excited okay so so now you get a
feeling for sort of the scope of the
parameter space that we're dealing with
and so what I'm going to show you here
is if you put this all together and you
do a simulation and in this case one of
the co-authors sean hill who we've been
working with quite a bit he was the
major one developing this on and he
spent about five years calibrating this
with the biology you can actually come
up with something that looks like a
sleep wake cycle so what we're seeing
here is we're seeing here the potassium
leak current and as this potassium leak
current starts to rock
now you see it transitioning into sleep
things go quiet down here what you're
looking at is this is essentially visual
noise it will start coming back in a
second the visual noise will be accepted
and once that visual noise starts coming
back then you'll begin to see the the
rest of the circuit begin to activate
again and I you see it recovering okay
so how did we get here well we did the
best we could with the biology we
programmed it at a very fine level we
did the best we could to try and
calibrate with macro level observations
and then we did a whole bunch of
parameter tweaking and then we got a
result this is a seminal paper and I
think in many ways it's a good piece of
work but it shows you the huge gap
between what sciences can actually do
with these technologies and what you
would consider we really have have a
deep insight in the discovery and i say
this i think you know I've had these
conversations with Christophe cook who's
very well-known I'm obviously not a
biologist but then Christophe agrees he
says it's it's a challenging state of
affairs okay so like I said before we
cool stuff we got these parameters we
could actually reproduce sleep-wake
cycles and satisfy a bunch of biological
constraints but there are a lot of
parameters here I mean I you know in
statistics there are common methods for
trying to determine if you've over
fitted data I mean like if you have more
model parameters then you have
observations then that's that's a real
problem now I don't even know how to
count those here because the
observations are from you know from
empirical studies and it's not like it's
a single data point so I don't even know
how to do that computation so i have no
idea if we over fit it or not i think
the interesting thing here is that
somehow they were able to search this
huge parameter space and come up with
something that matched I mean that's
interesting I mean if they couldn't find
it that would be a challenge in and of
itself but that search is hugely
time-consuming and this is one of the
areas where I think I alluded to before
I think that progress in science using
computational discovery
massive computation storage are helpful
but there are huge methodological
problems and so this idea about you know
how can efficiently explore parameter
space is very interesting on you know
and then we get into some of the more
details about the the methodology about
there really weren't systematic ways
about going about this and the last part
and Sean actually is really good in
terms of the codes he produced those of
you who have worked with scientists and
looked at science codes especially
produced by you know graduate students
working in domain sciences it's a
challenge to make your way through this
it's very hard in sean's codes were very
nicely structured but that's a rarity
okay so now again the last section here
about sort of random musings this has a
little more of a computer science flavor
to it so I think you know there's a lot
of things that people talk about for
science in the cloud so I think this is
an obvious one scientists are always
interested in more capacity this idea
that you might rent you know a huge
number of cores or huge amount of
storage rather than buying it and having
to maintain it I think there's a lot of
lot of appeal to that but I think it
goes a lot beyond that you know one of
the huge problems today and it's
actually not limited to computational
science it's actually a big problem even
with experimental science it is
reproducibility a lot of mentions of
this but but in the Eric computational
area there's a lot we can do to address
this so I mentioned before this issue
about codes that are poorly structured
one of the implications of that is it's
very hard to take someone else's code
and get it to run on your system and
reproduce what it is that they've done
and and if we don't have reproducibility
you can ask a lot of questions about
where our science is so I think that's a
huge concern obviously if you can you
know pick up someone else's code and use
it that's a huge deal on we've been had
some discussions from time to time with
NIH with their computational biology
department and they're saying that one
of the issues they have is that there's
so much resource that's wasted just
basically getting to the point where you
can do a study and if instead the that
the underlying algorithms and data were
already present in code in a common
repository luck on a cloud that would be
a big advantage in terms of efficient
see of research dollars and that gets
the last point about that yeah there's
this opportunity to be much more fish in
terms of how we do do research okay so
those are the peel their number of
issues i classify the issues really in
in two categories first of all there's a
system stuff that we as computer
scientists know ok so the brain
simulation is an interesting one you
know I work at Google we're not known
for fine-grained parallelism we don't
build supercomputers in the you know the
sense of like a cray so what are we
doing with brain simulation so that's
actually one of the questions we're
trying to explore a bit and you know the
initial parameters of the space were
that we would just look at this in terms
of how to adapt maybe some of our
infrastructure to maybe do something
more in the way of fine-grained
parallelism but as we got into more the
details of the science and as you could
tell these models are very very
approximate and for someone to say that
you have to do X or Y to be faithful to
the actual architecture of the brain you
really have to make an argument from
from the biology as to why it makes a
difference and so one of the things that
we're exploring is when we make
approximations can we also do some
judgment as to maybe there's a trade-off
between the fidelity of what it is it's
done relative to the biology not
relative to some other computation
relative to the biology versus what is
the computational advantage like how
much parallelism you can get so I think
that creates basically a whole new set
of trade-off so you might explore on one
of the things that I've been very
impressed with we worked with a number
of biotech organizations in Seattle and
we as computer sciences what we think is
easy and trivial is surprisingly
non-trivial to scientists even though
they have a great technical background
on for example MapReduce or you know
Hadoop you know the same kind of
programming paradigm going from a
scientist who's working in our you know
just in a high-level statistical
language maybe does a little bit of
scripting but you know it's not doing
you know thinking in terms of you know
engineering something big to something
where they have to worry about mappers
and reducers it's a non-starter I mean
you just can't start the conversation
so one of the things we really have to
pay attention to us if we're going to
provide scientists with basically a
low-friction situation where they can
work with data small data on their
workstation and see a relatively low
friction transition to using their same
algorithms and code at scale when now
we're working with many terabytes or
even petabytes we've got some work to do
because the main ways we're doing it
right now really or not they're not
receptive i mentioned before in that the
first first case study the one on
molecular dynamics one of the huge
challenges was after we produce these
many terabytes of data how do we analyze
it and we found like a very scalable
sequel was effective you're not not all
these analyses can be done with a sort
of a declarative approach I mean
sometimes you need an imperative
language like are so be able to do
something like that at scale and have
interactive performance so you can
actually you know explore some data is
extremely helpful ah this is an
interesting one so um another thing that
came out of the study with molecular
dynamics remember that the progression
was that you know first we had to get
our infrastructure to work right and
then we got to this point we're
essentially we're debugging the science
we had gotten a couple milliseconds of
data and now so we could sort of figure
out whether or not the science was
working right okay so what's going on
there is that we're not doing a whole
bunch of short runs in fact we're doing
some very long runs and what we're
interested in is how these trajectories
you know these orientation of the
molecules and 3d space how they're
evolving and so we needed something
which is sort of a combination of
traditional batch wear something runs
for a long period of time with something
that's interactive they can tell us ok
here is the current state of my system
here's what's going on maybe even some
pictures and some animation and we've
been calling this introspective badge so
it sort of combines that the the two of
them together and that seems to be
pretty important to science because of
this nature of wanting to do very long
computations and wanting to see how the
system itself is if it's actually
working as expected on the last one I
mentioned in terms of system challenges
is I think in some way scientists are a
little bit ahead of us in the computing
community
in that we're really focused on our
clouds and to them it's it's sort of
obvious they need multiple clouds
because the data are stored in multiple
places their store multiple aces because
they're too big to move there there's
sort of multiple places because of
transporter data flow they store
multiple places because there are
proprietary concerns there's a whole
bunch of good reasons why the data will
not be in the single cloud and so we did
this has some very interesting
implications for us in terms of we we
ideally wanted the data in one place so
that we could bring computing to the
data but the data are inherently
distributed we've got a challenge in
terms of how we build algorithms I tote
so those are the site those are the the
systems considerations you know and i'm
going to i guess i can't get myself into
too much trouble here because nobody
here is really a domain scientist by
trade I was at a meeting last week I
guess a couple peony oh and Jeffrey were
both ours where I got myself into
trouble very quickly but by saying that
scientists are really bad software
engineers and and that their immediate
response which I had to agree with was
well software engineers are really bad
scientists and I i agree i think that
didn't mean the saying that it has some
implications but but let's sort of blend
this together try and put this in a
little more positive light on so um one
of the huge challenges here is being
able to develop your code in tandem with
understanding with the science this is a
little different kind of thing than what
we do in in computer because we sort of
think of you know more like gathering
user requirements and then maybe it's a
rapid prototyping cycle but you know
still you know we're not trying to
painstakingly calibrate with detailed
literature the other thing which I found
really intriguing is um testing so now
this comes to two levels one is the
obvious level and the obvious level is
that you know as a software engineer you
want to make sure that your codes just
do what they're supposed to do there's
another important one of those and that
is that the testing you're also
interested in is if I've made up change
to my model all those things I've
painted Kingsley calibrated to to the
biology or to the chemistry of the sign
in some way do they still calibrate
right me to us as software engineers it
seems like another kind of sort of unit
tests right I mean sort of an obvious
thing it's totally foreign to the
scientist this should be part of the
methodology on so this is another one
which I think it's interesting sort of
like you know once we get past all these
problems we can build you know
individual models let's say of chemical
dynamics then cell behavior then maybe
tissue behavior and organism behavior
and he caught in behaviors in the
ecosystem you'd like to have you know an
integrated stack of models so trying to
figure that out okay that this one here
is actually sort of a funny one okay so
there's some obvious things here about
you everybody hears about scientists
have all this data it doesn't
interoperate so that's a problem there's
been a lot of attempts to try and
develop common schemas and the reason
why they fail is not so much that
scientists are obstructionist they're
not MMA that's a little bit true but
it's not totally true that I think a lot
of it is that the science itself is
evolving and nobody wants to be left in
the straitjacket so I don't know exactly
how we accommodate that but bill Howe
had some interesting idea about maybe a
progression of schemas that might be
using so I mentioned this culture of
software engineering okay so this is a
very interesting one about the culture
one so I had a discussion with an NIH
program manager on I was actually on the
side of it but then I sort of had to
insert myself who is saying to someone
else is that I don't understand all this
complaint about you know software and
and and you know having software be
maintainable you know software languages
die all you have to do is just described
you know the general principles in an
article and that's enough and I was
really frustrated I was trying to
explain to her you know what this means
in terms of not having reproducible
results in like and the one argument
that really caught her attention because
she is a bench scientist by training as
a look for computational discovery
software is your lab notebook software
is where you're putting all the
information about how you calibrated
with with empirical results it's all in
the software it's not some general
principle it's all the detail and that
seemed to resonate with her but I think
there definitely is a cultural challenge
okay that's the end of my random musings
and i will pass microphones around yes
we say your name first I'm neo Martinez
and the hi Joe I was wondering with the
neural um let's see comput simulations
what was the biological insight do you
think that was achieved by that exercise
so um the main thing what there were
several results that are somewhat
technical but one of them that was very
interesting was the central role that
the potassium leak current played and
that was a man just go back quickly
maybe I can show you just that I didn't
explain that part of the diagram so well
okay oops let's go back here okay so
that's this a chart over here and you
see how it's flat and when it gets to
the point where it shoots up that's when
we go into sleep and it was that there
wasn't a good understanding about at
that time about how that might be a
pivotal role in terms of making this
transition from sleep to wake and just
wonder if I could talk long enough so
it's just about to come up okay now
you'll see it go so once that potassium
leak current Rises now we go into sleep
and that mechanism was poorly understood
that was probably the most critical one
there were a little bit more in the way
of other technical details but I think
in general what you're looking for with
stuff like this is you get some insight
from the computation side and then you
have to verify it in the lab in some way
oh thank you for your presentation ahsan
tiberium I have a question regarding
your first research you had running
about a year of computation for your
protein membrane analysis how many cores
very included in that and how many
proteins have you been simulating if you
know okay so um thank you the I gave you
that the time lapse are the project of
course there were periods of time when
we weren't running anything because are
either our science was broken or
infrastructure was broken but when we
were running on it was on the number of
cores varied tremendously but it was in
the many thousands yeah another question
yes is there anything being done to use
tomography to visualize and let's say to
analyze what is happening in real time
in the brain and then they say using the
cloud to to try to see real time what is
happening there so so finally have
questions you know what might be done
like in real time to do analysis of the
brain there there are some amazing
studies that are going on for using
really high-level information from the
brain with if they don't even need the
cloud I mean these are localized
equipment to be able to do control it's
actually pretty impressive then there
was a this month's I think I Tripoli
spectrum has something about you know
quote mind-reading we're actually you
can but they've done is they've looked
at fMRI studies and related that to the
visual input to try and then decode
essentially the transmissions that are
going on inside the brain now none of
this actually necessarily involve the
cloud but you know that's very
interesting from the point of view of
science um in terms of like real-time
data being extracted to the cloud it's
not something we're involved with I know
maybe other teams are other question hi
Lucas cancer from Czech Technical st in
Prague is there something in your
compute infrastructure that you use to
simulate the brain that is fundamentally
different to what you Google is using
for other tasks or you know aspects that
would be different so with the current
weight and this is actually sort of this
is a direction we're going not a
direction where I think we've
accomplished yet so right now what we've
done is we've taken the work that this
reference paper sean hill and mika
tonini have done and we've been able to
this was code that sort of had rotted
over a period of several years we've
been able to reproduce it on modern
hardware we've gotten reasonable speed
up just from that on so I think the next
step is to try and understand what we
might do
ooh with this kind of simulation inside
the cloud if we're going to scale
massively and we don't have answers
there yet any other question okay thank
you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>