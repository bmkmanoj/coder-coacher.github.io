<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Random Walks on Sandpile Groups | Coder Coacher - Coaching Coders</title><meta content="Random Walks on Sandpile Groups - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Random Walks on Sandpile Groups</b></h2><h5 class="post__date">2016-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qGb_iG20K_g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone so for some
years I had a dream of the spectral gap
of a random walk being related in the
inverse way to the spectral gap of
another process on the graph like the
easy model and this and then I saw this
remarkable paper that Daniel wrote with
Lionel Levine and John Pike which
realizes this kind of dream in the
setting of the sand pile so I'm very
happy we'll hear details about this from
Daniel Dorothy please
thank you you vote for the invitation so
this is joint work with Lionel Levine
Cornell and John Pike who is just
finishing the postdoc at Cornell and
will be starting at Bridgewater State
University in the fall and it's that
archive paper okay so I know there are
some people here who are experts on
everything to do with sand piles and
other people who may not know very much
about it so for the beginning of the
talk I'm going to briefly introduce what
the sand pile group is and what the
model is and then I'll talk about what
the random walk is that we have defined
and then there is also to be able to
prove about it so you start off with a
graph I'm going to have it be a simple
undirected finite connected graph the
simple part is not that important but it
is necessary for it to be undirected in
order for some of these things to work
and the number of vertices will be N and
the sand pile group is a way of defining
an abelian group from this graph so you
might think okay how do I take a graph
and create a group and it turns out that
a number of different people have had
different perspectives on this but they
all ended up with the same group in the
end and so they gave many different
names and so it was invented by
statistical physicists because they
learned to study something called
self-organized criticality it wasn't
invented by combinatorial lists because
they invented a game called chip firing
even an economist decided to
invent a version of this and it all
comes down to the same thing in the end
and then there's also a connection with
kind of discrete arithmetic geometry
which I don't know anything about but
they have their own way of thinking
about this and then more recently there
is something called sandpile PDE
that was invented that you know has to
do with the scaling limit and there are
connections with computability theory
you can kind of make a version of the
halting problem that has to do with this
anyway
it's a nice enough model that it has
interesting connections with a varied
list of other fields of math so what I'm
going to do is to define this random
walk and talk about mixing properties of
the walk so here's the description of
the model we have a finite graph here's
a graph and you choose one vertex to
designate as the sink that one and a
chip configuration on the graph assigned
some non-negative integer number of
chips to each of the non sink vertices
so if I could for instance have three
chips here one chip here and three chips
there and you say that a vertex is
stable if the number of chips of the
vertex is lower than the degree and it's
unstable if it's equal to the degree or
higher and so a toppling operation is
allowed if the stack is unstable and
what happens then is that the stack of
chips topples over and one chip gets
sent to each of the neighbors so if this
vertex topples then the three will get
replaced by zero and then this will
become four and this will become two and
then the chip that went to the sink just
disappears so
then maybe this one can topple so this
one it now has four chips so it can
topple twice if it wants or once let's
only topple it once so then you have two
here and this becomes two three and this
becomes two one
and now this one is still unstable and
this one is also unstable but you can
see that if you just keep on toppling
then eventually enough chips will fall
into the sink that you wind up with a
stable configuration and the so called
abelian property of this model is that
it doesn't matter in which order you do
the toppling 's you know I could do this
one next or that one next and so forth
at the end when it stabilizes it'll
stabilize to one unique configuration
and it's also true along the way that
the number of times that each vertex
toppled does not depend on what already
have chose to do the top lengths so this
was proved by Darr back and I think 1990
from the statistical physics point of
view okay so that means that if you
start from any configuration then you
can define its stabilization which is
down at the bottom of that slide which
is uniquely defined okay so I let
capital s be the set of all the stable
configurations and it's not a group but
it has an operation which is point wise
addition followed by stabilization and
the operation is associative because of
the abelian property that if you first
add a21 and a.22 but then a 2 3 or the
other way around you always wind up with
the stems the same stabilization at the
end so I define a subset of the stable
configurations called the recurrent
configurations which are the ones which
are always reachable so if I imagine
here that I have for instance one chip
here at 0 and 0 and then you kind of
continue to add
chips at some point you're never going
to be able to get back to this
configuration just because if you have
some stack of chips here something there
and some there then it's not going to be
possible to topple so many chips into
the sink that you recover this so even
though this one is stable it's not
recurrent and there is a test that you
can do to see whether a configuration is
recurrent or not you know the definition
is just as given that if it's always
reachable from any stable configuration
at your current so one configuration
which is always recurrent is the one
where the number of chips at every
vertex is equal to the degree minus one
so if you put two here two here and one
there that you know each vertex is
filled to capacity so that one is
definitely always reachable and then
anything which is reachable from that is
also recurrent and so it turns out that
if you just focus on the recurrent
states that that forms a group and so
you might wonder what about inverses
well it turns out that those exist
you might also wonder what about the
identity because the identity in the
stable configurations is just you know
all zeros that's okay but it turns out
that there is a an identity
configuration which is recurrent and
that on you know big graphs has a
beautiful structure which I'll give a
picture later in this talk okay so the
graph laplacian matrix has a lot to do
with the structure of this group and
gives another way of describing it so
there's n vertices remember including
the sink so the laplacian matrix is n by
n matrix on the diagonal you put the
degree of the vertex and then on the off
diagonal you have either 0 if the you
know an entry IJ if vertices I and J are
not adjacent and one if they are
adjacent and if there's multiple edges
then you can have you know negative
numbers larger than 1 so it's a
symmetric matrix and the sum of every
row in every column
zero because you have the degree and
then minus one for each edge on the
other entries okay so notation let Z 0 n
be the sum the set of all column vectors
where the entries sum to 0 or 0 and the
same thing for real numbers and if I
have a chip configuration like this I
can lists you know some ordering of the
vertices say 1 2 3 4 and then I
associate that 1 2 3 4 and then I
associate with this configuration the
column vector 0 0 1 minus 1 so I just
list the number of chips at every vertex
and then at the sink I put a negative
number of chips so the total sum is 0
and of course there's a unique way to do
that so in this way any chip
configuration can be identified as an
element of Z 0 n and once you do this it
turns out that the sand pile group is
isomorphic to Z 0 and mod a lattice of
full rank
so that is either n is contained in R 0
n which is an N minus 1 dimensional
space and the you know I would look at
that Delta Z n as the integer span of
the columns of the laplacian matrix
that's also n minus 1 dimensional
lattice so this is a finite group and I
look at it as configurations mod top
links like if I were to topple this
vertex without worrying about you know
that there aren't enough chips that
means I take three from here and I put
them you know one two and there and what
that does is it subtracts a cut the
third column of the laplacian matrix
from this vector you know you go minus
three plus one plus one plus one and
that's one of the columns with laplacian
matrix
so this is a way of writing the sand
pile group kind of explicitly so that
it's clear that the group doesn't depend
on which vertex you chose to be the sink
because this description is independent
of which sink vertex you chose and it
also follows from this that the order of
the group is the number of spanning
trees because the theorem says that the
your principle minor of the laplacian
matrix is the number of spanning trees
okay so this slide doesn't have anything
to do with the rest of my talk it's just
you know things about the sand pile
group so this is a very cool result of
Melanie wood if you take an erdos-renyi
graph with fixed probability P that does
not depend on and the number of vertices
and you send n to infinity what is the
group is it a cyclic group for instance
and she conjectures that it is cyclic
with probability zero point seven nine
and this is the Riemann zeta-function so
it's the product of the inverse values
with Riemann zeta-function at odd
numbers and you know she has a good
reason for making this conjecture and
indeed the upper bound is proved and the
lower bound is conjectured to be true so
you know you can write this down but
actually finding out what the group is
is a very difficult question and then
this picture there if you've gone by one
of the posters on the second floor it
has this exact one this is a picture of
what the identity element of the
sandpile group looks like when the graph
is a large square grid where the
boundary is identified as the sink and
then you pick the recurrent
configuration which acts as the identity
and it's color-coded because it's the
square grid each site has zero one two
or three chips at it so it's colored one
of those
or colors and it's this beautiful
fractal structure on the outside and has
the solid green square and then it was
proved by Levine pegged inand smart that
there is this kind of variant of
Apollonian circle packing they call
Apollonian triangulations I don't think
that for they that they proved that the
Apollonian triangulations appear in the
identity element but it's almost
certainly true that it has to do with
this so all kinds of cool structure now
I'll define my Markov chain which is the
actual subject of the talk and this is
very simple you have a configuration and
you just choose one of the vertices
uniformly at random and you add a chip
and then stabilize so that's one step in
the Markov chain and you can see that
even if you start at a configuration
like this eventually you'll end in the
recurrent States and stay there forever
then at that point it's a random walk on
the sand pile group so I will denote by
P the transition matrix for the chain
and I want to know how fast it converges
to stationary distribution which is
uniform because it's a random walk on a
group and the I'll be talking about
total variation distance and also l2
distance and total variation distance is
given by this it's kind of if you take
the work the worst possible set where
the measure that you have differs from
the measure that you want then what's
the difference in the measures that are
assigned to that set and of course it
doesn't depend on the starting
configuration because it's a transitive
walk ok so the mixing time with
parameter epsilon is the first time that
the variation distance drops below
Epsilon and this is not a reversible
random walk it
would be reversible if you at every
stage you could either add a chip or
take away a chip but you're only adding
chip so it's not reversible but it is a
normal matrix that is commutes with the
time reversal and so all of the bounds
that you would expect to be true for
reversible change are also true in this
case if you look at say the absolute
spectral gap or the absolute relaxation
time so that's what this slide is about
so this formula that the total variation
distance is bounded above by the sum of
powers of eigenvalues absolute values of
eigenvalues because they're complex but
it's normal yes it commutes with its
time reversal which is just in this case
the matrix commutes with this transpose
so and then this thing is actually kind
of cool that the the moduli of the
eigenvalues of the random walk are
independent of the choice that you made
for the sink vertex and this is true in
the way that I've defined it but for
instance if you were going to make a
continuous time version of this Markov
chain then the mixing time would depend
on the sink that you chose it could be
off by a factor of n depending on which
sink you choose but vote for the
discrete-time version where you know it
chooses also sometimes you drop a chip
at the sink with probability 1 over N
which does nothing then this Fundy which
is the l2 distance from stationarity
actually is independent of the choice of
sink so that can be proved
banging this week old cookies time don't
differ much um so what happens is that
when you you can have a situation where
it's very nearly periodic if you do the
lazy chain yeah but this one isn't very
lazy it's only lazy with one over n
factor all the eigenvalues the absolute
values the eigenvalue stop yeah those
themselves do but what happens is that
when you change the sink then each
eigenvalue does a rotation but each
being valleys are different rotations
but their absolute values stay the same
yes yes okay so based on that I want to
look at the absolute spectral gap which
is the smallest value of one minus
absolute value of lambda for a
non-trivial eigen value and the absolute
relaxation time which is one over the
gap and then this is the standard upper
and lower bounds for mixing time in
terms of relaxation time and they work
here just as if the Markov chain were
reversible so lower bound is the
relaxation time upper bound is the
relaxation time times the log of the
order of the group and the order of the
group is the number of spanning trees of
the graph it should be pretty large okay
so a couple of examples these are really
the main two examples that I'll return
to through the talk because they
represent kind of two extremes so you
can have a cycle
and split the sink up here so it's an
end cycle and the number of spanning
trees is n because you just remove a
single edge and you get a tree and also
the number of generators of the random
walk is n because you can choose any one
of these n vertices to drop the chip at
so it's the group is Z mod n as it turns
out and in fact what happens is that you
jump directly to the uniform
distribution after a single step because
the end generators all produce different
elements so the random walk mixes in a
single step perfectly and the other
extreme is when you have a complete
graph
so here's my sink and a label special
vertices V and W and I'll keep track of
the number of chips that V minus the
number of chips at W as the Markov chain
proceeds okay so let's say that you add
a chip at V then that number will
increase by one and if you add a chip at
W the number will decrease by one and if
you add a chip anyplace else then maybe
it causes some very complicated sequence
of toppling x' but it has the same
effect on V that it has on W because all
of the other vertices can't distinguish
between V and W and so the only thing
that can happen is let's say it starts
out that V has more chips than W they
get added chips and then eventually V
topples but W doesn't and when that
happens V sends away n minus 1 chips and
one of them goes to W so the difference
mod n does not change and so if this
this Y quantity only changes when you
drop a chip at V where it increases by 1
or at W where it decreases by 1 and then
if you just keep track of that statistic
it's performing a simple random walk on
Z mod n with you know lots of laziness
it moves one to the left or one to the
right with probability 1 over N
otherwise it states and the amount of
time that it takes for that to mix is n
cubed so definitely the overall sandpile
chain has to mix in time at least n
cubed and in fact it's n cubed log n
because you have it not just for these 2
vertices V W but any pair of vertices
and so based on that you can get a lower
bound of n cubed log N and this thing
there is actually an eigen function for
the sandpile chain that if you take a -
V - ADA of W and take root of unity -
that power
and it turns out that's that exact
function is an eigenfunction the
eigenvalue is one minus constant over N
cubed and then you have many of these
and that gets you a mixing time at least
n cubed again and then that is the right
answer that's the right mixing time for
this random walk okay so two lessons
come from this first of all there seems
to be at least in these two cases an
inverse relationship like U of L was
saying there's introduction between
the simple random walk and this chain
you know here simple random walk takes N
squared but the sandpile chain because
instantly here the simple random walk
mixes in constant time or instantly with
the right laziness and the sandpile
chain takes n cubed log N and the second
thing is that we could probably make
progress by looking at the
eigenfunctions you know if we can
compute those then those will allow us
to find out information about this
spectral gap in mixing time and so okay
random walks on groups there is a great
theory for this and if it's non abelian
group you have to look at the
representations
it's an abelian group so all the
irreducible representations are
one-dimensional so all you need to look
at are the characters that is the
homomorphisms from the group to the unit
circle in the complex plane okay so this
is the general theorem for
eigenfunctions of random walks on a
billion groups you have random walk on a
billion group and mu is the measure
driving the walk then no matter what mu
is the eigenfunctions are the same they
are the these characters within the
eigenvalues depend on the measure which
is driving according to this formula
which is reasonably simple so that's I
guess due to die CONUS in the 80s
probably
okay so well I mean yeah the proof is
about one line but you had to come up
with the idea anyway okay so so we had
this way of describing the group as you
know a lattice mod another lattice and
so if you just do some like diagram
chasing say okay what are the
homomorphisms from this thing to the
unit circle then the right answer is to
take the dual of this lattice MA the
dual of that lattice and when you do
that you wind up with this thing the one
on top is you take the pseudo-inverse of
the laplacian matrix x e0 and and then
you mod out by something which is not
important you know that is what it is
isn't important but the point is that
the dual group kind of naturally has
this lattice structure and so the first
main contribution I think of our paper
is a more concrete way of looking at the
eigenfunctions you know that one back
there is also going to be very useful
but the concrete approach is built out
of these things which we call
multiplicative harmonic functions so an
example of a multiplicative harmonic
function is say on this graph I'll have
the function value be 1 here 1 1 at V is
Omega
and at W its Omega inverse where adult
Omega is a 5th root of unity and the
property is that it's kind of a
multiplicative version of the harmonic
property when you take the value here to
the power of the degree it has to be
equal to the product of the values of
the neighbors so here Omega to the
fourth equals Omega inverse because
Omega is a fifth root of unity
so that's a multiplicative harmonic
function and the theorem is that first
of all these form a group by point-wise
multiplication its preserves the
harmonic property and the characters are
determined by these things and the
character group is isomorphic to the
group of these multiplicative harmonic
functions and the correspondence is a
reasonably simple formula if you have a
chip configuration with say one two
three four and five chips then the value
of the function is Omega to the third
times Omega inverse to the fifth and
then the even better thing is that
there's a very simple formula for the
eigenvalue based on the values of the
multiplicative harmonic function it's
simply the average so in this case it's
1 plus 1 plus 1 plus Omega plus Omega
inverse over 5 and if you imagine a big
complete graph you can put ones
everywhere except for a single Omega and
Omega inverse and that's how you get one
minus constant over N cubed for that
eigenvalue okay so there's two different
ways of describing the dual group that
means there has to be a correspondence
between them and so I'm going to
describe that now so suppose that you
have this H function and you write h
equals e to the 2 pi I times G so in
this case the blue one is H and the
green one would be G so G would be zero
zero zero one fifth and 4/5
and so the property that the G has is
that it's harmonic mod Z that you know
1/5 times 4 equals zero plus zero plus
zero plus 4/5 that's actually true but
if you have for instance zero times four
then it's the sum of the other ones plus
my as an integer so another way of
looking at that is that the look when
you take the grafoplast and apply it to
G you get a vector whose all coordinates
are integers and then if you subtract
off the mean so that the overall sum of
the coordinates is zero that's that G
bar over there then you get your dual
lattice factor and of course you made
arbitrary choice for G you know I could
have added any integer value to these
and when you do that you get an answer
for G bar which is off by an element of
this WN so that's the correspondence so
I will call this pseudo inverse Laplace
same lattice to be the dual lattice and
so this means that the the eigen
functions and the eigen values have a
lattice structure and that will let us
you know prove things about the spectral
gap in the mixing time okay so this is
the way that the lengths of vectors in
the lattice relate to the eigen values
so you can see there's this 8 and the 2
pi squared so there's a up to a constant
factor the gap between 1 and the modulus
of the eigenvalue is determined by its
Euclidean length in the dual lattice if
you pick the shortest vector in the dual
lattice that corresponds to that
multiplicative harmonic function so for
this one I guess you would take this and
then it's sum is 1 so you have to
subtract 1/5 from all the coordinates
and then you have to massage it so that
it's the shortest possible one once you
have done that
then the squared length divided by n up
to constant is the gap from one of the
eigenvalue this is not that hard to
prove the the the reason that it's off
by constant has to do with the
approximation of cosa like 1 minus
cosine x by x squared over 2 which is
very good near the origin but once you
get kind of far off it's off by a
constant and so what that means is that
the absolute spectral gap of the Markov
chain is determined by the shortest
vector in the dual lattice with an
exception which is sometimes the
shortest vector in the dual lattice is
actually one of the ones that
corresponds to the trivial eigenvalue it
can happen so once you have got rid of
all of those then the shortest remaining
vector in the dual lattice determines
the spectral gap of the sample chain
okay so these are the bounds that we
proved on the spectral gap the so we
have the maximum degree of the graph and
then beta is the spectral gap of the
graph laplacian matrix so the lower
bound upper bound for the spectral gap
of the sandpile chain and if you apply
it to the cycle then this upper bound is
completely terrible the lower bound
neither bound is correct but the upper
bound is really awful so the degree is 2
right so lower bound is 1 over N and the
beta is 1 over N squared so actually
this right hand side is n cubed so we
just proved that the spectral gap of a
Markov chain is it most n cubed ok so
it's actually equal to 1 it's neither 1
over n nor bigger than 1 of course it
can't be bigger than 1 on the other hand
for the complete graph this is correct
on both sides the left side is n cubed
the right side is also n cubed and in
fact whenever
graph is an expander either a regular
expander or nearly regular say the
overall ratio of the max degree to the
min degree is bounded by universal
constant or something like that then the
left and the right side of this bound
have the same order so we have the
correct order of the spectral gap for
the sandpile chain and this is the
inverse relationship or at least the
best we've been able to prove for the
inverse relationship because for the
expander that's where the simple random
walk mix is the fastest and that's also
where this upper bound is the smallest
so that means that the spectrograph of
the sand pile chain is as small as it
can be up to constants okay so the next
slide will have proof of these two
actually neither one of them is all that
hard once you have built up the theory
of the lattices yes hmm of the grass
epoxy so that means that if the degree
is D and it's an expander then beta is
proportional to D
or complete bruh so I guess I think of
expander as like when you normalize the
laplacian then the spectral gap is
constant
oh and you don't normalize it and it's
regular the spectral gap is proportional
to the degree what is beta beta is D or
something D or n yes I guess deep that's
right it's not normal as possible I see
yeah this seemed to be the most natural
thing that was true the relationship in
terms of the graph laplacian so I mean
if it's regular then it doesn't matter
which one you take if it's not regular
then I think this is the relationship
that's true and if you were going to try
to figure out what if there's
relationship in the normalized laplacian
and there might not be a nice one
okay so the lower bound this is
basically saying that a because it's a
lattice it can't have points which are
right next to the origin so we imagine
let's say that we have the
multiplicative harmonic function and the
multiplicative harmonic function takes
values on the unit circle right like the
blue values there and the situation that
would lead to an eigenvalue very close
to one in absolute value is if all of
the values of the function were in a
very narrow arc of the circle yes so
when you're defining multiplicative
harmonic functions did you require yeah
but they if you allow them to take
values in C minus zero and you require
that the sinc be you know normalized to
be on the unit circle then they all are
on the circle by a maximum principle
thing so that's not a restrictive
assumption okay I guess the sync has to
have value one but pretend that it
didn't that would be okay imagine that
all of the values of this thing were
here so that means that the average
value would be very close to the unit
circle and I want to claim that it
cannot happen unless they actually are
all at one point which would correspond
to constant function so when you take G
which is you know the the angles then G
has to be kind of integer harmonic and
so you take the laplacian and you get a
number which is composed of the
differences between angles now if the
angles are too close together then you
get a sum which has to be less than one
but it's an integer so therefore it's
zero so that means something like this
is not possible if the interval is too
small and if you
to figure out what bound that gives you
it gives you exactly the lower bound
here so for the upper bound
remember the spectral gap is determined
by the length of the shortest vector in
the dual lattice so the generators for
the dual lattice are these IIE J's or
standard basis vectors you have to do AI
minus EJ because it has to be you know
coordinates summing to zero but in fact
all of these are short vectors if you
have an upper bound on the operator norm
of the inverse the su inverse of the
laplacian so the operator norm the l2
operator norm of the pseudo-inverse
is equal to one over the spectral gap of
the graph laplacian and so when you run
through that you get this bound you know
constant over beta squared and that's
the same 1 over beta squared that comes
here so I'm not in this part I'm not
glossing over any hidden details that
really is all you have to do no there
aren't ing tells to gloss over I mean I
guess what I'm trying to say is that we
were very surprised to find this inverse
relationship but then it turns out that
the proof is relatively short okay so so
what this says is if I take any basis
vector then I multiply it by the pseudo
inverse now I have a vector in the dual
lattice and it's a short vector and so
in fact I have a full basis of short
vectors so at least one of them is a
short vector which isn't one of the bad
ones that secretly corresponds to the
trivial eigen value and then since the
vector is short we know from here that
the spectral gap is bounded
of by you know that length squared over
n okay so that's what we can say about
the spectral gap so how about the mixing
time so upper bound for the relaxation
time is d squared N and the size of the
group could be as large as D to the N
what it actually is is usually some
constant to the N where the constant is
less than D let's say like D over 2 or
something like that so the width log of
this is close to M log D in practice and
so the standard bound in terms of
relaxation time is d squared n times n
log D and it turns out that we can
improve that d squared n Times log n
which is better right this gives n cubed
log n for the complete graph and the way
that we do that is by using this thing
called the smoothing parameter which was
invented by me John Cho and reg EV who
are computer scientists studying lattice
based cryptography but it's a lattice
invariant and it just happened to be
exactly what we need and they already
proved the things that we need to use so
that was very nice so of course we want
to use all the eigenvalues and not this
just the spectral gap all of my bounds
are l2 bounce so I'm not going to do any
kind of delicate stuff relating to l1
versus l2 the sum of the powers of the
non-trivial eigenvalues you can show is
bounded above by this thing so X is dual
lattice vectors what this actually is
each lambda corresponds to a particular
vector in the dual lattice you could say
you know here's your lattice and then
you have a fundamental domain for your
lattice and in this case I guess there
are six eigenvalues but if you so the
middle sum is bounded above by a sum
involving these six points but that in
turn is bounded above by the sum over
all the nonzero points in the dual
lattice which is not that bad because
these are the closest ones and it's e to
the minus norm squared you're not adding
that much when you do this and then it
turns out that this is the quantity it's
kind of Gaussian sum of squares of
lengths of vectors in the lattice that
was considered by me chancho and reg F
so what they say all right
suppose you have any lattice in RK you
let lambda star be the dual lattice
which is defined in this usual way and
you take a Gaussian so that's a tile RK
with copies of the fundamental domain of
the lattice you pick a central point and
then you put pick a Gaussian which is
centered at that point and has
covariance matrix to be a scalar
multiple of the identity so it's as kind
of a blob and then you take the quotient
by the lattice so that now you have a
density on the fundamental domain and
the question is how close is that to
uniform on the fundamental domain
clearly if the Gaussian has very small
variance is very far from uniform but if
the variance is large then it will be
close to uniform because you know the
red boxes we very small compared with
the scale
Gaussian so it was proved I think using
Poisson summation that in fact the L
infinity distance between this quotient
density and uniform is exactly this type
of Gaussian sum and then the smoothing
parameter is defined to be the amount of
variance that you have to take in order
for the other infinity distance to be
epsilon C it doesn't matter what value
of C you choose whether you you take the
centered here or up there its
translation invariant it just changes
the location where the L infinity thing
is achieved so this thing when R
increases the distance decreases you
pick the value of R that makes this sum
equal to epsilon and that's the epsilon
smoothing parameter and this is exactly
the sum that we had in the previous page
and so it says at the bottom the mixing
time of the chain is controlled by the
mixing the smoothing parameter afford
the lattice for the original plastic
lattice not the dual lattice and if you
see down here if this thing was less
than 4 epsilon squared you know four
times variation is sin squared less than
four epsilon square the variation is
will be less than Epsilon so that's this
bottom line here and then the good news
is that there are upper balance on the
smoothing parameter that were proved
already by Magento and reg EV that we
can just use and that allows us to get
this upper bound for the sandpile chain
on the complete graph which actually is
an upper bound for its sample chain on
any graph with n vertices but in
particular the complete graph it says
that the mixing time is that most n
cubed log n times particular constant 1
over 4pi squared and then if you add
constant times and cube then it decays
exponentially and then we get a matching
lower bound for the complete graph based
on the argument that i gave earlier with
the Omega and Omega in
verse and all of the different versions
of that in different pairs of vertices
so those 2 together give cut off for
this particular Markov chain which is
nice and it's probably true that for you
know a few other examples I complete
bipartite graph with something like that
that you could also get cut off by these
methods but certainly if the graph is
interesting at all then you'll need to
do better for the upper bound how much
time do I have left
silence okay well I have two slides so
that should work
oh that's just yeah this this is what
our complicated numerical computation
it's just it's not real it's just you
know you do various approximations and
this is what happens you know just like
this 35 is not real and the fact the gap
between the 1 and 35 is also not you
know it doesn't have cut off with the
window of n cubed okay so let's say that
you take a big square grid or a torus
which is the grid that wraps around on
itself and this is what the physicists
cared about to begin with so what do our
methods give okay so if you want to
lower bound on the mixing time the best
that we can get is a diameter bound
which is N squared order I guess here
unlike the pressed of the talked so far
there are N squared vertices and not n
the upper bound which comes from the
smoothing parameter is N squared log N
and then just yesterday I heard that Bob
Huff has a proof that the lower bound is
also order N squared log n that he was
able to find enough eigenfunctions short
do lattice vectors in order to get that
matching lower bound up to constants and
we don't know anything about cut off or
anything like that
yes they should demonstrate okay then
the last thing this is a group of
physicists in Australia and what they
did they did computer simulations for a
you know relatively small grid group
operation yes okay and they found this
very strange pattern they they took you
know many runs of the chain and they had
a few statistics as they kept track of
and they followed their convergence
toward the stationary value and this is
on a log scale and the y-axis and linear
scale in time and x-axis and you see
that it decreases at a kind of a steep
rate and then it decreases by a slower
rate after that point so they claim this
is convergence on two time scales that
you know at the beginning it has a fast
rate of convergence and then it has a
slow rate of convergence and so we're
trying to figure out you know why that
might be and one theory is that there
are kind of two groups of eigenvalues
there are the eigenvalues with a gap of
n square and the eigen glugs with the
gap of n and that kind of the two
clusters of sizes of eigenvalues lead to
the two slopes of lines in this log plot
well that's just hypothetical you really
have no idea what's the source of this
picture that's it
other dimensions huh let's see let's see
so well if it's if the dimension doesn't
is dimension is constant then it should
have basically the same upper and lower
bounds as two-dimensional torus because
if it's bounded degree then you have I
know this is number of vertices so n to
the D log n and then the lower bound I
think my diameter should also be n to
the D and my guess is that whatever
Bob's argument is could probably also be
extended to D dimensions but I don't
know what is argumentation that I know
actually doesn't work very well because
the convergence is too long can be
acquired say you are better less it
sounds like when you form something like
that
I don't think so I mean one of the
issues is that the the group here is
very large like for the cycle the group
is n but for a you know a complicated
graph the size of the group is the
number of spanning trees and so even if
the even if it mixes fast so to speak
the mixing time is still going to be at
least comparable to the number of
spanning trees which is very large
number and so if you want to sample like
uniformly from the underlying graph then
that should be a much easier problem
than trying to build this much bigger
thing with spanning trees
oh you're right data that was just wrong
statement okay thank you for correcting
me yeah yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>