<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Programming Devices and Services with P - Lecture 2 | Coder Coacher - Coaching Coders</title><meta content="Programming Devices and Services with P - Lecture 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Programming Devices and Services with P - Lecture 2</b></h2><h5 class="post__date">2016-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/24R0DEHbmk4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
I showed you an example of a particular
Heisenberg in a storage system then I
introduce the programming language be
with this particular tool chain so you
have a P program which has code and
models and specifications and tests and
then the compiler both generates
executable code and it also does
concurrency unit testing at the very end
we talked about the difference between
modeling and programming okay then we
talked about the specifics of the P
language in particular it's the it's an
actor style model of computation or also
known as a state machine communicating
state machines we did this little
animation to show you how state machines
are running concurrently and
communicating with each other by
exchanging messages each message is an
event and a payload value a data value
we looked at how p programs get executed
then we started looking at issues of
modeling in particular model functions
and the dollar sign the dollar
represents non-deterministic choice and
at towards the very end we we looked at
model machines and we we we attempted to
understand how to model the timer
facility and an operating system using a
state machine so that's where we ended
yesterday do you have any questions
about the material yesterday okay great
so continuing so we have so far looked
at the P programming model and we have
looked at the distinction between coding
models now i'm going to show you what
specifications look like right so in
general when you have a concurrency unit
test it captures a host
of executions lots of executions very
compactly but the real power of power of
concurrency unit test comes from the
idea that along each execution that the
testing tool generates in in that unit
test some deep specifications can can
get checked so P has various mechanisms
for writing down these specifications so
there are some specifications that you
get for free or with very little effort
the most important one is unhandled
event exception so as I explained
yesterday unless any unless an event is
explicitly deferred in a state it will
be dq'd if it is add in from the front
of the queue okay only if the event at
the front of the queue is deferred would
you with the control skip over that so
if the event of the front of the queue
is removed then they must be a handler
available in the state to process it if
no handler is available then unhandled
event exception is thrown and that's an
indication of an error then p we also
have data types and you can you can
create you have sequences and maps and
tuples name tuples like structs see
structs and you also have this type
called any so any is a little bit like
the void star type and see or the object
type in language like Java or C sharp so
sometimes you want to send values over
the wire and they could be any kind of
value so in the type system you capture
that by using the type any so once you
introduce the type any in the language
you have to introduce a cast operation
also because only any type no operations
are loud so you have to down cast into
some reasonable type before you can
start doing operations on it so if there
is an illegal cast
an exception of thrown this is very
classical no surprises here you have an
assert statement in the language also so
you can basically do local safety checks
within a machine by using the assert
statement the assert statement can can
only refer to the variables that are in
scope inside the machine okay so
sometimes in P programs it is also
important to check not just local
assertions but also global assertions
assertions that talk about correlations
among the state of multiple concurrently
executing machines so the local
assertion that I mentioned earlier does
not give you that power because in a
local assertion you can only access the
state of one machine so in opposing you
wanted to say that when a machine m1 is
in a state X machine and two must be in
state why how do you capture something
like that so in in P the mechanism for
pulling that off is monitored so we have
two kinds of monitors safety monitors
and Linus monitors so I've use the word
monitor and specifications
interchangeably so a monitor is another
state machine it basically subscribes to
some events and the idea is that it is
just observing those events as they are
flowing from one machine to another and
if an event that it is observing is sent
from one machine to another it also gets
funneled into the monitor and the
monitor can take some action on it so
the monochrome monitor would typically
update its local store so because the
monitor can observe any event that is
flowing in the system it can observe
events that are being generated by
multiple you know different machines and
that's how you establish correlation
between the states of the Commissioner
I'll give you examples of that so a
safety property generalizes the notion
localization you can capture global
invariants similar to local assertions
its violation is exhibited by a pie
night execution okay so that is
relatively easy to understand now yes
monitor as a machine that exists
separately it is just observing the
calculations that are happening in the
system and when events flow from regular
machines from one to another a copy of
those events gets funneled into the
monitor also but the monitor exist
separately so actually i will show you
i'll come back to live pneus monitors
later first let me show you a safety
monitor just to make the ideas a little
bit concrete so here's how you write
down a safety specification in in P so
spec is a keyword safety is the name of
this state machine and the monitors
keyword you can after that list a
sequence of events and these are the
events that this monitor this
specification is interested in observing
so in the initial state so pending is
initialized to 0 the default value for
in types and then in the initial state
there is this handler on thing whenever
a ping event is observed it increments
spending by one okay and here the
assertion that there's a there's an
assertion here i would say spending is
less than or equal to 3 okay so note the
interesting thing here is that the ping
event may have been generated by any
machine that is running in the system
okay doesn't matter what machine
generates that event it's going to be
funneled into this monitor that's how
you establish correlation between events
that are happening across machines okay
okay so going back to lightness
properties who can I get a show of hands
how many people have heard of lightness
properties before one person whew 345
okay all right so but you must have you
must be finally familiar with the issue
of termination so when you write a while
loop you expect it to terminate
regardless of the input input value
right if you write a nonterminating
while loop it's a sign that there's a
bug in your program so liveness
properties generalize the notion of
these non termination bugs in sequential
programs to concurrent programs so the
issue of non termination in concurrent
programs is more complex because even if
each event handler so you know in a
concurrent program such as p event
handlers are executing inside each
concurrently executing machine right it
could be that each event handler
terminates but the program as a whole is
not making any progress so the following
could happen that the event handler and
event handler execute it it terminates
before terminating it sends an event to
another machine that another machine
processes that event using an event
handler which terminates and then I went
back to this machine and this keep doing
the same thing now looks like you know
the computation is proceeding but it may
be that there is some kind of a loop
here where there's no logical progress
being made in the computation so that's
what lightness properties try to try to
capture so similar to non termination
problems violation of liveness property
is also exhibited by an infinite phrase
so and infinite trace is a violation of
the lightness property if something good
does not happen in it right you're
waiting for something good to happen but
it just never happens ok so I'll give
you an example now
let's say so here's a here's a lightness
specification this lightness
specification observes two events
request and response req and resp so
here we see some new attributes so we
see that states are annotated by Cole
these these attributes Cole and hot okay
so when this machine starts running it's
in this start state in it and it's
waiting for the request event when
request is observed it goes to the wait
state in the wait state it's waiting for
a response and then it goes back to be
in its state okay so what this is trying
to capture is this idea that every
request must be eventually followed by a
response okay so what what we want what
what thus cold and hot business is
saying is that we don't want the
temperature of the execution if you will
to become very high so how does the
temperature of an execution increase
well any increase is if the execution
stays in a hot state for every time step
that it stays in a hot state its
temperature increases by a little bit so
if it stays in a hot state forever
without entering a cold states of
entering a course which resets the
temperature back to zero okay and we
don't want the temperature to become
infinity right the the temperature
becoming infinity is a sign that we are
not making progress yes
so yes so typically this the notion of
lightness the classical notion of
lightness uses infinity as the threshold
oftentimes you can convert aliveness
into a bounded safety bounded likeness
property by imposing a finite threshold
in that case the violation is not an
infinite execution anymore it's just
becomes a finite execution which
increases the temperature more than
certain amount that may be possible to
but I don't know how to do that yeah so
so so what we what the slime is property
is saying is that you the temperature
should stay by night so what that means
is that you can't stay in a hot state
forever after a certain point you have
to enter a cold state to reset the
temperature back to zero so so this this
particular definition is inspired just
some background thing I mean you don't
you don't really need to know about
Omega regular properties and ravine
acceptance conditions and whatnot but
those of you who are mathematically
inclined and enjoy automata theory
perhaps might be interested in this
little tidbit so I'm sure many of you
have studied the theory of finite
automata so finite automata are acceptor
of finite strings right what is the
theory of Omega automata also an Omega
at Omicron is an acceptor of infinite
strings okay so you again have a finite
alphabet and you define the set of
infinite string the older alphabet and
an Omega otamatone breaks that set into
the set to into the members that are
accepted in the members that are not
accepted okay so
just like in finite automata there is a
notion of accepting states there's a
notion of accepting states in these
Omega comet also i'm not going to go
into that and one particular classic
notion is a ravine accepting condition
so it turns out that what i am defining
here using coal and hot states is
equivalent to the ravine acceptance
condition and that is nice because what
that means is that any property that is
expressible buy one of these omega
regular atomic that can be expressed in
our system also so this is very
expressive because you know all temporal
logic specifications can be compiled
down to omega automata and this is able
to express all omega regular property ok
ok so we talked about how to partition
infinite executions into good executions
and bad executions good executions of
those whose temperature stays finite by
bad executions of those whose
temperature becomes infinity right
however the story doesn't stop there yet
you know this slide pneus business as a
can of worms so you have to understand a
few more things it turns out that
there's a notion of fairness constraints
and in order to do to distinguish good
from bad you also have to pay attention
to fairness constraints so let me
explain that using an example so imagine
that you have a machine a and a machine
be so machine is sends to machine be the
request event ok and then machine be on
receiving requests it sends a response
back to back to it right so in the in
the semantics of P it is possible to not
scheduled be at all ok so i can choose
to create a schedule
an execution in which machine may
execute the send instruction and after
that I never scheduled be okay this is
what you would consider an unfair
execution because it's not given be
chance to execute well if I did that
it's not surprising that the
specification we saw in the last slide
is not satisfied of course in that gives
be is not given a chance to execute no
wonder that monitor is going to explore
with a very high temperature the request
was sent but no response was for coming
well the guy was going to send the
response and not even allowing it to
execute okay so what that means is that
you should only generate consider an
infinite execution as a violation of the
lioness property if it is fair if it is
not fair then it is nobody's fault it's
okay you know don't worry about that
okay so that is an issue of fair
scheduling and N and the P liveness
checker actually does do fair scheduling
of machines so it will not report an
error trace if it is unfair the second
issue is you know the issue of your
choice so let's make the example in the
previous slide a little bit more
complicated so machine a sent to machine
be the request event okay and machine be
was given a chance to execute so we did
fair scheduling so we're going to give
machine be a chance to execute however
what what is happening in machine be is
a little bit more complicated so before
sending the response back to be machine
be is polling some external world maybe
it's talking to some kind of a hardware
sensor it's waiting for some input from
the user a human being so it's just
polling for that input and only when the
input arrives is it going to send a
response back okay so this is usually
how what polling loops look like yes Oh
that's a typo that's a typo that should
be a a here so so the way I'm modeling
polling here is by using non
deterministic choice so you know it can
succeed anytime well I mean the issue
with this kind of modeling is that a
dollar 10 there's an execution in which
dollar always evaluates to true in which
case this while loop will not terminate
what if this while loop doesn't
terminate that no matter how many
chances I give to be to execute we will
never get to execute the send
instruction ok so again we run into
trouble ok so this is providing such an
execution as a violation of the previous
liveness property is also kind of bogus
right because there's this assumption
that machinery is making that eventually
its polling loop is going to terminate
because some user input will arrive wait
ok so to model that we introduced this
notion of fair non-deterministic choice
so that is indicated by double dollar ok
so double dollar means that if that
choice is evaluated infinitely many
times then it must evaluate to true
infinitely many times and false
infinitely many times notice that this
notion of here in this is an infinite
airy notion of fairness it is not this
probabilistic notion of fairness we're
in a long enough time the number of true
evaluations and the number of false
evaluations is roughly the same for
example it would be considered fair if
it evaluates to true once for every one
trillion times it evaluates to false
that would be completely fine from from
this Infinity fair in a standpoint ok
and that's all we need to show that when
every request is eventually followed by
by a response because eventually this
double dollar is going to value to false
and then we are going to exit this loop
okay all right questions
but a spin lock no I think that you can
model spinlock pretty accurately using a
combination of fair scheduling and fair
choice because so imagine that a
particular machine is spinning on a spin
lock because the lock is currently held
by a different machine well because of
fair scheduling I'm going to give that
machine a chance to execute infinitely
often it will continue to make progress
eventually it's going to release the
lock and then my lock will I'll get the
lock but maybe you're worried about the
issue that if there's contention so one
guy is starved out so there's two guys
contending for the spin lock one guy
always succeeds at the expense of the
other yeah I don't know if you can walk
I don't know I don't know maybe you
cannot model that with this not sure I
don't think about that okay
a random variable honor yes yeah
no it's very simple you don't need to
resort to probabilities to evaluate that
you look at an infinite execution if
that infinite execution evaluates to
this double dollar infinitely many times
meaning that it goes through this
control location infinitely many times
then it must be the case that it is
evaluating to returning true infinitely
many times and value ating Falls
infinitely many times yeah you can just
examine the exhibit end you can examine
infinite execution right if you don't
like this idea that you are examining an
infinite execution you can characterize
it using a formula the using a universal
formula with universal and existential
quantifiers for example yeah we don't
all I'm trying to say is that it's all
logic map discrete logic to define this
kind of notion of fairness you you don't
need use probabilities more questions
okay
alright so I'm going to show you some
code now okay so just to wrap it all
together because some of these examples
are kind of abstract so I want to show
you a real example or to give you a
better sense of what what you can do
with P so the example I've chosen to
show you is a faded detector example so
this is a very common component of
distributed system the idea behind a
failure detector is that there are these
physically distributed notes and notes
can go down for a variety of reasons you
know the hard disk tails or power goes
down or there's some hardware failure so
there is a failure detector code that is
running on some node and it periodically
holds all the nodes in the system and
then ask them to send a response back to
to say that they are alive right and if
it doesn't receive a response within a
certain amount of time then it just
considers that node to have pale and it
allows listeners you register for
failure notifications and all the
listeners who have registered for the
failure notification that get notified
if certain node is deemed to have failed
so what you want to do is we want to we
want to write down the code for the
foliot for the failure detector NP we
want to compose that with nodes that
this failure detector is trying to
estimate failures off we are going to
show you i'm going to show you how to
write a test driver which is which
captures the concurrence unit test for
checking that the feeder detector is
working correctly and i will show you
concrete safety and likeness
specifications for this test case
okay can folks at the back of the room
see this the code here if not there are
plenty of seats available in the front
please move to the front okay so at the
top here you see event declarations okay
so each event declaration has a name the
name of the event and then a type given
after the colon so here is an event ping
and its payload is of type machine
machine is the type or all you know
machine identifiers it's the value that
is returned when you create a new
machine right and the only operation you
can do on it is pass it from one place
to another and you can of course send to
a value like that so then here we have
this machine at the failure detector so
there's a bunch of state variables in
here you can see some of the types that
I was talking about earlier you have
sequence map integer it also uses a
timer of course it has to use a timer
because there's no other way to see
whether i noticed fail or not you can't
wait infinitely long just to get a
response back from a node because if it
has failed you will never get a response
back okay so mm so it starts execution
in the state in it and it Misha Liza's
its variables creates the new instance
of a timer and moves into the sending
state I'm not I'm going to rush through
this code I'll highlight a few
interesting things about it but you
don't need to you know follow all the
details of it one thing to note here is
the event-driven structure of the code
so you have these states and inside each
state you have these event handlers so
this action here for example annotated
by entry is the code that gets executed
when you interested when the pong event
is received in this state then you
execute this code here right on time out
you execute this code
so what is going on here is that in this
the the set of nodes that are being
monitored there are dresses are in the
nodes variable and inside the alive
variable we maintain the set of those
nodes that subset of nodes which is
currently deemed to be alive so we send
a ping message to the nodes that are
currently deemed to be alive and if they
don't respond within a certain amount of
time then you remove them from their
lives ok I want to show you the logic
for handling cancel cancel timer so
remember we were talking about the model
of timer yesterday where if a time where
is canceled and it can respond with
either cancel success or cancel failure
well every every once in a while the
faded detector has to cancel a timer
that it previously started in that case
it has to go through this protocol so it
sends to the timer machine the canceled
event and then it waits to receive
either cancel success or cancel failure
so if and it records which one we
received in the local variable timer
canceled and returns it so if timer was
successfully cancel that means we don't
need to wait in the client in the client
of this function don't need to wait for
the timeout event to show up but if it
if it has if it is evaluate to false
then you have to wait for that time out
even to show up ok so that was the fila
detector now I want to show you some
specifications ok so so here's a safety
specification so when I wrote this code
I wrote the code with the expectation
that when these things and pawns are
flying back and forth so Bailey detector
is sending these ping messages to the
nodes and the nodes are responding with
a pong message I wanted to make sure
that the number of outstanding ping
messages is always bounded
okay that's what that was my design
intention I'm trying to capture that
here in this logic here so I'm
maintaining a map called pending from
node IDs to the number of outstanding
thing messages okay and whenever a ping
happens i increment pending by one and
whenever pong happens I decrement
pending by one okay and I I just wrote
down the cistercian I sort of created a
ballpark estimate that they should not
be more than three pending thing
messages so i put this assertion here
okay so this is this was my safety
specification then i also want to show
you a lightness specification so to see
that to understand to appreciate the
lightness specification let me first
show you the unit test so the unit test
is indicated by this main machine called
driver so this is like the test driver
the test driver creates a whole bunch of
nodes right so this loop it creates two
nodes it initializes the the lightness
monitor it creates the faded detector
machine and then it does something very
very interesting it sends to each node
that was earlier created the Hulk
message so the hot message is a way to
simulate a machine failure right so all
is a special event in p when a machine
dq's the halt event it just terminates
execution ok so this by by creating such
a test harness we have basically
injected failures into the machine add
some arbitrary after some arbitrary
delay ok so now what this test harness
roughly is doing is creating a bunch of
machines and then terminating all of
them after some are non deterministic
delay so in this liveness monitor what
we are trying to do is write the
property that eventually
all the machines are detected to be dead
to be not alive okay so I have a heart
state in it and control stays in this
hot state as long as the number of nodes
and the alive set is non-empty when that
set becomes empty then a transition is
made into the done state so because this
is hot and the this is not a hot state
in order for an execution to be correct
we must make sure it must happen that
eventually the notification that the all
the machines are terminated is sent to
the clients so that's what this line is
mantra is trying to check okay so that
went by pretty quickly did I lose
everybody any questions
alright likeness is hard okay alright so
moving on so let's let's switch gears
now and move to the algorithmic issue of
how do we test large search spaces so in
in this in a concurrency unit test
supposing you have yeah so this is a
good example so this is modeling the
state of a pea program okay and it's
imagining that there are three machines
that are that that get started and in
every state you can schedule one of any
one machine so depending on which
machine your schedule you get to a
different state you can see that there
is this exponential explosion in the
branching factor so in each state so in
the first state you have three outgoing
edges from each one of those states you
can again schedule any one of the three
machines that currently exist you again
get three outgoing edges and so on so if
you get down to a depth k then the total
number of possible executions is going
to be 3 to the power K that make sense
so what's happening here is that the
number of possibilities is increasing
exponentially with the length of the
execution so we of course want for your
realistic program we do want to have
tests that they go deep into the
execution of the program to test those
kinds of behaviors but we're seeing that
this is going to become common to really
really challenging too many behaviors of
tests so what do people what have people
have done in the past you know 20 you
know past several decades so this is a
problem well studied in the literature
so people have done that first search is
that ok well it's a very large so it's
space but we can still try to do search
it
the simplest search procedure is depth
first search and it is implemented in
many model checker how does anybody is
anybody here you does not know what a
depth what depth first search is who
does not know depth first search okay so
depth first search hmm okay ah ok let's
try this okay so depth first search is a
graph algorithm the idea is that you
start from the initial state of the
graph and you are trying to find out and
okay so you have a target state and you
have the initial state and you're trying
to find out if the target state can be
reached from the initial state okay so
we're going to start traversing the
graph so you start from this initial
state okay and then you pick some
arbitrary order of the edges and you
explore them in that order so let's say
we explore these edges left to right so
so what we'll do it will have a stack so
we push a zero on the stack and then we
take this transition and we arrive at s1
ok well s one has more children so we
are going to continue the execution so
now we're going to push s1 on the stack
and then examine its children and so on
right so depth course simply means that
we are going to go all the way to the
left and keep going keep going until we
arrive at a state which does not have
any children imagine that the this graph
is actually a tree so there are no
reconversion paths and so on ok so we'll
go all the way to the to the left here
we arrived at a state that does not have
any successor so that means that this
state is fully explored if you will then
we pop it from the stack ok then we look
at the current state at the top of the
step we see ok wow we have explored its
leftmost child let's look at the child
right next to it
so we get that guy right and then we
push that on top of the stack and we
keep keep doing the story right so the
search proceeds you know like this like
this like this and so on right this is
called depth-first search ok so it's
called depth worse because it's going
deep in one direction before popping
back up there are other styles of doing
execution for example we can do
something that is called breadth-first
search so what does how does breadth
first search work let's take a look at
that so you start with s0 again you
always start with a0 ok now is this time
what we're going to do is we're going to
put that in a queue as opposed to a
stack so we put that in a queue then we
again you know take s1 we put that in
the back of the queue ok so this means
that we are always DQ items from the
front of the queue this time we're going
to look at s0 and then we find our it's
not fully explored so I'm going to take
s2 then I put that in the back of the
queue then again I have a zero output s3
so I first generated as zero then s1 s2
s3 then I look at s1 i'll add s4 s5 s6
to the back of the queue then I will
take s to add generate so basically this
is going level by level as opposed to
going all the way deep so that's
breadth-first search ok so that was a
quick tutorial on deaf person
breadth-first search so depth-first
search is very popular because for
programs with high branching factor this
is much more space efficient than a
breadth-first search right imagine thing
about this way imagine that the depth of
the graph is small compared to its
bushiness right so with the depth-first
search the space bound on the stack is
equal to the maximum length of the pots
right whereas with breadth-first search
basically you know after at some point
you're going to have all the states in a
certain layer so we know that those
states are
increasing exponentially with the depth
so you're a space requirement is going
to grow going to increase exponentially
for a while right so that's why for
bushy graph often depth-first search is
much more space-efficient yes yes yes so
that's right so how do you deal with
cycles and all that stuff right so in
this here this was a very simple
situation because we had a tree so we
knew that you cannot revisit a state if
you follow a different path so there are
two complications one complication is
that the graph may not have cycles but
it could still be a dad it could be that
you know the leftmost chil of s2 was s6
right so there's rican versions another
complication could be that well you know
s1 could have a path back to a0 it is
actually a cycle in the graph both those
problems are solved using the same
mechanism that mechanism is a state cash
so in addition to the stack or the queue
you also maintain a set of visited
states and when you encounter a new
state before pushing it on the stack you
first check whether its present inside
that state that inside that set if it is
not present only then you push it on the
stack but before doing that you add it
to that set what that means is that if
you revisit a state from multiple
different paths you're not going to it
if it was visited once before it would
be present in the set that's why you
will not add it to the stack the second
time to make sense pardon
ah I see because you are thinking that
because I think that the reason you are
saying that you should add it is because
you are thinking that if you arrive at a
particular state like this and when you
arrive it like that they are really
different things so you should continue
exploring this path also well that may
actually be not true it may be that if
you doesn't matter how you arrived at a
particular state for it the future of
its execution it doesn't matter how you
arrived at it the information content in
the state is the same so its future
behavior will be the same also so you
should actually if you have already
explored all paths the first time around
then the next time you arrive at the
same state you can forget about it you
don't have to explore any further so
that's our state caching world yeah
that's right that's right by the way you
you are really into randomization what
is the deal here yeah yes no no I
understand that actually have a question
so is randomized a randomized algorithm
your research area you're in Saturday's
networking okay randomization I see so
you're absolutely right so if the size
of the graph so all these algorithms
that have been describing they cannot do
better than the number of nodes in the
graph okay so in fact you know the
number of nodes in the last layer is
already exponential in the depth of the
graph so the total number of nodes is
already going to be exponential so none
of these algorithms is going to do
better than the number of nodes in the
graph so if you're the number of nodes
in your in the state transition graph of
your program of your P program or your
concurrency unit test is very large then
most likely search is not going to
finish and that is a central problem
everybody is wrestling with right ok so
I also will propose various sorts of
solution some of which involving
randomization and some not but you know
it's good to take a moment and
appreciate the challenge here okay so
everybody with me on that things are
looking up I think we went down with the
whole lightness business but I think
things are breaking up again okay so
okay so we are talking about depth first
search and we did all this discussion
about the size of the graph and whatnot
and it's very large lot of LA so what's
going to happen is that if there is a
bug let's imagine okay so let's imagine
that you're
doing depth-first search so the way I
explain that for surgeons you go like
this then you go like this then you go
like there you go like this and so on
right now imagine that there's a bug
over here okay now you're really screwed
why because you know if this thing is
very deep and this is going to get stuck
over here and you're gonna run out of
time whereas it might be that you know
the bug is really shallow it was
available at this level do here maybe
the bug was here but because you got
stuck doing all sorts of crazy things
over here you can't even find a cello
bug okay so this is a well known problem
with depth first search that when you
have very large graphs for which search
is unlikely to terminate it gets stuck
in some corner case of that graph and
may it may even fail to find shallow
bugs okay so we have an alternative I
told you about this other us known
before we go there ah i'll get to that
as an advanced thing so I mean we talked
about this other thing right you don't
have to do that for search you can do
bread first search right you do explore
the graph layer by layer so if you first
explore s0 then you explain all the guys
over here then you explore all the guys
over here in that case you are going to
give more of an opportunity to find
shallow bugs okay but that has its own
disadvantages it is not space efficient
okay all right so this is the sort of
the general story so what what what
researchers have come up with is a
general class of techniques called
search prioritization for finding deep
bugs okay so the basic idea behind
search prioritization is very simple
it's saying that when you have a very
large graph you treat it like an onion
so everybody has seen an onion there are
these onion rings okay so what you're
going to do is we are going to impose
different kinds of onion rings on the
graph so for example there is a blue
dashed onion over here and then there's
a black solid onion over here
so both these these onions are of
different characteristics they are
slicing and dicing the graph in
different ways so if there is a bug here
denoted by this little ladybird ladybug
it may be that it is within the second
onion ring of the blue dashed onion but
it may be within the 5th ring of the
black onion so in this case I assume
that regardless of which onion you
choose the size of each onion ring is
roughly the same ok in this case you are
better off which which which particular
onion are you better off examining any
guesses which onion should be picking
this case to find a bug quickly the blue
guy right because the obvious idea here
is that we do it with peel the search
onion ring by onion ring ok that's the
clue here and we're going to hit that
but in the second onion ring if we chose
the blue guy but we cannot going to hit
it if we chose the black guy until the
fifth layer ok so this is the general
story and this is of course parametrized
by these onions which are also called
prioritization strategy so let's look at
a few prioritization strategies so turns
out that that breadth first search that
I was describing to you earlier it's a
kind of prioritization strategy saying
that you have a weight on executions the
weight of an execution is is its length
and you you basically have this onion
ring where onion rings where you have
all executions with length zero then you
have all executions with length one then
with length to that will leg three and
so on ok well people figured out that
you know that's good but it's it's going
to be space inefficient so then they
combine breadth-first search and depth
first search and they call it iterative
def bounded search so it rid of that
bounder surgeons the idea is very simple
you're going to do depth depth first
search but you're going to do depth
first search with a bound ok so you can
say I'm going to do a depth-first search
with bound one so what that means is
that if ever the length of my stag
becomes
is about to become too I'm going to stop
exploring ok so I'm going to look at
executions only up to the dev bound and
within the that set i am going to do
depth-first search ok so i try to get
the best of both worlds I'm space
efficient and I'm also doing iterative
deepening so I'm looking at executions
onion ring by onion ring ok so this is
this all good both these things are
reasonable but of course you know they
fail to find deep bugs so we solve the
problem of finding shallow bugs but now
we are we we are in this other problem
now we can't find debugs so going back
to the slide I showed you earlier
remember i was saying that oh you know
that person you went like this better
than that right and then it's just
messing around over here and for that
reason it doesn't find the bug over here
well what if you know the bug was over
here right it was deep over here in that
case if you are doing the search layer
by layer and the number of states is
increasing exponentially with the number
of layers now you don't you will not be
able to find the bugs over here right so
this is the problem with a fit of
deepening days on that depth criterion
you don't give a good chance to the
search algorithm to find debug ok yes
when you're doing it right of deepening
yeah i think so that's one wasteful
thing about iterative that bounded
search so when you increase the depth to
k plus 1 you end up exploring all the
states up to depth k also so that's
wasteful about iterative deepening with
the depth-first search yep okay yes yes
that's correct once you have found an
error you don't need to explore any
further no no it's not because it's
because no the case the case that we are
talking about here is where is when you
I mean you are a depth you first
explored up to depth k you actually
didn't find an error that's why you are
incremented the depth to k plus 1 so now
you're going to explore all those guys
that were up to depth k to generate the
states that are at depth k plus 1 right
so all that work is wasted that's what
we're talking about because in those we
already know that there is no error but
in order to generate any state at depth
k plus 1 you have to go through all the
states that are at depth k or less yep
no it should be the other my intuition
says the other way okay
I mean but I mean you are suing some
kind of a distribution right that these
bugs are sort of uniformly distributed
that's not the characteristic of these
bugs they are more like the closer
analogy would be like you know needle in
a haystack kind of deal ya know so it's
not like that I think yeah no I mean not
in the kinds of systems I mean programs
in general are not well behaved I mean
you are you you when you write code
right it's often like this you make a
little change and your conditional
evaluates from true to false and then
you just go off and some you know some
part of the world that has never been
seen before right it's just completely
that the behaved the program completely
changes by switching one conditional
somewhere yeah
that first you do the first layer then
you do the last layer while the problem
is that these this graph is not given to
you explicitly you can only construct
this graph by executing the program you
see what I mean so you went to get to
the last way you have to first go
through the intermediate layers right
yep oh boy I should have completely
skipped the liveness business all right
hand okay where are we okay so so we at
this point where we have been examining
this business of about doing reach
ability and a graph from various angles
sometimes we find shall over sometimes
we find debugs some if you if you are
good at finding shallow bogs we are
barren finding deep bugs if you're good
at finding debug your barrack finding
shallow bugs and so on and so forth
right so so what we what we want is that
so it is pretty clear that one strategy
is not going to nail everything so we
want many different strategies which can
all be potentially trying okay so what
other strategies can we can we pull out
right so we should think about that so
here's another strategy it's called
schedule prioritization so what we're
going what we want to do here is that we
don't want to do prioritization based on
the length okay we want to look at
entire schedules and figure out a way to
prioritize an entire execution over
another entire execution because the
problem with the other approaches with
that we were doing prioritization based
on length and that's why we were getting
stuck in one place or another so the
question is how do you how do you
specify the prioritization function that
says okay well I want to prioritize the
schedule a 1 over the schedule a 2 okay
how do you do that because this graph is
not given to you as an explicit entity
right the graph becomes available to you
as you explore the program right so
these schedules are not given to you in
your hand you
not really examine them until you have
generated them right so that's the issue
okay so so what that means is that we
need to somehow refer to schedules in an
indirect way in an abstract way so i'll
give you a few ideas that have bubbled
up in the literature so one idea is use
context switches to prioritize schedules
so here the idea is that so what is the
context which I mean this phrase context
which comes from multi-threaded programs
that is a program is being scheduled by
the operating system so you know it runs
a threat for a while and then it's time
quantum expires so the OS pull the
thread down and then schedules another
thread in the program in its place ok
and then that's how it keeps going back
and forth among different threads so our
context which happens at a point when
I've thread is stopped and whether
thread is scheduled in its place ok so
why so one possible strategy we say that
let's bound the number of context
switches ok so the interesting thing
here is that bounding the number of
context switches does not bound the
execution length it rather bounds
something that is a little bit like the
interference in an execution between
concurrently executing entities ok
because the reason it doesn't bound the
execution itself the length of the
execution itself because each context
itself the number of steps for which a
thread execute before being interrupted
that you don't you not specifying any
bound on that ok so this was the first
proposal for scheduled prioritization in
the literature and it has some very
interesting properties so so there is
this you know this whole business about
model checking how many people have
heard about heard the phrase model
checking here
okay well i think i'm gonna skip this
one let's keep going yes pardon
yes
no I don't see why I mean it's a
different way of bounding the number of
contexts which is a prioritization with
respect to a number of contact which is
it's a different way of it's a different
onion right it's a different onion right
so you can do schedule search-based with
using any onion so I didn't understand
the argument you are making
oh no no it's not like that so if a one
is prioritized before a to roughly the
way to think about is that there will be
a whole bunch of guys whose priority is
equal so your first generate all of them
without generating all the other
execution whose priority is lower right
then you go after them then you in that
order right so you are always have this
order of execution in the search you
first explore the executions and lower
onion rings before you explore the
execution than larger onion rings
yes that's true yeah so yes I agree with
you so in fact that particular wastage
that you're talking about it's the same
kind of wastage that occurs in iterative
Deb bounded search right because even an
iterative Deb bounded search when you
are generating the executions with depth
k plus one you actually go through
executions with depth k so that problem
still remains that problem still remains
yes but that problem you already had to
begin with as you can see even here
right that problem was not introduced
because we were doing prioritization
with respect to schedules I think that
problem fundamentally comes because the
this transition graph is not given to
you explicitly it has to be discovered
by executing the program okay okay so
then so we talked about schedule
prioritization with context switches so
then there was a tweak on it which is
you know you distinguish between two
kinds of context switches forced and
unforced so a force context which is
basically you know a thread arrives at a
point so it's blocked on acquiring a
mutex for example it can't make progress
the OS is forced to schedule a different
thread that's a force context which and
enforce context which are a preemption
is one where the thread can make
progress but because it's time quantum
time limit expired it was scheduled out
so so a tweak on context which is
scheduled prioritization with context
which uses with preemptions where you
bound only the number of preemption you
count only the number of preemptions
that occurred in the execution and
that's the the privatization number so
this particular technique was
implemented in a two called chess it was
a concurrency testing 24 multi-head
programs and I worked on that a few
years ago and what we discovered when we
applied it to benchmarks you know even
large multi-threaded program is that you
often find bugs with a few preemptions
only looking at execution with a few
cream
so it's a good prioritization strategy
at least for shared memory concurrent
programs another nice thing about this
prioritization strategy is that if a bug
is discovered with few preemptions the
execution looks easy it's easier to
understand on the other hand if there's
a lot of preemptions going in and
happening in an execution it becomes
very hard to comprehend what is going on
so it becomes harder to find the root
cause of the bargain to fix it questions
yeah I mean these are all you know you
know onion complete tactics ultimately
you are going to run out of time doesn't
matter what onion you use and there will
be all these behaviors that you haven't
gotten to and maybe there's a bug
lurking there and you didn't get to it
these are not you no proof strategies
these are more like search strategies if
you want to do proofs then that's a
different you know that's a different
talk yeah okay so so so so there's a new
so this is yen yet another
generalization of schedule
prioritization which generalizes all the
other things that that I have mentioned
you before so that's called the
prioritization based on delaying
schedulers okay so how should I explain
this okay let's give it a shot okay so
so let me let me first define what a
scheduler is a deterministic scheduler
so a deterministic scheduler is a
strategy for executing a concurrent
program in which the next process or
machine that is picked is fixed based on
the history of the execution so far okay
so as an example consider a round-robin
scheduler right so this is an
illustration of round-robin scheduling
imagine that you have three threads t1
t2 t3 and we first picked e for you
first execute t1 then we execute t2 we
execute t3 and so on then t1 t2 t3 and
so on so the way you would actually
implement this kind of scheduler is that
you maintain a queue so in the queue you
have it's initialized with t1 followed
by t2 followed by t3 you pick the guy
that is at the front of the queue you
schedule it and then you put it at the
back of the queue and you again pick the
guy that is at the front of the queue
your schedule it and put it the back of
so this is deterministic because only
one schedule is going to be generated
okay this is great for if you if you
wanted Gemma Gnostic execution but this
is not so good if you are interested in
using this to search the space of
possible schedule because you are just
generating one scheduled what's the
point right it would be so many other
possible schedule but then what we are
going to do is we're going to start with
a deterministic scheduler and we are
going to provide a mechanism to perturb
it so that if enough perturbation is
applied you can generate all possible
schedules ok so this perturbation can be
captured by what i call the delay
operation ok so the delay operation can
be anything that you want right in this
particular case the delay operation
basically is implemented by saying that
whoever is at the front of the queue
don't schedule it just put it put them
at the back of the queue so that way you
get to skip some threads every once in a
while so here for example I schedule t1
put at the back of the queue not t2 t2 i
applied delay so I didn't schedule it I
so if I do that why is this oh no no I
see so I schedule t1 then I schedule t2
and then I apply the delay operation so
what that means is that the guy who was
at the front of the queue was t3i
instead of scheduling it I just removed
it and put it at the back of the queue
so now when I schedule the next guy it's
t1 because T 3 has will move to the back
of the queue ok so by injecting a delay
here but for the rest of the execution
by following the same default
deterministic scheduling strategy i have
now generated at different schedule ok
you can imagine that if you did not do
the delay here you did the delay over
here you would have generated yet
another schedule so by applying the
delay operation at different points
inside the schedule you will generate
different schedules right ok so here we
generated a whole
of schedules different from each other
by choosing the point at which to apply
the delay operation but we forced
ourselves to apply only one delay
operation we did not use to delay
operations while you can try to use to
delay operations so then in that case
you are going to generate more schedules
and you can go to three delay operations
you generate even more schedules right
so that story here is that this is yet
another kind of onion you basically
define an onion by defining a
deterministic scheduler with a delay
operation and then you keep generating
more and more onion rings by increasing
the number of available delay operation
if your scheduler and the delay
operations are defined in a nice way
then you will be guaranteed that if you
give enough delay budget you will end up
generating all possible schedules so
this is a very general mechanism that
subsumes all prior schedule
prioritization strategies right because
it takes as a primitive this concept of
a deterministic scheduler and a delay
operation and you can completely
influence so it is a very general way of
defining the onion right
how do you convince yourself
no I didn't understand the question
so it is certainly it is certainly true
that if your number of delays is bounded
by D then the number of possible paths
that you will generate would be
exponential in D only it will not be the
full set but notice that you can keep
increasing d if you increase d to be
very large than in the limit you will
generate everything ok so then you can
basically do you can peel the onions by
using a particular delaying scheduler so
what you will do is you will first
generate executions with zero delay then
inside that execution there are let's
say that the execution has length L then
you pick a particular position in that
execution to inject the delay depending
on which position you pick you will
generate more executions with one delay
and then you keep doing that and you
will generate basically a graph of
execution over executions right where
the each layer corresponds to execution
with a certain number of delays
reachable with a certain number of glaze
ok so now let's turn our attention to
sampling ok so sampling is here yet
another tool in our Arsenal for
searching large state spaces so why is
sampling randomized sampling good so it
is good in my opinion the most important
reason why it is good is because it is
so simple so simple to implement imagine
the simplest possible randomized sample
algorithm it's called a random walk so
you have a graph ok and you start with
the start state and you pick one
successor at random you walk to that one
then you pick another such as a
successor at random you walk to that one
right and when you reach the end and you
arrive at a state that has no successors
you have sampled one entire run in the
in the graph and then you can restart
from the very beginning and find another
run so it is trivial to implement you
don't have to maintain data structures
like stacks or cues or whatever each
sample is completely independent from
each other and as a result you can also
efficiently paralyze the search so you
can start lots of random seeds you know
and on as many machines as you wish and
each one of them is executing this in a
in a loop so very easy to trivial to
paralyze so for all those reasons and
also notice that it is already not
biased towards exploring shorter or
longer execution it's just a random walk
so it will just keep running the program
if you have a bug that is deep it may it
may end up finding it so random walk has
this sort of nice property you know it's
a trivial property but nice nonetheless
that in every sample every sample can
generate any execution with nonzero
probability right and that's nice okay
so so people have investigated random
sampling also because if you are dealing
with you know finite testing budget in
terms of time then oftentimes you know
if you try it use deterministic search
strategies search ends up getting stuck
somewhere or the other so why not throw
random sampling into the mix also so the
basic random walk algorithm it's very
nice and it does often find bugs pretty
fast but it is also true that if if you
have a hard to find bug that requires
certain events to happen in a certain
fixed sequence and there's a sort of
long chain of these events then the
probability of finding them at large
step will become managing leak slow
vanishingly low so we would like to be
able to use the notion of prioritization
even to direct a random search also so
there is a way to combine random
sampling with the prioritized search
also so instead of deterministically
generating all executions with one
then all executions you delay you can
try to do a random walk on this graph
itself so you start with the execution
with zero delay now instead of enumerate
in all places where a delay can be
injected you randomly pick one location
to inject a delay so you start doing a
random walk on this graph itself okay
turns out that this is pretty useful and
in in in in many many many programs many
test problems it actually works better
than the vanilla random graph okay so I
was going to show you some of these
tables but it's not that interesting and
I'm also running out of time so I'll
basically conclude now I just want to
conclude by saying that prioritization
is applicable not just to scheduling
choices but also to non-deterministic
choice dollar right so if you have a
sequence of lots of dollars lots of non
deterministic choices in your test
program then that can also cause a
exponential explosion right instead of
you have a branching factor of 2 so if
you have chaining of non deterministic
choices so if you have a chain of n non
deterministic choices you will get you
know a tree at at layer and it will have
2 to the power n nodes right so that it
does that is itself a hard search
problem even without the complication of
scheduling choices while we can use
prioritization to prioritize
non-deterministic choices also we can
just have this programming abstraction
that we always prioritize falls over
true so false has caused 0 and truest
cost one we just tell the programmer
this is how the testing tool is going to
behave so if you want to explore a
certain choice before a different choice
take that choice code it up in your test
harness by doing that on the false
branch as opposed to on the true branch
so the programmer the tester can use
this this abstraction and program
against it so in my in my company
engineers in office core platform they
they were familiar with the
system and the prioritization is it
implements so they implemented this
privatization of non deterministic
choices in a framework testing framework
for client server applications called
private they're using it over there
pretty cool okay well just a reminder
that p and p sharp r is available open
source check it out and I'd love to hear
your feedback from you guys and I hope I
have convinced at least some of you to
work in the area of programming
languages and verification all right I'm
done yes no no that's not true because
the way you model asynchrony is by
having a machine so imagine that you can
model the environment be asynchronous
input as a test machine that sends
messages to this program so because they
are both running concurrently the input
from this test machine can arrive
anytime after an arbitrary delay that's
how you the model asynchronous input I
mean it will try all possibilities right
there is no notion of time so it will
just try all the places you made all
things
we have a seizure ah yes yes good
question yes it does so yeah I didn't
emphasize it but since you asked I will
show you the code check this out so this
is the how the timer cancellation is
being handled look at this route whoa oh
I think I hit the debug button sorry
so
so in this code here the failure
detector sends to the timer machine the
canceled event right and then it is
waiting for a canceled successful
canceled failure so here it does a you
there's a statement in p called receive
so this receive statement blocks looking
for either one of these two events okay
and then if it receives canceled success
it execute this code when it receives
canceled period execute this code so
this blocking receive is implemented
using continuations this is precisely
what happens in c-sharp with the await
construct right so if it is if this
event is already available then you
don't create the continuation you just
zip through it but if it blocks then you
package up the continuation and your
stash it and then later on when the
event arrives you unpack it and then
start executing no more questions okay
so let's thank yas again
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>