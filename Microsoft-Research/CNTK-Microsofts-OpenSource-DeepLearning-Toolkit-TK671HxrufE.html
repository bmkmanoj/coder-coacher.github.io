<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CNTK: Microsoft's Open-Source Deep-Learning Toolkit | Coder Coacher - Coaching Coders</title><meta content="CNTK: Microsoft's Open-Source Deep-Learning Toolkit - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CNTK: Microsoft's Open-Source Deep-Learning Toolkit</b></h2><h5 class="post__date">2016-05-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TK671HxrufE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome back after the coffee
break I hope you enjoyed the discussion
as you head out there and I also hope
that the next session will give you a
lot of material to discuss about and the
next break during lunch it's my great
pleasure to introduce you to Frank sytem
Frank's IRA has been with Microsoft for
15 years 13 years of that he spent with
Microsoft Research in Asia Frank is the
only principal researcher I know at
Microsoft Research who has not completed
his PhD instead his credential comes
from the very exciting work he has done
around neural networks and especially
deep learning and speech recognition in
fact he and his team were the first to
demonstrate how well deep neural
networks could work for speech
recognition so this has started a huge
flurry of research in that area today
frankly give us an update he will tell
us about the project that his earlier
research on speech recognition has
evolved towards he will be introducing
the toolkits the NTK give me a hint and
hip welcome Frank and his talk so I will
talk about deep learning and
specifically about learning deep neural
networks with CNT K which is the
computational Network toolkit of
Microsoft so maybe you know that
Microsoft was one of those early
pioneers of deep learning we were sort
of part of the first wave after Geoffrey
Hinton has first shown how to reliably
train deep stacks of networks and
nowadays deep learning is pervasive in
the whole company it's basically in Bing
for example speech recognition relevance
Skype translator you heard about that
already hololens and many projects in
Microsoft Research use deep learning in
the company so just to give an idea why
that is such a big deal let me start
with a slide this slide shows you the
historical development of speech
recognition error
so my background is speech recognition
so I'm most familiar with this and this
was compiled by the American government
and one of the scenarios that's very
important is this curve here the
switchboard scenario which is about
transcribing general topic conversations
between people on the phone now why
would the American government be
interested in that as a different
question but what you can see is that it
plateaued around this time and so it
turns out in 2010 my colleague don't you
in Microsoft Research and Redmond showed
that you can actually get very nice
gains with deep neural networks on
speech recognition and I started to
collaborate with him and we were able to
basically take this scenario here and
push it down from a writer rate of
something like 24 percent to 14 percent
just by a single technology that we
added to this by using deep neural
networks and now we're approaching nine
eight so we have a real chance of
reaching human accuracy which is
measured to be at about four percent so
let me just give you an idea on on what
that means to have a speech recognizer
that runs at about eight to nine percent
this is a demo that our former corporate
vice president gave four years ago in
China in Tianjin and what you will see
is him speaking and this will be
transcribed life by our speech
recognizer that has deep neural network
based could you please click on the
video few years just read a lot
combination of statistical techniques
and big data have allowed us to do a
much better job than we previously were
able to do in being able to translate
webpages or other kinds of information
into other languages okay so you can see
that this is really really accurate and
this was life on stage and the speech
recognizer was in no way adapted to this
specific task so the one thing that we
then did on top of that once you get
speech recognition that is so good you
can start combining it with other things
and what we combined it with was with an
automatic translation system could you
please play that video
but this technology is very promising
humanity shows videos he loved then we
hope in a few years that we'll be able
to break down the language barriers
between people so this was this was the
system that then evolved into Skype
translator and indeed within a few years
we did break down the language barrier
between people to quite some degree
another important result we had with
deep learning was the imagenet
competition so you can see recognizing
or tagging images is an extremely
difficult task if you go back to 2010 we
were at word our error rates of
something like 28% and that has come
down basically by an order of magnitude
by today just through the use of deep
networks and this is a system that was
developed by my colleagues in MSR Asia
and Beijing and was able to win
basically the first rank in all five
categories of this particular
competition so deep nets are really
really big deal for Microsoft and I
think for many of you as well so what I
would like to do today is to talk about
CNT K what is it how would it look like
if we were to use it what's the typical
things you need to do give you a feel
for that I want to do a deep dive into
some of the interesting technologies
that we have in there that maybe other
toolkits do not have and if we have some
time at the end I will also show you a
couple of examples on concrete examples
on what it looks like so let's start
with what I'm gonna first read that to
you and then explain it in more detail
so C n TK you heard this this morning is
Microsoft open source cross-platform
toolkit for learning and evaluating
different networks C NT k expresses
arbitrary neural networks by composing
simple building blocks into
computational networks it can be very
complex and it is production-ready so
let's start with the first point so C NT
K is Microsoft's open source
cross-platform toolkit so this kind
interesting CMT has developed in an
open-source model also inside the
company and outside the company and the
tool originally was created by my
colleague to make our work a bit easier
and modern create more flexibility for
our research what four years ago was
open sourced about a year ago to complex
and then the beginning of this year we
moved it to github with a more
permissive license so this is not
licensed under the very premise of MIT
license so nearly all of the development
we're doing is in the open you can see
all the work that I'm doing there it's
just in some branches there's nothing
hidden and this is today used by speech
windows hololens for example bing MSR
and many of those contributors in those
teams started out as a sort of bottom-up
activity they got interested in it tried
it and now some of them have evolved
into at least temporarily full time
contributors to the CNT Kay project so
and the first CNT K train models are now
at a point where they are still being
tested but they're receiving real
traffic from users on being for example
of speech very important I guess for you
is that it works on Linux not just
windows-based Linux is a first-class
platform for C NT K in fact our main
server farm that we use internally is
running on Linux so I do the software
development on Windows but if I want to
run a bigger job I submit it into a farm
that is running Linux ok so the next
point C NT k expresses arbitrary neural
and that arbitrarily complex neural
networks by composing simple building
blocks into what we call computational
networks so what does that mean let's go
through an example this is sort of the I
would say the canonical deep network
it's deep because it has more than one
hidden there so it takes an input you
multiplied with a matrix at some bias
and run it through a non-linearity in
this case the sigmoid that's your first
hidden layer output you take that as an
input to a layer of the same structure
get a second layer output and use that
as an input to a soft max function with
again another fi-in transform here and
what you get is a probability
distribution over classes that you would
like to class
so what you also need to do is you need
to define your training criterion so
typically we use cross-entropy
which essentially gives you a measure
for how well your probability
distribution represents the the correct
answer and then you have to sum up over
this criterion throughout the entire
corpus and that's what you're maximizing
for so maximizing means you're adjusting
these parameters so that this value
becomes maximum so to turn this into a
see antique a computational network the
first thing we need to do is is to turn
it into our representation so you can
see this is the way you write it in C
ntk it looks very similar to the
original maths and then we will
translate this representation into a
computation graph so you see exactly the
operations you have an input it gets
multiplied with a matrix you add a bias
you apply the sigmoid and so on and so
on
right and that is what we call a
computational network so in these
networks all the nodes here are
functions that are applied to data and
the edges are values these values in CNT
K can be arbitrary near arbitrary rank
tensors and we also support sparse
tensors for sparse input data for text
workloads and the corresponding labels
and for embeddings so one really really
that this graph representation has a
bunch of very very good properties very
helpful property so one is that it
provides you with automatic
differentiation so what does that mean
well the way to train those parameters
is to basically run data through the
network and look at what the output is
and then sort of kick against the model
a little bit to make it better the next
time it will see the same data more
correct more close to the correct answer
so for that you're computing a gradient
with respect to the objective function
of a net this gradient to these six
parameters in this case and the nice
thing is so actually the hard thing if
you write those kind of things for
yourself is to get those gradients right
but by this representation all you need
to get right is the gradients for the
individual nodes and then you can use
the chain rule to compute any gradient
by just back propagating gradients
through this network
another thing is that this graph
represents what we call a deferred
computation so in some toolkits for
example in torch all the operations
you're executing are running right away
in our case this description gives you
this graph structure and that has a
couple of advantages so for example that
this graph structure is then handed to
an execution engine that performs the
computation and that means we can do a
lot of things interesting things like
optimizing execution and also optimizing
memory and I just want to give an
example of what the memory optimization
would look like so if you look at this
graph each of those nodes carries memory
for its output or its result that it
computes and this memory needs to be
kept around in some cases because when
you have back propagating your gradient
you sometimes need those values one more
time to compute your gradient that is
the case for example for the matrix
product and for the sigmoid however the
plus here that operation does not need
the original value to propagate the
gradient so we don't need to keep it
around and by being able to look at this
entire graph we're now able to see and
say ok this plus and this plus and this
plus they can reuse the same memory that
that's a big deal if you go further when
you're actually using the network as
opposed to training it you don't need to
keep anything around for the gradients
so here you can get away with two
buffers you always just need the output
and the next result and then you can
reuse this buffer here and so on so it
becomes very very memory efficient the
same thing when you do back propagation
the gradient that flows through here you
literally only need two buffers in this
example that just are used exchanged you
know one after the other so this
typically gives you something like a
factor of maybe 1/3 or 1/2 reduction of
memory usage which can be a big deal
also because we have an explicit
representation of the graph we can do
neat tricks like editing them after the
fact which allows you to do something
like a layer building training where you
start with a small network and then keep
adding things to it so this
computational network is really the core
of of CNT K
and it's so basically by having this
Lego like composability you are able to
support the wide range of standard
network types like feed-forward DNS
recurrent networks long short-term
memory
convolution DSS M deep structured
semantic model and sequence the sequence
model and most of them are just composed
out of primitive blocks and it also well
you can use these applications than
these these networks in many different
applications including speech vision and
text and you can also combine them for
example you can do image captioning with
this okay computational Network now the
last point is CMT K is production ready
so what does that mean well CMT K is not
just a research tool it's also meant as
a tool for product groups to build the
actual products and if you build
products with newer networks you're
dealing with very different dimensions
so for example the Skype translator
model was trained on one point four
billion samples now try to do that with
Fianna or something like that right so
this is very tricky so CNT K really is
optimized for being able to handle very
large workloads for example of course we
make use of GPUs but another very
important thing is that out of the box
you get algorithms for paralyzing
training over multiple GPUs and multiple
servers so here's an early benchmark
that we ran that was done in December so
these versions of the other toolkits are
from that time frame they might have
improved
we haven't rerun this again so what you
see is the blue column here is a
comparison was that a standard
feed-forward network six layers or
something like that four layers you see
that CNT K on a single GPU is about on
par with torch and cafe but the moment
you go to multiple GPUs it scales much
better right and then if you go across
multiple machines that is something that
those toolkits do not support or in case
of tensorflow
did not support at that point in time
whereas ent K still gives you very very
nice scaling here ok so now I would like
to talk a little bit on what would it
look like if you wanted to use CNT K in
your work
so the first thing you need to get an
idea of is the general architecture of
CNT case so you can see there are three
main building blocks first you need to
be able to read your data second is you
need to describe your network and third
you need to run the learner the SGD
learner over that so in order to run a
CMT k job you need to basically you run
an executable and give it a
configuration file and a couple of extra
parameters that you could also put into
the config file and what you can see
here for example you say that it's
supposed to execute a sequence of
actions and then for every action you
have to have a corresponding a
definition block so for example the
Train action while you tell it what CPU
or GPU it should run on in this case
pick the best you give it where the path
of the model goes and then you have the
configuration blocks for the three
components the reader the network and
the learner so the reader the reader has
two purposes one is it reads your data
and it interprets the binary file
formats that you typically have for
example there are readers for reading
JPEG images or speech features another
important thing that the reader does is
randomization now for SGD you need to
randomize your samples it's very very
important but when you're dealing with
1.4 billion samples that's a tricky
business you don't want to always have
to pre randomize your data if your data
is maybe half a terabyte large or
something it's gonna be very
inconvenient
so what C NT K does it gives you an
on-the-fly way of randomizing the data
and it does that in blocks so we are
still disk efficient we don't keep
sending the disk head you know running
back and forth on the disk all the time
and you just get that for free just
configure it and then you get basically
the randomization automatically done for
you the next thing is the network so how
what do you need to specify you need to
say what are the inputs of the network
what is the network function including
the learn herbal parameters and what are
the outputs you're looking for so and
these network networks are described in
what we use this kind of silly name for
we call it them brain scripts because
they're sort of you know little scripts
that describe what our little brains are
supposed to compute
and we have an custom description
language for that we also call that
brain script but not everybody likes to
use new languages and people sometimes
prefer to stay in their you know known
world so we're actively working on
making the same functionality available
from Python C++ and.net languages it's
gonna land in two or three months so
what would that look like
so you already saw this network here and
what the corresponding formula looks
like but you need to do a little bit
more for example you need to define the
inputs so this simply says that this is
an M dimensional input now what is M you
need to tell it right maybe a 40
dimensional feature vector and you have
9,000 output classes that's typical for
speech and then you also have to get
declare the learnable parameters so in
this case it's a parameter of N by M and
so on you need to do this for all the
layers now that quickly becomes sort of
unwieldy for example the imagenet
network that i talked at the beginning
that got 3.5 percent has 152 layers so
you don't want to write this so the next
thing you can do is you can create
reusable blocks so this is a reusable
block that defines a parameter and then
directly performs the corresponding
network operation and then you can use
those down here right so you pass it the
inputs and the corresponding dimension
and notice you can actually also pass
functions around now that's already
making life a little bit easier but the
other the next thing you can do is
leverage the fact that these functions
can be used recursively so you can
actually define a macro a function that
defines a whole stack so for example L
is the number of layers if the number of
layers is 1 then it's just the same as
the other macro that I should the other
function I showed you but if you have
more layers than what you would do is
you will first create a network of one
layer less and then pass that as an
input to one more layer and so now you
can very easily describe you know
networks of whether you're lengths
you're depth is basically parameter rise
you can easily change that number and
change it to 6 hidden layers or
something like that ok so this gives an
idea on what it looks
like to describe networks in CN TK sold
in summary it uses a very
straightforward easy understandable
syntax it allows you to create these
reusable modules which is very useful
and that gives you also the possibility
of composing things on a rather high
level and soon again this will be
possible to be done from Python C++
and.net languages so so what does this
stupid named brain script actually comes
from well on one hand we think it's a
really great name because it perfectly
expresses sort of our grand ambition
right to build a brain but actually it's
acronym more accurately expresses where
we are with respect to that vision yes
okay so just to close this off the last
thing you need to do configure is your
learner so I mean there's not many
things you can configure you say
whatever you give it a momentum
parameter mini-batch size whether you
want to use things like ADA grad one two
things to call out is that CNT K allows
automatic adjustment of learning rates
and mini-batch sizes and the other is of
course for the mighty GPU setup you need
to configure some things there okay so
what's a typical workflow well to Train
all you need to do is you run CNT K with
a config file right you can also I
showed you that already chained multiple
operations together so that's very easy
now if you want to paralyze that it's
very very simple all you do is activate
say parallel training true now you
better make sure that it picks the GPU
automatically so you don't run all the
parallel of things on the same GPU and
you put an MPI exit call in front so
this call basically says run on four
servers and it runs four processes on
each server and the assumption is you
have four GPUs in each server so this
gives you a 16 GPU run very very
straightforward now when you finish
training a step you might want to do
some additional manipulations of your
network so you have this ability to edit
networks and then maybe you want to go
back rain and run multiple iterations
built multiple layers and when you have
your final model you want to use it and
you can apply it in two ways
once you can apply it from file to file
so you read a data file and write out
the prediction probabilities and the
other is so this is another command here
and the other way is you can use it from
code we have a library that you can call
from C++ it's called eval DLL even on
Linux and the dotnet version of that
only exists on Windows ok so now I hope
you got a bit of an idea on what it
looks like to use this the next thing I
want to look into is some of the
algorithms that we implement in C NT K
that are a bit more interesting and that
other toolkits don't necessarily give
you so the of course it has the basic
features SGD the computation network is
not an idea that we invented every every
other tool has that as well I already
mentioned we have these algorithms for
automatic tuning of learning rate and
meaning of exercise and already
mentioned memory sharing the three
things I want to talk about now is the
way that CMT K handles time the way it
helps you by packing variable length
sequences into mini batches and I will
say more about how the parallel training
actually works and let's let me point
out the other toolkits that none of them
stops you from doing exactly the same
algorithms that I'm going to talk about
but you pretty much have to write them
yourself they don't come stock okay so
handling of time how does it work let's
take our example from before and let's
add a recursion into it your recurrence
into it let's create a recurrent Network
so the first thing we need to do is
introduce a notion of time so now these
samples are no longer independent
they're meant to be on a time axis and
the next thing we want to do is we now
want to feed the output of a network
back as its own input but the the sample
from the previous time step so this is
what a recurrent a simple recurrent
Network looks like so in C n TK we do
the slightly different we actually do
not have an explicit mention of time
here instead we can refer to a past
value by an operation called past value
so there is no explicit notion of time
you're not supposed to write any for
loops or something like that by yourself
so how does it work well let's take our
network again and turn that Network into
into this one here but first I need a
bit more space so if you now add these
things in here this is what you get to
get these little local loops here and
what you see is suddenly now this
network has a cyclic dependency inside
of itself so CNT K has an algorithm to
discover those cyclic dependencies basic
the minimum cycles that you have in
there and then C and D K we'll know that
that is the part that has to be unrolled
in time because one frame depends on the
previous frame everything else that is
still drawn in oops that is drawn in
black here does not have that time
dependency and that's very important to
know that because that allows us to
compute all of those in parallel and
doing things in parallel if you're
working with GPUs is the most important
thing to get efficiency so you're really
if you have a chance one to launch you
really want to launch for example this
part of the operation as a single CUDA
launch whereas here you really have to
do this step by step you have no choice
and that can give you a factor of two or
something like that of speed at least so
now if you compare that for example with
tensor flow and tensor flow does not
have this automatic inference instead
you literally have to write a for loop
where you say your new output and state
is equal to for example in this case an
LS TM of your previous state and that
that's kind of a very weird way of doing
it because you're mixing the idea of
imperative code for unrolling and
deferred code execution for the final
graph so in a sense you're generating
code here and in CNT K you don't do that
you just everything is done in a
deferred fashion the loop itself is part
of the deferred computation and it's
open to all the optimization and
everything that we can possibly do so
another thing is well when you run this
this recurrence it means you have one
time one frame you have to multiply it
with a matrix then you can compute the
next time step if demographer the matrix
again
and typically these matrices are so
small that GPUs are not efficient you're
not actually using most of the GPU cores
that you have available so one thing you
can do about that is to batch multiple
sequences together so for example you're
computing maybe 20 sequences together so
in every single time step you're
computing 20 frames in one goal that
already makes a huge difference in terms
of efficiency and so in order to do that
CNT K will help you to batch these
sequences together into mini batches so
let's just walk through let's say we
have a total mini batch of I don't know
seven sequences and we start with
sequence number one now we add the next
one yeah we can put it here and now you
can see this first frame already allows
you to execute two time steps or two
samples at once but the next time step
has to be a separate launch because it
depends on the previous time step ok now
we're a bunch of more sequences AHA so
look sequence 3 actually is short enough
that you can actually utilize this part
and you can also compute this in
parallel with sequence 1 let's add a bit
more taking 4 5 and 6 and maybe you have
a short one we stick it in here and now
we're done we cannot fit any more data
so the rest basically becomes sort of
padding and CNT K will help you to
create exactly that structure now
there's a couple of interesting things
if you want to launch a parallelizable
operation on this you can just take the
structure flatten it into a matrix where
basically every each of those things
here is a column and to a bit CUDA
launch for example to compute a sigmoid
now or the softmax now this big CUDA
launch will also operate on those
padding areas here but it's garbage in
garbage out we don't quite care it just
runs over it and computes garbage for
those padding areas but we never look at
them again so that's totally fine but
how about the recurrence so for
recurrence we have a problem let's look
at this time step here this one we can
do all four in one go we now go to the
next time step we have a problem because
at this point we must make sure that we
don't carry over the state from the
previous sequence instead we need to
reset the state so CNT K has all the
logic built in to handle that correctly
for you the same thing is in back
propagation right for the gradient
and the third place where you need to
make sure is when you read during a
reduction over the the whole mini batch
that you don't accidentally include the
garbage that has been accumulating here
during the processing
so again chdk does all of those things
for you automatically
and yeah we really get the automatic
speed-up typically if you go say from
one to twenty parallel sequences you
nearly see a speed-up of twenty and
that's because with one single sequence
you very very rarely utilize your full
GPU if you go beyond twenty thirty
that's just a rough rule of thumb you
will not see any further speed-up
because at some point you start using
your GPU properly right but at least you
get to this point cindy kaye will just
do it for free okay so recap users never
explicitly talk about time or see time
except using these relative past value
and future value specifications and
those are sufficient to infer the
complete new structure of your your
network and CNT K does the automatic
packing for you as well okay and if you
know some other tool kids do this in a
different way because they are not able
to pad and pack these sequences together
so they use a technique called bucketing
what you do there is your grouping
sequences of similar lengths into mini
batches but they should be close enough
and because they're not able to put
multiple sequences back-to-back and in
CNT K you don't need to do that okay so
the next technique technology I want to
talk about is data parallel training so
that's a big deal that's really a big
deal I just want to give an example when
we built the sky translator model we
were able to gain another twenty eight
percent relative word error reduction by
using a very specific kind of recurrent
Network that's on top of the initial
numbers that I showed you but it turns
out that only that gain only
materializes when you run this on the
full 1.4 billion samples if instead you
use a smaller set like this switch board
set I talked about in the beginning
that's only 250,000 samples the gains
are much smaller it's only four percent
relative they're so small that you would
yeah that's actually not important I'm
not going to look at that any further so
it was very important for us that we
were able to run this line of
experiments on the full 1.4 billion
samples otherwise we would have never
discovered this that this technique
really works that well and so being able
to paralyze your training is not only
important to build products fast but
it's also important to have this
research cycle and optimization cycle
and be able to do that on basically
production size data and that is really
really critical so the way that CNT K
paralyzes is we call it data parallelism
so imagine you have a mini batch of
1,000 samples 1,024 you have 16 workers
then every worker will grab 64 samples
and process them and at the end of the
mini batch now they have to exchange
these partial gradients that they have
computed and the problem is that these
gradients are big that's very very
expensive so let's do a little bit of a
calculation so for example for a mini
batch size of 1000 a model was 160
million parameters it takes about
one-seventh of a second that's maybe a
slightly older generation of a CUDA card
okay 20 I think takes about one-seventh
of a second to compute this but how long
does it take then to exchange the
gradient across machines it's about 1/9
of a second in the same order of speed
right so that means the communication
cost totally dominates and you cannot
parallel eyes that you can only paralyze
the first thing so even if you're
paralyzed that thing make it ten times
faster your total time your total speed
will not be their speed it will not be
more than factor of two because you're
totally dominated by the time it's
totally dominated by this communication
cost here so that's the thing that's
what makes paralyzing neural network
training very very hard and non-trivial
so how do you approach this problem well
there's two things you can do one is you
can say well I just communicate less
data for every exchange maybe some form
of data compression the second is can we
communicate less often
both of these approaches are used in CNT
cave so the first technique is something
that I developed it's called
one bit SGD and that's a very
interesting technique so what you do is
you quantize those gradients before you
exchange them the gradients that each of
the workers have computed and turns out
you can actually quantize them very
aggressively if you use a very specific
trick you can quantize them down to a
single bit so instead of having to send
32 bits you only send one bit but the
trick is it's very important the trick
is this when you when you quantize so
aggressively you're making a lot of
error there so some of the gradients are
too small you're overestimating them
some important big gradients you're
transmitting only a small part of that
because you can only transmit two levels
of value basically so what you do then
is well you know what the other side
will receive you know what you wanted to
send and now I compute the difference
this is the error that you have not sent
yet you remember that and then edit into
the next mini batch and then that next
mini batch has a chance again to
transmit some of that but it's also
again quantized to one bit so you have
this continuous residual that always
carries over what it has not transferred
yet so in the end you will always
transfer everything will with full
accuracy actually but just you know at a
certain delay you never transmit
everything at once so it turns out this
really gives you literally a 32 times
reduction of communication cost and that
now now you can imagine right before we
had 1/9 of a second now it's 1/9 divided
by 32 now suddenly the whole equation
works out and you can get very nice
paralyzation gains the other thing we
also do is maybe we can communicate less
often and that is we can increase the
size of the mini batch so we have this
technique that once in awhile tries and
see what happens if I just double my
mini batch size does it still give me
similar or same convergence and it tries
that on a small subset and if it works
it just continues with that big mini
batch size then it turns out in the
beginning you start maybe with a few
hundred samples in a mini batch but very
quickly it goes up to a few thousand and
at the end of the training you're at a
hundred thousand so communication cost
basically completely disappears it's a
problem now there's also a recent
technique that we have just I think
committed three weeks ago to the
to the master branch it's called block
momentum and that takes this idea a bit
further of course the ideal situation is
if you don't have to exchange the
gradients after every single mini batch
so imagine you just run maybe 300 mini
batches in a roll and then you sort of
aggregate this block gradient this
master gradient that basically is the
difference from where you started and
where you are but going through some
complicated nonlinear path and if you do
that on maybe 16 parallel workers you
can then take these block gradients
aggregate them over all machines and you
get a really big gradient that you can
then apply to this initial model and if
you do that you will immediately diverge
it's not going to work
but we can take a lesson from something
called model averaging model averaging
is pretty much the same technique except
you're not summing up those gradients
you're averaging them and it is well
known if you averaged them actually you
don't diverge so it's just a problem
with the magnitude of what you're adding
now model averaging is kind of bad
because you're making very little use of
the actual gradient that you're
computing so the idea of block momentum
is this you first do what model
averaging does you only add maybe one
case of case the number of workers you
only add one case of your gradient to
your model but then you remember that
you have not yet added 1 minus 1 K of
that gradient and put that into an error
residual just the same as for the one
bit technique and then used it yes this
excuse me in the next block update and
if you do that you get very nice
convergence and it works very very well
and so here's a couple of numbers so for
example this graph shows scaling over
GPUs this is the number of GPUs and this
is the scaling factor speed-up that you
get and we can look at the red and be
yellowish or what is this actually this
color greenish column so the reddish is
the one bit technique and the greenish
is the block momentum technique and you
see it's always slightly better and in
this experiment the the the the person
who did this my colleague jung chung he
didn't even go further with a 1 bit 1
bit technique here and you see you get
very nice scaling you scale 54 using 64
GPU
now at the moment we're not really using
bloc momentum as a default technique
because it's too fresh
we still need to gain a bit more
experience on does it really work very
very reliably on a broad range of
techniques but definitely for speech you
can see how it works if you start with a
single GPU baseline of 11% word error
rate both one bit and the block momentum
technique here they don't really harm
accuracy at these speed ups okay so the
last thing I would like to do is go
through a couple of examples so I'm
gonna start with this ResNet this is
this model that gave us 3.5% image
classification error rate that model has
152 layers and the basic idea here is
that the authors of this paper changed
what they try to learn in a layer so
you're not trying to learn anymore a
layer that transforms an input to an
output but instead your default is that
you just pass on your input and you only
learn sort of a small correction term
and that gives you very very nice
convergence properties because your
gradients really come you know propagate
through the network very very very
nicely and this way you can really go
ultra deep in this case 152 layers and
this is how that team won this
competition in all five categories so
what does this look like in CNT K so
this is the actual structure that was
used it's basically a one by one
convolution three by three one by one
with a bigger feature map and then you
add the original input to it and after
each of those steps you have an relu
non-linearity and the translation is C
and D K is really really straightforward
right so you have this function here
which takes the inputs dimensions and
this is the one by one three by three
one by one convolution so now what is
this convey and layer it's one of those
were useable blocks that you write
yourself for this and it basically
defines a parameter and then executes
this convolution primitive that we have
and then it's followed by a batch
normalization there which is something
you write yourself it basically has you
define the parameter and then you call
the
that's normalization primitive by the
way these two convolution and bachelor
ization goes straight into the nvidia
library so they're really really
efficient okay and once you have this
the next thing you can do is the same
trick building and build a whole stack
because we need to build a lot of stacks
of networks here all right you don't
want to write this all out so we use the
same trick with a recursion and then
what with that building block this is
the top level of a description of this
resonate very very straightforward okay
so recap convolution pooling
rationalists all standard operations and
it's very straightforward to describe
even 150 to layer Network in this I mean
obviously I just showed you how to
program right this is nothing super
special all the other tool kids can do
this as well but I just want to give you
a feeling on what that looks like in C
NT k another thing is a technique that
recently gets a lot of traction is
called sequence to sequence so what this
technique does is it Maps entire
sequences for example an English
sentence to another sequence for example
or French sentence these sequences don't
even have to have the same lengths right
so it's very different from the previous
RNN that I showed you and they normally
run your input sequence through an
encoder network which is a recurrent
lsdm for example that creates a sequence
of hidden states for every input and
then to generate the output they run
another decoder a decoder network and
then there is something between is
called the attention model that models
on what input token is the one that is
most relevant to generating a specific
output token so especially in machine
translation they're not necessarily
always in the same order you have the
problem of word reordering and so also
in order to run this model you need to
use a technique called beam search which
maintains multiple hypotheses and only
picks the best at the end of the
sentence and all of this can be
described just by CNT K using the
computation network and we ran this at
the moment on a small task for
four-letter to sound conversion we
didn't run yet machine translation in
this so it translates a letter sequence
into the corresponding phoneme sequence
how to pronounce words in English and
with this technique CNT K gets very
close to the best published number on
this task and what does it look like
well it's very straightforward right
well okay so one thing is special
because your input and your labels they
now live in different time axes that's
something you have to tell CNT k and how
do you do that by explicitly creating an
axis and passing it in and we use this
custom axis for the input and for labels
we use a default axis that also exists
and then you run it through well a stack
of networks
that's the encoder the decoder runs the
thing through also the same stack of
networks but it has an additional
function that you had that you pass in
that models the additional information
that it carries over through the
attentional model from the input and
then at the end you have your regular
cross-entropy
output here so what is this recurrent
lsdm stack is exactly what I described
before it's a recursive definition here
I use a slightly different syntax you're
also able to define those things as an
array and then at the bottom of it you
have an LST M now also the LST M is just
written in terms of C n TK primitives
there's nothing special about that if
you if you were to go through you can
directly take this picture this is the
famous picture of the LST M and method
one on one to the corresponding lines in
the code here and if you wanted to
change it into a GRU for example it's
very easy just make the necessary
modifications here and the rest will
just work okay so now this is the
resulting network it has 600 nodes so
I'm showing you this just to show you
how complex networks can be and yet how
many how little code you actually need
to describe those and chdk can just
handle this just fine okay so I'm done
with the bulk of my material I just want
to give you a bit of an outlook so
what's on a road map the one thing that
we really are investing heavily in now
is to turn CNT K as a library into a
library as at the moment it's it's an
executable that you're running for most
of the applications and having it as a
library is
it gives you a lot of benefits so one is
it allows you to define your network in
your favorite language you don't have to
use brain script for that but the second
was probably more important is you
probably already have a lot of code
lying around for dealing with your data
so data reading will become much easier
if you're able to just reuse your
existing components and then call from
your code into CN TK to do the rest and
it also gives you the possibility to do
sort of more non-standard STD variants
for example in reinforcement learning
you have to enter in to leave your
computation with you know generating new
data once in a while and that will also
be enabled by this we're also working on
increasing the expressiveness there will
be a better handling of nested loops and
we will have increased sparse support
which is becoming very very important
for text workloads another thing is
we're going to push it towards very
large models models that are so large
that they don't no longer fit into a GPU
or that the activations no longer fit
into a GPU and lastly Asia is very soon
coming out with a GPU skew that will
then also be able to run cm TK okay so
in conclusion cm TK is Microsoft's open
source cross-platform toolkit it runs on
Linux Windows and has bindings for
dotnet and C++ for decoding I talked to
you about this notion of computational
networks for representing your network
which has all these goodies of automatic
differentiation deferred computation
potential for optimization the automatic
techniques that we can deploy for
packing data in mini batches and the
necessary the resulting potential of
speed-up that we have there I described
to you the data parallelization
techniques of one bit STD and block
momentum and I think it's pretty clear
now on how ctk can just support this
wide range of different model types and
different scenarios and lastly it's
production-ready
so that concludes my talk if you want to
know more you can go to the CN TK github
website all the source code everything
is there again it's under an MIT license
ok lastly if you have any
question you can ask you can raise an
issue on github that's our current way
of asking questions and how you do
things or you can also just shoot me an
email so thank you very much
thank you for the excellent talk we have
some time for questions thank you I am I
am Calvo from Center for computing
research Mexico and I have a question
too about the propagation of the
gradient the inverse you say that with
one bit you are able by remembering how
much are you not transmitting but what
happens if you have this accumulating a
good render and over
oh you mean you keep keep accumulating
and it gets piles up no that wouldn't
happen and the reason is that we
dynamically determine the range of the
data that were transferring so this is
done in this particular implementation
independently for every column of the
matrix so what I do is for every column
i compute sort of what is the
reconstruction value for one and the
reconstruction value for zero and it
turns out the the easiest way and also
working very well is to threshold at
zero so you look at positive numbers and
negative numbers and I didn't get any
further benefit of adjusting this
threshold or learning this threshold or
something so basically what I need to do
is then I compute the minimum square
error estimate of all the positive
numbers and the minimum squares estimate
up to all the negative numbers and turns
out those are just the means okay so
what is that just transmit the means of
all the positive numbers and the means
of all the negative numbers and that
automatically prevents the problem that
you described okay thank you cost a
little bit of extra data I mean you're
sending maybe 2048 bits plus another you
know 64 bits for two floating point
numbers just once yeah well no you do it
in every - every time and for every
column but it's still it's a small
percentage of the overall data so it's
not a big problem okay thank you very
much hello I am sorry from when you
spent University of Sao Paulo thank you
for your presentation was great and I
would like to know about the question
when we work with neuro net
we need to choose the configuration the
number of layers etc so you have they'll
come raisers to indicate that the best
network usually is the simplest
networking in terms of generalization so
what we can do in terms of evolutionary
neural networks using these frameworks
so to run different configuration
because you showed only one single net1
configuration right and usually we need
to run several and in several different
experiments change the topology the
number of layers or parameters yeah
it turns out that unfortunately that is
still what you have to do today there is
no really good method of choosing things
automatically there are some techniques
for optimizing hyper parameters for
example but in general you have to run
those trainings you have to run a large
amount of training jobs so what makes
that convenient in CNT k is this full
configurability but you really get that
from any programming language right this
is unfortunately what you have to do and
it turns out once you run those kind of
experiments you don't even benefit from
parallelization anymore because actually
more efficient to paralyze over the
number of parameters that you're
sweeping over sorry I can't give you a
much better answer it's just a matter
it's a bit of a black art you have to
read papers you have to talk to people
that's unfortunately still how we do it
I'm sorry about that we have time for
one last question yeah Thank You Fabio
Gonzalez from Murcia Nacional de
Colombia came for the talk very nice
toolkit a question about the speed-up
that you chose when comparing to the N
or tensor flow which are impressive
where those with recurrent neural
network we're sequence of recurrent no
that was a feed-forward neural network
okay I think on recurrent networks you
might even have a slightly larger
advantage because we are able to
optimize which parts of the network have
to be computed with recurrence and which
are not right to be really honest I
don't know what the anno does in that
area when I looked at tensorflow it
looks like it
optimize anything that treats everything
as a separate time slice because you are
required to write it in that way right
right and you as a programmer have a
choice to do that by yourself you can do
conversion operations where you've you
know batch everything together into one
big tensor perform the operations you
can do and one go and then just you get
the data back on to the individual
slices right but you will have to do
that and it's it's it becomes very very
complicated ok so so you sped that with
recurrent neural networks you you will
get high-grade speed-up oh yes I would
okay and about that the speed-up well
and you mention different mechanism new
mechanism that you implement it was like
the one that contributes the most to
that to speed ups
well definitely the the ability to not
unroll that's very important so I can
give an example when you do the back
propagation through an unrolled
recurrent Network the early version of
the code alright
okay so if you if you look at this this
recurrence here the original version of
CNT K had this parameter inside that
loop so that meant when you ran the back
propagation when we were at this point
you would be back propagating a gradient
there and also there for every single
time step and at some point I realized
hey this is actually not really
necessary we can wait until we have
finished this entire loop and then
compute this gradient in a single
operation and when I made that change
that change alone sped up the training
by 15% relative so these things are
important in the end and in terms of
other speed-up well obviously the
ability to just paralyze that is the
other thing that's really important and
I think another thing you can also just
see you when you look at the code the
entire CNT K is sort of designed around
keeping the GPUs busy so we try to avoid
unnecessary synchronization points
between GPU and CPU we try to make sure
that every time the GPU is done with an
operation the next data is there so for
example data reading is happening
I'm currently and so on it's very very
important to do those okay all right we
have come we have to conclude now if you
have any other questions please make
sure to catch a funk after I stop
thanks again thank you very much you've
been a wonderful audience</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>