<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stochastic Dual Coordinate Ascent and its Proximal Extension for Regularized Loss Minimization | Coder Coacher - Coaching Coders</title><meta content="Stochastic Dual Coordinate Ascent and its Proximal Extension for Regularized Loss Minimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stochastic Dual Coordinate Ascent and its Proximal Extension for Regularized Loss Minimization</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wlO7FbKAfm0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
well it's actually nice to come here and
talk about this part of that his
influence from Microsoft see one thing I
want to interested in is the problem of
solving very standard machine learning
problem it's a linear machine learning
problem generally you can write it in
this particular form which have PW which
is a you earlier we talked about the
primal problem with some loss function
and some regularizer so that's the
problem we are interested solving
essentially this is an optimization
problem and the examples include the SVM
like a large history question so that's
the very people used a lot in the
machine learning up applications then
you can also have some other loss
functions and like absolute loss or
squared loss and so on so one thing we
want to differentiate is the property of
loss functions like SVM and the largest
regression saw Lipsius so called Lipsius
which is basically a derivative is
bounded and the other one is smooth
smooth means you have a second order
derivative so there are two different
type of properties we were looking at
for this loss functions and you really
want you to the analysis it turns out to
be a little bit different so for the
loss functions you can see some are
smooth some are Lipschitz and some are
bows and the some are just well
so what we are interested in doing is
solving this a primal problem which is a
PW which is a standard of in the machine
learning literature it turns out that
there are the dual problem the so called
you can define the from this primal
problem
the basically given any primal problem
you can define your problem essentially
for each safai you can define the common
assuming we are now assume is convex so
you can define the complex conjugate
which is a Phi star and then you can
write down so-called a dual problem in
this form and that the ideas they are
related so what the real relationship
the relations are the follow so first
you have a primal is P W and it U is a
slight different formulation which will
be in terms of alpha and the D alpha
prime is always a larger than to you so
that's the first property a second the
property which will be useful for design
algorithm meaning that at the optimal
solution primal and the two are the same
and you have a relationship of primal
and dual variables so this one is
relationship which means if I start to
solve the problem of alpha and then I
can at the whatever alpha I can use this
relationship to get a primal solution
this you're using the Alpha to catch you
the country the primal solution and if
you can solve a very good alpha through
a dual solution that will get with the
primal solution another thing which I
want to say is it's a handy when you
want to do the convergence analysis how
much is a check typically we are
interested in how well you solve the
primal problem which is a right hand
side primal sub optimality you have a W
and there is path W of course Bastable
is unknown so you cannot directly
estimate the right hand side which you
are interested in but then if you you
are working with
the primal dual formulation because this
weak duality what you know is that the
right-hand side the the Primus above
optimality can be estimated using the
life on the side which you can observe
because you just have any W and alpha
you can estimate the so-called reality
gap and if you want to say I want to
solve this problem to epsilon a cura see
I just want to look at the left hand
side with my W and the Alpha to say
whether I can smaller than AB some
accuracy so this is one thing which will
be useful for the for this primal dual
formulation you can estimate the
convergence directly so this is one
thing essentially what we are interested
in is there are two formulations they
are related to by this form and then you
can estimate the convergence so that's
the and here is an example you can write
the data to prime of arbitrary fungus
kind of record relized
lost problems but the important the wine
example is SVM which a lot of people
want to solve you have a primal problem
which writing in the primal formulation
the first and you have a dual
formulation you can write down the
detail is not important but essentially
you you you have a see a see kind of the
relationship and a dying you have this
relationship which is in the second
essentially you want to explore this
structure to solve the our course to
solve the primal problem but the the
ideas we want to do that on the two
because we already mentioned the primal
to have some equivalence what you can do
is to try to solve the view I will find
alpha to max my my dual objective
function and oh I have relationship
because we know that this should be the
relationship you're observing and the
dual coordinate is and AB is a very
simple idea at each iteration you can
optimize this two objective function
with a single coordinate and you you
keep the rest according to the
in fact essentially you just update a
while according at a time and the
importance that we also want to stack a
CTO coordinate which is the one version
of geocoding the essent meaning that you
want to pick the update coordinate alpha
alpha I this kind of alpha I randomly
and the des it turns out to be important
that we will talk about that example of
this one we will be here the jump land
has SM oh it's very different than this
particular formulation because they have
a constraint but roughly using that your
updates they they probably not actually
I don't quite exactly know that details
right so the but but it's a kind of a
deal with you you know do I think that's
actually probably the one with the maybe
right maybe the first algorithm to do
with you all right if I'm if in the
machine learning but you probably know
more but I would from my understanding
probably okay that's okay so but okay it
turns out it's actually important that
we will talk about that later but I
believe s mo is probably first what is
one of the first try to do these at
least try to play with the deal to your
optimization instead of primal
oh is that true oh I didn't know okay I
thought its random so I might be
mistaken there the other one is a live
linear which a lot of people use input
um this idea but there are other people
implement I had my only implementation
really uh probably a little bit later
that I said no but around the same time
so but I will talk a little bit later
about this idea
oops okay so here are just the
comparison because a lot of people
nowadays in the recent literature people
seem to forget about do coordinate
descent where I sent they are more to do
with online learning stochastic gradient
descent so I want to make her just some
comparison just to see see the
difference there are some difference but
the SGD update rule if you look at for
the for this SVM it's like it's like
there and the highlighted apart the
right part is basically the
computational intensive apart after the
main competition is the inner product of
your W and X your coordinator is a
similar to main computation W and X you
can argue this one might be a little bit
the competition about this can be pre
computed so this one is not critical
you can pre computed that after that
this one under this one has the same
complexity and have a very similar
update through in some sense if you look
at that the update here W comes to there
and this w is somewhat similar so I just
want to look after Association to say
they are very similar complexity here is
the result of you compare that the the
earlier work actually is what a corpus
office or something right there I speedy
update which is pretty well known as to
the implementation of SVM this is a full
relative small Amida lambda is the 10 to
the minus 6 the first moose allows may
be basically says ice most SVM lost a
little bit so this is
essentially the results of Sgt which is
very typical
if you ever implement SGD this is what
you can
turns out the stochastic do you call in
essence it's a have this curve so this
is the one which I would random it's
kind of linear convergence when you have
a smooth loss and here is a little bit
of variation of the stochastic gradient
a dual core in it I said it's just the
instead about computer choosing random
you you'll play in a box each I poke you
random permute your coordinates so so
essentially but those are really linear
and they channel River or do these are
the I box the desire box right so it's
kind of how to say is really shockingly
fast the one when we first look at that
in the sense of if you think about that
in numerical optimization lamb is very
small in this case ten to the minus six
which generally means condition number
is very very bad and these kind of
linear convergence is really really fast
when you think about a problem like this
kind of you conditioning problem so the
question is how to explain that which is
essentially what I want to talk about
basically so why basically when you look
at this graph you you
it's really quite its basics and nothing
like what you would expect from yeah
although this one we okay in the in the
current literature people people you can
all this be thing they they even nor
that so essentially I don't need to pick
two updates we are just picking one of
things so as I am Oh / - Q larly pick
two updates just just maintain these few
constraints of equal sum equals zero
that constraints so here we we know have
constraints yeah and so we need a one so
we didn't compare it always s mo because
of that so this one is a simpler version
but but I mean history you should have
say as I'm always well pleased the first
but now people look at a simple stuff
yeah okay but this one is just smooth
the SVM so we'll talk a little bit about
the basic use you have a SVM just smooth
a little bit
you make the corner they have a current
corner right as we am you just make it a
smooth yeah this is not zero one knows
this all comebacks loss the other one
lost this one you have to figure out
something else but oh empirical oh okay
yeah that that second a issue the right
the 0 1 loss probably now doesn't make
that much difference after you after few
iterations so here we are more
interesting optimization problem they
say how can you optimize this but that's
of course it's good if you're
practitioner you want to say whether you
need a 10 to the minus fifth accuracy
you probably don't need that maybe maybe
you 10 to the minus 3 is enough but even
your mum even 10 to the minus 3 accuracy
this guy goes forever Allah so it's just
saturate and the other one just a
shooting a few iterations I sent an
iteration so you care there the other
way it's much longer
oh this is kind of the green here we
have one increment only fermentation but
the deepening your summation the is kind
of this stuff that's important apart but
I will talk about it later
here is the the true SVM basically it's
a it's a non-smooth loss function its
hinge loss and you you would see that is
slightly not quite linear but still
faster than example again you want to
understand this kind of convergence
property it's still faster than than the
SVD here so we want to understand
essentially why that's converge so fast
so that's one purpose of that so let's
look at the the theoretical analysis
earlier one so that's why I motivate
what we want to do these of course as I
mentioned we want to
find the something which is close to the
primer optimal basic very interesting
say the prior after three iterations my
primal objective function is not worse
than epsilon plus your optimum value so
roughly this you're at analysis the
following the earlier ones for the SGD
we can directly analyze the primal and
the typical it analyses one over epsilon
absolute lambda absence so that's the
convergence after seeing that so many
examples for stochastic you call in the
essence
there are analysis in the earlier ICM
paper which is a basically a linear
paper they are following that with much
earlier work of law and the song
essentially it's kind of a linear so
that that's something good about this
the problem of that analysis really is
this guy can be arbitrary small Y you
see the first one you see the SVM I
showed you it's actually become flatter
and flatter this is more actual
asymptotic so certainly if you look at
the proof this one really depends on
these small is the nonzero eigen value
of your data matrix that if you have a
lot of data it can be really arbitrary
slow to close to zero so that's there
are some primal cordon analysis you
comply to this in scenario they assume a
different structure based for example
there are no lambda here and this one
actually has no lambda there but
essentially they assume a different
structure they can approach with your
problem by the resulting or it is not
not like that it will be slower and all
these three analysis also doesn't hold
the even for largest reversion
simply because the general assumed
smoothness in the duo and you cannot
real
I assume although just requiring that
you because it's not smooth so yeah
right okay actually this one should it
be the inverse of that it should be the
I think oh yeah right this is smallest
eigenvalue of X DX
Rus it is a problem yeah because your
your dimension can be a larger than your
basically smallest the Aki model can be
very very small so there's no way you
can control that yeah unless you you
have a very finite dimension that is a
different story but typically we are
looking at a huge dimensionality in the
current situation so it's not very good
and the other thing is even apply this
kind of analysis only for you like this
paper says do sub optimality and turns
out there's a gap you have to be careful
when you because we are interested in
primal so there are some issues are
there what's really needed is something
like Primus of optimality what you are
like a gap essentially what we want to
do is actually do a little gap in this
work just the this one I think I can
skip later basically just say the
following if and just you can don't have
to have a high level message the big the
say the following
there are our problem with very smote
you sub optimality but a very large
primal sub optimality so there are large
essentially means that you want you when
you do the two analyses of two of
optimality is good it doesn't really
current here you have a good primal
solution and so there are there there
there there there situations like that
so you do not really is the insufficient
it just to analyze the views of
optimality so that's it's a message here
basically I just want to show our
results what essentially what we can
prove what we can prove of the following
first moose allows turns out is a fast
rate essentially you have linear
convergence rate when you have smooth
loss which includes our largest
regression was the most SVM for the
hinge loss which will be the Lipchitz
button our smooth
you can't slower convergence this will
be very similar to STD however we we
know that the behavior actually faster -
VD why you steal practical so there's
the explanation for that
you know analysis basically you can play
around with the analogy to say if the
hinge allows for exam it's almost moves
you can manipulate it to say I can have
a faster convergence rate with something
but a distancing depend on your data
essentially which at least say why at
least you you should expect a
convergence rate because you can turns
out even though the worst case bound is
similar to SGD in practice when your
data is good then you have a faster
convergence rate so that's that's time I
seen Arielle what we are looking at
essentially this one explains why it's
the fast in the sense you to prove fast
convergence rate now I want to compare
this particular because some people if
you just first see that you say oh then
why don't use first just run SVD we know
it slow down afterwards I will just run
gradient descent which will we know have
a linear convergence rate for this kind
of problem it turns out if you do that
you get convergence rate very different
if you do batch gradient descent this is
type of convergence rate okay this is n
times gamma lambda and n times 1 over
gamma lambda in our case if you run
directly stochastic doucourÃ© innocent
you get a faster rate in the following
sense it's n times become n plus and
similarly if you do this Lipschitz a lot
is the same thing so what you really get
out of this kind of you're solving a
deterministic problem but what you
really random you trying to do this the
Buick on innocent is that you really
have taken advantage of the property of
a problem in the sense you can secure
significantly improve the patch
convergence rate just by using this
particular algorithm
making n times become M plus so that's
the that's one thing just make sure it's
actually very fast and that that's why
you can see that very fast convergence
rate if you just do patch GD gradient
descent you will not be able to see that
convergence so the difference can be
very significant if n is very large here
is another thing I just want to mention
our analysis only holds the stochastic
read The Economist and essentially have
the randomized the sample you know in my
earlier of implementation which was
wrong is we and a lot of people would
think about that just you randomize your
order once then you start using cyclic
code in the descent turns out that's a
bad idea
because if you look at that this is what
you got if you're starting to randomize
once you this is smooth loss you you
randomize once you're starting to run
your cyclic coordinate descent this is
essentially what you get it's somewhat
like the SVD it's probably the with a
patter but are not really much so if you
compare they're comparable and if you to
the randomized version you get these two
curves is much sharper sharper
convergence it's linear this guy is not
really you cannot say this linear and
this our bound so the red curve is our
bound yeah I do have a better than we
can talk actually I do have the
intuition why this is the case but from
the proof you can you can pack
engineering intuition but that's true so
I miss one here then it's just too much
to even choose we can we can talk yeah
yeah I can tell you why
but essentially but this is chapter
surprise although although one we packed
engineer which I think finally Explorer
nation is actually is somewhat
surprising so that's actually main
reason why there's a lot of people
people doing this do you think for long
time but then later date it goes to SGD
just because they I think most people
just looking at this version so no now
the other version
and that the other thing I want to say
here is the random iodine effort is
crucial because the you to see a
practical behavior second the thing is
if you look at the game we want to say
we're a little bit different than theirs
is that the the the disguise bound they
are working with suppose the cyclic and
and randomized the date intended to
differentiate but if you look at this
bound is only from fortunately on cattle
at fast is like these right so this will
be much worse than our about let's say
we we do have a different bounds if you
analyze cyclic your theoretical
compounds cannot be better than this one
is actually not necessarily there's some
other primal we can see is this one and
this one this one has sometimes you
early you see a faster eventual rate for
this one but for the first few
iterations this could this reach for
this particular example this looks like
that but we will see another the main
thing about this one pushing we cannot
analyze that so this will be interesting
one to say what you can say about this
one so the only thing we analyze is the
this guy but yes so in practice maybe
you say they are possible they have some
analysis on these multiple imitating see
so right now we we don't know how to do
that
right right yes yeah that would I think
that will help because what happens is
for the blue thing biosample just you
you see a lot military basically if you
just go in one pass you you don't see
all the data you really just ignore some
data you have to see of your path to
even see all the data this guy
guarantees you see all the data once you
see all the paths so so indeed if you
intuitively will say this probably good
idea
unfortunately it's much harder to
analyze so we don't know remember I
think Xiao used it a lot yeah yeah it's
good to be yeah yeah yeah it's good to
be so for other data they could but no
but also it's dependent you give a lot
of us think don't know what is smooth
loss the power lambda so we will see
actually the same data there there are
slight difference later on here is your
basically motivated by these we know
that at least the smooth things you
could because when you have a smooth
loss we show it's a faster linear
convergence rate so therefore maybe you
really should have smooth the loss even
you work with SVM you you try to smooth
that fear so instead just to make a
corner a little bit of smoother and this
is ICBM this is smoothing and once you
do that hopefully you got the faster
convergence but it doesn't affect your
classification much here is basically
says classification this is actually a
very narrow region essentially say is
the classification is not its like third
digits something it's not really much
affected and you sometimes smooth batter
sometimes and Ausmus is better but here
is a convergence rate if you smooth a
little bit this one is no smoothing this
is ice via it's not linear
why not smooth a little bit
it's become a more and essentially
asymptotically the you look at our
sympathetic hero rate it becomes a
faster so this one is more like a linear
one you have this wire smooth largest so
it's really kind of more for linear
convergence what you would get so why is
really this one probably didn't do I
thought that this one doesn't affect as
GT too much it's more affecting of the
Rui Rui so it's basically similar to
what we are talking about for the as DCA
now that's we are not I don't think that
this one is getting per muted but that's
good point maybe I don't believe that
will will change much simply because I
actually does have the main problem is
not a permutation the main problem is
the learning rate it has a very very
small so you really cannot do much
anyway you you do have this kind
behavior that you cannot really get you
can have a lower bound is envy knows
much there is no way you can do that
linear there's absolutely no way
HP you can because actually one thing is
the learning rate gets smaller smaller
so there's no way you can really fix so
as you see more learning rate but you
got a lot of population but you have a
lot of lightning
for the further for the statistic called
interessante yes we were I think that
also we do but for the HPD may not I am
pretty sure it doesn't because that's
typically people stop alright this one
is yeah so one we compared research II
but but at this one yes all this cos
beta says when your smooth you can
faster convergence which is consistent
with the water we we are seeing the
smoother the the faster convergence
that's the message and and you basically
this motivates you we want to maybe
smooth doesn't a lot of things to like
because it's a zero one or something so
yeah this doesn't change it too much
it's a very small region you you look at
a little peak but if it is this way if
you look at this small region and it
sounds predictable this one you can
actually get a smaller area when you
have a smoother this one you also get
smaller you have smoother this one got a
little bit larger I rewind smoother but
the something here you get a smaller so
Peter says you can you can there's no
reason to not smooth that is the other
ones y-axis
these are loss so this basically say the
the server suburb team primal sub
optimality primal sub optimality not
added are defendants motion is a
parameter so
why
alright convergence yeah yeah yeah they
compare the different apply their
themselves
Vizio changes the problem but the
question is how how fast you can solve
that particular problem up to certain
accuracy typically 10 to the 5 and minus
5 would be really sufficient for machine
learning I don't believe anybody want to
go through 10 to the minus 10
it's more than enough I hope this thing
right so mostly I would say probably
this level is pretty good already there
are additional religions what turns out
that there is 2008 GM our paper I think
they have a newspaper earlier maybe 2005
306 first moose allows they're similar
actually similar so that one is I think
that people their papers do kind of
cycle a lot but people didn't notice
it's actually the rate when they compare
it's so good so essentially they have
similar rates for the for the largest
problem
but they were somewhat complex always
and turns out it's not necessary there
there are more thinking about something
else there is a reason the paper about
this paper particularly which got a
similar rate by modifying the SVD so
they are also kind of if you think about
this kind of a version of the Castillo
coordinate in some sense because they
are keeping the gradient keep ingredient
a essentially keeping the viewer
variable so this one is very recent the
paper they can same the rate for the
smooth loss for announced moose loss
this guy is at the same group how people
get something like using a they have
different motivations they are they're
okay have something like basically all
this is solving on simplex so they are
thinking about exponential gradient has
something to do with simplex so they
decide that this guy I think
a frank wolf or people don't know is I'm
a greedy algorithm it's also can be
applied for simplex and they try to
turns out that this one pours down to be
the same they have a different
motivation they got the same and the
bounds also the same as ours for the
Lipchitz a loss for the particular SVM
case you mean this one right I don't
believe so I think they will be
comparable
it's comparable yeah I mean random uh if
you tell me also having to formally
prove the give exact contention I think
this is a special case of people calling
in some way they say they did but they
are not writing in that way but that's
essentially what they are doing
that's my my if you ask me that that's
what are my interpretation and also the
rate would be the same that's one thing
I want to just give another some a
little bit just show you more intuition
about these methods do according to
accent because turns out it's the reason
we are interesting that is there seems
to be very fast rate so let's look at
against I'm a comparison of this one
with Sgt first compared pounder then
we'll look at this in example pounder is
a speedy convergence rate is this one
stochastic convergence rate is it is
this one so if in some case if you're n
is much larger okay so this one may be
now to really say essentially says one
lambda is a large a very large you would
expect the Sgt to converge you're faster
at least in the first epoch because
somehow this one is the end
this guy doesn't it end especially this
larger means when lambda is large one
harmless more is not a problem turns out
that's
you can observe this question so the
question is just this just compare
bounce by the way I want to see whether
the reality it happens this is where the
univer ality when you compare SDC a with
SGD just to focus on the first few
iterations what lambda is small as we
see here this earlier version is this
one I think is just for the SVM laws so
it's a so so you you can't this behavior
the basically this one says SPC
dominance but one alum is large as I
mentioned if you just look at the pound
turns out Sgt may be better initially
that's actually the case so if you look
at this guy this is also the things to
wear that cross so original we see these
always dominate this guy but here there
are some things to light very different
but again that are simply talking rid is
still faster here I don't know about
here maybe it suggests up to the Machine
precision because they may just keep
afloat
I think shoddy this I didn't ask him and
here what were maybe some other other
reason this one doesn't look right it
you can ignore but essentially this
looks right here the first few
generations indeed the SVM is the faster
so if you look at that
I think the stochastic to you catch up
in the force iteration here under that's
consistent so why idea is maybe you
should combine that at least one lambda
is large just combine these two to get a
little bit the better of both words but
this one only one you have lambdas
learned large versus final embrace more
you you still can just run the pure the
case that you're calling it
yeah this one
that's true because this one lamb it is
fairly large yeah so the convergence is
a fairly fast if you have a lot of data
yes that's true I agree so that's you so
in this case I agree so that that's why
we can say combined if you along the
large you have another option to see
combined let's combine these two could
be right but it typically actually the
interesting is typically you want to use
small lambda so this scenario the
question is how interesting is this
scenario larger lambda but maybe
somewhat interesting but in many problem
your actual returns I want to use the
small arm for ya
yeah this will explain next the next
slide if you look at the updated rule
the things happens is this is the step
size of SSG D it's a larger step size
this guy is a small step size if alarm
it is verse Morsi is that order of one
over N this one turns out to be good
actually it's more stable because you
don't want to specify to be larger than
one in some sense an OSD actually not
that great in the sense of lambda T over
N it's very strange when you have a
lambda very small you've got a huge step
size in beginning at least that away and
it's those algorithm are written of
course you can say I modified it a bit
but still you've got a huge step size
but the one I'm just small lambda is
large it doesn't happen
essentially this is a better step size
because you are doing more aggressively
try to optimize a function this one
turns out that you try to reserve that
in the later round so that's you don't
see a lot of a big if n is very large
you don't see a lot of big progress
earlier and essentially what you can do
is just the change this to VP and the
essentially yaadein you can modify them
basically you can you can you can show
something like this make more progress
earlier in the earlier on one is smaller
than and yes yes you changed other than
of course you flatten something like
that then you get so basically here is
the convergence you can get you
initialize with the SVD by just changing
that yes and then you do that this you
run the STD basically you can write GDI
sir you can use this version of STD you
just change that to be T under than you
you starting using the real stochastic
then you just start to a little bit of
manual in the treaty oh god is this guy
so with the initialization you can lock
login which will be faster than this guy
when it's log lambda but
sure you can I think the show craft but
you you you to see you do see
improvement so it's more match SPD if
you do that this way this is a route you
can cut in our case versus when lambda
is large say why I'm a square root n you
will have a change log log log in to log
log in and you will see they are
matching but I didn't show that here in
the thoughts so I will talk a little bit
about the extension of the this is your
call in essence the extension is the
following so originally when people look
at that we use G W is the two
normalization basically W square here
also I talked about X will be a factor
here we are looking a little bit more
general framework you want to have X on
matrices and W will G will be any strong
convex function so example of that will
be you have multi-class logistic
regression so this will be your you can
write this one in the matrix term in
this case by multiple class largest
regression what structured SVM is
another example you can look at your
regularization to say instead of instead
of l2 regularization like the standard
you're adding for you tomorrow one so
that's that's the generalization you can
look at the to formulation it turns out
to be similar
you have primal you can write down the
duo and you have a relationship of
primal deal here instead of a summation
you have the summation you take our
gradient so that's what you have and
this one you can argue that as a general
edition of the standard the Buick on
innocent by using this by using its
gradient yeah and this can be computed
little say that but here is the the
algorithm basically you look at this
this thing and you you have a do again
we are interested in solving to you
then you have and this is the way you
took do some approximation instead of a
solving the view itself you try to
approximate this part of the deal basic
G's a regular riser you want to keep her
the the loss function view the same but
you want to do a upper bound of your
view of your you just do a expansion by
this strong converse the assumption you
go upper bound and this one can be
efficiently solved the point of why we
want to do that
is that at the original one you can when
you to the update of a delta alpha is
not efficient but he wants you to this
approximation you can do an efficient
update of alpha just like what we see
before for the SVM or SVD and so on so
the key is you want to do this
approximation once you do that you just
again run the randomly PR pick I the
instead of a solving that you you're
solving this this version of you just
the approximation with you each time you
just update approximation so the idea is
to make it easier to solve updated ruse
imple so that's the main idea here
trying to use the upper bound of G
instead of a G itself example you can
work out the for the for the L 100-102
is the largest figure you don't need to
care about this part this is a lot just
think about the main thing is when you
work out that one does the you have a
typical truncation step which is
essentially shrink your a software also
cause of the stress holding you have
you're shrinking your your your each
observation towards zero - something
zero and optimizing this is if you
you're familiar with the literature you
see a lot of these kind of stretch
holding our one related but essentially
that that's a typical shrinkage people
apply here when you would write down a
to you this will be the form you go with
the cat and and and basically you keep
half a we and W with a truncation of B
with certain parameters
and here is the wisdom basically you you
this essentially the algorithm you would
solve at the main point our writing here
again similar to earlier you have a
truncation of a V times W this world the
computation you fish more complexity
part X is square again this one you just
needed these two the highlighted part is
the most computationally intensive apart
the other part are the one dimensional
problem which is simple to solve here
under this one again you can pre-compute
so the key part is really the
computation is solving computer this one
so that's the the most computation
integer part for each step and this one
I would just say very similar to Lynn's
work do everything the two averaging and
the compare with this is like comparing
SGD versus the SDS PC a physically this
one is very much the this one extended
to the SGD here is the extension to the
duo called innocent so that's that's the
analogy if we look at the convergence
rate for the for this this version turns
out the the analysis could be done
exactly the same way you got this you
get the smooth lost function you get
this kind of convergence for your smooth
loss and you get Lipschitz the last you
can this one and you can also say the
same argument to say if it's
asymptotically almost smooth function
okay the faster than this one so here is
the for solving l1 with the smooth loss
you have smooth allows function you want
to solve l1 if you apply this particular
formulation you can choose the lambda is
Oh absolute to get absolute accuracy and
this is the computational iterations
needed and of course you can you can
apply it you averaging for the STD
version it will look at this
computational complexity and you can
apply that algorithm this is the theorem
of our ties algorithm so
algorithm will be this complexity it
will be n divided but the reason they
have a square root of epsilon will be
because they are using a nest of
acceleration method then you can gather
and over square root absolute if you
compare this rebound for the interesting
case which will be the epsilon not too
small if we extremely smaller than then
this will be the fastest but if it's and
also some we saw 1 over N square which
generally will be okay for for the
machine learning applications because
machine learning you don't really need
to be smaller than one away in some
sense so in this situation this one will
be faster than this guy
this one also passes and and then this
one so so again the the convergence is
better so I think I could go a little
bit about it but I'll quickly just say
you don't need to look at that so the
the main idea about this proof is is
actually the key lemma or simple Emma
it's not really hard to prove I think
it's just one page or a little bit more
than one page you can prove that but the
key observation actually is that you
include improvement can be pounded by
using the Ujala cap this is your ally
cap you say if the Orca is a large then
then you have to have a make a big
improvement for for the two step that's
the key idea and if you apply the
different loss you gather this one this
one particular loss for this moose loss
in this term vanishes basically this is
where you can use to get linear
convergence basically you apply this
lemma essentially says if you each time
is the in improvement you will be not
too small if your your if the disk is
large its basic your little cap is large
another you can because you're like
alpha is larger than that you use about
primality PC you can prove linear
convergence rate of a few once you
provide you the linear come
do this automatically implied linear
convergence the rate of the duality gap
because of this inequality so
essentially the idea is you really the
key key observation is really that you
are the kappa improvement when you to
the to con in s and it has to be
randomized because everything is taking
inspect ation you if you don't render my
this this actually doesn't hold complete
the breaks then you I don't see any way
you can modify this proof to deal with
non randomized situation so that's why
this Rita if you look at this cyclic
version it's not very good but is that
it's angle II that's idea so the summary
of course if you really interest you
have to look at the paper but a summary
is this one actually works pretty well
and this beam freaked out and one reason
of course is a lot of people earlier arm
played around with a cyclic version not
the stochastic version and so people
moved to a CD because there is kind of a
similar in some sense and the important
one thing important of course we have a
knowledgeable in coordinates you really
have to use this classic version
randomized version and the previous now
it's not very satisfactory our analysis
so once you do the stochastic version
it's really you can get very fast
convergence especially for the smooth
laws the other thing is you can
generalize this proximal generalization
turns out even the analysis very
analogous in some way to the two
averaging the main difference here I
think that there are some you repeat the
use alpha you have to and butter you can
you can do that like that you are
averaging you can extend this one to the
proximal version you can handle more
complex law regularization problem like
l1 or some other problems and
essentially you can same performance
with the standard aversion and if you
compare the rate you have a better than
traditional methods in the interesting
setting like it's statistically
meaningful setting so then and that
turns out to be a very good choice
okay yeah sure like a job right
so I probably is a channel I think this
is what go in the channel right so the
this guy promote that of course the rate
you can get focusing on will not be as
good so that's for theoretical issue but
practice issue they are worried about
the randomization so they they don't
like the randomization so if you want
really to distribute the system last
scale there are still rooms to twist to
see what is the best way maybe you don't
really need for randomization and other
things but we haven't really try explain
early experiments a lot but let's say if
you don't worry about randomization I
probably would say this might be better
than the quasi-newton yeah potentially I
don't know if it will make but they
they're they are worried about that yeah
and yeah and and then you have to twist
some research to to figure out</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>