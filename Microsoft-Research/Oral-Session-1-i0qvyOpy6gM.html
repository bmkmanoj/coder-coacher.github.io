<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session 1 | Coder Coacher - Coaching Coders</title><meta content="Oral Session 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session 1</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/i0qvyOpy6gM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so thank you very much for having me
here when I was first contacted invited
I had a little back and forth arguing
with these with people who invited me
that they perhaps had the wrong person
or the wrong conference because I've
never even been a remote co-author on a
on a paper here and it best probably
some of the pedestrian tools that we use
probably cite some of the work that was
done here maybe a decade ago so I'm not
going to talk to you about anything from
the perspective of the algorithms and
techniques that I assume most of the
conference will focus on but I hope that
all in true interest you for some of you
introduce you and for others interest
you in a type of data and type of data
usage that that your techniques can have
tremendous play they do again we're
probably using things that are a decade
too old and I think that as it as the
field advances and I'll say more about
this we're going to need much more and
much better so the title of the talk is
small data sort of N equals me so I'll
be going into this and more depth but
where the focus on big data to in many
many fields is usually about big in and
the more in the merrier right the more
people the more you can aggravate the
more you can bottle across the better
and that is true and good but there is
another opportunity in this space that
has to do when you actually just get to
focus in on the individual and on the
range of data that we as individuals
generate largely implicitly and so
that's in the broad what I mean and I'll
start off talking about this mostly in
the context of health where I've done
the most work and where I think this is
advancing the earliest and where the
opportunities are pretty clear but keep
in mind that increasingly people who
look at health
really recognize that a lot of the
health issues we face or as much about
life and lifestyle as they are about
particular medical conditions and just
more broadly as I'm talking about things
that sound more like the quantified self
which tends to be focused more on on
health and wellness issues this could
have just as much play when you think
about the quantified student in the
context of all the online and digitally
based learning that's happening
quantified consumer quantified commuter
and so forth but let me ground it
initially where it has been grounded to
date so if you think of this holy grail
of personalized precision medicine it
has been advancing through extensive use
of computation and I believe that a lot
of the work done here and you'll even
find paper is referring to these
different contexts such as big data
mining across electronic health record
the sort of work that era Corvettes and
other people do in terms of mining web
web mining for being able to discover a
previous unknown side effects of a
medication just off of discussions going
on on blogs and that's a really still
just begun to tap the power of that in
part because we've just begun in this
country to more systematically move
towards getting digital health records
to happen and have data liquidity and
all of that but very important great
stuff has contributed and will continue
that second sort of pillar of this
approach is in all the omics right from
genomics to metabolomics it can't be
done without huge computational support
the volumes of data are tremendous the
what you can do based on reasoning from
a model of understanding the mechanism
is very poor because you really in that
space there so little really understood
about what's going on in the context of
genomics it all has to be discovered
through analysis also very important
pillar to this and what I want to talk
about though is this if you will a third
pillar has to be the same
hi otherwise not a pillar I won't not
going to argue about the relative births
of these things but at least another
component here is on these data about
the particular individual whose health
is being is being managed and so mobile
health the digital traces we generate
more broadly and how we can harness
these things that were previously
unmeasured about an individual's
function and behavior to fuel
personalized personalized medicine so to
ground this even more let's just take an
example so a patient with rheumatoid
arthritis maybe mid 40s early 50s tends
to stride get around that age more tends
to be more in women and the nature of
the of a rheumatoid arthritis is that it
has these it wanes in wax it's a it's
something that has flares they don't
understand the reason for the flares the
flavors aren't particularly predictable
and currently as you're trying to find a
treatment that works for an individual
to maintain her at her best function and
to have the least pain interference in
interference with her with her job and
personal life you're trying to balance
basically an optimization problem
between excessive use of medications
that have their own both short-term and
long-term side effects steroids primary
course of treatment and wanting to keep
her relatively keep the pain
interference low from the inherent
symptoms of the condition and also if
flares are entreated and they're allowed
to get worse they become much more
intense than if they're treated early so
and there isn't enough understood about
the mechanisms to be able to predict how
much does this individual need it's not
like it's not like blood serum
cholesterol it's on like cholesterol
where they understand sort of it pretty
well without that much variability how
much lipitor you need to take to
maintain healthy blood serum cholesterol
in context of rheumatoid arthritis
and crohn's and lupus and all of this
sort of family of inflammatory
autoimmune diseases it's much more
empirical than that and so the
individual is going back and forth with
their clinician if they have access to
that ready access to good care trying to
find this optimization to minimize their
number of flares to minimize their
intensity of them to keep down the
extent to which they have to use a
medication so why am I telling you this
whole story well right now the feedback
process in all this is an individual
coming into the doctor's office after
they've been putting on to a new
treatment plan and there's a
standardized instrument but the
instrument is a set of questions that
you get asked about how is your pain
level bin over the last two months or
six weeks or six months since your
treatment plan was adjusted and that
level of retrospective recall
particularly with respect to things that
have their own natural variation and
confounding factors is very difficult to
do accurately we have we have a feedback
loop with really poor a integrity
veracity of the information that's
coming back so there's a lot of work
going on to try to understand the
disease better the science of it and
there's work going on in just how in the
context of what isn't known about it do
you help to treat such patients but the
current situation is that most eighty
percent of all RA patients have flares
at unpredictable times no one is symptom
no one is symptom free than under under
treatment and this treatment has other
side effects you're optimizing with and
these newer biological agents that are
produced sold and used if you look at
the numbers it's really shows you that
not enough is known about it and it's
not enough just to say let's use the
best of the biologics because it's only
in fifteen percent of the cases that
they get seventy percent percent better
so whether it's rheumatoid arthritis or
crohn's or lupus or in some cases behave
neural interventions around obesity or
depression anxiety chronic pain all
sorts of chronic diseases where there
isn't yet a really good blood-based
biomarker for you to fill in and get
really good signal what I'll be talking
to you about is that this N equals me
this small data can help to fuel these
important feedback loops of health the
one I've been talking about more is to
the clinician how does the clinician
know in a more high with more high with
more temporal resolution and with more
accuracy than what they get just through
retrospective recall how this patient is
doing on this new course of treatment
the patient might be expecting and it's
a really dramatic improvement and to
them nothing has changed but these
medications take a long time and the
extent of improvement is is a subtle in
many cases so you don't want to ignore
the signal this is actually a good
course of treatment to continue with and
so that's a big piece of the feedback
loop there's also just the self-care
piece of the feedback loop so if you're
on pain medication it is given to you
with a prescription that says take as
needed and then the individual is faced
with their own optimization perhaps with
a family member of how much of this pain
medication to take for the most part
these are addictive pain medications
because they're actually developed for
acute pain and your body desensitizes to
them and so they naturally have this
addictive quality to them and there's a
huge problem in this country for sure
other places as well of increasing
overuse of opiate-based pain killers and
that whole question of how much do I
take in order to function versus be
worried becoming about becoming
addictive not to undertake not to under
over consume how do I get a better look
at what is optimal for me and then
finally one of the reasons why we don't
have more personalized pursuit of
medicine that's not really affordable
the amount of interaction you need with
a clinician now to maintain and really
get that level of information about how
you're doing is too labor-intensive and
there's really no evidence for it so in
health care we have this problem which
is that you can't do a doctor is not
supposed to be creative right it's an
odd thing to be in working with with
health professionals because you are not
supposed to be creative you're not
supposed to reason about and hey this
sounds like a good idea everything has
to be evidence base which makes sense
when you realize how little is actually
known about the mechanism that you're
treating so you can't afford to be
creative you can't reason from first
principles because you don't have enough
of the information about those first
principle so things in practice can't
happen until there's evidence and there
really hasn't been a scalable affordable
way of getting evidence about how people
treat themselves and behave outside of a
clinical setting so if you ask a
dermatologist whether diet influences
your your your child's teenage acne okay
whether there's something you can adjust
and diet instead of putting on my
accutane they will tell you correctly
that there is no evidence for that no
evidence for that doesn't mean no it
just means there's no evidence for that
if you ask them what's the time if if
there was going to be connection what's
the time constant what delay should I
look like should I look at it an hour is
it 12 hours is it a day don't know
because who would necessarily bother
doing that kind of a detailed research
study and it because in many contexts
they're not and I was just joking
earlier I've recently moved to New York
City from really a lifetime aside from
graduate school in in California and
perhaps in the rate of my speech you've
determined i have been asked my whole
life if I'm from New York ok so I'm
finally from New York and as I moved to
New York I have found my inner
Californian which comes to things around
health care so if you ask a doctor in
New York City from my limited sample
about something that has to do with well
you having sinus issues instead of
taking some steroid based nasal spray
should I try eliminating dairy they look
at you like with this roll of the eyes
and like you're from la la land beak and
there is not any real evidence about
these sets of things including all the
supplements that at least some
us by in whole foods or other places
completely irrationally perhaps with
knowledge of that it's irrational but we
have no real basis of understanding what
works at what and what doesn't at what
dose over what course of time yeah I
won't ask this because it's a it's ask
you to disclose a private information
but there's some number of people in
this room that take ginkgo Balboa or a
glucosamine or these sorts of things you
read about on the web that you go to
Whole Foods and you take with varies
consistency I don't expect you know how
long it takes for any of those things to
take effect for you to even know whether
it would take effect so the opportunity
here is not just about how we care for
ourselves giving feedback to the
clinicians but there's actually an
opportunity now to begin to learn some
of these things that weren't affordable
to learn and it's in nobody's particular
financial interest to do these studies
without it being really inexpensive to
do so there's this opportunity to change
where evidence comes from medical
evidence largely comes from randomized
control trials and it will continue in
those remain important probably some of
the biggest shifts about how these will
be done is that you'll need less of a
control group you can just take your
control group you know from the web from
the ether find that set of people
increasingly as we have electronic
health records and stuff on the web
you'll be able to find your control
group in the web to compare things too
but a randomized control trial you take
a medication you do that randomized
treatment to the randomize them to the
two treatments you find that one outcome
the purple a treatment gives
seventy-five percent better results
better results in seventy-five percent
of the people twenty-five percent did
better or just as well on the pale blue
one and as my co-founder of open em
health who is a real doctor that means
an MD likes to say when some mr. Jones
comes and sits in front of her in her
office she doesn't know if he falls into
the into the 75 or the 25 percent now if
it's chemo or surgery and all sorts of
really not reversible treatments that's
all the information she gets and she
just has to work with it but if somebody
has
chronic condition then they're going to
be on this medication for a long time
they're going to be on it until a better
medication comes along because chronic
conditions don't really go away one of
the reasons that health payers and
provide that health payers in particular
are starting to introduce really serious
programs about preventing diabetes is
because for the most part once you be go
from being pre-diabetic to diabetic it's
very hard to turn that physiology around
so and once you have diabetes you
largely have it for life and then you're
managing it for life so if she's about
to put somebody on a course of treatment
the opportunity now to find out what
works better for that individual put
them on one have a washout period put
them on the other do a bunch of this
passive measurement that I'm going to be
talking about and determine what's
better for mr. Jones and in doing that
be able to contribute to the evidence
base that might eventually lead to a
better understanding and predictive
understanding of what differentiates
that 75 from that twenty five percent so
in that same example of rheumatoid
arthritis as I gave that I gave before
it does have to do with the opportunity
to use this in the course of treatment
but also has to do with being able to
just use these technologies to help move
the evidence base in the science forward
so Bob Darnell who runs in New York
genome center is looking at a cohort
study looking at people with rheumatoid
arthritis and having them do blood spot
samples once a week so that you can
actually do the RNA transcription
analysis to try to see what's going on
in the course of the disease as
somebody's going into what they call a
prodrome before they go into a flare
because currently they only have blood
from people when they're in a flare or
after they've gone into a flare as
they're treating them with medication
and the just the logistics of all of
that being able to have access to people
through their mobile devices with just
the mechanics of reminding people having
them take a photo of the little bar code
that they put on the blood spot because
now you suddenly have a time stamp
associated with that with that blood
spot just this
mechanics but in addition we're
measuring passive activity of that
individual because what he really wants
to do is when he sees signs of that
maybe their function is starting to
decline and that he wants to get daily
blood pricks from them and if you're not
a type 1 diabetic it's very difficult to
get six months worth of daily blood
prince he wants to do adaptive sampling
and so data about how somebody's
function can also feed into supporting
this kind of science so in the large
it's about leveraging this is a sweet
spot although as I'll discuss it's not
just from the mobile but now for the
most part increasingly everyone is
walking around instrumented and so it is
both a continuous source of data about
me as an individual as ways a way of
interacting with me as an individual and
it's about gathering those data and
being and the algorithms to turn that
into some sort of actionable data for an
individual one way to think about this
goal in the context of health is that
really if probably most of the people in
this room or you or a loved one is
diagnosed with something you will go see
a specialist and you will be treated
through frequent interaction with that
specialist to do that optimization but
most people in the world in this country
for example most people would be treated
through interaction with the primary
care physician rheumatoid arthritis
depression anxiety at Crohn's disease a
chronic pain for the most part you're
just going to get to see a primary care
physician and in many contexts and then
certainly worldwide people are treated
in a clinic where it you're being
treated by primary care physicians but
by an array of them over time and
there's not some individual and so part
of the opportunity here is to bring some
of that precision of treatment that
method to primary care and clinic based
care and really across a range of a
range of conditions many of which I
mentioned so let me
down a little bit into the type of data
I'm actually talking about I would
expect most of you have used and worked
with and seeing these types of data
before so hello doesn't want to advance
for me anymore any clue
sorry play slideshow okay nope doesn't
like this slide okay I'm going to I'm
going to tell you this next slide but
i'm going to manually skip over it i
don't know what its problem is you see
it this way now you see nothing okay let
me keep going so one of these yay i
tricked it so one of the primary sources
of data that we've been using most
aggressively and actually a lot of this
a lot of this work started long before
we had smartphones even in the days of
PDAs people started doing work with
being able to capture a GPS time series
for looking at different patterns and
people have been using step counters and
accelerometers actigraphy data for quite
a while but now we all walk around with
the ability to capture activity
actigraphy that's continuous measurement
of your movement so at the minimum and
what's often most relevant here is just
sedentary and versus ambulatory but in
other contexts you also can get speed of
walking and and and other things from it
so it's accelerometry it's really
geolocation data only sometimes does it
come from GPS at this point we use all
kinds of other RF information to be able
to get geolocation data with lower power
consumption on the device and the
opportunity to have continuous 24 by 7
activity location time series about an
individual is really very rich if that's
all we had we could already do and
happen for a while do a fair amount of
this because think about that example of
the rheumatoid arthritis patient instead
of now coming in and saying well in
general I think I'm doing a little
better had a couple of harder days but I
think I'm doing better overall now it
would be some uh some representation of
what time she's leaves the house in the
morning every day over the course of the
six weeks or two months did she was last
seen and how many hours she spends out
of the house and how longer her
ambulatory
periods those are not bad indicators
they actually correspond to some of our
specific clinical questions that
somebody treating somebody with
rheumatoid arthritis asks because
morning stiffness is one of the symptoms
of the disease so getting out of the
house somebody's general level of pain
interference and disease interference is
often reflected in how much time they're
out and how much they're actually
physically moving around now at this
point don't think throughout what I'm
talking about I'm not talking about
doing anything diagnostic I'm not
talking about having a model of what an
RA patient looks like and then being
able to apply it to your pattern and say
you fit in here right now and I think
for some time the biggest power of this
is to say for an individual relative to
her baseline how is she responding to
treatment or is she going into a relapse
and to inform that optimization problem
for the individual ultimately will have
lots of this data and will get the the
priors and the models and the predictive
part of it much better but there's
tremendous richness and applicability in
even pursuing this a patient at a time
and I think that's one of the great
potentials here is that we don't have to
wait for mass penetration of the
technique in order to benefit from it
other data just comes from the mobile
apps the reason that people buy and
carry these phones with them that it is
the last thing they stop paying for as
the essay if they're having if they're
an economic hard times it's that one
point of intricate of connection to the
internet for people who are don't have
as much discretionary income or very
little at all and we use it because we
can run so much of the logistics of our
lives through it and it's by using it
yes it's with us so the activity
location is then something we implicitly
generate but the apps we use generate
analytics as you all here know and
there's information in that analytics so
there's information and when I play the
games I play
and there's information and how I do on
them so when you play words with friends
and the words you use with friends right
and the same for email and the work
that's been done on on things like
Facebook postings and sentiment analysis
on these on Twitter and Facebook these
more publicly available feeds but that
private communications we do the
opportunity to use analysis of how I use
it to produce data analytics for me is
what I'm talking about now there are
health apps themselves specific apps
that you can use to get information from
people that are beyond the standard
instruments the questionnaire like the
standard way of getting is information
on how somebody's feeling their mood or
their effect which is an important
indicator in many in many contexts is to
ask them the PQ nine a nine point
questionnaire about a bad effect that's
been measured and validated with however
that means I don't know what you
validate it with because there isn't
actually a physiological way of
understanding mood but it's been studied
and it's been validate as much as you
can here's an example of an app that JP
Pollack who this PhD several years ago
at Cornell Ithaca with Jerry gay
developed and tested and showed to be as
reliable as the standard instrument have
you ever need of you seen this so it's a
photographic it's called photographic
effect meter and if in the context of a
study or an app or whatever app usage
response you want to get some of these
information about their mood or effect
instead of asking you man I important
question this app pops up and they
simply tap on the image that reflects
their mood and the images are not the
same all the time so you don't always
choose the banana right you don't get it
it's randomized but he's taken photos
off of off of available photo site and
did very systematic studies to show how
did it compare to people understand
answering through a standard standard
questionnaire so some of these apps are
just things we use anyway the way we
tweet the way we use Google
apps within what we do when we're using
games our emails our calendar and our
and our communication patterns but some
of them might be explicit apps like this
another app I like to point to was put
out by the Veterans Administration for
returning soldiers who have PTSD and
it's an app that's a portable tool to
help individuals treat with heightened
treat it deal with their heightened
anxieties so panic attacks as we would
think about them as they come back to
civilian life and are exposed to things
that might remind them of their trauma
but actually don't represent a safety
concern so this app gives them a tool
that's in their pocket in between their
therapy sessions to help them do guided
relaxation breathing exercises prolonged
exposure exercises things of that sort
and we've done work with the VA so
that's not just a portable tool for that
for the veteran but that the data about
how they're using it and how they've
used it in the week between therapy
sessions begin to become data for their
clinician as well one of my favorite
startups in this space is ginger I oh
and they're a start-up out of MIT there
now largely california-based and ginger
i/o is really doing this they collect
passive data activity location and
communication usage and phone call and
they have their machine learning engine
that takes this back and is doing
analysis for an individual to compare
them to some of their models that
they've been doing but also as I said
compare that person to their baseline
and they are working with folks like
Kaiser Permanente to use this as a tool
for example to monitor type 2 diabetes
patients one of the one of the the
biggest issue in type 2 diabetes is
adherence to medication and to dietary
regimes regiment so that your diabetes
doesn't become worse if you don't adhere
to when you're diabetic there are severe
physiological consequences and they
become more and more severe and so
helping a diabetic actually adhere is
key again it's one of these things once
you have kidney
the damage you can't roll back from it
so what ginger IO is doing is this data
with the opt-in by the patient's
themselves is coming back to the
clinical care system to help the
clinical care system identify which of
these type 2 diabetics might be becoming
or exhibiting symptoms of depression
because they have found in studies
depression that works against adherence
intuitively not surprising and so
they've had very good results in helping
the clinical care system then focus in
on those individuals that maybe need
some more intervention earlier to help
them with their with their adherence and
ginger io has a number of other health
examples out there very interesting
company just another example in this
space is a colleague tempting chowdhury
and colleagues again from Ithaca
coincidentally I has a tool whereas
ginger IO is a tool for the clinical
care system mood rhythm where am I ok
mood rhythm is a tool for bipolar
patients themselves one of the very
important things in managing bipolar is
to try to maintain good diurnal
consistent rhythms sleep patterns work
patterns this has been associated with
helping to better manage the that both
the the episodes the frequency and the
intensity and so it's a tool that
through this passive monitoring instead
of requiring everything to be carefully
journaled by the individual helps that
individual see and and guide them
towards maintaining better rhythm so
I've been talking about data streams
that are largely just implicit passive
and software-based and just leveraging
this main device and I but of course
there are real sensor streams to write
those physiological things that can be
measured by an increasing set of
wearable medical devices and wearable
consumer devices and I don't mean to say
that these are unimportant but I don't
think that they're more important if you
look at how analytics has changed
businesses and the world and how we do a
ton of things it's all and so much
through secondary use
of digital traces that are left behind
through use of software based services
and there is that same value back for
the individual that's possible even
without and but certainly we don't have
to say one of the other in conjunction
with these particular physiological
things when you could measure them but
you don't have a device that measures
what's the state of your rheumatoid
arthritis there is no device that
measures what's the state of your
Crohn's disease there is no device that
measures what's the state of your pain
and so while these devices are important
in some ways they sometimes I think take
away from looking at the intelligence an
individual can get from all the other
data streams they generate and to take
that further it's really much more than
just about mobile so it's way beyond
mobile and as I said before it's way
beyond health so there's our household
cable boxes it's another source of data
about household household rhythms when I
am working with some folks at time
warner cable and one of the VPS there
who's been very supportive of starting a
small test bed in New York City to
explore the small data concept her
example which is a little not as easy to
achieve as it sounds she said I get it
I'll be able to convince my father in
law that his hearing really has declined
over the past year become show him the
extent to which he's been amping up the
volume control right relative to 12
months ago so no one big killer app and
really not any one data stream that
tells the whole story but we live our
lives so digitally that when we think
about how for an individual or for a
household we could bring these data
streams back data from the mobile
carrier a lot of what we do we captured
much too much information and then
feature extraction we throw most of it
away a lot of that location time series
for that rheumatoid arthritis patients
and certainly for older patients that
tend to have less smartphones at this
time you can get that information back
from data that the carrier has or could
have
I think one of the most interesting data
streams that will increasingly be able
to leverage is our financial
transactions all those swipes we do
during the course of the day and the
opportunity maybe for the first time to
have an estimate actual measurement
around dietary consumption whether
you're trying to look at gluten
consumption or dairy consumption and did
it really make a difference this winter
season that you had you know reduced
extent of your colds and congestion
because you actually eliminated how much
dairy you and your family were we're
consuming that's not something you see
overnight and it's a little bit of a
hard thing to just keep track of and yes
you can take pictures of food but that's
a limited duration and generality of use
social all the data that we can get from
our social and communication patterns
there's a talk there's a story I tell in
my in my tedmed talk which in part
started me thinking about all this in
retrospect which was from a couple of
years ago and my father who was in the
field of computer science but was 90
when all of this began to begin to occur
and he passed away a couple of years ago
and in those months of his decline which
wasn't preventable it was it was his
time I'm not trying to tell you a story
about if only we'd known what we could
have done in this case we just had no
clue what was going on and he went from
being this incredibly functional 90 year
old caretaker of my mother who's who's
not well to somebody who couldn't
function almost sort of overnight and we
no clue what was going on and his
cardiologist had no clue what was going
on didn't show up in his EKG and when I
took him to the emergency room the
attending there looked at me like I was
some you know neurotic daughter who
didn't wasn't willing to accept that her
father was 90 but the night because the
9th year old man presented to him as a
normal nine-year-old man but that nine
year old man was entirely different in
the way he was behaving than two weeks
ago and none of this shows up in the
electronic health record but when i
started thinking back I realize it wait
a minute he'd stopped responding to
email yeah now you might say Debra why
you
back a nine year old man to be
responding to email but he was among the
first people writing email okay i was on
the ARPANET from the age of seventeen
and nineteen seventy seven when I went
to Berkeley communicating with my
parents through email instead of having
to talk to him on the phone much easier
way to communicate with parents when
you're 17 and go away to college right
he was using email forever he was always
responding to our emails and his email
suddenly stopped it also turned out but
we didn't know it that his walks around
the neighborhood became shorter and
shorter and more unwarranted interment
and he was an avid Walker and we only
found out after the fact he'd stopped
cooking dinner at home it's generally he
would cook a meal at home but his
purchases at the supermarket became
lesser and lesser he wasn't even aware
of it he didn't mention it to his doctor
headon particularly mention it to us and
we had no idea what was going on and how
would this have helped because his care
team and the family and he could have
had some knowledge of what was happening
could have had some way of letting him
and us deal with that end of life in a
somewhat different way and I don't mean
to be melodramatic in the way that TED
talks try to get everyone to be
melodramatic and somewhat self-conscious
of the fact but really it is it was part
of what brought together to me how we
could use these kinds of passive traces
to help people not with one killer app
not with always about saving lives but
helping us with x and contexts where
where we really could use better
information and so lots of opportunities
here from really being able to take our
consumer transactions as small data and
turn them into something relevant like
are we seeing a reduction or a shift in
how much faster processed food is being
consumed if you want to take an
initiative in your community and try to
encourage in the schools to have
everyone start doing a competition to
actually cook at home once a week and
with more fresh components instead of
processed ones how we had you execute on
that well our small data can start to
feed into things like that
communication as language patterns not
just the timing of that somebody has
stopped communicating an email but the
language we use in our email looking at
the range of a vocabulary and sentence
structure is indication of cognitive
decline of cognitive fatigue a lot of
the complaints around chronic disease
and even some of the medications for
them is people just complain about
chronic fatigue excuse me about
cognitive fatigue how do you begin to
measure these things let alone things
like chronic fatigue and other disease
context in which we're working and in
all these cases it's no one data stream
as you guys all know you get data from
different sources sensor fusion in that
sense and it's about integration and
fusion and it's all about the
sense-making and it's sense-making
across these noisy time series and you
want to be able to do correlation
eventually you want to be able to
understand things around causality but
initially it's really how do we model
the individual by being able to run
algorithms across their time series to
look for their responses to feed back
into their care to feed back into what
we understand as I was saying before so
that's why even I started at doing this
work in the context of open em health my
early background is Internet Protocol
stuff and I grew up with the IETF and
and other structures in which you have
open modular infrastructure and that's
how you get amazing commercial
ecosystems sometimes when you work with
people in health I found myself for a
while talking to people at NIH and
Washington DC and as a joke I started
saying you know open source is not
socialism and the first time I said it
now he's just being flip I realized by
the look on the person's face that I was
actually telling them something and so
it's this whole in the context of health
IT and part of the reason why we have
such horrendous health IT in this
country is that it has not have merged
with any architecture or modularity or
openness in that commercial in that
commercial ecosystem and eventually this
will be tying this back into those
electronic health record systems getting
data from them figuring out what data
from these small data streams actually
are
anukul irrelevant so the end by
mentioning so the two key challenges
here to make this come true one is and
the main one is how do you make clinical
sense out of raw data or application
sense if you're applying it in the
context of something like quantified
student so folks I've talked to an
education when I showed them that
picture of the N equals me data serving
the patient the clinician and creating
evidence they say that's the same story
we have in education how do we help that
individual student optimize the
practices in the studying the modalities
to fit to their learning how do we
optimize what the teacher and the
teaching system knows and how do we can
we take advantage of the fact that
everyone's they're acting out their
learning digitally how can we start to
learn more about how people progress but
we have the raw data that isn't the
problem actually turning it into
something meaningful all the modeling
and analysis that you guys do is what is
what's critical and I tend to think
about these things as if you will sort
of behavioral bio markers we don't have
blood based biomarkers but through a
layered architecture can we do easy to
do the lower level state classification
starting to do that that summarization
and then for a particular condition
you're looking at different actual
metrics that are relevant so eighty
ninety percent of this is the same of
the underlying machinery of what you
need for chronic fatigue or lupus or RA
or crohn's or depression but when you
get down to that individual condition
you're going to there's going to be some
different trend and marker that you're
going to look for that specialized to
them and so we need modular tools to
begin to do this kind of identification
complicated slide but it's just
something you see in in in work that's
been done for years in the context of
whether it's adi odata voice recognition
image recognition and the community is
starting to have more of a modular
shared approach to this so we can all
get better faster and the second
challenge that I want to end with is
really about governance so our mobiles
happen to emerge thanks to the iphone
and android in which the control over
what you run
here in data you collect shifted from
the carrier it used to be that that ATT
verizon it said a determined what
software ran on your phone and now
because of app stores we decide what
runs on our phone we register for
accounts for RunKeeper or Fitbit or you
know I triage or PTSD coach and so we
actually have personal access to the
data that we generate through these by
and large but that is not as true with
respect to all the online things we do
or the financial things we do while you
can get access to your data in many
cases and increasingly I think google
just announced yesterday the ways in
which you can get access you'll be able
to do an export of your calendar and
gmail data there aren't standardized
open API is personal data API s where as
a user subscriber I can opt in to
programmatically get my data to come
back to some sort of a personal server
in the cloud where I can then run apps
and services over them and I believe
that's the model we need so I'm not
saying that everyone is a quantified
self ER okay that's in an East Coast
again I have to explain quantified self
for those narcissistic Californians that
like to measure everything about
themselves right and then they do there
python in our analysis and they go to
meet ups and all those things okay but
if that's not how it's going to play out
in large right the everyday consumer
downloads apps the everyday consumer
right a decade ago an app was microsoft
word and nobody no consumer on the
street felt that they had any connection
to causing an app that was just
specialized for them and their needs to
come about now you hear people for years
you people talking about the app that
they want to design that they want to
use so think about the consumer
downloading apps that going to help them
to optimize their commute their
consumption their new intervention to
try to help their diagnosis ADHD kid you
know does it make a difference if they
change what they give them for breakfast
and at mealtime yes so
but to make this happen the data is
already being captured it's not about
introducing new privacy violations
because i'm not talking about any new
data i'm talking about just letting an
individual get access to their data back
in a 8 via programmatic way and having
standard ways that we begin to design
apps that access those just like you
download apps that access your location
calendar and so forth and to the extent
it has to do with privacy or those
things a word which is just has so much
baggage to it that I don't really want
to use it it does introduce transparency
if you are using the data that is known
about you there's just about being an
educated consumer you can unmask you can
live in the world with having greater
understanding and not be so surprised by
this NSA stuff so as I said it's not
about health it's really beyond that
about life and we are doing a small data
test bed in New York City and look
forward to collaborations in other
places you want it where we can get a
density of this kind of behavior and
then just to end with a really bad pun I
said the other day in a meeting this
really isn't rocket science but you know
maybe it's pocket science
so we have time for some questions if
questions could approach one of our
program chairs who has a microphone so
Terry if you want to perhaps start and
in the meantime with the speaker for the
next session pass so fascinating talk
actually something you said at the end
made me think it is possible to get your
FBI records yeah if you ask for them I
wonder if we can get our NSA records if
we ask for them and you know that that
might be a source of personal data that
be very valuable for us agreed go to the
aggregator one more question for your
shirt um so I have a suggestion maybe
you've thought about it before but with
your phone there's something you could
do to get some of the randomization that
you normally do in medical studies in
order to infer causality between
decisions that people make and outcomes
oh sorry this guy um I just sim who I
mentioned at MD and PhD in
bioinformatics from UCSF and rich
Kravitz from UC Davis uh we do some work
together on exactly on that and so
there's a nih project called trial list
which takes it which lets you begin to
do that randomization in the treatments
that people what they take and when and
the potential is is all there that's
starting to happen i think it's a very
important component of it okay we have
time for one more question thank you for
the talk it was really fascinating I
wonder if you can comment about the word
study published in PLoS about a year ago
they serve it a lot of papers that tried
to use also the mobile intervention and
stuff like that and found only two out
of 80 had significant outcomes and also
we know that about the average time
between the time people buy a Fitbit
until they stopped using it is less than
a month so what is missing right by the
way did I mention Fitbit
okay I'm sorry um I had a picture of it
okay I'm trying i have this little thing
i'm doing i'm trying to see how many
days i can go without having the
conversation that includes Fitbit
nothing against pitt bit but it's just
been such a focus so fitbit already
delivered i think in a they made
progress in that they understood that
they wanted to represent data back to
the individual in some interesting way
the problem is is that data is like on a
web page and not as much in play which
if you just go back to the great work
that james land a and sonic salvo did at
university of washington and the Intel
lab on you be fit the work they did back
in the PDA days i'm just curious how
many of you have read this seen you be
fit so they as you as you exercised it
grew a garden in the ambient display of
your PDA and then at the end of the week
it went clean left behind some
butterflies if you've met your goal and
then you'd grow the garden again and
aside from this reaction which is like I
don't want flowers growing on my phone
you know okay so think about it as a
fish aquarium or something like that but
the way in which you tie what you do two
more ambient displays and doing this in
a more modern way what's been tested so
far are like individual measurements and
without the multiple data streams and
without the interaction with the
individual at the same time anything
that is about obesity that addresses
only activity i think is doomed to fail
we have to address the dietary component
of it and we have no way of measuring
dietary intake in any way most people
don't he'll have a whole hell of a lot
of choice processed food is cheap and
addictive and so there are system at
there are systemic problems on the side
of this issue around obesity and our
food environment that no Fitbit or in
different combinations of those data
streams alone are going to address which
is part of the reason why i work more in
the context of some of these chronic
diseases and still trying to
figure out how we're going to solve some
of these more fundamental prevention
problems that are much more market
economics and and food environment than
they are any kind of self-monitoring
okay if we can just thank deborah
another time for a very inspiring talk
okay so the next talk is a scalable
influence estimation in continuous-time
diffusion networks this is a winner of
an outstanding paper award so if all the
authors could come up quickly I have
money in envelopes so that I can hand
those out and if in the meantime the
speaker Nandu to set up you have any
other authors just two of the authors
here should be another one oh man well
so Nandu Manuel Gomez Rodriguez hung
user later okay someone's someone's
taking the money for hung you good
hello ok so cool okay
yeah this is a reminder for speakers
they're supposed to come before the
session and check this 10 minutes before
your session for speaking to make sure
you come and check your projector works
ok nan do with scalable influence
estimation in continuous time diffusion
network ok I'm very glad to present our
joint work about the scalable influence
estimation in continuous time diffusion
networks so now we are surrounded by
social and information networks over
which diffusions of information events
barrows take place constantly in
people's daily life so intuitively when
some influential users has have been
adopting certain new product they will
actively influence the behaviors of
their friends and the which will further
invoke a large number of follow-ups for
adopting this new product through word
of mouth so one particular question we
think to answer is how can we predict
how many users will be influenced by
such influential users and then how can
we identify such influential users to
trigger the largest follow-ups in the
future so both the influence estimation
and the maximization problem are
interesting because adult hiders won't
efficient under effective campaign for
the new product but they are also very
challenging in that we are actually
sensitive to time so clearly thing about
a thousand buyers per day can be much
better than say a thousand buyers per
year right and we want our solution to
be scalable to deal with large
real-world networks in practice so in my
following slides on I will first
introduce our proposed continuous-time
diffusion model which can realistically
approximate the underlying true but
unknown diffusion mechanism and then I
will present an efficient algorithm for
both the influence estimation and the
maximal
asian problem and in the end we are
going to evaluate the performance of
sorry the performance of our algorithm
by using both synthetic and a real
diffusion data so traditionally on the
diffusion process has been modeled as
discrete realms and no they can be
infected round barrel but in reality we
know that the propagation can actually
not diffuse iran's right but to to model
the timing requirement people have to
partition artificially the continuous
real time line into discrete runs but to
be honest it is really not trivial to
decide how long each run should be and
how many rounds do we really need so to
better model the diffusion timing
information we propose alternatively the
continuous-time diffusion model where we
explicitly model the mutually
independent transmission time associated
with each edge so specifically for each
other from not J to I we associate a
transmission probability density
function over the transmission time
which is also called the transmission
function to describe how likely that
note I will be infected item TI given
notice sorry because I cannot see the
slide on my computer given given Dolce
has already been infected at mt j so
from this new perspective we can see
that we actually have a network with
random address and if we fix a
particular set of sampled transmission
times over the address the actual
infection time for specific node simpler
equals to the shortest path from the
source and then the inverse value of a
given set of source node
naturally be defined as the expected
number of infected node before time T
which further includes to the summation
of individual in function probability
that a particular node I can be infected
before time T and took to further
calculate that infection probability we
can simply marginalize the drones
likelihood overall the observed
infection times but you can see they are
using graphical model inference but as
you can see this into this integration
is actually challenging in that we have
to consider a possible infection state
for each node before time T and if our
general transmission function over the
address when normally have no clothes on
a solution so to overcome the
difficulties of the inference problem we
further transmit our original problem
into a graph theory problem where we do
not want to calculate the infection
probability individually but rather when
we have a given particular set of
transmission times over the address we
only we actually only care about the the
whole song of this indicator function
which further equals to the size of the
neighborhood given a particular source
node where the neighborhood includes all
the possible note that can be reached
within distance t and then from these
new observations we can simply first
draw a set of many sets of independent
transmitted times over the address and
then we can approximate that expectation
by taking the average across all
different samples and to calculate the
single summation here over a particular
sample network it is really trivial to
check whether or not up for a particular
node I whether or not the lens of the
shortest paths on this particular sample
that
work is less or equal to my distance
team thank you and then but we can we
can see that only using this simple
shortest path algorithm can now be
scalable because too if to estimate the
influence of a single source node we
have to run the algorithm one time and
then to decide which node is our best
source node we have to run the algorithm
for all the nose which immediately will
result in the quadratic time complexity
with respect to the number of nodes and
this cannot be scalable when dealing
with large real networks so instead of
calculating this individual shortest
paths for each node we want directly
estimate the neighborhood size by using
covalency algorithm developed from the
field of theoretical computer science
the basic idea is that given a
particular set of sampled transmission
times over the edges will further drove
em independent random labels associated
with each single node according to a
union rate exponential distribution and
then surprisingly by using covance
algorithm if we treat each single node
as a possible cells this algorithm can
simultaneously estimate or find all the
smallest random label within distance t4
out for all these possible single cells
not altogether and then we can simply
estimate the neighborhood size using a
very simple unbiased estimator based on
the extracted minimum label using the
property of exponential distribution and
if we have multiple sources we can see
that the Union neighborhood of each
single cells has a property that the
minimum label of this Union liberal host
Union neighborhood actually equals to
the minimum label extracted for
on each single cells and then we just
treat the minimum of other list labels
from each single cells as the least
label for this multiple south side and
use that to estimate the possible
neighborhood size for this multiple
cells node and so the over the overall
algorithm actually includes two layers
of randomization first we can draw many
types of independent transmission times
over the address and the based on a
particular set of transmission times
over the edges with further drove and
independent random labels associated
with each node and we can convince the
algorithm to find the minimum label
within distance tea or sociated with
each source node or together and then we
use and then we can simply estimate the
true influence function by taking the
average across all these two sets of
independent samples and we can prove
that if we drove large enough number of
sets of independent transmission times
based on each of which we drove I'm
sense of independent random labels with
high probability our algorithm can
produce a prediction with the arrow at
most epsilon this further implies that
when we have a much longer time window
the estimation problem will become a
little bit difficult because we need to
draw more samples however in practice we
have discovered that if we have large
number of sets of independent
transmission times this will actually
enable us to use very few random labels
to still achieve very good performance
so then once we have this efficient
influence estimation algorithm we can
directly plug it to solve the influence
maximization problem which is np-hard in
general but the cool thing is that our
objective function is non-negative
tonics and modular function and the a
greedy algorithm can achieve at least
thirty three percent of the optimum
value using the exact inference function
and also because the estimated influence
value given by our algorithm Yuri
introduces some estimation error but we
can still show that even with this small
estimation error the greedy algorithm
can still have a performance guarantee
only with respect to a small additive
errors here and so for experiments we'll
use both synthetic and real diffusion
data to evaluate our algorithm in terms
of the accuracy of the selected the
accuracy of the estimated influence the
solution quality of the slightly
resources as well as the scalability so
first on a synthetic data site where we
know the true continuous-time diffusion
model will compare our estimated
influence denoted by the blue dot with
the exact influence given by the exact
influence algorithm and we can see that
across different networks of different
structures the estimated influence is
pretty close to the exact value and the
performance is independent of the
network structure and will apply to the
influence maximization problem we see
that are on different networks of
different structures our algorithm can
always at least perform at least or
performs the other competitors by at
least twenty percent and the performance
is also independent of the network
structure and then to evaluate the
scalability figure a and B shows that
the proposed algorithm is actually much
faster in several magnitude when
compared with the simple simulation
algorithm and the exact influence
algorithm and the figure c verifies the
case that if the includes the number of
nodes to millions of knows the algorithm
is still able to scale up
and finally for the real to decide we
first learn the continuous temp
diffusion model using eighty percent of
all the Cascades based on which we
selected the number of sources that can
maximize the future influence and then
what we verify whether this is true in
the testing case by using twenty percent
for testing and the you figure a shows
that the estimated influence produced by
Algar our algorithm is closed to the
true influence extracted from the
testing data and the figure p NS e
further verifies our intuition that the
selected sources can indeed trigger the
largest true follow-ups within very
short time period under testing data
right and so to sum up we propose a very
efficient randomized algorithm that is
able to scale up to millions of nodes
and real network sites and which
improved performance in terms of the
accuracy of the estimated influence and
the solution qaulity of these selective
sources in real data and for the future
work we would like to apply our in our
influence estimation algorithm to other
interesting questions such as the social
welfare maximization problem the user
engagement maximization problem and to
develop more general continuous-time
diffusion model yeah and welcome to our
poster in this evening and the code will
be released Christ one maybe recently
okay thanks okay
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>