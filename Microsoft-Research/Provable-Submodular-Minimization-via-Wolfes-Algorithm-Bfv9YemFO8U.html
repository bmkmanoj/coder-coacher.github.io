<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Provable Submodular Minimization via Wolfe’s Algorithm | Coder Coacher - Coaching Coders</title><meta content="Provable Submodular Minimization via Wolfe’s Algorithm - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Provable Submodular Minimization via Wolfe’s Algorithm</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Bfv9YemFO8U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so this is a some work that I'm
going to some work with pratik we all
know and privation to our intern in
summer 2013 so please ask questions
whenever something is unclear so this
talk is going to be about two
fundamental entities so submodular
functions which are some sort of
discrete analogue of both concave and
convex functions and I will tell you
more about them and in particular I will
be interested in someone function
minimization and projection algorithms
so given a body in some n dimensional
space and a point outside it I want to
find an algorithm which finds the
nearest point or some other point on the
body to that following point so this
this is a very fundamental primitive and
optimization especially constraint
optimization so this talk is going to be
about the marriage of these two things
so so what are submodular functions so
they are set function so given a
universe of some n elements it
prescribes a value two subsets of this
universe and I'm going to assume that
these values are integers not
necessarily positive integer assisting
teachers and what makes a set function
submodular is this decreasing marginal
property which says the following I can
take a subset and an element not in the
subset so the increase in the function
value and I add that element goes down
as a set of the subset gross so it's a
mouthful so the way I like it best is
how my seventh-grade economics teacher
taught me if you have no mangoes then a
single mango is really good I really
want it if I have two mangoes then the
third mango is well not that great if I
have five mangoes already the 6 manga
will give me diarrhea so I really don't
want it so that's what's up modularity
is its as you have more the marginal
utility for next item goes down so an
illustrative thing to keep in mind is
the following submodular function so
given a set s the value is just the a
convex function on the size of the set
so that the this let's see so this
marginal utility just corresponds to
decreasing that
okay so that's an abstract definition
but I want to say that sub model
functions are everywhere and I would bet
that you have you guys have any every
one of you have seen one of it so for
instance if I take a graph and my
University set of vertices and the
function is given a subset of vertices
look at the number of edges crossing it
that function is submodular if
university set of edges and I ask well
what's the largest number of edges that
form an acyclic component that's the
molar function but they come up in
various guises so for instance if my
universe is a collection of random
variables and given a subset I ask the
function values the entropy of these
random variables that's a submodel
function if I have a matrix which is
positive semi definite and the universe
other so it's an n-by-n matrix and the
rows are the universe and given a subset
s I look at the principal sub matrix
take a determinant take its log that's
the motor function so they appear in
many guises and an exercise for you guys
would be in your research figure out
once I wanted a function that you have
come across the function is the number
of edges crossing the cut so in this
thing there are I guess five edges
crossing is blue set so that's a
submodel function as you have more
vertices the extra cut value increased
by a vertex outside goes down so since
it appears in many forms this sub motor
function has had applications in
economics operations research algorithms
and more recently there's a lot of work
on sub modernity and machine learning
because they come up in clustering
variable selection experimental design
and so on and so forth so it's a very
important primitive and and
understanding them is is it like a key
thing to do yes yes
now so the property is the following so
I take a subset of our disease and I
look at our vertex outside it so when I
add that vertex I look at the difference
in the cout function that difference
should be go down as I have more and
more vertices so I'm sorry yes super
sense so here's the way to think about
it so when I add a new vertex the cut
can increase by how much well at most
the neighbors of these vertices right so
when I add say this vertex these are the
edges that it can increase by but some
of those edges might already be there in
the cut exactly so so but then if i take
a smaller set thing will be still be
outside a subset so let me let me just
give an example right so look at if i
add this vertex to this blue set these
edges are already in the cut so the
increase in the cut value will be these
two edges correct so that will be
whatever that million is yes we will go
up a million no no but so the this this
difference grows when s so if T is a
subset of s this is this is a larger
difference it's think of it as a
marginal increase when I take a new
vertex how much does the cout function
increase by that marginal increase is
going to be bigger for smaller sets if I
had nothing then I will have a million
extra edges if had everything then maybe
all my edges are already covered
okay maybe we can talk offline but this
is this is not a deep it's just I'm not
evil 2 i'm not able to explain it
properly you're adding one element s the
bigger super steps is a smaller
difference oh you're right yeah so
you're not comparing the number of it
just before and after the addition exact
you are compelling there's before the
number of pitches after adding a small
asset to the darkest yeah please respond
miss Paula said address on the larger
side and in what ways you have is a
petition when you quickly why am I can
one may think about it certain number of
people you so we take the number of
people outside with me one of us write
that number
okay sorry about that uh okay uh okay so
so what's what's the problem that I'm
going to look at I'm going to look at
this the following optimization problem
so I have a this function given to me as
an Oracle so I take a subset and it
tells me the value so there are 2 to the
n possibilities and I want to find the
set which has the minimum function value
and the amazing thing is that this can
actually be solved Paulino in polynomial
time polynomial in n also there are 2 to
the N different subsets I can do this in
polynomial time and the property that
helps is obviously some modularity and
this has been an object of study for
decades and I would like to point out so
it was started by jacket months in 70s
but the first polynomial time algorithm
was given an 81 and in the it was
basically an algorithm which showed that
the problem is in py using something
called ellipsoid method does not you
cannot say it is a 22 good algorithm and
the people who are trying to hunt after
this of a real algorithm is they can
code up you know which gives you the
minimum and in 85 cunningham gave an
algorithm in the true sense of the word
but it ran in pseudo polynomial time so
what does pseudo-polynomial mean so it's
polynomial in n the number of elements
and the value of the sub model function
so if capital F is suppose the largest
values of model function then it runs
polynomial in capital f note that a true
polynomial algorithm would run
polynomial in log F which is a size
required to represent F but this was
pseudo polynomial and then later on
around 15 years later this was made a
polynomial time algorithm and this was a
big deal so these people won the
Fulkerson award for this and then it was
made strongly polynomial which means the
functions value was not dependent and
this is a current record so in 2006
alden gave an algorithm which runs in
time n to the 6 plus n to the 5 e 0 when
e 0 is the evaluation or act
time the time that you require the
Oracle requires to you know return the
function value so you can think of you
as one for the stock so it's n to the
six which is rather large and what I'm
going to talk about in this were today
is actually some work that was done in
1984 so it's a it's a heuristic it was a
heuristic by fuji chigan wolf and this
heuristic as I will show later actually
works well in practice and in some sense
it beats this bound but it was not known
how good it is in fact the best upper
bound on this algorithm was known was 2
to the N square so really bad so
basically what we did is show that well
it's not that high we showed that this
heuristic gives also runs in pseudo
polynomial time it's not polynomial yet
with a pseudo-polynomial and sort of
this is the bound so there's an extra f
square where f is the maximum value of
well a single turn oven on the function
so that's what I'm going to talk about
next so any any questions till now there
is a real for the this this heuristic or
in general
so what is known is there so what is
long so there there are experimental so
people have considered a segment of
functions and then experimental they
plotted graphs and said they profit
probably takes somewhere between n to
the cube to n to the four-time but it's
just by graph plotting so there is no
proof that it takes at least that much
time I don't know of any lower bounds so
I will describe the algorithm Al's is a
challenging algorithm sort of analyze
any other questions so one question that
he should be asking is obviously there
was not a four-legged animal proving
these things but this is why Philip
Wolfe and it's extremely hard to find
his photograph Robbie do you know Philip
Wolfe is you had IBM for 30 years so I
so as you can see you I know spend most
of the time the slides trying to cull
pictures of people and Google in return
me Wolf's picture but Facebook did he's
on facebook and I think it's him you
know he was in IBM and I you know
friended him but I have not heard back
he's 87 years old so we'll see ok ok so
let's let's get into the unit brass
tacks so so one object which is going to
sort of be it is key for why this
problem Evans run in polynomial time is
this geometric thing called the base
polytope so what is it so it's a it lies
in n dimensions so there is a so x1 to
xn such that it satisfies lots of linear
inequalities so it says for any set s of
the elements the sum of excise on that
set should we ignore the value of the
sub model function so for any set I get
a hyper so sort of a half space and it
and this positive is the intersection of
this half spaces and there's one extra
equality constraints or one hyperplane
which says that the sum of all these
numbers should be equal to function
value at the whole set so some geometric
object with 2 to the n constraints it's
really ugly looking so there's a picture
by you to nick harvey
in three dimensions I guess so it's
really ugly so why is it nice because it
has this fascinating property that if
you want to actually minimize or
maximize a linear function over this 2
to the N faceted polytope then you can
do that in almost linear time so for
instance if I have c1 c2 CN as a vector
I want to minimize summation Cixi then
the algorithm is just this sort them set
x1 to function value at 1 X 2 to this
thing and so on and so forth this is not
not important what these are but this is
this is the optimal value so transcend
just sort time plus I have to do this
these many function calls so really fast
so if u is 1 it's like linear time so I
can optimize over this polytope in
linear time okay how does this deal with
submodel function minimization why is
this positive important for minimizing
the sub model function I can minimize a
linear function over this weird polytope
how do I find the minimum value of F
well here's the sort of a sort of
remarkable theorem so this is due to
fuji sheegay he said the following so
this is my base pollito it lies in some
dimension I'm going to draw in two
dimension always this is the origin and
I look at the point on the base polytope
which is the minimum euclidean norm so
this is the minimum point X star so it's
at some point it has some negative entry
some positive entries you just look at
the sort of the coordinates which have
the negative entry so that's a set of
elements that is a minimizer of the
salon phone so it come it converts a
completely combinatorial problem into a
problem in geometry now I have to find
the minimum norm point in this rather
weird polytope but once i find it i just
read out the set so so in some sense
this boils sort of reduces submodular
function minimization to finding a
minimum norm point on this on a polytope
projecting the origin to the polytope
and i'm going to change gears now and
talk about wolf's algorithm
to find the minimum norm pointy are you
have any do you have any intuition why
the theorem is true yeah I have a lot of
intuition but I'm not gonna give it now
okay but yeah I have a lot of inclusion
so we prove something stronger than we
had to generalize this will come to that
but I'm not going to talk about that
sure yes because sorry FF if F is
non-negative right then zero may not be
in the polytope but it's all lies in the
positive orphaned so they so yeah but
even if it is not in the polytope the
whole thing lies in the positive or 10th
so the minimum normal have all
coordinates positive so I not pick
anything so is empty side but which
makes sense but I not should give much
intuition all points satisfy that not
just the minimum nonpoint okay so i'm
going to soak it so now i'm going to
change gears not talk about submodel
firmness for a while and talk about the
following problem given a polytope i
want to find the minimum point so it's
minimizing a quadratic a function or
polytope and it's not an easy problem
although it comes up in your doing
projected conditioner whatever
constraint where you understand you have
to project onto onto your convex set but
it's actually quite a non-trivial
problem so what is wills algorithm
building on its basically using the fact
that suppose I could solve linear
optimization on a positive very fast
which is true in the base polytope I can
do it in almost linear time so I'm going
to use that as a primitive to solve this
projection problem okay so first some
intuition so what do you have to do we
have this polytope this is the origin
and I want to find the minimum norm
point in so convex hull of these
vertices that's a hard problem so going
to look at an easier problem so this
problem is easy suppose I give you a set
of points and if this is in n dimensions
this number of points is say less then
n and I look at the FI in hull of these
points so what is in a fine hell it's
sort of a translated subspace minimum
transit subspace which contains all
these points so if there are two points
it's the line containing them if they're
three points the plane continuing on and
so on so finding the minimum long point
in the f fine space is actually easy
easy in the sense that it's it can be
done by matrix inversion and now here's
the observation really simple
observation that if i look at the
minimum norm point X star then this X
star actually lies in the convex hull of
some points in this case just to that's
that this X star is not only the minimum
nor point in this convex hull it's the
minimum nonpoint in the whole f-fine hub
so in this sort of integer we
intuitively clear in this
two-dimensional picture this point this
line is perpendicular to this line so
it's the minimum nor point in whole of
this line but this this holds for any
any dimension so that's going to be
that's the intuition so motivated by
this let me let me give a definition by
wolf it's so it defines something called
a yeah sorry so any point lies in the
convex hull of some point sorry x star
lies in the convex hull of at most n
plus 1 points in fact if its minimum
normal it will line at most n points
every point lies in the convex hull of
some vertices right it's in the X star
is in the polytope yeah x star is the
minimum nonpoint in the polytope and it
lies in the convex hull of some f some
points not all not all points but some
points so that it's not only the
minimizing the convex hull it's a
minimal in their final no no so i can
always express X star I can always find
a set s so that X star is in the convex
hull of s and it is the minimizer in the
affine hull there exists a set s of
these vertices yes I find Hal of s so
for instance ok it's in 2d it's only a
convex combination of these points but
if I have a point in the middle I can
express a confirmation of many points
but it will not be the final minimizer
of all these points but there exists a
set s so that it is also a minima in the
a final and I find how minimization is
easy so wolf defines this concept called
a coral he says a set of points s is a
call if you look at the affine hull look
at the minimum norm point in there find
her that actually lies in the convex
hull of this set is so it lies inside
the simplex form by the setters that's
called a coral and what is he trying to
find he so the algorithm will try to
find these corals because on corals when
I do a find minimization I know it's in
the convex hull and we'll hope that I
find a coral with separate so the affine
hull or south of the hyperplane
separating that and the origin separates
the positive one other side so so in
pictures ok so in pictures it's trying
to find this affine hull which separates
the origin and the positive that will be
proof that it is the minimum nor point
ok so that's this is what wolf is trying
to find the coral you're ever you think
disturbed yeah it so it is a facet but
it is also the perpendicular it's also
the affine hull exercise minimize of the
fine hull and it's
correct yes and it also is the minimum
nonpoint in the hole I find hell of that
facility yes so so that's what he calls
yes so why he called does not call it a
facet because he's not going to work
over facets he's going to work over
general set of points for which only
property he needs as the affine
minimizer lies in the convex hull okay
so let me sort of give a cartoon of this
algorithm so here's the polytope if you
remember this is the minimum nor point
somewhere here so how does he start he
starts off with a general point a he
puts it in the set s so this is a
trivial coral because you know what is
that fine hull of a it's a it's
convection of a is a so that's only one
point so this is the minimizer so it's a
coral so it checks if the hyper plane
that is perpendicular to a separates the
polytope from the origin it doesn't so
it moves to it fights tries to find a
point here try it's right next thing we
will try to find a point here so how
will it find that well it'll just
minimize a transpose x over this pollito
to find this point B that's one one way
of finding it okay so when it finds this
be so this is the linear optimization he
solves here then he adds b-2s so this is
the new green set is s and it checks if
this new set s is a coral so what do i
need to check i need to look at their
final which is this line look at the
minimum norm point does it lie in the
convex sir in this case it does if it
does this is my new X so this one
original like this is my new X and I
repeat so what is repeat mean I check if
this separates origin from the body
doesn't i optimize I find the point this
time it is C which is the linear
optimization with X so this is the new
Green Point and I add c2s so at this
point of time I need to check if this
three points form a column so
so this is the convex hull this triangle
and in 2d what is the affine hell of
three points it's the whole plane so the
minimum norm point is actually the
origin over the whole plane this is the
minimum norm point so in this case it's
not a coral because the minimum norm
point the origin lies outside this
triangle in this example so it has to do
something non into a non-trivial when s
is not a quorum so this is why this is
what he does he says this is my original
point this is my new F min the minimizer
I join these line and see where it
intersects the convex hull so it
intersects here and that is my going to
be my new point so little bizarre but
that's what he does why what does it
gain by doing that since this point lies
in the boundary of this hull out of
these three points I can remove one
point one point is not necessary to
describe this point anymore so in
particular in this case a will vanish so
it moves a point a from the set s so
when so so when I don't get when s is
not a coral I'm decreasing size of s
when s is a coral I'm going to increase
the size of s and then I repeat so what
does it reach mean I now this is my new
s I check that fine minimizer in this
case is going to be in the convex hull I
move the point and now it's a coral so I
do the linear optimization so in two
more steps basically i'll be at the
minimum hall so so this is just the
pseudocode basically has two loops so
one is called the major loop when it
finds the coral and as a linear
optimization and that's that's this
cycle and then there is a minor loop
when it's not a coral it tries to do
something which sounds a little weird
till s becomes a quorum so in the major
cycle the size of the coral increases in
a minor cycle the side of the coral
decreases so if you think about it if
you look at the the size of the corner
of the potential that's sort of
oscillating that's sort of the
problematic reason why this little hard
to analyze
how much time does it take this
algorithm well in each iteration which
is either a major cycle or a minor cycle
this is an expensive state to find the
minimum norm point in the fine house
it's polynomial time but it requires
matrix inversion so let's think of this
as n cube and then it has to solve this
linear optimization problem every time
but in the base polytope case that's
linear time but these are the two
expensive steps in each iteration so now
really what you want to understand is
how many iterations does it take to go
from here to the minimum point yeah so I
am at a point X and I'm moving to the
point y which is not in the convex hull
okay so sorry the question was how do we
decide which points to remove yes in
general so let me just in picture is a
so so you are at a point which is in the
convex self and you're going to a Polish
is not in the convex hull as you move
one of the coordinates required to
describe will become zero so why has
something which is negative because it's
not in the convex hull X is all positive
when I'm moving one of the coordinate
will it become zero those are the things
i will remove because does that make
sense x is described is a convex
combination why is also a linear affine
combination and when I'm moving yes
exactly exactly actually I don't break
Tyler I remove all of them that becomes
0 more I remove the better but because
more progress in hville ok so those
details i'm not i'm always maintaining
convex combination and that so in if you
think about it that's determined no no
arbitrariness there ok so as I said the
number of iterations is key so how many
iterations is it
take so what is not too hard is that the
norm of this point X as it is moving
here strictly decreases okay it's not
too hard to show that so but what does
that mean it means that the same coral
can never repeat again accordingly the
set of points so that the affine
minimizer lies in the convex hull why
because when I find a coral the point is
the affine minimizer and since the norm
is strictly decreasing the same coral
cannot repeat how many corals are there
well there are some V number of vertices
there are n we choose any ways of
choosing it so that's V to the N the
number of iterations at moes the number
of corals which is gives you this n V to
the N where V is the number of vertices
of topology so V is if V is like number
of vertices could be exponentially nan
so this gives like 2 to the M square so
wolf this this analysis due to wolf in
76 and here's what he says he says it's
a calculable number but the result is
preposterous lee large ass with linear
programming we must resort to
experiments to determine whether the
method is practical and then he does he
basically takes some random polytopes i
am putting another quotes because what
is an impala do so but what he sees is
that if n n is the dimension and m is
the number of points the vertices and
it's it grows really slowly for a random
volatile and he has a bunch of these
experiments and he says it's a very
interesting paper by the way he says
that it should be logarithmic in the
number of vertices rather than
exponential thats his gut feeling he
says something like that ok that's for a
random pollito this is way back in 76
and then I think sort of died down as a
known I don't know if many citations of
this till there's a paper written by
fuji she gained I suit on in 2009 who
basically revived thinking about whether
this Fuji shake a wolf which is just
applying min dump on the on the base
polytope can be a practical algorithm
and they did an experimental paper and
this is a log-log plot this is the food
is she gay wolf algorithm and these are
all the provable algorithms you know
that run in polynomial time so this
experiment suggests again again this is
on some
small data but it suggests that maybe
you can you know you can actually prove
that it's polynomial time so what so
again coming so what do we prove we
prove that for any polytope in not too
many iterations and Q square by epsilon
square iterations I get to a point whose
norm is within epsilon additive epsilon
of the of the min norm and Q is actually
the maximum point in the condo this can
be rather large it's a little irritating
I think this can be replaced or the
diameter but I don't really know how to
do it but that's what we proved about
wolves are there for any polytope so
basically if you think of Q as some sort
of a not too large constant then in
linear number of iterations I get
epsilon square close in min norm okay
but how does it help in submodel
function minimization because Fuji
shakers theorem which said that if I go
to the mid norm and read out the
negative entries that seem like a very
delicate quantity but it's not so there
is a robust version of food ashika so we
have a proof but this has been sort of
noticed by other people that in fact if
you have a point in the base polytope
which is not minimum norm but it
satisfies circles of this technical
constraint for all Q then I can find a
set s which is very close to the minimum
so f of S is at actually at most the
minimum plus some to an epsilon so it's
sort of a robust version i just i don't
need exactly the minimum norm if i'm
approximately minimum norm I can find
approximate minimizer's and now if I
have an integer value then i can set
epsilon to be anything less than 1 by 2
and then this will be the minimizer so
you can do that and then you get
basically combine these two and get this
pseudo for momentum analysis okay so I'm
not going to talk about robust Fuji
sheegay but I am going to spend some
time on this analysis
so any questions here yes so q so the
max normal in the in the in the
sub-module polytope is is is NF square
okay a sketch so why why why does this
algorithm work okay so so there are
major cycles which increases the size of
coral minor cycles which decreases as of
coral can go all over the place but
let's assume so let's take a major cycle
where there are no minor cycles so what
does it mean I mean this was a coral and
then the next step again I get a coral
so I don't do this minor cycle again so
in that case one can actually argue that
the norm significantly decreases so
basically this was my old point this is
my new point and that an XO lex you and
q old which is the sort of the linear
optimizer they lie on the same affine
space so you can do some sort of
Pythagorean theorem and get that the
norm decreases by something which is
large so this is just the dot product so
this is basically this is just saying
that this square minus the square is
just the dot product is not something
deep except forcing equality now why is
this large well if this were small then
I will terminate and go home because i
found a point which is already epsilon
close so this is at least epsilon square
so this this is my stopping condition so
i have not stopped so this quantity is
large so i am making some progress so if
i had no minor cycles ever then I will
you know in the reciprocal of this I
will be done but of course that's stupid
because if I never have minor cycles
then in n iterations I will have n
points or n plus 1 internations of n
plus 1 points in s and then that fine
minimizer is 0 and unless 0 is in the
positive i will do a minor cycle so
minor cycles are the challenge so why is
it a challenge so here is a minor cycle
so this was a triangle that was my s the
may find minimizer was why and my ex new
was on this line the problem is that
the previous slide showed that Y is
normal in skate 0 is very smaller than
the X holds norm but x new can be very
close to ex-old and you don't get
anything okay so then you can say well
you know just do your minor cycles there
will be not too many of them and next
time you get a coral you try to argue
that you get a big drop again there that
is a problem because you run say 17
minor cycles then you at some point X
car current and ex new so you get
something which is in the which is like
in the previous slide but this
expression I cannot against say it's big
because this Q was a linear optimism
some other point it could be 0 for all I
know so the challenge in wolf's
algorithm I think the reason why it was
still unanalyzed was this minor cycles
cause a big headache so how do we get
away get about it it's it's a little of
shake it's not a very I don't think it's
a the final word yet so what we do is
this so we know that if in the major
cycle there are no minor cycles I have
good drop to show basically that if
there is at most one minor cycle also i
have big draw why is that so what is
what does that even mean so I do as in
the picture I go to the affine minimizer
I go to this new point and now the when
I throw away one extra point from s this
becomes a coral so these are two to
coral things so why is this good well
what do I know I know that X old and why
are far apart in norm because why is the
affine minimizer so it's large think of
this is large so if z was very close to
Y then I'm done because because just
just by have some sort of averaging
argument so if Z is far away from this X
old and very close to i am done so i may
assume she is very very close to ex-old
but now this new x new is also an affine
minimizer on this point on this on this
affine hull so i can again apply the old
theorem and prove that the difference
between Z is Norman X news norm is
rather large where large depends on
and cue old so I am getting technical
here but the point is if you squint and
say well Z is very close to ex-old so it
will be high of like X old then I know
that this quantity is is large so in
basically in summary if I have only one
minor cycle I still can prove that the
drop is at least epsilon square by
something some eight u square so I went
a little fast here but sort of trying to
give a flavor of what kind of proof is
the proof breaks down if I have more
than two minor cycles it's I don't think
it's an inherent thing but one should be
able to prove more okay so so what so in
some what did we show so we have this
minor cycles major cycles major cycle
minor cycles and if there is a major
cycle with at most one minor cycle norm
significantly decreases but what if this
never happens but here's a very simple
but crucial fact that my nurse articles
are increasing size of s major cycles at
decreasing size of s by one so if you
have this major major minor minor major
major minor minor this can cannot happen
for long without having one major cycle
with at most one minor cycle ends so if
you just plus one minus one plus one
minus one the string of them and if
every and the running some cannot
increase more than N or decrease more
than zero then somewhere there will be
either two pluses with at most 1 minus
in them just a simple fact so what it
shows is that in 3 n iterations I will
have significant drop so I have an extra
n factor here and that's that's
basically the proof so it's a it's
really a cheeky fact if you ask me I
think this n should be removable ok so
I'll ln here with some takeaway points
so wolf's algorithm is actually quite an
interesting algorithm which works well
in practice for finding the minimum nor
points of polytopes I don't think there
has been as much experimental work done
on it as should there are some new works
that are they are doing it but mostly
for submodel functions but you can ask
you for any polytope so concrete thing
is well can you actually
they're on submodel apologize runs in
polynomial time not that not f square
not even log F so we did the following
experiment we take a we took the graph
the line graph and we look at the min
cut on the line graph and increase the
capacities so as you increase the
capacities the function value increases
capital F increases and we plotted
Wolf's number of iterations visiting
fries with F at all it doesn't it just
stays flat while so it seems to suggest
that it might be actually polynomial
time and that would be a really cool
result and more bravely you know prove
that Wolf's algorithm runs in polynomial
time for all polytopes that will be
super fascinating I'm not sure I believe
that but there is no disproof of that
and so with that I'll end so I have a
slight
Oh blimey
so just to name clustering a variable
detection okay so here's one so suppose
you want to cluster points and the
points are actually random variables and
you have to break it into two points how
do you do it there's no you know there's
no agreed-upon way of doing it one way
is this thing called minimum description
length so they're random variables I
want to cluster them and I say well to
describe this whole ensemble what's the
minimum number of bits you need so what
this animation was trying to show that
if you if these are random points and
actually this class is just a scaling of
these random variables so once you
specify the left the right is
determinacy specified that is some sort
of good clustering so if you stare at it
basically is trying to find a partition
of these random variables which
minimizes what is called the mutual
information between them which is a
submodel function so this was actually
is not I'm not inventing this is a paper
by nurse women at all but more
interestingly here is a new thing that's
quite fascinating so this is a kind of
problem that you know machine learning
people want to solve so it's least
square with a regularizer term and what
is the regulars are trying to do so one
property might be he wants to spar safai
the optimum value so there's this thing
called structured sparsity it is trying
to say well they actually you would like
the support to have some more properties
so minimize some not just the
cardinality but same minimizes of model
function and this has applications in
variable selection experimental design
and so on and so forth and what was
short is that so without going to the
details if you want to minimize this it
reduces to minimizing some translator
for sub model function resolve the
submarine so you can use these
algorithms to solve that in fact they
use the minimum norm point algorithm
there's a lot of experimental results
trying to do this so this has again this
itself as a lot of applications in
various areas of machine learning
semi-supervised learning unsupervised
learning and so on i must say i'm not
expert on understanding the applications
but it does have and they're really
excited so for this one so this one the
main thing is that this way of this
regularizer if i have a generous amount
of function no one has looked at it but
you could use it so you could use so i
showed you this submodel functionalizing
from data from matrices so that is used
for doing some sort of variable
selection page in variables little so
their papers written now whether they
are used in practice I think it's too
early to say they're all quite recent
but they may be and they are they're
decently fast and in fact they do the
minimum point algorithm or some other
variation of that yes so how does this
compare experimentally to other
heuristics for Euclidean projection onto
general poiitics so what are the other
heuristics I don't know how I also don't
know I know of wolf and I know a frank
wolf which was 15 years earlier by wolf
himself so the difference between wolfin
frank wolf is that wolf is maintaining a
simplex which grows and Frank wolf
always maintains a double time how does
it compare experimentally I don't think
know anyone has done a rigorous
experiments we have done some and Frank
will also gives a pseudo polynomial time
guarantee but it is actually 0
polynomial so in this example where this
is line graph with increasing capacities
Frank wolf iterations grow with F wolf
doesn't so but I think that's an
excellent question what are the other
heuristics for projecting on a ran on a
general polytope so I don't think it's
seen that much work yet this is really
cool will toggle really cool
so i can ask a question so how many of
you had not seen some model functions
before this okay how many of you think
that in in if you think about it now you
can find a sub malfunction if I do a
better job in explaining what they are
but oh so much it already has now they
are everywhere I just did a bad job of
explaining what similarities are but
they are very as basic as convex
functions one might say discrete
analogues of convex functions okay
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>