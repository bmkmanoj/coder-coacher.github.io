<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Software Defined Transport | Coder Coacher - Coaching Coders</title><meta content="Software Defined Transport - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Software Defined Transport</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FdU7w_ICr_I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay let's get started um it's my
pleasure to introduce chiu hong qiao is
a graduate student at UIUC where he's
been looking at network resource
allocation in data centers as an intern
at MSR Redmond he was a part of the team
that did the software-defined networking
work for his thesis work he's been
looking at software-defined transport
protocols and he's going to tell us more
about it so oldie thank you for the
introduction so hi everyone she out from
here you see and really happy to visit
here and give this talk about
software-defined transport a
programmable never architecture we
propose for optimizing cloud performance
so to begin with this talk let's start
with the code it's anticipated that the
whole of the populace part of the United
stay well within two or three years we
cover with a network like a spider's web
any guesses what day this is from anyone
which year 70s 1970s anyone anyone else
very good very good ah it's actually
from 1840 a in aiding for the way from
London actitudes so this talk is about
designing networks and people have
designed network 400 of the years
already and this is for electric
telegraph networks and so when people
decide but in the past when people
decide different kind of network they
are constrained in different ways right
for example when people designing why
they are a network they are constrained
by where the population is for example
and today we are moving into the cloud
and what's so special about cloud what's
the different about cloud so today cloud
service provider had built their own
network infrastructure such as data
central networks as we already know and
intra datacenter when they build their
own backbone as well and what's so
special about cloud is there's our
challenges and also is exciting part so
first it's very critical infrastructure
we have today in a cloud because it's a
lifeblood for the service for our for
the cloud service provider deliver very
improtant user traffic essentially
everyone is relying on this critical
infrastructure and also it's extremely
expensive right people have build those
big data center and expensive one the
capacity capacity goes across multiple
continents for example google has been
more than 7 billion dollars in the EF
2013 for their data center
infrastructure and so we also cut this
we also find this a very exciting domain
to be working so first they have unified
control central control that you won't
get in for example in the past in the
internet and we also have this new
proposed idea about a software-defined
networking that allow us to have more
flexibility on troll with a network so
with this there's an opportunity for us
to have a new architecture that can be
feasibly implemented because of this you
know new control central control of the
network and have impact there and sdn
can be tied into this architecture
to be deployed so so we have those
challenges about has to be around very
efficient because it's expensive
resources and you know it's very
critical so let's take a step back and
look at today's network and how does
today's architecture provide you so they
satisfy our requirements or not so how
we run today is today's network is
essentially we have a whole bunch of
protocol a super protocol with knobs and
dials that you can team about like tcp
bgp so on and so forth for example we
have some routing protocol you can run
across across routers or you have some
in-house protocol run across this end
hosts they are very complex but the key
issue is there's no clean programmable
API for them essentially it's hard to
optimize the network for both
performance and efficiency goes to
certify the service requirement and we
think there's a key three key problems
for today's network architecture first
it's not very flexible they run some
predefined algorithms and if that's not
what you want you won't get what you
want and second it's very hard to reason
about essentially there's a big gap
between what those protocol can provide
you the property they can provide you
and the high level service requirement
goes we want to satisfy in the network
for example in the multi pens if there's
internet work the operator may want some
high level service quality things like
you know I want some bamas guarantee
occur for the distance like three copies
per second across this set of VN to
death said of the end or some latency
guarantee for the another lieutenant or
something like mix our prioritization
and the fairness for the rest of tenant
and that's essentially hard to do with
today's protocol you can't just take the
s to this protocol say take my words and
try to optimize the network for this
it's hard and one another so she is you
devise some new protocol to achieve your
goal but essentially that's also hard
because you have limited deployability
with today's model often new protocol
required changes advil switches or
and has even both or even both so to
solve the problem our vision is to make
today's transfer architecture more
programmable and we believe this can be
served as a killer application for cloud
network optimization so that's the name
of the architecture we propose cause SBP
software-defined transport so this
architecture is integrated with
software-defined networking and the key
idea of software-defined networking
include an open interface thing
interface to the data plan and also a
logically centralized controller where
you can centrally control the network
forwarding the software-defined
networking gives you only the low level
asses to the network forwarding plan
right it doesn't help you what's the
northbound API we want to assess for it
for example to optimize the network
performance essentially what we really
need is eco system that allows us to
build around the optimization targets so
there's some rust consensus about the
protocol we use today like open for
block code but there's a very little
consensus about what's the whole control
framework you want to do to for the
network optimization so to do this we
built on top of SDN in another important
building block in our architecture is
this also the interface to the end hosts
and then where we have another
centralized logically centralized
controller hot controller to collect the
flow demand from n house and also a low
key in specify the flow rate about this
in-house sending behavior then on top of
this we have another layer because
software are resource optimizer where we
run our actually optimization algorithms
even the floating man by the host
controller and also based on the
topology information you get from the
SDN controller also you get I provide
this nice interface to the narrow
operator so
they can specify their service code such
as you know utility function or
transport policy they want to enforce in
the network any question so far for the
architecture all right so this is the
I'm going to briefly summarize the key
result what we get when we apply this
architecture to different places when we
apply this to the inter data center when
what we get is we can carry sixty
percent more traffic than today's
practice and then we found we have
congestion free now update with this
architecture and we can work with
limited switch memory so we have
satisfied some practical concerns there
and also for data central networks we
find stp will help us to save sorry so d
% mean flow completion time and help us
to suppose three times more delight
flows than today's practice and also we
can handle thousands of flows south and
also network a network size with
thousands of servers scale up to
reasonably large size with very fine
grain flow control so this is where I
published those papers and the colossal
and where the paper get published so the
first to plug a publishing second son's
13 2012 and the last part is my ongoing
work is due to presenting open
networking summit next week so I'm not
going to talk about every detail pieces
of those contributions but today I'll
talk about just three items here so
first we look at when optimization when
we apply this to win how much benefit
would get there and then we back the
second one we look at the data center
network how can we push this central
control limit in the data center where
we have you know thousands of tens of
thousands of servers can we achieve that
scalability with this central control
and then out if time there's I will talk
about my other related work so let's
first look at when we look at the intro
data center one the biggest issue today
we are facing here is low efficiency
people
on this network with very low efficiency
without STD architecture and we provide
to example to show you why first this is
a the time series of the traffic
workload major by a production into DC
when at Microsoft and this is already a
very busy link and but you can see the
traffic goes through peaks and valleys
across update and the current practice
is because you don't have a very nice a
mechanism to protect the improtant
traffic so the current practice is your
provision the peak so that your
important user facing traffic will
always get through the network by doing
so the average traffic utilization of
this link is below fifty percent in
other words more than half the capacity
is not used there's an easy way to solve
this problem right if you can have this
nice flexible control of the network
forwarding behavior you can leverage its
traffic realistic in inter data center
win today we have those background
traffic and non background traffic
background traffic are those you know
replication oh you know indexing those
big chunk data you're moving they can
delay for hours without compromising
their service quality so by simply doing
if you know this information and do
global rate control then by simply by
adapting for background traffic then you
can feel the same amount of traffic with
help the capacity and those you know
more than fifty percent pick reduction
will help you either accommodate more
traffic or you know you can delay your
expansion of the network yep cushion
scheduler the higher level to just
schedule the background tasks during the
interrupts that's a good question so the
question is can we do this at
application level the issue is in when
we have essentially too many
applications and from the application
point of view you don't really know you
know other who are the other application
competing with the network resources
with with you so yes you can do some
scheduling and application level but
that would be hard to for them because
they don't have this global information
about the whole other computer yeah all
right another efficiency comes from in
flexible forwarding of today's model
today people use NP RS T it's a traffic
engineer protocol help you to find the
path they call tunnels and to satisfy
the flow bandwidth constraint so here we
give an example where we have three
flows ABC arrive one by one and assume
in this example each link and carry at
most one flow then when flow a arrive it
would take the shortest path from source
to destination while satisfying the
bandwidth constraint and flow see will
do the same but you cannot take the
shortest paths because it will conflict
with flow be well failed to satisfy
brain cancer and so end of it take the
second show this one and so does flow
see end up taking a very long path and
if you have the global view of the
neighbor and you can globally coordinate
the forwarding plan in the right hand
side we show much better allocation
about the flow forwarding plan where
lots of link capacity get free up you
can accommodate most loads and the
Latins is smaller all right so when we
apply STD architecture to here we try to
maximize our to improve the overall
efficiency through the central control
floating plan and also the rate control
of the end hosts and one key challenge
we face here is how do we scalable
scalable II apply this architecture to
win while you know does not compromise
much about the computational overhead
like we have lots of flows in a when as
you can imagine how can we scale bow to
that scale so we here's a couple ideas
why
is to do hierarchy right this is not
simply just once controller to every end
house or switch that definitely won't
skill we have multiple layers in between
that try to aggregator for example we
have been with broker sit between
controller and in-house that collect the
end house demands and do aggregation and
sender aggregated bang GWA the request
to the controller and on the out another
direction when you gather resources
allocated back to the to the Penguins
broker you will allocate do more
fine-grained allocation to the in-house
under its control and also we do this a
graph aggregation essentially our
algorithm does not solve the graph in
the physical layer right we do link
level aggregation like each node in our
problem is is a data center in society
instead of real switches which actually
consists of you know tens of switches to
link the variation and we also do flow
group aggregation where we only look at
source TC destination data center and
then a few priority classes double also
we want algorithm to be run very
efficient so how we do our resource
allocation is we allocate resources
class by class we have a few private
classes and with each class there's
multiple flows we want to ensure maximum
fairness depends on weighted maximum
this depends on their their weight
alright so the key challenge here is how
to the right the maximum fear in this
it's actually a very hard problem to
compete compute when when you have the
freedom to choose the path and also
control the flow rate and today's
practice will take up two minutes at our
target skill of you know 40 to 50 days
enterocytes and the solution we propose
is an approximation algorithm that
computes faster than today's practice
essentially what happened here is you do
this in multiple stages and in each
stage you have for example some upper
and lower bounds limits on the flow
rates you can cite or each flow then for
flows we're um
stand our multi commodity flow solver to
maximize throughput while give
preference to the shooter flows to the
shoulder passes and then at each stage
some flow that cannot get what they want
we code those flows are freezed and
their rate where we will get this fixed
rate and they'll pass it they will still
participated in the next stages
computation but their flow rate will be
fixed then eventually you go through
this to multiple stages until every
flows race Caffrey's then you get the
allocation those are the free stream
flow rate or the rate you are going to
allocate each flow so although we didn't
explicitly control the fairness within
each stage but this is great because if
your flow rates in the next stage you
will still get there because the other
people's flow rate get bounded in each
stage and theoretically we prove this is
a approximation algorithm so are some
you know performance and you know wrong
time trade-off here you can have and you
can deviate from the maximum fishery by
at most effect of alpha empirical
empirically you found this actually runs
much better than the worst-case perform
then the worst-case performance and we
can approximate this problem when the
alpha equals to two approximation
algorithm we can achieve high
performance while takes only sub seconds
to compute and this is a figure we show
the comparison where our approach with
alpha equals two can actually
impractically approximate the maximum
fear failure rate very well while for MP
OST which does not have this fairness
constraint have in their algorithm then
can you know deviate from the fear
maximum fair share a heavy d and bunker
plea yep
application are you assuming that all
flows are neckcloth limited or are you
because flows can be host limited you
know because of annuals constraints
because of storage they may not be able
to generate a rig at the maximum fair
rate just do some are confident yes so
when the end house send their floating
men you will based on hasti you know
application level or you know system
level bandwidth constraint if you can
use only see if the nick is one gape is
per second they can only consume say 200
megabits per second you won't request
for gigabits per second and there's some
estimation here right um depends on
application some application you may
better know your resource demands and
how much pain which you can use
something may not then you have to do
some estimation and you know learn over
time dynamically demand this tuition
Yorkshire yes yes yes any other
questions all right let's move on to
next challenge so what we're trying to
do is achieved I question see in the
network and that requires frequent
changes to the network Bo's forwarding
plane and the rate control so you're
essentially reconfiguring the whole
network in a you know very small time
frame clarity in our setup is like five
minutes or 10 minutes and doing so may
create lots of you know chance when you
update the network and in the worst case
that you may drop lots of package when
you are switching passes and a challenge
here is how to ensure congestion free
even during the network update so that
you won't get this transient congestion
will happen when you are optimizing the
network right so it's actually a very
tricky question and let me take this
example to show you whenever when
there's an initial stay in the levins
that you want to move the target stay in
the right hand side again assume each
linking a company just one flowing have
two flows here assume you essentially
this is a distributed system right so
you cannot change the routing folding
table at different nodes at the same
time
right and because there's a delay you
can come dig so suppose the flowbee get
updated first sorry Flo a guy updated
first to move it to the to the lower
path and then you get a congestion
happened ok suppose on the other hand
suppose the flow become updated first
condition will still happen it's just in
at different places so actually in this
example there's no congestion free
update sequence you can find so how can
we solve this there's no feasible
solution to move close to there from the
target state to their destination scale
without making add without always
subscribing during the update so the
idea to solve this is to leave a small
amount of scratch capacity on each link
and that can be used to accommodate
additional flow student in the update so
back to this example where assume in
this case we have a slack of the network
which it is the one stirred up link
capacity reserved then now each flow can
use up to two-thirds of the link
capacity now we get a feasible update
sequence here you first moved half the
flowbee to the top path and then you can
move the whole flow eighth to the bottom
path then you move the rest of flow be
to the top ass and you are done you will
have no congestion at any stages so this
is great but this is just example so how
do we find this in general that the
slack always guarantee you find a
feasible solution and we prove the
answer is yes if you crafted this slag
capacity in your network s as a mono
scratch capacity we proved there always
is a congestion free update sequence
within 1 of s minus 1 steps so here one
step consists of can consist of multiple
updates to the routers it's just there
out it's just there update order it
doesn't matter yep it changes in the
network
you sleep so when you want them to
happen they happen but they could be
delays there to be delays yeah that's
why we're we are doing this the events
they shouldn t previously it happen the
way you want so we have multiple stages
right within each stage you don't have
to control it doesn't matter you just
issue all the half days okay but across
stages so you have to wait until all the
updates in this stage they are finished
and successfully installing the network
then you can start your next stage yeah
yep it's like a constant survival
network operations your voice using said
yeah if you remember capacity something
like that I will talk about how we
utilize those and use capacity later the
imv back fifty percent head of the
reconfiguration not to reconfiguration
it and go now out to audition um then
you lose capacity there right you lose
this past yes point hmm sorry I don't be
operating at how do the same on luelinks
right you can reconfigure an introduce
contention to only throttle and both
back to fifty percent let the contention
contagion won't happen because it took
it is either hunt center at the
congested think when the reconfiguration
is finished and all go back up to having
the same you don't lose it a third of
your network offense to you anyway
that's one beautiful solution then but
that sounds very similar to congestion
to me right you lose half the class
capacity during the update you either
let Peggy get drop or you to raise
rather win before the NAPLAN back its
source so you can get congestion and
everywhere so you lose half capacity for
ten seconds yes and then run at full
capacity for five minutes rather than
running in 66 capacity for five minutes
yeah I will show you how to run hundred
percent capacity during the update and
also in the normal time yeah alright so
this is the way we found it we form a
linear programming programming program
to find this contingent free update and
the key variable in this LP form is the
following b ijs this the rate we want to
control for flow I
at pasture at that pass and so for we
have initial state that give us be idea
dear oh and we have targets they give us
bi JK and found we want to find that
intermediate States pro12 blj came as
one and a very imprudent constraint to
to allow us to find the continued free
updates the following essentially what
we are doing here is to protect the
worst scenario this works scenarios
specified that you know for any link the
worst case is for all the flow snapped
repairs this link if they got increased
flow rate after the update they have
updated already while for the rest of
flow will get decreased flow rate they
haven't and we protect this war scenario
so this issue ensures that there is no
condition what happen and we prove this
is will give us the optimal solution in
the sense that minimum number of steps
you will use you can you can use to find
a solution now backs to the question I
talked about why are we doing this you
know crafted cup added you are unused
ten percent capacity that's also for
five minutes that's also wasting lots of
resources here and it's expensive so we
want to utilize it and the way to solve
this is again we leverage it trafficker
istics so for inter user user facing non
background traffic we really want them
to have you know no congestion will
happen at any time for those we let them
use up to 90 capacity and use the
algorithm I just talked about we can
ensure ten percent slag for them so they
have you know smooth updating the no
congestion will happen doing up network
update but then we when we compute
different classes of traffic with
computing class by class right when you
compute the low priority classes like
the background traffic here now we allow
them to use all the capacity to indy no
more time then so that's how we are used
all the network capacity during the
normal time but then but then they were
in they will experience some network
congestion during the update where the
congestion is pounded and you know it's
not too bad for them because they can be
delayed yep
only a huge number of actual application
clothes right right so how are you I
keep splitting a big flow is it on a per
application flow I mean it seems like
this very large number of application
flows and they could have any
distribution of raids across Jennifer
how are you going to split these rates
without splitting individual application
flows across multiple cars yes yes so
the way we do is we try to differentiate
apps first so like different apps have
different you know families broker that
help them to allocate the resources and
then we have some fairness also we do
proportional fairness across different
flows within the app so essentially
there's multiple layer happening here
right so for example apps has lots of
flows a proper app you know Paris broker
and then browse broker you have some
data center level best broker that
collected the man from different apps
the end hosts and host level whatever
you're calling that the tiniest flow
right did you ever split down across
multiple class no no we don't we don't
that will create some packet reordering
issues we don't do that yeah so but when
we have enough amount of flows so we can
do very fine-grained splitting a crow in
the flow level that's we do some
evaluation on that and we found that's
good enough for me for us yeah all right
so let me talk about the overall
workflow so first when we run the system
we collect the flow demand from the host
controller and also we collect the
topology information from the SDN
controller and then we computer globally
compute computer resource allocation
also the contingent free update plan and
if there's an Afghan we want to globally
change the network we first notify the
services with decreased allocation they
can you know reduce the flow rate now
and then we update the network
forwarding plane this could take you
know one or more steps to ensure there's
no congestion happen to the interactive
traffic and then finally we let the
service with with increased flow rate
they can start sending within retire
right now so build this platform or
build this prototype with 16 switches
and the 32 servers for evaluation and
then we do both protocol Valerie's
prototype evaluation and data-driven
evaluation that allows us to scale to
the larger scale where we take the you
know this is a real choices from the
inter data center entity and for
prototype valuation we found key to key
observation here one is less one can
this is the project because one can
achieve very high throughput ninety
ninety eight percent of an optimal
master the optimum master assumes you
know it's a realistic Oracle knows every
flow demand with zero delay and can you
know control the folding plan with the
road away as weel control freak bad luck
with no delay so you won't get into this
congestion free issues and the other
constraints as well and also we found
this can achieve fears of can achieve
very flexible sharing so in protein
interactive traffic is always protected
while the background traffic will
automatically readapted and we also do
data-driven evaluation to scale up to
the today's scale and we found again our
solution can achieve near optimal
solution under this practical setting
and this is using today's trees
traffic traces try to scale up to see
how much additional traffic we can
accommodate compared to this practice we
can accommodate sixty percent more
traffic the MP OSD while still
satisfying the service requirements now
try also try to decouple the benefit
from different building component so
without the rate control so essentially
this is the case where you can only
control the forwarding plane but not the
end halls behavior you got roughly
twenty percent game here all right
questions let's move up so next I will
look into how can we use this
centralized controller to inside the
data center right in data center we need
more scale and the reason for that is we
want more fine grain flow control right
instead of aggregating the nose and
flows into you know whole bunch of large
trunks we try to do more fragrant
resource allocation and in data center
you could have you know thousands or
tens of thousands nodes in large network
there how can we possibly do this
essentially this is some trade-off
between sky ability and flexibility
right today's networked transport
protocol they are quite mostly quite
scalable because essentially they're
distributed Prokos but they are not so
flexible in the sense knee you can a
program them free you cannot deploy it
in with low cost and when we apply to
when this is the case we got we got very
flexible cases but it's not fine grain
flow control so the question here is how
to push this boundary towards more
flexible yet scalable our resource
control so here's a couple ideas we
propose we adapt to scale the transport
rate control the first idea is just use
the local control right your central
controller has larger timescale you
cannot control in say micro second time
friend then you have some local control
you instructed in the end house 22 to
react to that for example suppose we get
the network and a controller you have a
flow initiated
just use TCP or any other transport
protocol that fit to control the rate
before the central controller allocate
their their comments to the 2d source
and also in the network we give high
priority for those you know new flows so
that they can get through easily and the
next idea is we handle only long flows
right there's a mixup long and short
flows in the data in the data centers
and STV cannot control when s we can
scale up to control other flows we
control only long flows so when flow
started they we assume their ship flows
so they don't have to talk to the
central control digit they can just
saying with high priority when they send
for example more than a certain number
of pies they will be classified as long
flow then they have to talk to the
central controller about their fuel
demand and through the global through
the central optimization your later get
a flow rate allocated then after you get
the allocated great if all back to be
controlled by the SDN to be controlled
by the SDV controller and then also
switch to the lower priority how is this
different from proposals like hendra
from the USGS coast where they were sort
of using this controller based
architecture for not close looks very
similar yes and the key question here is
about this capability we try to answer
and this is this is ongoing we try to
see how much how fine-grained flow
control we can achieve and when we want
you to do you know more flows more Frank
Gore and real-time scheduling in the
data centers so if you just look at long
flow it's probably easy right depends on
how large the flow is and how large the
data center scale will be right to
you're only dealing with mom clothes
right which is what they were doing
right right right pressure I guess I was
expecting something beyond lon flow so
you can do controller
lose something else is going to talk
about the next idea yeah so next idea is
to do fast pero rate flow rate
computation so essentially we one
centralized control we go on global view
so we want to compute in a single
machine but single machine does not
scale up to handle in a large network so
how can we do we we can we can use multi
stress and this is smart algorithm we
propose to use flight level simulation
to compute the flow rate using Matias
read this yep is this capability problem
latency to the centralized controller or
get the resources under centralized
controller HD it's the CPU resource at
centralized controller yeah so we do
some backup and envelope calculation and
we found this lots of overhead right
there's a Latin see from in-house talk
to a controller and also the damn with
for the control package and also how
much time you need to spend on the
controller and how much time you need to
spin until the you know result get back
to the End House it turns out where you
want to scale the large net large data
center network the computational time
computation time is is the key
bottlenecked prevent us from scaling up
because if you are talking about packing
flows nine kilobytes how are you gonna
get to a controller and back in a
reasonable amount of time just following
up on that this latency I can imagine
computation being of big bottleneck yep
whatever you can see to the controller
or short flows seems like just as big a
problem right right so that's what I
just say it is under the under the
assumption that my earlier idea get get
up Lloyd so essentially you don't
control those very shop flows in the
rationale for that is you know most if
you look at the data center traffic
distributions most of the traffic
generated by the long flow and the
majority of fools are short so what you
are doing is you can let loose of the
control of most up flow while you still
get opportunity to control most of the
bite in the network
yep converting all of their long fluids
too short close to take advantage of
that yes that's some you know that's
essentially you're asking how how how do
we do when people try to gain the system
right and that's a big question right
essentially you can do this not just
here but also the traditional way PCP
right you can always create lots of
flows and get larger total should put
and that's something we think this huge
in the future where we can look into
where we didn't handle that for now okay
so just try to give some idea about how
this you know parallel computation works
essentially the whole this whole thing
will happen in a controller compared to
try to compute the flow rate for each
flow based on some you know service
service requirements so essentially the
flow level simulation assume you have
two flows there with a demand one for
each then each thread in a controller
computes handles the computation for a
link so based on some you know operators
specified transport policies you do some
scheduling here at flow level for
example suppose you want fairness then
how we got for example weighted fairness
you have zero point seven zero point
three lk two different flows and this
link after it compute this information
you propagate to the next next link in
the downstream links and then eventually
we will go to a destination and then
that's the rate you are going to
allocate for each flow and you'll get
back to the sender that's the allocated
crypt so there's other optima our
optimization we apply to this algorithm
increase like we use a pro link 30 bit
so that the product each thing so that
they won't you know keep Rick precum
recompute right if the input flow rate
remains the same the output should be
the sense or you'd have to recompute
sure normally Mac Smith is done using or
filling out have you done something
beyond that isn't one what's the key
idea sorry
the key idea here is we try to there's a
two things different that you know what
everything cannot do for us for example
this is more general than other feeling
in the sense that you can improve them
pretty much any kind of you know cueing
that's a disciplines here instead of
just to you know what maximum fitness
and the second thing is with one
parallel computation that you cannot do
in the water filling algorithm so
essentially here we each link can
compute by a thread allocation so
wonderfully can achieve maximum fairness
with your maximum fairness and other
things worked out allocation schemes
that you can achieve here for example
you can do prout ization you can do
demos guarantee all right and another
key benefit is the scalability how a
show yep dependency more links because
the point is that you can't really
compute you know they're right when I
the link in the Bentley because you know
once you find the bottleneck linking it
to a compute other links as well right
the component that's a good point so I
don't have time to go into the detail
but the high-level idea is we use this
dirty bit 22 for each link to indicate
if the input flow rate right essentially
the output of the previous link get
updated then you need to recompute right
that's the only dependency will happen
across links and we use it 30 bit the
good the typical way to solve this is
you puke you put mutex mutexes right
because you have now this link for this
link and assessing here i want to check
i will update this I'll recompute only
when the dirty bit is is is false right
if it's dirty then I will recomputed
then you probably want to funds ensure
there is no concurrency issues so you
place mutex but that will slow down the
whole computation
doesn't allow us to skill as nice as
nice as using my lips right so but this
is only just one bit if you think about
it you can flip in an atomic fashion so
how we do is you clean before you
recompute and then you set a bit after
you change the input flow rate and we
can show that you this will allows you
you will never get into this bath state
where you know the link is actually
dirty but you mark as Queen so you will
never get computed does that answer the
question cool all right and and there's
other optimization with take to avoid
affiliation and also ensure this is work
conserving a location let me talk about
the evaluation platform for this so we
have built this test bed at UIUC where
you have 13 we have 13 open flow
switches and for pic servers net allows
us to drive 1 2 1 12 gigabits per second
and the result we found here the axis
shows the narrow size in terms of number
of servers where access shows the
control interval with log scale so
essentially you want to smaller control
interval while we give the constraint
here is you can satisfy the flows most
of the flow in the network so the key
results we find within within today's
data center workload with today's date
as a network this central control with
foregoing flow level rescheduling can
help us to scale 2,000 uploads with you
know sub-second control interval in that
case we found we can still handle more
than ninety-five percent of the total
bytes sending the network and the second
thing is our algorithm help us tools to
scale up to scale up with four threads
we got roughly linear linear scaling
improvement
so this is one quick demo I just want to
give you some sense about how these
things work right so in the right hands
in the right upper corner which shows
the topology me lated then we have full
flows to send from the same sender two
different destination so it's it's one
too many traffic pattern and now I send
this each flow has 200 mega byte and
without our STD control this is what we
get the flow essentially sure fairly
using the TCP today and over finish at
times 7 now we see it switch back to our
spp mode we are running you know engine
a 10 house to talk to the controller and
reschedule the flow phase and assume we
do the same experiment again but assume
now we have some prettified waited to
instruct it for each flows so we're
doing some flow flow level in a weighted
next in fairness and when some flow can
complete the you will recompute and
allocate the flow resource to the other
floors as well and we plot the result we
can see some flow for example the ref
logo got higher weight they can complete
much faster because they got higher
bandwidth in in the in the early stages
and by doing immolating this in software
we can flexibly emulate wide range of
scheduling policies without constraining
by the hardware resources all right have
roughly chopped minutes to be finished
talk about my related work so this is
the to work I mentioned in this talk and
we also down pre-emptive scheduling with
a distributed protocol again along this
space I didn't get time to talk about
today and I also did some work on
Wallace scheduling and also some second
ever security projects we have done
including this detecting malicious proxy
IP addresses using machine learning
techniques and also we do anomaly
detection for the operational network
using the big big Network locks and the
technique here we applied includes time
series analysis and also hierarchical
heavy hitter detection so dude I also
down this buggery project where I want
to look at the network traces and try to
define try to detect the bots using
simply using the network graph network
communication graph also down some
network measurement project and the this
project called cox cable or try to
identify to understand what the root
causes of the internet clock
synchronization accuracy and also a data
center topology project if you get any
questions for the talk and our happy to
answer for now if no i will briefly talk
about this jellyfish project with the
last 10 minutes i still think about this
kalevala transport so how long is a big
how long is a long flow for you because
if it takes a second and if you have a
technique network yeah then chairman
must be very longer so for the result we
show the stress show is roughly seven
280 megabyte are assuming one gigabit
network or 10 gigabit NATO how long a
bed network so if you are doing to 10
gigabit network it would be like a
factor of 10 more that could be the case
that could be case if you want to
accommodate if you are saying everything
is the same right you think you know
it's kind of scary never you know
thanking network we're probably not the
faction of flows that you know are above
your treasure that probably bit is very
small I don't know depends on how how
fast the server server side also can
service I us go up by 10 times right you
can accommodate 10 times more pm's there
then you get 10 times more traffic
network traffic right so it depends on
which one's faster yeah yep so what you
do exactly kind of use a centralized
controller because it seemed that you
are dealing with every link separately
right so you could have a controller for
each length and then you are not really
kind of losing the centralized
as in there is a kind of any information
that you are using beyond information
for that particular in on it depends
right in the in the ingress links you
still need the demands for for all the
flows there you have a computer for that
particular link with that have all the
information for them probably probably
yes but you have to then you have to
handle that the issue happening in the
distributed manner right now whereas we
are talking to a cross just different
threads and we have lots of shared
memory we can use so we essentially
sleep a bit then everything is fine but
now if you run this into instead of two
threads you run in the two machines then
you have to maintain the state you have
consistency issue there you have to be
handled and I am not sure if that will
babe I us the same same performance
improvement using muddy muddy nodes yeah
how exactly do you deal with the short
folder then what exactly do you assume
about them as in do using that they will
be using some part of the link in terms
of your own computation or you just kind
of ignore them we just cannot become
then give them high priority and the
controller will adapt to that and
because they are using you know
relatively small fraction of the total
network capacity just let loose of the
control plane all right
let me quickly go through this another
interesting work I've done in the
topology design where we that we call
jellyfish essentially we use random
graph topology apply to the data center
networks so essentially we just connect
data center randomly to key codes we
want to achieve in this project why is
high school put with the minimal cost
essentially you want to support you know
lots of capacity and to a lots of
throughput because we want big data
analysis you also want a vm placement so
that you your vm placement will not
constrained by your network bottlenecks
at the same time to date people are
adding keep adding capacity to their
system in a daily basis like Facebook we
found they need to expend there's a
Supra da increase in a daily basis every
day they got more and more resources
usage and this topology allow you to
have EV incremental expansion by easily
adding server and switches without you
know having worry about the whole
structure so this is how jellyfish works
you have reacts observers on top of that
your top RX which is then you from a
random graph at the switch level so this
is what we code jellyfish and the name
comes from the intuition that this make
the network capacity lake less like a
you know fluid that's like a structural
solid but more like a fluid essentially
everything is Brandon when you add
things into the network you have EV a
few Capo's webs and then you get you are
finished you don't have to maintain the
certain structure as we done in today's
data center network lifefactory s
tenderly also looks like a real
jellyfish so one key question want to
answer is okay this naturally the sloppy
structure of the random graph allows you
the expansion but how much how much
super we compromised by having using
this random graph
so this is the result we get as compared
with factory today's structure topology
people widely used for data center
network and the excess shows the
equipment cost essentially we're using
the stand using the identical equipment
as compared factory with gel with
jellyfish and the y-axis shows how many
servers you can pack into this
infrastructure while as while satisfied
the for sending rate so that they won't
block by the network essentially you fun
twenty-five percent more servers we can
stop put with jellyfish and at the scale
of roughly you know to three thousand
servers and the improvement increase
over skill so when I talk this to people
about half the people would think this
is review the rest of health people
think this is not intuitive so depends
now which have you are in right but
usually we want to give some intuition
let me use the last five minute to give
you some intuition why we get higher
super than today's structure networks
sure what though yeah we use a server
level random permutation that's the way
we try to stress the network yeah okay
so some intuition here if you want to
fully utilize all the capacity in
network so how much throughput we can
get in the left hand side of this
equation is the total capacity over the
used capacity per flow right so total
capacity is essentially how many links
and what's the capacity / link and the
use capacity people flow is how many
flows and what's how many hopkins how
many helps they go okay if you given
this network then you cannot change the
things you cannot change is the link
capacity right that's the budget you
have so if you want to maximize
throughput you want to minimize the mean
patterns so that's our mission to
minimize the mean pestilence then we
look at Jellyfish and compared with
today's structure network like a battery
in the left hand side
well this is probably too large scale to
be useful to see so we look at a smaller
scale graph where they have exactly the
same number of servers number of
switches number of degree okay it's just
in the factory you have this clean
layers you have fat you have a larger
core in the in the network or and you
have thinner in the end H while in the
jellyfish is a randomly generated graph
okay so let's randomly choose our origin
for both graph and then move round
origin one cup to helps three four five
so up to five hubs in Jellyfish we
already reached 12 out of 16 servers
well there's only four server can be
reachable info hub in factory and the
main reason why we can reach most of the
servers in the shorter path length in
theory it's because it's a good expander
and you can expand you can reach out the
larger set up servers in in the fewer
Hopkins and if you look at fat32
understand why factor is so bad in terms
of mean in terms of path lengths you can
actually for those links in the red
color started red color you can read
most of the link in factory are not
useful to regard to reduce the past lens
I can remove all those red links and by
all the mean path length will remain the
same so what we know so far is this
random graph will give you high school
put as well as flexible expandability
yes different pattern like all 2002 oh
yeah actually otro is we think it's
somewhat easier then random permutation
essentially otro you have flexibility
there to go to everywhere and we look at
those different traffic pattern as well
by our flow level simulation and we got
similar results yeah
okay sure let me grab up so this is just
a beginning and I have one minute I
cannot talk about this in details but
there's the scenes we also look into in
our research like can we do even better
is the random graph is the best solution
is there any optimal graph that give you
a better solution and also look at
system decide sorry also we look into
assisted system design issues like Renan
where people will always feel scary that
can i generate a terribly bad graph that
even you know get disconnected or
cabling issues it's like a cable in
spaghetti monster you're handling like
lots of railing cables go run switches
and how to do routing and congestion
control without today's you know
structure all right I think I will stop
here and if you have any questions are
happy to answer thank you or just a
quick question so it sounds like with
your render brassy and I think that's
one of those bullets you saying you make
it operationally harder right to
maintain in the basin sorry which one
polish operation others okay right
because have the operation you are
thinking what the guy who does and in
cable stuffing my next things right it
has some anything that at the network
right so how does he know where has a
connecting singles on with the top coin
tossing right inside ready okay so first
of all let me show you something fun
okay this is a test bed I use for the
demo video and one natural question is
this looked like a jellyfish you know
capo is quite messy here and do I really
do this and the answer is yes and the
proof is this picture well we connect
then randomly but in practice you won't
do this this is a bad example
essentially it is a bad example yeah so
what we do in in data center for example
how we really do is for factory and
jellyfish you both
create a blueprint about you know how to
put forth of the switches connect to
which product switches and then the
people who actually do this you know
cabling stuff they just follow the
blueprint and you know connect them
accordingly so there's if you think
about the cabling there's not much
differences between jellyfish or factory
because essentially you just get a
blueprint and you just follow the
instruction and come back then is that
simple because you have to maintain it
you're expending the sender things
fairly happy to be cable it's not a
one-time thing right right right so we
we do have some evaluation about that
yeah yeah it's going to follow up and
say I'm in the factory has a number of
other well understood properties applied
what happens in the event of particular
individual Freddy cloths and and kind of
uniformity patterns and there's a whore
above all things I mean I guess if
rather than saying to me this is not
saying jellyfish is a good way to build
a network it's saying at least from the
bandwidth per equipment cost point of
view factory is a bad way to build an
error because even a random graphic and
does better or not that's right okay
right um rather than their run well
actually I haven't got time to talk
about it actually we formally prove that
ran and grabbed is a class out near very
close to optimal performance crap yeah
yes I haven't talked to that about but
yeah there's it is a sensible structure
which is actually you know those as well
on your ego metric of interest and also
preserves and all the other good proper
fix right right okay so two things I
want to talk about one is how far is it
jellyfish away from the optimal graph
and one particular class of graph we
look into is weak 0 degree diameter
bounded graph and I don't know if you
know Peterson graph is why example where
we have 10 notes about equal diameter
public web and for that class of graft
is we have constrained about degree in
diameter and why you want is you pack
has as many notes as possible to the
graph you want
large earthquake that satisfy those
constraint and we think those are close
to optimal for us because we are kind of
kind of finding the same problem in a
different way right we have degree
constraint for switches and we want to
minimize the diameter while we have this
you know network sites constraint as
well so it kind of similar problem to us
and when we look into those today people
don't have a very nice algorithm for
that so only like five or six very small
size less than 20 knows graph are known
to be optimal the others are you know
heuristic computed and when we look into
that and compared with random graph we
see the super we got is roughly ninety
eighty six to ninety five percent away
from them and that's pretty close we
think that's some good evidence about
optimality of jelly of jellyfish and
another thing I want to talk about about
operation right I think if you think
about the operation cost is something
jellyfish would be bad it depends on how
you operate a network right if for
example for example failure detection
suppose some link goes down you want to
figure out which think it is and want to
you know go there and you know update
the cap cable depends on how you do this
right for example when I talk to talk to
Microsoft datacenter people gns people
as hey this is a cool idea about
jellyfish and is it yeah this is really
great but the largest issue is today
they have run some management script
they are using for example they run this
code for mashpee full match pin to pin
every in house to every other end house
so it give give you a connectivity
metrics essentially and they use
something similar to factory by the way
and whenever there's a link failed or
you know a switch fail they can easily
by look at the mattress and figure out
oh this is that course which is failure
because we have this structure so you
can figure out all this block fail
is represent some switches so they can
do this easy things to do and that that
does not apply to to run and grab right
but if you have a much better tools
today to matthew network then everything
can be able to make it down for example
you're wrong you run this you know link
discovery protocol which will tell you
what's your current graph looks like and
you have you know blueprint about the
original graph and you do grab
comparison you know which lingo fails
then you can easily you if you autumn
eyes auto make things that make the
management in more you know automatic
way and you don't have to do some many
things to look at that then I don't
think operation costs will be a big big
deal there yeah and we also take that
cost into account in the sense that
jellyfish actually need a little bit
more long cable than then factory
because we have some long distance
across different clusters we do do cause
analysis hook into that including for
example adding a cable inside a rack
inside a cluster apart which took you
roughly 10 US dollars and for for hiring
people who do the actual actual you know
to the Ettrick a bowling pins and even
after accommodating those costs we see
we still get pretty nice performance
gain those will lose you roughly just to
three percent at a scale of out of the
healthy improvement at the scale up to
3,000 servers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>