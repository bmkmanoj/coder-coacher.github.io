<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Graph Multi-partitioning and Higher Order Cheeger Inequalities; Anand Louis - Georgia Tech | Coder Coacher - Coaching Coders</title><meta content="Graph Multi-partitioning and Higher Order Cheeger Inequalities; Anand Louis - Georgia Tech - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Graph Multi-partitioning and Higher Order Cheeger Inequalities; Anand Louis - Georgia Tech</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/o3BQkHzLJvI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so so I talked about higher-order
chigger inequalities and for graphs
multi partitioning problems so firstly
what is graph partitioning right so you
are given a graph and you want to
partition it into some pieces and
generally how a measure of how good the
partition is is some function of the
edges that go between the pieces and the
size of the sets okay so one of the most
well studied problems is this partial
skirt problem where you are given a
graph and for any set s we define its
expansion as the ratio of the edges that
go out of the set divided by the size of
the set okay and we denote this by fee
of s and the expansion of the graph is
the minimum value of V of s over all
sets s of size at most n over 2 right
yeah so this is a fundamental np-hard
problem and it's sort of in of interest
to the Markov chains people because if
you were to do something like a random
walk then the mixing time of the random
walk depends on the expansion of the
graph and many of these divide and
conquer algorithms call like you first
find this partial skirt records on both
pieces and so on so trying to compute
this partial kartashov
very important np-hard problem so so
there's this thing called chiggers
inequality which is a very fundamental
way of measuring the expansion of the
graph so the chiggers and equality says
this so you look at the laplacian matrix
which is for a d regular graph it is the
identity matrix minus the adjacency
matrix divided by D so see this matrix
is symmetric and diagonally dominant
right it's easy to see that the smallest
eigen value of this matrix is 0 right so
you just take the eigenvector with the
old ones vector and that is an
eigenvector with eigenvalue 0 and a very
simple exercise to show is that your
your graph has something like K
connected components then the first K
eigen values are all 0 okay so what she
goes in equality says is that you look
at the lambda 2 the second smallest
eigenvalue
and the expansion of the graph is at
least lambda2 and at most two times
square root lambda - and the proof of
chiggers inequality also gives you an
algorithm to find the set which
satisfies this upper bound so you sort
the entries of the second eigenvector in
decreasing order let's say x1 through xn
x1 being the largest and xn being the
smallest and you look at the cuts
defined by the soldering okay so si is
the set consisting of the first I were
to seize in this ordering then the proof
essentially says that the minimum of fee
of Si over these eyes there is at most
two square root lambda two so it gives
you a simple algorithm to find a set
which satisfies its upper bound okay so
so in this talk I'll talk about two
problems you are given a parameter K and
the first problem is to partition the
graph into K pieces so as to minimize
the maximum over I of P of Si and the
second seemingly is your problem is to
find a K partition that minimizes the
total fraction of edges that you cut
okay so it's easy to see that whatever
upper bound you prove for the first
problem will also be an upper bound for
the second problem so the second problem
is an easier problem so let's do that
first
I mean so whatever upper-bound you prove
for the first problem is an upper bound
for the second problem right so the
first problem I want you to find a K
partition s 1 2 s K so as to minimize
this term max' over I of P of s I case
you partition and say you makes you much
care about the single number and in
other case in a way you care about K no
not in this particular case but there
are many questions you next see the
graph see where you want to say I mean
they've got all the numbers in not even
you would like to know how many edges go
it's been any two classes so it's much
much harder but the top one is nothing
terms of optimum right right I'm saying
it's the like a spectral upper bound you
know something like in terms of function
of K or lambda K or something when you
see up on oh is it the solution you're
talking about although not for you the
number so the total number of so suppose
you prove that this term is at most say
some alpha then I'm saying that the
upper bound here is also at most alpha
and we talk about absolute numbers not
approximation records so was I
so for the first problem so there's a
very simple recursive algorithm right so
the chiggers inequality gives you a way
to find one cut so you find that cut
remove those edges and add self loops in
their place okay for each edge UV that
you remove you add a self loop at you
and a self loop by tree and then you
repeat now we have two pieces look at
which piece has a smaller second
eigenvalue and do this again till you
get K pieces and I'll show you that
where is this very simple algorithm cuts
at most square root lambda K times log K
fraction of edges all right so this is
an absolute upper bound it's not like a
approximation factor or something okay
so to prove this is the first
interesting observation is that if you
remove any edge from the graph your
eigenvalues are only going to decrease
okay so I'll prove this for you and it's
a very simple thing so first note that
so let's see be the set of edges that
you are removing then the adjacency
matrix of G minus C minus the adjacency
matrix of G is a diagonally dominant
matrix so do you see why the adjacency
matrix of G minus C minus the adjacency
matrix of G is a diagonal dominant
matrix so which entries do these two
matrices differ in only the entries
corresponding to the edges in C right
everything else is the same
so in this difference you'll get a minus
one for every edge that you removed but
for every edge that you removed he also
added self loops both the endpoints so
you'd get a plus one in the diagonal
entries corresponding to them right so
this is a diagonally dominant matrix and
all diagonal dominant matrices are also
tightly dominant matrix is something the
values of the diagonal the song yeah so
for every yeah so tiny government I mean
that the I comma I at entry is larger
than the sum of the iro and larger than
some of the ith column item I intend to
use in other news yeah so that's you
overcast the draft
know so I'm adding Celtics in their
place so it's still D regular so when
I'm counting the degree I'm counting the
self loops also right so it's D regular
so it's diagonally dominant and positive
semi-definite okay so let K be an
arbitrary number so the K eigen value of
G minus C is given by you minimize over
the subspace of rank K and within that
subspace you maximize X transpose the
laplacian of G minus C times X divided
by X transpose X right so let me just
rear-ended summation and write it this
way right so the laplacian of g and g -
e differ in exactly this term and what
is this so this is a PSD matrix right so
this term is always going to be
non-negative so let me just throw that
term away and I get a less than equal to
and what is this term now let's exactly
lambda K of G right so I just proved to
you that if you remove any set of edges
the eigenvalues only going to increase
each eigenvalue can only decrease it
cannot increase so what does this mean
so when I'm making the I it cut in my
recursive algorithm I can bound it by
lambda I of the original graph which is
at most lambda K of the original graph
right so each cut I can upper bound by
square root lambda K times size of the
small smaller side okay so the total
fraction of edges that I'm cutting is at
most two times square root lambda K
times summation over I of s I so I
should have said this before so the
notation I'm going to use in this talk
is that si is going to be the smaller
side of the cut and si compliment is
going to be the larger side of the cut
okay so the total fraction of edges that
I'm cutting is square root lambda K
times summation over I of cardinality of
s I right so I just need to show that
now that the summation of over I of the
cardinality of s is
most the side of the graph times log K
right if I show that then I'm done so so
this is a simple counting argument to
show this I'll construct a tree I want K
nodes of size log K which has the
property that if you look at any level
the sum of weights on those levels is
going to be at most order e ok so I'm
going to construct a tree on K nodes on
each of these nodes I'm going to put one
of the s eyes so I said the trees of
size trees of height log K and the
summation of weights on each level is
something like order size of the graph
if I can show such a tree then I'm done
right well that shows that the summation
of this s is at most e times log K yeah
so as a first attempt let me use these
roots to construct a tree so I'll make
si the child of SJ if si is obtained by
cutting the smaller side of the previous
cut otherwise I'll make it a sibling of
a stream so let me illustrate it with a
picture over here so in this graph so we
put V as a root of the tree and the
first cut I make is a trivial cut so let
me just make it the child of me now the
second cut that I make is obtained by
cutting s1 complement right so using the
second rule I will make it a signal
sibling of s1 now the third cut that I'm
making is obtained by cutting s2 so by
the first rule I will make it a child of
s2 and the fourth cut I'm cutting its
obtained by cutting s1 so again by using
the first rule I'll make it a child of
s1 so at this point you should you could
probably see where I'm going with this
right if you look at any levels the sets
that I'm putting there or already
disjoint so the sum of weights at each
level is at most e by construction
right but as you can probably imagine
the height of this tree could be huge I
mean it could be as good as that's
largest key if he's time you were
cutting the same size the smaller side
of the graph right but note that at the
tree that I constructed has this point
has this property that every node is at
most half the weight of its parent right
so I can use this to shrink the size of
the tree so suppose my tree looks like
this suppose my tree has a long induced
path in it I know that each of these
nodes is at most half its parent because
it was obtained by cutting the parent
and I'm only putting the smaller side of
each cut in the tree so suppose I were
to shift all of these things up so how
much is this thing going to increase by
the summation of all these blue things
is at most the largest blue thing over
there right so at any level the
summation of H is at most going to
double it's still going to be at most
two times the size of the graph right
and now the tree also has a property
that every non leaf node has degree at
least two so it's height is also log K
right so yeah therefore we'll be all
done so the summation of all this sis is
at most e times log K and as I showed
you before the total fraction of edges
we cut us now some order lambda K times
okay any questions
yes so the only load one you can prove
is lambda2 unfortunately so this is not
tied at all
come on little buddy boy I saw the 0
then lambda K is what's a zero rate yeah
so but how do you put a function oh you
can put something like lambda k over k
if you want but that could be very small
so you cannot put a lower bound but a
tight example is that there is no tight
example but there exists a family of
crops for which this value is square
root lambda K log K the law case under
the square root okay so this is not the
key looky everything oh yes you cannot
improve that
so yeah one other thing is that any of
these the partition that you obtain here
could have the individual sets
themselves could have very bad expansion
right in general you cannot say anything
about their expansion so that's what I'm
going to talk about next so so in the
graph G if you pick any K non-empty
disjoint subsets then at least one of
them will have expansion lambda K okay
so this is similar to the easier part of
chiggers inequality where you show that
the expansion of the graph is at least
lambda 2 and probably the more
interesting part is that there exists a
1 minus epsilon k partition s1 s2 appel
has 1 minus epsilon K so it's that each
set has expansion at most square root
lambda K log K so this is similar to the
harder part of chiggers inequality where
you show that the expansion of an edge
the expansion of a set set is that more
square root lambda 2 and ya in this case
these two bounds are tied the lower
bound is tight if you take the boolean
hyper cube and the upper bound is also
tight for oh I should also say that this
theorem was independently proven by
current Lee and Travis on also ok so the
upper bound is tied for what is known as
this noisy hypercube so basically so
easily you take the vertices of a
boolean hypercube k dimensional boolean
hypercube and put a complete graph on it
and the weight of an edge XY is like
epsilon to the Hamming distance between
x and y ok so at this point it's easy to
show that the first K eigen values are
at most Epsilon and with a little bit of
work you can also show that any set of
size at most 1 over K will have
expansion at least square root epsilon
log K so this follows from some Fourier
analytic tools and most basically what
is known as the reverse wannabe Beckner
inequality so if you find a K partition
then at least one of these sets is going
to be small
than one over k therefore this is really
the best you can prove for a k partition
okay okay so first let's do the lower
bound the lower one is easy part so you
want to find suppose you let s 1 to s K
be some K disjoint subsets and again so
we know that the lambda K is obtained by
minimizing over rank K subspaces and
within that subspace maximizing X
transpose L X divided by X transpose X
so what is it T that you are going to
plug in over here well there is only one
thing you can do right so you take T to
be the span of the characteristic
vectors of s 1 through 2 s K and this is
going to have rank K and at this point
you can essentially show that the vector
which maximizes system has to look like
one of the characteristic vectors of
this set or something like very similar
to that so that is roughly a main idea
of the proof ok
okay so 4k partition I told you that you
can obtain a 1 minus epsilon K
partitions is that your expansion is
like Polly 1 over epsilon times square
root lambda K okay so you really cannot
do away with the Polly one over epsilon
over here you really need that thing
over there you know because because in
general if you wanted an exact K
partition then there exists a family of
graphs for which max of max of I over
few V of s I can be much much larger
than square root lambda K you know as
large as K square times K square divided
by square root n so the family of graph
essentially looks like this you start
with a k cliques and a central vertex
and then you add edges between the
central vertex and every other vertex
individually with probability P okay so
I get an upper bound on lambda K from
this from the inequality I showed you in
the previous slide know these K sets
have small expansion therefore lambda K
has to be small the size or clicks
complete graphs no edges between this
central vertex and every other vertex
with probability P okay so you can say
you can add well I wanted to make it now
I want you to make it an unweighted
graph if you are fine with weighted
graphs then you can just add put weights
of H P over here
now I just wanted to make it an unrated
drop so that was the point behind this
quick identical right so instead of
Bayesian random you can just have the
stickers so first first if you had and
just do one vertex over here over here
yeah right so well so if you add
vertices so I really want this to be
uniform because you know if you do
something like that add vertices to some
vertex over here then huh so if you if
you do it like this and it's easy to
argue what this value is going to be
it's easy to prove a lower bound on this
I mean it should work if you do it that
way as well but I'm just saying you know
regular things look nicer I'm king of
the must be think of them as ways think
of this all these edges having you
gateways to the edges so that's a single
graph but hey more of a description on
graphs but all of those graphs are
isomorphic okay so let's just assume
that it's a these edges have weight P if
you are fine with weighted graphs
okay so in that case you can show that
so the center in the partition that
you're going to produce you'll have to
put the central vertex in some piece
right and whichever piece you put it in
you're going to pay a huge expansion
because the central vertex has a lot of
edges incident on it now so suppose you
put it in Si then fee of si but the
central vertex is going to be like K
times the expansion of the rest of the
pieces and this fact is enough to get a
huge gap over here where appropriately
choosing the value of P it's the point I
want I'm morning wanted to make is that
you really need this poly one over
epsilon here you cannot play some small
constant over there I mean as you get
closer and closer to K that thing has to
blow up okay okay so let's see how to
prove this so Chi goes analysis
essentially shows that you don't need to
start with the second eigenvector you
can start with any vector X which has
small support and you can obtain a set s
which lies in the support of X whose
expansion is at most this quantity over
here okay so this quantity I would like
to think of it as the average distortion
of the l1 embedding given by this vector
so so the embedding is a canonical
embedding right where you map vertex I
to the value X I so then this term X I
minus XJ is like the distortion of this
edge and what this term is like under
some weird normalization this is like
the average distortion of this embedding
so she goes analysis essentially says
that you can start with any vector and
find the subject set s which lies in the
support of this vector and whose
expansion is at most the average
distortion so if I can find theta K such
vectors each having disjoined support
and each having average distortion small
average distortion then I'm done right
because I'll plug in this lemma to each
of these vectors and each of them will
give me a set right
so that's going to be my main goal for
the the rest of the stock to come up
with theta K such vectors which have
this joint support and have small
average distortion okay so I don't know
how to start with such a vector so but
the top key eigenvectors give me an
embedding into R to the K I'd the
canonical embedding where you map the
vertex I to the vector corresponding
consisting of the iaith coordinate of
each of the eigenvectors okay so this
embedding satisfies some very simple
properties first one says that the
summation of lengths of these vectors is
equal to K right so this just follows
from the fact that you have K eigen
vectors each of them having length 1 and
probably the most interesting property
is this that the average in a product
square is equal to K so this again
follows on the fact that the K eigen
vectors are orthogonal to each other but
the reason why I want you to notice this
is that if you were this is a very
strong property it is not satisfied by
say some random vectors in all to the K
you know so if you were to pick some
random vectors and R to the K and
normalize it in this form you would get
that the averaging of R square is like K
square so this thing being equal to K
indicates some of that there are already
some clusters among these vectors which
we will crucially exploit later on and
this thing is also easy to see that the
average distortion of this embedding is
at most lambda K right
well this follows from the fact that
each coordinate has the source in at
most lambda K because you know they are
the first K eigen values okay any
questions
okay so the rounding algorithm is very
simple
probably very simple you pick G random
Gaussian vectors G 1 through 2 - GK let
x1 be the projection of all the VIS on G
1 now let X 2 with all the projection of
all the VIS on G 2 so on let X K be the
position of all the VIS on G of K and we
wanted to make these vectors disjoint
support right so what's the most obvious
thing you would do you know go to each
row and zero out all but the largest
value okay so let me look at the first
row let's say the first value is the
largest value so I'll keep that and zero
out everything else in the first row
then I go to the second row keep
whatever is the largest value and then
zero out everything else
and I do this for each row okay so yeah
so now I have K vectors each of which
have disjoint support and now I need to
show that all of them or some fraction
of them have low distortion right so the
algorithm clear to everyone or any
questions okay so so and so why does
this algorithm work so an intuitive
reason why this works is that so if you
think of the blue vectors as the VIS and
the brown vectors as the gaussian
vectors that you picked then essentially
what the algorithm is doing is that the
support of your vectors is going to look
like the support of x1 is going to look
like this thing this cluster around g1
right roughly or mostly and the support
of g2 is going to look like the cluster
around g2 and so on right
oh yes I'm finding Wales to square yeah
so I'm finding these clusters in so I'm
just saying that this algorithm that I
did before is going to find these
clusters roughly or you know some large
fraction of this and which is roughly
what we want to do right because I want
to find these clusters and if the
classes are sort of close together then
they would make give us like good sets
you know and so on now that's what I'll
prove okay okay so roughly the analysis
is going to go like this so recall that
to show that so this is the term average
distortion and I want to show that this
is small for a constant fraction of the
indices right so I'll show you that the
expectation expected value of the
denominator of each of these vectors is
like log K and the expected value of the
numerator is like square root lambda K
log cube K okay so their ratio is like
what we want so is this sufficient for
us probably not right so I'll also show
you that the expected value of the ratio
is bounded by some constant times the
ratio of the expected value for some
constant fraction of indices which is
like square root lambda K okay okay so
let's do the denominator first that is
easy thing so again keep this picture in
mind so this is the first entry and I
did not zero this out and I zeroed out
everything else from the first row right
so what is the property that I did note
zero this out well now all the gi's are
independently chosen so any of them
could have been the largest probably
that this is large this is one over K
right and okay so let's say I did note
zero this out with property one over K
and suppose I know that this is the
largest in this row what does that
expected value look like
so this is simple exercise so if you
were to pick a standard normal random
variables and look at what is the
expected value of the max it is I'm
sorry less than equal to 2 okay okay
so keeping this in mind I get that the
expected value of the first entry or the
expected value of the ayat entry in the
first column is VI squared log K I'm a
condition on it being non zero condition
on it being largest in its row okay so
it's overall expected value is exactly
this term divided by K the expected
value of the item is norm of VI squared
log K divided by K so to compute the
expected value of the denominator I just
sum this over all entries right and
recall that I had normalized the VI in
such a way that summation over I of di
VI square was exactly equal to K so the
denominator is exactly equal to okay
okay any questions so this was easy part
so the numerator requires a bit of work
so the numerator consists of terms which
look like X I and XJ right X I minus XJ
and as before I can calculate the
expected value of x I when X is nonzero
and I can calculate the expected value
of XJ when X I XJ is nonzero right so
your X I XJ are X I and XJ are like norm
of V I squared times this V I told G 1
squared if this thing is the largest in
its row against its world all this I
want you to keep this picture in mind
this is the projections and this is the
thing that I did not see row out okay so
X is like normal VI square times this
random variable if it is largest in its
true otherwise it's 0 and same thing is
X J so I need to calculate so there are
4 cases here
case 1 both of them are 0 which case I
don't care they're both of them are 0
case 2 is and both of them are nonzero
well I'll ask you to believe me on this
but the probability of that happening is
small so that is probably not something
you should worry about the most
interesting case is when one of them is
zero and the other one is known zero
okay so we need to bound the probability
of that event happening so so this is
what it looks like so there's this unit
sphere VI tilde and VJ tailed are
vectors on this unit sphere and I want
to calculate the probability that XJ is
greater than equal to zero and X is zero
right
so suppose g1 were to look something
like this so when would XJ be nonzero so
this would roughly be nonzero when you
know BJ told aligns itself with aligns
very well with g1 right so just keep
this picture in mind so this was my
rounding algorithm so so when would be
one G ones be the largest in its row and
also fit is the largest among all these
projections that means v1 is aligning
very well with g1 or at least better
than how much it's unlike aligning with
all the other G is right
so if say XJ is going to be nonzero then
it means that most probably it is going
to lie in the shaded region around g1
over here right because if you could say
that if it does not lie in the shaded
region then with high probability it is
aligning with much better than with some
other GI okay so to calculate the
probability that XJ is greater than zero
and X is nonzero
so that should depend on the distance
between these two vectors right if these
two vectors are very close to each other
then they would behave the same then
these two random variables would behave
the same these two vectors are
orthogonal and they just behave
independently right so using this you
can show that the probability of a cut
being made the sense that probably that
one of them is
and the other one is non-0 is upper
bounded by one over K times the distance
between these two vectors times square
root lambda K so the square root lambda
K comes because it is like the size of
the ket cap on the unit sphere okay and
this was formally proven by a Cherica
McCarty Kevin McCarthy chef any
questions
okay so moving on so now I'm ready to
calculate the expected value of x I
minus XJ
so when X is say 0 and XJ is nonzero the
expected value of x say looks like this
VJ square times log K the probability of
that event happening is this and when
the opposite event happens X is nonzero
XJ is greater than 0 the expected value
looks like this and again the
probability of that happening is this
right so well I'll ask you to bear with
me on this slide this is the only
satellite that has mat on it this much
math on it so by doing some sort of a
simple rearrangement I can upper bound
it by this term 1 over K times the sum
of length times the distance between
them times the log cubed K so I'm
collecting this square root loci and
this log K ok so at this point whenever
you see a term that looks like this it's
sort of asking for you to plug in Cauchy
Schwarz inequality right from this over
all edges and up like Cauchy Schwarz
over there so what do you get the 1 over
K stay that is as it is you get
something like VI minus VT whole square
summed over all edges times summation
over I VI square right and what was this
value so if you remember this picture
that I showed you before the summation
of the edges of VI minus VJ square is
lambda K times this thing and this thing
is equal to K so summation over I
summation of over edges of VI minus VT
whole square is lambda K times K okay so
let me plug that in over here so there's
this lambda K times K this thing was
also just K so the square root K and K
cancels and you are left with square
root lambda K log Q okay okay so any
questions okay so let's do a quick
overview so we bounded the probability
that X I one of them is zero and the
other one is nonzero which was this and
we know how to calculate the expected
value of one of them
and the exact value of separately and I
didn't show you what to do when both of
them are nonzero but that is that is a
very simple case and happens with very
small probability so you can ignore that
so once you do know how to do this you
just sum them over all edges up like
coffee Schwarz and then you're done okay
okay so till now I showed you that the
denominator is log K and the numerator
is square root lambda K log cube K and
now I'll show you that for a constant
fraction of their indices the expected
value of the ratio is bounded by some
constant times the ratio of the expected
values which is square root lambda K log
K okay
so again so in this point we will
crucially use this property that we had
of our embedding that the average value
of the in a product square is like 1
over K and so this already indicates
that there is a high correlation in two
clusters right so as a thought
experiment just consider the following
case when any two of the VIS are either
same or orthogonal to each other okay so
any two VI and VJ they are either equal
or they are orthogonal to each other so
in this case I claim that you the
rounding algorithm is like throwing K
balls into K bins so what are the balls
and bins over here
so the cave-ins are the k vectors that
you have and the K balls are the K
clusters of the VIS that you have right
and why is this like throwing balls into
bins so if you recall what a rounding
algorithm did
so you took projections of some VI onto
the GIS and whichever one was the
largest you sort of assigned it to that
vector right so the vectors are like the
bins and I say that my ball has gone
into some particular bin if the vector
that I picked is largest in that in that
there's a VI that I picked is largest in
that GI right so the rounding algorithm
is sort of like throwing K balls into K
bins and what happens when you throw K
balls into K bins you would expect that
something like 1 minus 1 over e fraction
of them are non empty right assume for
this experiment consider thought
experiment where two V eyes are either
equal or orthogonal to each other
so in this case yeah the when when one
vertex goes the whole cluster goes with
it
and since they are orthogonal they are
independent this is so this is just an
intuition and so in this case you get
that a constant fraction of the
denominators are large you know like
whenever one pole goes into a bin that
Bennis like large and so this is the
intuition behind trying to show that the
denominator is large but unfortunately
this does not work it would have been I
mean we could not make it work it
probably works I don't know we have to
go through the boring way of trying to
bound the variance of the denominator
okay so what does the variance look like
so the variance is the summation of
expected values of X I and XJ and what
does this term look like so we know what
is the expected value of x I when it's
nonzero it's something like VI squared
times log K we know the expected value
of XJ when it's nonzero
it's only like VJ square times latke and
what we need to bound is the probability
that both of them are nonzero right so
again remember that recall that X I was
this random variable if the projection
on G one was the largest in that row
similarly XJ so so I'll show you that
the probability that both of them are
non zero can be upper bounded by the
inner product of VI tilde and VJ tilde
whole square divided by K plus 1 over K
square ok so again think of this picture
in mind so again so let's consider the
two boundary cases when VI tilde and V J
tilde orthogonal to each other
okay so in that case what is the value
of this probability you know so if bi
tilde B J tilde orthogonal to each other
then these two random variables are
independent random variables right X ax
a are they totally independent random
variables so the probability that both
of them are greater than 0 is 1 over K
Square and the other boundary case to
look at is say when VI tilde is equal to
VJ tilde so in that case what is the
probability that both of them are
greater than 0 it's just 1 over K right
which what we have over here so
essentially what this statement is
saying is that the general case is
convex combination of the two boundary
cases the two boundary cases being when
once when they are both completely
independent and one when like completely
correlated so so proving this requires a
bit of work and I'm not going to prove
it for you did you start to believe me
so roughly a high-level idea is that you
can think of each X I as a function from
the Gaussian space to the real line
right so because each X I is a function
of the V I and all the
G 1 through 2 G K and you can and you
can write this function in terms of its
hermite basis
you heard my polynomials they form a
complete eigenbasis for the gaussian
space so this time some Fourier analysis
tool analytic tools on top of this gives
you this statement essentially so but
yeah
proving that requires some amount of
work but anyway so let's assume this and
try to finish the proof of the variance
so I have B ice so I had that VI square
norm of EI square times VJ square times
log square K times this term over here
so rearranging summation I can write
this as in a product of VI VJ whole
square divided by K plus summation over
I of VI square whole square whole square
right so what were these two values that
we had so again going back to this
picture
note that summation over IJ of bij whole
square is equal to K and summation over
I of norm of VI square is also equal to
K so if I plug this in over here what do
I get this is equal to 1 this is also
equal to 1 so your variance is is 2 log
square K right so any questions at this
point so here's the most crucial part
here is this is where we crucially use
the fact that the average the total
inner product square is small you know
so this sort of indicates that your
vectors are strongly correlated and this
is where we are using this correlation
you know so if you were to pick say K
some end random vectors in K dimension
they do not satisfy this bound and that
is what we are crucially using here ok
so at this point we are sort of done
right I showed you that the expected
value of the numerator is square root
lambda K log cube K so using Markov
inequality something like 99% of the
denominator numerators are smaller than
something smaller than this smaller than
some hundred times this
right and I can bound the denominator by
using chebyshev's inequality right so
the expected value of the denominator is
log K the variance is log square K so
the probability that the denominator is
at least log K over 2 is something like
you know 1/4 so a constant fraction of
your denominators are large if we're
taking the Union bound over these two
events there are at least likely over
eight such indices for which the
numerator is small and the denominator
is large and that's it so we are done
any questions
all the constant yeah
my Proteus
right so well I've actually proved a
bigger theorem for you I told you that
you can get one minus epsilon times K so
for to do that you need to do this like
much much carefully but right now this
proof only shows that you get like some
constant times K sets yeah so that
requires a pretty much the same ideas
but a little more carefully way careful
analysis so you can also ask this
computational question that given a
parameter K I want to find the best K
partition so as to minimize minimize the
maximum expansion alright so this can be
thought of as like a extension of this
partial Scott problem so this is
actually a very tricky problem because
you know so any of the standard SDP
formulations that you try they can have
an unbounded integrality gap you know
they can be as large as you want them to
be so standard integrity gap is that you
would have like one indicator vector for
to indicate which set you'd have like K
vectors for each vertex to indicate
which set that your vertex belongs to
and using some you can really show that
this has an integral to get back as big
as you want and so I not say this in
much detail but cost you and I we came
up with earnest interesting SDP
formulation which inherently takes care
of all the problems that we seem to have
with the standard SDP formulation and
using a similar rounding algorithm we
can we can get an a we can get an
algorithm that outputs 1 minus epsilon
times K sets each having expansion up
times square root log n ok so more as
you are noted so any upper bound so an
approximation algorithm for this does
not imply anything for them in some
partitioning okay so yeah that's all I
have to say and probably the most
pressing question is can you get can you
lose this by criterion S in the
approximation that I had can you get
like exactly K say
okay non-empty disjoint subsets such
that each has expansion square root
lambda K log K there that I was that was
for partition so right now I'm just
saying that they should be K non-empty
disjoint subsets I'm not insisting on a
partition so this could just be like k
non-empty disjoint subsets so if you
remember the example that I showed the K
clicks would themselves be these K sets
that you want and so another problem
that is of interest related to this as a
small set expansion problem we are given
a parameter K you want to find set of
size n over K which has the least
expansion so what I showed you before is
that so what I showed you today it also
implies an upper bound for small set
expansion right if you find a k
partition then one of those sets is
going to be smaller than n to the it's
going to smaller than n over K so for
small values of K it is fine
but for large values of K it is it can
be improved
so like Arora Barak and Sawyer showed
that you can upper bound this fee small
you can find a set of size n over K
whose expansion is at most lambda sub K
to the C where C is some constant like
65 times square root log n base K so
here so this is interesting if your K is
like some n to the epsilon
well you essentially lose the log n to
the base K factor here so if you are
interested in like sub exponential time
algorithms for unique games then the K
that you are going to plug in is going
to be something like n to the epsilon so
in that regime this is more interesting
then the then the theorem I showed you
and and that's it thanks for coming and
I'd be glad to take questions
and you always examine
the number of edges from si the rest of
the graph are there any results in which
you care about the edges which in Si and
SJ h IJ which would be much that would
be like some regularity partition thing
like similarity regularity and stuff
yeah I haven't seen any problems like
the points that you described what you
want a small expansion into each right
which is a much thicker for the
ColecoVision
so you want to get the maximum disk a
square pairs for example I take some
some some some know what norm of the
secret so whatever they are mean
millions of treasure what you want to
get this case the K square dimension
vector do some yes
even just maybe every minute is much
much harder
yeah I doubt if it has any connections
to the eigenvalues but obsession with
all your videos I mean it's nice to have
okay so but but but why what's one same
I miss the aim to connect again the case
I can value to perficient's or to learn
about partitions well the reason why we
started the pigeon entered I saw your
aim is to learn about partitions yes I'm
just saying yeah so right well yeah the
reason why we started looking at this is
because of its connection to unique
games so so I so generally if you said
like I said so if you get like better
bounds for this you could get like
slightly better algorithms for unique
games so that is why you know all this
and that is why this mod set expansion
problem game fame in the first place so
that's what people to actually practice
to colleges in Vienna also there are
templates for the methods so you know
one of the things that seems to be we do
it because we can because maybe be
curious those out that every
so well they can kemberly pick something
no questions meeting more suggestions of
your problem better
the fingers continue</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>