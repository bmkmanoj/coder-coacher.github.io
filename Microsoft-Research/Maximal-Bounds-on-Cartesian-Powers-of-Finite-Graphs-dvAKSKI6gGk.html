<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Maximal Bounds on Cartesian Powers of Finite Graphs | Coder Coacher - Coaching Coders</title><meta content="Maximal Bounds on Cartesian Powers of Finite Graphs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Maximal Bounds on Cartesian Powers of Finite Graphs</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dvAKSKI6gGk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright good afternoon everyone we're
happy to have Jordan Greenblatt update
us on the latest in this topic of
maximal inequalities on cartesian
products topic which was essentially
started here few years back when
alexandra Cola was a postdoc so anyway
we're happy to hear Jordan thank you
thank you for the invite and thanks
everyone for coming so the the topic is
sort of a funny split of classic
harmonic analysis and graph theory so
since I assume most people here have a
better graph theory background I want to
explain some of the harmonic analysis
sort of classical context for this and
and lead it in just to give some some
motivation so one of the most important
objects in all of harmonic analysis is
the hardy little wood maximal operator
and this will be the Euclidean one on rd
and they're going to be some minor
technical assumptions on f but since
we're not really talking about euclidean
space I'm just going to completely brush
this over so this is defined as the
maximal average over all spheres
standard at the point X of the function
f its identity all our functions are
going to be non-negative but i'll try to
remember to put in absolute values if i
don't just assume they're there in as
far in as they can be there are some
silly conventions but it doesn't matter
for all we care if they can all be
non-negative and i'm going to use em for
maximal functions throw out if it's
ambiguous because there's more than one
in one section i'll just use subscripts
or something like that okay so yeah this
is the the maximal average or supreme
will average overall ball centered at a
point
for an example in dimension 1 we have
the indicator of an interval so f looks
like you know it's just a step and then
MF the maximal function looks a bit
different so in the interior here it's
going to equal one because you can just
take a radius small enough that the
entire ball is contained in the interval
so you get this and on the boundary
sorry on the boundary it's a half it's
pretty easy to check and then it decays
linearly so by this I mean it's bounded
above and below by constant multiples of
1 over X resent your inverse linear
sorry I would have said grows otherwise
I didn't want to double negative ok
sorry it grows in verse linearly um ok
i'm done i'll get back to this actual
talk so two things to notice here one is
that it's an l infinity contraction so
if F is bounded right no matter what F
is if F is bounded then all of its
averages share the same bound and so the
maximal function also does however not
only is it not Mel one contraction is
not even bounded no one right this is a
nice integral function it has total mass
two but this one has has poor decay or
inverse growth and as a result it just
goes to slowly off to infinity to to be
integral and so a natural question is
what about 4p in between and this is the
first classical theorem we're going to
talk about this is the hardy little wood
maximal theorem which says
that for all p greater than one is
actually specific part of it but but
it's the part that's relevant to us for
all p greater than one the maximal
function is bounded up to a constant in
LP by f this this dependence on p is
inevitable as that picture would suggest
as P goes down to one the the bounds are
going to go to infinity I'll get back to
this dimensional dependence in just a
moment this was later strengthened by
stein i think rather remarkably to the
spherical maximal operator which is
exactly what you'd expect here we're
looking at spheres centered at x surface
of spheres rather than interiors of
balls keep in mind by the way that this
operator pointwise dominates the hardy
little with maximal operator simply
because an average over a ball is just a
weighted average of averages over
spheres so this is a this is a much more
dangerous operator I guess for lack of a
better way to say it what he showed is
for aldi greater than or equal to 3 this
is later extended to two by borgen p
greater than D over D minus 1 test on
delta's see why that's necessary we have
an LP bound again and the constant will
be dependent on on p originally the
dependence was very bad oh sorry to the
dependence is very bad right so there's
there's bad dependence on p but
remarkably as long as p is greater than
this threshold we do have this bound and
also the the constants in the inequality
can be taken to be non increasing in d
once the dimension is high enough for a
given p that it crosses the threshold
these constants are not increasing
simply be
cuz you can view the the averaging
process over a sphere and dimension D
plus 1 as to two random processes
picking a random direction in the sphere
of dimension D plus 1 and then averaging
over the cross-section picking a random
element the cross-section and those two
combined it inherits the bound it's
actually very elegant method called the
method of rotations um so oh this is the
stein he's an important guy but still
seems weird and so in particular this
lets us get rid of this dimensional
dependence the original dimensional
dependence from the classic proof is
actually exponential it's it's a nice
proof but it's but it's ugly dimensional
dependence but here for a fixed p
greater than one up till up until d
crosses this threshold and cuts under
cuts under p we can just use the
classical constants its finitely many
cases and then we can just use the
constant that comes for all dimensions
higher for the for the spherical since
it dominates the ball and this is the
sort of thing we're going to be talking
about today dimensional constants for
spheres in the setting of finite graphs
rather than euclidean space but before i
step forward I want to say since I
imagine this is new to some people that
it's not just contrived there are
applications in fact this is like I said
one of the most important tools and
analysis one reason is for point wise
convergence questions so the first thing
I learned and I think a lot of people
learn as a corollary which you've all
put it out was not the original proof
but implicit in the original proof and
is in the actual original proofs of a
lot of other point wise convergence
things questions is the lebesgue
differentiation theorem which says that
if we average a ball sorry average the
function f over a ball centered at x and
send the radius to 0 then this limit
exists and is equal to f of x for almost
every X so if you like almost surely
over any reasonable probability
on euclidean space and and this is sort
of remarkable if f is continuous this is
the fundamental theorem of calculus if f
is you know hideously jagged that's uh
that's not at all obvious I think for
almost every ax I um and this this
uniform control that comes from the
maximal bound is what what gives us this
now in a discrete setting you know
point-wise convergence is less
meaningful but what's not is
probabilistic bounds so here's where
I'll introduce our first non classical
theorem which is a the work of haro kala
and shellman that you've all alluded to
earlier and this is on the hypercube
which we're going to write as K 2 to the
N because we're generally doing it for
clicks but we'll use this funny notation
on k 2 to the N we define PK to be the
spherical averaging operator expectation
of a function over the sphere around X
here K has to be an integer from 0 to n
because that's the diameter of a
hypercube and our spherical maximal
operator is again what you would expect
it's the maximum over K of what they
proved is that there is a dimension
friel to bound in other words I mean for
fixed hypercube it's a it's a finite
dimensional space it's not particularly
interesting but asymptotically there's
no dependence on this dimension n which
is a which is a rather remarkable and
non-trivial phenomenon and the way this
came up just to give some motivation was
in the context of alexandra looking at
the UGC unique games conjecture rather
here at Microsoft right and so without
going into the full details of the
original problem I'll give some some
intuition for how it came up and how
this sort of thing might come up
naturally and the image she had was you
know imagine you have a hypercube and
there's there's some information spread
out around the vertices and the way you
can find out information is by starting
at any seed point on the hypercube and
work outward so you pull all of the
neighbors of your seed point and get the
information there and if you get get
good information you can use that to
examine the the sphere of radius 2 which
you can use to examine the sphere of
radius 3 and so on now imagine there's
an adversary who wants to obscure this
information and they have a limitation
on the amount of bad information take
these turns the grain of salt I'm trying
not to be too quantitative in the
interest of saving time but um but but
you as the player have control over how
much bad information they can spread you
know thinking of it in terms of mass
sort of vaguely um and what they want to
do is spread bad information so here
I'll use these chunks to represent bad
information they want to represent bad
he wants to spread bad information in
such a way that eventually starting at a
given seed point when you survey a
sphere you get a large portion of
misinformation and then it ruins the
process for the rest of your search of
the graph radially outward and so you
know imagine we started at this seed
point here there's a little bit
information but it's not enough to throw
us off our track here on this second one
there's a there's a little more but
maybe it still doesn't hit that
threshold but here on the sphere of
radius 3 we see a ton of bad information
and there's so much don't we look to the
sphere of radius 4 and onward it just
ruins any hope of getting the right
signal meanwhile since since you as the
player are allowed to pick any seed
point which are adversaries challenges
here
is to spread the small amount of mass
around in such a way that no matter what
seed point you chose eventually there's
a sphere generally not at the same
radius but eventually there's a sphere
that that has a high probability of bad
information so say at this point in the
sphere of radius 1 we have this massive
glob of bad information and since the
sphere of radius 1 is small that ruins
that seed point and so the adversaries
goal is to limited spread the limited
amount of bad information in such a way
that it will radially spread to any seed
point and and pile up there but this
says that it can because as the player
we pick the amount of that information
the mass of the bad information low
enough such that F here is the the
function describing the you know mass of
that information to given vertex this L
2 norm will be small which means this L
2 norm will be uniformly small right so
we can pick the same parameter for all
the hypercubes which is part of the
problem and that means by chebyshev that
there has to be some point actually a
lot of points with small maximal
function so that means that the average
is over all circles are pretty small so
what this did in practice was eliminate
a bad research direction so it did serve
a practical result but it didn't give a
positive result it just narrowed the
search on the other hand I I mentioned
this partially because I think it's very
believable because you know the idea of
signals emanating radially is a very
natural one this gives some some idea
that that this could be useful in other
contexts so uh oh and I forgot to to
follow through on what we're doing today
uh later on in the same year this is
published in 13 uh my colleague been
Krause show that for all p greater than
1 we have a dimension free downed and
that well the week type endpoint fails
but
I'm not going to mention week 11 anymore
because I realized that's whole can of
worms but he showed that there is
depending on p and it will explode as T
goes to one dimension free bound and
then together with a alexandra and ben i
joined and we change this to arbitrary
cleats where it will depend on the size
of the clink it will depend on p but it
won't depend on dimension and so
throughout throughout the rest of the
talk we're going to think of that the
size of the base click is fixed and what
we're what we're kind of trying to deal
with is dimension going off to infinity
okay now that the i already mentioned
steins work stein looms very large in
this and the sort of framework for the
whole theorem came from an old paper of
his i think from 61 and the theorem was
the papers on the maximal ergodic
theorem so it's sort of a variant on
organic theorems if t is a self-adjoint
markov operator
and m is the maximal operator given by
the largest power for a given X the
largest power of tea then we have a
dimension free or not dimension free we
have a universal LP bound um so I think
this is kind of a remarkable result and
given that it is the framework for this
the theorems we're talking about today
we would wish that for all spherical
averages there's some corresponding
index such that we have a point wise
bound of the form for non- functions PK
of f is bounded up to a constant by some
self adjoint markov operator which i
mean here we'll take the the is the
stochastic adjacency operator so this is
just a random walk this is kind of the
canonical form here it would be great if
we could say that this spherical average
is pointwise dominated by this but
unfortunately it's not actually true and
it's not actually that hard to see why
so each of these operators is given by
the expectation of F over a radial
probability mass radio with respect to X
that is we can see what these
probability masses are how they act on
the spheres so keep in mind by the way
since the the cliq power is vertex
transitive it doesn't actually matter
this Delta Center directs all spheres
are the same up to automorphism I mean
all seriously given radius or the same
about a morphism and so this is pretty
simple this is just a delta right this
gives mass one to the sphere of radius k
and that's zero to all other spheres
with this guy will do so if you one kid
you this yeah so what I'm imagining here
is just a random walk of some length so
so here k tailed is standing in for any
length random walk and I want to show
that it's not going to be possible I
mean show weekly that it's not going to
be possible to find a length such that
if we take a random walk of that length
that probability mass is going to
dominate this one up to a constant so
maybe it'll be clear after I draw the
next graph if not please ask um so we'll
get something with some some you know
central limiting Gaussian looking
structure from a random walk and it is
possible if Big K is small enough that
we could actually pick a long enough
random walk that this peaks at that
sphere right that the expectation of
this of this process is this radius but
even so we're going to be capped at
1-over rad cane ass to that sphere does
that clarify what I'm getting at ok cool
thank you um the point is if you have a
function with a lot of mass concentrated
at the case fear this will see that mass
much more than this probability mass
well so this is hopeless but we are
inspired to compare averages and it
turns out that if we compare averages of
these with something like averages of
these we can get such a domination on
the other hand it's not clear why
dominating azzaro means we go define a
second of these fear achill averages
would tell us anything about the
spherical averages so we're going to
come back to this to these probabilistic
bounds in a moment but first we're going
to see how to split up
the spherical average is in how to split
up the spherical averages into a into a
smoothed out part given batches are all
means and and a part that can be handled
with spectral techniques so for a fixed
big k if we take the jizz oral mean of
the first big case vehicle averages and
look at the difference after the Apple
summation formula and a little
application of koshish Schwartz cozy
Schwartz in k this is still a point wise
bounded x x is fixed throughout this
computation but koshi Schwartz and K we
get this is equal to the sum from k
equals 1 of I'm writing it in this forms
that you can see exactly how you would
derive it using cosi Schwartz times the
square function k equals one of the big
k read k
this first factor is actually less than
equal to 1 not even up to a constant and
so the upshot of this is that we can
bound for all Big K we can get the
uniform bound at the spherical average
is pointwise less than which is ro mean
plus the square function which is a pain
to rewrite in particular we can get rid
of the dependence on K on the right side
right if we supreme eyes / / Big K here
we get I'll define these in a second the
smooth maximal operator and these are
all non-negative some ants so there's no
harm in sending big k up to big in where
we have the smooth maximal operator is
simply the average of the sorrow means
Jesse average thank you that would be
the second design uh yes the maximum of
the Jews are 0 means up to Big K and
this square function down here is this
extended up to a big up to N equals 1
now at the moment it might not which
might be an understatement it's probably
not evident why this is a useful way to
break it up hopefully it will be by the
end but the motivation is that these
smooth kernels are much easier to bound
from this probabilistic standpoint we
will be able to do that in the way that
this failed whereas the square functions
play very nicely with l2 as square
functions are wont to do and we'll be
able to use spectral methods there and
between them will be able to get taking
a supreme over K that the spherical
maximal operator is bounded in l2 just
take a sin in this point why is bound to
a2 + l2 bound by the smooth plus the
square function so let's put the square
function off to the side right now we'll
come back to this at the end and finish
up with the smooth maximal operator over
here yeah so I want to leave these up
what about k coefficient yeah the k co
efficient should be there because you
didn't cover on this left over it's
squaring up kicks way
we got a que from the avila summation
formula we put half of it in the fact
that disappeared in half of it here by
the way if you want to reference on that
that whole thing is in the stein maximal
ergodic theorem paper it's in our local
and showman they have reviews of a lot
of the stuff um ok now for any we go any
further I just want to draw out the
probability mass induced by these smooth
kernels which look pretty simple it's
just a uniform but I want to just view
this is on the order of 1 over K since
that's all I care about it's just a
uniform selection of the first Big K
spheres and then 0 after that ok um this
leads us to what I'd said earlier that
we would be sort of borrowing from
Stein's techniques if not black boxing
his his result and so we start with the
same seed bound that he does which is a
much older theorem of hop Dunford and
Schwartz which says if I said this is a
continuous version there's a classical
discrete version and this follows just
by looking at Riemann sums and limits if
we have a semigroup is a semigroup of
markov operators
so this means if you're not familiar
just that the operator T plus s is equal
to the operator T can post the operator
s if you want just think this is a time
evolution right if you freeze time after
s seconds start it in the freezer two
seconds that's no different than just
letting it run for s plus t seconds um
and that is sort of the intuition we
have in our semi group so if we have a
continuous semigroup of markov operators
I didn't say continuous this is trivial
for us because all of our spaces are I
mean they're getting larger but they're
all finite internet themselves so for
soft reasons continuous is not important
but I'll leave it up there so if we have
this family and we define the following
maximal function so this is time average
we have this maximal operator then
predictably we have that NLP and
equality so depending on p for all p
greater than one this is could almost be
viewed as a corollary the stein proof
from earlier except that these don't
have to be self adjoint but that's dumb
because that's like saying that you can
build house lamps because you can build
lasers this is low-tech used to prove
that that's why I bother to restate it
so so our goal then is to compare this
probability mass to a probability matt
radio probability mass generated by
something like this and the way we do
this is by introducing the noise
semigroup so I think Mona should race
I'll race over here
so the noise semigroup this is just on
the click the base graph is
computationally given by e to the
negative 2 laplacian with the
conventional question is positive
definite I've seen both conventions but
but but more geometrically intuitively
we can view this as a very natural
process a diffusion process right so if
we look at k4 right this is the mass
that's induced by placing a delta at one
of the vertices and then just letting it
diffuse naturally and it treats all the
other vertices equally because it's a
click this won't be true if we look at
other graphs but in this case it has a
nice simple expression and we're
actually going to linearize it for um
say t less than small enough see we can
basically write this up to harmless
constants as a convex combination of the
this year being the spherical averaging
operator on the cliq so sarcastic
adjacency if you like
linearizing at 0 we get um we can we can
use this as the noise operator keep in
mind by the way that when we look at the
smooth maximal operator this is what
we're trying to bound in LP now it's
good enough to do this up to CN the
reason being that if we look at the mats
over K greater than equal to CN we can
just bound this crudely let's say look
at P norm this denominator takes care of
everything it is less than or equal to
well we here we have n plus 1 here we
have n plus 1 copies of something the of
an LP contraction and so yes so there's
no harm in just truncating our smooth
maxvill operator at a multiple of n past
that it becomes trivial and that's why
this linearization is really does no
harm so when we look at what our noise
operator does when we apply a noise
operator to a to a click power we're
just making of tensor tensor powers of
this operator right so if we're looking
at let's say k 3 to the five so we have
five components each of them with a k3
in it we started a point X which is some
product and then you know start the
clock as T increases all we're doing is
steadily moving the mass from here
to hear independently in each component
so in particular this gives us an iid
the radius of the outcome is an iid some
of iid variables and this induces the
following probability mass so here we're
going to say for now just fix little k
less than or equal to big k everything
about fixing a big k here the only
spheres that are seen by this
probability mass are spheres with radius
less than this big k marker so um so
those are the spheres I care about in my
am I bound so if T does should be square
it is close to K over N in the sense of
being within square root of a square
root of K over N away from krn we get
something like this something Gaussian
looking that peaks and Peaks close to ok
so that K lies within up to a constant
one standard deviation and in particular
this is rad k in particular this means
that it gives probability on the order
of 1 over rad k2 that's here
and the significance of this is that
when we integrate we take a H's ro mean
so we're going to take the time average
up to K over N oh sorry notches are me
today that time integral we want to see
what this does what this probability
distribution so let's put a delta in
here but this probability distribution
does too to the sphere SK for an
arbitrary little kay Lester go to Big K
and this is bigger than we just restrict
the integral to where it gives
significant mass right up to a constant
it gives full mass to to the sphere
radius k what maximum not full and so
this will give mass one over rad k for a
duration of red K over N to the sphere
we care about and so up to a constant
this is at least one over Big K the
upshot of this is that if we average if
we average these these noise operators
we will have a probability mass that up
to a constant dominates the smooth
kernels and in particular we saw right
we have the semi-group bound I had said
that the Reaper ammeter ization was
harmless what I meant by that is that if
we define our noise operator by a noise
maximal operator by supreme / 0 less
than tau less than or equal to see these
means of
that this inherits the LP bounce this is
like the opposite direction for each of
them to go because instead of
concentrating or averaging over a bigger
interval they're averaging over all you
mean instead of going to the more
singular spherical right um we've
already taken that leap the reason that
this is useful is that here we lose
basically half a power of K when we
average whereas here we get back a power
of K we average hear it so in the long
run the smooth kernel is going to
basically gain more by smoothing out
then then the noise operator that that
might be a little too pithy but uh but
this bounds the smooth operator but now
we still have to deal with this square
function and if you got lost in the last
part that's fine because we're sort of
starting over with the with the spectral
portion
okay so we have our square function over
there what remains to show is that it's
bounded in l2 will actually square both
sides by F I'll put a question mark here
so let's take a look at these
expressions well this is the sum over X
and the click OK from one up to end I
might switch to litter marker now
rearranging the sum this or the whole
point of square functions we can bring
in l2 norm all the way in here and what
we get is the sum over K of the l2 norm
squared of this difference if you're not
used to this computation it's pretty
quick um but the point is these
spherical averages on the klique power
our self adjoint and they commute if you
don't believe me we'll see why in a
moment but uh but they do so they can be
simultaneously and ortho normally
diagonalized yeah be an orthonormal
eigen basis for the spherical averaging
operator family then we can write this L
2 norm as
the sum over all eigen functions in this
basis of I'll explain this expression in
a moment although you will likely figure
it out from context so I'm borrowing
some notation from pouria analysis this
really is discrete Fourier analysis by
this i mean the eigen value of this
function associated with the kate
averaging operator here I mean the inner
product between the function f and the
function V um Oh started a k here and
the upshot is that I can bound this by
sum over K of the soup or max I guess
over V of these eigenvalues squared
times the 2 norm of F squared right this
is an orthonormal basis so this some
comes out to the 2 norm squared and I'm
just pulling out the largest of these
coefficients and so all of a sudden
we've gotten rid of F and now this is
just a question about the spherical
averaging operators and their spectral
properties right so now we're gonna have
to say more about about the the
eigenvalues and eigenvectors but but
we've gotten it to it to a nicer form I
would say
notice because of the structure of the
cliq really because the cliq has
diameter one that there's a net very
nice relationship between averaging
operators on the click powers and on the
underlying click which is that averaging
over the case fear on the click power is
the same as just picking uniformly
randomly a que subset of the end
components and taking your and applying
the operator that's the stochastic
adjacency on the click and all of those
bad components and then the identity
everywhere else right so PK of delta is
just pick k components and change those
and only those and these eigenvectors
here are just tensor products of
eigenvectors of the of the adjacency the
stochastic adjacency here of the cliq
simply because i mean these are convex
combinations based on this observation
of tensor products of the cliq adjacency
and the identity so certainly we can
orthonormal e diagonalize that by this
and what we get is up to a normalizing
factor be our basis is the space of
functions of this form why is in k MN i
mean by this is see here is a primitive
em throught of unity and we identify
vertices with with a numerical label 0
through m
this is just more for ease of notation
than anything else and then this is just
a numerical dot product out but this
isn't these are the tensor products of
eigenvectors and what we see is that the
more of the components of a given Y that
are not 0 right the more components that
are not the constant I ghen vector so we
see c YX this is on the cliq has
eigenvalue 1 if y equals 0 right then
it's just the constant vector and this
is a markov operator and negative 1 over
m otherwise this is a pretty
straightforward computation so what we
see is that the eigenvector sorry the
eigenvalues here are given by the
probability that if you take a random
component in the spherical averaging
operator and that operator is the
adjacency rather than the identity on
the click and then you take a random
component in this eigenvector and that
is an oscillating vector not the
constant that probability determines the
eigenvector because if and only if
there's a collision do you get a 1 eigen
value and this plays into some massive
sum but the two observations to be had
here are first of all if V has a very
low frequency right that is V has very
few oscillating components then then
these are going to be very similar
they're not going to be small it will be
close to 1 but they will be close to one
another if he is you know all ones with
one oscillating component the
probability the difference in
probability here of a of a decay is is
just 1 over n right because it's the the
the difference in probability that the
first component here and the first
component here are the adjacency rather
than the identity
so as a result this gives us with a
grain of salt that if V has our non
constant components then the difference
is equal to with some shifts of indices
but for all intents and purposes equal
to R over N times PK right to the lower
the frequency the Morgaine we get from
that difference and um if K is bigger
sorry the product of K and the frequency
is large that means a high probability
of collision which means a high
probability of cancellation in the case
of the hypercube or contraction of by
powers of em in the case of higher
clicks we have that this thing is
bounded by e to the negative Omega R K
over N we're this corresponds to the
probability in some sense of a collision
of a of an adjacency and this in this
operator and a an oscillating component
in the eigen vector and so as a result
this thing has to be smaller than I
guess I should put quotes around here
too if I put a put an equals has to be
smaller than K times r over n e to the
negative R K over N and basic summation
tricks give you that this is mounted up
to a constant by one if you like I mean
think about this is an integral in K and
integrate by parts and and that does it
that bounds this thing in l2 so I mean a
lot of steps but um
well my friend of the silently how does
it bound depend on the size of the thing
the linearization process shouldn't care
much cuz as the quick gets larger it
just gets more and more like a diagonal
the I mean larger am means quicker decay
I'm not sure is that his short answer
but I haven't I mean I've been so busy
trying to actually get the dimensions
rebounds for other graphs that I haven't
thought much about how how it at the
actual constants within a class of
graphs it's an interesting question and
at some point i'll probably go back
through and check you get matching lower
realms you get matching lower bounds do
you I mean you find the right rate of
defendants your eyes weekly grows yahoo
these constants give em a good price um
so so you're from the argument and order
mention Lobos uh i cant rember have to
top my head right no thank you um yeah
that's the that's the basic proof i'm a
little overtime but i'll just say where
the project is going cuz as well so my
main thing now is trying to figure out
to what class of graphs this phenomenon
extends and for what class it fails for
instance for powers of trees we have
exponentially bad bounds it's not that
hard to see i have a proof on my on my
website at my UCLA website if you'd like
um the lp bounds of the operator grow
exponentially in dimension for trees
it's just a balance but they actually
the maximum function G establishment
that function themselves were it I mean
I can icons about the truth is that
sorry 68 again it's not just that the
balance you can prove our bad but the
actual behavior that yes yeah the point
is that if you take very large radius
spheres in a tree it really only sees
leaves so if you just put mass on powers
of leaves then then any point sees that
mass um I don't want to go into more
detail for time reasons so the main
result I have now is that the five cycle
graph has l2 bounds for its ball maximal
operator which is a far cry from what I
really want but it deals with a lot of
the main issues caused by the cliq
having this nice property of a simple
relationship between averaging and the
cliq powers and averaging on the cliq so
I'm optimistic that that will move
forward and we'll be able to get small p
you've all had a nice suggestion for for
a reference that was an improvement on
Stein's original paper that this was
sort of following maybe yet small peas
through simpler means and to get
explicit bounds in p which would be nice
but but that's where it's going and I'm
hoping that I can prove eventually that
at least for kaylee graphs of abelian
groups you can get dimension free bounds
and then if that's true try to expand it
as far as I can and see where it stops
thank you very much so what's your guess
on what's the right condition on a
ground what um at the moment so it's the
techniques clearly require that the
spectral techniques anyway clearly
require that the spherical averaging
operators in the base graph can be with
one another because that's the necessary
and sufficient condition for them to
commute in the in the graph powers and I
think there is a decent chance that that
is sufficient for dimension free bounds
but that's definitely not obvious so
what that's true
but that you need a billion so what what
was the Assumption divine Mindy Kaling
graph what is kaylee graph of an abelian
group will have that property it's not
really a move it may or may not it
depends on if it can be you know
instructed otherwise it would include
distance regular it would include kaylee
graphs of abelian and I'm trying to get
a sense for in the process what that
condition means geometrically I have
some guesses of that but those are
things that I don't want to say because
I haven't exploited enough and they
might be stupid guess it's if the base
ground is any transitive grafica then
you have the counter example in life
sorry so the trees they have these very
different leads are so different from
the center but if you have the
transitive grab for all purposes of the
same should be enough yeah it's a good
question um piquÃ© and piquÃ© minus 1 to
commute in this case it's old ex
transitive it's possible but I don't
think that's obvious I mean I don't
think it's obvious for non abelian
groups which are a special case of that
but
yeah so it might be it might be a it's
an odd condition right now it looks very
algebraic and contrived I'm hoping that
there's a more natural way to phrase
that condition but but at the moment my
main focus is is getting this five cycle
bound and moving towards because I think
it'll also become easier to characterize
when I have a better sense of the proofs
for wider class of graphs okay thank you
very much this may be P a means your
initial people too yeah I sent to say
this part way through I decided to only
show p equals two because this is
something you've all had alluded to the
the proof for p less than two uses
techniques that are totally different
and very technical it's interpolation
theory and most of the juicy stuff is in
the l2 bound so you know it's possible
that i'll be able to present on the LP
bounds if you've all suggestion comes
out but at the moment it's just not
worth the time because its pages of
stuff there's a good reference in neb
Owen Stein's paper and in the stein not
similar got a theory paper diamond
simples the first start was too right I
mean the proof does start this is the
beginning of the proof but but then the
rest so it's a convenient stopping point
but the rest of it is just like very
arithmetic and complex analysis e so all
right say no more questions
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>