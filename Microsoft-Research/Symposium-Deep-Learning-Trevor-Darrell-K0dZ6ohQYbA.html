<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - Trevor Darrell | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - Trevor Darrell - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - Trevor Darrell</b></h2><h5 class="post__date">2016-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K0dZ6ohQYbA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so our second speaker is Trevor Daryl
he's a professor of computer science
division at UC Berkeley he and his group
has done many influential work in
especially in deep learning for computer
vision so recently he his group
developed region-based CNN which is a
state-of-the-art framework for object
detection and also his group is well
known for developing cafe which is a
widely used open source library for deep
learning so today he is going to talk
about adaptive articulate and actionable
deep learning thank you very much so I'm
very excited about the performance and
success of deep learning and in computer
vision especially not just its absolute
performance on fully supervised tasks
but it's flexibility and ability to work
on a number of related problems and also
to to work across modalities and so
that's the focus of what I'll talk about
today summarizing some of the work going
on in in my group and in collaboration
with others at Berkeley how these visual
models not only can adapt across visual
tasks but to language and then
ultimately to robotics and so that gives
me the title adaptive articulate and
actionable deep learning I'll mention
some of our recent work on deep learning
that can adapt to new domains especially
non IID settings I'll spend a
considerable amount of time on this
models that can jointly learn visual
representations and natural language
descriptions focusing on what's very
recently exciting making these models
compositional and and able to
individuate objects in a scene not just
tell a story about the entire scene but
talk about or or individual objects
within a scene and then finally I may
actually defer to my colleague Peter
abeel who's speaking later in this
session about our joint work which also
takes these models modifies them in some
important ways and directly uses them to
learn action policies from
from visual inputs so first a few words
about domain adaptation we want both to
we want to train in one environment
maybe we learn images from the web test
and another environment may be the
classic example is we have a robot going
around in an environment and you know
even the best deep learned algorithms
deep learned representations are far
more invariant to domain shift than any
prior visual representation that we used
but but indeed even deep learned
representations still suffer if they if
the training data doesn't match the
testing data and there are some very
simple and efficient things that you can
do to improve performance to adapt deep
learned models to new domains and of
course it goes without saying that the
simplest thing that you could always do
is fine tune your models and so if you
have lots and lots of label training
data in your target domain we don't even
need to put up any slides on this you
can just go ahead and fine tune so we're
interested in domain adaptation when you
have few or no labels in your in your
training data maybe one or two per
category how can you adapt a deep model
with such little training data and
indeed the one of the main underlying
concepts we explore is this idea of
minimizing discrepancy across domains
and this is informally related to the
classic idea of data set bias that
Antonio efros so excuse me Alyosha
f-frozen intent and Antonio taraba
introduced several years ago when they
had this paper a paper than some of you
may remember from the vision community
the name the data set game if you can
guess what data set your samples are
coming from you probably have a problem
and their paper articulated why and so
that is one good idea one can employ
let's have visual representations that
are invariant to the domain shift that
we want to to not suffer from let's
minimize discrepancy so in our most
recent work we explicitly optimized for
this in
in a deep architecture we train deep
domain classifiers and penalize them for
actually being able to tell the
difference between domain labels we can
show that this learns representations
that actually brings samples of the same
class across domains closer together and
then finally classifiers that we're
trained in the source domain work well
you can do all of this without any
labels in the target domain any class
labels so I probably don't have time to
go into detail in this architecture but
it's essentially two paths one for each
domain where in addition to the classic
the standard classification loss over
here I don't think you can yeah okay
over here we also have these additional
losses for domain confusion and we can
visualize the network actually learning
representations that can't tell the
difference between domains the second
twist here is if you have a few labels
how can you make the most use of them
well you can obviously fine tune but you
can do something slightly better than
that which essentially amounts to taking
the idea of the dark knowledge concept
from from geoff hinton of a few years
ago which he applied to simplifying
networks and I guess you can think of
that as a or generalize that to this
idea of domain adaptation because you
can train more efficiently with the soft
label with the labels that you have if
you take advantage of inter category
correlations in the source domain and
apply them in the in the target domain
so you can see what activations you have
in the source domain across categories
not just taking the single hard label
loss but training and the target domain
with a soft label loss as well and both
of these so we are final architecture
integrates both of these ideas for
domain adaptation task correlation
transfer through these soft labels and
domain confusion and both of them help
and really outperform just you know
collecting the data together into an
initial training set and as I said many
times you have no label data in your
target
and need to make use of it so this is
the paper at ICC V and it's on archive
and if you'd like to read more I'll
refer you there so now I'd like to talk
about the second major bullet articulate
how do we have representations that not
only can recognize images and do so
across domains but but actually tell a
story about what's happening map
directly from images to two words and
I'm sure everyone in this room are
nearly everyone in this room has seen
the great plethora of models and
advances that happened over the past
year of architectures of form similar
that you see on this slide where you
have a visual CNN processing an image
and being input to a language model
perhaps a recurrent language model based
on an lstm they could then generate
sentences that or captions related to
images and I won't say more about those
models than that because they've gotten
so much attention over the last year and
we had one of them and many other folks
in the room had had others and actually
nearly all of them are I would say all
of them have the following problem maybe
not all of them but certainly ours had
the following problem which is they
really were limited to their training
data and they weren't compositional in
important ways and one way they weren't
compositional is they could only talk
about things they'd seen in previous
caption data these models were trained
on paired corpora of images and captions
and they learned to generate captions
that were appropriate when they were you
know within the space of their training
data as you would expect but that led to
unfortunate performance when you for
example have this picture of something
here swimming in the water and it's
really not a dog sitting in a boat in
the water which is what our our system
or many systems might that were trained
on the available corpora at the time it
would have put it out put but we have
lots of other sources of knowledge lots
of other sources of visual data we know
that that's not a dog because we've seen
lots of dogs on imagenet and that
doesn't look like a dog and there's
actually lots of unpaired text data as
well that tells us a lot about language
models that go beyond the language
models that are baked into the
corpora so the first compositional
captioning model i'm going to put
forward is what we call the deep
compositional captioner and it really
takes advantage of lexical
generalization taking advantage of other
data that we have unpaired data either
image tag data or just text data and
indeed it can learn to take this image
and tie and speak about the otter that's
sitting in the water even though it's
never seen a caption involving an otter
before and we do that by extending the
previous model to have structure to
allow it to be separately trained across
different data sets and integrated in in
interesting ways and one of the key
ideas at least in our current work is
for each new term we want to bring into
the the captioning model for example
from imagenet we have to find a proxy
word in a language model in the paired
corpora perhaps to say how should I
speak about otters for example you might
find that you speak about otters the way
you do about dolphins or you might speak
about zebras the way you speak about
horses and once you've integrated a
zebra actually I think zebra is in ms
coco but for a word that's not an MS
Coco once you've integrated it in for
example I think the next slide has that
I don't think there's a toad in the
typical corporate people trained on so
the typical models wouldn't be able to
caption this as a toad but knowing that
toads are sort of like birds and that
can be automatically discovered from
from the models our model is able to
caption that appropriately we also have
this results on video captioning with a
similar idea and if you'd like to learn
more about this we have an archive that
you can read by Hendrix at all ok so the
second major idea about captioning these
language models I like to put forward is
more traditional linguistic
compositionality especially when you
look at visual question answering
there's been a lot of I think very
exciting models on how attention can be
used for visual question answering but
but I think we really need compositional
competence
to make a serious dent in visual
question answering maybe not for the
data sets that we have today but I think
to convince anyone that we really know
what we're talking about when we answer
questions and by compositionality we
mean the ability to separate out
different processing structures for
example finding the stuff that are that
are attributes versus finding certain
nouns and then doing operations such as
counting on them so we have recently
this is joint work with den dan klein at
UC berkeley we recently I've recently
destroyed my laser pointer but that's
probably fine we recently proposed an
architecture called neural model neural
module networks which composes a new
neural graph corresponding to a sentence
in this in the current version of the
work it's actually using a traditional
NLP parser but that can obviously be
replaced with a deep architecture
although the traditional architectures
were exceedingly well for the class of
questions that are actually in these
data sets and comprises a new deep
architecture for each sentence that it
sees and trains them jointly for example
if you wanted to have a sentence what
color is his tie it would create a small
little visual routine to find the tie
extract the color and answer the
question in the interest of time I'm
going to skip through a few of these
slides and and point out that what's
really powerful about this is when you
want to have very complicated logical
queries is there a red shape above a
circle that's something that many of the
existing networks have trouble with but
this approach does and currently an
architecture based on this approach does
very well on toy problems that are more
or less impossible for existent for many
existing architectures to solve but it's
able to ask complicated queries about
you know how many how many square green
things are there in this image and that
would be harder for the traditional
approaches or these many of the
traditional approaches that have been
reported and this architecture also does
well on the traditional vqa data set
that we see in a wonderful poster over
here
although we're not it's not clear that
there's enough complexity in those
questions yet so I think that's a topic
of future work and again there's an
archive on this if you'd like to see it
in the one minute I have left hopefully
two minutes I have left I'd like to talk
about another extension of this of the
integrated vision &amp;amp; captioning work that
I think it's particularly exciting for
robotics or for any any setting where
you'd like to talk about things in the
world to an agent we call it natural
language object retrieval but it's
really this idea of referring to things
in in scenes and we have recently
proposed a model that not only can talk
about an entire image but actually can
configure out which region in a scene is
something is the region that somebody's
trying to refer to so if I were if I
want to speak about something to a robot
or an agent I'd like to use natural
language and say the guy with the blue
shirt or the guide to the left of the
guy with the blue shirt holding a pole
and I'd like the robot to know or the
system to know which person I'm talking
about so that's what I've just said so
we have a new architecture that
generalizes the existing vision language
models to include object a spatial
context position with an image and and
and is able to take reap image regions
and and score strings that are presented
to refer to a scene and that there's a
very comprehensive data set that
tomorrow berg collected called the
referred game dataset which has actually
been out there for several years and
hadn't really gotten much attention we
show results on that that I think are
fairly exciting there's the white hat
can be found in this image the leaves on
the left tree can be found and so on and
so forth so we have an archive on that I
think there's also some interesting
related work from from google from kevin
murphy has a similar model and also i
guess a great new data set also on the
way and we look forward to that so i
have negative time and so for my last
thing i can just give it to Peter Beals
talk about it
in the day how we have joint models that
that map from pixels to action and I've
summarized these three topics that are
really exciting to show how deep
learning is bringing different fields of
AI together kind of core machine
learning for domain notation vision
language and robotics and these are the
archives that I mentioned today and I'd
like to thank all of my co-authors
explicitly for all the work they did on
these projects thank you so we have time
for a few questions so can you please
come forward if you have any questions
so I have one question so how far are we
from doing some more deeper level of
question answering such as understanding
the intention or intent or some why
understanding what's going on with the
image I think I'll just have to read
these archives and you'll learn all that
well yeah deeper when it's a hard
question because I have to know what you
mean by semantics and deeper I mean you
mean the way peep so why a parson you
mean the way people do it you mean you
mean like a system that can behave the
way a person behaves when it answers a
question in the sense that it can
explain what it's doing or yeah well I
think that's not a very exciting area of
future research of how we can have these
explainable architectures and why I
think there's reason to believe that
that these deep models can be trying to
do that okay thank you yes Trevor so for
the example where you are dynamically
constructing networks you can you give a
flavor of what kind of methods you're
using to instantiate the network these
yeah that is just his standards it's a
standard Stanford parser I guess I I'd
have to defer to my it was really a
Jacob andreas who's Dan Klein student
working on that but it's a traditional
approach to the parsing step and once
you've parsed the the phrase the
semantic interpretation is done with
deep architectures that are jointly
trained across all sentences so that the
grounding of the term red is common to
all of the modules that involve the word
red
ok any other questions ok oh yes
so Trevor I can imagine you know 50
archive papers describing great work in
variety of areas but you know system 3
doesn't do what system 27 does and how
are we going to integrate all these
capabilities into a single larger system
isn't that system 98 ok so I'll it's
okay i guess i I'm not sure that yeah I
mean what I do think that as i started
the talk i do believe that it's far more
likely or it seems there's much more
promise for that in these architectures
or in the deep learning framework than
in traditional I mean language and
vision people have been talking about
joint grounding for ages and very little
happened that really looked looked
exciting to people outside of that
community until these deep architectures
we could just start gluing them together
and maybe now we're going to start
gluing them in slightly non-trivial ways
that aren't just you know
undifferentiated goo but have structure
inside of them but it'll probably still
be a long way till we have you know
broad universal AI competence but it's
more I'm more excited than I used to be
about about that topic okay so thanks
thanks a trapper again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>