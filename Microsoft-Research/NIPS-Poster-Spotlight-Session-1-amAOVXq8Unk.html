<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 1 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 1</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/amAOVXq8Unk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so hello my name is Christos
framily this and this is joint work with
a sauna bath my boxee be at Caltech the
title is lasso with non linear
measurement is equivalent one with
linear measurements so we're interested
in high-dimensional data inference where
then I the number of observations is
very large but what is also very large
is the number of variables to be
estimated and in particular this is very
different than the classical way of
thinking in statistics in which the
number of observations is large but the
number of variables to be estimated is
very small and so the classical theory
is not applicable here in order to make
this more concrete think of the linear
noisy measurement model y equal ax + Z
we want to recover X so all this nice
theory that we know from classical
statistics about least squares M&amp;amp;MS
tomatoes and so on is simply not
applicable in high-dimensional setting
and so it has only been very recently
that under appropriate randoms
assumption on the measurement matrix
where we are able to give precise error
guarantees on regularize dem estimators
in particular for example we are able to
answer how good is the estimated would
get from penalized least squares also
known as generalized lasso here f is a
structure in using function associated
with with a particular structure of the
unknown vector for example if it's
sparse may use the l1 norm and so on and
and and so this is this is a recent work
but this is not what this paper is about
in this paper we go one step further and
watch what happens if we have non linear
measurements okay so nonlinearities may
arise because of uncertainties in the
underlying model so we hit this AI
transpose X not with a nonlinear
function see but more interestingly this
this this nonlinear function may
represent some non-linearity which are
which arises from from design okay so
for example think of quantization we may
want to have one bit measurements which
are cheaper and easier to generate and
then we want to ask how do we recover X
naught and so in this paper we ask
whether we can use lasso to recover X
naught and this may sound at the
beginning as
not a very good idea just because last
is not using explicitly the nonlinear
function but there's actually good
reasons to ask this first of all we
might not even know the non-linearity
but even if we do so and we would say be
tempted to use some penalized maximum
likelihood estimator then this might be
computationally inefficient when
compared to using any of the lasso
solvers and the third point is that it
actually turns out that the last
performance well now if you want to know
how well and if you want to know how how
this is related to linear measurements
how we use this to design optimal
quantizers please stop by or poster and
I'm very happy to talk more about thank
you hi so my name is Jeff films and I'm
going to be talking about so much or
hamming metrics this is work with
Jennifer gillenwater reship I are
Bethany lush and roll kadam be and
unfortunately Jenny isn't here at nips
this year so we're all familiar with the
Hamming distance something that's used
and it's quite useful in a variety of
applications of machine learning I
Hamming distance however has some
shortcomings and that's trying to
essentially be illustrated here by these
figures where let's say that you have a
set of four images V and you have
several subsets a B and C and you were
to measure the Hamming distance between
say sets a and B and sets a and C the
Hamming distance would say that the
distance between these two sets are
identical despite the fact that there's
some sort of commonality amongst the
subset of images in a and C namely that
they consist of several buildings and
one way of dealing that with that would
be to think of Hamming distance as
perhaps a sum of differences of bit
vectors and then maybe grouping together
the bits that correspond to differences
of buildings and not penalizing the
building differences as much and another
way of dealing with that would be to
essentially do some sort of Mahalanobis
distance type thing but the problem with
Mahalanobis distance is that it doesn't
really offer any nice mathematical
properties such as meh tricity this
would be discreet metro city over binary
vectors now it turns out that if you
impart a sub additive function on top of
the symmetric difference between the two
sets a and B that immediately offers a
metric discrete metric but the problem
with that is that it doesn't give any
nice computational properties and so
what we study in this particular paper
are essentially a class of optimization
problems on objectives that look like
the following we have essentially a set
of bees and a function f which is a sum
over of the of these metrics these
discrete metrics and we're either
interested in minimizing over a which is
kind of analogous to like k-means except
that this is sort of a discrete metric
based k-means or if we were maximize
over them that's kind of analogous to
diverse diversity selection so if it's a
Hamming distance then these are
computationally tractable if it's sub
additive so we saw that we have a metric
everywhere then unfortunately its inner
proximal and the minimization case and
very hard in the maximization case
however if it's not only sub additive
but also sub modular like many problems
that are associated with so much what
functions suddenly things start becoming
very nice yet you don't lose natural
naturalness and so what we study in this
paper is a variety of different
approximation bounds in the sub modular
case under very different circumstances
if you're interested in seeing what all
these numbers mean then please come to
poster 71 thank you hi I'm Martin slot
see this is joint work with Finley done
at Rutgers so let me start by recalling
the classical setup of compressed
sensing where the goal is to recover
high dimensional but search bar signal
from few possibly a noisy linear
measurements so the idea of recovery
from highly and complete information is
subsequently we develop further and what
is known as one with compressed sensing
were in place of the linear measurements
the only observed signs so in the
literature has provided results of the
form we need zone too many measurements
recover the singer up to certain
accuracy or in the one bit case the
normalized signal up to certain accuracy
so in our paper we try to bridge the gap
between these two cases by considering
vivid quantized measurements that is on
top of our linear measurements we apply
quantization map that maps a real value
to a codebook of cardinality to to the B
for supposing that we are free in
choosing the parameter be this year's a
natural question namely suppose we are
given a fixed budget of bits be what is
the optimal trade-off between the total
number of measurements and the number of
kids we use per measurement so obviously
the straight of depends on the noise
level on the specifics of the
quantization scheme and the recovery
algorithm so an hour paper we
concentrate on a single recovery
algorithm that is based on a general
approach to nonlinear compressed sensing
proposed in a paper of plan machining
the approach is very basic in that
minimizes linear objective for a certain
constraint set so why do we call this
margin regression or be with margin
regression well it turns out that SB
tends to infinity it's not hard to show
that the approach of Clan machine is
equivalent to margin regression so
analysis proceeds basically as follows
we first separate the estimation of the
direction that is a normalized signal
and the norm of the signal we then
derive an asymptotically tight bond on
the l2 distance of our normalized signal
and the margin regression estimator you
then decomposed as a rate function our
NSM and another term that only depends
on B and we then position to compare
beem beem pivot measurements and be
private measurements by evaluating the
ratio of Omega and Omega P Prime so all
of this we then get the following result
send me that be equal to 1 is optimal
vest
the direction that be equal to 2 is
optimal for estimating the direction and
the scale the Lord max quantization is
an optimal quantization scheme and we
also do an extension to other noise
model so for more details are you
encouraged to stop by the poster thank
you hi everyone my name is changing and
this is joint work with sham caca de um
the title of our paper is super
resolution of the grid so what is super
resolution super resolution is a
technique to enhance the resolution of
an image system and it has wide
applications in medical imaging
microscopy imaging as well as speech
recognition tech microscopy image as an
example the image captures emissions
from a few isolated particles or points
in the space however the diffraction of
light blurred image and we need to
denoise the image in order to accurately
estimate the locations of the particles
in a slightly more abstract way we
assume the object X T can be represented
by a sum of K isolated points in the D
dimensional space in applications the
points can represent molecules or nine
spectra in the speech we also assume
that we only have access to some course
information about the underlying object
in the form of been limited and noise
distorted measurements so the goal of
soup resolution is to estimate the
locations of the points news with high
resolution from the chorus measurements
by taking advantage of the underlying
sparsity of the signal XT intuitively as
a pairwise separation between the points
become smaller the problem becomes
harder and the complexity of the
algorithm obvious choice tomato points
should depend on the minimum separation
defined as Delta I'll jump to present
our result we propose an algorithm to
solve the super-resolution proper and we
prove that
in order to achieve the target accuracy
in estimation our algorithm runs faster
and it's less number of measurements
than existing algorithms in particular
the existing algorithms either comes
with no performance guarantee or the
complexity increases exponentially in
the dimension of the system d and
inverse proportional to the minimum
separation Delta however our algorithm
the complexity only depends
quadratically in the dimensions of the
system and surprisingly the complexity
does not depend on the minimum
separation at all and the key of the key
of our algorithm is just to go off the
grid for more details about our work
please stop by the poster tonight thank
you
hi I'm Zota Sabo from the gas unit
University College London this is a
joint work with the Bharatiya para mudar
from the Pennsylvania State University
Department of Statistics the topic is
optimal rates for random free features
Colonel techniques represented by the
applicable and popular technique to
capture complex relations typical
applications of Colonel machines our
task which can be expressed in terms of
function values and function derivatives
just to give you two quick examples
Colonel Jay glitch regression is an
example for the first case where we have
input-output pairs and we want to
capture this relation from a pupil
specific hilbert space determined by
o'connell indefinite financial family
fitting is a is an instance for the
second problem which can be expressed in
terms of Colonel kernels and kannada
relatives unfortunately this flexibility
of kernel methods has a price they
typically scale reasonably bully in
order to mitigate this problem several
approaches have been suggested in the
literature such as incomplete russki
decomposition nistrim methods catching
or random freer features which is the
focus of our paper so any continual
shifting variant kernel can be written
as the Fourier transform of a
probability measure by the Bachmann
theorem this is the idea of the random
30 feature to replace this integral DS
expectations with an empirical average
by the constructed finite dimensional
representation these random Fauria
feature tick enables one to rewrite the
problem in the primal and apply fast
linear techniques
there exists theoretical guarantees for
the approximation property of the random
for a feature-based cannot technique
hakuna on in uniform sense on a compact
set s the bound depends linearly on the
diameter of the set and as 1 over square
root of number of footage features up to
logarithmic factors in this paper we
derive the finite sample uniform
guarantees for fine for this random free
feature-based the cone approximation and
instead of the diameter of the M we got
the bound which depends logarithmically
on the diameter of s this rate is also
apt optimal asymptotic sense we also
find that sample applicant ease and
bounds for kona derivatives if you are
interested in the details or have any
questions our post with 95 hi I present
top k multi-class SVM which is a joint
work with matthias line at solent
university and burn she led the Max
Planck Institute for informatics the
main motivation for our work are
difficult classification problems with
significant class overlap for example if
you look at the four images on this
slide they all look similar and one
might expect that they belong to the
same class but the ground truth
annotation says that these are examples
of four different classes this is what
we call ambiguous classes and they
typically appear in problems with the
large number of categories which we can
expect to be even more common in future
note that this is not just a few
outliers due to label noise because some
images are in currently multi-label for
example there may be a river in a park
but also there is not always a clear cut
between the classes like between the
pond and the lake in that case it is
sometimes difficult even for humans to
guess correctly on the first
attempt therefore it makes sense to
allow a few guesses this leads us to the
top K error which counts a mistake only
if all K predictions were wrong this is
a popular performance measure which is
used in benchmarks like imagenet so why
not optimize it directly to this end we
propose an extension of the multi-class
SVM of Kremlin zinger so instead of
looking just at the maximum prediction
score as done in their formulation we
take the average of the K largest
predictions and threshold it at zero we
call it the top K hinge loss because it
is a convex upper bound on the direct
non convex top-k a loss which you can
see at the bottom of this slide and
because it reduces to the multi-class
laws of karma in zynga for k equals one
for training we propose an efficient
optimization scheme based on stochastic
dual coordinated sent to optimize the
dual objective we need the projection
onto what we call the top K simplex as
you can see it reduces to the standard
simplex for a k equals 1 and 4 K larger
than 1 there is a non-trivial upper
bound on every dual variable one of our
main technical contributions is an
efficient algorithm for projecting onto
the top K simplex experiments on five
datasets including large scale ones show
consistent improvements in top K
accuracy and sometimes even in top one
performance so please come see us at
poster number 61
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>