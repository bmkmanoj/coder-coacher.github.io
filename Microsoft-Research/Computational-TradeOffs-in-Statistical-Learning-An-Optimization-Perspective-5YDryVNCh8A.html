<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computational Trade-Offs in Statistical Learning: An Optimization Perspective | Coder Coacher - Coaching Coders</title><meta content="Computational Trade-Offs in Statistical Learning: An Optimization Perspective - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Computational Trade-Offs in Statistical Learning: An Optimization Perspective</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5YDryVNCh8A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay this figure it is a like a girl he
was a student of Peter Bartlett and
Martin wait right and he's me talking
about some computational trade-offs and
statistics ok so the talks going to be
kind of at the intersection of some
statistical and computational issues and
particular we are going to try to
understand how how what happens when we
try to take computational issues into
consideration jointly with the
statistical complexity of the final
solution that is being computed and the
results will involve joint work with my
advisors Peter and Martin along with
other collaborators at Berkeley so let
me begin by briefly motivating why we
might need such a joint study of
statistics and computation and start
with an example that I was working on at
Yahoo research the summer so the problem
where we were looking at is how to
display an ad given a web page and the
criterion we won optimizes the
probability of a click by a user and we
formulate this as a classification
problem we're given and a pair of an ad
in web page we are trying to predict
whether it will attract a click or not
and yeah we can do this because we have
the past click locks which we tried
which we try to use to learn a good plus
five on the future data of course the
problem is that these logs are growing
in real time so you can't really think
of having a small subset of examples
that we are dealing with we of course do
some sample the big logs for
computational purposes but despite the
subsampling we were dealing with as
because 17 billion examples and the
reason you actually need that much data
is because the model being estimated is
not small either the in particular we
formulated as logistic regression and
our parameter had something like 16
million dimensions so we really did need
a fairly large amount of data however so
it might be very easy to think that this
is a purely computational issue at this
point and we just gotta come up with an
efficient algorithm to solve this
problem and at some level one of the
cases I will make today's that is not
entirely true and this will become more
clear in the second example I'm going to
talk about today which pertains to
what's being called a netflix challenge
where we want to predict ratings that
users would assign to movies and we get
to see a certain subsample of these
ratings so in particular the data set
that Netflix made available had
something like 100,000 examples and the
number of parameters in this case was
staggeringly large much much larger than
the number of samples so people have
attempted a lot of different ways to
approach this problem and a lot one
particular class of solutions that
people have attempted have been based on
convex optimization problems and what we
will show is that these convex
optimization relaxations that people
have looked at so now you have to solve
that relaxation and if you think of sort
of the worst case complexity of known
algorithms to solve these relaxations
then we can do much much better than
that worst case complexity if we if we
take the statistical structure of this
problem into account we will get a much
better result and we will see the
details of this so so kind of the case i
want to make at some level is that we
rather than viewing the sort of
statistical or computational complexity
of these problems in isolation what we
want to really do is try to understand
the joint behavior of these two
phenomena and sort of a very conceptual
cartoon that i want you to have in mind
is so we will first start from a simpler
picture not taking this into account
which is often what a statistician would
have in mind where they are trying to
infer a parameter theta star we can
think of this as a P dimensional
parameter we get certain data to we get
n data samples to infer this parameter
and for instance in the cliq prediction
example each each sample would consist
of AG
i spare you observe 01 value labels
whether it got a click or not and you
have got features encoding the add web
page pair and then you infer a parameter
theta hat from this data and you try to
ask the statistical risk of theta hat
and maybe as a function of N and P
typically for instance what I propose is
to augment the picture a little and
actually think of what's going on
computationally as well inside and so
the risk can be for instance something
like the distance between theta hat and
theorists are in in l2 or in some other
metric or it could be some other loss
function that you compute and used to
estimate the quality of theta hat and so
the picture that we would bear in mind
today is to take the the computation
into account as well and think about
what we do with the data that the data
goes into a black box here I'm not going
to describe the workings of this black
box right away for now but whatever this
black box does we get B units of
computational time again for now we will
keep this at an abstract level and we
would like to understand the risk of
theta had not just as a function of
number of samples and number of
parameters but also as a function of the
computational time now and the reason
why this is interesting is if we
understand this quantity then we can try
to answer how the statistical and
computational aspects interact with each
other and how we can trade one off for
the other rather than just having a
separate understanding of the two two
quantities now the biggest missing pick
piece from this picture of course is a
specification of this computation of
that box and I haven't done that because
that is kind of the hardest part when
trying to attack this problem so what I
will do in the talk is a I will go over
a series of examples in each example
there is going to be an Associated
notion of this computation of that box
and computational budget and we will try
to answer questions such as this in that
framework okay so the first example we
are going to look at is to try and
understand the fundamental computational
complexity for a class of certain for
certain classification problems and the
notion of computational complexity here
is going to be in terms of queries to a
certain noisy gradient Oracle and within
that framework I will present certain
minimax results for computational
complexity of some convex estimation
problems and some sparse estimation
problems and we will see more details
the second aspect of the talk is going
to be about computation of
high-dimensional estimators I'll start
by describing some computational
challenges that we face in high
dimensions but how we can avoid these
challenges if we look at the statistical
structure of the problem and these
results will have implications for
problems such as sparse linear
regression and low rank matrix
completion low rank matrix completion
being sort of the netflix example we saw
in the beginning and very briefly i'll
if time remains i will talk about how we
might generalize these ideas if we want
to solve these problems in a distributed
fashion rather than solving them all on
one computer then okay so starting with
the first example m estimators are a
fairly broad class for estimate is that
people look at in statistical literature
and the very nice aspect of them
computationally is that they can be
thought thought of as solutions of
optimization problems so so typically
these estimators are expressed as
minimizing a certain loss over the data
and under some parameter space and
consists of many common examples such as
maximum likelihood estimation where this
loss will be just the negative log
likelihood or least squares regression
logistic regression and so on and
statistically these estimators are
pretty well understood so what's for
instance known is that under mild
conditions this theta hat would converge
to a population parameter theta star
that minimizes the expectation of the
same loss function right so again but
the thing I want to point out here is
that theta star again is expressive
solution of an optimization problem
although in this case the optimization
problem is not fully specified because
we do not know this distribution
otherwise the statistical problem would
be trivial
definition and the risk is the
expectation of this fall for this part
of the talk yes exactly now in order for
computational tractability we are going
to make certain assumptions first so we
are going to assume that the loss
function is convex in the parameter
theta and we are going to assume that
the parameter space Omega is convex
which a lot of interesting examples
would satisfy and the upshot of these
two assumptions is that we can think of
both theta hat and theta star as being
solutions of convex optimization
problems and that's from a computational
standpoint that's naturally relevant
because there are lots of algorithms
available for solving convex
optimization problems and in particular
a class of algorithms that has been very
effective for solving large scale up
large scale problems is the stochastic
optimization framework and I'll talk a
bit more about this framework over the
next couple of slides but the question
we will try to ask today fir first is
what is the minimal computational
complexity of the best possible
stochastic optimization algorithm to
solve problems such as computation of
theta hat or theta star so so note that
what we want is the is a lower bound on
the complexity of the best possible
algorithm we are not going to just do
this reasoning by taking handful of
algorithms and maybe reasoning about
their complexities yet during this time
because theta star is the thing you
eventually want to compete with right so
so we will never be able to be computed
because we don't actually so you don't
have access to the distribution and
hence you cannot compute quantities such
as the value of the risk or the gradient
of the risk but you cannot you can
compute stochastic estimates right just
by drawing the sample and so which is
why the stochastic optimization
framework kind of applies for to theta
hat and theta star computation yeah okay
so now again I told you most things
except the most important thing which is
what is the notion of computational
complexity here and because we are in a
convex optimization framework we are
going to use the notions of complexity
that people in convex of my
framework have found to be very useful
so we look at the notion of Oracle or
black box model of complexity that was
proposed in the work of nemirovsky and
hewed in from 1983 so the idea in this
work is that there is a certain class of
functions that that we have and one way
to think about this in the statistical
setting that we are talking about is if
you look at this problem and let us let
us ignore everything else there is a
certain loss function and then there is
a distribution inducing the expectation
and by varying this distribution you can
get many many risk function many many
different risk functions and of course
we do not want algorithm that just works
for one distribution we want an
algorithm that works for many many
distributions so by looking at the class
of all risk functions that are induced
by taking different probability
distributions we can get a function
class such as this the class is mass an
estimator for every distribution you
have the function that map's an
estimator into the expectation of the
loss and a of this estimator under this
distribution yeah exactly so the
functions will be exactly these expected
loss functions L is fixed for now I mean
you can you can think you can vary both
L and distribution on just for
simplicity talking about keeping it
fixed different is indifferent
distribution oh so the no no so we are
thinking of this as a function of theta
we rewrite this is there is a function
of theta exactly it's a function of
theta so so really the the thing on the
x-axis here is theta right so when I
think of a concrete from is some free
parameters in it all um yes it could and
those were so so for instance you could
just think of so you could think of l of
theta here as being something like
absolute value of theta minus Z but but
you could have a scalar such as a
constant times that and that will give
you scaled versions of the same quantity
or you could in general sort of have a
recent ring as well so sort of theta
minus Z minus theta are not kind of a
thing and and so you you can very easily
have free parameters again that so what
we will see actually is we are going to
talk about this function class script F
abstractly for most of the talk and then
you can you can take whichever one you
are interested in to obtain complexity
estimates and whether you I mean
basically if you do not take a big glass
of free parameters then you will have
you will have a small function class
sort of optimizing or estimating over it
will be easier and the bigger you make
your class the harder the optimization
problem will get in this model as your
function class grows okay so so we have
this class of functions now unknown to
us an Oracle picks a function little F
from this class and the way we are going
to think about that is again in the
simplest case you can think of this as
now Nature has fixed a probability
distribution inducing this expectation
right and we of course do not know this
distribution but we would like to
compute the parameter that minimizes the
risk function so the parameter that
minimizes the
function f pic by the Oracle the way
this happens in the in the black box
model of complexity is by algorithm
making repeated queries to the Oracle so
at time T the algorithm queries Oracle
with a parameter theta T and on the
stochastic and this is actually just a
stochastic first order Oracle the Oracle
responds with a noisy gradient of F
evaluated at theta T and again in the
statistical setting it's very easy to
obtain these noisy gradients because you
can just pick a sample from the
distribution that you are interested in
and evaluate the sample gradient and
that's just unbiased by definition and
you can have now an algorithm basically
corresponds to how it utilizes this
information to compute its next query
point right so for instance the simplest
example would be the stochastic gradient
descent algorithm which just takes a
step in the negative noisy gradient
direction with some appropriate step
size to get the next iterate and repeats
this process over and over okay now in
this order model the way we talk about
the complexity of an algorithm is by the
number of queries it makes to the Oracle
so we are really thinking of these noisy
gradient evaluations as units of our
computation in measuring the complexity
in those units and so for a fixed method
m and for a fixed function f you can ask
what is the minimum number of queries
needed to minimize F to precision
epsilon right now that would be kind of
unsatisfactory because that's specific
to a particular method and what we at
the outside what we wanted was an
algorithm independent understanding of
computational complexities and that
brings us to a notion of mini max
complexity where we take the best case
over all possible algorithms in this
family yes that can interact with the
stochastic first order Oracle and for
each algorithm we measure its worst
Oracle complexity over all functions in
our class script F because we only want
algorithms that work uniformly over the
function class and this best worst case
complexity is referred to as the minimax
complexity of the function class script
F and what we will see today our results
about them this minimax complexity for
various choices of the function class
script f fixed or the class m is always
going to be algorithms that can interact
with this stochastic first order Oracle
so for instance you could then change
the Oracle and get a different class of
algorithms but we will talk about just
just the first order Oracle today so the
first class of functions that we start
from is the class of convex Lipschitz
functions these are Lipschitz with
respect to a particular metric I can
affirm it that for simplicity but what
you get is that the minimax complexity
of esta may opt Amaya's ation over this
a function class scales as P over
epsilon squared where p is the number of
parameters and epsilon is the desired
precision okay so first thing to note is
that we get a linear scaling with the
number of parameters in this case and
question if you have lots of free
parameters well these these are actually
the statistical parameters easier this
statistical yeah these these are from
the function exactly these yeah right
and the second thing to note is that in
this case the lower bound cannot be
improved any further because in a
uniform sense over this entire class of
functions stochastic gradient descent
has a matching upper bound now one thing
we often see in optimization is that the
complexity estimates of functions
improve in presence of certain
additional structures such as a non
degenerate curvature these looks like
very what's bad about these so what's
bad about it for instance is the
dependence on you you might want a
smaller dimension dependence and you
actually do a based on so so here I make
Lipschitz assumptions with respect to a
certain norm if you make Lipschitz
assumptions with respect to a certain
different norm you get different
dimension dependence and correspondingly
also get a different optimal algorithm
so they're not good hbu UK you can get
all the way to dimension independent
actually if you are if your l2 lipschutz
then you actually get a dimension
independent result and that was that was
actually a 1 over epsilon squared lower
bound on this problem was already known
you since 1983 in the work of nemirovsky
and you did is there something I don't
understand how happy is the number of
parameters you want to ask exactly but
it's also the number of number of
dimensions in your optimization problem
it's so frenetic minute always challenge
that's horrible if I have two output
parameters how would it be better than
linear and P so I am NOT talking about
the computational time that spent inside
the Oracle for computing the stochastic
gradient and the time that you spend in
computing the stochastic gradient is
definitely always going to be well okay
no I you you can actually in some cases
compute a stochastic gradient in fewer
than ofp because you can sort of
computer a randomized cordon a single
coordinate gradient and sometimes the
computation of that can be independent
of P even if you just pick a coordinate
uniformly at random and just if you can
compute the gradient in that direction
in lesser than 0 of P then even that's
not necessarily true you've been in
other cases where you could do kind of
dimension reduction we pretend that you
did the emission reduction so so we will
we will see we will see one concrete
example of dimension dependence being
nicer in two slides yeah
statistical so this is well not quite
because so you're still measuring the
number of queries you are making to the
Oracle to solve the optimization problem
right so I'm not talking about the
minimax error statistically I am just
talking about the minimax complexity of
solving the optimization problem to a
precision epsilon here no not
necessarily because in terms of
optimization the result applies just as
well to computation of theta hat it is
not necessary that I just think it's for
right so here we are in a completely
abstract framework where we just havin a
function that we are trying to minimize
and we are trying to compute its minimum
and based on these noisy gradient
queries and we are talking about the
number of queries we need to make to an
Oracle in order to minimize the function
but obviously there could be like it
much faster algorithm but it will just
have not to you not to be in this
framework that either unless I he looks
at these affairs though they're
precisely and yeah so for instance if
you can do exact radiant evaluations you
can do much faster algorithm
statistically or in a lot of problems
that does not happen because the cost of
computing a full gradient can be much
more than the cost of computing a
stochastic gradient magnitude of the
noise does it ain't anywhere yes I have
suppressed that dependence for
simplicity but its present in the
results okay so very briefly I want to
mention that the dependence on the
precision epsilon improves from one over
epsilon square to 1 over epsilon if you
assume that the functions have a
non-degenerate curvature just an
undiagnosed lee matches with the known
upper bounds again in particular again
stochastic gradient descent is an
optimal algorithm the thing that I want
to talk about at a bit more length is an
example where the dimension dependence
can be much nicer so in the statistical
literature people have talked recently a
lot about what happens if you are in a
high
space but you have certain additional
structure on your parameters in
particular people have looked a lot at
the sparsity structure so in the same
way and we we talked about the
optimization of convex lipschutz
functions again but now we assume that
the optimal parameter theta star has at
most s nonzero entries and we expect us
to be something significantly smaller
than the total dimension so these sorts
of problems for instance one example
where you would encounter them is if you
had statistical data and you were
actually trying to compute this the
sample optimum on a statistical problem
that does admit a sparse solution so for
instance if you are doing something like
sparse logistic regression in high
dimensions you would actually encounter
these sorts of objective functions where
your theta star has only a few nonzero
entries enough to estimate all address
which are louder than epsilon
not necessarily because suppose there
are quite so suppose your error kind it
all depends on what the what the sort of
error metric looks like right so what
you're saying is true if the error
metric looks like an L infinity norm but
if it looks like say n L 2 norm and you
have a lot of entries that are below
epsilon and you miss all of them then
they can add up to still something
significant squared yes so it's a bit
confusing because the Lipschitz
assumption that needs to be made for
this result is different from the
Lipschitz assumption that needs to be
made here and in particular if you if
you make the same lipschis assumption
here you get P squared which is why the
two the two results are completely
consistent it's just that in this
problem you cannot make a non trivial
statement if you if you make the same
Lipschitz assumption as before you know
are you really doing any non-sports
solution are you keeping a bond unlike
the one norm of the parameters because
you haven't can't discuss geometric
constraints so because this is a lower
bound I I actually assume true sparsity
on the solutions the algorithm that will
match the lower bound does use of a non
constraint though master bond right this
is then you it depends on the 11 which
is a skill um yeah so so the results
okay that the lower bound is assuming
that you have your sparse but you have
an l infinity on the parameter and then
based on that so yeah based on the L
infinity you can relate l 1 and l 0 is
really a minute yeah yeah kind of
correct and so yeah the key the key
difference here is that we get the
logarithmic scaling which which is for
interesting because this is also
something we do see in the
corresponding statistical results for
for the conversions of theta hat to
theta star and in the second part of
talk I will talk a lot more about sort
of the interplay and how these two
complexities might correspond to each
other in a lot more scenarios and again
that the lower bound is unimproved
because there is a matching upper bound
for the mirror descent algorithm in a
certain geometry I think I'm going to
skip most of the proof intuition so well
the only sort of high-level message that
I want to convey is that even though I
showed just a few examples today the the
results are based on completely generic
quantities that you can kind of compute
for many many function classes and use
the framework to obtain results for many
many other function classes for many
many other structural assumptions on the
parameters and kind of all you really
need to do is to sort of construct
packings inside function classes which
kind of represent a hard subset of
functions for optimization and the only
place very we're sort of the the key
sort of new insight where this differs
from what people have already been doing
in statistics is the metric in which you
actually construct this packing which is
something that sort of if you if you
look at it it ends up being very natural
for optimization I'm not going to go
over what the metric is because it takes
a while to describe and but but once you
have this packing site you can kind of
use this metric which captures a lot of
properties of the function class so
things like strong convexity just
naturally enter the results through the
through the metric we use and I did not
again talk about how the geometry of the
parameter set enters the results and
again that naturally enters when you try
to construct packings in this metric it
just automatically shows up but rather
than stay on this any longer I kind of
want to talk about the second aspect
which tries to illustrate
more of these correspondence between
computational and statistical
complexities and that the case we will
use for this is computation of
high-dimensional estimators under
structural assumptions ok so the setup
here is going to be that we have a
certain parameter space again as before
we are in p dimensions but now it's a
bit more statistical so so we really
have a distribution we are going to
think about p theta indexed by each
parameter theta and unknown to us there
is a parameter theta star and we receive
n samples drawn I ID from P theta star
we would like to estimate theta star we
only have a few examples so in the high
dimensional setting p is quite often
much much larger than the number of
samples n and sort of a canonical form
of estimators that many people have
looked at for these high dimensional
problems is to try and minimize a
certain loss function over the data
samples subject to a constraint based on
a regular riser are equivalent you could
of course put the regularizer up here in
the objective to are just equivalent so
statistically what people have shown
quite remarkably in a lot of cases is
that theta hat can be consistent for
theta star even when P is much much
larger than n if theta star can satisfy
certain structural assumptions
computationally of course as we saw
there is again a similar dependence on
dimension when we try to solve
optimization problems such as the
computation of theta hat and what we
would like to understand is do the
structural assumptions that help
computational analysis also make the
optimization for theta hat any easier I
will try to answer this question by
taking an example of a particular
optimization algorithm and showing that
this question can be answered in
affirmative of course since I am
choosing the optimization algorithm I'm
going to make my life easier and choose
the simplest one I can come up with
which is projected gradient descent so
this algorithm is iterative it just to
it's repeated steps in the negative
gradient direction with some step size
note that this is the true gradient here
there is no noise in the gradients this
time and we are solving a constrained
optimization problem so the gradient
steps can take us outside the set in
which case we just project back right so
this is a very very classical algorithm
and its convergence is quite well
understood so in particular if the loss
functions satisfy certain smoothness
assumption then it's well known that it
converges at a sub linear rate like one
on T after T iterations which kind of
looks like the blue line here where what
is also known is that if the loss
function satisfies an additional strong
convexity assumption then this algorithm
converges very very fast so there is a
contraction factor Kappa such that the
log of the error decay is linearly and
that's why this is called linear
convergence now to understand the
behavior of projected gradient descent
on these high dimensional problems we
are going to start with a specific
example of sparse linear regression so
the setup is going to be that we've got
n data samples each sample consists of P
dimensional data vectors x i's and the
regression outputs are generated
according to linear model exceed a star
plus W and because we are in a high
dimensional regime we need to make
certain structural assumptions so in
particular in this case we assume that
Kira star has at most s nonzero entries
and in that setting an estimated a lot
of people have looked at is the
so-called lasso estimator where you try
to minimize a least squares loss on the
data samples subject to an l1 constraint
on the parameter we try so so so the
pair xiii is our is an iid sequence
chosen from distribution some joint
distribution over XY pair for now not so
just that why is generated according to
this distribution so you draw an X
according to an arbitrary distribution
and then pick where W is again some zero
mean noise
okay so yeah select X I and there's some
distribution WI is some independent
noise and that's it yeah yeah exactly
exactly right okay so so this is the
problem that we want to understand and
there's been a lot of work both
statistically and computationally on
trying to understand the properties of
this algorithm in particular there's
been a lot of off-the-shelf methods that
have been applied to the computational
problem as well as people have developed
specialized algorithms unfortunately
none of the results that we know so far
even explain the behavior of projected
gradient descent on this very simple
problem so to explain why let us first
just see what the algorithm behaves like
when apply to this problem if we just
run the algorithm on the problem for
many different problem sizes what we see
are linear rates of convergence and let
me remark that this phenomenon holds not
just for sparse linear regression but
for many many other structure problem in
high dimensions so for instance things
various low rank problems and so on to
see how this is not explained by the
current theory let's dig into the
assumptions that theory often makes a
little deeper so often it's assumed that
we have the smoothness and strong
convexity assumptions for the linear
convergence and let's see these
assumptions in a very simple setting
where the data vectors are just drawn
according to a normal distribution with
co variance Sigma okay so what the
smoothness assumption requires is that
if you take the hessian of the loss
function then its largest eigenvalue be
upper bounded now in this case the
hessian matrix is just x transpose x
over n and standard random matrix theory
tells us that the largest eigen value of
that is at least the largest eigen value
of Sigma plus quantity p over n and if p
is much much larger than n this upper
bound is going to be terrible so we kind
of do not really have smoothness
effectively is p is much larger than n
what's even worse is that strong
convexity assumption
does not hold so strong complexity
requires that the smallest eigen value
of the Hessian be bounded away from zero
in this case x transpose x over N is a P
by P matrix that has ranked at most n so
there are at least p minus n zero
eigenvalues and you are not going to
have a non degenerate curvature on this
problem so numerically this looks like a
pretty ill conditioned problem if you
think about it from sort of the worse in
a worst-case scenario and still we are
getting a pretty nice convergence when
we apply the algorithm to it and we
would like to explain this gap to do
that we start from the strong convexity
assumption and look a little closer at
it so what strong convexity requires is
that if you look at a loss function and
its first auto Taylor approximation then
this gap be at least a quadratic term
and this does not happen in high
dimensions so what we do is we could
have some slack we allow a tolerance
term that depends on the same regular
riser are that we use to constrain our
problem and because this is now a weaker
assumption we call this restricted
strong convexity and what we will show
you is that this assumption because it's
weaker can hold even in high dimensions
so just for just very quickly what I
want to say is for instance taking the
special case of sparse linear regression
this assumption is already well known in
the literature from the name of
restricted eigenvalue and people know
how to prove it so we can just piggyback
on the existing literature to just get
this assumption for free from from the
statistical papers we also require an
analogous upper bound so the smoothness
assumption that is often made requires
that the loss function minus its first
orbital approximation be at most a
quadratic term and again this does not
hold with at least with a good constant
in high dimensions so we allow ourselves
some slack again this time a positive to
all wins and since again this is weaker
we call this restricted smoothness and
again we will be able to establish this
in high dimensions for many many
problems
this is the same regularizer are that
you are using to constrain the problem
exactly yeah so the upshot of these
assumptions is that despite being weaker
they allow us to get good convergence
results so recalling the optimization
problem we have what we can show now we
are back to the case of a general loss
function so if the loss function
satisfies the RSC and RSM assumptions
then there is a contraction factor kappa
so that we do converge linearly at a
rate kappa just like we were seeing
however there is a catch we don't
converge all the way down to zero so
there is a there is a because we are
using weaker assumptions we cannot
converge all the way and there is a
minimum error epsilon squared which
depends on the distance between theta
hat and theta star and we can only
converge to an accuracy epsilon squared
now from an optimization point of view
this might strike you as a very strange
result because I am trying to solve this
optimization problem and I don't quite
guarantee conversions to its minimum so
what's the point what we have to keep in
mind here is that the goal here is not
here a height we are trying to solve the
underlying statistical problem and we
want to recover the parameter theta star
and there is already some distance
between theta hat and theta star now it
might be the case that K ducky that you
get is much farther from theta hat then
theta hide is from theta star in which
case we've got a terrible estimator in
our hands it might also be the case that
the theta T you have is quite close to
theta hat and that gives up to a small
constant theta T is as good an estimator
as theta hat is and if we can compute
theta T fast and we have a good
statistical quality on it then we are
happy right so what we are going to try
to show is that in many cases it is
actually the second scenario that we
find ourselves in you want the
simulations do you actually see this
kind of thing that you get if they are
evil enough yeah so you have to you have
to work a little hard to make the
problem actually sort of bottomed out at
epsilon squared and you can even see it
oscillate within that radius but you
have to be a little evil in the problem
design so
I find that a little unrealistic and I
haven't included the results the
simulations today okay you your support
set is much larger is that no no so we
are we are actually not going to make
any claims about about support recovery
this is where you bouncing around oh no
no no it's so it's in the cases where
you do not have exact sparsity if you if
you take an approximately sparse
parameter then then you get the bouncing
around yeah okay so to see how we might
have this sort of favorable scenario let
us go back first to the sparse linear
regression example we as before make the
sparsity assumption and for now we
assume that the data is again Gaussian
although that assumption can be relaxed
fairly easily so what we can prove is
that under these assumptions are a C&amp;amp;R
sm hold with high probability now this
is a this is a bit strange because I am
going to make an assumption here that I
cannot know in practice ever this can be
relaxed it makes the presentation much
harder and the proofs much harder so I
am presenting the simpler version today
so we assume that the constraint radius
Rho at which we constrain the l1 norm of
parameter theta is set exactly to the l1
norm of theta star for now and if we do
that then we can show that we do have
linear convergence as predicted and the
tolerance which we get has a linear
convergence to has the following nice
form so the first part is just the
distance between theta hide and theta
star which is the statistical error on
the problem times this factor that s log
P on n and for statistical consistency
on these problems you anyways need s log
p over n to be vanishing to be going to
0 otherwise statistical parameter will
be inconsistent so so we expect this to
be a very small constant times the
statistical error of the problem which
is exact
what we had claimed and the other thing
I want to point out is that the
contraction factor Kappa here is
actually a random sample dependent
quantity and typically tends to improve
with sample size so we can verify this
in simulations if we run the problem on
a fixed number of parameters for varying
number of samples initially we do not
see our C and RSM assumptions being
satisfied and the algorithm actually
oscillates but as we increase the number
of samples further we start to see
convergence and the convergence
accelerates as the number of samples is
increased further because the
contraction factor improves sorry so
this this improvement is due to that
happen and we will see a more
quantitative demonstration of that in
two slides like the seats exactly when
one log when is the movies one is
exactly doesn't work in one point yay I
i yes i I don't I don't remember the
base I I believe it must be natural log
but I mean even the statistical theory
requires this to be at least like twice
as log B or something no I will find is
still oscillating is also in five is the
first time he's tied to ya know that a
lot of people will be angry with me
breaking statistical lower bounds if I
claimed such things okay now similarly
you could instead of varying the number
of samples you could fix the number of
samples and vary the number of
parameters and again we see as the
number of parameters is increased from
5,000 to 20,000 the conversion slows
down because the contraction factor
versions now we're interesting part is
that we can actually capture the rate at
which the slowdown of the contraction
factor happens quantitatively and we
what the theory predicts is that instead
of holding the number of samples fixed
if you rescale them appropriately with
the number of parameters then this
slowdown should not happen and the
contraction factor should stay constant
so I need if we do to do the rescaling
we see that the three curves exactly
line up on top of each other meaning
that
a particular quantitative prediction is
sharp another problem that falls within
the purview of this theory is the low
rank matrix completion problem and the
way we formulate it is that we think of
our observations as randomly picked
entries of a matrix cedarstar corrupted
with some zero mean noise and we make
the structural assumption that theta
star has ranked at most our and in this
case an estimator that a lot of people
have looked at is to minimize the least
squares error on the observed entries of
the matrix subject to a constraint on
the nuclear norm which is just the sum
of singular values of the matrix and
what this does is it tries to enforce
sparsity on the vector of singular
values so it gives you a low rank matrix
because a small number of nonzero
singular values gives you low rank right
why is what you're gonna get here going
to satisfy that
that condition of yours yes oh so that
remains to be proved and we proved that
yes yes yes so so it can take
substantial amount of work to establish
the RSC and are some assumptions they
are not always straightforward to
establish and I in this case actually
does take some work to to establish them
and again if we set the constraint
radius very very favorably for now then
what we get is that we have linear
convergence to an error that again
related to the distance between theta
hat and theta star for this problem not
in this case not in this case so this
this problem has a very tricky structure
on the RSC and RSM that is a bit
different from the sparse linear
regression problem and you actually do
not get the time so so this is exactly
sort of the minimax distance or not
minimize the distance we know between
this theta hat and theta star there is
an extra log P compared to the minimax
rate for this problem but the this theta
hat minus theta star is exactly on this
order so in this case you can lose
constant factors but up to a constant
factor you still have the right answer
again we can verify this in simulations
and initially we see no convergence
starts to converge and convergence
accelerates as the contraction factor
improves actually equals theta
sterilized yeah yeah yeah so yeah so so
we are not guaranteeing exact recovery
with the optimization from guarantees as
well is this the computational issue is
this is discourage you um actually so so
bad okay so it is possible actually that
in the case where theta hat is exactly
equal to theta star you might be able to
improve the result because typically so
at least the cases where we kind of know
how to prove theta hat is equal to theta
star it often requires some sort some
sort of incoherence on theta star and it
is quite likely that if we assume in
coherence are
C&amp;amp;R sm assumptions will improve and we
might get a better scaling here we might
we might get the same sort of theta
height- head of start our mentoring in
here that we had for sparse linear
regression so so yeah that is actually a
good point but I haven't checked what
what we get for inquiry this is without
incoherency this this assumes this just
assumes an L infinity bound on theta
star immigration case it was that that
was it harder that was that was just
without it without any assumptions so
you just require data star is a sparse
and you you basically require sort of
the the I mean you essentially require X
to have some sort of a sub Gaussian
design right ok so again these are just
a couple of examples the results extend
to arbitrary regularizer from a certain
family and apply to many other problems
in particular for instance group
sparsity with which is of interest for
solving multitask learning problems they
apply to other loss functions so for
instance you can show these assumptions
for generalized linear models although
again the the technical aspects are a
little more involved there and finally
even though everything I showed today
was where you have a loss function with
the regularizer based constraint the
results also apply to the form where you
have a loss function plus regular riser
and in that formulation has the nice
aspect that you do not need to know
these quantities such as l1 norm of
theta star or nuclear norm of theta star
for that particular formulation so so
actually setting the parameters in that
formulation is hard is easier and so
quite naturally the proofs get harder as
going to mention something about
distributed stuff but I think in the
interest of time I am going to just skip
right to the conclusion and would be
happy to talk more about it offline so
what's this unit okay okay sure sure so
so kind of the basic picture we are
going to have we sort of carry for the
distributed framework in our heads off
and is that we've got a big data set we
split up the data set into smaller
chunks giving each computer and network
a chunk of the data so based on what
each computer can define its local loss
function based on just computing the
sample loss over its subsample and then
minimizing the entire the entire loss
over the data set kind of corresponds to
just minimizing the sum or the average
of these local functions and you want a
distributed algorithm to minimize this
so in particular you what I mean by
distributed algorithm is if you are
given a sort of communication topology
underlying these this network then you
only you want an algorithm that respects
the topology only communicating along
the edges in this graph right so there
are kind of like the message passing
algorithms that people have seen a lot
do people usually run these things in
clusters that are not click services so
even so in a cluster you basically have
it is not it is not exactly a click it
is more often like a sort of hierarchy
because you're okay with everything
within the same rack talking to each
other but then you want to minimize the
across rack communication and so on so
there are certain constraints that are
definitely present and there has been a
lot of work in in so this particular
form has been kind of one of the
canonical forms of distributed
optimization problem the people and
controls and distributed algorithms
literature have looked at for many many
years and there has been a lot of work
what we did was so we also proposed a
new algorithm to solve this problem but
the emphasis in our work was to
understand how the structure of the
network interacts with the convergence
of the algorithm and so we characterized
the convergence dependence on the
structure and showed kind of things that
you expect to see that a complete graph
topology is the best apology and
expanders are almost just as good and if
you go to the chain graph that is going
to be the worse unfortunately all the
results that most of the results that
were present before were kind of the
worst case results that you get for
chain graph so so kind of having a
topology based convergence rate was one
of the key contributions in this work
and we showed robustness to certain
things like stochastic communication so
if you've got link failures or if you
just want to randomize the communication
to save on communication then you can do
that we showed robustness to stochastic
gradients so again you don't need to run
the underlying algorithm with exact
gradient evaluations but what was
unsatisfactory about this and a lot of
the past work was that typically we
observed loss in convergence rates due
to decentralization so because we are
trying to compute this solve this
problem in a decentralized way where the
functions f eyes are not known globe
centrally but known split over a network
we were kind of seeing that having a
complete
where everything is present in the same
place is the best thing and while that
makes sense in a completely arbitrary
case where these local functions f eyes
are arbitrarily different it does not
necessarily make a lot of sense in the
statistical case because these functions
share a lot of structure they are based
on iid data from the same distribution
quite often and we got have we got to be
able to exploit their dependencies
somehow and so o hard here along with
other with Lin Chow foredeck aligned
Ronnie gladbach track had some very nice
work on showing that you can actually if
you run a more synchronous algorithm
then so the algorithm that you get in
the most general setting they can often
be very very asynchronous if we run a
slightly more synchronous algorithm
compared to them then they showed that
you can actually get up to linear speed
ups as a function of the number of nodes
instead of having a slow down due to
having the having the problem split over
a network and we have we have a sort of
further built upon that and sort of hat
we have an algorithm that is slightly
more I synchronous but still less
synchronous compared to less
asynchronous compared to the arbitrary
case which still preserves these linear
speed ups at least asymptotically so so
kind of again here the the key message
that I want to convey is that this is
this is a very well researched problem
and distributed algorithms but because
people were looking at very general
structures if you further exploit the
statistical dependencies in the problem
you can actually get a much better
result then that is possible in the
worst case okay and by extending these
ideas and combining a lots of other lots
of other tricks and techniques we were
actually able to solve the sort of click
prediction problem over a big data set
that I mentioned early on over a network
of thousand nodes training in a in a
matter of something close to an hour
okay so I hope wrapping up the message I
managed to convey a bit is that in many
modern statistical problems data is not
the only bird like computation can be an
equally important bot bottleneck and
what is very fruitful for many of these
problems is to try and attempt a joint
analysis of their statistical in
computation complexities rather than
trying to understand them individually
and this is a challenging problem in
general so we looked at various
different ways of posing the general
problem and saw some results about
fundamental computational complexity of
certain estimation problems about how
one can leverage statistical structure
to for better analysis of algorithms and
how we can get better distributed
algorithms for statistical learning
problems very briefly now this is just a
brief very very brief survey so there's
lots of more directions for future work
and lots of other ways of looking at
these problems so for instance i talked
about one particular oracle model of
computation today there are many other
Oracle's models and in particular the
one I described today capture certain
computational aspects of the algorithm
and ignore certain other computational
aspects so getting more and more
realistic Oracle's would be very
interesting for getting more and more
realistic complexity results in the
Oracle models getting more better
asynchronous algorithms for distributed
settings that can still preserve
speedups again thing is going to be very
important because you can't really
afford to always run a very synchronous
algorithm in a distributed setting and
one thing that I did not talk about
today but is off a lot of interest to me
and that I have worked on is again
oftentimes you just want to impose a
hard computational constraint on your
algorithm and demand that it do
something reasonable within that
constraint rather than kind of just
potentially giving up and not doing
anything and in particular we want some
understanding of what can be optimal
algorithms statistically under various
computational budget constraints when
imposed in a in a hard manner
and this is again something I've looked
at for at least one one scenario of
model selection and extending these two
other statistical settings is again of
great interest to me and the community
as a general I think so thus all and
thanks a lot for your attention so in
your ways at the in the first in first
part of the talk to babies minimax
raised you kind of ignore the geometric
at least instead of theorems you enjoy
the geometric families because really
what you have is unbounded perhaps so
you can assume that the parameter set
for instance what we assume contains an
L infinity ball of radius R and then the
radius of the L infinity ball enters the
results and for instance so you can see
and so for instance if you take in so if
you take an l infinity ball of course
then it's just a unit ball of l infinity
then this is just one but for instance
if you take a unit ball in the l2 norm
then this becomes like one on route d
and a 100 p so you kind of that get the
dimension you lose the dimension factor
over there to ball and so on so you have
with the ball and contained is the
function of the normal this is the
Infinity know so so unfortunately the
way with the way our constructions work
right now are they kind of start from
doing atkins we start from doing a
construction exactly over the L infinity
ball always so so that is a I have I
have ideas for kind of making all that
cleaner and also sasha has a paper where
he's also tried to do similar things but
again they they yeah so that there are
there is hope and there are ways I think
to get around this but for now it's all
sort of L infinity centric near this is
a medicine screaming are very much
the geometry of the problem and this
doesn't show that that algorithm is tied
for life well so actually there is there
is the recent result of carthak which I
think so in the online setting Karthik
head of paper at nips which shows so his
paper was called on the universality of
Mira descent which was trying to show by
arguing about the value of a game but
and Karthik was telling me that he has a
stochastic result but it's not available
yet so I I don't know that might already
be addressed now yeah you mentioned sort
of Netflix completion from so that might
be a little harder to validate or not
yeah so unfortunately we at least the
existing theory does not give a direct
way of paralyzing it although I I think
some of the more recent work we that's
kind of close to completion that we have
might might help develop parallel
algorithms for that as well but yeah we
we don't have anything at the moment at
least I mean one thing is that if a key
bottle I can sort of the netflix problem
is that in order to at least the kinds
of algorithms I talked about today you
have to sort of do SVD s over a big
matrix so if you you could just consider
paralyzing the SVD part and that's
something that numerical linear algebra
people have looked into and that would
already be a substantial help if you can
paralyze the SVD part for the problem
but it's again in practice you don't
need to do a full SVD because these are
low rank matrices there there's lots of
sort of approximate SVD tricks that
people do use so yeah it's kind of still
open how how to properly paralyze the
problem I think
practicing serious or far away Center
for lower bounds
say so how how do you bounce apply to
send from uh which phone he said he
doesn't happen yeah right you know if
you don't ok so then give up on the
paralyzation so what's your general
setup can I apply to this madness
competition from no no no not this this
distributed showed up right now does not
apply oh he just wants to know how well
your non distributed results on the
netflix problem are relative to other
best known ooo icic so as far as i know
vide ok so under certain conditions
people have studied actually
computationally computationally much
simpler techniques based on just sort of
doing a sometimes even doing a single
fresh holding of the singular value
vector can work but that requires again
stronger conditions and error under
these conditions i do not think that
there was linear convergence known for
any matrix completion algorithm to the
best of my knowledge i think what people
were using mostly was this sort of
augmented lagrangian or nestor all-star
algorithms which all I mean the
assumptions that went into them were
sometimes very similar to the RSC and
RSM assumptions but often those
assumptions were kind of just made
without proof and part of the work that
we did was to make sure that we have all
the conditions we need proved concretely
so I think and this really i think is
kind of the best possible we can hope to
get i mean we can we can hope to maybe
improve this part a little but this part
kind of is the best possible we can hope
to get i think because even if you had
true smoothness and strong convexity you
cannot do better than linear convergence
so yes yes yes oh def well okay for my
race completion you can never have a
righty you can have you buddy yeah yeah
under incoherence I hope it will
okay see what you have in the first one
because it is all right right well no I
mean I mean you can you can like you can
have very very correlated designs you
can have basically taupe alerts designs
that would grossly violate our IP and
that still you can get I mean we
actually do do do simulations on that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>