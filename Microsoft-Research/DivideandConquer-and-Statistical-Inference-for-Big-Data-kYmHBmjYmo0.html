<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Divide-and-Conquer and Statistical Inference for Big Data | Coder Coacher - Coaching Coders</title><meta content="Divide-and-Conquer and Statistical Inference for Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Divide-and-Conquer and Statistical Inference for Big Data</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kYmHBmjYmo0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you I'm delighted to be here it's
been a great day with great talks this
is my second time in China in two months
talk about big data it's a phenomenon
all over the world and there's lots to
do if you're a student looking for
things to work on this problem is just
getting started so one of the main
things I want to do today is introduce
some of the conceptual issues having
surrounded big data it's the kind of
field where one person sitting in a room
thinking about the problem can actually
have a really big impact which seems
kind of surprising because you think you
would need huge numbers of computers and
huge amounts of data to work on the
problem almost by definition and I hope
by the end of the talk you'll be
convinced that that's actually not the
case so there's a phenomenon which I
don't really have to spend much time on
I think everyone knows about it there's
huge amounts of being data being
collected and John Hopcroft talk sort of
reminded you of some of this one of them
four factors is big science generates
huge data sets astronomy particle
particle physics or it's good examples
genomics the other is information
technology which is the one which is
perhaps more present in all of our minds
activity on the Internet
I'm generally masley data for
personalization and then linking in
these sensor networks are becoming
Perseids so that's the phenomenon but
what's the intellectual problem really
is it just big is it just store more
storage more capacity more of everything
that solves the problem I you know so
computer scientists think about the
world in terms of resources the
classical resources have been time and
space to sort of underpin the field for
all these years and energy to some
degree data has not been thought about
really as a resource it's the things you
apply resources to okay but what's new
is that data really now is being viewed
as a resource something you give me that
I make use of and get value out of and I
get knowledge out of it so if it's a
resource we have to combine it with all
of our other resources like space and
time and we're worried about the
trade-offs in designing a system that
takes into account that we have some
data resource we have some time with
some space and they trade off in various
ways
the funny thing about data though is
that it's an unusual resource it should
be the case with any resource the more
of you give me the the happier I am
alright if I give you more time more
space you're happier and happier and
with data that's not the case and so if
you go into a company and ask them
what's their biggest problem and the
problems they have too much data and so
in our current state of a knowledge it's
really really the case that more data
causes more trouble and we have to work
around this issue so there's two reasons
for this one statistical and the other
is computational the statistical ones
that may be a little more subtle so
let's spend a little time on that so
let's think of like a database person
thinks our data is is an array a table
the rows are say people and the columns
are descriptors of people so classical
databases maybe you had a thousand rows
thousand people in your database
how many descriptors would you need well
maybe not a few no more than a few say
where the person lived their age their
height their income and that's enough to
kind of make some of the distinctions
you might care to make in a database now
we're thinking about billions of rows
because we're really interested in all
the personalized details of all of you
so if you live in yunjin and you like to
ride bicycles and you love Michael
Jackson what's the probability you're
gonna click on my ad or what's the
probability that you're gonna have a
certain disease or etc etc where is it
every single one of you so we have data
about all of you okay so the number of
rows are growing somehow linearly maybe
the number of columns have got to grow
too because we're interested in all
these distinctions maybe some of the
columns are your genome they're what
food you ate yesterday they're your
preference in music what books you like
to read and so on so forth will collect
all these things the problem is that
we're interested not just in the
individual columns we're interested in
combinations of the columns okay it's if
you live in Tianjin and you like to ride
bicycles and your favorite food is come
called chicken then will you do
something that's a particular
combination of all the columns and the
problem is this I have an exponential
number of combinations of the columns so
if I'm a linear growth in rows linear
growth in columns I have an exponential
growth in the number of
I'm considering so let's think about a
medical example let's suppose one of the
columns is liver disease alright so I'm
interested and I look at all the ones
every time I see liver disease that's a
one if it's a zero you didn't have liver
disease so let's now look at all the
cases were there liver disease there's
gonna be some combinations of the
columns which is gonna perfectly predict
liver disease alright maybe it's you
live in two yunjin and you'd like to
ride bicycles and you eat bananas all
right every such person had liver
disease alright so now you arrive with
the doctor and they say where do you
live and you say chun-yin and you say
what do you like to do on your on
Saturday ride bicycles what's your
favorite food bananas they say we have
to take out your liver oh I'm sorry
alright and so the problem is that in
any data set if we have exponential
number of things we're looking at and
linear numbers of items in which to
verify those things we're gonna find
meaningless patterns just by noise alone
alright and so it's Dana get bigger and
bigger and bigger this problem is
getting worse and worse and worse okay
so big data is not this wonderful thing
give me all the data I call all the
knowledge in the world big data is a big
problem give me this data it's harder
and harder to turn into knowledge real
meaningful things that are real that we
can act on and believe okay so now
statisticians worry about these issues
this isn't new how do I get rid of the
noise and here the bias and get the
knowledge out but statistical procedures
are themselves algorithms and they take
time and we have to run them on the
computer and maybe in big datasets they
take too long all right I can't make
quick decisions anymore so we really
have a really big problem on our hands
we don't know how to run statistical
procedures and make good decisions
quickly at scale alright so in fact the
second reason this is a long slide but
it's an important set of points the
first reason is statistical the second
really is computational algorithms take
a certain amount of time to run say in
log in and cubed and squared all right
suppose I need to make it as a decision
in a second like in an online auction or
less than a second right you start
giving me a certain amount of data and
my in log n algorithm it's perfectly
good it'll run in the one second but now
you give me more and more data and
suddenly in log in it's not gonna finish
in that allotted amount of time
what do I do I have to start maybe
throwing away the data but what rate do
I throw away data it could be my
statistical error will grow if I start
throwing away data too fast so I have to
throw it away more slowly but now it'll
take me too long alright so we really
have a big problem when we bring
together time with statistical error
with growth and data set sizes we don't
have algorithms that will gracefully
scale to larger and larger data alright
so this these are real problems if you
go with the industry and look at its
groups that have really large data sets
these are the problems that are facing
and I view these as fundamental and
difficult here's one way to say a
theoretical goal which tries to get at
this give an inferential goal that's the
statisticians kind of goal usually a
risk function and a fixed computational
budget a minute of second an hour
provide a guarantee that the quality of
inference will increase monotonically
has data accrue without balance so
without balance we like a computer
science I don't have to face this
problem every generation I want to face
it once and for all what are the
principles that scale statistical risk
against time and I think we're very far
away from being allowed to solve this
problem I think this is going to take
several decades to solve so I'm gonna
talk about a little bit of progress
we've been making we've been working on
this one of them is kind of a bottom-up
problem we're gonna try to bring
algorithms more fully into contact with
statistical inference and I'm gonna go
to my favorite algorithm principle which
is divide and conquer and tell you about
how it relates to statistical inference
and it's a it's a interesting almost
paradoxical inference is about
aggregating things bringing things
together the more things you brings
together the small the error bars are
the better things are defined Congress
about somehow separating things out so
they sent how fight each other the basic
ideas and statistics and computer
science or some time a little bit in
conflict and that intrigues me so I'm
going to talk about that and then we're
going to go back to this more
theoretical problem and we'll talk about
how to trade or was it me the trade-off
statistical efficiency and computational
efficiency alright first problem
something we call the Big Data bootstrap
so acknowledge some of my colleagues at
Berkeley I've been working with this on
this for a couple of years now so the
bootstrap is the solution to a really
important problem
assess the quality of inference most
machine learning research doesn't worry
about the quality of inference you have
an input it goes into some box of some
kind and out comes an answer say nine
point five all right but most
statisticians aren't satisfied with that
they want to know what the error bar is
on that nine point five so for example
if the number was bigger than ten you're
gonna take out your liver okay and the
number came out nine point five all
right well is that really nine point
five is there a big error bar is it
possible that it's actually bigger than
10 or is it really clearly not bigger
than 10 all right to make real decisions
in the real world you have to have error
bars all engineers know this and we
computer scientists should know this as
well so I'm in fact worked with people
in the boot in the in the database
community trying to build databases that
query comes in out comes an answer with
an error bar on or on every single query
that's what we should be doing as
real-world engineering oriented people
error bars everywhere okay
so let's turn to a different field
statistics and ask how do you get error
bars on things well this is kind of
interesting on very simple things like
the sample mean there's a formula you
can pick an elementary stat class and
you'll learn that if you calculate the
sample mean and it's nine point five
there's something called the standard
error of the mean which is a formula
it's something like this sample standard
deviation divided by the square root of
the number of data points and so you
plug that in you get an error bar all
right but what if you didn't calculate
the sample mean what if you calculated
the median you sort the data to find
that thing in the middle that's called
the median and that came out to be ten
point two what's the error bar on that
well there is no formula all right so
how do you get an error bar on something
like the median all right well there is
an answer that's something called the
bootstrap which will allow you to do
that it's a generic procedure for
calculating error bar so the problem is
ask me that bootstrap doesn't scale to
large data so we'll talk about that so
what's the quality of inference issue so
we have data x1 through xn a machine
learning or stat person might calculate
some kind of a prediction or a parameter
estimate based on the data so I'm
calling it a parameter but it could be
the output of a classifier it could be
any kind of prediction let's call it
theta n I'm not interested in this talk
about the ad and
for scaling that black box I'm
interested in the quality of theta n
what's the error around it okay and
there are procedures for calculating
that not just the theta n but the
quality of theta n okay so here's what
you would do if you're an ideal
frequentist statistician this is almost
the definition of being a frequentist
which is that you wouldn't just have one
data set of size n you'd have multiple
data sets of size N and on each data set
you calculate your median say or your
other estimate okay and now because
they're different data sets they're
going to fluctuate and you look at the
spread of those things and that's your
error bar okay that's almost by
definition what you mean by an error bar
it's the error you get on multiple
realizations for the data all right well
you don't have multiple data sets if you
were the supreme being in heaven you
could do this again and again and and
generate data and look at the
fluctuations we only get one data set
all right so let's think Inception about
how you might be able to nonetheless get
around this issue and get error bars
even though you have only one data set
well the data came from somewhere
there's an underlying population up
there in heaven that generates data and
so let's let's make a little histogram
like object here a distribution
reflecting the underlined population all
right so if you were the supreme mean if
such a thing exists you up in heaven
where you'll be able to do what we have
on this slide here you would take the
population you would generate one data
set two data sets M data sets and each
one of your data sets you'd calculate
the thing you care about the prediction
theta 1 theta 2 theta M and then they'd
go into some formula for the spread and
that's your error bar all right now one
nice thing about this is that if the
supreme beam has a parallel computer he
or she could do this in parallel because
you can generate these data sets in
parallel each computer goes off computes
the estimator and then gets it all
brought back together to calculate the
error in the estimates okay now we don't
have multiple data sets we can't do that
slide but what we can do is the
following we observe that if there's an
underlying population the data came from
that population and we can think of the
data not as a list of numbers now but as
a histogram
you probably all know how to make
histograms right out of the data the
histogram itself is a distribution it's
called the empirical distribution but
it's a distribution it can be used to
generate more data because it's a
distribution even though it's not as
curvy as the night one up there
it's itself a distribution you can
sample from it all right so there's this
beautiful idea which is called
approximation which is that that
empirical distribution approximates the
truth because it came from the truth all
right and so you can pretend now that
you don't have that interline truth but
you have this approximation of the truth
and now you can live in a world where
you're the supreme being and that's the
truth and you can generate data from
that truth and you can do it not just
once you can do it multiple times so
it's a very subtle but very deep idea
which you could take one data set and
from that generate multiple data sets
that idea is called the bootstrap and
it's due to Brad Efron in 1979 he got a
National Medal of Science in the United
States for that so talk about awards
that's kind of the biggest one you can
get for this very simple idea one person
sitting in a room thinking a little bit
about the problem and and realize this
can be done so here's the picture you
take in the data and from the data you
form a histogram and you can sample from
that histogram multiple times M times
say you can do this in parallel this is
that even though this algún was
developed in 79 there was no cloud no
distributed anything this was a
perfectly paralyzed above procedure okay
and then you bring that all together on
your central server and you've got an
error bar now and so you could do this
for any query you could do why you know
why can't we have all of our databases
doing resampling on the cloud it sounds
like a wonderful idea and that's sort of
the thing that I thought about a couple
years ago why don't we put the bootstrap
into computer scientists more fully but
there's a God show so let's think about
a terabyte of data sitting maybe on a
server somewhere or maybe it's already
distributed if you resample if you take
that data set and you do what I said you
take it and you sample from it well I
should go back here I mean I go back
with it yes I can do this right what
does it mean to sample from that
histogram right there I should make this
clear
all right well it was based on the N
data point
but forget that that's gone it's now
just a histogram it's a distribution I
can sample from it any number of times I
want but let's sample from it end times
because that's the scale or data
occurred on and so I need a sample from
the end times get the same size data
sets all right now I can do that once
and do it again and again and again what
does it mean to sample from a histogram
it means to take one of the points it's
a discrete distribution so I'm going to
get one of those points it's it's
discrete and let me get it with
proportion to the probability proportion
of the height so I get one of those
points and then I do it again
I could get the same point over again or
I could get a new point and I do this
again and I do this n times some of the
points that form that histogram
I'll get several times and some of them
I won't get at all that's exactly the
same as sampling with replacement I take
out a Sam thing I put it back in I could
get it again I could get it again and
maybe I could get a different one all
right so the bootstrap is often
described as take the original data
resample it with replacement n times and
do that many times
that's the bootstrap okay now if I
sample a terabyte if I sample n data
points with replacement you can do a
little mathematical calculation to see
you get 0.632 of the data points occur
in your resampled data if I had a
terabyte that means I get six hundred
and thirty two gigabytes all right now
I'm not gonna send six hundred and
thirty two gigabytes off to my my two
hundred servers so I can do the
bootstrap that's good hopefully that's
gonna hope lessly
blow away my network all right so you
can't do that with a terabyte of data
this sounds like a bad probably there's
our main procedure for getting error
bars on arbitrary estimators we can't do
it at scale and so only getting you know
terabytes you know small date of these
days all right so this is a serious
problem all right so there is another
statisticians worked on this for a
little while
they weren't thinking about computation
very much they're thinking about theory
the bootstrap fails in some cases
actually and another procedure came out
that fixes some of those theoretical
cases it's called subsampling but it
seems like it's the answer and it's not
gonna be but let's think about this for
a minute
so subsampling is the same pictures
before there's an underlying truth you
get a data set and form the histogram be
done that and now what you do is you say
well in in is too big let's now take a
sub sample of size B maybe size square
root of n to really bring it down a lot
now I can commute my estimator on the B
points and I'll get a certain number
know nine point five but I could down do
that again because there's many ways of
taking B points out of in so I could do
this many times and we get some multiple
values of the estimator I'll get some
fluctuations all right so it sounds
pretty good out of one data set I'm
getting multiple values of the estimator
the problem is is that if you just do
that naively that's wrong because B
points the size of the error bars
depends on the number of points right if
I have lots of points and we get small
error bars small numbers of points big
error bars you know the sample the stats
you divide by square root of n for the
classical sample there the standard
error of the mean all right so I'm here
to get fluctuations but they're going to
be on the wrong scale and for a
statistician that's everything if you
get the wrong size the error bars you're
just wrong okay all right so you can't
subsample and just naively compute
estimators and then return that as the
answer that just gives you the wrong
answer this seems problematic because
you have a really big data set the only
thing I can think of doing is taking
smaller sub samples of it you know it
seems hats do you have to do that but
you get the wrong answer so these guys
who put proposes realize this of course
and it's this key issue arises that
you're on the wrong scale so they said
well let's take that estimate that
that's that error bar which is now too
big because I had smaller data sets and
rescale it so it's now on the right
scale right but how do you rescale you
need to analytically correct the error
bars and you don't know in general how
to do that so for some black box at
somebody and a database system put in a
so-called user-defined function you
don't know how the scaling is for
classical estimator like the mean it's
the square root of the neighbor data
points but it's not true for other
estimators okay so any but that's a
problem you'd have to do theory for
every single black box that someone put
into your database that seems a little
bit problematic
well there's even a worse problem which
is that if you try this in practice it's
an engineering defect so we did some
bunch of experiments here's one example
we took
50,000 data points of synthetic data
it's a hundred dimensional space that's
what D means and we do least squares and
calculate the error bars and then
because we did the synthetic we know the
true error bars and we can compare and
see how good our error bars are all
right and so here are the results of
doing that actually maybe one more point
on this slide if you'll see up there do
I get a but no B of n the last the last
next to last bullet B of n equal into
the gamma so that's a fraction so n to
the one half is the smallest size we
take and into the one would be no
subsampling at all so we range between
1/2 and 1 as our sub sampling procedure
alright so in this slide you see there
error which we can compute because we
have it's synthetic data and you see on
the x-axis time as this algorithm is
subsampling away and running and running
and running and the different curves are
the different values of that gamma
exponent all right so you see when gamma
is 0.5 which means square root that's
the black curve at the top
the error is terrible actually I should
say first the blue curve is the
bootstrap so that's kind of our gold
standard here we know the bootstrap is
gives correct error bars there's a
theory behind the bootstrap so you can
see is a computational object it sort of
comes down pretty quickly and stabilizes
to some fixed error because the number
of data points has been held fixed here
that error doesn't go to zero it sort of
stabilizes some value anyway with square
roots subsampling it just fails it's
that black curve at the top it's not
approaching the bootstrap you know in
reasonable time when you get point six
that's the green curve it's still
failing now point seven point eight
those are the curves below the blue
curve there it's beating the bootstrap
it's a more efficient use of the
computers of the bootstrap which is kind
of surprising the bootstrap is viewed is
really really efficient it has a 1 over
N and convergence instead of 1 over
square root of n like the central limit
theorem it beats the central limit
theorem which is every way you know it's
pretty surprised about when that was
proved in the 70s 80s so for those guys
it actually beats the bootstrap that's
pretty cool but now for a point 9 it
goes that's the light blue curve it
fails again so there's a little it fails
it succeeds it fails there's a little
ray
of gamma where it succeeds but that
value is not known off priori it's
problem dependent we knew the truth here
so we know the right value but you
wouldn't know that in practice so it's
an engineering solution this just isn't
this isn't this isn't workable okay so
we have another idea this is called the
bag of little bootstraps which which
does solve this problem all right so
it's gonna work with small subsets of
the data so it's a divide and conquer
procedure all right so it's appropriate
for distributed computing all right but
you don't have to do any analytical
rescale Alena somehow magically can get
the right scale even though it doesn't
rescale analytically all right so it's a
little simple idea even though
bootstraps been around for 40 years
statisticians weren't thinking about the
computational problem of scale so they
didn't really worry about this issue so
here it is here's the idea on a couple
of slides let's go back to that picture
where we had the underlying truth we
have the data we get of size N and we
took a sub sample of size B so we're
gonna still think in those terms okay
right but now notice that the data of
size B also come from the truth they
went through an intermediate step but
forget that for now they still they also
coming in directly from the underlying
truth all right so you form a histogram
based on your sub sample it also is an
approximation of the truth it doesn't
look very good in my little picture here
because I you know in is now 10 but
think about terabytes and gigabytes
it'll be also a good approximation of
the truth okay so now forget that you
ever had the truth and now you're in a
world where you're the supreme being and
that is your truth and you can sample
from that it was based on B points but
it's a distribution you can sample from
it as many times as you want and so
here's the key conceptual step that any
of you in the room could have thought of
which is that you take that object even
though it's based on viewpoints and you
sample it in x you now are on the right
scale because you got a data set of size
n which is what you needed many of the
points that are going to repeat it many
many times because it's only these
points and that's small and you sampling
it many many times in okay and some of
the points that might not occur at all
okay now you had B points which is
really
small now let's think distributed okay I
got my little sub sample of size B I can
send that to one of my processors on the
on the just you know my distributed
servers okay I don't want to explode
that back up to size n either on the
network or locally that would be stupid
but I don't have to I just keep a count
of each of the B points how many times
did it get subsampled okay so it's
computationally efficient wherever you
put the data alright so we can take that
object right there and sample that n
times and now we're back on the right
scale and we can do that
multiple times and do the bootstrap
starting with that object instead of
starting with the original size
histogram alright so we bootstrap the
sub sample we're putting together
subsampling with bootstrap all right so
I'm gonna show you a picture here which
summarizes it's an l2 loop
it's a nested procedure you take your
original date on the left there you take
a sub sample of size B on one processor
and then you bootstrap that sub sample
resampling it with replacement n times
not B times if you get out now a bunch
of estimator values and you plug that
into your formula for your error bar so
that top box there is on one processor
it got a bootstrap error bar which is
actually a correct error bar because it
was based on samples of size n ok all
right so why don't we just have one box
up there we've solved the problem with
one box well it was based on one sub
sample of size B which is small right
and so it's gonna be correct but noisy
so whenever you have something that's
correct and noisy just average so we're
gonna think about now the fixed one
quantity there's a third dimension here
which is this distal risk how error how
much error do you have all right so your
relation to a three dimensional plot but
what we're gonna do is think about fixed
wrists like you know accuracy 0.01 I
want to make sure you guarantee me I
have accuracy 0.01 and now you can trade
off time and samples to achieve that in
many different ways
all right so look at some point at
regions on this plot a statistician
would say well there is a certain if you
want to achieve a certain fist who fixed
risk at some point you have to have a
certain number of samples or you can't
achieve that below that number of
samples it can't be done that's called
min
Mack's theory and statistics so there's
a well-known theory and it has its
independent of run time whatever
algorithm you can't do better than
something that's minimax
similarly in computer science there's
some notion of a lower bound alright
that you can't solve a certain problem
below a certain run time no matter how
many samples we're talking about and
it's a little harder to kind of think
about because it depends on the
computational model but it's sort of
there
you know implicitly in competition
complexity theory alright so we got to
think about actual real algorithms as
points on this plot they achieve a
certain risk with a certain run time and
a certain number of samples and
presumably most good albums will be some
curve like this a stupid alga will then
be up there to the right okay
so we're gonna use all those algorithms
as we weaken our algorithms as we get
more and more data all right so we've
thought this through and I'm going to
kind of go quickly now I'm running out
of time a little bit but in what in a so
called denoising problem kind of a very
classical place where we can actually
get some analytical results so D noisy
means you give me data Y which is a
noisy version of X star or Z is the
noise with some scale Sigma and I want
to recover X star from Y okay how can
you do that well I want a imagine my
possible solutions are on a set s so
that was at the very top the signal
comes from some set s may be a sparse
vector as john hawkmoth talked about and
i want to recover that with some high
accuracy alright and so a way to set
this up as a estimation problem as you
say let's solve a convex problem where I
take my sample mean Y bar I try to find
that X which as close as possible to Y
bar ranging over X in the set s that's
the natural estimator there in the set s
okay maybe s is now a complicated object
I can't do this I can put a convex
relaxation here and minimize over the
set C which is a convex relaxation of s
all right so a picture Oh makes that
clear maybe s is a discrete set you know
sparse vectors and I can look at the
convex hull of them as my first C which
is a convex set and talk about
characterizations of estimators we'll
start to see and not s ok and in fact
there's a notion of a cone there which
you can see on the right which is
defined up there
sort of around the optimal point all the
directions that lead you to stay inside
the feasible set alright so here's our
one theory there's a theorem we were
able to prove which is good a guy lead
to some results which is that the risk
of any estimator based on that convex
set C is upper bounded by the left end
quantity there is the risk so I want to
get upper bound on that an expectation
on something called a Gaussian
complexity so I'm doing a little bit of
math late in the day here I apologize
for that but if you want to read more on
Gaussian complexities it is an object
studied in in functional analysis and an
empirical process theory it's called the
Gaussian complexity and it's gonna be
our guide to get to working on this this
trade-off curve ok so we can compute the
Gaussian complexity alright so corollary
of that theorem is that if you want to
achieve a certain risk you have to have
a certain number of sample points which
is bigger than that and then that upper
bound ok all right so that was with one
see but maybe that C is also
computationally hard to work with it
requires n cubed or in squared to work
with that see to do optimization over
that kind of set so let's look at some
weakening z-- of that set so C prime
might be a little bit bigger and a
little bit easier to work with and in
general we can get a hierarchy of these
things and these things have been
studied in applied math and combinatoric
Sancta theoretical computer science
they're called a relaxations you have
various simple forms of them shown there
and they are ordered by computational
complexity and what we are talking about
is ordering them by statistical
complexity and then trading them off
alright so in the context of this
denoising problem you can carry this
mathematical program out and get results
and so here are a couple of examples if
our set s consists of a set of cut
matrices and again i think john talked a
little bit about that what that set
means then you can characterize the
runtime and you can characterize the
sample complexity and you see results
which are you know super polynomial P to
the 1.75 for run time as you look at
complicated C's and simpler and simpler
C's and you can so the run time is
decreasing and the sample complexity is
staying fixed at square root of P
so this tells you you can go to much
simpler algorithms when you get big data
and not lose in statistical risk at all
if your problem is to optimize over the
space of cut matrices and we've done a
bunch of other examples here's just
another one for quickly a if your signal
set is all perfect matches in a complete
graph then if you work with the convex
hull of that your run time is lower
bounded by P cubed if you work with
another relaxation call the hyper
simplex it goes down to P to the 1.5 and
then we similarly can get results that
it go as square root of P for the sample
complexity and so now we actually really
have trade-offs we have statistics in
the right column computer science in the
middle column and we can move up and
down these columns simultaneously to get
a desired level of accuracy and within a
fixed run time let me skip over the last
example but just to say that that you
can now go to many of these hierarchies
and carry out the statistical analysis
as well okay so one of the big mess take
homes from this analysis so far is that
it turns out that as compared to
theoretical computer science where the
goal is not to get statistical risk it's
to get accuracy up to some approximation
ratio they're weakening z' really can
destroy your accuracy right here because
we're trying to worry about statistical
risk as a quantity we don't care about
arbitrary approximation ratios it turns
out that the simple algorithms can give
you very very effective procedures that
scale surprisingly well to large larger
larger problems so the fields of Thera
computer science and optimization theory
and applied math of it often study these
objects but again not thinking of them
as statisticians thinking about them
from the point of view of kind of raw
approximation ratio okay I am now
finished and I've done that in time one
kind of meta point here alluded to
earlier is that I hope you see that
there wasn't a lot of talk about
building computer systems here I think
that's fantastic and important research
in and of itself and programming and
software and all that this was about the
conceptual problems of big data thinking
through what are the really the problems
I'm trying to solve
when can things go wrong when can things
be made correct what are the algorithmic
consequences of that
and this is really just getting started
the very very few people have been
thinking about this because computer
science has been worried about one class
of problem statisticians and other and
there never been thinking about them to
be jointly so the other meta point then
is that if you didn't understand a lot
of this talk it was a bit technical it
may be that you probably should go take
some statistics classes if you're a
computer scientist if you're a
statistician I don't know if they're any
urban here you probably should take some
computer science classes
alright the future is for those people
who can take these two fields and
integrate them in one brain not have to
bring in every project the statistician
and try to get them to talk to the
computer scientist using some
translation software but one brain can
bring those ideas jointly together how
do I think about my statistical risk the
errors I'm gonna make when de to come
into a computer and how do I think about
scaling that so I don't have it break
when I get more than 100,000 data points
all right that's just not been what
we've been teaching students and so you
know we're still not doing a very good
job of teaching students that I think we
may talk about that later right so even
though there aren't classes on this you
guys can go and learn about yourself if
you're taking computer science classes
take a few stat classes or pick up some
books and sit them by your bed and read
them every night and make sure you learn
how to do these two things together
that's the future of this field thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>