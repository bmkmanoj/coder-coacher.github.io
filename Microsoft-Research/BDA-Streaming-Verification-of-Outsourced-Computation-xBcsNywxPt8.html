<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Streaming Verification of Outsourced Computation | Coder Coacher - Coaching Coders</title><meta content="BDA - Streaming Verification of Outsourced Computation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - Streaming Verification of Outsourced Computation</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xBcsNywxPt8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so this is sort of covering a some
recent work with a bunch of people Amit
and who's in the audience Andrew I'll
commit smacker Justin and Kevin who's
around as well so I'm going to start by
talking bit about streams and this is a
topic that was touched on yesterday by
Sudipto and will be also touched on
again by several talks in the afternoon
so you know one of the the first takes
on on how to deal with big data one of
the the valid approach is to say can we
sort of process large amounts of data
with a single or constant number of
passes over that data to track some keep
some small summary information that lets
us answer some some interesting
questions you this came out of data
coming from networks but more generally
we can sort of think of streaming over
many different kinds of data and this
has certainly been very productive in
terms of algorithms in terms of what we
can do and so we have a pretty good
understanding of the kind of things that
we can do in streaming there's still a
lot of active questions around what we
can do the main issue in streaming is
that because you're keeping a relatively
small amount of information relative to
the full size of the data then typically
we're working in the domain of
approximation approximate answers
probabilistic guarantees and so the kind
of parameters that we're interested in
are things like well how you know what's
the space accuracy trade-off how much
time does it take to process each update
how accurate do our answers get what's
that probability of error and so on so
what we know a lot of sort of fairly
fundamental sort of basic analytics kind
of questions we can answer very
efficiently in the stream model so you
know the most basic question is how many
different things have I seen if I've
seen a huge number of different items
streaming past actually how many
distinct items how I see and you know
can I then sort of pose cardinality
questions about subsets of items we can
also pull out so that the peaks the
significant
entries in in these distributions we can
look at information theoretic quantities
like entropy so you know we have a quite
a few primitives that we can then build
on to do more advanced things but at the
same time you're streaming is kind of a
hamstrung by the fact that actually a
lot of very basic questions are hard in
this model you know in in and you know
what we mean by hard hardness here means
that to actually give any accurate
answer to these questions we need to
devote an amount of space that's
essentially proportional to the size the
input so in other words this this Idid
love having a small space summary of a
large amount of input sort of goes out
the window and what are the kind of
canonical hard problems here something
as basic as saying well I've seen you
know billions of items seem past did I
see you know an item of type X just that
sort of simple indexing question we know
it needs us to devote a large amount of
space more generally things like in a
product between two vectors which comes
up in in any kind of similarity
computation again is something that's
hard to do accurately we can get some
weak approximation to this and I'm it
will talk this afternoon about new
stronger lower bounds for various
problems via communication complexity
and information complexity arguments so
this is sort of a bit disappointing view
it says that you know well there's a
bunch of stuff that we can do fully in
this streaming model there's enough hard
things that we're going to we can't
really think about doing general-purpose
computations here so what I'm going to
talk about today is sort of take saying
well can we still take some of the good
parts of streaming and sort of combine
them with a more powerful computational
setting and it comes up in the following
sense all right so so you know if
there's lots of things that I can't
compute fully in the streaming model
well then let's just go to someone else
who's got lots more resources who can
store the full data get them to do the
computation for us and and you let them
do the work and then we sort of hit on
this this is a fundamental question well
if someone else is doing the work for us
some outsource entity is is you know
promising that they're doing the
computation giving us back an answer how
do we know that they're actually telling
us the right answer how do we know
they've actually
on the work that we ask them to
correctly rather than just you know made
up some answer and said that that's what
they've done so all interested here is
it sort of looking at this this
multi-party situation where we have you
know what someone we're going to call
helper or prove oh who's actually going
to go off and do the work and we think
of ourselves as sort of the weaker
streaming entity who wants to verify you
know is this answer correct and in
particular can we do that in a way we're
verifying the answer is a lot less
effort than computing the answer so this
this has some deep theoretical
Corrections it connects you know I can
sort of throw out terms like
communication complexity Arthur Merlin
model streaming versions algebra zation
all these things but I'm also going to
try and convince at least towards the
end of the talk that there's there's
some practical application of this that
this is this is something that we can
actually implement a mate work and you
know why we want to do this you know I
sort of hinted generally this kind of
cloud computing scenario where we throw
our computation to the cloud and then
get some answers back we'd like to know
that so in microcosm you can get the
same situation where within a particular
system you have various components where
you don't necessarily your server
outsourcing effort to those components
and again you'd like to cross-check so
you this is a situation just where you
can also think this sort of as a
lightweight checksum for computation to
try and see you as they're just been an
error and when you're dealing with huge
amounts of data the chance of you know
disk read errors cosmic rays disrupting
your CPU become sufficiently large that
maybe you want these kind of checks on
on the correctness of a computation so
so here's sort of the first setting of
trying to formalize this a little bit
and look at just a sort of a simple one
round approach this so here what we're
thinking of is again we have a sort of a
week it will verify our strong prove a
helper who is
they're both seeing the same input which
we can think of as streaming past and
the helper is or prove our is going to
provide their proof to the verifier of a
particular computation so what are the
properties that we want here essentially
the main properties sort of down here
are that we want the probability of the
verifier accepting a romance I should be
small and that there should be some some
proof some some help string that the
prove it can provide that with high
probability convinces the verify to
accept the correct answer and you again
what are the parameters here the
parameters are it's essentially what it
turns down to is opposed to to sort of
space related to parameters what is the
size of this proof this help cost H and
what is the amount of memory that the
verifier needs to perform their checks
so let me let me sort of work through
one fairly specific computation and show
how you can actually construct protocol
in this setting that has the desired
properties so we're going to focus on on
this problem of frequency moments people
who have seen streaming before know that
streaming people love frequency moments
it seems like sort of a canonical
computation and the reason for that is
actually that there's sort of something
something fundamental about this problem
and it actually you take upper bounds
techniques that work for this problem
tend to generalize and give interesting
algorithms a whole bunch of other
problems so what's this problem the
problem is that we see a sequence of
items in this you know long high
dimensional stream so each each item is
you drawn from some very large universe
of possibilities well at WI denote the
frequency of item I and what we wanted
is just simple mathematical quantity sum
over all items of the the frequency of
the I item raised to the power K this is
difficult because you know the items are
coming distributed so we can't easily
directly compute WI from the stream
there's so many different items we can't
keep track of them so to compute this
exactly the natural thing is just to
keep a counter for every different item
type and keep track of them and
and at the end of the stream do this
computation and it's something people
looked at you it's it's something that's
relatively hard to approximate for
moderate values of this parameter K as
well so so what are the kind of results
that we get here is a little bit you
know hard to sort of pars completely off
off here but what it's basically saying
is that actually we have a nice
trade-off that says for any setting of
these two parameters hmv where H is is
sort of the size of the proof and V is
the space at the verifier so long as the
product of those two terms is larger
than n the domain from which the items
are drawn then we have a protocol where
essentially that the size of the the
proof scales linearly with this
parameter H and the space the verifier
scales linearly with this parameter V so
to make this concrete what this is
saying is that with space that looks
roughly like square root of the the size
of the input for the verifier and a
proof of comparable size also square
root the size of the input we can
compute exactly this quantity in this
this sort of proof setting and you know
in some sense that's surprising again
it's saying that you know this this huge
thing then we have two resources which
are both only square root the size of
this huge input so what do we do this is
this isn't you know here here actually
going to sort of pull out some some sort
of algorithmic technique that turns out
to sort of solve this problem so what we
do is we start thinking about input as a
sort of a long array so think of this is
the array of item frequencies right the
first item has frequency three second
item has frequency 7 3rd item has
frequency 1 and so on so the first thing
we don't going to do is sort of think
about how do we lay this out we're going
to reconfigure it in this sort of array
as an H by V array so we have the sort
of the full domain we know the size of
that domain we couldn't we can reformat
that array site or that into this array
and then here's sort of the the sort of
the the trickiness here we can think of
that
array of values as defining the values
of a polynomial right that there is some
polynomial that if I evaluate it at this
location sort of 11 will give me three
there's the same polynomial if I
evaluate it at this position 12 will
give me seven at this position will give
me 0 and so right that's just the basic
facts if I have a sufficiently high
degree polynomial i can define one that
degrees with this this way and then the
trick is we're going to use this
polynomial essentially sort of like an
error correcting code like a checksum
because we're going to take that
polynomial that agrees with our input
fully and pick somewhere outside the
domain of the input to evaluate that
right so in particular we're going to
pick some random r which the verify
takes which is hidden secret from the
prover and evaluate this extension of
the input at this random location so
think about sort of setting up some some
sort of set of values away here's his
sort of the region where the input is
defined sort of extrapolate out that
same polynomial out somewhere else and
evaluate it down there and this is
called the low degree extension of the
input evaluated at those locations and
then the protocol has the following form
that the prover is going to send some
information which is meant to help us do
this particular computation so what
they're going to do is is essentially do
sort of a partial aggregation they're
going to send a polynomial so here we
had a polynomial in in x and y that was
sort of giving us this full range of
values the part of the prove is going to
send us a polynomial only in X that's
basically the partial aggregation of the
function that we want it's aggregated by
summing over J values of this sort of
partial entries of the function for a
particular X so for one value of exits
it's essentially the summation of
one row of the entries in one row raised
to the power J for a different value of
x it's the corresponding summation of
values rates of power J for that row so
why why is this a good thing well
essentially because of the way we've
constructed this check for the verifier
the verify can say well if this
polynomial that the prover has sent me
is as is claimed then I can evaluate
that sent polynomial at my secret random
location are so I can evaluate what
should be the sum of entries raised to
the power K for this this Rho R and then
the verify has got because of their
evaluation of this extension of the
input the necessary values to then
compare and check right so based on the
information that the verify has kept
they can see if at the secret location
are there calculation agrees with what's
given by the prover and if they agree
essentially we can argue that the chance
of you if what the previs sent was not
honest was not as it was claimed than
the chance of that agreement happening
is very small so we're pretty convinced
that actually this is the right
polynomial so there's there's a some
amount of of reasoning and algebra
behind this but the basic idea is to say
you can't come up with this interesting
sort of algebraic check on the honesty
of this computation so where's the
streaming part of this the streaming
part of this where this gives us most of
our power is in the fact that the verify
can do this evaluation right we're
asking the verify to so think of the you
think of reformulating the input
restructuring it interpreting it as a
polynomial extrapolating it to a random
location and evaluating it there and and
sort of what's maybe surprising is that
all of that can be done in the streaming
setting because that actually turns out
to be a completely linear operation so
yes sir they said it too
extolling the virtues of linear
computations as you know being very
powerful tools being very flexible
working in streaming also sort of
trivially working in distributed
MapReduce settings so what's vital here
is the fact that the the verify can do
this incremental computation of the
evaluation the low degree extension at
the random location incrementally as
that function as that input is defined
by the street and in some sense the
reason for that is is fairly simple that
if you look at what that computation is
then every update that is received in
the input you can think of as being an
impulse right it's it's it's an signals
and you're adding on to that array the
input and additional array that's one at
the location of the update and zero
everywhere else and then what we want to
do is what we can say is that our
evaluation of the extension of the input
is equal to our evaluation of the extent
of a current evaluation of the extension
of the input plus our evaluation of the
extension of that impulse polynomial
defined by the location of the update a
be evaluated at yeah the relevant
locations and what you know what this
turns out is is that this this sort of
update polynomial this this piece of a B
is again it looks a little bit
off-putting the first time you see it
but it's something very simple it's
called a Lagrange polynomial and and
moreover it's just sort of a
multiplication of things that we can do
in the field so we can evaluate it very
efficiently in in small space
incrementally and you know if we really
want to engineer this we can start doing
tricks like pre computing partial values
look-up tables that make the computation
of this update very simple so when you
put all these pieces together this
actually says that we have a nice
protocol in one round for checking this
particular computation and then why you
know why is something like second
frequency frequency moments of interest
to us well I sort of said it place a
bunch of other things I sort of directly
I said that in the traditional streaming
model something like in a product is
hard once we have a primitive like this
in a product follows immediately right
so we've shifted the model and in this
model we can exactly compute inner
products in this verified way here's
sort of the the the way you do it using
this primitive you just observed at the
inner product between x and y can be
expressed in terms of the second
frequency moment of combinations of x
and y you can more directly just adapt
that protocol that i showed in terms of
extrapolation the input to verify that
directly if you don't want the exact
answer you can sort of improve the cost
the trade off by looking at the
approximate say second frequency moment
by taking approximate methods and sort
of plugging them into this framework and
you can do other things you can sort of
approximates a the maximal value they
make some weight maximum frequency of
any item based on connecting that two
approximations of sort of larger and
larger values of the frequency moment so
once you have this you know there's a
lot of sort of simple things you can do
and as we go and i'll talk about even
more general computations so what that
is is is that just with a single round
of proof essentially where the prove it
can provide the proof don't need any
direct interaction with the the verify
they can just sort of publish that
polynomial that they need to provide and
then go offline then we have a pretty
good set of results we have essentially
you sit in that in that particular
example the trade-off I gave you have
sort of square root n space to the
verifier square root n two sides of that
polynomial proof that the proof has to
provide it turns out that if we if we
took if we can tolerate more interaction
between these two parties you can
exponentially improve both those
resources so now think of rather than
just a single proof being sent from the
proven to the verifier I think of sort
of a multiple rounds of interaction
between the two players where the prover
sort of make some initial statement the
verify comes back with some so you think
if it's like a challenge and we go back
and forth
until the proof till the verifier is
satisfied how does that work essentially
it takes the the the previous idea and
sort of generalizes it from one round to
multiple rounds and if you're familiar
with interactive proofs you'll sort of
see echoes of the kind of some check
protocols that occur there similar kind
of idea so before we did this sort of
layout of the data in a rectangle now
you can think of this as laying out the
data in a hypercube but actually it's
probably easier to visualize as laying
it out at the root at the leaves of
binary tree and a similar sort of idea
that the verifier is going to pick sort
of a location in this hypercube this
this generalization of the hypercube or
instead of binary coordinates they're
allowed to pick sort of arbitrary
integer coordinates and extend the
function out into that space and
evaluate a particular function is
evaluated centrally the input
extrapolated out to that random location
and use that as their check then what's
what's the interaction going to look
like the interaction is going to sort of
proceed down this this sort of binary
tree structure the prover is going to
make essentially an initial claim that
relates sort of directly to the overall
output then in each round you can think
of the the verifier secret location as
being sort of a coordinate in this is D
dimensional space and each round the
verified divulge is one of those
coordinates and then forces the verifier
to incorporate that into their
computation so essentially in the first
round the verifier the proofer makes a
claim about sort of the whole input and
then in the first round of earth at the
pru of the verifier says okay well i'm
actually interested in the input
restricted to this particular coordinate
this initial this first coordinate and
then they in second round they say and
actually I'm interested in it restrict
to that first coordinate and that 2nd
cordon so you can think of it as sort of
incrementally specifying a particular
coordinate until in the final round the
verify has specified exactly the
coordinate that their entry
and they're forcing the the approver to
make a claim about the evaluation of
their function at that coordinate and
then the reason that all this works as a
way of sort of checking the correctness
is that in each round you can relate the
claim made in the previous round to the
claim made in the current around and
check consistency so I haven't gone too
deep into technical details but so that
this is the general idea that you're
sort of the the proven that proven makes
an initial claim about the whole input
which gives you the answer that you want
and then you progressively tie that
claim to more and more specific claims
until it becomes a claim about something
that you've previously used to check you
can then use to check the correctness
because of requirements on consistency
okay so this is this is very nice
because all the costs before that were
polynomial now become logarithmic or
poly logarithmic and it's it's actually
fairly cheap to to do this calculation
so you know so far this this sort of
looks like a nice exercise in theory
because I've shown you lots of confusing
looking polynomials and I've talked
about some fairly specific functions
that look somewhat disjoint from what
one would ever actually want to do so
let me sort of say how these ideas then
can be made sort of more general and
more practical and so in in general
you'd like to not just look at
particular computations but look to look
at sort of more general computation
functions and actually there's a nice
framework laid out in a stock paper by
Goldwasser Kelly and Roth bloom that
sort of sets up a framework that we can
then instantiate with this ideas so what
goes on here what goes on in into this
setting is it says well let's look at
general class computations those that we
can sort of model naturally as
arithmetic circuits so these are sort of
circuits and the traditional sense of
circuits but where the gates are
arithmetic operations so say addition
and multiplication and how are we going
to use this kind of technology
essentially what we do is we start with
the prover so that should say prove
prove acclaiming a value for the output
of the circuit
and then we're going to successfully
successively reduce this claim about the
output of the circuit or training about
properties of the circuit at each
successive level until we get from the
output of the circuit to the input level
of the circuit at which time the prover
is making a claim about a low degree
extension of the input at some random
value which is exactly where the
verifier has evaluated the input
extended to that random value and again
so using successive agreement checks
between each iteration each round to
enforce global consistency right so you
picture sort of showing the same thing
we want to think of the circuit is
essentially being layered we have an
output layer an input layer and some
intermediate computation layers we want
to reduce the claim amount the output
layer into a claim out the input layer
using these kind of polynomial low
degree extension evaluations to enforce
consistency between each step so there's
a lot of sort of engineering details
involved in actually putting this all
into practice I'll sort of skip over
some of the details in terms of what
goes on here 11 major issue is just that
we don't want you know naively
implemented this kind of thing forces
the verifier to sort of compute
functions over polynomials that define
properties of the circuit and actually
if you sort of directly iterate over
those then you're effectively iterating
over all the gates and wires in the
circuit and so it's not clear that you'd
do anything better than just directly
evaluation the circuit so where you can
really win here is the fact that most
computations are not arbitrary but
they're very structured and that when
the strutt when the wiring of the
circuit has a significant structure to
it then you can more effectively do
these computations and compute the
contribution of sort of the gate all at
once rather than one by one so let's
actually show some numbers here's what
happens when in some work with with
Justin where Justin Taylor so that went
off and implemented all of this
and what it shows is that so what are we
seeing here we're seeing a couple
different problems in this setting one
you're fairly standard one computing the
second frequency moment that's fairly
straightforward second one looking at f0
that's a number of distinct items that
turns out to be much more complex and is
uses essentially some various tricks
with encoding that as in an arithmetic
circuit and lastly a pattern matching
problem that sort of says imagine your
input as being a large string your query
is you to find occurrences of a
particular pattern in that string and
what this shows is is that you using
either standard addition and
multiplication gates or sort of throwing
in a couple of extra gates then we can
do this reasonably efficiently the
certainly the communication size is very
small we're talking about kilobytes of
communication the number of rounds is
moderate talking about hundreds or
thousands of rounds the time for the
verifier to do their check is is almost
nothing effectively the limit of what
what what we can measure in terms of
timing and and the time of the proof or
is tolerable right so this is these
these instances are not huge by any
burning measurement but it's saying that
at least this idea is sort of on its way
to practicality that in sort of hundreds
of seconds of effort on the prover we
can verify these computations so let me
just sort of wrap up by saying sort of
where this this workers has been going
the last year or two you know how we can
sort of further attack various of these
costs so one observation is that in
building these kind of provers so the
main bottleneck in this work is
engineering an efficient prove er
engineering approver who's who's effort
is not so much substantially worse than
the cost of doing the computation itself
so on observations that the provers
working all this is is actually quite
data parallel the same for the same
reasons that the the verifiers work is
the linear function of the input the
previous work is sort of parallel over
the data so you can paralyze in
particularly you can take advantage of
things like GPU
for acceleration so so Justin others had
a hot cloud paper on this last year and
showed you know the kind of expected
tens to hundreds of times speed up by
shifting the work on to the GPU most
recently justin has it has a nice set of
results in crypto coming up soon that
basically says with with more engineer
with more careful choice of how you
define these polynomials that are being
evaluated you can reduce the effort of
the prover bite by allowing them to sort
of recycle more work and that actually
saves you a log factor and you know
what's the log factor well you know
these are moderately large domain so a
log factor is is tens so in particular
when you implement this stuff it shaves
off factors of 100 so again you're sort
of going from these proven times of
hundreds of seconds down to prove as a
time at a time for the prover in the
realm of single-digit seconds nice
recently with the MIT and Justin we've
been looking at sort of other variations
of this problem so ideas around when you
have this when you have data drawn from
you know an immensely large possible
domain say the the sort of domain of all
possible pairs of ipv6 addresses then
within that huge domain your data is
relatively sparse so the results that we
have that I talked about so far have
dependency on on polynomials in the site
of the domain and what we'd like to do
is actually reduce that to being
polynomials in the size of the this was
the true sparsity of the data it turns
out you can do that it introduces some
additional problems in in sort of
checking the the correctness but you can
take those results in terms of cost cost
that the scales with the size the domain
and turn them into cost the scales with
the size of the support set of the data
and you there are many variations as
lots of other people look at these kind
of questions from a more crypto
perspective so some recent work coming
up in signaled this year we're looking
at crypt using cryptographic tools to
generalize this from a sort of a
two-party model of a proven verifier
into a three-party model where you have
a data owner the server that's doing all
the work and then possibly untrusted
clients who are trying to actually find
answers to
things so I think I probably exhausted
the time available so let me sort of
just leave this slide talk about open
questions up there and and stop there
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>