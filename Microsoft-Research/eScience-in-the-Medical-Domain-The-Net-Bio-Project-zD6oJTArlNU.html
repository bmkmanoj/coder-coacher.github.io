<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>eScience in the Medical Domain - The .Net Bio Project | Coder Coacher - Coaching Coders</title><meta content="eScience in the Medical Domain - The .Net Bio Project - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>eScience in the Medical Domain - The .Net Bio Project</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zD6oJTArlNU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
you
following such a fantastic talk is that
you have much of your motivation already
completed for you and Haiphong has done
a fantastic job of outlining some of the
developments around the growth of
sequencing data and some of the
potential applications that are now
making their way into the research and
into the clinical communities I would go
as far as to say that we are in very
broad agreement even on it the choice of
some of our slides so we will see that
in a moment so our agenda today is
fundamentally about tools and
particularly about the use of tools
based on the.net bio project so whilst
this is not sort of an introduction to
the project because it's been around for
a number of years now it is a bit of a
snapshot of where the projects it's and
some of the work that's being done to
extend it and some of the work that's
being done to build things on top of it
but for those who haven't seen it i'll
spend a few slides just introducing what
the library actually does and some of
its history and some of where it might
head in the future they will move on to
some increasingly interesting
applications both in terms of its
application directly in high-throughput
genomics but also in its application in
other contexts so it's extensions into
proteomics and also the possibility of
integrating it with other languages and
also even to utilize it as a backbone
for the idea of doing bioinformatics
somehow in the browser so as hope on
pointed out very nicely genomics has
kind of come a long way over the last
decade or so and we've particularly
agreed on this particular on this slide
as a way of indicating some of the
changes that have taken place over that
time fundamentally the.net bio project
is about dealing with genomic data and
it's about sequence data whether it's
really nucleotide data or proteomic data
but what has accelerated the
use of libraries such as net bio and
other open source approaches to
bioinformatics is the need to deal with
very very large amounts of data and
particularly the need to deal with very
very large comparisons which arise
because of the data which is arisen
because of next-generation sequencing
projects and what we're going to talk
about here is just some of the nature
some of the changes in the nature of the
computations that have been brought to
bear in recent times so as we've moved
over that time of course we've seen not
just one sequencing revolution but a
number and so as was pointed out before
we're starting to see people moving not
just to work based on a small set of
sequences a reference sequence or a
small set of individuals but rather
population studies based upon sequencing
of a very very large number of
individuals so we're starting to see
early signs of personalized medicine
some of the predictions may well have
been optimistic but we're certainly
starting to see movement in that
direction and we'll particularly focus
here on the notion of genotyping and
identification of snips based on
resequencing excuse me based on
resequencing projects so they're not led
by Oh project itself began a number of
years ago as a project initiated from
Microsoft Research and microsoft
research connections and it's a language
neutral open source tool kit for
bioinformatics based on botnet
technologies the current versions built
on top of dotnet 4.5 fundamentally it
allows you to build applications based
on any number of cl are compliant
languages so C sharp F sharp ironpython
iron Ruby and so on it's open source
released under the Apache 2.0 license
under the auspices of the article and a
shin and it's fundamentally a
foundational library on which other
people can build software and to enable
people to make contributions as one
might expect of a genomics what is
fundamentally a genomics library it
provides a class library of standard
for some of the things which have
underpinned by automatic computation
over the last decade so passes for
particular file formats support for
different types of sequences fundamental
algorithms for search and assembly and
alignment and also an ability to extend
its capabilities by linking to external
web services through a well-set-up
facility for doing so so this is just
one of the help files giving you a
fairly good idea of some of the
algorithm namespaces so we have
alignments we have assemblies we have a
port of mama a whole bunch of different
algorithms and facilities being
available for use so in recent times
the.net bio project has grown in
acceptance and also in importance
amongst people working in the
next-generation sequencing area
including from some people who are
involved in sequencing companies and
sequencing startups the idea of the
library of course was that it would
provide fundamental data structures to
encapsulate what we are working with so
fundamentally sequences fundamentally
about pausing and of course the ability
to execute such algorithms relatively
quickly by taking advantage of some of
the parallel extensions and other access
to high performance computing which is
provided by the dotnet libraries so the
library itself is both a collection of
functions or collection of methods that
is there to be exploited but also it
comes as a set of pre assembled or
pre-constructed tools to enable people
to to use things so there are a number
of developed utilities which ship with
the library so these exist for sequence
comparison and alignment so things
around mummer and nuke m'a also
utilities for working with with sequence
alignment map files and their equivalent
bam files and also tools for de novo
assembly of sequences from short reads
so that's a thing called padena
one of the other aspects of the library
witches caused it to get some interest
from educational institutions in
particular is the integration of these
tools with offers tools such as Excel
which makes them very useful for
undergraduate bioinformatics classes and
so we've actually written some material
to work in undergraduate bioinformatics
classes based on the.net bio add in
ribbon that we see here and also some
additional exercises and tools which
we've created for that purpose the.net
bio project has an increasingly active
community built around biotope lakes
calm as we can see with the site here
and fundamentally it's maintaining
around 800 downloads per month so it's
becoming an increasingly successful
project not necessarily what used across
all of the pharmaceutical industry and
big science but it's getting increasing
acceptance from companies such as see
bio the US Food and Drug Administration
alumina Amira's by a nano genomics and
also from another number of research
organizations in a number of different
countries such as the CSIRO in Australia
for example there are training materials
and undergraduate resources available
from the site and from members of the
community so what I wanted to talk about
today then with some of the applications
which we're working on which others are
working on with this library and some of
the future directions in which it might
head I'm going to focus particularly on
a thing called DD rad seek which is
about next-generation sequencing methods
for rapid genotyping so Jenna typing is
is a fundamental aspect of the modern
sequencing toolkit it's crucial to any
sort of clinical diagnostics based on
sequencing it's also essential for the
identification of life scale infections
particular types of tumors all of these
sorts of things depend on some sort of
careful identification of sequence
variation the traditional approaches
have been based and I say the word
traditional with a certain sense
irony but traditional approaches to
molecular identification of this sort of
variation have been based on large scale
single nucleotide polymorphism chip
micro arrays and traditional is of
course ironic because it's only been
around for a number of years but it's
already being superseded by Ricci Qin
Singh approaches the rise of
next-generation sequencing means that
what was once a ludicrous idea that one
could go away in sequence another human
genome or another few human genomes is
now becoming much much more feasible but
there are still some limitations it's
it's not feasible to do large or it's
not cost-effective at this stage to do
very large-scale sequencing of human
populations in order to identify
unbounded genomic variation that's not
the way things are done just yet we
still need to have some sort of
selectivity in the samples that are
being created but we want more
flexibility than that which is provided
by the now traditional methods of snip
chip micro arrays so the problem with
those is that microarray designs of that
kind depend upon a very good background
knowledge of the variation which is
being sought and this of course requires
a certain substantial prior knowledge of
the sequences and then known variability
and that's often a variability and a
knowledge which is not actually
available to the researcher so it's
important if we're going to look for
genotyping which scales and
investigation of large scale variation
across a thousand or more human genomes
for example then we need to look at
other methods now a number of
researchers notably in Edinburgh and
then in the United States have developed
methods based on what they call rad seek
which is restriction site associated DNA
sequencing and the essential principle
of this is to have some use of
restriction enzymes to cut out a small
section and well known sectional will
constrain section of the DNA sequence
and thence to identify variation based
upon that particular sampling
so the alternative here is this use of
rad sequencing on non next-generation
sequencing platforms but the idea here
is of course that you do this for quite
a substantial number of samples generate
a large number of reads and then in some
sense collect the reeds and identify the
variation through clustering and
analysis so there are two versions here
which we're going to talk about our
focus in collaboration with a group at
Harvard is based on the double digest
rad seek which we've seen here the
essential variation is that the double
digest rad seek allows you to get
greater control over the sample by using
restriction enzymes at both ends of the
cut so what you end up with is far less
chance of any sort of unwanted selection
or unwanted variation of particular cuts
and so by having the use of restriction
enzymes you much at both ends you're
able to control the cut that you're
making in a much more refined sort of
way and hence reduce the complexity of
the data analytics problem that comes
later on the essential issue here then
for data processing and variation that
comes in is to group the reads for the
individual cuts by sequence similarity
into clusters but we need to ensure some
reliability and we need to ensure that
we deal with appropriate throughput the
existing approach for this particular
method is based on some cloud storage
using Google Docs and some Python
scripts the collaboration that we've got
at the moment is utilizing the dotnet
bio libraries to improve the performance
and also to introduce some new methods
for the detection of ploidy which will
come along along to in a moment so the
essential idea is that we want to ensure
the quality of the clusters which are
generated we want to make sure that we
can believe the genetic loci which are
identified and that the summer of
busness to the estimates of variation
which are put in place so fundamentally
humans are diploid organisms
so individuals should not exhibit more
than two possible haplotypes haplotype
sequences and what we're looking at here
is some potential variations in the
cluster data which are not in fact the
result of the sorts of snips type style
variation that we're seeking these can
be due to just straightforward
garden-variety sequencing error error in
the clusters where unnecessarily similar
sequences are actually grouped together
when in fact they should not be and of
course also some contamination from
other samples which is is something
which one would like to avoid but
necessarily arises in laboratory
practice so the focus for us at the
moment is on the problem of ploidy
filtering so we need to have some rapid
and robust method for detection of
ploidy levels within the cluster
previous approaches and the one that's
being used in the pipeline at the moment
is the use of a simple count and some
very simple metrics derived by maximum
likelihood estimation based on top of
those counts but other suggestions have
included hidden Markov models and
discriminative sorry discriminative
classifiers such as logistic regression
or the support vector machine but at
this stage we're working with a mixture
model which has been adapted from
previous work which has been used for
straightforward snip detection so we're
using a mixture model with Poisson
distributions for each haplotype
parameter estimation by our expectation
maximization and the software here is
implemented on top of the dotnet bio
libraries and ultimately of course the
goal is to discriminate between good and
bad clusters so here's if it's very
difficult to make this look as visually
impressive as some of the talks that
we've had earlier so I'm afraid I'm just
going to talk you through what we what
we've got and the ploidy analysis tool
enables the researcher to set up a bunch
of quality scores which are essentially
put in place as filters to allow the
choice of whether these clusters are
actually good or bad and whether those
clusters should be utilized in
analysis later on the progress of the of
the run is detected is some you know
monitored rather at the right where we
see the number of samples which are
detected the alignment quality which is
a standard quality score measure and the
maximum read quality which is a measure
of the actual quality coming out of the
sequencing project and also we're
starting to see what we're seeing in the
good clusters which are emerging and we
see the distinction here between all
pastors in the middle there and the good
clusters at the bottom which would be
the ones that we might decide to keep
the dirt score that we see there the
dirt maximum that's an agglomeration of
clusters with or a pool of Buster's if
you like which do not successfully
satisfy the requirements of the ploidy
which is associated with the audition it
also illustrates some of the things that
we're able to do very very quickly
because of the underlying tool support
the underlying library support so BAM
and Sam file processing sequence
alignment and comparison and management
of alignment objects and of course the
management of camera objects which helps
us lead towards alignment free
approaches as well so for a single
cluster just to give you some idea of
how this might turn out you have a
particular cluster which has 179,000
reads which filter down to some 2138
distinct reads which correspond to 48
individuals which was actually the
number that we thought were there in the
first place and so on this occasion the
ploidy detection is acceptable and that
cluster is one that we can work with
there's some additional stuff about the
the read quality scores and so on down
the bottom there which I won't consider
at the moment we see here in the source
code the use of the existing bio dalga
rhythms and also an additional set of
library functions around additional
metrics for this sort of work and other
next-generation sequencing projects
which is being created by our people as
part of this project so to conclude the
talk some other active projects so this
is the first step in a major pipeline
around the DD Red Sea
that's a collaboration that I've
mentioned before between us and
evolutionary biology group at Harvard
there's tool support for next generation
sequencing integration are interfacing
to the Select dome datasets which is a
set of confirmed positive selection
interactions in the evolutionary biology
community and that's driven by Nigel
Delaney out of Harvard pattern language
searching sorry pattern searching
languages for dotnet buyer which is a
project that weird pursuing at QUT and
it we're also just starting to work on a
couple of other projects which I'll
mention quickly here one of the more
interesting ones is is just has one
sorry is one that has just started with
a student working on a type provider
under the F sharp environment for
interfacing net bio and the.net
environment with resources from NCBI so
that's starting slowly with him working
on the production of a type provider the
GenBank files but the goal there is to
extend that over time to a wider range
of entre based resources one of the more
interesting ones which has got somewhere
at this stage has come from someone in
my group who just went away and did it
which was to look at the dotnet bio
project and the existing c-sharp net
source code and to utilize it as a basis
for computation and particularly for
visualization support in the browser and
that was the compilation of net bio to
javascript by taking advantage of a
thing called script sharp which some of
you may be aware of so the idea here was
simply to provide a set of
bioinformatics objects and some simple
bioinformatics algorithms which could
actually work within a browser obviously
you're not going to do your vast
next-generation sequencing pipeline
within the browser but certainly there
are times when you wish to do
transformations and reformats and simple
pattern discovery within the browser
when you're visualizing something and
this is really quite an exciting
prospect so to give you some idea this
is the sea shapes all
code which utilizes calls to the NCBI
web services as supported by dotnet bio
and here's the corresponding java script
generated by using c sharp and the x oj
s server is something which is a
compiled version of the.net bio project
source code as i spoke of before so
finally the project itself resides at
bio codeplex com we welcome
participation welcome anyone who wishes
to talk about it and I'm certainly happy
to talk about it there are a number of
other people here who are actively
involved in the project Simon Mercer and
others in particular you can see here
that there's also a call out for the use
of this project as part of in the
imagine cup and those of you who are
Doctor Who fans may even find this as a
way of looking at Matt Smith the
previous or the existing doctor so
finally some acknowledgments from
Microsoft's research connections from
Michael from Simon Mercer and Rick
penguin and all of those people to my
own people at QUT Lawrence and amber and
various other student contributors and
our collaborators at Harvard Nigel
Delaney Brant Peterson and emily kae the
ladder to being from the hoekstra lab
thank you very much thank you very much
very informative talk Oh is there any
sort of questions or comments
thanks very interesting I just I'm just
curious you listed a lot of projects on
DNA sequencing and also on proteins owes
his Ernie is there any specific projects
using the using this tool for RN
sequencing the i mean the the tool so
that the library itself supports you
know caters for any quite a number of
different alphabets but at the moment
people are not using it for for RNA seek
its or it's it's not a major focus of
people using the library that I'm aware
of certainly if people want to talk
about it and are interested then it's
certainly something that people may be
interested in collaborating Thanks any
other questions or comments so some of
that the emerging project are you
talking about protein structures and so
on so are in order to proceed to more
sort of translational biology and so on
then you may need a bit more different
types of data and images or text another
database so then the problem may not be
just a sort of our interoperability of
software that's more data in total
ability would become more crucial issues
so is there any kind of a movement
towards that direction like a common
want ology or some descriptive our terms
I mean standardization there's been some
very basic discussions around metadata
but it's not something which is I mean
people have had their hands full working
from what they've you know from what's
there and adding on things that are very
specific to their needs at this stage
people have not looked at trying to
extend it towards anything which is
really under the sort of the Semantic
Web
type of umbrella but nevertheless it's
something which you know could be
supported if there's sufficient if
people had sufficient interest and
compelling application to make it work
for them at this stage mostly people are
looking for those sorts of services by
using that sort of the web services
hooks to you know to just treat them as
external data rather than rewiring the
internals of library to work with them
with any questions or comments
ok so thank you very much for thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>