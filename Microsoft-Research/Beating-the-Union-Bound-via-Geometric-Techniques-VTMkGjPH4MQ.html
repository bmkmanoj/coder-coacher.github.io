<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Beating the Union Bound via Geometric Techniques | Coder Coacher - Coaching Coders</title><meta content="Beating the Union Bound via Geometric Techniques - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Beating the Union Bound via Geometric Techniques</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VTMkGjPH4MQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright good morning everyone we're very
happy to have Raghu mecha from the IAS
will tell us how to beat the Union bound
so i'll talk about beating the Union
bound by geometric techniques so what is
the Union bong it's a basic fact in
probability theory it says that if you
have any events or some probability
space then the probability of the union
of the events is at most the sum of the
individual probabilities particular is
the sum of the individual probabilities
is less than 1 then with nonzero
probability none of the events occur and
this was popularized by addition it's
one of the most basic techniques in the
probabilistic method but even before air
dish there is one literary figure who
made prominent use of this and I'll give
you three guesses literary figure ok
close by what is your question there's
one literary figure who made use of the
Union bound for exactly this is it when
you have eliminated the impossible
whatever remains however improbable must
be the truth it's kind of a paraphrasing
of the Union bound ok so despite its
simplicity is amazingly effective in the
probabilistic method and some notable
examples are the original application of
the method by edish to showing the
existence of ramps across very good
Ramsay gaps and Shannon who also used it
to show the existence of very good error
correcting codes Johnson lindenstraße
who use it to show the existence of very
good metric embeddings and so on there
are many other such applications as can
be seen this beautiful book by alone and
Spencer
on the other hand sometimes the Union
bound is indeed to knave and it's not
enough to capture the full picture and
one salient example of this is the
lowest low dilemma it says that if you
have any events as before but the events
are deed dependent meaning each event
depends on at most D other events and
the probability of each event is not too
large then the probability of the union
is at most one is less than one and if
you use the union banda here you get
nothing if n is much larger than the
degree D so it beats the Union bound and
we'll keep seeing this symbol whenever
we succeed in beating the union bar and
this result it says this existential
result is amazingly effective and the
Spencer says it helps us find needles in
haystacks on the other hand the will
come across this concept again just to
make this result constructive or
algorithmic and this required further
breakthroughs by starting with the work
of back and more recently with the work
of moisture and moisture and tarnish and
so on all right I guess this is the
first Louis and air dish formulation
yeah yeah yeah yeah yeah this is extinct
in 77 it was for and was improved to e
later on this is first result had a four
in it
so here I'll talk about two examples the
first orbiting the Union bound the first
is to construct a discrepancy
minimization and here I'll talk about a
new approach for rounding linear
programs by Brownian motion and then I
will talk about a limit theorem for
polytopes and here I'll just state the
result and give some applications to
learning theory to learning theory and
in both cases we will use some nice
symmetry and geometric properties of the
Gaussian distribution to go past the
Union bound and thats kind of serves as
the underlying or unifying theme between
these results and here's the outline
they'll talk with the two parts and then
finally I'll finish by mentioning some
of my other results and summarizing the
work so let's start with the first part
so in the basic setting here in
discrepancy minimization is you have a
set system which is a collection of em
sets or some universe of size n for
example here let's say the universities
of size 5 and s 1 is the set 1 34 s 2 is
to 35 s 3 is everything and so on so a
collection of sets like this and the
goal in discrepancy minimization is to
color the elements red or blue so as to
minimize the maximum imbalance so for
example let's say we color consider this
coloring here then the first row has an
imbalance of three the second row or the
second set has an imbalance of minus one
and so on so the maximum imbalance or
the discrepancy of the coloring is three
and the goal in discrepancy minimization
is you're given a set system and you
want to find a coloring Chi such that
which minimizes the maximum over all
sets a new set system of the discrepancy
and this is a fundamental kommentarer
concept with many applications and let's
look at a few examples to familiarize
ourselves with it and one of the first
and notable examples of
bouncing discrepancy was a celebrated
result of rot from 64 we showed that if
the sets in your set system correspond
to arithmetic progressions over Z n then
the discrepancy is at least n to the 1
quarter here here m can be much larger
is usually about n squared with house m
or so n is instead of all of em is the
size of the set system and here you are
taking all arithmetic progressions so
there will be roughly about M Squared
sex ok
like you haven't said one three five
seven you can you also get two four six
and so on and mutter shake and Spencer
also gave a matching upper button
another example that is well studied is
the case of half spaces so here you have
a bunch of points over some let's say
the plane and this sets in your set
system correspond to indicator functions
of half spaces so you draw a hyperplane
and all the points on one side will
correspond to one set like s1 here it
draw another hyperplane and all the
points in this side will correspond to
s2 and you consider all possible hyper
planes and all possible sets that you
get in this form and once again we have
tight upper and lower bounds for the
discrepancy of this prop of these sets
of this set system as well so
discrepancy minimization and dispense
the theory has many applications and
here I'll talk about one cute
application that i like to ray tracing
in computer graphics so ray tracing is a
very kind of a nice concept in graphics
where you give some textual description
of the OFSAA figure there's a sphere at
so answer coordinates is the slab at so
answer coordinates there's a triangle
somewhere and so on and the goal of the
repressor is to take this text
description and render an image that
corresponds to this description okay and
this image is actually generated by one
such ray tracer which I was glad to use
because this from an undergrad project
as it did long time back so how does the
ray tracer work so ray tracer works as
follows you specify the position of the
camera and the position of the screen
and there are objects that you are
trying to render so it shoots some rays
from the camera finds out where the race
intersect the objects and then retraces
back onto the screen and then renders
them according to the properties of the
intersection point
it's a very simple concept and one
important operational question here is
to which two is to decide which points
to shoot raised from for instance we
can't shoot the race from all the points
on the screen because that would be too
computationally intensive and the choice
of the points makes a big difference in
the quality of the rendered image for
example if you choose the knave grilled
then you get a lot of aliasing errors
and what people do in practice or in
graphics is to choose a set of points
whose discrepancy with respect to small
or with respect to simple simple
geometric shapes like half spaces
triangles circles is small and this is
like actually what practitioners use the
computer they have some preset
configurations which minimizes
discrepancy and they returned it they
uses in their ray tracing programs and
more generally discrepancy has many
applications I mean I think the
intuition is that when you do this every
shape gets kind of the same number of
points but the evidence is probably more
empirical you can really clearly I mean
if you do it from the program can
clearly see aliasing as being minimized
with respect to discrepancy it's
probably more empirical missing
something I mean wouldn't greatly
now the grid is not so good why let's
say because it depends on how fine a
grid you choose so if you if you fix the
number of points then the grid is not as
good as some other configurations so for
instance if it take a let's say take a
square which is smaller than the
resolution of the grid then you're kind
of or like a rectangle which sits inside
between the two lines so you can do well
with grid 2 but then you would require
finer and finer resolution or more
points but I mean for a small square it
will more or less is good design yeah
first the thing is how small a grid do
you need foot forget the number one
given number of the questions not better
than the small squares maybe it is a
question of which collection of sets you
are trying to write so let's say you
take a rectangle okay very very thin
rectangle you don't want to lose those
two
shapes like that might be problematic if
you use the grid
and discrepancy has many applications in
complexity theory where they correspond
to good average case lower bounds to
communication complexity where it is one
of the main techniques for proving lower
bounds to computational geometry where
it is useful in building small data
structures and to graphics as we saw
before and also to pseudo randomness and
there are many more applications as can
be seen in these two nice books and here
i will talk about one of the cornerstone
results in this area which is a Spencer
celebrated six standard deviation
suffice theorem it says the following if
you have a set system with n sets in it
then the system has discrepancy at most
six times square root n and it's called
the six hundred deviations of files
result because there's a six in the
theorem statement and square root n
corresponds to the standard deviation of
a set under random coloring and the
square root and bound here is actually
necessary we don't know it right
constant in front of it but more or less
it's tight and what's most interesting
here is that it beats the Union bound if
you take a random coloring and use a
Union bound across the different sets
you can show that the coloring gets
discrepancy square root of n log n but
on the other hand there exists as
systems where random colorings get your
discrepancy theta of square root and log
n so Spencer's result beats the Union
bound in this sense
yeah oh
and another aspect of this result is
that the Spencer's original proof had an
ingenious use of the pigeonhole
principle and hence was non-constructive
so it gave no algorithm for finding such
a coloring short of enumerating all
possible colorings and in fact alone and
Spencer Spencer conjecture nearly 20
years ago that there is no efficient
algorithm for finding such a coloring
finding a coloring which gets
discrepancy oh of square root em and
like all good conjectures this was shown
to be false recently in a breakthrough
work by bansal you sure that there
exists a randomized algorithm for
finding a coloring with a discrepancy of
square root n and here I want to talk
about a new elementary and geometric
proof of Spencer's result which also
gives an algorithm to find such a
coloring and I want to stress that our
result is truly constructive in the
sense that even though bunzl given
algorithm for the problem the analysis
of his algorithm still appeal to the non
constructive proof so he has some semi
definite programming relaxation and to
argue about the value of the program he
used Spencer's non-constructive proof
whereas our algorithm itself gives a
proof of the result and more than the
specific result I think the technique we
introduced for finding such a coloring
that we call edge walk which involves
some rounding linear programs where
Brownian motion seems to be of much
broader potential and could be used
elsewhere and very recently about 23
weeks back rot wash used some of our
techniques to give the first
improvements for the bin packing problem
in nearly 30 years so it seems to have
other applications which America their
investigation
does that mean just with a fixed
probability your algorithm right we'll
give you one more and you can also do an
I mean make a deterministic but I think
hear the punch line is just finding such
an one such algorithm and here's the
outline of the algorithm I'll describe
the algorithm in its full glory and also
sketch its analysis so the algorithm in
verse two steps one is the partial
coloring method which was used by
Spencer and we will also use and next
I'll describe the edge walk algorithm so
let me start with the partial coloring
method so the partial calorie method was
introduced by back and the philosophy is
as follows in the discrepancy
invalidation problem we said we want to
find a red blue or 1-1 coloring and
becks I just said instead let's find a
1-1 0 covering so where some of the
variables might get a value of zero but
if you don't put any restriction here
the problem becomes trivial you just
output the all zeros vector so backs is
so instead let's find a partial coloring
where at least half of the coordinates
are non zero and the eye is as follows
let's say you find a partial culling
where half of the coordinates are
nonzero like the one shown here and miss
I say once you find such a coloring
let's hide the things which are non zero
let's forget about that and recursively
solve the problem on the remaining
unfixed variables so you find another
partial coloring on these guys let maybe
n over four elements and then repeat the
process until you cover the holes where
they are all these are available all the
variables and if everything works out
according to plan the hope is that the
first partial coloring gives you a
discrepancy of square root n and the
second partial coloring gives you a
discrepancy of square root of n over 2
because now you are working or a
universe of size n over 2 and then n
over 4 and so on so you get a
geometrically decreasing series the
total discrepancy is off square root I'm
which is what we want and more
concretely the first step is you have a
collection of n such systems and sets
over some universe of size N and you
want to find a 10 minus 1 coloring such
that the coloring a small discrepancy or
fruit n and most importantly at least
half of the coordinates are nonzero
right so here I'm simplifying if you put
an M there it will be square root of n
Times log of M over N but I think this
is the main step once you get this it's
not too hard to get to other things it's
square root of what number of sex on the
roof oh here its square root of number
of variables number of elements times
log of M over N does the precise bond
and our main result is that there exists
there is an efficient algorithm to find
such a coloring and that's what I'll
talk about and because there is an
efficient algorithm to find such a
coloring as a corollary we get that
there exists one such coloring which was
the main point of the previous results
so let me now talk about the edge walk
algorithm and to motivate the algorithm
let me rephrase the problem in a
geometric language so here's the
discrepancy set up you want of coloring
which minimizes the maximum imbalance
there is a different way or perhaps more
intuitive way of measuring the
discrepancy which is to look at the
matrix vector product of the incidence
matrix of the search system with the
coloring vector then the discrepancy of
the coloring is just a maximum entry an
absolute value of the matrix vector
product so you want a plus or minus 1
vector so the Infinity norm of the
matrix vector product is not too large
and further if you let V 1 through vm
with the indicator vectors of this else
in your set system what we want is a
coloring Chi such that the inner product
of Chi with each of the indicator
vectors is small and now this
constraints dinner product of Chi with
the indicator vectors being small these
are all linear constraints except with
the caveat that you want guide to be a
plus or minus 1 vector and this
naturally suggests the approach of
looking at the linear programming
relaxation for the problem and if you do
that you you end up with the following
polytope p which is a set of all X so
that each entry of X is at most one in
absolute value and the inner product of
X with each of the indicator vectors is
small so this is a linear programming
relaxation of the discrepancy
minimization problem and our goal is to
find a nonzero lattice point inside this
pollito not just a non zero point but an
integral point which has many non zero
coordinates that would correspond to a
good partial coloring
and in the rest of the talk i will refer
to the constants of the form absolute
value of I X I at most one as color
constraints because that's where they
come from and the inner product
constraints as discrepancy constraints
because that they correspond to the
discrepancy of the set system so here's
the goal set up again you have this
polytope and we'd like to find a highly
non zero lattice point inside this
polytope and for intuition let's use the
distance from the origin as a proxy for
how nonzero a point is that's just for
intuition and so we are starting at the
origin let's say and our goal is to find
a vertex which is kind of as far away
from the origin as possible but still
inside the polytope and the way we do
this is to use Brownian motion so we do
a random walk in n dimensions until you
hit the boundary of the polytope and
once you hit the boundary of the fault
of we need to decide what to do we still
want to keep moving away from the origin
because we want to increase the distance
from the origin on the other hand we
don't want to cross the polytope so we
take the greedy approach which is to do
the Brownian motion but now constrain
yourselves to lie within the boundary
that you hit he do a random walk or
Brownian motion until you hit another
constraint and then you repeat the
process until lo and behold you hit the
vertex that you wanted to get at the
beginning so this is the full algorithm
and the claim is that this algorithm
will find a good partial coloring ok
motion is through faction right right so
if Brown yeah it is through fractional
points so in the mainland morally the
stopping criteria is just go until you
reach a vertex vertex of the pollito but
in reality we just stopped after certain
number of iterations because we wanted
to be discreet walk but this final point
is also likely to be fractional so most
of the coordinates will be integral
except some coordinates half the
coordinates will be integral half the
coordinates will be fractional and will
then recurse on the fraction coordinates
as before so half the coordinates will
be integral that's what you want to show
the face sort of coloring face or the
discipline system exactly that's the
point so I mean let me describe the
algorithm fully because this continuity
continuous Brownian motion is not
exactly is nice for intuition but when
you try to implement it it's it's a bit
problematic so to make the algorithm
formal let me describe what's the random
walk that will use so the random walk is
pretty natural you have some subspace V
and given your current position you just
choose a random Gaussian vector in your
subspace and take a step of length the
size gamma in the direction where gamma
is some step size you want to take tiny
steps not too too large so the walk
looks something like this okay and
another minor issue is that in the
description I say you stop the walk once
you hit a face but if you're doing a
discrete walk you might not hit the face
but might overshoot it and the solution
is just to introduce the slack and near
each boundary so if you get too close to
the boundary then we'll say we hit it so
let me now describe the algorithm so the
algorithm gets us input some vectors
which you should think of as the
indicator vectors of your set system
and it runs for certain number of steps
which depends on the step size gamma but
let's not worry about the
parameterization too much so I start
from the origin and the dick th step let
color T denote the set for all color
constraints or color phases that are
nearly hit this is the set of all
coordinates I says that the absolute
value of the earth coordinate is very
close to one and let discrepancy T do
not the set of all discrepancy
constraints that are nearly hit this is
set of all vectors we change such as
dinner / to VJ with a current vector is
very close to the threshold that we
don't want to cross and as I said you
still want to do the random walk but not
change any of these constraints which
are very close to being violated so we
just walk in the subspace that is
orthogonal to all of these constraints
so if you let v tebe the subspace you
just pick a random gaussian vector in
that subspace take a tiny step in the
direction and repeat this process and
the number of steps you have to do this
will roughly be 1 over gamma square
where gamma is the step size so that's
the algorithm and the claim is that this
will find a good posture coloring with
some with non-negligible probability now
let me give a sketch of the analysis and
the actual proof is not too much more
complicated you just have to write some
tail bones but it's not too hard yeah
you may overshoot so it happens it very
tangled up but then you
you keep you staying at the point of
actually apps nevertheless we don't
create so you I mean it happens it's
only tiny probability that you kind of
ignore such events so if your step size
is much smaller than this lack you
introduce you're never going to or shoot
down the point for you you you stay
actually off the face but yeah yeah so
you can think of the slack as being
polynomial e small such that you know at
the end if it's 1 over n you can just
drowned it to anything you want it's not
going to affect the error in the
discrepancy problem could you use raw
emotion if you didn't care about
although if you just agent efficiency I
think so I mean yeah probably and let
this up yeah you should be able to
Brownian motion is basically like the
case where gamma is going to 0
so now let me describe the analysis the
setup is the same you have this polytope
and we'd like to find a highly non zero
the lattice point and the punchline for
the analysis is that the discrepancy
faces are much farther from the origin
than the color phases okay so let's say
we're shooting for a constitute of 100
times root n okay then the discrepancy
faces at a distance of hundred from the
origin because each of these vectors
Vijay has norm at moes Luton and the
color phases at a distance of one from
the origin because they're just saw the
form X is less than equal to 1 and x
designed by the way we design the random
walk it looks Gaussian in any direction
with some variance so you do some
calculation of the martingale tail bonds
and you get that the probability that
the walk hits a discrepancy phase is
roughly exponential in minus 100 square
it's like a sub Gaussian tail and the
probability that the walk hits a color
phase is roughly exponentially minus 1
is not too small ok and as a consequence
of these two bounds you get the
probability that the walk hits a
discrepancy face is much smaller than
the probability that it hits a color
face it's so you fix any discrepancy
phase in any color phase it's more
likely to hit the color phase and then
when you take a kind of expectation or
all the discrepancy faces Christmas when
you say a lotta hits do you mean
that it ever hit this entire business
that you've defined with yeah the face
doesn tunnel right
okay and because of this comparison
what's morally true is that you end up
hitting the cube faces more often and
let's say you don't worry about this
dispute as an issues and you run the
walk until you reach a vertex but when
you reach a vertex you should have hit n
constraints and we know that most of
them are color constraints and if you
are satisfying n constraints and most of
them are color constraints half of them
must be color constraints that means you
get a good partial coloring so the
estimate you have there on the top is a
song about one rounded tip from the
origin until you hit some constraint
right when you say for with you
mr. busy place is smaller oh so this is
or the entire run of the algorithm let's
fix a discrepancy phase the probability
that his third discrepancy phase is much
smaller than the probability that hits
any of the or like you know an average
cube constraint so this is wrong reason
you give its fears the first time it
hits like this the start clear right but
no same kind of intuition actually works
even afterwards because yeah I cheated a
little bit and you're catching me on
that is that it's not any color face but
like most of the color phases which is
so it might not be true for let's say X
1 but it'll be true for majority of the
color phases this is a small understand
the quantifiers here you are fixing two
phases one description a face and one
for location right so it was a
probability that it is the discrepancy
face throughout the run of the algorithm
it's roughly this guy then with
hyperbole you never had any time hey no
but there are many of them right yes
they're not exponential 100 is a combat
time smelly 100 is a constant here you
should think of maybe if you put a six
here then it might mix
it's a constant and the number of
discrepancy constraints might be larger
or is comparable to the number of color
constraints but really this when you say
a half you could improve that too yeah
but what once you hit one of those
described it's your face so your yo-yo
random walk is no longer truly Gaussian
yeah but what happens is the first
inequality still holds because if you
look at the projection it's a Gaussian
with lesser variance so you're doing
better and for the other guys it's it
won't be true for every color constraint
but we do on average and yeah it means
like the intuition and you write down
the calculations it will come out to be
true on average you might decrease the
variance in for one of the color
constraints but not for all the
directions simultaneously
okay so that's basically the proof and
you just have to write some tail bounds
and use this averaging argument to
formally verified and the key point here
is that we use the symmetry of the
Gaussian distribution to argue this fact
and that's where we beat the end up
beating the Union bound in the analysis
so let me summarize so you use our edge
walk we go we call it the edge walk
algorithm because you're basically
trying to walk on the edges of the
polytope as much as you can and so it
gives an algorithmic form of the partial
coloring lemma you find a good partial
coloring and then you recurse on the
unfixed variables as in Spencer's
original argument and you combine these
two to get Spencer's there so here's the
symbol so I wasn't completely sure I did
some cruel estimates and what's a matlab
code to figure out the constant it came
out to be 13 it probably can't we get
better but some first order
approximation say 13 sorry just which I
understand when you choose the amount of
this slide for the places you're saying
you
it just depends on em on yet on air
seemed obvious to me because I mean
somehow it might depend on like hapless
that and just get to each other or
something so am right so you choose it
let's say you choose it one over N
squared or something okay and you'd run
this whole algorithm and then you
whatever slack you have you just round
it to the nearest 1 or minus 1 and
because it's 1 over N squared and each
vector has norm and you don't hurt
yourself much
so before moving on to the second part
let me just say that the algorithm I
described you can do it for any polytope
this is just a random walk on polytope
so you just start from the origin if it
contains the origin or start from any
point inside the polytope and then
repeat the process as before and here
let's look at kind of the ideal scenario
where you do the Brownian motion and it
seems to introduce some strange
distribution on the vertices of this
polytope which kind of is cute towards
vertices which are closer to the origin
in terms of the bonding hyperplanes it's
like an iterative rod you're misusing
your thumb grounding it's kind of an
iterative rounding scheme but this long
what passion point a little is a vortex
this really you returning a vertex of
the point of anyway and grounding you
start with a vertex of the koalas are
actually want to couple the faction and
down one of the here right no grounding
you saw the we saw your we were
optimized a linear function you write a
vortex of a koala kid with that's a
special case of rounding rounding is no
but the drugs or get to apologize what
is a relaxation forward open bottles of
your tu papa right you could build the
same complaint to me yesterday
should I think of this as the integral
yeah it's like you're trying to find an
integral solution so you find this
fractional integral solution and then
you iterate on the remaining things and
so on the problem is if I have as they
say an integral product in the context
of the political point and I don't know
what that I pro players are so i can't
really do it so if i have they say i
start with the integral part of which is
a conical above the integral points i
don't know optimize over right then I
don't know what type of plates are right
how do I do
please Mike want opening signal anyway
there yeah I mean have a power reaches
around unity our modern or for any
polytope you just so here's the rounding
algorithm if you want you have a
fractional solution you do this walk and
you get some vertex and you look at the
coordinates of the vertices which are
now integral that's if it's not then the
rounding algorithm as sale but that's
the approach so you find some
coordinates which are integral okay yeah
then the algorithm is failing for those
kinds of its it's not a universal
grounding algorithm so if it succeeds it
will succeed so this is the approach for
finding an integral solution I mean so
you're it will work if this vertices of
the polytope are such that they have
many integral coordinates but that's a
success I mean that's when its success
if it is if the part of this correctly
has this property to begin with you know
I should be strong just by running an MP
anyway I'm finding out
no no but think of like which vertex
there might be a vertex which has some
integral coordinates but not all
vertices have this property and you want
to find one such vertex just as in the
discrepancy minimization problem it was
not even clear that there existed such a
vertex
and in fact the these vertices are much
fewer than the total number of vertices
they are exponentially small fraction of
the total number of vertices and you
want to kind of isolate these vertices
but closer to the ocean but on the other
hand there some implicitly yeah
so maybe if you expand your definition
of grounding included but my sense of
rounding is you take a faster you can
get a little fine the side gets to the
integral point iteratively and if it
fails let's say it gives the all zeros
thing which is like a trivial rounding
it doesn't give anything meaningful
thing is a rounding algorithm in
principle and the applications also use
this as a rounding algorithm there's a
way of rounding using it around
discrepancy constraints and so on and in
fact Roth watch also use it in this
context he finds the LP solution for
this bin packing problem and uses our
algorithm to round the LP solution not
really because just like two three weeks
old and grant a time to look at
so here's the first part of the talk and
let me now talk about the second half
and here I'll just state our result and
mostly talking about some applications
so water central limit theorems and I'm
sure most of us know what limit theorem
sir but let me just quickly run through
some examples to basically to set up
some notation so the central limit
theorem says that if you are n
independent identically distributed
random variables then the sum of the
variables after appropriate
normalization looks like the standard
Gaussian distribution and it is one of
the most basic results in probability
and has many applications and
pictorially here I had the probability
density function of one random variable
two random variables and so on and you
can clearly see the shape of the bell
curve emerging an other example is large
deviation bouncing probability we'd say
that if I mean specifically for
independent bounded random variables sum
of independent identically distributed
random variables probability device too
much from the mean is somehow comparable
to what happens in the Gaussian kids and
such bonds are crucial friend and also
many randomized algorithms and their
gain following the same kind of broad
paradigm of limit theorems because
you're comparing the behavior sum of
independent things to that of the
Gaussian distribution and why should we
care about limit theorems in algorithms
or in computer science applications
mainly because they give us a nice way
to translate discrete problems to
continuous problems where we often have
many more sophisticated tools like
calculus or convex geometry and so on
and one nice example of this philosophy
is the method of convex relaxations for
comet oral disk optimization problems so
there you look at relaxations which are
now continuous problems and then you can
do some rounding or so on so limit
theorems have a lot of applications in
computer science to working theory or
social choice theory to learning theory
complexity theory
communication complexity and also in
pseudo randomness and here i will talk
about one limit theorem and its
applications to questions in wedding TV
and learning theory so my work has led
to two generalizations of the central
limit theorem to multi-dimensional and
discrete versions which are motivated by
such applications when problems in
learning theory and here I'll talk about
the multi-dimensional version so to
motivate the multi-dimensional central
limit theorem let us look at a special
case of the classical central limit
theorem says that if you have n random
plus or minus 1 signs then there some
looks like the standard Gaussian
distribution which is a very very
special case of the central limit
theorem but there's a geometric way of
looking at this result which is as
follows let's say you have the you know
these universe of the n-dimensional
space denoted in two dimensions here due
to limitations of PowerPoint and red
squares you should think of as n bit
boolean vectors and blue circles as
Gaussian random vectors random Gaussian
vectors and the central limit theorem
says that if you take a hyperplane in n
dimensions which slices this space then
the fraction of boolean points that lie
on one side of the hyperplane is close
to the fraction of Gaussian points that
lie on the same side of the hyperplane
and given this geometric interpretation
it's natural to ask what happens if you
have instead of having one hyperplane
you have two hyperplanes is the fraction
of boolean points on the same side close
to the fraction of Blue Point still what
if you have three hyper planes or more
generally you have many hyper planes
which now form a palto and this leads to
the question of central limit theorem
for polytopes so you have a poly
open n dimensions and we would like to
know if the boolean volume of the
polytope which is the probability that a
random plus or minus one point license
at the polytope is close to the Gaussian
volume which is the probability that a
random Gaussian vector lies inside the
polytope and this is what we'll show a
limit theorem in this context but let me
just put some conditions under which our
theorem alcohol so we'll assume that our
polytope has a polynomial number of
facets because that's the most
interesting case for applications and
most importantly we also assume that the
polytope is regular in the sense that no
variable is too influential for any of
the bonding hyper planes let me explain
this so geometrically this says that the
polytope any none of the bounding hyper
planes of the polytope are aligned with
the coordinate axis vectors for example
the hyperplane x1 will not be regular
and the hyperplane x1 plus 1x sum of all
the excise is a regular hyperplane and
regular polytopes are our appear
reasonably commonly in dense integer
programs and so on but most importantly
regularity is needed for such central
limit theorems without regularity you
cannot have a central limit theorem so
it's a reasonable assumption to make and
in joint work with Russia and clients we
showed that if you have such a regular
polytope then the boolean volume of the
polytope is close to the Gaussian volume
with the manner which is little o of 1
the little of one is actually something
like log squared the number of phases
but it's most importantly a little off
one where r goes to 0 as the dimensions
go to infinity these planes are from the
coordinate axis it
I mean not in terms of the distance from
the origin but in terms of orientation
young
so here's our statement are limited and
restated and once again the notable
thing here is that it beats the Union
bound there's a rich body of literature
on multi-dimensional central limit
theorems but unfortunately if you apply
those results in our setting you end up
with a bound which is at least linear in
the number of facets of the polytope and
it it's hard or implicitly I think this
is because when you specialize those
theorems to our setting you end up
bonding the error for a single
hyperplane and then kind of taking a
union bond across the different hyper
planes but you need the condition that
it's husband or even facets so the right
so it's it again so why do you need the
vitalzym pure result of life form so it
applies to the most like the error is
actually it applies for any positive
with any number of facets but it's the
easiest to write down so if the number
of facets this K there is like log
squared K so as long as case may be
slightly sub exponential you get
something meaningful if you know this
potential you could shave off all the
corners or but length of course but not
explanation but is Mouse stuff between
cool no right right we can handle up to
something like 2 to the root an order
I do today enter some foreign and yeah
well again it doesn't depend on the
distance of the fences from the or
captures the large deviations taste as
well no I think it's to improve I mean
it's to impress eyes for large deviation
bonds because it's not capturing the
behavior details even the usual central
limit theorem you can't get large
deviation bonds from that and our bonds
are actually nearly optimal we get
comparable lower bounds as well so you
get something like log squared K and the
lower bound is square root of lock it
and before going into the applications I
won't have time to delve too much into
the proof but let me say a few words the
first thing I want to say is what makes
us think this is possible that you can
beat the Union bound in this context and
if you look at the literature in
probability one of the main techniques
they have for proving limit theorems is
are these variational methods and at the
core of them is if you bond if you want
to bound the error you need to bound the
probability of some of regions which are
close to the boundary so when you do the
analysis things which are very close to
the boundary on either side if they have
too much volume then you will be in
trouble and for the case of polytopes
let's say you start with your poly top
and look at all points which are at a
distance of epsilon from the boundary of
the polytope so you get k of these
strips like this like the one shown here
and there is a beautiful result due to
nazarov be sure that the Gaussian volume
of this blue regions is actually square
root of log K times epsilon and if you
use Union bound you get a bound of K
times epsilon because each strip has a
volume of epsilon and there are K of
these strips so this is one of the
places where you beat the Union bound
and it took it knowing this result was
was prompted us to think
such results exist and you can use it
case or k is the number of facets this
is in n dimensions but the figure isn't
doing powerpoint and finally the actual
proof of the result uses this this is
one place where we beat the Union bound
and you also use some other results from
non traders from convex geometry but
what I find surprising is the proof
actually uses techniques that were
developed in the context of designing
pseudo-random generators so the result
has nothing to do with pseudo randomness
or designing prgs but some of the proof
was at least morally inspired by those
techniques you can d randomize it but we
thought the width of this ordinance part
we probably won't come across this at
least us so let me now talk about the
applications of the limit theorem and
you'll see how to use the limitation to
beat Union born in various cases the
first application when talking about is
to noise sensitivity let's say you you
have some election where people cast
their oats and you decide the outcome of
the election based on a majority and
what we would like to what we are
interested in here is what if there are
errors in registering the words so you
have this words again and there are
errors and what we'd like to know is if
the errors caused the outcome of the
election to change and this can be
captured by this nice concept from the
analysis of boolean functions called
noise sensitivity so he has some boolean
function f and some noise parameter
epsilon and the noise enca tivity of f
is the probability that the function
evaluated at a random point changes its
value when you perturb each coordinate
of the random point with some
probability epsilon
so coming back to the oting scenario so
the election scheme here will be decided
by the function f &amp;amp; noise sensitivity is
the probability that if you flip each
word with some probability with the
error probability epsilon the outcome of
the election changes that is nice and
stupid x is also at random so it has
many applications in analysis of boolean
functions starting with the work of con
calor and linear who implicitly use it
to shows and Fourier concentration
properties ha star in 97 you who used it
to show some optimal inapproximability
results benjamin akala and SRAM who
formerly i think as the first base it
was formally defined and also use it for
prob some questions in percolation and
so on and here i want to talk about the
noise and sative ax t of majorities or
more generally weighted majorities and
here Paris showed that the noise
sensitivity of any weighted majority
with noise rate epsilon is that most two
times square root of epsilon okay and as
you been asking before when you have a
question about one majority it's somehow
natural to ask what happens if you have
two majorities or three or four or
several majorities and this leads to the
problem of computing the are bonding the
noise sensitivity of polytopes so you
have some polytope in n dimensions and
the noise institute of the polytope
geometrically means the following so you
generate a random point and we're
interested in the probability that when
you put at this point it ends up
crossing the boundary right if you take
a random point perturb it and we want to
know when will it how likely is it
across the boundary of the polytope and
using our limit theorem we can show that
the noise sensitivity of any regular
polytope is at most poly logarithmic in
K times a factor which is polynomial in
the noise rate at
yeah bullion points here and once again
this beats the Union bomb and the best
previous result was basically said that
you have the you have a bond on the
noise instead of the single hyperplane
and if you have care things the noise
sensitive can increase that most by a
factor of K here is the number of facets
and how does the proof go at a high
level the proof basically says the
following so you have that you want to
compute the boolean noise sensitivity we
use our limit theorem to translate the
boolean problem to the Gaussian world
where the Gaussian analog of noise
sensitivity is the surface area or the
Gaussian surface area of a polytope and
there there's a beautiful result of
Nazareth which I briefly mentioned
before says the Gaussian surface area of
a polytope with K facets square root of
log K times epsilon the square root of
long tail so here it's important that
our limit theorem itself have very good
dependence on the number of facets
because if you lose too much in the
limit theorem even if you have very good
bonds for the Gaussian well you don't
get anything and so we have to beat the
union bond in both cases and one
application of a noise sensitivity bond
is to learning intersections of half
spaces and let me skip this application
for now and let me just summarize by
saying that in both of these cases we
bet the Union bound by using some
geometric techniques especially some
symmetric properties of the Gaussian
distribution and there's other example
of along the same lines in my work on
computing the supreme of gaussian
processes let me briefly mention some of
my other interest and then i'll conclude
so one of the major focus has been in
pseudo randomness and constructing
pseudo-random generators and they're
been focusing on constructing
pseudo-random generators for geometric
shapes like half spaces polynomial
threshold functions polytopes and here
the pseudo-random generators are
naturally motivated by questions in
complexity theory but they also have
applications in algorithms especially to
a dimension reduction counting solutions
to integer programs and so on in two
bounded space algorithms where they're
helpful for making some streaming
algorithms efficient and simulating
random walks efficiently and so on to
bond adepts accuse and to counting
algorithms so that is one third and
another thread is hardness of
approximation where the focus of my work
has been on the unique games conjecture
which is probably the most important
open question in this area and which if
true would solve many other important
open problems like the complex term max'
kurt vertex covers passive skirt and so
on and here my work gives the best
evidence in support of the conjecture
and suggests a qualitative approach
approving the conjecturing by using
alphabet reduction and this is one of
the things had thought about in the past
and will probably think about in the
future and in learning theory we work as
focus on learning half-spaces
polynomials and more recently to
adapting the smooth complexity framework
from framework from algorithms to make
certain seemingly intractable problems
in learning theory tractable and I also
done work in data mining where the focus
has been mainly on matrix rank
minimization problems so you have a
bunch of linear constraints on matrices
and you want to find the minimal rank
matrix which satisfies these constraints
and this has many applications majors
completions such as a Netflix challenge
or recommendation systems metric and
computing metric and buildings colonel
learning and so on and hear me this is
more empirical work so the focus has
been on giving simple and practical
algorithms which scale well for
high-dimensional data and also have some
nice rigorous guarantees saying summary
my work has basically been around using
the structure of randomness in the form
of limit theorems and the geometry of
randomness for various questions in
algorithmic proofs or complexity theory
pseudo randomness learning theory and
vice versa and it is these connections i
think that from the core of my research
and i'd like to explore these things in
the future thank you questions
they might have further time for
here it is you have a what persib esta
lower bowel persistence beautiful domes
of poly toes you need square root log k
times epsilon square root of it up to
some constants if you have a positive
with K facets it seems like they're
excellent to the half and square root of
lock loop we have log squared and also
the regularity assumption so I think it
might be easier to push the cave down
than to push the epsilon down because I
think getting square root of epsilon in
our techniques might be much harder then
like we use what's called the Linda bug
method and even for a single like if you
use that method even for the central
limit theorem you end up getting a bond
like epsilon to the one fifth so that
might be more harder than they okay the
first smoke k is nothing better than
cable capsule
so you have our car result and it beats
the union bond but it has the regularity
assumption which building in the Holy
scale oh right yeah
thank you again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>