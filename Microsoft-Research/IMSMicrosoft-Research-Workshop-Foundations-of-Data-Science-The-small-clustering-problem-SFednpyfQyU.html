<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IMS-Microsoft Research Workshop: Foundations of Data Science - The small clustering problem | Coder Coacher - Coaching Coders</title><meta content="IMS-Microsoft Research Workshop: Foundations of Data Science - The small clustering problem - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IMS-Microsoft Research Workshop: Foundations of Data Science - The small clustering problem</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SFednpyfQyU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi Rebecca Stearns I just joined Duke
University previously I was at Carnegie
Mellon thanks so much for the organizers
for being here um so I'm going to talk
about how I work on sparse data
specifically for records English so
we're the big data conference so I work
on merging many different databases to
remove duplicate entries and so that's
known as record linkage deduplication
entity resolution and it has many other
names and so specifically you can view
record linkage as a clustering problem
and that's what I'm going to focus on
for most of my talks in new methodology
and I'm going to talk about how some
popular approaches such as the deer
schley process or more generally the
Pittman your have a lot of limitations
and so what it has to do with is as the
number of data points per cluster it's
not expected to grow without bound and
because of this it doesn't really work
very well for record linkage and so I'll
get into this in terms of motivation
more throughout the talk and so what we
propose is a very new novel clustering
model that captures this behavior and
it's actually a model and not a process
and that's that's pretty important and
the applications in terms of why this is
important is it it applies to many
different important applications such as
precision medicine census data
investigative journalism I do a lot of
work on human rights violations off or
the Syrian civil war but it also can be
used for credit fraud and many other
applications so i like to do these
catchy examples to illustrate what
record linkage is so I've picked on see
Feinberg enough who was my postdoc
advisor so now i'm going to pick on mike
jordan who i wrap in a bandwidth so
whenever i talk about mike jordan i used
to talk about the basketball player cuz
i played
so I used to play basketball like so
used to talk about Michael Jordan alive
but now I'm a statistician so I talk
about Michael Jordan Knight and so when
I talk about Michael Jordan my friends
you know I talk about Michael Jordan
Michael Jordan and then I say oh he's a
really great trummer and then they say I
didn't know Michael Jordan the
basketball player was a drummer and so
then you kind of see this issue that
comes up of Michael Jordan gets confused
with Michael Jordan and then if you go
to google and google michael jordan you
know the basketball player pops up
whereas if we're statisticians we're
usually looking for our beloved michael
jordan down here who's you know done so
many great things for our community the
literature etc and so the question is
well how do we distinguish these two
people if they're in databases and how
can we properly build models for these
so I view this as a graphical model so
you can think of having different
databases here I'm just viewing them as
Apple Amazon and Google but it could
really be anything and then I'm
illustrating a cluster or so I'm
representing a person here as a node so
Larry Wasserman for example and each
person or entity has attributes or
features that are hiding behind them so
they have an address they have an
occupation they have a gender larry has
two cats he has a favorite color which
happens to be purple and then if I look
at Mike then I probably think that this
Michael Jordan and this mike jordan are
the same person because if i look behind
at their features and attributes then
they're the same but then if i look at
Mike I jordan and mike j jordan and then
I look at their attributes if i'll
actually look into them that i see well
maybe this mike jordan is a drummer in
this mike jordan as a basketball player
and so they're not the same person and
then if i go a little bit further just
to further illustrate the example and
this information is publicly online on
the white pages then i can actually look
up Michael Jordan's address and his age
range and you can actually get his
telephone number which is little bit
scary and then for the Michael Jordan
basketball player you can only find this
information so he has it he has it
hidden sorry he's apparently he's over
65
and so these are clearly not the same
Michael Jordan so any model or method i
propose i want to be able to say that
these aren't the same are not the same
Michael Jordan but then if I go back if
I have two versions that are similar but
a little bit noisy I want to be able to
say that these are the same Michael
Jordan as in the statistician who is
also a drummer so I'm going to propose a
bayesian generative model and why
bayesian so bayesian because i want to
be able to do the record linkage
simultaneously so across all databases
and within each database all at one time
and i want to do this so i can propagate
the uncertainty err error of the linkage
process and so why do I want to do this
I want to do this first of all so i can
know how much linkage error is across
and within all databases but i also want
to do this so i can go do other analyses
after and then use the record linkage
error to be put in those other
subsequent analyses so for example if i
want to estimate a population size then
i'm going to do something like capture
recapture and i want to be able to
propagate that error and if I just want
to do something as simple as logistic
regression then I also want to be able
to propagate that Aaron from the linkage
process and so that errors really
important and using a bayesian way you
can do that very easily where is from a
frequentist perspective you typically
have to put a bound on it so I've done
some work in terms of taking records and
you cluster them to a hypothesize layton
where the layton is basically random and
it's dropped down from an Oracle and
from working on these papers I found
that it wasn't clear how to properly
model or put a generative process on the
layton entities and so that's what I'm
going to talk about so bear with me now
I'm gonna lay out a little bit of
notation and then contrast what I'm
gonna call large clustering which is
more popular clustering which is
probably what you're all familiar with
where some something new that I'm going
to define which is what we call small
clustering so I'm going to define X 2 xn
to be the records or just any data you
have that we're going to divide into K
random clusters and then each cluster
will course but correspond to a single
Layton entity and then the Z's are going
to correspond to cluster assignments and
then each cluster Simon is really nice
and that it induces a partition pie on 1
to N and the partition will be drawn
randomly from some distribution which
I'll define in a new model in a few
slides so this partition which we call
pi n is going to be a set of mutually
exclusive and exhaustive subsets on 1 to
N and each subset corresponds to a
cluster so now if I think about the
number of data points going off to
infinity so when n is really large then
I have an infinite sequence of records
and i also have a corresponding infinite
sequence of cluster assignments and i
also have a partition on the positive
integer so i'll define this to be pi set
of em and so the probability that some
record end is assigned to a cluster k is
simply just what we define to be p NK so
it's just the probability that z n is
equal to k and then if i want to know
the proportion of the first end records
that are assigned to a cluster then it's
simply just summing up this indicator
and dividing by n so nothing
mind-blowing here yes why do you think
you have ok because i'm just so that the
k is random so what all i'm doing here
is i'm saying that I'll get into that
later where I'm basically conditioning
on K but for here everything's under
changeable but here I'm just saying that
I have some records and i'm going to
divide them into K clusters so you can
imagine for now just a Kingman paintbox
okay they very comfortable because
they're exactly so there's many existing
approaches for how to partition what we
call pi end
and this will yield an infinitely
exchangeable random partition and so
here then the zns are identically
distributed not necessarily independent
and of course then this probability of
ZN equaling k doesn't depend on the
particular n and then by the strong law
FN of k is going to go to p and k almost
surely and so the observed clustered
frequencies will converge to the cluster
probabilities and then this the
important point here is that the size of
the cave cluster is going to grow almost
surely linearly with a number of records
which is not what you want for sparse
clustering or sparse network data and so
if I think about it so if the number of
data points is growing off to infinity
for record linkage what is really
happening is the number the size of the
layton clusters is negligible compared
to the amount of data that I have and so
that's very different from what you have
from traditional clustering approaches
yes so I'll define like so it's it grows
that it grows sub linearly so what we
define is what we call this small
clustering property so this is new so we
say a distribution on the partition pi n
or on the cluster Simon's on the Z's has
this Cutlass clustering property if the
f NK is going to 0 almost surely as the
data as the data is going off to
infinity so it just means the cluster
sizes as I said are going sublunary in
the number of records and of course
following from this property if some
sort of distribution has this property
then it can't be infinitely exchangeable
and so this is a very key point here so
just as a motivational example of what
typical data for record linkage looks
like and just motivating why you would
never want to put a dershlit process or
a CRP on it then over here on the Left
we have a hundred thousand campaign fine
donations from 2011 2012 and so what I'm
looking at on the y-axis is the log
number of clusters and on the x-axis is
the cluster size so if I look at for
example the first cluster it's simply
saying that their log 10 to the fifth
basically clusters of size 1 and out
here there are in this bin basically 10
clusters of size 85 and so you have this
interesting kind of distribution here
and if you try and simulate it using a
CRP of basically using the concentration
parameter that's very small of point one
or even smaller then you don't get
anything that would replicate anything
that would look like this finance
campaign data so what we propose instead
is we call it permutation of Poisson
sizes we call it poops because you could
use it to find perpetrators in crime
data k is the number of clusters and is
the number of data points assigned to
each cluster and what we do is we take K
and we draw it from a Poisson with
parameter alpha so Poisson is a
distribution that's really late tailed
so it will have this knife nice property
that's very different from for example
the deer slow process and then
conditional and K the number of data
points is drawn iid again from opus on
now with parameter lambda and so alpha
is similar again to the concentration
parameter and lambda is equivalently p
expectation of what you think the
cluster assignments will be so now we're
going to assume that the data points are
given because traditionally in record
linkage we typically know that they are
like any applied problem and then the
vector of the cluster assignment Z is
going to be drawn uniformly at random
from this set of permutations
and then the cluster assignments then
define this really nice random partition
on 1 to N in which the number of
non-empty sets is at most K and so from
this we can each cluster but what is
then sir no and so the ends and 12
uncarrier the data points and k is the
cave cluster and then the Z's of the
cluster assignment let's sing this
record goes to this cluster so then the
marginal distribution of the partition
which we call PI C for this particular
model perps can be written in closed
form and you can drive what we call a
receding algorithm so again it's not a
process but it's akin to the restaurant
process and if you run the receding
algorithm for long enough it's going to
generate approximate draws from the
marginal distribution and so if
basically we just repeat the following
so if we remove a record which is akin
to like a customer in the restaurant
process from the curtain cluster you can
think of here of table then you simply
then just receipt the record or the
customer so at each occupied cluster
with probability it's here proportional
to one so you can contrast this with the
Deerslayer Chinese restaurant process
where it's proportional to the number of
data points and so here you don't get
the richer get richer property which is
what you don't you don't want that and
then at a new cluster table it's
proportional to e to the minus lambda
and then waited by the concentration
parameter so I mean in some sense you
just posturing based on a usual mixture
model yeah exactly so the nice thing
here too is perps doesn't depend on any
distributional assumptions of the data
so here I'm just going to show a simple
example of how we can do it for record
linkage under categorical data but you
could put anything in terms of the data
on top of that so here we're going to
assume that some record can set consists
of L fields or
or different features so something like
gender date of birth and then the
records again our index by N and the
clusters our index by K and then each
individual is represented by again a
cluster assignment and the cluster
assignments correspond to a partition
and then we'll assume all of the fields
are categorical value just for
simplicity here and we'll let 12 ml
denote every possible value for a
particular field or future L and so the
generative process in terms of
categorical is very simple it's simply
that for a particular data point we put
a multinomial distribution on this with
parameter theta and then theta simply
comes from with your slave so we're just
doing a simple multinomial dear schley
since its conjugate and then we're
drawing from the perps which is a prior
on partitions of all records and this
actually depends on Z n which I'm
masking because writing all of that out
is a very technical detail that's beyond
the scope of the talk so just to show
you how it works in practice for some
survey data this is a sample survey
conducted by the Bank of Italy about
every two years and I'm merging together
the 2010 survey that covers about 20,000
individuals with the 2008 survey with
again about 20,000 individuals and there
are about 10 different categorical
features or fields here and so we have
unique IDs for the status at that are
based on social security number and so
we can run basically the unsupervised
method and then check to see how well we
do in terms of of accuracy based on our
posterior pots and also based on a cenar
and FDR so here we run perps for record
linkage and we look at the posterior
distribution of the observed population
size and sample so here's the posterior
distribution the red line is the truth
on a subset of the data which is 520
and the posterior mean is 522 the
posterior standard deviation I think is
around 17 it's number of unique
individuals after duplication so we use
unique IDs to find out how many so I
have unique IDs for the dataset based on
SSN so right but here I was saying that
I'm running it on a subset of the data
just for some I'll get to the
computational issues at the end so here
I'm running it on a subset of the data
so on the subset there are a total
number of 797 individuals and when you
run the entire method there's only five
hundred and twenty unique individuals on
the posterior mean from our method
estimates set at 522 and then the
standard the posterior Center deviation
is 17 and here we simply compare it to a
Chinese restaurant process so here we're
comparing it to a Chinese restaurant
process with a comparable concentration
parameter and you can see that the
population size is much it's
underestimated quite a bit and hear what
you can see is that if you set the
concentration parameter basically almost
as sorry i overestimated next David if
you set the concentration parameter
basically almost 20 then you get really
great inference but you're basically
putting a uniform prior on it and you're
also doing something about
methodological e that doesn't make any
sense and theoretically that where
you're breaking difa naughty and so
while you're getting good inference
you're also you're doing something
that's statistically not sound um and so
this this kind of illustrates the
difference here
sir we're super weird scaling limits
like even if you can pull it off
computationally predictions yeah I'm
going to get into this here yeah so here
we do a comparison in terms of this
small example in terms of comparing it
to the most comparable method and
literature which is just a simple
mixture model for two databases using an
improper prior and we compare it on the
false negative rate on the false
discovery rate um and we do much better
in both the error rates and in time so
their method takes an hour hours takes a
minute however ours doesn't scale for
high dimensions well and it's because
you're doing these random permutations
on that factor and this one minute is
even when implementing split merge um so
we're working now on figuring out how to
scale something better than that you
basically need better proposals uniform
proposals are not all does that answer
your question no I meant that the even
if she could do the conversation exactly
I infinitely sighs population finding
cluster to if it rolls right but I think
if you had better proposals then oh for
this one yeah we could talk yeah that's
something we haven't thought about for
the exact posterior um so just to wrap
up now we're looking at so we've looked
at some sensitivity analysis of the
concentration parameter and lambda and
done comparisons to the chinese
restaurant process and so no we'd like
to do is actually put priors on alpha
and lambda so that they're actually
chosen not by the user but but by a
prior we'd also like to do computational
speed ups other than split merge for
high dimensions to something with butter
proposals we're working on a string
model that's based on a first order
markov model I'm so that's currently in
progress and we're also working on
simulation studies in roc curves and I'm
happy to take any questions oh and I
just like to thank briefly the
organizers again
also the templeton foundation for
funding my research jolly so quickly are
you aware some work done by Peter
McCullough um I'm aware of a lot of work
done by Peter ricola good as what you
did it reminds me you get a couple
papers on something keep other
communication process book for four
classifications so you may want to take
a look okay it's all his webpage okay
great thank you very much on mathematics
rules okay I'm not aware of that but
thank you it's a song to be like lots of
sensors okay that's a really cool the
theoretical part yeah I mean what a
series time okay reminds me take a look
okay thank you is there a way to kind of
get get roughly like do some sort of
preliminary kind of half-ass clustering
I can get this article for sleep
together but you don't know how the hell
with in that cluster they're clustered
and could apply your thing after all you
know I mean so yeah so we've thought
about doing something like random
forests and then using that as like
something like a and getting them oh you
got him down away and getting them down
into a thousand people or doing
something like hashing and then using
that is kind of like a starting point we
just that's an idea we thought about but
we haven't we haven't implemented that
so but that's a great question also the
plus on do you let that depend on the
number of records um you have a plus on
for the number of clusters it seems like
that should grow with some smarts great
with the number of riders yeah so
currently it doesn't it doesn't grow
with our records so that's probably
something for future work in future
direction we've also thought about like
putting like a lucky process instead
for a preliminary clustering it seems it
in the case of trying to identify the
same person across records you can use
something like the lexicographic
similarity of the names right rather
than starting right so in terms of are
you talking about just in terms of doing
some sort of dimension reduction no no
just some to reduce a space of clusters
like icicles I don't get them you get
everyone who's like I've said we're
doing the Michael Jordan right yeah it's
just a Michael Jordan cluster so really
the way that we currently do that is
using locality sensitive hashing which
is like is a super low hanging fruit and
there's some linear time methods now in
the literature by ping lean one of his
students that appeared in tips last year
and so that's what we use now on all the
attributes are you can run it in about
60 seconds and get a high recall and
reduction ratio of around ninety-seven
percent each year microsoft research
helps hundreds of influential speakers
from around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>