<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Program Synthesis from Refinement Types | Coder Coacher - Coaching Coders</title><meta content="Program Synthesis from Refinement Types - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Program Synthesis from Refinement Types</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KZwpQIpqbf4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon everyone I'm Russ Delano
and it's my pleasure to introduce Nadia
Polycarp Abba who has a long history of
doing interesting things in verification
she was a PhD student in birth or meijer
group in at ETH and wrote a thesis on
semantic collaboration which extended
the sort of things that we had tried to
do in the dispatch our project and the
VCC project here in at in the rice group
and extended it in ways that that we
could not have imagined that at the time
to let you write specifications of
programs and then he has looked at
specifications in different languages
written them fully verified collections
library she is she was and she did an
internship here with me ha Moscow
working on security properties of the
the TPM which you did in the context of
the VCC verifier the list goes on she
said could participate it in several
verification competitions and there was
one that I thought that she was the
clear winner and somehow them the no
winner was ever announced I don't know
what happened to the contest was very
strange but in my eyes she was the
winner um yes and she's she's worked on
on a tool called boogaloo which executes
boogie programs and if you know what
boogie programs are and the partial
commands you will be wondering how this
is done and you're welcome to ask her
afterwards and many of you who are for
example in the ironclad project or have
used Daphne will be familiar with and
will love the calc statement that is in
them in there too to write proofs and
that's also Nadia's work here so today
though she's not going to talk about any
of that she's gonna talk about synthesis
and let me just end by announcing that
the that if you're watching this talk
online I'll be monitoring questions if
you have questions you can type those in
as well so with no further ado welcome
with Nadia thank you very much your
stuff um so I'm currently doing a
postdoc at and wait
she with Armando sellers and so I'm
working on synthesis because
verification was too easy for me and I'm
gonna talk today but something that
we've been doing with Armando in the
past like half a year and it's spring
census from refinement types it's very
much working progress still so there are
some things that maybe you're not
perfectly working yet but I will be
really curious to hear what you have to
say about this and be happy about any
feedback okay as we all know developing
programs is hard and developing correct
programs that always do what they're
intended to do is even harder and the
goal of program synthesis is to help us
with that by providing us with a way to
describe programs that is more
high-level more concise or more
intuitive than what programming
languages mainstream program languages
can offer us today and in the end the
program synthesizer will transform this
description into something that is still
efficiently executable but since the
difference between those two
descriptions is quite large there is no
algorithm that can just simply do the
compilation and usually some kind of
search is involved in census so in high
level most program synthesizers look
like this there will be some component
that can explore the space of candidate
programs and there will be another
component that can check that a
candidate actually matches the
description that the user provided and
then give some kind of feedback to the
Explorer and this goes on and on until
we find something that actually matches
all right and if we want one important
point is that if you want the synthesis
process to be completely automatic then
of course there ification has to be
completely automatic as well and the
whole field of protein synthesis is
quite large and I'm going to focus on
like one foot specific area which is
ematic synthesis of recursive functional
programs which is an area that is very
popular in recent years and in that area
people are using mostly those three
different kinds of inputs of
specifications and corresponding
verification procedures because of
course the verification procedure and
the input language has have to match on
some level so one kind of specification
language that people use are simply
input output examples and of course how
can we verify a program for a set of
input output examples we can just
execute themselves we use like a good
old testing and some tools that were
successful with with this kind of
specification are a sure which was by
sumit and house and then myth is from
steve stine george and his group and
lambda squared is from rice and UT
austin then another kind of census tools
in for functional programs uses bounded
checking as the way to verify candidate
programs and what I mean by bounded
checking is you have some kind of
executable assertion or probably you
have a program that is unoptimized used
as a specification for generating an
optimized program and so the techniques
that I used here are usually likes
at-bat asset-based bounded checking
techniques and so sketch is sort of a
popular tool that works in similar
principle but there isn't variant of
sketch though it's called sin track that
actually does census automatic census of
functional programs and finally this
goes all the way to the other end of the
spectrum where as the specification we
use fully formal specifications perhaps
written in some kind of rich logics with
quantifiers or recursive predicates and
as a way to verify programs against
those
locations we can use deductive
verification and lay on from Victor
conscious group is one example of such
tool so all those techniques have of
course their advantages and
disadvantages and trade-offs so here are
pretty much located from like the least
formal that required least level of
expertise from the user to sort of the
most formal but of course they all have
their disadvantages as well so if you
look at those both techniques on the
left they only provide guarantees of
correctness for a finite number of
inputs and as a result they might not
work as well for more complex programs
so for input and output examples the
result is that for a more complex
program the user might have to provide a
lot of inputs and outputs and think
about a lot of corner cases for example
I know that for miss even a program is
simple as dropping a certain number of
elements from the list requires 13
input-output pairs and for for this
those kind of tools that do sort of
exhaustive bounded checking well there
if a certain bound is not enough to
guarantee that the program is really
correct the checking has to go up to a
higher bound and then it gets slow so it
doesn't scale to to check into programs
on many many inputs on the other hand is
the deductive verification doesn't have
this problem of scaling too many inputs
but it does have the problem that it is
rarely fully automatic because your
deductive verification you know that
sometimes you need to give some hints as
to how to instantiate quantifiers or
even worse you have to provide
invariance so when it be just great if
we had something that gives us unbounded
verification but fully automatically of
course that would be great and if it
works for every big enough class of
programs that there would be what we
want so in this work we decided to try a
different kind of input language
for synthesis and a different kind of
verification procedure which is based on
types and type checking so it's not a
new thing in functional programming
community to use types to specify
computations for example if you are a
haskell programmer you probably familiar
with this tool called Google and what
Google does I can show you right now
it's a website where you can search for
functions from standard Haskell library
using their type for example if I'm
thinking well what was this function
called that takes an integer and the
value of any type a and produces a list
of a's of the length of the first
argument and i don't i don't remember
its name and then Google will tell me oh
the first result the hookah returns is
replicate and this is exactly where I
wanted I wanted some number of copies of
the value but of course the high school
type system is good enough to do a
search in the standard library but it's
not rich enough to describe a go for for
synthesis so we need a more expressive
type system for that and we decided to
use refinement types so the refinement
types this term is used a lot in
different contexts so I'm going to
quickly explain what we mean in this
work by refinement types if you're
familiar with work a Ferengi Jolla and
his group on liquid types then this is
exactly what we mean and if you're not
familiar I will just go through the
basics of this kind of refinement types
so we call them general decidable
refinement types and basically what it
is is a conventional type think ml or a
high school type that is decorated with
a predicate that restricts the range of
values that this service type has so for
example this one here describes the type
of natural numbers and the difference
between refinement types and why we call
them decipher if
ayman types the difference between those
and general dependent types is that
those predicates are drawn from some
kind of decidable logic efficiently
decidable by SMT solvers and this is an
important fact because this makes our
verification decidable so this judgment
over here would say that a variable end
has this type of natural numbers and
this particular type we're going to
abbreviate as nap later okay so where we
can express with those um yes okay about
you differently design appetite by the
statistics that is do that kind of a
commutation class so it's basically the
whole approach is parameterized with
what kind of logic you want to use in
the environments not sort of specified
but the logical people usually use it
would be linear integer arithmetic with
interpret functions and erase so this is
kind of a a class that is we are well
explored and and can express a lot of
stuff but you can plug in other logics
there as long as you can decide them
efficient thank you okay what can we
express with those so not only we can
talk about base types restrict base
types like integers but we can also give
refinement types of functions for
example this type over here describes
function max maximum of two integers as
a function that takes to unrestricted
integers X and Y and returns a value
that is greater or equal than both of
those of its arguments and as you can
see in function types we can give names
to the arguments so that we can use them
later and the type of the result and
this is what makes it depend on function
types and this is what lets us express
pre and post conditions and finally we
also have algebraic data types which are
polymorphic for example this one here
says that excess is a list and each
element of this list is a natural number
so this not again is just an
abbreviation for the
so the definition of list is just what
you would expect in a with a regular
type system but as you can see we could
instantiate this type parameter here
with a refined type to express a
non-trivial property that every element
of the list is greater equal to 0 and
not only we can express such Universal
properties of data structures we can
also talk for example about the length
of the list using the construct called
measure so you can think of a this
measure length as just a function
defined inductively on the list but it's
really syntactically restricted in such
a way that you just have a one
definition of length for every list
constructor and in terms of verification
what happens with this definition is
basically there is a syntactic
transformation that takes those
definitions of length and just append
them as refinements to each to the types
to the type of each constructor of list
and at this point you can just treat
this length isn't completely than
uninterpreted function so you can forget
about this definition and everything we
know about length is just that the
length of nail is 0 and the length of
cons can be calculated from the
arguments in this way okay and using
this and using measures we can express
recursively defined properties about
algebraic data types and the cool thing
about this is that the type system
actually works for us to instantiate and
generalize those properties completely
automatically so whenever we are going
to construct a list a type system just
by using the type of that for example
the cons constructor will generalize the
this property that all they own in the
natural numbers and whenever we are
matching a list so when I we're
deconstructing a list we will get those
properties back and blue automatically
without needing any kind of heuristics
to instantiate quantifiers and this is
why refinement types have
been so successful in doing in in
verification of non-trivial properties
with very little to no manual input and
doing these things completely
automatically so they've been used in
verification how can we use them in
synthesis well let's try to use a
refinement type to specify a synthesis
go so remember this function replicate
that I showed you in Google how could we
specify such function well I want to say
give me a function that takes a natural
number and the value of any type beta
and returns a list of values that are
equal to this second argument and of the
lengths that is equal to its first
argument okay this is basically a
complete specification of replicate and
in order for synthesis to work we also
have to provide some kind of components
that can be used as commutation
primitives so in this case we provided
with obviously the list data type and
also we would give our census procedure
increment and decrement functions over
integers for which we also have to
provide their refinement types but by
the way we don't even need their
implementation and the goal is now to
find a function that has this type and
uses those component and can't is
allowed to use those components okay so
let me show you how this works with our
prototype implementation real quick oh
no first first something that I forgot
to mention is so look at this type of
the list elements here surprisingly if
we replace it with just beta this
specification is as good as a previous
one and this could be surprising at
first but really if you think about it
since this type parameter can be
instantiated with any refined type what
this specification is really saying is
that whatever property ax happens to
have every element of the list must have
the same property including the property
of being equal to a particular
you so this actually shows us how
expressive polymorphic refinement types
really are because they let us abstract
or quantify over refinements so let me
show what our prototype implementation
would do given this as input so I have
prepared here and for replicate so it
takes us split second to generate this
implementation here which basically says
it will be a recursive function that the
algorithm decided to name f2 we take
send and the why is arguments and it
synthesized this branching here if if
the length parameter is less than equal
to 0 which basically means 0 because
it's type is natural then we'll return
an ill and otherwise it will cons a why
to a recursive call of the same function
f2 on the same why argument but the for
the first argument is decremented which
is what you will expect but note that
the algorithm was able to infer this
condition here which is very nice but
I'll tell you later how it's done ok let
me show you another example that is a
little bit more involved so this is
insertion into sorted list so we want to
synthesize a function that takes an ex
of any type beta and it takes an
increasing list of a list of beaters and
it produces an increasing list whose a
set of elements is the union of a set of
elements of XS and the singleton set X
what is an increasing list how can we
define a sorted list using our
refinement types well it's actually very
easy we say that an increasing list of
office is either an empty list or to
make an increasing list of office we
have to con some alpha to an increasing
list of elements that are greater equal
to the head right but that's very easy
so
here we assume that the comparisons are
actually generic here so they're defined
on any Alpha whatsoever all right and on
top of that we can just as we added the
length measure in the previous example
we can add this a different measure that
returns the set of elements of the list
but exactly in the same way and with
this definition at hand we can define
this insert function okay so for this
example it would take our tool slightly
longer because it's a more complex
example but so in just over one second
we can synthesize this implementation
which is again recursive function with
two arguments and would match on list
and say if it's if the list is empty
we'll just return a singleton list legs
and then otherwise it will compare X to
the head of the list and then if X is
less equal to the head of the list it
would just context to that whole list
again and otherwise it will constitute
that Y to the recursive call off of the
same insert function which is again the
implementation that you would expect it
doesn't seem like much at first but
actually to verify such an
implementation you need some non-trivial
reasoning if you think about it because
to verify this branch that actually this
convoy to insert ex-wives produces a
sorted list what you need to know is
that all the elements of the list that
is returned by the record by the
recursive call a greater equal than y
and basically this means that you need
to know that you if you insert something
that is greater equal to y into a list
of things that are greater equal to y
you get a list where everything is
greater equal to y which basically means
that you have to strengthen the
specification of the insert function
itself for it to provide you with this
property which requires some kind of
specification discovery
and in the language of refinement types
what it really means is that we have to
figure out a refined instantiation for
this better type here to say that in
this particular call to insert its type
won't be just an increase of beat us but
it will be an increase of something oh
there should be bitter sorry not int of
something that is greater equal to Y so
how do we do this how do we
automatically discover predicates like
this and in general how do we do this
type checking of requirement types
completely automatically which is what
we need for synthesis yeah by the way
since this non-trivial reasoning is both
this is actually a first example of
automatically generated implementation
of insertion to linked list that is full
that is fully where unbounded Lee
verifies because Leon can generate this
as well but it cannot verify it because
it cannot discover its kind of
properties all right so so how do we do
this type checking well the thing is
there is a technique that can infer this
kind of refinements completely
automatically and do refinement type
checking completely automatically and
this is liquid type inference from again
orangey jealous group at UCSD and this
technique relies on a combination of
conventional Hindley millinery style
type inference to infer the shapes of
the refinement types which means the
conventional type that underlies that
the refinement type and then it uses
prague abstraction to infer the
refinements well so can we just use this
as the verification for a synthesis
procedure and just be done with it well
there would be nice but unfortunately
this method is fundamentally a whole
program analysis and how do I mean that
well let's see how a liquid type
inference we do on this example where we
want to check this expression if X is
less than zero then a singleton list
with minus 1 otherwise singles in this
with one against this type list
naturals obviously it does not have this
type there is a type error here but how
would liquid type inference do here well
the thing is the liquid type inference
is meant for really type inference in
the context where no user type
annotations are provided so it doesn't
even assume that this top level that the
type of the top level expression is
known it just tries to discover type of
every expression from the types of its
sub expressions completely from scratch
so let's say doesn't know this type so
let's look at the all the sub
expressions of this expression and so we
know what types of 1 and minus 1 are and
we know that those lists expressions are
all of that list but we don't know the
instantiation for the generic parameter
so what liquid type inference would do
it will first invoke Hindley Milner
which would infer the shape of all of
its types and so it will know that it's
a list of integers but we don't know
what the refinements are yet so then
they would insert predicate unknowns in
all the places in the in those import
shapes where the refinement is missing
and then do we use a predicate
abstraction to reconstruct those
refinements bottom up and completely
bottom-up style so for example and they
would come and they would construct the
strongest refinement that is allowed by
the sub expressions for example here
since the nail is not restricted but
anything is strongest type is a list of
false then basically to discover the
type of discounts we would have to take
some kind of list upper bound of of
those two and we would get the list of
minus ones and here's in the same way
we'll get a list of ones and then at the
top level will take sort of least upper
bound of those and let's say in our
language we can only express it this
type as list of true and at this point
we see that list of true is not a
subtype of list of gnats and we discover
that there's a type error but you can
see that we had
to analyze the whole program before we
could we could discover this type error
and there's really two problems here
first problem is that the type
information is not propagated top down
because we don't even assume that there
is any type information at the top but
the second problem is that there really
those two stages there is this handler
milner shape inference which is known to
be a global approach so it generates all
the unification constraints and then
solves all of them for the whole program
and only after that first phase is done
for the whole program we can start in
frontier refinements and this kind of
whole program type inference might work
completely fine in the setting of
verification but it's really a terrible
idea for synthesis and let me give you a
little analogy to show you why so let's
say you have a combination lock and you
a verification is like when you're
pretty sure that you know the
combination and you just want to double
check that you're not wrong so in that
situation you are not so you're not
really hindered by the fact that the
lock will only tell you if the
combination is correct one once you get
all the numbers right which is like a
global global verification but synthesis
is like lock picking so if you really
don't know the combination and you want
to determine the combination then it
would be really great if that lock could
tell you for every digit if the digit is
correct or not you would be able to pick
that lock much faster right so we need
this kind of magic lock technology for
modular verification technology to
enable scalable synthesis ok how can we
modify this global bottom-up liquid type
inference to make it modular and enable
scalable senses well first of all we
have to make use of the fact that we
actually have this top-level type
available and try to propagate this type
information top down so in this case
let's say we know this must be a list of
nuts we have those sub expressions
so we can easily propagate this
information down to both branches of the
if and basically say well if the whole
thing is the list of gnats then but then
branch it must be a list of nuts after
the under the assumption of of the if
condition and the other one is also must
be a list of nuts under the assumption
of the negation of the if condition
something like this unfortunately we
cannot propagate type information all
the way talk down to the leaves because
it's not possible to propagate it
through function applications a type of
a function application doesn't uniquely
determine the type of the function and
the type of the argument so at this
point we sort of have to switch
direction and go put them up for a while
until those directions meet and this is
really the idea behind bi-directional
type checking which was discovered by
Pearson Turner into that in the year
2000 and we will be using this idea here
so let's say we got down here and then
we start top down and then we start
bottom up but then at this point those
two directions meet and this is where we
can do our type check and this will be
much more local so at this point we do
some shape inference and then we
discover that there is a type error
without even looking before when looking
at the second branch of the if so we
really made this this type checking much
more local and much more modular okay so
basically this is our proposal for a
synthesis from refinement types it's
just like before except instead of this
Co program liquid type inference we use
this new technique which we call model
or refinement type reconstruction and it
combines the ideas from bi-directional
type checking and I just told you it
still uses the same kind of techniques
motivated by predicate obstruction to
discover the predicate and one technical
challenge that really has to address is
well now we cannot do a phase of shape
inference for the whole program before
we start discovering requirements so we
needed to find a way to
to leave shape shape inference and
refinement inference and this is turned
out to be possible and a one other thing
that we do differently from liquid type
inference is since we're doing things
mostly top down we are actually
inferring the weakest types instead the
weakest refinements instead of the
strongest requirements and doing that
allows us to use exactly the same
mechanism that we use for inferring
types to infer branch conditions in the
conditionals which is what you saw in
the first example because if you think
about it we have a mechanism for
predicate discovery why not use it for
it for branch conditions okay so at this
point just putting it all together the
whole enumeration and verification parts
of our approach I can just show you the
first example again the replicate
example but really step by step how the
whole search works so on this slide what
we have is so the current go type this
is what we want to since s and the
current available components this is the
environment and we can use in the
current program that we that is the
output of census so the first thing that
our two will do is look at this gold
type and see well it's a function type
so we know that the out we will be a
function so it's really easy to deal
with that so basically what we do is
we'll say well to synthesize a function
is really just synthesizing its body
given its arguments so it will add the
arguments of the function like N and X
into into the environment into the set
of available components and it will also
give this function a name because it
wants to make this function recursive in
its first argument but not the second
because this first argument is a type
nap which has a predefined well-founded
order so the tool is allowed to recurse
on this argument but not on the second
one which which has a type which
we don't know anything about and to
enable recursion what the tool does is
basically adds as another component to
the environment and the same function
which would basically be used as a
recursive call but note that it's type
is slightly different so instead of the
first argument just being of type not
it's actually of type something between
0 and strictly less than n which so
basically our tool weakens the type of
this function in such a way that it can
only be called an arguments that are
strictly smaller than than the one that
we are originally called with which will
guarantee that all the recursive calls
terminate and by the way if you're used
if you're in verification you're
probably used to that you can ignore
termination arguments for a while and
say you're only verifying partial
correctness but instances you really
cannot ignore termination at all because
non terminating programs are always
shorter than terminating ones so you
will always get garbage if you don't
take care that the probes are
terminating okay at this point our
target type is this we just need a list
of betters that are of length n so the
tool will first try a bunch of simpler
expressions that are just functional
applications but it will not succeed so
at some point it will decide to
introduce a conditional but the
condition is still unknown and
represented by this predicate and known
you one and then at this point the tool
will focus on synthesizing the first
branch so for the first branch it will
start enumerated function applications
from simplest to more complex and the
simplest expression that can go into
this first branch and has the right type
shape so it's a list of bidders would be
an ill and empty list so it tries this
value nil for the first branch and then
it would try to use very good
obstruction to infer the weakest
condition under which the
this would be an appropriate
implementation of the function so at
this point so as you can see you won
this uses an assumption here as a sort
of a path condition and this point it
will infer that under this condition
that n is less than equals 0 this is
actually the type of meal is actually a
subtype of what we want they can equal 0
oh that's so this is a very good
question I kind of avoided the question
of how we actually infer those
predicates but so what liquid types do
and what we do as well is we are given a
set of atomic predicates or rather
atomic predicates templates and all the
predicates that we infer are just
conjunctions of those atomic predicate
so here I assume that the tank
predicates that way given our variable
less than equals 0 variable greater
equal 0 or variable non equal 0 from
those we can make various kind of
inequalities and ND qualities but we
always infer the weakest one that fits
so here since this one is weaker than
equality this is what we'll get but if
you add a quality as as an atomic
predicate then you might as well get a
quality that that's just a matter of
luck then because those two are
incomparable they will be incomparable
syntactically but semantically actually
no we do we do semantic checks on them
as well to cut the search space so you
will so you will actually get this one
anyway all right so now we are done with
the first branch and then of course now
the task is to synthesize the second
branch under the assumption of the
negated of the negative condition so now
we add this knot and less than equal
zero to the assumptions and now we have
to synthesize again an expression that
has this type so again the tool will try
a bunch of will start trying function
applications starting from the simplest
ones so
no cannot be made to satisfy this this
restriction on the lengths in this in
this case because we know that n is
greater equal to zero and the length of
nil is zero and we know that from this
requirement so Neil doesn't fit so we
try something bit more complex maybe
there will be cons and for cons we have
to synthesize the arguments now and
again for each argument we will start
trying simpler expressions first so at
some point we will arrive to this cons
of X and then as the second argument
accounts we decide to use this recursive
call and at this point we have to
synthesize now the arguments for this
call so what I wanted to draw your
attention to is that when we are
synthesizing the first argument of F we
are actually really lucky here because
the precondition on this first argument
is really strong so this precondition
will be used to filter the candidates
for this first argument very locally so
for example if you if we will be trying
and here then even before since s in the
second argument and going all the way up
to the type of the whole else branch we
will know that n is not a suitable
candidate here because we know that a
suitable argument has to be less than n
so n is not suitable ink and it's
definitely not suitable and this point
we know that we have to choose that end
locally okay at some point this will
give us the desired result and we don't
have any holes in our program anymore
and this is done so as you can see the
enumeration part of our senses procedure
is at this point really basic so it
really does explicit enumeration from
simpler expressions to more complex but
so what we really put some thought into
is that verification part so we try to
make make it as modular and as automatic
as possible and already this combination
enables lets us synthesize some
interesting programs but we're hoping
that if we also make the enumeration
parts motor at some point then we will
get even very results
so this prototype back in the
specification if try to understand what
is in here it says it makes and I bring
him yeah so we renamed the arguments
here big so because N and X already
taken with just picked fresh names for
the arguments because we don't want to
repeat them so aliens of given gone
straight so basically m was initially
the argument that was given a hue so and
this this end would be added to the
environment we just take the end from
from that type and add it to the
environment and that n is also used in
the type of system automatically
employed that we should be less than
game oil well this is just because this
is the name of the first argument of a
function in in the outermost call so we
are we are inferring the body of F cold
with N and we know that if you want to
make recursive call from there then the
first argument has to be less than this
end yeah I agree it's not very clear
here because the assumption that every
time function will make because it won't
on only the first argument and it will
be greased somehow well there's a so
basically this method is also parametric
with respect to what particular order
you choose to make your recursive call
term recursive calls terminate so what
our tool uses at the moment is it
chooses the first argument that it can
recurse on and just uses that one but it
will be also possible to it actually has
a switch to make for example the
lexicographic tuple of all the crucible
arguments for example if i want to
censor a program that eats two lists so
dragon takes to listen arguments so what
kind of thing would come up there
so for data types what we do at the
moment is basically you're allowed to
specify the measure that the measure
that will be used to compare those lists
so for example if you define length of
lists and length maps list of integers
and integers already have an order in
our predefined order in our system you
can just say compare list by length or
in other instance you can say compare
them by elements this is one of the
choices we could also make of course
structural recursion they'll also be
possible but we thought this would be
more flexible if we do it this way more
questions okay so yeah that tool that I
showed you is called synced from
synthesis and liquid and it's available
on bitbucket it as I said it's still
working progress hasn't been really
released yet but hopefully it'll be soon
and you're welcome to try it and it's my
last slide I present my kind of vision
for where this project might be going so
I showed you who go in the beginning but
wouldn't it be cool if we had something
like Google but that uses refinement
types and can do both more precise
search in the documentation but also if
the function that you're looking for
does not exist it could synthesize
function from using all of those
functions from the base library as
components so I call this google plus so
for example we give Google Plus
something like well I want to function
that takes some value ax X and the list
of X's and produces an integer value
that is equal to the number of
occurrences of X and X is so here I
basically just using another measure on
lists which returns a bag of like a
multi set of elements that I called bag
and then I say well it's that it's the
multiplicity of X in that multi set that
I want because there is no a primitive
function in in Haskell that returns
the number of occurrences of an element
in list this query would require to do
some little synthesis and then maybe
what it return would be something like
this yeah with that yea maybe then he
cares have friends on google her circles
um yeah questions so I'm trying to think
how would a technique work with this
specification was in the form of example
so what you mentioned that one problem
with examples is that you might require
too many of those but one way to avoid
this aspect is to say that you are
looking for a small program or the
smallest program that you can find which
matches the examples and then I think no
less number of examples would do the
trick so for instance in your case of
copying the number of in a specific
element a certain number of times I just
even give you one example with large
value of n like five or six my feeling
is that a program that you synthesize is
in fact the shortest program that will
be consistent with that example right so
but for example the one of the tools
that i mentioned myth is using exactly
this heuristic that they are looking for
a shortest program program but as i said
they still need 13 examples to specify
drop and they their paper even says that
it was not trivial sometimes to come up
with those examples and its really an
interactive process where you think you
have specified everything but then to
always comes up with some corner cases
and and one of the reasons why they need
so many examples is they have this
property of trace completeness so when
they are synthesizing a recursive
function because you don't know you
don't have any specification for the
function so whenever you would have the
synthesized implementation is using your
crush of coal you need another example
that would specify this recursive call
so basically if you're specifying length
lists you specify something on a list of
length for you have to specify also for
length 32 and 10 and this is how I think
those sets of examples get
get larger I think that is how it's
limited by the technique there you think
because i can assume that if i want to
specify a drop of function i will give a
long list and i'll specify one crop to
celebrate and output that won't be
present and simplest function would be
to draw or if because i remember for
scheduling me doing by examples doing
listed eat we only need it later okay
yeah maybe that's the limitation of
their technique but so i think what
would be really cool is to combine those
right because and i think it's even not
that difficult because examples are
refinement types in some way right so
bringing examples into this framework
will be great because of course the
disadvantage of refinement types is that
you cannot express anything you can do
express everything you want because it's
still decidable logic and the
combination of those things and examples
there'll be a really great any because
even your deco will probably have this
issue if the overall is specification
for the function is not strong enough or
inductive enough to prove its
correctness and you need to actually
find it you're you need to be able to
strengthen it as well yeah this cannot
happen that of course your specification
is not strong enough but we think that
it's even an advantage system in some
cases to be able to provide a partial
specification because one other problem
with examples is that what if is
basically you have to know what the
output is of the program and not might
not be trivial or all the time for
example you want to specify insertion to
red black tree you basically have to
know how red black trees work to be able
to specify the output whereas with
specifications it's much easier to say
here's invariant of relative tree and
here's what I want in terms of the set
of elements and then you go figure it
out so this is really yeah it's right
over here
so they're cool site like a CL 2 and
Isabelle that that construct proofs of
inductive sorts of things you mentioned
that pictures insertion into sort of
this but first that actually verifies it
as well what we've offered something
like ACL to do with when you can make
any constructors terms there are
executable program this is very good
questions probably I mean I don't know
really an answer these questions it's
probably possible to use yet of course
there's a lot of research in the area of
proof senses that is kind of stuff that
was kind of separate from program
synthesis even though we know
theoretically that it's the same thing
so I think there's much more potential
and bringing that work that like old
school work and provinces more into
perham senses and seeing what those
things can do yeah I was really when I
said this is the first verified
implementation synthesized
implementation I was really comparing
with those sub really program synthesis
tools that yeah that I was considering
but it would be a good comparison to see
they look at this other tool briefly to
endure like the expressive power and
I'll eat that for a year or two bites
like that we find the types that so yeah
I mean as I said theoretically of course
there are limitations but so I I think
so what I learned from tarantula and his
his group is that people are finding
more and more creative ways of arranging
those types inside air to express
properties that you wouldn't think
before I would be expressible and of
course things so the type system that we
use here only so only have those
features that i showed but their
research on on very own type checking
actually went further than that and they
have more features that we hope we can
later so they have things like abstract
refinements for example where you can
parmiter eyes your type not just by a
type as in polymorphic types but also by
predicate so you can easily specify
things like let's say filter using those
abstract brigades oh yeah and so I think
those this kind of refinement types are
really just this surprising combination
of their surprisingly expressible and
still decidable and think and I thought
it was really worth exploring person
says but of course there's the reticle
and limitations this one great thing for
refinement types is that you're able to
locally prove the search space is this
in comparison if these were not there
what what would the time be you so as
you just hang out n to it type you don't
have these local times for each variable
right so much I mean I cannot say how I
can really compare with a different kind
of specification but what we did in the
paper in our preliminary experiments is
basically we did the synthesis in this
way with local checking and then we
disabled local checking and just doing
all the checking on the top level and so
we saw that well what you would expect
for very small examples there's no
difference but with the bigger examples
there was a big difference so maybe I
can even bring it up oh yeah right here
so for example examples like append
deletion from a list and both of the
functions on sorted list that we try to
synthesize timed out I think timeout was
like two or three minutes so it could
not synthesize it with the whole program
analysis but with modular analysis it
would take legs on some kind of seconds
and yeah so it's it's where you would
expect this kind of whole program
analysis doesn't really scale as we as
we go to more complex problems
and so um thank you all for coming in
for your questions so I yes gonna be
here all week you like to chat with her
11 at least so I thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>