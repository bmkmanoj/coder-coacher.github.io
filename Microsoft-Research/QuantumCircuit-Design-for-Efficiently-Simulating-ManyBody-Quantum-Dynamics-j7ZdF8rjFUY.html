<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Quantum-Circuit Design for Efficiently Simulating Many-Body Quantum Dynamics | Coder Coacher - Coaching Coders</title><meta content="Quantum-Circuit Design for Efficiently Simulating Many-Body Quantum Dynamics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Quantum-Circuit Design for Efficiently Simulating Many-Body Quantum Dynamics</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j7ZdF8rjFUY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so today the quantum architecture
and computation group is welcoming
Nathan we've here to talk with us he is
a postdoc at the Institute for quantum
computation ICC at University of
Waterloo and today he's gonna be talking
about designing quantum circuits for if
efficiently simulating many body
dynamics so Nathan I'll turn it over to
you great thank you very much Reza
so thanks a lot for coming well you've
heard what I'm gonna be talking about
this is work that I've done
collaboratively with SATA crazy as well
as Barry Sanders and the outline of the
talk is laid out as follows so first I'm
going to provide an introduction to
quantum simulation algorithms and then
I'm going to discuss by providing some
simple examples how to simulate a few
very primitive many body hamiltonians
then I'll try I'll use the intuition
that we've developed from this
discussion in order to show how to
construct a circuit that automates all
arts Aria classical algorithm that
automates all of this reasoning and then
finally I'll discuss how these results
can be optimized by grouping terms in
appropriate ways for parallel execution
so in general quantum dynamics is known
to be very hard to solve or at least is
believed to be very hard with the best
known algorithms that we've got I did a
did a search and the most sophisticated
quantum simulation that I was able to
find was performed by this supercomputer
and it was able to simulate the dynamic
quantum dynamics on a 42 qubit system at
the cost of roughly a hundred million
dollars now this is something to me that
was pretty impressive because I've I've
done these things by hand and general
dynamics I have a really hard time
pushing upwards of 20 cubits 42 cubits
is a substantial feat especially because
of the fact that it gets much harder as
you try to make the simulator work for
bigger and bigger systems so in general
the number of the computational
resources needed in order to perform the
simulation will scale exponentially with
the dimension of the system so forth
about 42 cubits well it took only one of
those supercomputer clusters but using
optimistic scaling we need to 443 and so
on and so forth this gets absurd because
if we want to even simulate a modest
scale quantum system say 115 qubits
going through the math and computing the
weight of all of the processors in the
supercomputer required to simulate the a
hundred and fifteen cubits system in the
same amount of time we'd require at
least an amount of processor mass that's
equivalent to the mass of the earth so
something's wrong here especially if we
believe that nature contains 115 qubit
systems in it ie systems of say 115
interacting electrons so how is nature
getting away and answering these
questions without requiring a
supercomputer the size of the planet and
the answer I believe as well as many in
my field believe is their quantum
information so the basic idea is we'd
like to get around this problem by
exploiting quantum mechanics in order to
simulate quantum mechanics and the
intuition works as follows let's imagine
that we've got some physical system that
we would like to simulate and
specifically we have a quantum state
that's in that physical system and it
follows some natural evolution described
by a system Hamiltonian to a final state
over here we'd like to simulate that by
taking this initial quantum state and
constructing a state that's logically
equivalent to it in a quantum computer
then instead of this nice smooth
evolution that we get by the Hamiltonian
we approximate the evolution by a
sequence of quantum gates that are
permitted in for the particular quantum
computer the result of applying those
quantum gates will be a quantum state
that if everything is done correctly
should be logically equivalent within
some air tolerance to the actual state
that we would have observed in the
physical system so that's the whole idea
behind this simulate of this method of
using quantum computers to simulate
quantum dynamics there's some really big
strengths and there's some really big
limitations so I'll start with a bad
news first
the bad news is that unfortunately you
actually get zero information out of
this
system by doing one simulation I mean
zero information because of the fact
you've got a quantum state and you
haven't measured it
you don't haven't extracted any
information from it if you want to learn
something from it then you can perform a
measurement but the problem is for
certain things you may require a large
number of measurements like if you want
to learn the quantum state in general
you'd have to do some process like
quantum state tomography and that's
exponentially expensive so you're not
going to get you're not going to get an
efficient estimate of the quantum state
as a result of one of these simulation
algorithms also in order to guarantee
that we can do emulate the evolution
with a small number of gate operations
we need to make several promises about
the form of the system Hamiltonian we're
simulating this won't be efficient for
all possible systems but if these
caveats are made then we can find
expectation values and certain
eigenvalues with by repeating the
simulation nepali law polynomial number
of times and processing the data so the
other big strength about this is unlike
algorithms such as Shor's algorithm we
we don't necessarily need very many
qubits in order to outperform what the
best classical computers can do as the
discussion previously about the mass of
the earth was hopefully alluding towards
so I'd like to now provide some
background about what how the field is
progressed originally this idea of
simulation was proposed by Fineman when
he was noting that he wasn't able to
write very efficient simulation code for
quantum systems but quantum systems seem
to be doing it naturally and it's led
him to propose the idea of a quantum
computer but since it took a quite some
time since then for somebody to actually
formally talk about how to how to do
these simulations in the first case was
done by Seth Lloyd in 99 where he showed
how to simulate quantum dynamics in the
restricted case that the Hamiltonian is
a tensor product of sum of tensor
products of low dimensional systems so
that work was subsequently improved by
our enough
and Andrew Child's and others in 2007
this ayat was improved further by
Dominic berry Graeme oak is Richard
cleave and Barry Sanders who came up
with substantial optimizations to the
algorithm that caused it to run in near
linear time as opposed to like quadratic
time or T to the three halves it
recently myself and a few other
co-authors have generalized these ideas
to rigorously show how to simulate time
dependent Hamiltonian as well
so that's sort of the many of the
current results involved in quantum
simulation algorithms how are the
complexity of these algorithms typically
assessed and the way they're assessed is
in a very high-level perspective often
what we what we're interested in is a
number of times a Hamiltonian blackbox
is queried so this black box works as
follows you provide a quantum state that
encodes a row and a column and it
outputs in another quantum state and
then coding of the matrix element of the
Hamiltonian hxy also there's an
additional black box that's often
considered that will tell you the
locations of non zero matrix elements
but those are the two things that are
considered to be the resources that are
required for these algorithms and the
cost is given by the number of accesses
you have to make to this black box and
the best of those algorithms that I
described previously has the following
scaling for the number of calls that are
needed so it scales slightly worse than
the fourth power of the sparseness of
the over our overall Hamiltonian nearly
linearly with the norm of the
Hamiltonian and the evolution time and
sub polynomial e with the error
tolerance of each one a matrix is a D
sparse if the each row and column
contains at most D nonzero matrix
elements okay
so that is that is basically the results
now there's a bit of an issue that we
would like unfortunately to the best of
my knowledge very few ion traps have a
black box so the question is how do we
go about taking this from this
theoretical description that uses this
black box to something more fundamental
that we could consider actually
implementing and in general
unfortunately it's very difficult for an
arbitrary black box to consider doing a
circuit decomposition of that however
for some particular cases specifically
local Hamiltonians results are known the
bet of the recently Weimer Muller
buckler and Leslie's an auskey found a
trick that actually can be used in order
to simulate many body systems without
requiring a black box or doing any of
this sort of nonsense so the main
drawback behind their techniques
unfortunately they went through the
analysis and they were correct about
their trick but you have to have a lot
of knowledge and a lot of intuition to
see how to apply that trick to be able
to simulate a general Hamiltonian it's
not something you could
straightforwardly tell a computer to do
and it will output the correct result
also they don't use optimizations
proposed by previous simulation schemes
such as the use of high order
integrators or other such tricks on what
we what we do is we rectify these two
problems we make it automatic so that so
that a computer can directly output a
quantum simulation circuit and also we
include the optimizations that are used
in the best simulation algorithms today
so that's basically what we do now I'm
just to give an outline for the rest of
the talk what I'm first going to do is
I'm going to discuss you know simulation
of many body Hamiltonians and we're
going I'm going to define what I mean by
a local Hamiltonian then I'm going to
show how to simulate some extremely
simple hamiltonians and lead in
eventually to the classical circuit
building algorithm so what do I mean by
a many-body system well this is probably
the most trivial slide of all of them
what I mean is I mean a system that has
many inter
acting bodies all right so that's
somewhat tautological but there's many
examples in physics that have these
properties for example isaac models that
have interacting magnetic dipoles on
some sort of a lattice or qubits in an
optical lattice they're all described by
these many body systems out of them
we're interested in a particular case
we're interested in k local Hamiltonians
and these are hamiltonians that are the
sum of terms that are tensor products of
at most k non-identity Pauli operators
so for example the following opera
operator here H is two local meaning
that's a sum of terms that have our most
tensor products of two distinct Pauli
operators so that's what I mean and many
important hamiltonians end up falling
into this category like the Heisenberg
model Isaac model tort code Hamiltonian
um yeah check should be tensor product
identity on both of those cases yes
in this particular case should be tensor
product identity on that side and tensor
product identity on this side so there's
many cases of physically relevant
Hamiltonians that fall into this class
and people would like to see me like
them but there aren't any efficient
methods yet so out of k local
hamiltonians there's actually two
distinct classes that we I'm going to
talk about briefly in this presentation
and cases are physically at local and
just strictly k local and you notice
there's actually a pretty big difference
a difference is if you put it make a
graph of each qubit in the system and
draw an edge between each qubit that
directly interacts with each other many
physical models only have nearest
neighbor interactions so what that means
is that means if you take a look at the
graph each vertex has constant degree
for visit for these physically local
hamiltonians however generic k local
Hamiltonians allow you to have a
complete graph of interactions so the as
you might imagine this case is
substantially simpler than the generic k
local case and because of the fact that
many physical systems fall into this
category it's worth us discussing the
state of difference between the
performance of our algorithms in this
case and in that case so how do we go
about simulating these things so I'm
going to present the easiest possible
Hamiltonian you can consider simulating
of this form it's a tensor product of K
poly zet operators with a weighting term
in front of it so the way you simulate
the reason why this is the simplest is
because of the fact that it's already
diagonal in the computational basis so
what that means specifically is that if
you act with this on a particular
quantum bit string it will just give you
that quantum bit string back so the seat
to see why this is so easy let's
consider the action of the time
evolution operator which is defined to
be e to the negative I HT
on an arbitrary initial state by
decomposing s is the sum of different
computational basis states we end up
getting that by definition so since the
is that PolyJet operator just acts as a
phase flip depending on whether the
corresponding bit is 0 or 1 we can
actually take a look at what this
operator does it will simply provide a
phase flip depending on what the values
of the firt up in this case the first
qubit is the second qubit and all the
way up to the cave keep it here I don't
mean direct some I mean exclusive or
between all of these things so what that
means is that means that we can simulate
the evolution by just simply doing a Zed
rotation on a qubit that encodes the
parity so if we can keep store the
parity of all of these K bits here and
perform a Zed rotation on it
then we'll enact exactly this phase
rotation that we end up seeing here and
that's the idea behind the simulation
circuit so the corresponding circuit
that ends up doing this is as follows
it takes an initial qubit string
performs the controlled-not gate on this
last qubit to compute the parity then
does the Zed rotation on that parity
qubit and undoes the computation of the
parity again we need to undo the
computation of the parity to make sure
that we don't require very large number
of N syllabus to do this computation so
now that that's done let's talk about a
slightly more complicated example let's
say that instead of Z tensor K I switch
the first two operators to Polly X
operators instead of Zed this makes it
harder because Pauli X operators aren't
diagonal so we can't just do a rotation
in the eigenbasis instead what we need
to do is we need to transform to the
eigen basis of this new Hamiltonian and
we do that by performing the Hadamard
transform on the cave bit in the first
bit then we can treat that as a Zed
rotation in its eigenbasis
and then we return to the computational
basis via this transformation
okay so if we do that then the resulting
circuit looks identical except now
there's Hadamard transform different
clear on different qubits yeah so but
yes you're right if they didn't commute
then yeah so that's the resulting
circuit and you can imagine why is it e
much exactly the same the only
difference is if we replace those X's
with poly y operators now we have to do
a slightly more complicated
diagonalizing operation and the one that
works is pi by 8 gate to vi Hadamard and
then its inverse on the other side and
you might be wondering why the 6 and the
2 well if you do PI by 8 eight times
then you end up getting the identity so
that's why it's 6 and 2 in order to make
sure that we can represent these
transformations as just Hadamard PI by 8
and C naught which are which are a
universal beta set of gates so that is
the more complicated example so with
this you can see how we can go about
simulating any Hamiltonian of that
particular form that's a tensor product
of x y&amp;amp;z gates what we do is we form
single qubit operations to diagonalize
Hamiltonian we simulate the Hamiltonian
in its eigen basis and then we return to
the computational basis by undoing that
transformation in the first step and
that's it
so this isn't quite fully general
unfortunately because of the fact that
you can have a Hamiltonian it's a sum of
non commuting terms so if sum of X X and
Zed Zed because these two operators
aren't diagonal on the same basis we
can't apply directly the same trick so
instead what we do is we do a
time-honored approach we use a trotter
formula trotter formula says ok well if
we ignore the fact that these two
operators don't commute then we can
write the exponential of the sum as a
product of Exponential's and the air in
that ends up scaling quadratically with
the evolution time so it's only very
accurate for extremely small steps but
fortunately we can always break a long
simulation up into a sequence of short
steps and if we do that then we end up
with ski
if we break it into our steps then we
end up with scaling it goes like T
squared over R so we can always make R
as big as we want in order to make this
air arbitrarily small the big catch is
well unfortunately R has to be pretty
darn big in order to get this to work
out so you'd like to get a better trade
off in some cases so in that in that
case the answer is to use higher order
product formulas that actually respect
commutator 's and simulate their action
so an example of a slightly higher order
formula is this Strang splitting here
which is just like the ordinary trader
formula but it's symmetrized and the
reason why I discuss this is because
this formula is actually the basis
behind a much more powerful
approximation building technique which
is known as the Trotter well Suzuki
formulas and the idea behind it is well
we start out with this initial
approximation s1 and then we build a
higher order approximation by doing this
this following step for some value of s1
that you can compute what you do is you
take two x steps forward using this
integrator s1 between 0 and T you get
almost to the end and then you do
something bizarre take a step back and
then you do another two time steps
forward although this looks
counterintuitive and then something like
you wouldn't want to do you'd imagine
you'd always want to go straight towards
the end it turns out that by doing these
evolutions in this kind of convoluted
order what you can do is you can cancel
out many of those error terms so rather
than having an approximation which is
order T cubed this will end up giving
you an approximation as order T to the
fifth
it's like playing this
I can
sure your problems sorry there's a lot
of material here hi apologies okay so
this is a method that you can use to
turn s1 into a higher-order integrator
and this method doesn't just end there
it turns out you can plug s2 into the
same sort of approximation building
thing and make s3 which will be a
seventh order formula and in in general
you can do that and here are the
coefficients that we'll end up working
it for generating a chi for order
formula from it so that is the method
that in general is used in order to
fully optimize these simulations so by
doing this we end up getting near linear
scaling of the circuit size with T and
without using this optimization by
staying with a low order formula then
we'll end up getting something that'll
be like t to the three-halves or
something worse like that so that is the
technique that in general will allow you
to get around these problems involving
sums of hamiltonians however a question
you might ask is what the heck does r
have to be can you give me a value of r
and promise that if you choose this
value of r then the error will most be
epsilon and in general upper bounds have
been proven the two best results out
right now
are these results and they end up
predicting that the air and the Chi
order a Suzuki integrator - the actual
evolution is upper bounded by this over
here a numerical evidence suggests this
upper bound is far too loose it can be
too pessimistic by three two fours of
magnitude in some cases but nonetheless
the results prove that if you take a
value of a number of time steps that's
greater than or equal to this quantity
which is derived from the error estimate
here then you guarantee that your air is
going to be less than Epsilon
so that's the that's the importance of
this it promises that your air is less
than Epsilon and this is especially
important for simulation because it's
hard to assess the error in a simulation
you can't compare to the output of a
supercomputer
so it's important to have bounds on the
air but if you if you don't need to be
rigorous then you can take
are to be whatever you feel like so and
furthermore one of the things that's
important is optimizing your choice of
our is it's also important because a
total cost of implementing a simulation
algorithm is proportional to our okay K
is K here and should actually be KY
which is the order of the charter Suzuki
formula so I'm sorry that's a typo
so again just to summarize what the
strategy is is we want to simulate an
exponential that's the sum of two non
commuting terms and we use one of these
higher-order splitting formulas to break
this up into a sequence of Exponential's
that are just like the kind that we
discussed previously and then we use our
circuit decomposition intuition that we
developed from that in order to handle
each of those independently and that's
how you go about building a simulator
the question is how do you automate this
in a fashion that actually isn't more
work than doing a simulation in the
first place so the last thing you want
to do is make a Rube Goldberg
contraption type automation procedure
fortunately our algorithm is a lot
better than these examples of automation
so the idea behind our classical
algorithm is this what we do is we begin
it with input taking an efficient
representation of the system Hamiltonian
as a string we require a parameter that
tells how local the Hamiltonian is ie
K you know say it's a K local
Hamiltonian the number of qubits the
Hamiltonian acts on has to be provided
as an input and also the desired
evolution time and the desired number of
time steps used in the overall evolution
have to be provided as input then the
circuit design algorithm automates the
reasoning that I discussed previously
and outputs a string that represents the
quantum circuit so now let's discuss
what the inputs and the how to encode
the inputs and the outputs using yes and
I'm there's many ways this isn't unique
it's just something it was that we
arbitrarily came up with so the idea
basically is this the hamiltoe
Hamiltonian in general will look
something like this it'll be the sum of
these local terms with different weights
in front of them so the way we encode it
is we store the weights as a vector so
we store each of those is this vector I
denote boldface a and then we also store
strings that encode each of these
individual terms that they it'll tell
you whether it's an x times an X or Y
times a Zed and if we store two strings
that encode those pieces of information
then we have a complete description
of the Hamiltonian so that's the basic
idea the way we practically do it is as
follows we encode each of those those
terms as a concatenation of two strings
and the string represents the first
string L represents the numbers of the
three types of Polly operate our
operators present in a given term so for
example in this X 0 X 1 case there's two
X operators there's no Y operators and
there's no net operators so we store
that as a string 2 0 0 the S on the
other hand will tell you will tell you
the locations of those Pali operators so
in this particular case it yeah and it
stores the locations of the X operators
the Y operators and the set operators
separately to avoid ambiguity so in this
particular case there's no Y or Zed
operators and the X operators act on the
0th qubit and the first qubit
respectively so that's how we would go
about encoding it and a full example of
the Hamiltonian is given here so this
represents the encoding of the total
Hamiltonian this description is also
efficient for constant K because of the
fact that we only need a constant nut
sized string to represent each term and
furthermore there's n to the K such
terms and K is a constant so for K local
Hamiltonians there's there's a
polynomial number of terms where n is
the number of qubits in the in the
system and each of them has constant
size so therefore the input is Paul I'll
put a polynomial and so the input is
polynomial and therefore it's reasonable
to work with
they're in you need not ignore most this
this is how they stand and stand alone
zero is one out of this yes
so in this particular case if the out
where the output is similar similarly
encoded so we encode a see not gate in
the output is just the string C naught
and I and J followed we refer to a
Hadamard gate is Hadamard times identity
and a PI by 8 gate person item Hadamard
and then I were indicating which qubit a
taxon and a PI by 8 gate gate is just s
and the cupid attack so so that's the
output and the output is designed to be
implemented in order from left to right
okay so the way that this will end up
working is this will end up producing
you begin with a description of a
Hamiltonian that you have at an abstract
level you encode that it's a sequence of
strings you feed that to your compiler
and the compiler will output another
string which can then be directly
interpreted as a quantum circuit so
that's the that's the overall idea and
again the strategy that we use to go
about and achieve this is we use a
charter Suzuki formula to turn our
simulation into a product of
Exponential's we then generate a
simulation circuit for each exponential
and concatenate them together to come up
with a simulation circuit that describes
one of the our time steps the final step
is very easy we just glue are identical
time steps together one after another
and that will enact the overall
simulation so now I'd like to discuss
how the how to encode the output of the
trotter Suzuki formula in just a little
more detail to make it in a fashion that
fits inside that paradigm so the idea is
is that using some algorithm doesn't
matter exactly how we end up getting
this sum of individual hamiltonian terms
H build I've denoted unfortunately h1
through h6
these are actual products of poly
operators so we have some output that
looks like this and then what we do is
we encode each of these terms as a
string again we don't want to store them
as a matrix because the matrix is very
big so we can uniquely specify each of
these terms over here by specifying the
Hamiltonian which is done by giving the
way
of the coefficient the number of each
type of poly operators in there and
their locations so that specifies the h1
and this specifies the evolution time
for the first term so we can come up
with a string representation for each of
these Exponential's that are in the
product excellent question
in general they they won't they won't be
the same the reason why is because of go
back that the this notice that these are
not just the same same length either
this backward step is much longer than
the previous two so in general many of
these these directions will be the same
but some of them will be different so I
need to specify which ones are different
for the base trotter they're all the
same
okay so then we use this output of the
algorithm as a sequence of strings of
this form as input for our circuit
construction algorithm so our circuit
construction algorithm is pretty pretty
straightforward although it looks kind
of ghastly so what we what we do is we
take as input a description of one of
these Exponential's and then what we
want to do is we want to output a
circuit that that simulates it and the
way we do it is we take each element in
Si X which is the description of the
locations of each poly X operator in a
term and then we want to transform to
the to its eigenbasis and we do that by
applying the Hadamard transform on the
qubit in which it acts so that's what
this first bit set first step says
second step says well do the exact same
thing for the Y operators instead of but
we have to use a slightly different
diagonalizing operator we've got to use
this ghastly thing here okay but it's
intuitively exactly the same so after
these two steps the term the individual
exponential has been transformed to its
eigenbasis so then what we do is we find
some particular value L which is the
maximum entry of all of these things
here and the reason why we do that is
because I mentioned that we want to
compute the parity when we do these
these things and we'd like to not use an
extra and Sylla bit to store that parity
so what this step does is it finds
arbitrarily the qubit that this Hamlet
this term acts on that has the highest
label and you and does the appropriate
rotation just on that cue on that qubit
so that's what this does it finds that
the qubit with the highest label that's
non trivially and interacted with then
for each for each element in this si
over here ie
each location that will at least one
poly operator acts on we need to compute
the parity and we do that by doing a C
naught with the control being the gate
that has the
thing in it and target being the last
one and obviously we don't do a see not
from itself onto itself if we can skip
that so we continue through the next
thing is the our zet operators that I
alluded to they are not in our
fundamental gate set so we have to
transform them into things that are and
to do that we use a solid avoid kitaev
algorithm as proposed by Nielsen and
Dawson but there may be more efficient
versions that could be in existence soon
but we I should of course mention that
in general there are in particular
architectures are zed gates are things
that are relatively natural to carry out
but we stay away from that because of
the fact that arbitrary precision
rotations aren't something it's a very
physical resource so we prefer to
consider what happens by discretizing it
into our fixed gate set these last three
steps are pretty self-explanatory
they just undo all the stuff that's done
up here so that's it some epsilon is it
so I would translate them to
Turin city requirements whatsoever a
queue time so how that translates is we
have you you from your value of our that
you use for the number of time steps and
the order of the charter Suzuki formula
that you use you know how many
Exponential's you're going to have in
your sum so you want the air in your the
in a solenoid kitaev algorithm to be
like epsilon over the number of
Exponential's because of the fact that
the air grows at most linearly with a
number of such terms so you can
guarantee then that the air will be at
most constant multiple of your air
tolerance if you do that okay so that's
what you do now
let's put this all together and again
describe how we would go about and build
the overall simulation from this and
it's again more or less what I said we
apply the Trotter Suzuki algorithm with
the inputs that describe that the
particular Hamiltonian and compute a
sequence that ends up describing the set
of Exponential's that are needed to
perform one small time step then we glue
our such as our such time steps together
and output that's basically it so now
that I've described the overall
algorithm I'd like to talk about some
applications to show how well this this
ends up performing so the classic one of
the classic examples that's worth
talking about is the honeycomb model
which is important because of its
relevance to topological quantum
computing so this is an example of a
physically to local Hamiltonian the
reason why is because each qubit
represented by a circle here is coupled
only to nearest neighbors and
furthermore each of them regardless of
how big we end up making a system each
of them is only coupled to at most three
other qubits it's because of the fact
that the number of couplings doesn't
increase with the size of the system
it's physically local not just K local
so that's what we
difference between white and black
there's a there okay basically they're
not this they're just not this they're
not of the same class all the X's can be
translationally mapped on to each other
whereas the couplings I have a different
sort of form from the whites and the
blacks so that's why they're listed
differently so if you take a look at
Burt the the Zed gates are sorry the
white over here it's a couple down by if
I said here we can't just translate that
down to this lower one without without
rotating and make these two equivalent
all the rest of these though can be just
mapped directly onto each other so it's
rotation
so when we go about doing a simulation
there's three different types of
interactions that we end up getting out
of this and each of those interactions
can be decomposed into a simulation
circuit that will handle them going
through the the work that I described
previously these describe the why why
couplings these describe the XX
couplings and needs to scribe the Zed
Zed couplings so we just have to glue
all of these steps together and then we
can figure out the cost doing this we
end up finding for a fixed number of
time steps are the following number of
gates required for the simulation which
is a neat result because of the fact
that nobody actually has a hat before
this at the best of my knowledge had
provided a decomposition into
fundamental gates like this and again
you can do better we listed in terms of
Zed rotations because of the fact that
the solid way kit to have could give
different slightly different results
depending on what your rotation angles
are for the given time so we n is the
number of qubits in a system
and the value of our ends up scaling
like this and you can treat little m
here as being order n in this particular
case so roughly speaking this ends up
scaling like N squared so it's very
efficient
we similarly we can also take a look at
simulating models of superconductivity
particularly pairing models between
electrons and Wu bird and lid are showed
that such hamiltonians can be written in
this particular form over here so this
is a this is a form that also is
amenable to our simulation techniques so
we can actually simulate that actually
even using the exact same circuits that
we described for the honeycomb model so
doing the exact same thing gluing all of
them together we end up finding the
following costs and one of the key
things to notice is before we only had n
times R basically but here we have N
squared and that difference is because
of the fact that the the model if I go
back you'll notice it actually isn't
physically local there's coupling
between every other qubit in the system
and that that ends the ends up
fundamentally changing the complexity so
our result ends up giving performance
scales like n to the fourth and slightly
worse than linear scaling and it's in
the time you want to simulate the system
for and this is much better than the
previous result which is nearly
quadratically worse in terms of its
simulation with time and has a worse
factor of n as well so that's what we
end up getting out of this in general we
can talk about what these this thing
will look like when we apply it to an
arbitrary K local Hamiltonian and in our
R an arbitrarily arbitrary physically K
local Hamiltonian and the number of
operations that we end up getting ends
up scaling something like this it ends
up scaling a little bit worse than n to
the 2k for a generic K local Hamiltonian
whereas for physically K local
Hamiltonian it's a little worse than N
squared that's a big difference
for a large value of K even for K equals
2 that's a nearly quadratic difference
so that's one of the reasons why
denoting the difference between these
two different classes of local
Hamiltonians is important for our
context ok so what I mean is I mean it's
like I would use for Big O notation
but I can't put a Big O around the
outside of it because of my use of
little o and the exponent
over here it ends up making a little bad
reading so when I the way to read this
is it asymptotically scales at worst as
the following function yes
so how do we optimize these circuits
further now that I provided a rough
prescription for how you can go about
doing it and the way one of the ways
that we can do this is that actually
many of the terms in the Hamiltonian
will commute and if we just were to
randomly put these non these commuting
operations into the charter Suzuki
algorithm then we'll kill that the depth
of our circuit because we'll do there's
all sorts of operations that in
principle could be done in parallel that
no longer will be able to if we just
naively put it through so one of the
ways we can optimize our results is by
finding the terms that commute with each
other and then grouping them so that we
can execute them at the same time so the
way that we do that is as follows so you
can see an example of this for the
particular this particular hamiltonian
and here are two different ways that we
can use the Trotter formula to
approximate the time evolution operator
in this case in the first case you
notice that these two operations in the
brackets commute because they act on
different qubits whereas in this
particular case we can't commute any of
the terms so the if we go through the
circuit design algorithm we find that
the resulting circuit for case a looks
like this and in case B it looks like
that and it's a little hard to get an
eye for so I drew this out so you can
get an idea of what the circuit depth
looks like and you'll notice that in
this particular case the depth is
reduced in contrast to the other one so
that is one of the one of things that we
can do so how do we end up finding
whether or not terms in the Hamiltonian
commute the way that we do it is we take
a look at the following relationship so
it turns out that an equivalence for two
terms in the Hamiltonian commuting is
that is this condition over here so
basically what I mean here is I mean the
size of a particular set of locations
where the poly operators both have
disjoint action is at most is equal to 0
mod 2 so there has to be an even number
of locations where they both have
different
and you'll see things like this in say
the toric code where you have you know
XX couplings and Zed Zed couplings that
are happening on the same qubit but
actually the two terms commute because
there's an even number of interchanges
and this is this refers to this this
criteria captures that as well as the
case where you actually have different
actions on each qubit so that's what we
get out of it and basically our grouping
algorithm for grouping these terms is
really straightforward we just compute
that term and then we group them all
together in a fashion that has all
commuting elements in a particular group
done first and then every element in the
next commuting group is done after that
and so on and so forth so that's how we
do that what the question is what does
this end up end up doing for our
algorithm and there's actually
substantial improvements in the
complexity for the two cases so in the
case of yeah okay so let's go down and
so in these particular cases we end up
getting scaling with a number of qubits
it goes like n to the 2k plus some small
sub polynomial function minus 1 in
general using using grouping whereas if
we didn't use grouping you remember that
note of the number of operations
required which is equivalent to the time
if we don't use parallelism ends up
scaling it worse like this so we end up
getting a reduction by a factor of n by
grouping in the worst case scenario by
using this this algorithm for physically
local Hamiltonians it's even better we
end up going from something that's like
N squared in our previous work to
something that is like n if we use
grouping and we and we perform all
operations that commute in parallel so
as a result this can actually really
really seriously improve the performance
of our algorithms in architectures that
permit parallel execution in
architectures that don't permit parallel
execution it's not clear what form of
improvements this will end up giving
so to conclude we present a constructive
argument that our constructive
destructive algorithm that generates
circuits for simulating many body
systems our circuits are more efficient
than previous constructions that have
been considered and also as an important
point we don't actually require any
ancilla bits for this we use n qubits to
simulate n qubits which is a big big
advantage for people who don't well for
existing quantum computing a computer
implementations that are limited to at
most 12 qubits or so at present and also
we show how to exploit parallelism which
can lead to improvements in the
execution time for things that allow
parallel execution there's a few open
questions one of the one of the
important questions I think is well this
grouping idea that we came up with is
there more that we can get out of it by
expressing the terms in in ways that
commute are we actually reducing the air
in a Trotter Suzuki formula more so than
what the upper bounds say equivalently
when we act when we do this grouping
we're all every term that commutes has a
simultaneous eigen basis but yet when I
was describing that previously I was at
every step even for the commuting terms
transforming to the diagonal basis
performing a simulation transformed the
computational basis then transformed the
diagonal basis in many cases an
intelligent choice of the diagonal basis
may allow us to forego almost all of the
basis transformations so that is one way
in which this can be improved also the
question is can we use compilers that
actually optimize the circuit output in
order to improve the performance of the
of these really naive simulation
circuits that end up coming out of this
algorithm and I would be very surprised
if they couldn't be in mode in many
cases substantially improved also let in
general we might not be interested in
every possible property of the quantum
state for example for these systems we
might only be interested in the average
magnetization or some quantity like that
and in those cases do we really have to
keep track of every aspect of the
quantum system there's some things we
can throw out because they won't affect
it
well that's an open problem and one that
I hope ends up getting addressed but I
don't know the answer to it and also a
final final question is can these ideas
be used in order to come a design very
efficient simulation circuits that can
be used in order to bootstrap a quantum
computer so well thank you very much yes
I will be very much interested in a case
study
if you would
if you will have some cases where you
have manually show
use the circuit size
the second the second point here yeah
yeah that would be something that would
be would be interesting this case study
might that we do
there's some
okay
concerning the third from the top
looks like like a projection axis
- son
and to some system whether with smaller
cubitt's expert
to see
while at the same time making sure that
whatever rejection loses
what we are interested
but you also start
put less into the Hamiltonian
it's a very good question but in many
cases aside from you know app direct
application of perturbation theory it's
very difficult to say what terms you'll
care about and which ones you won't
necessarily care about for particularly
observable
I mean you know if one term is a million
times smaller than all the other ones
then it's a trivial task but in general
it's ah I think it's hard problem
and how do you receive this help people
quite a bit before point
how do you see this helping the
bootstrap well the way I see this
potentially having an application for
bootstrapping is what ideally what you
want to do with bootstrap is strapping
is you want to have a partially
characterized and partially controllable
quantum computer that doesn't have
access to all of its qubits for a
because of poor characterization so the
idea is is you would perform a
simulation of how some ideal circuit
should perform on a subset of that
larger space and then based on the
outcomes of that you benchmark and
compare the performance of that
particular uncontrolled system with
different sets of control parameters in
an attempt to learn how to control that
and increase your number of effective
qubits in the system so you do this you
build a larger quantum computer after
reclaiming say one additional qubit that
you have and then you repeat this
process until you can control every
physical qubit in the system
there's a bigger question you seem to be
using implicitly as the ideal human
so
this was 15
folio all your qubits are ideal and
you're absolutely 100% correct in its
analysis is not taking a look at
imitators any form of Goethe
imperfections other than what happens by
implementing yard gates via solloway kit
F so all of this is done in a very
idealized model and decoherence yeah so
yes you're absolutely right there's much
more realism that we could build into
this but in many cases we that this is
going to be device specific so and our
goal here was to come up with a general
circuit building model that's
appropriate of on any platform
refinements to these ideas I think would
be very appropriate for optimizing a
performance of simulation circuits in
the presence of these problems which
will be specific to implementation when
is a lot of work then together I think
that might have had to go with you good
strapping questions talking about
worrying about a real system now we've
got a whole different layer that makes
it hard figuring out how to understand
the qubits you've actually been dealt
and under what circumstances for a real
system you're not going to be able to
perfectly control you're not going to be
able to perfectly reclaim them so the
question if that's why I think it's
still an open question and an
interesting one to determine to what
extent can quantum simulation algorithms
being used to build a bigger quantum
computer nowhere here did you talk about
preparation of the starters
and so what we're just gonna assume that
there's a sale you assume you can print
it you can prepare your favorite state
and or equivalently assume you've got
some machine that at the beginning of
the algorithm outputs that to you
have you implemented a simulator on top
of the open strings of this this was
just the work to figure out how to get
this this is just the work to figure out
how to gangsters
they have you covered all that for free
and you know I haven't
this is holding the paper everything's
in here except for the grouping bit that
bit is being added and in preparation
for a submission to a different journal
obviously what journal do it well we'll
be submitting it so shortly - a new
journal of physics and we'll have a new
version on the archive hopefully very
soon just I'm always looking for the
updated version whatever exactly
good stuff
I've read the paper twice after Shh
yeah I think yeah I think there's some
improvements can be made
your bangles well thank you very much
really grateful that everybody came</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>