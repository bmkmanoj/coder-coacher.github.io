<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Barrelfish: A Study In Distributed Operating Systems On Multicore Architectures Part - 2 | Coder Coacher - Coaching Coders</title><meta content="Barrelfish: A Study In Distributed Operating Systems On Multicore Architectures Part - 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Barrelfish: A Study In Distributed Operating Systems On Multicore Architectures Part - 2</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iw1SwGNBWMU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay ready to resume okay so I was going
through the three design principles of
the multi colonel model and we're up to
number two which is Hardware neutral
structure so the idea of the model is
that the operating system structure is
separated as much as possible from the
hardware there have to be some hardware
specific parts the message transports
and the device drivers and one advantage
of doing that is that the message
transports can be highly optimized
specialized to the particular hardware
and it also means we have a lot of
adaptability to change to change the
performance characteristics as necessary
so for example a late bind on the
protocol used to exchange messages or
the transport implementations so you
might change the way in which you
exchange messages in theory at least say
if according to the workload of the
machine or something like that you could
switch between a a pure message passing
to using a shared memory implementation
for example the third design principle
is replication of common state so what
we're saying here is that any state that
is potentially shared rather than
treating it as if it's shared and having
all the complexity of synchronization on
that state there by default we're saying
you should treat it as if it's a local
replica as if you the local core has
exclusive access to that state and the
kind of state we're talking about here
are things like scheduler queues and
process control box and so on so this
naturally supports a situation where we
don't have shared memory as might be the
case in a true genius system and it
supports when the the set of calls that
are running home is changed dynamically
okay so there's really a spectrum of
replication and sharing here so in this
diagram at the top there on the
left-hand side we've got all global
state being shared and and essentially
one big lock and over time this has
gradually moved towards finer grain
blocking and and somewhere around there
is where current operating systems are
and further to the left is is where
older operating systems might be moving
towards the middle we've got things like
k42 that are recognizing the problem
with locking and then over on the right
hand side completely distributed state
and maintaining replicas and the multi
kernel is basically coming in on the
right there and there's clearly a lot of
space in the middle for different
approaches as well I'm not saying the
multi kernel is is the ultimate perfect
idea of an operating system for
multi-core it's probably not but it's
it's a point in this design space so in
a multi Colonel we have the state being
replicated but we still have sharing as
an option as a local optimization so we
might have a shared replica between two
closely coupled calls for example where
by closely coupled I mean if we've got
hyper threading or calls that share an
l2 or an l3 cache it might just make a
lot of sense to to do actual sharing
them however because the default is not
to share them we can decide whether this
is appropriate at runtime and the basic
model remains asynchronous message
exchange
so just to come back to my picture
here's the the multi colonel model again
hopefully the the reasons why we've
structured it in this way have become
clearer from having gone through the
design principles I guess I've already
said that you know the multi kernel is
theoretically elegant it's a it really
is an idealized position so it's not
necessarily the way any implementation
is going to turn out I mean for example
this this message passing abstraction
means that making use of the shared l2
cache become you know it's something
that we do have to specialize explicitly
rather than if you have a more pragmatic
approach in your implementation you
might know that you're always going to
run on one particular architecture and
optimized for that as operating systems
do today but that this is this is the
ideal again just to to contrast with the
monolithic operating system structure
and the microkernel operating system I
mentioned yesterday a few examples of
micro kernels but I probably didn't
explain what a microkernel is so the
idea is taking as much operating system
functionality as feasible to servers in
user space so the microkernel just does
the really basic OS functionality you
know deals with processes and threads
and memory and low-level I 0 and so on
so quite often people say isn't barrel
fish and the multi Colonel really a
microkernel but I think it's fairly
clear that it's not and some further
reading um so we're not the only
research group trying to tackle the
problem the multi-core challenge for the
OS and here's a few examples cory false
tessellation and healing
also are all very different examples of
with with a similar approach to the
multi colonel okay so now i'm going to
move to telling you about barrel fish
the operating system so this is a joint
project between Microsoft Research and
ETH in Zurich is quite a large project
it was started I guess in 2008 and most
of the stuff that goes on happens at ETH
and there's a lot of students doing PhDs
and doing masters theses based around
barrel fish so it's been ported to a
bunch of different architectures and the
the main paper on it was published in
2009 it was one of the references I gave
in the last slide you can download the
source code if you want it's under an
MIT open source license and there's also
a very active user community quite a few
people are interested in doing research
have a barrel fish and there's the
mailing lists and the occasional
workshop and there was actually a paper
describing some research based on barrel
fish published at a workshop earlier
this year and I think that's and and it
was done by some people in Sweden so it
was nothing to do with ETH or MSR and I
think that's the first first group to to
actually publish their research based on
bow fish but we're hopeful that there'll
be many more so what does barrel fish
actually look like so in this in this
particular diagram this diagram was done
when it only existed on one platform
which was x86 64 bit in the colonel
there's a privileged mode CPU driver and
this is the software that
does the kind of core OS things it
enforces protection it does
authorization it time slices processes
mediates access to the core and and
other hardware it also does fast call
local messaging and it delivers hardware
interrupts to userspace drivers and the
cpu driver is typically invoked by a
standard system call and that has a cost
comparable to invoking a system call on
linux it doesn't share any state with
any other core so it's actually
completely event-driven and single
threaded and we claim in our papers
although I'm not sure whether this is
true that it's easier to write in debug
than a conventional kernel basically
because it's it's really small so above
that we have a distinguished user space
process called the monitor and currently
the main OS state that is replicated on
each core inside the monitor is
something called the capability database
so barrel fish protects resources using
capabilities and I'll talk a little bit
more about that later so one difference
here from the multi kernel model is that
we factored the monitor and the CPU
driver into user space and kernel space
respectively and that's not strictly
necessary according to the multicam
model we could have all of the monitor
functionality down in kernel space and
that would avoid some performance
penalties but we did this because we
wanted to keep the kind of reflect the
structure of the multi kernel model more
explicitly but actually there's
definitely a price to be paid for doing
that and and obviously if we push the
monitor into the kernel space and that
piece of code would become more
complicated
so the monitors collectively coordinate
the maintenance of global state and they
encapsulate much of the mechanism and
policy that you would find in a
traditional operating system they're
just normal user space processes and so
they get scheduled along with everything
else but they're very well suited to the
you know they love things that are
sending the messages doing the split
phase messaging and dealing with long
run remote operations so a monitor that
has to do something expensive involving
sending messages and getting replies to
from other calls can send all its
messages and then quiesce and something
else can be scheduled on that core to do
more work while it's waiting yeah
right these are distinguished processes
they do have special permissions no they
don't but they do have so that the it
all works through the capability system
and the the colonel the cpu driver
controls ownership of capabilities and
so the the monitor is going to be given
permission to you know it's going to
hold capabilities that user processes
are not authorized to to hold so it said
so yeah there's a little level of
indirection there okay so another
feature of barrel fish that is not a
part of the multi colonel model but
which turns out to be really useful is
this thing called the system knowledge
base so all this stuff I've been talking
about you know optimizing at runtime for
this and that really relies on having
some system wide knowledge of the
characteristics of the hard way that the
operating system is is running on and we
do this by having this server this
essentially a knowledge base that
storing facts either pre look backs or
fax learnt by for example measuring
communication latencies at runtime it
stores all this stuff inside the system
knowledge base and we can then use query
the knowledge base in order to figure
out how best to arrange for some
operation to take place and I will give
you a concrete example of this in a bit
so the message passing obviously it has
to be fast and ideally we do want to
specialize the implementation of the
message passing to the hardware in fact
we really have to other otherwise it's
too slow but it needs to have a
standardized interface so there are two
types of messaging on barrel fish we've
got the in the same core messaging which
we call local message passing or LMP and
between two different calls which we
call not very intuitively use a level
message passing or UMP and again this is
an aspect of the operating system where
we we've actually sort of explicitly
implemented it in an architecture
specific way and we package these things
up in what we call interconnect drivers
so local message passing on on between
processes or domains as they're called
and barrel fish on the same core can be
either asynchronous or synchronous the
synchronous version is in fact an
implementation of a technique that
somebody else devised and wrote about
Brian bashad back in nineteen ninety
called lightweight RPC and it's kind of
interesting if you go and read this
lightweight RPC paper you can see that
it was actually implemented on top of
the taos operating system for the
Firefly because they the claim in the
paper is that this RPC everywhere was
too slow in these fart you know Matt
performance of 4,000 our pcs per per
second or per minute per second really
wasn't good enough and so you really
needed to use this lightweight RPC but
there is definitely a trade-off between
complexity and performance there and the
Firefly guys were more interested in
having something that was
straightforward to program
user-level RPC again we use a technique
that Brian bashad published the
following year which I've sort of
alluded to before the the channel the
message channel is a region of memory
and the messages are basically cache
lines so it's piggybacking on the cache
coherence protocol and you get really
good performance by doing that but it's
kind of a low-level grungy thing to be
implementing so the replicated state
inside the monitors is kept consistent
using some sort of agreement protocol so
I'm going to give you a couple of
examples of what we actually do I have
to say that in the context of a
distributed systems summer school these
particular agreement protocols are very
trivial but that's okay you know we
don't need to to deal with things like
faults in this environment and there are
other concerns so the first one I want
to talk about is keeping TL B's
consistent so a TLB is a cache a CPU
cache of virtual memory mappings and
when that cash when an entry in that
cache becomes invalid and it has to be
flushed in a multi-core system
potentially the same entries could be in
other caches and so they all have to be
flushed the second example I'm going to
talk about is the capability database
which I already described and as I said
the capabilities are used to protect
resources in bowel fish and here we're
talking about virtual memory so to keep
this database consistent requires a
two-phase commit that
he is a one phase commit in future we
may need more sophisticated protocols
but so far we haven't found that to be
necessary there was an early sort of at
the beginning of the barrel fish project
Timothy Roscoe or mathy is the professor
at Zurich who's leading this project and
he took great pleasure in telling people
he was going to put pack sauce inside
the kernel haven't done that yet so just
just to make this discussion quite clear
I'm just going to run through virtual
memory at a very simplistic level but a
sufficient level to understand what I'm
talking about so memory is divided in
most machines into fixed sized blocks
called pages and the user process can
have a virtual memory address size that
is much larger than the physical address
space when a pages is mapped as we call
it its virtual address is translated to
a physical page address so that the page
appears in physical memory this allows
lots of different processes to have
pages in memory at the same time it
means that pages are specific to a
process so they can't you know access
each other's pages and it allows the
physical pages the pages in memory to be
moved around completely transparently to
the process so this means that pages can
be loaded anywhere in main memory they
you know it doesn't have to be fixed
this level of indirection helps that and
the fact that the the memory is broken
up into these fixed sized blocks means
that it doesn't have to be contiguous
that pages can be scattered throughout
so here's a picture I stole from a book
by Patterson Hennessy showing basically
pictorially what I just told you we've
got the virtual addresses mapping to
physical addresses when
a page is not in physical memory then
it's on backing store on for example a
disc so page tables are what hold this
the mapping from virtual to physical
addresses so when a the data that a
process needs to access in memory has to
be in its page table if the page isn't
in the the if the page isn't in physical
memory then there has to be a fault
generated in the in order to pull that
page from the backing store into into
memory and there's a valid bit in the
page table that indicates where the
pages the moment and these page tables
are themselves stored in memory and
there's a register that points to the
page table for each process and the
operating system will modify each the
page table of each process as necessary
so the the TLB the translation lookaside
buffer is as I said before this very
small CPU cache of those recently used
address mappings from virtual to
physical so I'm just going to go back up
to here there's something I forgot to
say to you so
No okay so the trouble is that that
overhead of page translation is if there
isn't a TLB then it's actually really
slow and in fact as virtual memory sizes
get really large we end up with for
example multi-level page tables are
pretty standard so this means that it
can be very expensive to look up the
address translation and so that's why a
TLB is really important and they're
small they're typically 64 to 2048
entries and on each memory reference the
CPU will compare the the page number
from the virtual address with with the
set of pages in the TLB and it does
although all of this in parallel so it's
really fast and the hit rates on the TLB
are typically ninety-five percent or
more okay so whenever this page table
mapping changes the TLB has to be
flushed so this happens for example when
there's a context switch when one
processor is swapped for another or also
when when the status of a given page
changes so again another helpful diagram
from Patterson and Hennessy showing
showing what happens the virtual page
the the pages first looked up in the TLB
very fast and if there's no hit then it
has to go through to the page table of
which there may be more than one there
may be multiple levels ok so this
flushing of the TLB is often referred to
as TLB shoot down and that's what we
call it in the barrel fish paper and it
will involve the TLB of every core that
might contain the same mapping so the
way it has to be done is that the call
that wants to flush the TLB have sends a
message to all the other calls and waits
for a response
in linux and windows what happens is the
the court send an inter processor
interrupt and then wait for some counter
to be incremented some shared counter in
barrel fish the monitor does this and it
sends it performs this one phase commit
and what I'm going to show you is how it
can exploit knowledge of the the
topology of the interconnect and the
caching hierarchy to improve the
performance ok so the hardware that
we're using for this experiment is this
weird AMD opteron that I showed you
before with the the huge disparity in
communication latencies between
different pairs of calls ok so the first
implementation that we did of TLB
shoot-down was pretty naive so the way
it worked is there was a single
broadcast you RPC channel so a simple
messaging channel and if you remember
the implementation of the of you RPC of
the messaging between cause is basically
piggybacked on the cache coherence
protocol so the actual behavior that
arises as a consequence is that every
slave every remote call ends up pulling
the same shared cache waiting for the
master to modify it so there's one cache
line which is going to be updated by the
master and all of the all the other
calls are going to be watching for
changes to that cache line to see to see
when it changes and and get there tlb
shoot-down message the problem is
particular to the cache coherence
protocol in AMD 64 s because when the
master updates its line immediately that
line is invalid
in all the other caches and although the
slave cause then have to pull the new
copy from the Masters cash so with any
calls you're getting this cache line
crossing the interconnect end times and
we see that latency grows linearly with
the number of course I'll show you a
graph in a minute so maybe a bit better
to do unicast to to send a message to
each other core but do it point-to-point
so now that the cache lines are only
shared by two cause it's better but it's
still linear so here's that the
messaging costs latency in cycles times
1000 on the y-axis and and calls on the
X and the top line is the worst case the
broadcast but we can do better we know
that those operons have a shared cache
on-chip shared by two it's the level 3
cash so the cache lines that are shared
by those groups of four calls won't
cause any interconnect traffic and be a
lot faster so we use a two-level
multicast tree for shoot down messages
send a message to the first core of each
processor which then forwards it to the
other three calls in in the package and
that second message is a lot faster
because of this shared l3 cache and also
all four processors can be sending
messages in parallel and there's no
interconnect contention we can actually
do even better than that which is to use
the fact that the machine is Numa and
there's as we saw before the different
latencies between pairs of course so why
don't we choose to send to the remote
the you know the single core on each
package if we choose the
ones that are the furthest away with the
highest latency and send their messages
first then the whole thing will happen
even faster here's a picture of that
actually happening so this is a if this
is a tool that we wrote actually we've
reused it over and over again for many
many projects but it's basically sucking
on events that are being generated by
whatever system we're monitoring and
then we just have time going along the
bottom from left to right and in this
case each line is a core here from zero
at the bottom to 15 and we instrumented
barrel fish to produce an event on
message send and receive and you can see
that the though the core at the bottom
which is the master is sending first of
all it sends to three other calls those
are its local calls and it also sends
two calls on different packages and I
think you can if you look at it
carefully you can you can see what's
going on with the hierarchical
forwarding of the messages and so the
costs when we do this are substantially
better the bottom line is the numeral
we're multicast and it's almost flat
with the number of calls and the
multicast actually the steps in the line
correspond to the fact that the we've
got these latency problem and we're not
overlapping the way that we are with
when we go numeral we're so this is
making use of specific knowledge about
the architecture that we happen to be
running on and but but doing so in a way
that's not you know we haven't designed
the operating system to do this all the
time this is done in response to a run
I'm query to the skb to say hey what are
the communication costs between these
different cause and you know what's the
caching hierarchy and so on and then
using that information to decide what
the best way is to to issue messages
40lb shoot down and so it can be done
differently for a different architecture
and then just another graph showing the
end-to-end comparative latency of the
TLB shoot down so bearish has a fixed
overhead which is more expensive when
you have a smaller number of calls
because it has to do a local send a
message to the local monitor and there
is as always a per message cost of
marshalling or demultiplexing and
there's various scheduler effects we're
also measuring it here on a version of
Linux and a version of Windows and
there's the error bars are showing the
standard deviation so the reason that
Windows and Linux perform worse as you
have more calls involved is because of
serially sending inter processor
interruptus to do the shoot down okay
are there any questions yeah
could you speak up ilysm microphone
around
the TLB is per core it's per core and
they're not sharing any state
so that the processes or domains have
the same virtual address space so if the
mapping changes and you don't update in
every on every call then that the the
process running on that call could
potentially go to the wrong page does
that make sense well we can talk later
if it's still confusing
yes I think you're right the answer is I
don't know yep yep yep well i think in
recent work that they have actually done
some work on heterogeneous architecture
and God barrel fish running but I don't
know what what they did specifically
because that was after I stopped working
on the project but actually Tim Harris
when he arrives Tim's been working on
bail fish right up to now so he would be
a good person to talk to you about the
more more recent stuff but it's a great
question I mean it yeah there's a lot of
you know we have this sort of nice
idealized stance on how things should be
structured and then we go in and try to
actually implement it and things don't
necessarily work out the way that we
hoped
yeah ya know
yes
for which we have cash translation 20 do
if you have to pay them
okay all right so the example the second
example of an agreement protocol in
barrel fish is the the capability
database so basically regions of memory
are protected by these capabilities
which have typed obviously they can't be
for jable and user domains keep track of
their capabilities in this data
structure that we call or see space so
capabilities can be turned into
capabilities of a different type
according to a set of rules and
capability can be split up into a number
of smaller capabilities so you know when
a program starts it's given a region of
address space and it can then subdivide
that address space and it can do
different things with different parts of
it as it needs to so monitors keep track
in the capability database of who owns
which capability and and what type that
capability has and these capabilities
can be copied around the system so you
can have multiple copies of the same
thing existing and a capability can only
be you know capability one type to only
be converted into a different type once
and it can be revoked although again
revocation is a very tricky problem and
I'm not convinced the bell fish
implementation tackles that that well
okay and these databases are kept
consistent using a simple two-phase
commit so what might go wrong that's bad
so say call one acquires what we call a
ram cap a ram cap is a the raw region of
physical memory it's the capability for
a region of physical memory it passes it
to core two so core two now has that in
the meantime call one turns that raw
memory
into a frame capability this is a
basically physical memory that can now
be mapped into the domains virtual
address space so it's basically giving
it a piece of memory that it can now use
and it maps it in the meantime core to
retype seeeeee this capability to a
c-note type and a c-note is a region of
memory that's designated as a place for
storing capabilities and now core two
passes that that region back to call one
and now call one in theory at least can
fabricate arbitrary capabilities just by
writing bits 22 it's you know innocent
frame capability which is has actually
been turned into also a kind of
privileged see node region which is used
for throwing capabilities so this is a
way of compromising the protection that
the capability system is supposed to be
offering so we really need to make sure
that when our capability is retyped that
that retyping takes place on all of the
calls absolutely certain and that
there's no possibility that that this
can happen so this this was actually it
says demo there because this was
actually part of a live demo that we
used to do and so here's our hardware
again it's a quad core AMD opteron and
here's the what happens if we do this as
a two-phase commit sort of naive unicast
using our visualization tool from before
so you can see pretty clearly cause 0
sending a message to every other core
and waiting for a reply and so on
and then doing it again and the total
time is about a hundred thousand cycles
which is not not great actually and
again we did exactly the same trick that
we did before by making the the
consistency the capability database
protocol Numa aware so once again we
were exploiting the the architecture
that we happen to be running on and same
thing call zero sends a message to some
on each of the remote packages and then
they forward locally and so now we get
loads of all overlapping and it's much
much faster so we've reduced the time by
thirty percent or thereabouts okay so
just to move to a slightly different
mode to finish this talk and I might
even finish early it's very exciting I
just wanted to talk about the challenges
of implementing barrel fish on
multi-core and actually i think these
apply to any and the implementation of
any any software and multi-core so as as
we all know good performance comes from
getting lots of concurrency and being
able to communicate between the calls
very quickly but concurrent programming
is problematic it was quite remarkable
how much deadlock for example we ran
into coding elfish you know it's really
hard even though this was done largely
by the students at eth who are really
smart and they really know their doing
furthermore it's really important to be
able to identify the bottlenecks and
that's not always straightforward and
and also providing the right AP is
so that people can actually do this kind
of programming without their heads
exploding in this message passing
scenario actual sharing of data can be
expensive and in as as I've sort of
started with the any of this talk
caching is great but it doesn't scale it
does need hardware support so moving to
the multi colonel model we're reducing
contention by replication and
partitioning but we're paying the price
of having to run these agreement
protocols and have all that additional
latency and complexity at a different
level so to make this concrete here's a
programming sample and this was an
actual you know fairly trivial example
that we ran into as part of developing
barrel fish but it makes a kind of
interesting case study if you like so
this is a some pseudocode showing what
happens if you want to start a domain
you use a program on multiple calls and
you can think of this as as like
starting a bunch of affinities threads
in some other system so the lines one
and two for I is 1 to the number of
calls create a new thread on that call
and then wait until some counter called
num threads gets up to the number of
threads that we want oh while that
counter is not at the number of calls
we're waiting for the next message and
doing something about it the next
message in this case will be the remote
core saying I'm done I'm so each call
since I'm done and when that happens
handling that message involves this
invoking this function thread create
callback which just increments that
counter ok seems really simple what
could go wrong so here's a here's our
visualization tool again so the first
implementation of this was really awful
40 mil
seconds this was this was pretty bad so
we put in loads of instrumentation you
can see the the key at the top I don't
expect you to be able to interpret this
in any way obviously it's impossible but
we had events for lots of different
activity going on inside elfish the
monitor working the monitor blocked the
monitor polling the monitor setting
pages 20 running the actual application
which was called span test XE the name
service the memory server all this stuff
and what we found here is that the
what's actually going on was that the
domain spanning was basically the memory
system was being used a lot the memory
server was being used a lot and
everybody else was essentially blocked
there's a lot of light blue they're
showing all the monitors were blocked
the the brown sort of almost vertical
lines are messages being exchanged
there's quite a lot of them particularly
going down two call zero which is where
the memory server was running so we
didn't have a and I don't know if this
has changed recently but at the time we
we only had a centralized memory server
so this memory server you know this is
OS a lot of our services are running his
servers and so we had one memory server
because it seemed like a good idea at
the time and so when a new domain wanted
some memory it had to send a message to
the memory server so we had one of those
and it was running on core 0 so was
obvious from that previous visualization
that the memory server was the
bottleneck which is kind of odd because
you don't really think of memory
allocation as a potential bottleneck on
the critical path of you know creating
what it's essentially a bunch of threads
but that's what we run into so we
basically ran a memory server on each
core and gave it a static partition of
the memory the idea of partition memory
server is is great in theory but in
practice it's actually it's kind of an
interesting open research question I
mean should how should you divide up the
memory how should you do load balancing
should you have a numeral way a
hierarchy of memory servers how and when
do we adjust the amount of memory that
each server has and and what are the
overheads in the long term of doing that
but anyway for the purposes of making
domain spanning fast we just did the
very basic thing and we also improved
our implementation memset which was
found to have some performance issues so
so now we got down to 9 million cycles
from 50 million down to 9 million which
was substantially better but still not
really good enough so we went back and
looked again so the monitors were using
the centralized memory server so we made
them use local core local memory servers
as well and we actually moved b0 off the
critical path oh and now we're down to
four million cycles like it's just
somewhat surprising how much performance
improvement you could get even better
change the API so this was done just for
the purposes of seeing how much better
it would be we haven't changed the API
but if you create the domains all in
parallel then suddenly you're sending a
whole lot less messages and we get down
to 2.5 million cycles approximately one
millisecond which yeah that's that's
that's probably acceptable for this
operation but it was a non-trivial
amount of work to to get to this point
and in fact at the time barrel fish
didn't really support doing it this way
and it wasn't obvious to us you know
that this nitty-gritty of designing your
interfaces
deal with this kind of operation is not
necessarily obvious when you start and
in general as I said it is actually
pretty hard programming with messages I
showed you and II Tannenbaum's comment
yesterday that we should all avoid
asynchronous message passing like the
plague so Tim section Friday assuming it
happens he's going to talk about
something called asynchronous sea witch
or AC which was developed as part of the
bowel fish project its extensions to see
to do non-blocking i/o and it actually
helped quite a lot so for further
reading basically please go to the web
page and check out what everybody's
doing it it's a pretty active project
and if if it appeals to you and you want
to get involved there's lots of lots of
room lots of nice projects lots of lots
of stuff to do the credits for barrel
fish bunch people ETH bunch people MSR
my conclusion which you can probably
already guess machines like networks for
good performance we need to pay
attention to the things that distributed
systems designers care about as well
which is basically vast messaging using
replication and paying attention to to
Layton sees and we believe that we
should be able to take ideas from
distributed systems and apply them in
the operating system and that's the end
of my talk are there questions Robert
yeah I forgot that I taken that slide oh
sorry I can show you later it's it's
it's just a line of prologue it's
basically figuring out the numeral where
the tlb shooting actually I don't
remember which of those two examples
that applied to but it was basically
asking the system knowledge base about
the inter call Aiden sees and using that
to decide I can certainly show you later
there's there's actually a paper in asp
loss done by the the guys from eth which
is about using the system knowledge base
to to program the APK pick stuff at the
beginning at boot time yeah
yes in theory yes
yeah I don't actually know the answer to
that question I think in barrel fish we
just invalidate all of them and is that
what you're asking whether you
selectively invalidate yeah i don't know
and i also i don't know what normal
operating systems do does anybody
anybody no no okay sorry i'd have to
look that up yeah
that would be nice so the crew yeah
yes yes yes so actually when we started
the barrel fish project we originally
envisaged this you know we should build
a distributed operating system nobody
has done it and you know we really need
one because it's kind of tedious not
having one when you have a lot of
especially now we all have so many
devices and computers and you know why
can't they be integrated at that level
but it turned out that just looking at
the multi-core thing constrained us but
you're you're right that there's
certainly scope to to push it out and
see if there's something we can do that
might make sense although my intuition
is that you actually want to use higher
level software rather than operating
system level but but maybe sending
messages makes that that wrong I'm not
sure sorry I can't you
yeah yeah yeah I'm it would be a really
interesting thing to try and do yeah
more questions okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>