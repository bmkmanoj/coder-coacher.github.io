<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficient Preconditioning of Laplacian Matrices for Computer Graphics | Coder Coacher - Coaching Coders</title><meta content="Efficient Preconditioning of Laplacian Matrices for Computer Graphics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficient Preconditioning of Laplacian Matrices for Computer Graphics</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g3WK5SNwEEM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
Richard yes I'm delighted to welcome
Dilip Krishna who was who completed his
PhD under Rob Fergus last year and has
done a bunch of work that I found
interesting over the last several years
you may be aware of the dark flash
photography work from a few years ago
but for me at the moment I'm really
interested in in this work they did on
preconditioning for a bunch of problems
that occur frequently in graphics and
computer vision for me preconditioning
has always been some black art that I've
never properly understood so I'm hoping
till it will help us now i forgot to say
and is currently a postdoc at MIT with
dyl Freeman so welcome to it looking
forward to the talking thanks Andrew for
inviting me and thank you for coming to
the talk so this was what we presented
at siggraph last year and basically as
the title suggests we present an
empirically linear time solver for
laplacian matrices which at some point
in the future we hope we can prove that
it's actually linear time in
construction and solve so laplacian
matrices arise in a number of
applications examples from 2d image
processing include tone mapping
colorization and edge preserving
decomposition the coefficients of the
laplacian which are the entries in the
matrix can range from homogeneous for
poisson blending two highly
discontinuous or highly varying for edge
preserving decomposition and solving the
linear system is the most time-consuming
step in all of these applications in 3d
mesh processing laplacian sarai's in
mesh smoothing reconstruction diffusion
and segmentation and one problem is that
most existing solvers are limited to two
dimensional regular grids and therefore
they are not applicable to 3d meshes
and we were hoping to build a solver
which can be used as a black box across
a range of 2d and 3d applications which
would be desirable to have and outside
of graphics and vision laplacian arise
in a variety of applications such as
spectral clustering the numerical
solution of pds which is the classical
reason why methods like multigrid and
multiple methods for solving integral
equations started and the solution of
relaxed np-hard graph partitioning
problems such as min cut so usually in
problems like these the task is to find
the lowest few non-trivial I ghen
vectors so our method is directly
applicable to these problems since it is
agnostic to the underlying application
so why can't we use direct solvers well
direct solvers provide exact solutions
for sparse linear systems that involve
the platians however they are not linear
in the number of unknowns and quite
memory-intensive so naive Gaussian
elimination is off and cube in time of n
square in memory where n is the number
of unknowns nobody uses Gaussian
elimination actually they use something
called necessary dissection which
actually involves some form of graph
processing where the unknown system is
treated as a graph which is split into
two sub graphs and then individually
solved and then combined and so forth
national dissection methods are what you
would use when you run back slash in
matlab static solver and this is off n 3
over 2 and 0 of n log in memory which is
much better than plain old gaussian
elimination and actually empirically it
works very well however it's still
expensive if you get to millions of
variables and the other thing is that we
don't at least in graphics and vision we
don't need
you know all of 10 to the minus 14
accuracy 10 to the minus 6 will do in
which case this opens the door for
iterative linear solvers the main
advantage of iterative solvers is that
they use only matrix vector
multiplications which is therefore o of
N in memory and time per iteration but
the number of iterations depends on what
is called the condition number of the
laplacian l which is defined as the
ratio of the largest and smallest eigen
values non-zero eigen values of the
laplacian the condition number of 2d
homogeneous laplacian happens to be a
function of the laplacian size n
therefore if we just naively use
iterative solver such as Jacobi
gauss-seidel or conjugate gradients we
still get nonlinear running x of n
square or of n to the 1.5 and this
condition number can be significantly
larger when we have non homogeneous
laplacian and so we need something else
to overcome due to use iterative solvers
effectively and that's where matrix
preconditioning comes in the idea of
preconditioning is simple it is to
transform a problem L X equal to B to an
equivalent system Q inverse l x equals Q
inverse B and if the condition number of
Q inverse L is small then we can
significantly reduce the number of
iterations for an iterative solver so of
course we want the condition number of Q
inverse L to be small we want the
condition number of Q inverse L to be
small however it must be it should be
efficient to apply Q inverse to a vector
and we should not need an explicit form
of Q inverse because otherwise that
would involve a matrix inversion which
is 0 of n cube so we didn't gain
anything at all so these are the the key
design criteria for a pre-conditioner
and there have been two main approaches
for building preconditioners multigrid
and combinatorial preconditioners so
multigrid is the is the classical
framework which has been around for
about 30 years now it started off with
something called geometric multigrid
which was defined for homogeneous
poisson equations and what geometric
multigrid does is to define the same
problem at multiple scales through fixed
hierarchies of cautioning and p
conditioning is performed by partially
solving the problem at multiple scales
from coarse to fine and combining these
results so there have been proofs which
show that GMG is known to be optimal for
homogeneous laplacian systems however
once you have in homogeneous systems or
problems with complicated geometry if
performance can be very poor so due to
this algebraic multigrid was developed
which uses so in geometric in GMG a cost
level operator defined using geometrical
arguments and as the name suggests in
algebraic multigrid and algebraic
formula is used to define the course
scale operator and this is called the
galerkin approach which just takes a
restriction matrix and applies it to the
fine level laplacian and p transpose LP
gives rise to a cost level matrix this
can be considered as something similar
to a gaussian elimination of certain set
subset of variables in the fine level
laplacian so AMG generalizes geometric
multigrid to many different types of
matrices however are the key the key
trade off for AMG is the preconditioning
ability and efficiency of applying the
preconditioner to a vector
on the other hand combinatorial
preconditioners are a relatively new
paradigm they were actually based on a
fundamental observation made by vaidya
in 90 and actually for this observation
he never actually published any of his
work he just made this observation and
it started this entire field of
combinatorial preconditioning and he
just left academy and started a company
to make the solvers instead of writing
papers about it and the idea was that a
so he started with the observation that
a laplacian matrix can be associated
with a undirected weighted graph where
the the edge weights in the graph are
positive and such laplacian switch have
non positive off diagonals are known as
m matrices and the edge weights in the
graph as you can see here correspond to
the off diagonal entries of teleplay
sheehan so the key idea in combinatorial
preconditioning is to take the original
graph and create a spasso graph such as
a tree for example which is quick to
invert and the tree is constructed in
such a way that it's it approximates the
original the original graph very well
and in the next slide we'll see what
good approximation means and examples of
such preconditioners our maximum
spanning tree or a low stretch spanning
tree which are particular varieties of
trees that can be constructed from the
original graph and so the guiding
example to creating this passer graph is
the energy preservation of vectors so if
you have a vector X which has values on
every vertex of the graph its energy is
defined as X transpose LX which can be
shown to be this formula where wuv is
the weight of each edge and xu + XV are
the values of
X at the endpoints of that edge or UV so
the upper and lower bounds of this
energy define the quality of
preconditioning of The Associated
laplacian l and Q so this is a
fundamental observation that the
condition number of Q inverse L is
related to this lower bound and upper
bound on the energies of Q so this is
what connects the corresponding
laplacian q q + l with their graph
versions you know with eq + e l to
achieve a low condition number however
requires more edges than just a tree so
it turns out that a tree isn't enough
and recent theoretical results by a
number of people have shown that you can
build something which is more than a
tree which has more edges than a tree
it's not a tree anymore however it can
be constructed in about Oh of n log in
time but the drawback is that they've
never actually implemented any of these
constructions because well it's not
clear why maybe it cannot actually work
that well in practice but the
theoretical results are very strong and
these are leaning towards close to
linear construction and solve x so an
important question is why how does
preconditioning help in applications so
here we consider the example of a simple
image colorization problem on this
spiral so we want to propagate the red
and blue strokes along the arms of the
spiral if you just use a naive Gauss
Seidel iteration this is what we get
after three hundred iterations so the
colors have barely propagated you know a
little bit and in fact if you if we now
run it for thousands of iterations it
won't make much progress beyond that so
this has been mainly due to the very
local nature of Gauss Seidel propagation
next if we use the ABF solver which was
work with rick Zaleski which we did in
2011 that's a hierarchical coarse to
fine solver we can see that it makes
much more progress in a single iteration
as compared to Gauss Seidel so the
colors get propagated you know well
along the spiral but we can see that the
red and blue colors start to fade away
after you know a certain length finally
this is the result of propagation with
our new preconditioner which is which
will show is much better preconditioner
than the ABF and sure enough so after
one iteration the colors are propagated
perfectly so that's why preconditioning
is powerful it allows us to take care of
the low frequency or stiff modes which
correspond to long-range interaction
between the unknowns in the system and
the better the preconditioner the faster
this propagation happens across the
entire system so our solver adapts much
better to the geometric structure of the
problem and that's why the strokes are
propagated much better as compared to a
BF 4 Gauss Seidel so our solver combines
ideas from multigrid and combinatorial
preconditioning the main problem with
multigrid in terms of efficiency is the
increase in bandwidth at cosi levels so
what we do is to overcome this issue by
interleaving the cautioning and with
specification steps so we take the
co-signing ideas from the multigrid
literature and combine it with the
specification ideas from the
combinatorial literature but instead of
starting with a tree as the
combinatorial preconditioning literature
does what they do is to start with the
tree and then add some edges to it we
start with the original graph and drop
some edges so in practice this is a much
more efficient approach because
constructing a tree itself is the event
log and operation and then adding some
edges to it is even more work so we have
something much more local which can be
off
so so as a result our solver actually
runs fast in practice so to understand
our solver better let us consider the
colorization problem again on this
simple image of a spiral so this is a
visualization of the five-point
laplacian defined on that spiral so the
edge weights here are inversely
proportional to the magnitude of
gradients between a pixel and its four
nearest neighbors stronger edges appear
fatter and weaker edges are thinner the
connections are therefore stronger along
the spiral then across it we first
greedily select a an independent set of
fine variables which are shown here in
red so no to find variables share an
edge the complementary set of variables
which is shown in black will form the
cost level system which is which would
be about half the size of the fine level
system these operations correspond to
permuting these operations essentially
correspond to permuting the laplacian
into the coarse and fine variable sets
then we are eliminate the fine variables
using gaussian elimination so this is
the P transpose LP which is similar to
the galerkin elimination in multigrid
and this results in a block diagonal
matrix where the fine component
component is diagonal because none of
the fine variables are connected to each
other and a cost level a cost level
component which is itself a laplacian so
now we can recurse on this and create
smaller and smaller systems but are just
requesting on this using gaussian
elimination will quickly increase the
bandwidth or the connectivity so what we
save in terms of the number of variables
at the course level is offset by the
increasing bandwidth of the Laplace
so this this this is the bandwidth
growth problem that is common in multi
grade so instead what we do is to use
ideas from the combinatorial literature
and specify the cost level matrix so we
remove non-critical edges which are
shown here in drag so unlike our
previous lock which also uses
classification the key difference with
our new solver is that the edges which
are removed are not regularly spread
through the domain so we choose edges
based on a certain criterion which gives
a much better preconditioner and the
process of specification reduces the
number of connections between variables
and the resulting specified matrix is
also a laplacian but it has much but
we're able to control the bandwidth
growth in the course level matrices then
we repeat the above steps to form a
hierarchy of subproblems so we mark the
final cost variables eliminate fine
variables specify and keep doing that
until reach a certain maximum size of
system at which point we can bring in an
exact solver to solve it accurately so
now let's analyze this a little more
which edges do we specify so the key is
that we specified those edges which
allow us to maintain a low condition
number between the specified matrix Q
and the original matrix AB so we show in
the paper that removing the weakest edge
of a triangle allows us to obtain a
condition number of 3 between the
specified and unspecified matrices
therefore what we do is to search
through the graph for all triangles and
remove edges with minimal wait until a
triangle free graph is obtained so in
the perfect case that we managed to find
independent
angles this would lead to a condition
number of just three between the
original and specified graphs that's not
always the case in practice but this is
our starting point but now if we just
remove just drop edges this causes a
deterioration in the overall condition
number so we improve this by
compensating for the loss of edges so
our compensation formulas take the
weight of the drop edge and add it to
the two remaining edges in each triangle
so this leads to a very simple weight
update formula just drop that edge and
add its way to the other two edges of
the triangle this ensures that the
resulting matrix is still a laplacian
and we've got some justification for
this formula in the paper while we don't
have a proof empirically we observe that
the resulting condition number by adding
compensation changes from a value of 3
to about 23 without compensation and 2
with compensation and finally we use a
greedy strategy for the coloring so what
you see so on either end of a dropped
edge are marked as fine which means that
to find variables can never be connected
to each other and the third vertex is
marked as course so this ensures that
the fine variables are independent and
we observe that on average for 2d images
and 3d surface meshes the number of
course variables is about fifty to sixty
percent of the overall number of
variables so this means that the
hierarchy will eventually terminate we
don't we don't have a huge number of
levels its log of 0 of log n levels
so we just let's compare our algorithm
on this image against the ABF approach
to show why a non geometric approach
helps the ABF approach has a
non-adaptive specification and coloring
scheme so it just takes it has this a
half octave selection which means that
at every step half the number of
variables as chosen as fine and the
other half has caused using a red black
skin so it's clearly seen that so this
is the result after a few levels of
cautioning and we see that it no longer
follows the geometric structure of the
problem on the other hand this is our
solvers geometry after a few levels of
cautioning and we see that it maintains
the shape of the spiral and this is key
to fast diffusion of the solution
through the domain that is to maintain
the relationship between the strong
clusters and not destroy them as we go
to the coast levels because these
long-range connections are the ones that
take the longest to be resolved and you
want those to be preserved as long as
possible in the hierarchy so some
results so we first start with
homogeneous laplacian of increasing size
so here's this is the value of n we go
from 10 to 4 all the way to a million
and this is the original condition
number so we see that it's off n and
this is the this is the condition number
of Q inverse air so with the
compensation it's almost constant and
this is what we would expect because in
the case of a homogeneous problem our
approach coincides with geometric
multigrid which is known to be optimal
in the sense that the condition number
does not change with increasing problem
size so one thing to note is that
without the compensation that is adding
the dropped edge to the other
two edges of the triangle the condition
number does not stay constant because
that's not geometric multigrid it slowly
increases as problem size increases
which means you need more and more
iterations for larger problem cycles so
we call our solver HSC for hierarchical
specified and compensate so here we
consider some linear systems arising in
2d problems H high dynamic range
compression colorization edge preserving
decomposition here we show condition
numbers of a number of different methods
these are state-of-the-art multigrid
methods combinatorial multigrid lean
algebraic multigrid and algebraic
multigrid a BF is our previous work and
GM g is geometric multigrid so we are
able to reduce the condition number from
0 you know a million over a million to
less than 10 the top offering methods
are shown in bold in each row and in all
cases so this goes from homogeneous
slightly less homogeneous and highly
discontinuous in all cases were able to
reduce the condition number
significantly and here are actual wall
clock time so this is not very useful if
if the solver takes a long time so here
we compare these problems and this these
are the number of unknowns so millions
of variables to the direct solver in
matlab as well as the other methods
where this way it did not converge
within a reasonable time is where we
don't show any results so we achieve
about 2x2 about eight x speed up over
the direct solver and about two times
faster than well 1.5 times faster than
communitarian multigrid which is a
state-of-the-art combinatorial
preconditioning based method
now we compare the against the dark
solver for two applications in 3d mesh
processing so this is geodesic distance
computation so here the idea is to find
the distances from a target point on the
3d mesh and red is closer blue is
further away and this is done by
diffusion so a single point is marked as
the source point and then heat diffusion
is performed for a certain number of
time steps and this gives us the
distance so we measure that for a number
of meshes of different sizes and that's
the time taken for by the backslash and
this is our set up and solve time so in
terms of the solve time we are up to 16
to 20 times faster than the dark solver
and here's an example of mesh denoising
where it's basically a smoothing process
and here we tested for meshes up to 14
million vertices in size and achieve a
similar speed up of somewhere between
three to 20 times faster
computing the smallest eigen vectors of
a matrix uses repeated linear system
solves so our solve work and therefore
also be used to accelerate I Gonsalves
what we do for that is to use a coarse
to fine scheme where we take the closest
level laplacian that we generate by a
hierarchy compute the lowest few
eigenvectors and that propagate it up to
the finest level and then refine it at
the finest level through some smoothing
so here's an example of segmentation and
we can get about 2x speed up over matlab
ike's and unlike is our solver is very
easy to code up and then it's it's
amenable to GPU acceleration for example
whereas my job sites which is based on
you know some lunch choice type methods
may be much more difficult to to to code
in on GPUs so here we show the error
reduction as a function of iteration our
solver is shown in black so this is the
error of the this is the residual of the
system which is the norm of B minus LX
where when we are solving LX equal to B
and four different problems actually a
compression colorization and at
preserving decomposition so we can see
that the error drops significantly you
know at every iteration and within 10 to
20 iterations we achieve a error
relative residual of about 10 to the
minus 6 so in the paper we also show a
simple heuristic which allows us to
quickly update the preconditioner for
diagonally shifted versions of the
original laplacian and this situation
often arises in practice so it's I I
think it's an interesting empirical tool
for solving systems repeatedly so
finally cotangent laplacian czar widely
used in mesh processing and when we
submitted the see graph paper some of
the reviews didn't like the fact that we
never mentioned cotangent laplacian and
the main problem is that these are not m
matrices because of the way the edge
weights are computed those are co
tangents of angles and when the so the
edge wait for this edge for example is
the sum of the cotangent of the angles
opposite the edge and if that angle is
obtuse then there's a some edges can
have negative weights and so none of the
actually none of the existing multigrid
methods or very few of the existing
multi methods work well and none of the
combinatorial methods work for
non-emergencies so in its original form
our method cannot be directly used but
what we do is to simply drop some of the
positive off diagonals and create a new
M matrix and then build a precondition
on that M matrix and just use that as a
precondition for the original non M
matrix so this is a heuristic but in
practice it works quite well and it has
been used in the multigrid community
empirically it gives us good results for
match processing problems so here we
compared to the direct solver for four
meshes of increasing size and we get
about twenty to thirty times speedup for
matrices of this type as well and while
using significantly less memory so one
of the problems with the dart solver is
that the memory usage goes up
significantly so it's many gigs for a
mesh of this size so we use less than a
third of that memory so details can be
so this is not in the paper the
cotangent of Russian result but the
details are but we provide examples in
our test code on how to use this
so in summary we've developed an
efficient solver for laplacian linear
systems which can be used in a variety
of 2d and 3d problems including
continental plush ins and the key is to
combine ideas from the multigrid
literature on higher art building
hierarchies and specification steps from
the community oriole preconditioning
literature and the result is a solver
that is fast in wall clock time and this
code on this web page if you'd like to
try future work will involve better
extensions to non m matrix laplacian so
so in the case of cotangent laplacian it
works fine but when we tried for other
kinds of matrices which involve very
long range derivatives higher-order
derivatives i'm sorry then we find that
our construction doesn't work well
actually sometimes it just fails because
the resulting if you just drop the
positive edges then we just get a very
poor preconditioner we don't drop the
positive edges the resulting
preconditioner just is not positive
definite anymore so it just doesn't work
so we'd like to tackle that problem and
we'd like to derive some theoretical
guarantees on how good our solver is
whether we are actually linear time in
construction for a wide variety of
problems and finally we'd like to
consider other applications that use
laplacian it's not just in you know in
the particular applications we've
considered here so yeah thank you
so what is quite yeah any questions so
you talked about you know the set of it
solve obviously we use the setup if
you're solving with the same lifetime
sorry yes you could reuse it if you
update diagonal yes yeah I should
qualify that only if the diagonal is
updated with a positive so if the tag
knows updated so that the resulting
matrix the scalar which is it's sorry if
the diagonal is updated with a single
scalar that's how we tested it wait so
something like L plus T times identity
okay don't we just use the the old wrong
condition preconditioner so we updated i
right it just depends on the value of T
so the closer it is to 0 the better the
preconditioner then as you go further
and further away the condition number
will just deteriorate and at some point
it will just take many then you just
take more and more and more iterations
to get literally recently regions oh no
it doesn't it doesn't fall off a cliff
it just well okay that's hard too it
probably will fall off a cliff at some
value of T when something funny happens
to the system at coaster levels I mean
maybe so what happens is you have this
diagonal its effect is starting to
defuse at the next levels and we are
ignoring that basically so at some point
there might be some phase change and it
could fall off a cliff so I'd say you
probably can't have two larger value of
T before it would break down yes so I
haven't tested it you know accurately
for different value for this fish change
behavior but it could happen
you mentioned you wanted to prove
various promises right a lot of the
challenges for them to grow right so so
the key thing is we so first of all we
want to prove that so we are using
b-cycle iterations which if you're
familiar with multigrid literature that
means the way we solve it is you go from
fine yes you do some smoothing then find
the residual take it to the cost level
do some smoothing take it to the coast
level do some smoothing esta go down all
the way then go back up all the way and
so this is one v cycle and that reduces
the error b- LX by a little bit and then
we just keep repeating recycles still we
have a error below a predefined
threshold those are the iterations i was
showing for the error reduction so now
the question is if you have any
different kind of matrix and will our
preconditioner make ensure that this
recycle will converge so we keep
reducing the error at every iteration
the existing the problem is we don't
know yet whether you could have a
blow-up of the error at the causal
levels we haven't proved it we're just
seeing it empirically but it could
happen that your error just doesn't
decrease for certain kinds of graphs and
then you'd have to do hundreds and
hundreds of smoothing iterations to or
thousands to reduce the error the
combinatorial preconditioning guys so
they use something called a w cycle so
they control the error at every
iteration at every level by solving so
at every level instead of just doing
smoothing so at this level there's a
full B cycle then at the next level
there's another v cycle which goes down
and up and the next level there's
another v cycle so that's called aw
cycle so it's significantly more
expensive but they can show that if the
coasting rate is fast enough then the
overall cost is off n but our coasting
rate is only fifty percent so if we do a
double you psychic
we are no longer linear so it's this
trade-off between how well we can keep
the condition number controlled from one
level to the next versus the size of the
system at the course level and the
preconditioner guys are the
combinatorial guys they can have these
theoretical proofs about you know that
they can cut it from they can cut they
can control a half a stick and caution
and trade that off against how much time
they spend solving we don't have that in
our current system so we have to find a
way to take their proofs and translate
it into our language where we can't
control the rate of co-signing as well
as they do do w certain you will nearly
have improved yes exactly but as it
stands right now at fifty percent
co-signing rate we couldn't do that it
would not be linear anymore for sure it
would be n log in at that point but if
he could somehow yeah exactly sorry so
if we could use a w cycle we are done or
if we could yeah then we are done or if
we could ensure that the condition
number is say one point you know epsilon
and then we show that this one point
epsilon cannot blow up as you have log n
levels then we are done too but if you
have condition number of two and you
have login levels then you have to to
the login which could be 0 of n so I
have a general symmetric positive matrix
right it's you know the place until
something right you know my dear look or
um so it's is it so by laplacian i guess
you mean an M matrix here yes yeah plus
and what's the plus something is a
diagonal or
they simply structure many local
interactions yeah and so I'm very
interested to explore those kinds of
systems the short answer is i'm not sure
but again if the system is just a small
perturbation of the laplacian it seems
like you should be able to eat reuse all
the work for the laplacian and do a
little bit more to get l plus something
solved on the other hand it depends on
the structure of this additional guy No
so in fact this is a very hot topic in
apparently I don't know but I heard it
from some applied map gade that this is
the big topic updating preconditioners
where they want to have cheap updates
for instead of having to rebuild
everything so right now everything is
very static you change your matrix by
one entry you just recompute everything
which seems inefficient so there isn't
much out there actually so I was when I
was doing the heuristic for the updated
diagonal I searched for a lot of the
literature and there isn't even anything
on that so for general matrix there
might be even less that there wasn't
anything actually yeah
ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>