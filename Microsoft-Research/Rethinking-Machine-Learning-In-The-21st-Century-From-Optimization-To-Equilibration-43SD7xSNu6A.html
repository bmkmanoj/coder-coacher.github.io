<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Rethinking Machine Learning In The 21st Century: From Optimization To Equilibration | Coder Coacher - Coaching Coders</title><meta content="Rethinking Machine Learning In The 21st Century: From Optimization To Equilibration - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Rethinking Machine Learning In The 21st Century: From Optimization To Equilibration</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/43SD7xSNu6A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright it's a pleasure to have three
the other one here shreya has been
working in AI and area of reinforcement
learning for quite some time has made a
lot of very nice contributions to these
areas he's recently been elected a
fellow to triple-a I and you know he's
going to give us a talk today on some
exciting new developments in RL thanks
um so I'm going to talk to you today
about some work that actually originated
out of looking at problems in
reinforcement learning but the
implications of this work stretches far
beyond reinforcement learning and
includes pretty much all of machine
learning so the main idea I'd like to
explain to you is what I believe is a
new framework to do machine learning and
it builds on one of the most important
foundations of machine running today
namely optimization but it takes machine
running well beyond the scope of
optimization to something i call
equilibration I'm going to explain to
you what that is and why it's going to
be extremely important for machine
learning to move beyond optimization now
when I talk to my colleagues about this
and I've talked to quite a few of them
and I've talked to a lot of smart
faculty candidates we've come to UMass
and instinctively the reaction is what
could be more general than optimization
and some people say well maybe it's just
math right the next level of abstraction
and so I want to show you that is not
the case in fact when you do
optimization in machine learning you are
really incorporating some very specific
assumptions about the world and these
assumptions are increasingly untenable
for the challenges that machine learning
faces in our increasingly networked
society in things
cloud computing the internet and so on
and so I really want to make the case
that the world has changed fundamentally
as all of us know and so to really
grapple with that we have to really move
the intellectual tools of machine
learning beyond optimization of course
when we talk about optimization in
machine learning we're really interested
in not just applying optimization but we
want to scale to very large problems and
so there's this notion that has become
popular in work and optimization
particularly convex optimization which
is to get some sort of scale-free
property that is you want an algorithm
that scales incredibly well to high
dimensional problems and so there's this
phrase i'm going to use from a recent
overview of optimization by sebastian
buick at princeton almost dimension free
optimization how explain what that is
but then I'll show you how intriguingly
our new results seem to show that this
idea applies in this more general
setting as well we understand the sort
of fundamentals of why we get this in
certain restricted cases for
optimization I think we don't understand
very much about equilibration so it
could be a sort of interesting challenge
see why why this is happening so but
before I get there I want to sort of
give you a little bit of background both
into what sort of stuff that people do
in machine learning where does this work
come from in particular many of you may
be familiar with parts of machine
learning but perhaps not reinforcement
learning or you may not be familiar
feeling at all so I'm going to cover
some background some history and before
I actually get into equilibration so I'm
going to begin with telling you who I am
on and I direct the autonomous learning
lab at UMass it's one of the oldest
machine learning labs in the country
the lab has been more or less in
continuous existence for over 30 years
and we've had a very long and
distinguished set of students postdocs
and visitors our first post doc was
someone named Michael Jordan not the
basketball player famous machine
learning researcher at Berkeley probably
the most cited machine learning
researcher he came to us as a
psychologist who had not done any work
in machine learning and as legend has it
one day he apparently said I'm going to
learn a little statistics he breathed
the air of Amherst and decided to become
a statistician and the rest is history
so I think Mike's at a high standard and
we've had really excellent students our
first PhD student was rich Sutton who is
a pioneer in the work of in
reinforcement learning who never stopped
working on his PhD dissertation his
dissertation was nineteen eighty-four on
temple the first learning he has a paper
in icml 2014 on you guessed at temple
difference learning because he really
wants to solve this one problem most
determined researcher I have ever seen
in my whole life so I'm going to talk
about his particular solution and why I
think ours is better you can download
all the publications that we've done
over the past 30 years at this website
we're also part of the school of
computer science it's a very
long-standing computer science program
and the graduate program that I direct
has been in existence for almost 50
years now so we were proud of it and so
this is the lab as it stands now my lab
co-director is Professor Andrew Bartow
or ND he retired a couple of years ago
so pretty much I manage the lab on my
own we have a very large number of PhD
students we've always had a large number
apart 14 right now and and that's pretty
demanding
job and I want to single out seven
students with whom have done this work
Tommy CJ will Ian Steve neck bow and and
you who's not here and really a lot of
the work I'm going to talk about our
ideas that they get the credit for and
I'm sort of the person presenting them
but the work could just not have been
done without these really bright
students so on let me start with a
project that we've started recently on
to give you a sense of the kind of work
that people in machine learning do using
optimization before we move on to talk
about equilibration so here's a project
that we get funding from NASA and from
the National Science Foundation and what
it is is we're getting data from a wrong
way away we're getting data from
curiosity the rover on Mars it's a sort
of traveling Science Laboratory lots of
scientific instruments on curiosity the
specific one we're working on is is
producing what's called spectroscopic
data using a process called
laser-induced breakdown spectroscopy or
libs it's basically a very powerful
laser and it can be pointed at rocks and
it pulverizes them but you know it's not
like terminator it's it produces a very
small amount of debris there's a plasma
generated you can read off the energy
and it produces these kind of readings
you see they're very high dimensional
and it's been back to earth now our
problem is to decode what the rover has
actually zapped and now this sort of men
in black picture you see here is me
standing with two of my students Tommy
and CJ in professor dar badar slab with
the identical laser on earth so we can
zap rocks on earth to our hearts content
and we can compare the readings we get
with those on Mars so you can think of
this as what's called a transfer
learning problem which is we have data
from Mars we have data from Earth
we had somehow figure out if what we're
seeing on Mars has anything to do with
earth what's different and so this is an
interesting machine learning problem one
that has very deep implications for
understanding planetary science and you
know whether ever there ever was life on
Mars and so on so on we came up with a
new algorithm recently I won't give you
the details of the algorithm but I'll
give you a sense of how it works just to
illustrate how we can use optimization
to do machine learning on interesting
scientific problems so a very canonical
way of thinking of doing machine
learning is you take some data and you
project it on some well-defined
structure and you see what you get so if
you think of canonical methods like
least squares the structure you predict
on it's very simple it's simply a linear
subspace so you can think of in the
dollar sign this vertical bar would be a
linear subspace you simply project your
data on that and see how well the model
captures the data in our case we're
doing something more interesting we're
projecting it on a mixture of spaces so
there is leaner spaces like this bar
there's this curved space in the dollar
sign and we have an unknown mixture of
these but each of the spaces that we
project on is simple but they may not be
clearly in so the idea is we project on
these spaces we see what part of the
original data is captured by this and
then we produce a lower dimensional
description of the data this is an
example of unsupervised learning this
goes beyond sort of previous work in
this area where you could either project
on the linear space by itself or one of
these nonlinear spaces but not on an
arbitrary mixture of them and so how do
we solve this we write out this problem
as an optimization problem and I won't
give you the details of how that works
except to say that with some cleverness
you can actually make it not just into
an arbitrary optimization problem but
something that is actually convex which
basically means we have good ways of
solving it
okay and the results from the algorithm
which I will not present are much better
than our previous work so we've actually
made some progress here in this so this
is an example of a large number of kind
of projects that people are doing in
very different applications of machine
learning building an optimization so
clearly optimization is a very useful
tool why do we want to go beyond it so
i'm going to show you two models and the
one on the left is what we know how to
do and what lots of people in machine
learning already do which is
optimization okay so we take some
problem like the transfer learning on
Mars problem we construct an objective
function which maybe looks like this
this is a very simple quadratic
objective function to solve this problem
we walk downhill we minimize the
function because its convex there is a
unique minimum almost any algorithm you
implement will work on this so it's not
very challenging it's challenging to
design a fast algorithm particularly
when the space isn't two-dimensional but
say a million dimensions and
particularly when you know to walk
downhill you have to know the gradient
of the function and sometimes you can't
recover the exact gradient you can only
get a very noisy gradient so you get a
direction that you think the function is
decreasing in but it might not be the
right direction so this the uncertainty
in computing the gradient can complicate
the algorithm and the number of
dimensions could be very large so even
though its convex it could be difficult
and so you need some cleverness but
because the problem is inherently
well-defined you can come up with some
clever solutions and i'm going to show
you later one such solution ok so we
have some function small f we're going
to minimize this function and and then
we're going to find the minimal ok that
we know how to do what's this over there
this is something i assume very few of
you have probably seen this is a new
model called an equilibration model so
as the name suggests it's basically
trying to compute some equilibrium
solution and the mathematical origin of
this model
from physics in mechanics we have some
object like a ball sitting on the
surface you want to find its equilibrium
position okay so this guy called stump
Bastion Italy who developed the
underlying mathematics for this and then
the line mathematics is called
variational inequalities and
interestingly enough it turned out that
it has actually a lot of interesting
applications not just in that original
setting but to a vast number of problems
that have subsequently been discovered
and I'm going to show you some of those
later in the top so I in a variational
inequality instead of a small F you have
a capital F what is the capital F it's a
gradient in this is a specific case it's
a gradient in general it could be some
arbitrary vector field that means for
every point in the space of feasible
solutions capital left returns a vector
so in this particular case think of
capital F is simply the gradient of this
function that tells you which way the
function increases in general capital F
as well see could have many other
possible instantiations ok so capital F
tells us gives us back a vector and what
we're trying to find is some
distinguished element X star at which a
certain property is true namely the
vector returned by capital F makes an
acute angle angle between 0 90 with all
vectors of the form X minus X star where
X is part of the feasible set now if
you're like me and you see this for the
first time it makes no sense whatsoever
ok what does this being so I'm going to
explain that to you later but here's the
interesting part every problem that we
can write like this turns into a problem
like that that's interesting lots of
problems that we can write like
equilibration cannot be turned back into
optimization so it's strictly more
general and i'm going to show you two
things one I'll take a problem that we
know how to solve with optimization or
we're trying hard to solve the
optimization and I'll show you why
if we move to equilibration we can solve
it better that's the reinforcement
learning part I'll show and then I'll
move to the equilibration apartment show
you lots of interesting things that we
can do now that we understand better how
to do equilibration so that's the plan
for the talk but you know after i did
all that i said it's not interesting
enough so you still don't know who i am
right so i can tell your story when I
taste the machine learning class I tell
stories and what happens at the end is
the students don't remember much machine
learning but they remember all my
stories and I figured some good comes
out of that at the end and they actually
remember perhaps even more than I think
they remember and so you're gonna get
some stories okay so Who am I where did
I come from why did i do any of this and
so on right so when you become a
scientist there's usually somebody who's
a big influence on your life right
various different people have come into
science for all kinds of reasons in my
case it was my dad so here's my dad and
and he's actually an archaeologist very
special kind of archaeologists he
deciphers ancient languages or tries to
and one of the languages that he's
actually successfully deciphered is this
language it's called tamil it's a very
old language but spoken by over 100
million people in the south of India
that's his name I Robert them maha they
weren't written in Tamil is very pretty
script the one he actually deciphered is
the one on the right this is Tamil as it
was written in the 5th century BC so
they were writing all over the caves in
the south of India that nobody could
read because they didn't know what it
meant and my dad sort of got interested
in this and said all that sounds like an
interesting problem he was actually not
supposed to be doing this he was working
for the Indian government but that
didn't seem to take up very much of his
brain so in his spare time he decided to
dabble him this and he got hooked so and
he's worked out in a long time but he
wrote a very monumental book that was
published by Harvard University oriental
series
Michael Witzel at Harvard who's the
valence professor of Sanskrit is the
editor and that's the book it's a very
impressive book now what's impressive
about this is that my dad is not on
university professor he has no college
degree he never successfully finished a
degree all of this is done in his spare
time so it's impressive that someone
simply dabbles in this can produce a
scholarly work that harvard university
oriental series considers it woody to
publish it's the first impressive thing
or my dad so as you can see it made a
big influence on me and now my dad
himself was motivated that's my dad two
weeks ago very proudly holding the
second edition of his book and he
himself was greatly influenced by
Professor M&amp;amp;L who founded the
Linguistics Department at Berkeley and
lived to be a hundred and one and was
publishing papers to 100 so I wish my
dad a long life and hope he loves to 100
and publishes papers each year he seems
to write more papers than I do with no
NSF funding and no grad students so
that's interesting and okay so the other
problem that my dad worked on is the
Indus script now this is a mystery this
is an ancient civilization that
flourished 2500 BC and we have data from
the civilization in the form of these
tablets nobody knows what they mean well
50 claims of decipherment have been made
none of them agree we don't know what it
means we have this text we can try to
parse it lots of theories about what it
could possibly mean nobody knows so my
dad says he finally figured it out after
40 years and he's writing a hundred page
paper on it I wish him the best of luck
and but you can ask Ken machine learning
solve this problem ah and my dad has
been telling me for a while you know you
do machine learning why can't you help
me like a good son I shall listen to him
but I didn't my colleague at ghostly
Washington Rajeshwari did listen to my
dad and wrote a paper that I should have
written in science what did they do
some people said including professor
Witzel at Harvard maybe this is not a
language maybe we're miss reading this
whole thing its business correspondence
in some ancient shorthand but it's not
language how would you check now this is
a problem that machine learning can
address and so this is the paper they
wrote it's a very simple idea in
appearing science well if its language
it should compress like language that
seems logical so they took long you know
sequences of different lengths the
computer what's called the entropy which
is simply how well does it comprised
they compared the entropy of in the
scripts tax to lots of known languages
ancient and modern they compared it to
entropy of DNA sequences of similar
length and fortran code and what did
they find well fortran code as we know
is very very structured compresses
extremely well better than any language
no surprise DNA is completely well not
completely random but much more random
doesn't compress well but in this group
compresses like pretty much most
languages that's right but this is
basically saying look at compresses like
language so maybe its language now this
is of course raised a lot of controversy
and see people said well is this a true
test I don't know looks pretty
compelling to me what I think it's
interesting this is something that
machine learning candle okay so that's a
little aside now this is very nice
saying by volume will walk home which is
a lot of influence in machine learning
which is in Latin it means do not
multiply entities unnecessarily it's
rather crudely translated these days as
Occam's razor a phrase I just do not
like because first it doesn't mention
that William said it it mentions Occam
Occam's just a town near sorry who cares
second it's sort of actually you don't
really know what William said but this
is what Bertrand Russell said William
said so we'll assume that this is
correct so we're going to use this as a
criterion right we want things to be
simple and beautiful all right so why
should machines learn let's start and
answer this question really quickly so
when I was a Carnegie Mellon my advisor
was Tom Mitchell
and I was sort of there as a visiting
grad student cuz he had just moved there
and I was finishing up so I spent three
years at Carnegie Mellon and I listened
to a lot of wonderful lectures by herb
Simon and Nobel laureate in economics
who was very interested in psychology
computer science machine learning AI and
so on he has this question why should
machines learn we can just program them
why should we bother learning you know
we humans learn because it's very
painful to program them and but it takes
forever for them to learn anything as
anyone like me will tell you who teaches
for a living but he came up with this
nice definition of learning which is
very useful right so learning is changes
in the system that are adaptive so that
it helps the system do the same task
better the next time wrong we're going
to use this definition so why should
machines learn can we think of an
example so and so when I graduated I
went to IBM Watson Research in 1990 they
did not have a machine learning group
those days it was not clear whether
machine learning was good for anything
unlike today and so they threw me in
with a new robotics group that they've
just formed but I known and you know
robotics two years later I produces book
and it went on to be quite influential
and Sean mentioned my triple AI
fellowship nomination and I was sort of
gratified that I was being nominated in
part for the work that I did at IBM all
those years ago because it was the first
book of its kind on robot learning which
has since become a major area so I'm
going to use this to motivate the
reinforcement learning problem and show
you how we solved it with equilibration
so in the reinforcement learning problem
we have the problem of teaching
something how to do a task like this no
the way this task was solved in the book
described by Dean Palmer loaded it was
they constructed a neural net that took
samples from human drivers driving a
real ford truck input images and an
output of the steering command and they
trained the neural net and it was the
first demonstration that machine
learning could drive a truck and this
was impressive because the work before
this on nav lab was humans programming
this truck using state-of-the-art
computer vision in those days and they
had failed and the truck was barely
moving with
computer vision but with machine
learning could drive 60 miles an hour so
it was very impressive so here's an
answer to Simon's question machines can
do things by learning that they couldn't
do with programming of course now we
know how to program machines to drive
and Google is putting a lot of money
into this so the world has changed but
we don't solve the driving problem this
way we saw the writing problem in a
different way we sort of learn to solve
driving by solving a sequential decision
problem and I'm going to use this phrase
by a famous Danish philosopher
Kierkegaard and he says life must be
lived going forwards but can only be
understood going backwards so what does
that mean this is the essence of the
solution to the driving problem how so
let's look at the first machine learning
program that was ever built this is by
Samuel many people in my machine
learning class have no idea who Samuel
is so here's here's a history lesson
author samuel was a researcher IBM who
built the first machine learning program
ever demonstrated in 1956 on PBS this is
a photograph the PBS show where the
computer is playing a game of checkers
you have to realize how revolutionary
this was nineteen fifty six people were
using computers to build atomic weapons
largely after the Second World War
nobody knew computers are good for
anything else the president of IBM TJ
watson said he thought the world needed
five computers that's it he didn't think
there was any need for computers beyond
five and of course this was a wake-up
call and TJ watson said wow this is
going to raise the price of IBM stock by
10 points on wall street that day and it
did because all of a sudden people said
wow machine learning computers can do
interesting everyday things but what was
truly impressive was it learned to play
on its own there was no teacher how did
samuel figure this out so he used this
algorithm called temporal difference
learning which my colleagues at UMass
Andy Barr Cho and his former student
Richard Sutton have done a wonderful job
studying over the past three decades and
written this beautiful book on
reinforcement learning which has been
cited a quite a lot and has had a lot of
influence in many fields I'm going to
briefly explain to you how T dealer
nning works why the original
wasn't quite right and why equilibration
led to its solution so let me contrast
the way TD learning works with least
squares let's take a very simple problem
from the book a problem that we all
solve every day let's say I'm going home
and I need to make sure i get home on
time so I leave my office and I think
maybe I'm going to take half an hour to
get home and I'm going to make sure that
reach it in time because my wife has
told me that dinner with our neighbors
is at this time and you better not be
late as usual so I say okay I'm going
take half an hour I start home and then
somewhere along the way around a
colleague I start talking to him and I
by the time I get to my car I think oops
it's gonna take longer or maybe you
ain't gonna take 40 minutes to get home
and then I start driving there's less
traffic on the highway than I thought
then I realized i'm going to take me
that long maybe 35 minutes i keep
revising the estimate eventually I get
home and I know exactly how long it took
me imagine you're doing this over and
over and over again well you get the
error signal between your predictions at
every point and the final outcome I
noodley squares well but you shouldn't
solve the problem this way you should
solve it using TD learning which gives
you the error signal right away but
every time I teach the machine learning
class asked the students how would you
solve this problem by getting the error
signal right away and nobody ever cracks
that unless they know the solution
already so it's a really non-trivial
step so TD learning compares the errors
over successive steps so you set 30
minutes in the beginning then you revise
your guests to 40 it says oh there's a
difference of 10 I can learn from that
but that's crazy because that 40 was a
guest to still it works that's the
amazing part because it turns out that
original error signal from 30 to the
actual time can be decomposed into the
sum of temporarily successive errors so
part of that error signal is already
available to you at the next time step
you don't have to wait to land this is a
genius behind temple different learning
used by Samuel and by many other people
the beauty of TD learning it is a simple
algorithm you don't need an advanced
degree in math or statistics to
implement it a high school student can
implement it and a half so it's
beautiful and it's
simple is it true how do we ask this
question yes we are trying to predict in
this particular example we're trying to
predict how long it's going to take us
to get home so at every point in time
like I'm more office I reach my car I'm
on the highway and one another highway
I'm getting closer to home when I'm
getting home so that I have to make sure
that I have an accurate sense of how
long it's going to take me to get out
this is an example right for Checker's
would be you know which what is the best
move I have to make here all of these
are sequential decision problem is
that's correct so at every point you
observe the state of the world and you
have to make a prediction and Eve yes
that's correct you can think over to the
martingale and but you don't have the
outcome like for example how long did it
actually take me or did I win the game
or lose the game until the end but you
still weren't too long even though you
don't know the actual outcome that's the
difficult part and TD learning shows you
how to solve that problem and the reason
it's important is because many many
problems that we solved every day as
human beings as animals involve
sequential decision that's quicker God
statement we live in the world sick data
comes to us sequentially the world
reveals to us information sequentially
that's the only way we can actually
learn because we don't have a huge table
that somebody's stored in a database
that we can process the data comes to us
sequentially so it's an important
problem for us to solve yes you have
another question noting different
learning algorithms it sounds like the
predictions are the same a different
points of time the error signal is
completely different in one case you're
comparing your prediction or each time
with the final outcome the other case
you're comparing your prediction age
time with the prediction you made it the
next time step so you're learning from a
completely different error signal
whatever done the lying machine learning
technology is
maybe your prediction is the same buddy
in the second situation you have a
better estimate of how accurate is your
prediction well in the second case
you're getting your error signal right
away you don't have to wait maybe it
takes you five hours to get home the
first one you have to wait five hours
you're not learning anything right the
second when you're learning at every
time step which is the real improvement
okay so but does this work is this
really right now the neuroscientist are
all excited because they have lots of
data showing the brain atrophy does
compute TD signals I won't go through
this literature but this is the analysis
unfortunately TD learning does not work
it's not a gradient method so we can
construct really simple of grounders
algorithms have been sorry problems
where if you look at the weights they
diverge so you run the TD algorithm the
weight simply blow up this has been
known for 30 years how do we solve it
well the way we solve it is we make it
an optimization algorithm right we we do
gradient descent so we have to come up
with the gradient method likely squares
where we write out an objective function
do gradient descent and we can be sure
we might not find the optimal solution
but we'll find a stable solution a
locally optimal solution unfortunately
all attempts to do this over 30 years
have not met with success the most
recent attempt was rich Sutton's work
where he tried to do this and he came
close but I would say not quite right
because the final algorithm is not a
gradient algorithm and so this is the
solution we came up with it's described
in a very long paper that I cannot
explain here sorry unfortunately it's
one hundred and twenty six page paper
written with seven PhD students on
archive the paper is long because we
actually show that the solution works
not just to fix TD learning but solves a
variety of other problems in
reinforcement learning as well it gives
us varsity scalability safety and
reliability but I'll just focus on the
TD learning part before I motor
equilibration so how do we modify
kirkegaard we say life can be understood
by going backwards in the duels space
what does that mean so let's understand
how to do dimension free optimization so
we're going to use this cute
trick by two Russians nemirovsky and you
didn't called mirror maps or mirror
descent uh they wrote this they
developed this in the 80s but it sort of
the book was obscured nobody understood
what they were talking about till about
2,000 and it became clear so here what
you do is you don't do gradients the
original space you map to a dual space
and do gradients the dual space if you
do that they showed you get very nice
properties we're going to use this
string now I won't go into the details
of how to map into the gradient space
except to say roughly what you do is you
find a suitable function like a strongly
convex function you compute its gradient
and that's a one-to-one coordinate
transformation on the original space and
you pretend that that's your new space
now by choosing that function carefully
you can get lots of nice properties so
that's what we do in our long paper now
we come to the heart of the new
framework what is a variational
inequality okay so we have a feasible
set just like optimization we have to
find and we have this vector field
mapping every point in the set it points
in a different direction we have to find
a distinguish element X star at which
one property is true what is that
property the vector F the vector field F
at X star points in such a way that
every vector of the form X minus X star
forms an acute angle with it an angle
breen 0 90 another way of saying it is
if I go in the direction f of X star but
the negative f of X star I project
outside the set and if I go backwards so
if I go from here x star go in the
direction minus f of X star I get
outside the feasible set because the
feasible set is convex if I project back
I should come back to the same place in
other words it's a fixed point if that's
true I found the solution to the
variational inequality okay you have to
trust me that this generalizes
optimization every optimization problem
can be written this way now
unfortunately you can't do gradients
here you can try a gradient methods not
very good but this is another cute trick
it's called the extra gradient method
and this solves variational inequalities
very nicely how does it work so remember
you have to find
point at which if I go outside the set
and I come back I'm in the same place
right so here's a very simple idea start
somewhere I don't know if I found the
solution let's go in the negative f
Direction project back oops I'm the
wrong place because I'm in a different
place now you can imagine keeping on
doing that that turns out to be a very
simple algorithm doesn't work quite as
well what extra gradient does it says ah
ok what you do is now at this new point
again projected the negative f direction
that gives you maybe this direction now
go back to the original point and
imagine this was the direction at the
next step it wasn't was direction of the
next step beyond that but so this is
what extra gradient does it's a clever
idea so you can combine these two ideas
and you get this algorithm called me
rope rocks recently published by
nemirovsky and what does it do it does
gradients not gradients in the dual
space but extra gradients in the
dolphins ok so two steps don't do
gradients here the gradients here and in
fact don't do gradients there do extra
gradients and if you do all of that
here's our solution to the temporal
difference learning problem which is two
changes first you should do extra
gradients and you should do extra
gradients in the dual space now my lab
coat director Andy barter looks at this
and says what have you done to my temple
difference learning algorithm it's not
pretty anymore but question is does it
work better and we claim it does in very
quick examples in the Baird example this
is the latest version of the revised
gradient TD algorithm by rich sudden and
it doesn't blow up but the variance is
very large and it has very slow
convergence this is our modification
using extra gradients and you can see
the variance is much much smaller and
the convergence is much faster on the
same problem and very we can prove and
we do at the paper that theoretically
also our bounds are much tighter here's
a robot example where we show much
better performance on a 20 dimensional
robot arm using our new method and
finally I in our long paper yes
yes this particular one we use the mean
square projected pulmonary remember
we're still doing optimization here or
we're using vis to solve optimization
problems so finally in a long paper we
show lots of other advantage for this
approach one of which is we can do
reinforcement learning on robots in a
safe way namely you can say I have this
very expensive robot I want to make sure
it doesn't fall over and I'm going to
tell you parts of the space that when
you're learning you should never visit
and we can guarantee that and how do we
do that we discover that people have
been programming robots with machine
learning algorithms reinforcement
learning algorithms we're using this
algorithm called natural gradient
popularized by somebody in Japan
calamari and I won't go through the
details how it works we discover an
interesting mirror map that makes this
algorithm a special case of murder
descent unbelievably in the last 30
years nobody had seen this and once we
discovered that and stay published in
last year's nips then it was very
straightforward to modify the mirror
descent algorithm that we had come up
with to make natural Greg game safe and
that's what we should that you can do ok
so that's basically saying that the
framework of variational inequalities
equilibration helps you take
optimization problems that are
long-standing and solve them without
even considering problems that are
beyond optimization but that's part of
the story why we should consider this
new framework the second part of the
story is it opens up a completely new
frontier for machine learning it gives
us access to problems that we can't
solve now and that's why we should do it
so let's see what those kinds of
problems are so we're all aware of the
huge influence of the internet on our
lives and you know lots of people have
said interesting things about the
internet but basically the internet is
like the free market system it isn't
designed by one person there are lots of
competing forces that shape the internet
and so you can think of the sort of
interesting way of thinking what the
internet the invisible hand of the
internet invisible hand is a famous
phrase of Adam Smith with the founded
economics
a long time ago and basically saying it
works in a sort of mysterious way
everybody is doing their own thing but
somehow the whole system works and it
took economist quite a while to sort of
show that Adams was largely correct
under certain conditions and much the
same is being used today the model the
internet lots of interesting work in
algorithmic game theory things like
selfish routing where people are trying
to understand how is it that we can have
these competitive forces work and still
produce common good on the internet my
claim is that if you're going to sort of
realize that the future of machine
learning is just tied up to the internet
all our data is coming over the web you
have to move away from optimization to
an equilibration model because there is
no single objective function and i'm
going to show you that in the series of
examples okay so you know I was
listening to the BBC at five o'clock in
the morning about a week ago and they
talked about this interesting workshop
called hack in the Box about how sort of
the Internet has just totally changed
from its design 1020 years ago and we
have all kinds of competing things like
natural neutrality security privacy I
mean all these competing forces shaping
the design of the internet and so we're
faced with this reality that there is a
no single way to model lots of things we
want to do on the web and so it's very
difficult to think of solving
interesting problems on the internet
purely by writing down an objective
function like we do an optimization what
we have to realize is they're going to
be competing forces they're all going to
be optimizing their own things and the
best we can hope to do is achieve an
equilibrium solution the question is how
to do that in a nice way we preserve
some interesting properties and it's not
just ad hoc and that's what variational
inequalities gives you so here's the
story from LA Times recently on where
verizon is telling Netflix to stop
blaming it for streaming issues and i'm
going to show you that this is entirely
predictable from our model this is
exactly what you'd expect to see because
verizon is base
saying well you know don't blame us you
know if your customers are having
trouble accessing our movies or you know
basically like you you're not routing
things correctly and Netflix is telling
verizon we're not writing because you're
kind of not doing what you supposed to
be doing and they're playing it blaming
each other game and this is because
they're competing in a different way and
they're sort of optimizing different
goals there isn't you can't sort of take
the standard machine learning approach
here and say well you know what let's
write our last function where we do a
linear combination of the goals of
Netflix and the goals of Verizon it's
not going to work we can't solve this
problem this way we have to realize that
we're beyond what optimization can do
here so so how do we solve this problem
okay and it's a story that I can tell
you in full detail but I'll give you a
sense of how the solution arrives so the
first thing to realize is by moving to
variational inequalities you are able to
sort of model of great variety of other
problems so you can solve sort of all
the standard non cooperative game
problems that people studying
algorithmic game theory in selfish
routing in areas like that you can solve
problems that are called complementarity
problems network equilibrium problems
and other things so all of a sudden if
you solve a variational inequalities
like using extra gradient or something
fancier all of these problems become
accessible to machine learning and
they're not now if you use simple
gradient methods or convex optimization
methods unless you make huge assumptions
like for example your jacobians are
symmetric or various other things like
that which are simply not true for these
more complicated problems so how do we
go from dimension free optimization
almost to almost dimension free
equilibration so we need an equivalent
equilibration algorithm which has these
nice properties and unfortunately extra
gradient is not that method okay so
let's go back and understand a little
bit over how this works right so we
start here we go the negative F
Direction project back you're a fixed
point we're done
that's extra gradient so extra gradient
takes these two steps and solves it but
unfortunately it doesn't guarantee or
doesn't really show and I'll show you
some results that get at this how do we
go beyond this all right so here's our
final modification of Coca God so what
we say is life must be lived going
forwards many times in the dual space
but can only be understood by going
backwards many times little space that's
going to sound like philosophy but it
works and here's the algorithm I won't
really try to explain it it's a little
complicated so now we're going to do
this sort of clever modification off of
mirror descent by bringing into it an
old idea from solving ordinary
differential equations it's called a
runge-kutta family of methods and we're
now applying runge-kutta methods with
miller descent not on a function that
your mopped amaizing or minimizing we're
applying it on these f mappings the
vector field mappings that are in
variational inequalities and and
basically we do a bunch of things like
that to make this work but rather than
give you the algorithm i'll give you the
results first you can do very simple
benchmark problems and here's one here's
a mapping where the mapping is a fine
but it's non symmetric so you can't
solve those of optimization so you this
is this is your vector field and this is
ax plus B that's a and you have to solve
it and here's our results very briefly
this is our runge-kutta method of a
certain order five and as we increase
the dimension the number of iterations
to convergence is pretty flat so it
looks promising but this is a simple
problem let's let's bump it up a little
here's an interesting model of the next
generation of the internet proposed by a
colleague of mine at UMass and under
gurney who is actually one of the
world's experts on variational
inequalities this paper recently won a
best paper war an ACM conference on the
internet so in this model what you have
is service providers like net
clicks amazon hulu and so on playing an
interesting kind of game it's called a
corn on nash game where they competing
primarily on the basis of quantity
they're like the opec oil producers we
don't set prices but they said quantity
then you have the network providers like
verizon AT&amp;amp;T competing on prices and you
have users and demand markets like
Cambridge you know and so on we're
choosing between combinations of content
and transport this cannot be sold as
optimization because the lots of
competing forces going on but you can
solve it as a variational inequality and
I won't get through the details of the
model it's kind of complicated and you
can look at the original paper there
lots of equations which describe the
full model ah but you can look at simple
examples for example let's take to
service providers a network provider
demand market you can pretty much work
out by hand actually what the model
should compute and you get things that
economists actually model a lot for
example what does it cost to produce
content what does it uses willing to pay
what are the transportation costs
opportunity cost and so and so for this
very elaborate model and you can show
the results the results are basically
how much content is flowing from a
particular content provider to a
particular transport provider to a
particular demand market okay this is
roughly how the model works here's our
algorithm so what it's showing here is
the number of iterations to convergence
we're comparing against extra gradient
and here's our algorithm which is much
better behaved over a fairly large zone
okay so it seems like we're getting
something like almost dimension free
converges because number converter
ations is not going away way up here's
another problem we just recently did
this is a large supply chain problem
which is sustainable and here our
results are even more intriguing
basically this is supply chains are you
know you order an ipad from apple and a
week later it comes all the way from
china to your doorsteps an amazing
supply chain that apple has set up this
particular one is interesting because it
tries to model how
supply chains could be sustainable so
these are two examples that are drawn
from twenty years of work on network
equilibrium problems so there's vast
number of interesting problems that we
can now go after using the tools of
machine learning and here's our results
look at our algorithm over four orders
of magnitude from problem size 102 promo
size million number of iterations does
not change it's pretty much converging
the one same rate now when I saw this I
was like really excited but I can't
explain why this is happening I
sincerely hope it's not a bug in the
code but we've run in several times and
so our task is to figure out exactly
what is going on here theoretically to
understand why we're getting the
scale-free behavior of course we were
building on ideas from the optimization
setting that have similar properties so
perhaps they're carrying over in this
richer setting and this is what we're
gonna explain ok so to conclude what
I've shown you is a new framework for
doing machine learning that starts by
moving away from optimization to this
more richer framework of equilibration
the underlying mathematics builds on
this idea of variational inequalities
where we're given a vector field and
we're given a feasible set and we have
to satisfy a certain property it turns
out every problem here can be written as
a problem here but then problems here
cover many other things for which you
can't really solve them as traditional
optimization problems so this is I think
a beginning of a new frontier for
machine learning when that opens up lots
of interesting real-world applications
of interest to many people in computer
science including things like you know
cloud computing and the internet and
many other things so let me stop you
thank you
yes no why well so when you say you're
solving an optimization problem you have
to define what do you mean by
optimization so when you write an
optimization problem you have an
objective function you're minimizing
something there is no objective function
in a variational inequality you're not
minimizing anything yeah but that's not
a minimization problem that's saying
find me an object with this property
that is not optimization optimization
means that you have to have a sense of
progress you're minimizing something
right so you can take a non convex
problem or a convex problem they turn
into V eyes but their RV is that don't
turn her into optimization problems
unless you have special properties so
for example if the vector field mapping
has a symmetric Jacobian you can show
that there's an equivalent optimization
problem but almost always in real-world
applications that is not the case
something that economists actually who
work in this area where last 20 years
ago that you can't solve for example
let's take the traffic equilibrium
problem every time sort of driven into
Boston this semester maybe a dozen times
and every time I come to Crane bridge
I'm a bigger and bigger believer an
equilibration because of boston traffic
and if you look at network congestion
traffic equilibrium Browns really don't
want to model them as optimization
problems because you have lots of users
sharing in space you have network
congestion or road congestion and that's
where people who study the economics of
traffic equilibria have given up on
optimization 20 years ago so no this is
not the way to solve the problem and
that's why the theory of variational
inequalities has been really richly
developed by these folks and and yes so
one way to think about it is that you
know when you're solving a problem like
this you have many different goals and
there isn't a sort of way of saying I
can take all of them and linearly
combine them
that doesn't make sense there is no
unique when you say optimization you
mean convex optimization in a queer
liberation there is no unique optimum
that you're looking for that's the point
because there are many Colombians well
so if you think of say non-cooperative
games so that is a special case of a
variational inequality where suddenly
the property you're talking about is
true that was when I solve a
non-cooperative Graham I'm trying to
find say for example a Nash equilibrium
and they could be multiple Nash
equilibria there's no notion of
optimization in a non cooperative game
but you have to remember a non
cooperative game is a special case of a
variation on equality so variational
inequalities to abstract that property
of you know the fact that there is no
unique solution in an optimization sense
only an equilibrium equilibrium solution
but it abstracts that to lots of other
problems where similar properties are
true and now the property that
optimization problems convert to vis is
true whether you're doing convex or non
convex has nothing to do with convexity
however if you have a convex
optimization problem it turns out it
turns into what's called a monotonic VI
and and because you the F mapping is
it's what's called a monotone operator
and so one way to understand variational
inequalities is that it is the nicest
generalization of convex optimization
that preserves all the nice properties
of convexity so now once you get if you
had convexity to begin with then you get
the equivalent nice property in the VI
setting but you don't need to have
convexity now that doesn't mean that
VA's are easy to solve there are cases
where we eyes are extremely difficult to
solve because you have a richer space of
things but you know you go find special
cases you you know it's whenever you
start with the framework that's more
richer you get more sort of challenges
computationally which I think this is
why it is wonderful machine learning
because we love challenges right we want
to solve hard problems
we want to find where machine learning
can really help and I think that's my
hypothesis he's outside you can ask you
that question great thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>