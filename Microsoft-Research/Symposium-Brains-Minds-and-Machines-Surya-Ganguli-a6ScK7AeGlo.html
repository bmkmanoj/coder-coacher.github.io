<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Surya Ganguli | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Surya Ganguli - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Surya Ganguli</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a6ScK7AeGlo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh it's my pleasure to introduce Syria
Syria was I guess my one of my advisors
from a PhD as well as a professor at
Stanford University and Applied Physics
and he will be talking about what subtle
some random stuff yes
there's one day everybody there's one
beer okay we'll see ya
we'll see I should be okay thanks so um
I wanted to talk about glimpses towards
a new signs of brains minds minds and
machines basically weaving together
physics computation in neurobiology and
I'd like to start by noting one of the
greatest ironies of human existence easy
start at least that small part of human
existence that intersects with nips and
that is that neural circuits evolved to
exploit the laws of physics to compute
but in ways that are fundamentally
different from that if the computers
that they themselves designed right so
they work at very disparate timescales
our neural circuits are very slow they
operate in the millisecond time scale
whereas computers computers have
nanosecond clock speeds they're
remarkably energy-efficient they
operated a 20-watt so we're all dimmer
than light bulbs literally
supercomputers operate at mega watt
ranges so they're faster they take pup
more energy yet their performance on the
types of tasks were interested in nips
at high-level AI is relatively abysmal
compared to neural circuits in humans
they have some other major differences
that are probably explanatory of this
difference in performance so the
computational path the intermediate
computational path that neural circuits
take are highly variable in analog
whereas computers are highly precise
they flip bits and very precise ways
from the beginning to the end of the
computation in a digital way information
coding and neural circuits is high
dimensional and distributed information
and computers are placed and precisely
localized registers you know where every
piece of information goes in the
computer so these are very very
different methods of computation and so
in order to to bridge this gap where
we're really going to need is a new
science of minds brains and machines
that seamlessly weaves together physics
neurobiology and computation to extract
conceptual insight into how our neural
circuits compute and instantiate those
conceptual insights and artificial
circuits I will not offer that to you
today I can this is going to be a
collective endeavor that will occupy all
of us for many many years to come but
our lab does work at the intersection of
physics computation neurobiology and we
have several vignettes that we'd like to
share with you
today so so one of the things that were
you know to two of the main ways that we
both model cognition and model structure
in the natural world are neural networks
and probabilistic generative models and
you know to understand how neural
networks compute we'd love to have a
unification of these two very disparate
methods of modeling and we've started
along those pathways in terms of
understanding the developmental
emergence of say probabilistic
hierarchal structure in both infants and
machines the other aspect of neural
systems is they operate using very very
high dimensional representations and
there's lots of interesting mathematics
operating in high dimensional spaces
non-intuitive mathematics and it's
likely that the secrets of high
dimensional spaces are incredibly
important for understanding neural
computation and so we have some results
in exploiting the geometry of
high-dimensional Aero surfaces to speed
up neural network learning also
equilibrium statistical mechanics has
had a huge impact on machine learning
you can just list the names right
Helmholtz machine Boltzmann machine
cavity method mean field theory beta
free energy there's a huge interplay
between machine learning is an
equilibrium stat mech but non
equilibrium stat mech which is the stuff
of dynamics and the stuff of life has
not had as much of an impact on machine
learning but I believe the time is right
for non equilibrium system mechanics to
start to play a role and we have some
results there that will discuss and also
we don't often have broad theories that
simultaneously consider trade-offs
between three very important physical
quantities and computation energy speed
and accuracy and we have recently
elucidated some fundamental limits in
the accuracy of communication given
constraints on energy and time that
serve as a benchmark for what we can
hope to achieve both in biology and in
machine so this is a lot to cover and
I'll just give you a highlights here but
fortunately almost everything I'm going
to talk about is published these are the
results and actually Andrew and I gave
back-to-back talks because we work
together on a lot of the same things and
so the first part of the work will be
with Andrew and Jay McClelland that the
second part will be a collaboration with
Joshua of NGOs lab the third part will
be was led by an excellent postdoc in my
lab now at Google brain research Joshua
soul Dickstein and the final part of the
was led by Shuba Neal Lahiri another
excellent postdoc in my lab and of
course I'd like to thank my funding
sources so so let's begin let's begin
with understanding the developmental
emergence of probabilistic hierarchical
structure and infants and machines so
this is again the work with Andrew Sachs
and Jamie Holland so here's a basic
observation about the development of
infant categorical knowledge okay so
even when you control for the perceptual
similarity of objects infants tend to
learn coarse-grained categorical
distinctions much earlier in
developmental time than they learn finer
scale category finer grained categorical
distinctions they can probe what
categories infants can discriminate
through looking time studies that say
age six months before they can talk and
there's actually a whole host of other
phenomena here that we've also analyzed
but for this talk I'm going to focus on
the development of categorical structure
okay and there's a fantastic review of
all of this phenomena in this book by
Rogers and McClellan called semantic
cognition so in this book and in
previous papers Jamie Collins and
company tried to simulate this process
in neural networks why do you get this
progressive differentiation of
hierarchical structure in human
categorical knowledge so what they did
was they did a very simple simulation
with an artificial toy data set where
you had a bunch of animals and plants
and fish and so on and that fed into
some common representation layer and a
hidden there and you could ask the
network various queries and the system
has various properties that are
properties of the objects in the real
world and they just trained the network
through back propagation and they did
they looked at the dynamics of learning
in the internal representation right so
each of these objects have our oven
already sent a shins the input but they
all develop correlated representations
in the in the hidden layer and they just
by visualizing these hidden
representations as they develop over
developmental time of the network they
found that the network behaved a lot
like infants so initially the weights
are random so the representations are
random then suddenly you can
discriminate between animals and plants
the most coarse-grained distinction then
different types of plants different
types of animals and so on and this is a
multi-dimensional scaling plot of the
tations in the hidden layer and again
you see this progressive differentiation
of hierarchical structure so when I
first thought saw this as i was at once
excited and Confused excited because
while these neural networks behave like
infants and confused as to why i mean
these are much simpler objects and
infants so there must be theoretical
principles at play that guide this
dynamics what are they so this is when
Andrew and I started thinking about
these linear networks and oh sorry and I
should note that these these kinds of
hierarchical representations are also
seen both in human and monkey and they
actually align this is famous work by
crees corte and company so the question
is how does it get there what's the
dynamics of getting there so you know we
made the bold assumption that maybe if a
linear network could also exhibit this
right because the learning dynamics of
linear networks are nonlinear these are
the learning dynamics of batch grainy
dissented slow learning rates you get a
set of horribly complicated nonlinear
differential equations we solve them and
as Andrew told you in the last talk what
what the system is doing is it slowly
building up the singular value
decomposition of the input-output
correlation matrix of your data a bit
mode by mode where statistical modes are
singular values that are large are
learned on a shorter timescale and the
time scale of learning is one over the
singular value so this is intuitively
appealing stronger statistical structure
is learned first okay okay but what does
all of what does any of this have to do
with hierarchy okay so the problem with
one of the issues with this work is they
were working with a toy data set what
we'd like is a more controlled data set
over which over which we have analytical
control and so the way that we
approached this was to describe the
world as some kind of hierarchical
generative model draw data from that
generative model statistical model feed
that data to a neural network and try to
understand how the statistical structure
of the generative model inputs the
imprints itself onto the learning
dynamics of the network I think this is
one route for research to go as we go
forward make the generative models more
complex and the networks more complex
and really try to understand what's
going on so this is sort of a first step
so what was our generative model well we
essentially mimics the process of
evolution
we had a a branching diffusion process
where individual properties or features
of objects for example can or cannot fly
would diffuse down the tree and each
time it diffused it has a probability of
changing and that that that feature gets
assigned to each of these objects and
your different features get assigned
independently through different
branching diffusions through this tree
so at the end of the day you get a
statistical structure where items that
are closer together in the tree or have
a more common recent ancestor and more
similar to each other and this is sort
of a classical description of heart we
structured data okay so now we know that
the learning dynamics of these linear
networks are sensitive only to the
second order statistics of this data ie
the correlation matrix or this or the
correlation nodes yep so basically the
single value decomposition of that
correlation matrix so to understand how
hierarchical structure is learned in
this simple network we just have to
understand the singular value
decomposition of datasets generated in
this way we can analytically compute the
singular value decomposition and we find
a remarkably simple structure the
singular vectors which what one set of
singular vectors can be thought of as
functions on the leaves of the tree
these singular vectors respect the
hierarchical structure of the tree where
the stronger singular vector is the
constant function the next strongest
singular vector with second larger
singular value is a low frequency
function that's constant on this branch
and the opposite value of but constant
on this branch so it makes it the most
broader scale categorical distinction
and then the next strongest singular
vectors make distinctions within the
branch and then finally the weakest one
makes distinctions within individual
branches right so this is how the
singular value structure of
hierarchically structured data behaves
and and this is a match between our
analytical expressions in the simulation
and so when you put it all together you
get a theorem essentially any network
must exhibit progressive differentiation
on any data set generated by this class
of hierarchical diffusion processes and
mimic evolution essentially the idea is
network learns input output modes at a
time one over their singular value
singular values are broader heart
distinctions are larger than those of
finer distinctions and these input
output modes or singular vectors
corresponding to the hierarchical
assumptions the tree so then you
essentially get the result and so when
you put it all together
you can analytically compute the
multi-dimensional scaling plots that we
do expect from these linear networks and
this is what we get we just arbitrarily
labeled the leaves you know according to
just labels and so now when you compare
the very very complicated hard to
understand simulations of nonlinear
networks with the exact mathematical
analysis of the linear networks you see
a remarkable qualitative match and so
this is the difference really between
simulation here in theory here we have
much more conceptual insight into the
origin of this hierarchical progressive
different' of differentiation of
structure in these nonlinear networks
through rigorous analysis of a simple
theory in the spirit of what Andrew was
was arguing for a remarkable surprise of
this is that the second order statistics
of these semantic features are powerful
and sufficient enough to drive this
hierarchical differentiation that was an
interesting surprise so there's a whole
bunch of other psychological effects
that we can analytically analyze and and
and conceptually explain in terms of
previous simulations but we don't have
time to talk about that okay so let's go
to the next part you know we need to
understand the mathematics of high
dimensional spaces and so Andrew also
alluded to this in his previous talk so
one of the one of the the intuitions
that we have about say Aero surfaces
over high dimensional spaces is that
they riddled with local minima right in
fact it's often thought that local
minima high error stand is the major
impediment to non convex optimization
however this is only true sort of in
low-dimensional landscapes this is for
example a caricature of a protein
folding landscape and it is indeed
riddled with local minima and that's
true because it's low dimensional but we
know that our intuition derived from
moving within a three-dimensional world
our intuition about geometry there is
woefully inadequate for generalizing to
our intuitions about geometry in in high
dimensions and in high dimensions things
are very very different in basically
typical random non convex Aero surfaces
local minima at high error are
exponentially rare in the dimensionality
instead saddle points proliferate and we
basically developed an algorithm that
rapidly escapes these saddle points and
beads up neural network training so this
was work that was done in collaboration
with some excellent graduate students
and with Joshua Benji oh ok so the ideas
again we exploit some results that in
statistical physics where they look at
the geometry of high-dimensional
landscapes and in the basic idea is
let's say you have an error landscape
over a million variables right and let's
say you you reach a critical point the
slope vanishes somewhere what are the
chances that it's a local minimum we'll
all million directions of the function
have to curve up that's going to be
exponentially unlikely unless you're
already near the bottom all right so you
can make this much more precise for each
critical point you can plot each
critical point in the two dimensional
feature space one axis is the error
level of the critical point the other
axis is the fraction of negative
eigenvalues of the hessian and basically
you might think that critical points a
priority live anywhere in this
two-dimensional space but it turns out
they don't they concentrate on a
monotonically increasing curve this is a
calculation done by statistical
physicists back in 2007 they concentrate
on a monotonically increasing curve the
global minimum is here the global
maximum is here that critical points as
you move up the energy ladder develop
more and more negative curvature
directions until you're at the total top
where in which all directions are
negative so basically at intermediate
levels of energy you have no local
minima you only have saddle points okay
now physicists would then say ah this is
kind of a universal result you know it
should be true for anything of course
computer sciences will fight back and
say no your analysis and random
landscapes is not relevant we're doing
special stuff on neural networks whose
error landscapes over synaptic weights
are very very different than your random
landscapes in physics were used to the
notion of universality where there
exists at least some questions whose
answers don't depend on the details but
in computer science were not as used to
that so I predicted that this would
happen in in neural network so I teamed
up with yahshua to test this and they
tested it in lo and behold that's
exactly what they found so in deep
neural networks trained on em nest and
see far 10 they found that the critical
points concentrated on a monotonically
increasing curve in exactly the
predicted two dimensional feature space
okay now what can we do
about this so it turns out that Newton's
method is attracted to critical points
of any index its attack attracted to
saddle points but you can cure this
problem by instead of dividing by the
Hessian and Newton's method dividing by
the absolute value of the Hessian and by
that I mean take the Hessian compute all
its eigenvalues and replace each one of
them with their absolute value it turns
out that that dynamics this dynamics is
rapidly repelled from saddle points and
you can justify it theoretically using
trust region methods as well and it
works remarkably well so here's deep
neural network training where you just
start off with stochastic gradient
descent and you see that you get these
plateaus in learning performance as a
function of time when you see a plateau
you might think oh my gosh I'm stuck in
a local minimum but actually when you
switch to this new algorithm which we
call saddle free Newton you suddenly
drop an error okay so so this this
algorithm can actually do something by
exploiting our knowledge about the
geometry of high dimensional spaces I
should say Yan Lagoon also has a
different method for arriving at the
same conclusion that saddle points
proliferate in high dimensions okay all
right so now let's move from equilibrium
to nonequilibrium stat mech so that's
this part so there's been a lot of
advances in nanako broom sisal mechanics
recently that are really very exciting
essentially you can show that the second
law of thermodynamics that entropy
increases emerges through Jensen's
inequality as a corollary of some
equality known as the dresden ski
quality it's quite remarkable that the
jensen's inequality is what leads to the
second law of thermodynamics from a much
more fundamental law if you want to look
it up you just googled Rosinski equality
and you'll get to that literature but
what this means oh sorry so this is work
that was done with my postdoc jascha
who's now working at the google brain he
just left my lab recently so so
basically what this is saying is we're
used to the idea the things becoming
disordered over time that's the second
law but transiently for small systems
and for short times things can the
entropy can transiently decrease which
means you can spontaneously create order
from noise okay so motivated by these
ideas we
we're going to try to help this process
along by using neural networks okay so
the basic idea in use neural networks to
learn very very deep generative models
of complex structured probability
distributions so here's the physical
motivation let's say you have a
complicated data distribution that you
don't have analytical control over and
you would like to learn a probabilistic
model for that data distribution the
basic idea is destroy structure in data
through a diffusive process carefully
record that destruction of structure as
a movie frame by frame movie and then
use a deep neural network to reverse
time to create structure from noise
let's not let the laws of diffuse of
physics do it for us itself but let's
let's replace the laws of diffuse of
physics with a neural network to help
this process along and again this was
inspired by recent results and non
equivalence of mechanics which show that
entropy can transiently decrease for
short time scales okay so let you know
just to put it pictorially let's let's
say this physical die distribution is a
complicated data distribution we can't
write down a formula for it or
immediately train a neural network to
sample from it instead what we do is we
let the die to fuse and it will then
become a simple distribution uniform if
you defuse for long enough and then we
train a neural network to start from the
uniform distribution and go backwards in
time and rekey at the data distribution
and so then you get a generative model
okay so now if this is going to work so
let me now demonstrate that this sort of
audacious idea works so here's the basic
idea so this is a classical toy model in
machine learning the swiss roll and
what's the diffusive process look like
well we just let each data point diffuse
with a restoring force to the origin so
the stationary distribution is an
isotropic Gaussian and this is the
diffusion process okay and then what we
do is we carefully record this movie and
we train a neural network where each
layer in the neural network just has to
go one step back in time and reproduce
what happened in the past given the
present and we do this over many many
layers and train it to do that if it
works we should be able to sample from
the swiss roll in a very simple way we
sample from an isotropic Gaussian in two
dimensions that's the only stochastic
step and then we feed it
through the neural network which could
be a deterministic step or you could
also have noise in the neural network so
if it works I should be able to take a
new Gaussian point cloud feed all the
data points through the neural network
and reconstruct the Swiss roll from
noise and that's what happens so it's
sort of work it works pretty well
there's some straggler data points out
here what the system effectively learns
is it learns a transient vector field
that takes points everywhere and
concentrates it on the Swiss roll okay
so now we can so we can now move up the
level of complexity so the next level of
complexity is a toy model of natural
images say the dead leaves model where
you throw down circles of different
radii and the radii have a power-law
power-law radius distribution to mimic
the scale free nature of natural images
the circles occlude each other to mimic
occlusion so you get all sorts of
interesting structure that mimics the
structure of natural images so this is
this is that this is a session right
it's okay I'm going to go back to movies
so this is a this is a sample of the
dead leaf of the dead leaf model today
so this is a sample of the dead leaf
model
yes sorry this is this is a sample of
the dead leaf model this is a sample for
the next best probabilistic model of
dead leaves that's out there and then
i'll show you how our model works so the
basic idea is we train the model in a
bunch of dead leaves we have the pixels
undergo diffusion so we have a diffusive
process that turns dead leaves and white
noise we train your very very deep
neural networks with a thousand layers
to reverse this process and at the end
of the day if it works you should be
able to turn white noise feed it into
the neural network and generate dead
leaves okay so this is an example of
that working so as you can see it gets a
lot of the structure and in fact it gets
simultaneously things that are hard to
get in natural images it gets sharp
edges as well as long-range uniform
regions and also long-range angular
correlations in the edge right so
getting all three of those
simultaneously can be difficult okay the
other thing we can do is we can train it
on trainees we can sample from the
posterior distribution over images so
what we can do for example is trained it
on textures and then you know maybe
replace the interior of a texture with
white noise but clamp the pixels in the
boundary so the neural network for these
images operates in a convolutional
fashion so then information from the
boundaries should be able to propagate
into the interior and fill in the image
so this is an example of bark so what we
do is we just take this initial image
and feed it into the neural network and
the neural network fills in its best
guess as to what was in the interior and
as you can see it does it does a decent
job it gets these long-range
correlations and edges especially and
also the homogeneous regions so that's
the basic idea so let's go back so I
should say that oh that's not what I
wanted sorry in case you missed any of
the talk it's a quick review um
yes I should say that for the dead
leaves model we achieved to our
knowledge state-of-the-art performance
as far as probabilistic models of dead
leaves go in terms of log likelihood on
the data that's the okay so there's a
couple of key ideas in here there's
actually two key ideas that I wanted to
to summarize is that we circumvent
simultaneously to problems that vex
machine learning one is the credit
assignment problem how do you assign
credit or blame to neurons that are far
removed from the final output okay and
we circumvent that by providing training
targets to every layer each layer only
needs to go one step back in time so
there's no back propagation needed
there's no credit assignment problem the
second problem in in stochastic in
learning generative models as often
times when you try to model a
distribution you try to model it as the
stationary distribution of some
stochastic process this inevitably has
mixing time problems because if you're
stationary distribution has multiple
modes then it can take exponentially
long amounts of time to jump over free
energy barriers that separate the mode
so that's the mixing time problem we
circumvent that problem by demanding
that the neural network get from a
simple distribution to the complicated
distribution in finite time so we are
not modeling the data distribution as a
stationary distribution but as the
outcome of a transient nonequilibrium
stochastic process I think these two
ideas will generalize in all sorts of
interesting ways both to understanding
potentially neural computation and
augmenting machine computation okay now
the final thing right there's this huge
order of magnitude gap between energy
and speed in machines and neurons so
this begs the question what are the
fundamental limits of energy speed and
accuracy in computation so computation
is very general but we worked on
communication and we came up with some
interesting limits there so this is
joint work with my postdoc Shubin a
Lahiri he's a man after my own heart he
was a string theorist at Harvard I was
also string theorist we both switch in
neuroscience he's actually on the job
market this year so I you know I just
thought I'd let you know
okay so it hasn't this already been
solved by information theory and the
answer is no so information theory is
very good on placing limits on the
accuracy of communication given energy
constraints but not time constraints so
if you revisit channel Shannon's channel
coding theorem it gives you bounds on
bit rate in terms of energy constrained
channel capacity but oftentimes
achieving some bounds could require very
very large block lengths you take your
messages you put them into a big block
and you code that block and you transmit
it but time is of the essence in biology
biological systems do not have the
luxury of coding things into blocks
because by the time they build up the
block they might get eaten all right so
so it doesn't interim classical
information theory doesn't deal well
with it delays another interesting
subject is the thermodynamics of
computation where they asked what's the
minimal energy needed to achieve a
computation and they actually showed
remarkably that you could achieve some
computations with zero energy
dissipation but the catch is it requires
infinite time what was the basic idea is
that they elucidated the fact that n the
energy cost of computation occurs mostly
in erasing bits if you erase a bit you
reduce the entropy of your computational
device you must dump that entropy into
the outside world because the full
system must increase entropy if you dump
entropy the outside world you dump it to
the outside world you dissipate energy
that's the basic idea so you could avoid
this by making computation reversible
arranging computation so you never have
to be raised bids but then if there's no
thermodynamic driving force pushing the
computation forward as opposed to
backwards the computation could take
infinite time so there's no really good
theory that simultaneously addresses
speed accuracy and energy okay so we
have a theory now that we've come up
with recently and the theory is
basically the following imagine that you
have some external signal lambda that
you'd like to transmit through and
through a circuit any circuit we model
the circuit we achieve generality by
modeling the circuit is any arbitrary
stochastic dynamical system and we need
some parameter of the dynamical system
and that parameters tau its fastest time
scale okay this could be an arbitrary
Markovian stochastic dynamical system
and the external signal
holes to the Markovian dynamical system
by arbitrarily modifying its raids and
now let's say you have a signal receiver
that can observe the states of the
system and from observing these states
it can reconstruct the input okay the
basic framework of communication okay so
we can we can quantify speed energy and
accuracy so we can quantify speed as the
speed with which the external signal
changes this is how much the signal
changes in one time constant of the of
the signaling system squared okay we can
quantify so by the way this is a
physical system so it's coupled to a
thermal heat bath we can quantify energy
as a patient by the amount of power
consumed times the time so this is how
much energy it dissipates within one
time constant of the signaling system in
units of thermal energy right so this is
energy and then accuracy is the inverse
variance of your estimator so we can
consider any arbitrary unbiased
estimator and accuracy we just define to
be inverse variance if you define these
quantities this way you can prove a very
simple theorem the product of speed and
accuracy is less than or equal to energy
right so if for a given accuracy you
want to code a faster changing world you
better spend more energy if for a given
speed given speed at which the world
changes you want to be more accurate you
better spend more energy what are the
proof ideas behind this well there is
some work recent work in nonaqueous
aslam mechanics which shows that energy
dissipation can be quantified through a
thermo dynamic friction tensor on a
manifold of boltzmann distributions
associated with equilibrium
distributions of the signalling system
this is this is done by Gavin crooks
what we did was we lower bounded this
friction tensor by the fissure
information which is naturally a metric
on a manifold of distributions so it has
the same properties as this thermo
dynamic friction tensor and then of
course we all know that from a fissure
information upper bounds accuracy
through the Kramer rebound and by
putting it all together you know there's
a lot of work involved you get this very
simple theorem if you define things this
way okay so now this opens the path
towards connecting two experiments now
we can ask well how efficient our
biological system subject to these
limits
and also how far artificial systems from
these limits and what do we need to get
there ok so we also I talked we also do
a whole lot of work in my lab where we
work very very closely with
neurophysiologist I haven't talked
anything about that work but we actually
make lots of experimentally testable
predictions we work closely with
neurophysiologists at Stanford to test
these predictions so we're doing a we're
having a lot of fun and actually I'm
looking for postdocs my postdocs are
looking for jobs and I'm looking for
postdocs so hopefully i'll get postdoc
before my postdoc gets a job but anyways
so so I I'd love to hear from you and
again this is mostly all published work
except for the speed energy accuracy
which is in preparation I'd like to
thank my funding sources my website is a
little out of date but hopefully you can
find most of these papers and website if
not just googling thanks
I think thanks for the talk so yeah I
just before asking my question want to
comment that I have a paper return of
ski on basically similar idea of using
density estimation in learning a
stochastic processes I think it's very
very exciting actually smile you may be
nervous your paper was my favorite paper
last year it was like oh thanks yeah so
you mum your paper but yeah sorry the
paper is called the Wilson machine we
call the CC machine oh thats chary
mentioned it yeah I remember now ok yeah
yeah so basically my question is the
things i've also been dealing with in
this way of thinking about the problem
yo it may be a very powerful model for
density estimation but it is probably
not useful for learning representations
have you thought about that so basically
if you if you make this process that
cause your static let's assume take
infinite time yeah it's I mean the
process cool going from one frame to
next frame is going to be very easy
right exactly yeah and so basically at
the end of the day you may have a very
powerful engine for you know generative
model but this is your learning anything
yeah I in my mother I only have centers
around filters basically yeah so I
sympathize with that a regardless we
looked at the internal representations
these thousand layer networks and the
first few layers were kapoor's their
heads everybody gets the boards yeah we
don't fully understand the
representations later on but i agree
with you if you have an upper bound on
depth then that forces a lot of
parsimony in the representation has to
create very intelligent representations
to get from where you are to where you
want to go in a finite number of steps
if where you want to go is extremely
complicated why not just take a very
very many steps but have an intelligent
a teaching procedure but I mean you
agree that the representation become
more powerful if you have infinitely
more assistance I mean the learning
becomes um don't care i agree with that
yeah so we can prove that in the
quasi-static libya yeah it's that with
the diffusion is slow and the number of
deps go to infinity yeah this process
will converge to the true distribution
of the data we have a proof of that
exactly i understand that but in that
process the
dition become more and more boring we
make no we make no claims about the
intermediate representation I think
except that the final representations
the data distribution again yes you're
dead leaves generator was very
interesting I liked it a lot I'm
wondering though how you measured the
training data probability or just a to
probability under this generative model
at the end it's oh yeah mobile it's not
an energy based model how do you measure
that probability yeah we use basically
similar ideas to the jars in skinny
quality where you can use nonequilibrium
paths to measure free energy differences
so this is one of the interesting uses
of address in CA quality is that you can
actually measure free energy differences
between two states by looking at
nonequilibrium paths and averaging over
them and that's what we do and the free
energy difference is a log probability
difference so that's how we do it it's a
bit complicated it's hard to explain
immediately but there's a way to do it
what we do is we averaged over multiple
paths through the network that yield
that get to the same data point that's
what we do multiple paths through the
network to get the same output yeah how
could you get we diffuse we diffuse back
to the beginning and then go through the
network bound essentially ah you use the
diffusion process yeah yeah makes sense
thank you okay
so let me ask the question I want to ask
Andrew before it's about the first part
of your talk so you're you're looking at
infant learning and trying to model that
with with big network somehow the brain
probably doesn't actually use gradient
descent right yeah yeah so how do you
think that would affect impact your
results so the brain must have some way
of solving the credit assignment problem
right it must back propagation is the
only way we know how to solve a credit
assignment problem and this is probably
the biggest open problem in I think in
in neuroscience is how does the brain
cell the credit assignment problem the
best I can say is that any method that
that might solve the credit assignment
problem might have learning dynamics
that's similar to back prom but that's a
wild speculation so it's actually quite
remarkable that back props learning
dynamics mimics that but but at the end
of the day it's not it's not that
remarkable right because it in this
simple data set the SVD controls the
learning controls the structure of the
data and so you might imagine that
almost any sensible learning algorithm
would pick up on the largest singular
modes first right it's just the second
order statistics right so so yeah I
don't know that's a kind of a muddled
answer but it's either speculative a
trivial all right just a quick question
about units so you can guess this is
yeah I Paul I was very intrigued I guess
that's the one I was just curious you
know about the speed accuracy yes sir jb
yeah the units of energy so your energy
there is again so it kind of log both
societies both sides are dimensionless
right if you look at it so energy is
dimensionless because I normalized power
times time by Katie okay so speed is
dimensionless in time but has units of
lambda squared accuracy has units of 1
over lambda squared so the product is
dimensionless so the units do work out
as a physicist I was also very worried
about units I'm totally sympathize with
you in fact dimensional analysis was a
clue for us to guess what the answer
would be before we could prove it
I don't know if I miss it but the
diffusion process so this was for a
single image or how does it generalize
oh yeah yeah so it's trained on an
ensemble of images right so it learns
the distribution and then you can do all
sorts of things right you can you can
actually do all sorts of things you can
sample from the distribution you can
infer by clamping parts of the image and
invert and infer the rest you can
evaluate log probabilities that was the
answer to Titans question so so you can
do all of it but you train on an
ensemble so it doesn't overtrained one
image okay um one last question ok this
is a question related to the previous
discussion about the back propagation
being the only known way of doing credit
assignment that kind of raises analogous
like we know how we solve SVD or matrix
factorization or sequence of them while
expectation maximization or alternating
minimization so maybe it's not
necessarily has to be back propagation
and there is obviously a yeah it doesn't
have to be but that's um i mean ii m is
like an alternating minimization over
two sets of variables right yeah or so a
multiple sets of variables it could be
multi variable single lane but the
fundamental i mean unless you can
exactly do the minimization which
justifies many am algorithms in either
in the in the m-step you're going to
have to do gradient descent at some
point right so gradient descent will be
a computational primitive in very very
complicated problems right where the
m-step can't be done analytically okay
yeah I mean a dream is that that may be
objective function minimization is not
the right principle for understanding
brain dynamics at the level of neural
dynamics of learning that that's a scary
proposition because the machine learning
we always think about objective function
minimization you have nothing else to
hold on to their but we know that
oscillators don't minimize any there's
no Lyapunov function for an oscillating
system and the brain oscillate so you
know let's we have to be very careful
about the assumptions we make all right
let's thanks three again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>