<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Software Support for Efficient Use of  Modern Parallel Systems | Coder Coacher - Coaching Coders</title><meta content="Software Support for Efficient Use of  Modern Parallel Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Software Support for Efficient Use of  Modern Parallel Systems</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FTvztyMO2UU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone um thanks for coming I'm
modern Maserati from research itself
engineering and it's my pleasure to
introduce a bill in chubby today he
joins us from rice um he is graduating
in a minute and and then after this he's
going to be joining HP labs he's very
interested in all aspects of high
performance computing from you know
parallel programming compilers
performance and today he'll be talking
about in s office of support for
efficient use of modern parallel systems
well it thank you thanks for the
introduction and welcome to this talk
I'm sure there are more people joining
online the title of this talk as i said
is software support for efficient use of
modern parlor systems in some simple
terms probably this is performance
analysis I used to be in Microsoft and
back in 2010 one day I felt more
adventurous I used to work in
microsoft.net clr JIT compiler and when
I felt adventurous dro to Glacier
National Park actually just packed up
everything and dro my car from there
went to Rocky Mountain National Park
further down I'd row and lined up at
rice that took me nine days dro for
3,000 miles and it was supposed to be a
nice place to start some research in
compilers and we will see how that has
turned out so far and I'm quite proud of
all the ship it's that Microsoft gave me
I don't know if you can see it but these
yeah quite proud of those so all right
so talks in high performance computing
computer architecture compilers always
begin with paying some homage to God and
more and he is a prediction about growth
of transistors but you are all aware of
Gordon Moore's prediction of growth of
transistors and so I will skip that and
go into something that's more in
resting somewhat controversial this is
from prob strings protesting who used to
be a researcher at MSR and he made this
corollary to Moore's law which states
improvements to compiler technology
double the performance of typical
programs every 18 years in about that
much time you would expect five hundred
two thousand x speed up in Hardware
whereas in an application you will not
see so much what this is saying is if
you took your modern hardware and your
favorite application and compiled it
with today's compiler and an 18 year old
compiler the difference in performance
would be only about 2 X in the meantime
the typical shelf life of hardware is
very short top 500 supercomputers remain
in that level for a very short time
typical life span of a hardware is about
four years as a result typical
scientific applications achieve only
five to fifteen percent of peak
performance on today's
microprocessor-based supercomputers and
that should be true for many other
applications need not be HPC
applications alone so there is a wide
chasm between what hardware can offer
and how much software makes use of it
modern yeah if i change the architecture
boohoo and I help with some fancy new
architecture that has these things
called registers you as a compiler
writers say hey I've got this really
good idea called register allocation
mm-hmm then my compiler technique for
that particular architecture really that
will outweigh presence yeah I just saw
look it will be done in four years and
will it work for next architecture no
it'll work from the architecture that
I'm running over if you do it within
four years go for it okay I go but I
want to go back to your stuff previous
slide so I think the gap the scientific
applications are a place where people
care most about performance and that's
the best gap you can get at most
applications like JavaScript and Java
copies bed and they don't even get but
do you personally I don't give on yet
like Judith a person right like point
zero zero one percent of what the yeah
yeah so how come people are still using
computers and are happy with that
compared to not having anything having
something is better right and the for a
long time we all relied on the clock
speeds going up and putting more
transistors and how long is that going
to carry forward right and there are
efforts being done to put JavaScript on
accelerators these days right to make
better use of respect you know is used
performance but I'd say these days
possums this this gap this gap can be
closed right and other gaps can be
closed and that's the only way to get
can be closed when closed is a strong
term can may not continue to widen as
much as it we are seeing for a single
thread of execution but if we are
according use of those for these
JavaScript kind applications so in that
so in that terms if you count it as the
about amount of flops available on a
processor there is still widening grant
app because flops available is
increasing well it's the hardware
changes less frequently like the typical
shelf life a top 500 supercomputers for
years it's going to get longer because
we're running out of things just cream
on to it to make them go faster but the
architecture is still changing you how
you lay it out do you put accelerators
are accelerators discreet are they on
ship so in these ways architecture is
still changing are the heterogeneous are
the homogeneous so these things are
still continuing to change and they are
changing at a rate faster than software
can catch up with them I think that's
the most important aspect this hardware
changes faster than compilers or
applications can keep up right and that
seems to be getting worse because the
way it's changing is not
speed it's other stuff that's even
harder yeah that is this thing that's
this thing where that architectures are
changing we use heterogeneity more
frequently than we did before there are
a few heavyweight goes in modern
architectures which are latency
optimized and many lightweight course
that our throughput optimized modern
architectures have a deep memory
hierarchy Numa is very common and even
sometimes memories excessively managed
for example the Intel's nightline
Knights landing processor that will be
deployed at nurse will have its special
memory that you can program on how you
want to use it power is a limiting
factor dramatic clock rate increases
thing of past and do a expense
Department of Energy expects more than
25 X power efficiency to reach exa flop
similarly software is also complicated
to deal with complications in software
we use modularity which is hierarchy of
abstractions made of libraries and
functions they have an advantage that
they give reusability and ease of
understanding a complex system for
example this is an architect's view of
how abstraction should look but reality
is different from that in reality the
abstractions are much deeper they are
delicately balanced and they interact in
much complicated ways abstractions inter
optimizations they introduce redundancy
by redundancy I do not mean resiliency
resiliency is good redundancy is the
overhead that comes with abstractions
and abstractions are written insensitive
to a context in which the execute
abstractions are context and input
insensitive so they are not heavily
optimized for a particular workload
inefficiency in software arise due to
various causes some of them are
hierarchical abstractions insufficient
parallelism load imbalance developers
inattention to performance poor choice
of algorithms data structures
ineffective compiler optimizations there
may be times when compiler may do more
harm than good and excessive data
movement sometimes if I want to classify
them I tend to see them in these three
top classes one is resource idealist for
example insufficient parallelism or load
imbalance wasteful resource consumption
where you are making the resources are
not idle you are making use of them but
that is not making progress towards the
computation start getting closer to the
end result and hardware software
mismatch for example Hardware may
provide new market echtua software
assumes flat memory hierarchy that is a
mismatch in software and hardware so my
research interest is in achieving top
performance on modern architectures to
do so I detect performance problems I am
interested in knowing why we are losing
performance where and by how much I
build effective performance analysis
tools that pinpoint performance losses
and a developer to identify problems in
the source code so that they can easily
fix them having found these problems I
have provided solutions for alleviating
performance bottlenecks by building
adaptive runtimes and designing
architecture of air algorithms to
ultimately bridge the divide between
hardware and software some of the
contributions are normal techniques to
pinpoint opportunities for tuning in
hybrid applications I have built some
lightweight performance analysis tools
to do so efficiently attributing
execution characteristics to source and
data in context I will some fine grained
instruction level monitoring tools to do
so on the fly redundancy detection
elimination for example detecting and
eliminating redundant synchronization
and a scalable synchronization algorithm
that is better suited for Numa
architectures I know Catherine is
smiling because price has some tradition
of doing scalable synchronization
algorithms so in building performance
analysis tools I usually apply these
principles measure accurately introduce
no blind spots if there is dynamically
loaded code in the light in the program
measure it as well
do not dilate what you are measuring for
example if you are measuring cache
misses do not introduce cache misses of
the two itself introduce low measurement
overhead so that the tool can scale to
large panel systems observe on real
production runs so that you observe what
is happening in fully optimized code and
attribute contextually this is a
recurring theme in this stock that
deserves a slide context is important in
large applications modern software is
made of layers and layers of libraries
you have math libraries communication
application framework and so on and we
often use general programming such as
C++ templates where instantiation is
important so if a tool says that you are
spending a lot of time in a wait you
exactly want to know how you reach that
weight state for example in this climate
code which has several components you
exactly want to know what weight is the
one that is causing problem say if main
calls an ocean component calls wait you
want to know that that is where you need
to focus attention for improving that is
why performance is highly context
dependent so I have organized rest of
this talk yeah this context this can be
like static code context and data
context yes you know the same function
when called on different data structures
we here Stephon differently I have tried
to address both where our data centric
attribution capabilities tell you that
in this code when called from Maine
called blah blah blah and it was
touching a data that was allocated in
this particular context is when you are
seeing certain performance anomalies
so rest of the talk I have raval I will
revolve it around these themes where you
have resource idleness and how we can
pinpoint and quantify how we identify
wasteful resource consumption and how we
can tailor for an architecture modern
architectures are increasing
increasingly employing accelerators this
is Titan which is second largest
supercomputer it is made of in AMD
opteron processors and GPUs so some of
the codes that we looked at that were
using accelerators where lamps and i am
d which are molecular dynamics codes
lula SH which is a hydro dynamics code
and chroma which is a lattice field
theory there are many many more codes
being rewritten to make use of
accelerators this is just a few that we
looked at sorry is running on this
orbiter yes consider this is one of the
communities are set a very important
important important here for example
Lula she's a proxy application that many
many people have looked at it's a
hydrodynamic scored how a body changes
when you hit it with bullet or some
material oh it's for flight simulation
when you hit something on the airplane
so they are playing this press right and
many other similar kinds of applications
that the National Labs get about back I
qualify relatives did they did frozen
chickens at the airplane until they had
this code and then they didn't simulate
really yeah the bits really changes
savvy new Sciences simulation so
challenge in tuning challenges tuning
these scientific applications to make
use of heterogeneous architectures and
an observation is that these codes have
a well-tuned CPU portion that has been
developed for last many years and an
emerging GPU portion so performance
analysis tools play a vital role in
tuning these applications for these
architectures this is the execution
trace of lamps which is a molecular
dynamics co
word on x axis is time y axis is various
resources resource being a CPU thread
and two concurrent threads running on
GPU and gray color indicates idleness
which means on GPU either nothing is
running on cpu it means it is waiting
for something to finish on GPU what you
can notice is there is a lot of idleness
they are being used in a mutually
exclusive fashion if GPU is running CPU
is I let's see if CPUs doing work GPU is
not doing anything the process here e
you're looking at a single machine yes
this code executes on a larger context
in a larger context it has exactly same
behavior if I unfold it for all the
current MPI processes so is there any
you but when you're when you are going
to talk about performance you're talking
mostly about a single machine and
extrapolating to the entire cluster know
what means sometimes if the folks there
are situations situation where you need
to look at all of them situations where
where the behavior is symmetric and you
can look at one of them so I de les
wastes compute cycles offloading an
entire execution to GPU waste CPU cycles
and only making use of CPU I'm not
making GPU you are not making good use
of your throughput optimized course a
better way is overlap CPU and GPU
computation if there are a couple of
ways to do it you could divide the
principle execution itself between both
cpu and GPU or if the code is not
amenable for that you could be using cpu
to prepare next piece of work when GPU
is busy doing current piece of work this
is pipelining the execution so having
identified that idleness is an important
source of performance loss in these
kinds of systems the performance
analysis that we do for heterogeneous
system focuses on idleness and
attributes idolos to its causes the
insight is that when a resource is idle
you can recognize that that is a symptom
of performance loss the cause of such
loss is another resource that is at work
so you can blame the cause simple
analogy if GPU is working and CPU is
idle the idle less that you notice in
CPU is because of
GPU having not finished the work yet and
vice versa another analogy is if you
have lock contention going on in a
multi-threaded code if many threads are
waiting to acquire a lock you can
recognize they being spin waiting or
something that is a symptom the cause is
the thread that is holding the lock and
you can push all this waiting as a blame
to the critical section or the other
thread that is holding the lock okay is
that analogy okay so one can tune code
regions that accumulate a lot of blame
and typically tuning such Code reduces
your critical path in a parallel program
because this as CPU GPU blame-shifting
because you shift the blame from a place
to it's from from a symptom to its cause
so in CPU GPU systems if this x axis is
time and I have two resources CPU may
upload a colonel colonel a to GPU and go
on to do some work and they are well
overlapped and only towards the end CPU
waits for about five percent time for
Colonel a to finish and then there is
another piece of code wave CPU of floats
another piece of work colonel be and
after a little bit of work now it wastes
waits for Colonel be to finish and that
weight is about forty percent and as I
said in many of these codes CPU is
already well tuned GPU is not so well
tuned a classical hotspot analysis that
tells you which resource is consuming a
lot of time would tell you that Colonel
a being the longest time consuming is
the one that you might want to tune but
because it is well overlapped tuning
this can give you only about 5%
performance improvement whereas Colonel
B which the blame shifting identifies
because there is a lot of idleness if
you tune this you gain a lot of benefit
in your end-to-end performance because
it reduces the critical path so the
insight is the top GPU Colonel may not
be the best candidate for Q the vice
versa is also true if you are interested
in tuning a CPU portion the one that has
less overlap with another component GPU
being here is the 12 q and that reduces
the critical path we have implemented
this in hpc toolkit which is a
performance analysis tool being
developed at rice in
versity it supports multi lingual fully
optimized statically or dynamically
linked application we do not do any
source code modification we take the
fully compiled optimized code we measure
performance using asynchronous sampling
of primers and hardware performance
counters because it is sampling based it
incurs very low overhead we attribute
performance to call stack full call path
we can do so in many programming models
it could be pthread OpenMP and p aye
khuda or any combinations of these it is
decentralized there are no centralized
bottlenecks and it scales to thousands
of processes yeah instrumental we do not
insert any instrumentation we do and
on-the-fly binary analysis and recognize
if this is the PC where will my return
address p and so on to an unwind your
contract networks and generally I reduce
Frank winters yes looks without rain for
pointers it is full of heuristics that
work very well and in the worst case if
you haven't found you can trawl the
stack a little bit to recognize oh this
looks like a return address and let me
use that we applied blame-shifting on
some important codes for example la
leche which has been already looked by
many researchers it is a struct dynamics
code and blame shifting identified that
thirty percent of GPU was idle due to
GPU memory allocation routines that was
happening in each time step in each time
step you would allocate a memory on GPU
and deallocated and during that time CPU
CP was doing it and during that time
GPUs idol so and when we recognize that
we notice that the memory amount of
memory allocated in each time step is
exactly same so instead of doing it each
time step one could just hoist it
allocate it once and reuse it each time
that gained as thirty percent
performance improvement in this code in
another code which is a piece endemic
which is a agent-based simulation for
spread of contagion the cpu and GPU are
being used mutually exclusively like the
one I showed in the
earlier diagram and we use pipeline
parallelism where as soon as a little
bit of data was available we started
computation on GPUs and that and we used
a particular run time called as MPI acc
being developed at argonne national lab
and that gave us about sixty-two percent
speed up in this application one of the
impacts is this technique is being
proposed as to be used in an important
accelerator manufacturers product
roadmap I cannot tell you who the
manufacturer is going on that's about
pardon me a little bit on HTC
application no I did not that's an
interesting one to try yes thank you so
window with something like build on
estrace that just looks at system calls
be sufficient understand this notion of
blame isn't me punk so so so too it's
been a while since I've been in clinics
but if us try something and you notice
that the CPU makes a system call which
play out to the GPU and then doesn't do
anything for a while until they're kind
of the response from from that IO call
it some sort that's sufficient to
realize that it may have some important
information thinking of it the
difference is you have to collect the
entire trace and then do a post-mortem
analysis collecting trace is tricky
depending on your size of parallel
program places can grow really really
large this is a profiling waste
application technique where as soon as
you identify idleness you
instantaneously blame the code running
on another component that is the
difference in blame-shifting anything
that you can do with profiling you can
do with tracing just it is post-mortem
and you have to collect a lot of data so
one of your go ahead fuchsia our hands
our top does this play really actually
to this particular executions over well
yes in other words that because general
can be dynamic we have a different
execution order of the same program such
this plan may not be valid yeah in a
dark based execution we are thinking
about it how to do blame shifting in a
tank based execution where the schedule
is not same all the time this is more
like this is more useful in statically
partitioned executions the one yeah you
may you may still find useful results in
dynamically scheduled ones but eggs
iteration to iteration if it changes run
to run if it changes then it is tricky
if the sampling is representative of
what is happening over time it makes
still give you good enough insights yes
I think so treatments is that yeah
basically I don't you find the impact
all the critical critical path on a dime
yes then we reduce this critical path
then the blame to say alternative
alternative place yeah then um but
potentially that this could shift to
another one so when the tax structure is
complicated it it could be hard yeah to
analyze that's so that's so there could
be many very closely related critical
path and you may identify one you
optimize this and then the next one
becomes the critical path that's the
issue anytime you are trying to optimize
critical path so when you then you have
to go off her hot critical paths so when
you have when you show the example and
if you go back one what are your
solutions was to do better pipelining
this one yeah but can we go back to that
example where you have the forty percent
okay so in this one it could be that
Colonel be is actually highly optimized
and this is an example where you want to
do pipelining or there
could be some work that's independent of
Colonel be that cadets you could shove
into here how do you know which thing
you should case-by-case at that point an
automated tool will not help you do too
automated tool tell us what's going on
where are the potential problems
solution is in the mind of the
application developer okay right so so
here you say you can either but you
could give advice you could say Colonel
be is your bottleneck you could shift
work independent work yes here or you
could optimize colonel me yes either of
them is a good option there are other
options not off the top of my head I
keep thinking I can't think of any off
the top of my head
yeah usually pipelining is something
that people prefer because bringing some
other piece of work can change your
critical path that may take longer than
this one right pipelining is more
directly going more localized yeah right
thank you welcome so going on to
wasteful resource consumption another
top cause of performance losses I have
split that into internode and intra note
how wastage happens across nodes and how
that happens within a node first across
nodes nwk me is this computational
chemistry code from do it is as flagship
quantum chemistry code it uses this
thing called as global array toolkit
programming where the data is physically
distributed onto several nodes but each
process gets this illusion of being able
to access the entire data structure
using get put style one-sided semantics
and the software is written in sp MD
style p gasps partition global address
space programming model and it is a
large source code with more than 6
million lines of code and about 25,000
files it scales quite while it executes
it has been shown to work on more than
100,000 processes it is widely used
there are more than 60,000 downloads
worldwide and about 200 to 250
scientific publications are produced per
year based on this framework this is
something impactful so in terms of the
software organization there is NW cam
chemistry code written usually in
Fortran and then it calls into this
global array framework which in turn
calls into armc i which is an aggregate
remote memory copy interface which is
being reorganized into communication for
exascale these days and all of these can
go on different substrates for example
it can use MPI message passing interface
or gas net which in turn can go on
InfiniBand dmap on crane machine spammy
on IBM machines and so on basically it's
a layered software
in big ass programs typically people use
a lot of barriers because that is how
you can enforce consistency of data
between asynchronous updates but
communication libraries for hpc are
typically written agnostic to an exit
execution context so developers use
concern make conservative assumptions
about underlying layers and the enforce
barriers on entry and exit to each API
for example each time this layer calls
into this layer there will be a bad here
here and one barrier here and call in to
hear bad here and a barrier these
layering of api's and layering of and
use of api's leads to redundancies as I
will show in the next slide this is four
lines of code written by an application
programmer that sits in the chemistry
component if you track down what happens
underneath this is the call graph you
will see that there are nine barriers in
the in four lines of code and in the
global array layer there is a sink at
entry sink at exit sink at entry sink at
exit and so on now of those nine
barriers at least three are redundant
for example the one that is called at
the exit and the one that is called at
entry there is absolutely no data update
in between these two but because
conservative assumptions put barriers
and that they lead to return in various
yes so with a smart lighting I just
aligned it is with the mystic umpires
multi lingual multi component there is
as of now I don't know of analysis that
does barrier elimination and safety it's
the other thing is it impossible no one
can do static analysis just yeah but
even if i'm working at the binary right
if i'm looking at x86 for that link time
if i do the airline then you can't I
have to find various very well then you
have to understand the data access
before a barrier and after a barrier in
the binary analysis ok can you explain
why I'm doing Angie a coffee instead I'm
hoping to erase why do i why does G a
copy need to synchronize the whole
program it just needs to synchronize
what we those threads that are accessing
G
yeah but it's kudeta a and B are spread
all over on all processes season okay so
I guess I haven't understood the
programming model and so I but this one
when I so many CGA coffee then there's
one thread that's coming yeah it's an
SVM d each per thread says let's copy so
your role is I copy the data that's
present in a on my local opposition to
my portion of B that's what it is
nobody's operating my G a G ok yes you
ok nine barriers in four lines of code
three redundant programs spent twenty
percent time doing barriers composition
of API leads to redundancies redundancy
is contextual you cannot go and delete
this sink because there may be another
path where it is called and that is not
followed by this which means then it is
necessary but friends have already come
to a barrier adding another barrier is
probably there is no load imbalance
problem but the cost of barrier itself
physics is high barrier incurs latency
and it can lead to a little load
imbalance because one process may get
stuck may get stalled may have slow cpu
GPU may get throttled so the arrival is
not perfect so to answer your question
it is multilingual multiple programming
models MPI openmp so that is why
elimination using compiler driven
techniques is tricky so our idea is to
detect redundant barriers at runtime by
intercepting certain communication calls
and speculatively alight redundant
barriers but when we do so we are being
conservative and lightweight so to give
some background on when barriers are
needed and when they are redundant we
have these two processes p1 p2 if there
is a put of data X and a get of X on p2
then barrier is needed because there is
a data dependence across the process
here over here to process is touch data
and they touch a different data after a
barrier so there is no data dependence
here
hence this is a redundant barrier over
here p 1 does a put of x and then does a
get of X there is data dependence but it
does not cross crosses boundaries so
even here barrier is redundant if there
is no data being touched after a barrier
those barriers are redundant and if
there is no data being touched by data I
mean the shared data not being touched
before a barrier than even that is
redundant so this led us into thinking
about how barriers are can be eliminated
when they are needed when they are not
so if you can think of two barriers won
consecutive barriers and observe all
types of excesses in between two
barriers you can classify them into
three categories and which means the
process did not do any remote memory
access remote as in any shared data
memory axis L means it accessed data but
the data was resident on the same
process local are means the process
access to data that was remote it was
not on the same process these ln and our
form a lattice for example if one
process did not exist share data another
process X is remote data the lattice
will result is as if you exist remote
data if you take driplets of a nexus
before a barrier and what is a nexus
after the barrier if you have an
execution trace of all processes the
entire execution trace of the program
you can look at these excesses and you
can say no exes before no exes after
that barrier is redundant whereas if
there was a remote exes before a barrier
and a remote access after barrier then
it is not safe to eliminate such
barriers then you can say well that
barrier is needed you could do this if
you have the entire execution trace
follower processes but if I am on a
barrier and I want to decide at run time
should I participate in the barrier or
not I cannot because I do not know what
is coming next so a priori knowledge of
a bag of an excess is unknown but there
is a silver lining here one can make
safe approximations if you notice what
kind no matter what comes after a
barrier if there was no remote memory
access if before a barrier you can
always remove such barrier so that says
if no remote memory access and a barrier
remove such barrier
if there was only local axis there is at
least one case where it is not safe so
you cannot eliminate such barrier if
there was the remote access before a
barrier there are two cases where you
cannot eliminate a barrier so
conservatively keep the barrier now you
might be thinking fine I as an
individual process know my kind of
excesses how will I know what is
happening in some other process to make
a global decision of whether to
participate in a barrier or script
skipper barrier for a second assume and
Oracle tells you if you know your local
status you know the locals the status of
the entire system I will tell you what
that Oracle is just assume there is an
Oracle and let us try to apply these
three rules to the code I had shown you
over here there is a barrier and a
barrier no remote memory exists in
between my N and B rule says skip such
barrier do not participate then there is
L&amp;amp;B lb rule says participate in that
barrier then there is N and B my NB rule
against is skip such barrier so that is
how one can apply these rules on the fly
so let us see what is that Oracle the
Oracle is simple it is actually a
learning and an illusion mechanism where
we identify local redundancy of a
barrier instance in its calling context
you arrive at a barrier you notice did I
make any axis from last barrier to this
barrier and you have learned something
about yourself and why learning we
replace barrier with reduction and each
process tells its local state in the
reduction and the result of reduction
tells you back was it needed in some
process or was it not needed in any
process if it is system-wide redundant
then such barrier becomes an elegant
candidate in that context if it is
needed in some process then we will say
we cannot alight such barriers and it
costs low overhead while learning
because passing one additional
information in body in reduction has
very little overhead once we have
learned something we speculatively mark
a barrier as redundant for a calling
context if all instances are globally
redundant and we alight future episodes
that are marked as a lesion candidates
full calling context years so each time
you arrive at a barrier you do a call
stack unwind and
record it so if I am a p1 process I
arrived at a barrier I would not have
done any remote operation my
instrumentation would unwind the calling
context p to derive do the same thing
and then they participate in why
learning they participate in a reduction
and they inform what they have found
about themselves and they learn
something about that in this case all of
they learn that there is Perry are not
needed and they locally make this
decision in that context it can be
lighted and they do it for certain
number of iterations which is tunable
and once they have learnt enough once
and then it arrives it says I am in this
context what is the learned thing about
it he lied so it does not participate in
the barrier and goes forward same thing
for another process as well it arrives
queries local information and skits so
can their we miss speculation yes miss
speculation can happen if when you
arrive at a barrier you are not
following the consensus that you had
made previously and we can always detect
miss speculation because you would have
broken the consensus as a process there
are times when you can recover for
example if all processes made me
speculation they all go into a reduction
and the understand otherwise we rely on
checkpoint restart facility that is
available in NW came and start from last
checkpoint in nwk luckily there were no
miss speculation some amount of training
made sure that once it is a redundant it
is always returned because of this PM
dstyle programming to implement we
instrumented barriers some remote memory
calls certain application specific
features that were bypassing underlying
whenever they were existing local data
sometimes they were bypassing well-known
calls and all this costed only 1% of
instrumentation overhead and scales
perfectly because most of it is all
local operations now how do we gain
developer confidence if we start doing
this kind of religion on the fly so to
gain more developer confidence we have a
multipass approach where in one pass we
only identify what are redundant
barriers and present it to the user in a
summarized way here in this context we
think this barrier is redundant do you
want to align us and if the developers
is just go ahead elide in the
actual production run only if the
context matches we will align so in a
production of NW came running on Crais
xc30 machine with about 2,000 processes
we were doing a simulation to understand
catalytic destruction of ozone there
were 130 k instances of barriers for a
code running on about running for about
30 minutes and they were spread across
eight thousand unique calling contexts
and there were 63 percent barriers that
were redundant sixty-three percent is
that of this one as you can imagine this
layering of software causes a lot of
redundancy and by eliminating them we
gained about fourteen percent running
time improvement I looked at the
checkpoint in is an application form now
the the library is to checkpoint for you
periodically father's not not for this
failure yeah and the coat of stain last
for hours yes next is the kind of
wasteful resource consumption that
happens within a node memory access is
expensive within a node because of
multiple levels of hierarchy course
sharing a cache and having limited
bandwidth per core dead right happens
when two rights happen to the same
memory location without an intervening
read so here is a dead right we have
index equal to 0 and X equal to 20 the
first X equal to 0 is a useless
operation that is a dead right and for
this discussion we will call this one as
a killing right what is dotted red right
is over here X equal to 0 and then we
print the value of x and then X equal to
20 now compilers can eliminate something
like this dead code elimination can L
can get rid of dead rights but what is
new is this piece of code that became a
cross in chombo which is adaptive mesh
refinement framework for solving partial
differential equations there is a
three-level nested loop in Riemann
solver and the program spent thirty
percent time doing this so what here the
first three
lines are an assignment to a four
dimensional array and then a check is
made if the check is true the same array
is overwritten with different set of
values so this is killing that if this
condition is true another condition is
being checked if that is true these
three or maybe that is overwritten again
with some more values now compilers
cannot eliminate all dead rights because
of aliasing and ambiguity aggregate
variables such as arrays and structures
and compiler supply optimization within
function boundaries or within some
limitations whereas there is always some
late binding code can be loaded on the
fly and there is partial dead right for
example it may be right on one path it
may not be right on some other path so
over here this code lacked for
performance a very simple way to fix
this is using else if structuring where
you do this thing first if that
condition is not true then you do that
if that is not true you fall to the
default one where it will eliminate all
dead rights for you doing so sped up the
loop by thirty-five percent and program
by about seven percent so I was
motivated to see how frequent these dead
rights are in any program so I ended up
writing this tool called as dead spy
which monitors every load and store and
maintain state information about every
memory bite and it detects dead rights
in an execution using a very simple
automaton each memory location starts
with a virgin state a read operation
takes it to an our state write operation
takes it to W a real following our I
takes it to our if you follow several
reads it will remain in the same state
or write will transition back to W if
there is a right that follows are right
the automaton will detect it and report
it that is added right now to make it
precise we are doing this at byte level
there are no false positives or false
negatives yeah
that anything that leads to a dead right
is itself dead actually is that don't
make sure that's true I guess if you do
a backward slice from a dead right
mm-hmm that implies by dead you mean the
killing or the dead today yeah
everything that lead led to the dead
right is also useless um I'll take that
back those computation may be needed
later yes is what there is some
transitivity there really did you look
at this is something that someone else
also suggested me that okay if you have
like ten percent I'd rights how much is
the previous computation compute causing
that one that is transitive relationship
which is interesting to explore thank
you it's insufficient if I just told you
that there is thirty percent I'd right
you want to know where it happened what
other two parties involved and how you
reached those positions so that is why
you need calling context that pinpoint
where dead right happened and we're
killing right happened so and we do the
source level attribution along with
calling context and I ran this on
several coats i am showing you only the
ones on spec integer referencement
benchmarks in general dead rights are
very frequent lower is better here
higher dead right is bad about twenty
percent dead rights in spec integer
reference benchmark and GCC had
particularly astonishingly high dead
rights sixty percent in on average and
for one particular input there was
seventy-six percent head rights so i was
interested in knowing what's going on in
here and it happened in this piece of
code called as loop register scan where
GCC is walking through basic blocks
belonging to a loop and identifying what
are the registers being used so the
quarter begins here by allocating an
array of 16,000 elements to this last
set array and then it is walking through
each instruction of a basic block and a
particular pattern matches it says okay
that register is being used and then
once a basic block finishes it says okay
my work is done for this block let me
reset the whole array with zero and
start doing it for next block so that is
killing this assignment because
typically basic blocks
short median use of number of elements
in this array is only two and you have a
16,000 large arrays so dense array is a
poor choice of data data structure here
it keeps happening over and over again
as you visit several basic blocks so we
replace this array with sparse data
structures I used splay trees because it
has this property of frequently exists
position being in your cash and easy to
access Foss texts that led to about
twenty eight percent running time
improvement in GCC for many other codes
where it was useful for example NW come
had a redundant initialization of a
large memory buffer and when we
eliminated that redundant initialization
we gain fifty fifty percent speed up
when running on 48 goes bzip had a
particularly overly aggressive compiler
optimization that was hoisted from a non
executed code path on to a hot code path
and yeah that's a bad optimization that
was causing a lot of dead rights because
of stack spilling happening over and
over again when we eliminated that we
gained fourteen percent running time
improvement hummer which is a code for
DNA sequencing had dead rights not being
eliminated because of aliasing and once
we made compiler aware the two arrays
were not alias it removed dead rights as
well as it did vectorization for us and
be gained forty percent improvement in
running time so dead rights spy there
were lessons to learn from it dead rides
are common symptoms of performance in
efficiencies choose your data structures
prudently suggestions to compiler
writers is pay attention to your code
generation algorithms know when they may
do more harm than good understand when
optimize okay that is one and profiling
for wasteful resource consumption opens
a new avenue for application tuning and
context is really important in
understanding feedback from a tool you
want to know where dead rights happened
in what context can you optimize
actually these context-sensitive tools
are useful in many other situations
there are tools for correctness for
example data raise tools
where you want to know where a race
happened and what was the other exes
that led to the race and you want to
know the full cost call on context that
led to it there are other performance
analysis tools for example reuse
distance analysis where you want to know
where was the previous use and where is
the reuse and if there is a common
ancestor that you can hoist your use and
reuse then you can make better use of
memory hierarchy we have written a tool
an open source library that anybody can
use for their fine grain instrumentation
we call it as cct live in the interest
of time I will skip this portion but you
can stop me later if you are interested
so the last portion of this talk is the
mismatch that happens between
architectures and applications or unka
rhythms and how one can tailor for an
architecture modern architectures use
Numa for example this is IBM power 755
which has for IBM seven processors each
processor is four-way smt sharing l1 and
l2 cache that forms your first level of
Numa hierarchy there are eight cores
sharing the l3 as a victim cache that
forms the second level of Numa hierarchy
and they are connected through a vast
network forming the third level in the
Numa hierarchy if you thought 3 is the
number of levels in a NUMA hierarchy
will be surprised SGI you a thousand is
world's largest shared memory machine
with 4096 course and each node which is
also called as a blade is composed of
two Intel nehalem processors and every
node can access the memory of every
other node using loads and stores not
get sent puts not sensors so to a smt
within a core forms first level of
hierarchy 8 cores per socket from the
second level of Neumeier are key to
sockets on the same node from the third
level of Numa hierarchy and this is
logical diagram of a rack of these nodes
and there can be up to three hops from
one node to other that forms fourth
fifth and sixth level in the Numa
hierarchy and several such racks are
joined together to form this half of the
machine and there is 50
from one node to the other that forms
eight levels in the Numa hierarchy now
locks are one of the fundamental
features in shared memory architectures
distinguishing between loads and stores
and Ken circles right from they should
behave the same yeah gets inputs are
also as problematic as actually they're
even more problematic than loads and
stores you're right just because this is
a shared memory machine people tend to
write open MPP threaded style programs
the little difference in gates inputs is
you do not have a cache problem they are
you take the data from somewhere put it
into your big large buffer so the issue
is more of the communication between
North node than anything going through
your memory hierarchy make sense that's
why okay so locks are one of the
fundamental features in parallel
programs and they have been studied for
a long time and centralized locks are
bad and one of the good ones that
scalable locks is mcs lock mellor crummy
Scott John electro me and Scott lock and
I will explain what is mcs lock how it
works and what are the deficiencies of
that lock when you where you are using
it on a pneumo machine so MCS lock has a
lock variable each thread arrives with a
record which has a status field and the
next field here 31 arrives and let us
say it arrives in new model main one the
protocol involves swapping this lock
pointer to itself and if you had no
predecessor then you are the lock holder
and you enter the critical section in
the meantime let us say another thread
arrives in a different domain with its
own record now it swaps the stale
pointer to itself now this was cashed in
this domain so that was a cache miss for
this one now it is waiting and it wants
to tell its predecessor that he is the
successor and it knows who predecessor
is by swapping the tail pointer
it goes and pokes the next pointer and
says he is the successor while poking
that it incurred a cache miss because
this was present in a different new
model main and now it goes into a local
spin this thread eventually finishes
critical section and it wants to tell
its successor that he is the log holder
so first it accesses its next pointer
but that was cashed in this line because
that was the last accessor and now it
goes and touches the status field and
says you are the holder that is another
cache miss at some point this thread
realizes that the status has field
status has changed and because it was
previously exist by a thread in
different domain it also incurs another
cache miss and then the data that t1
exists is most likely to be except by t2
that is why we were holding a lock so
that whole data moves from one domain to
another domain so lock and the data axis
in the critical section keeping pawning
between one Yuma domain to another Numa
domain indiscriminately for example in
this MCS lock you have a queue of you
have a chain of waiters they may belong
two very different Numa domains where I
have indicated each node with the name
so data can just keeping pong in between
various Numa domains how bad is this it
is pretty bad actually here I have shown
what what is the time it takes the lock
to be passed from one level to other
level if you pass it within smt peers
what happens if you pass it to to code
sharing the same socket was not if you
pass it to from one code to the other
and so on on SGI you a thousand this is
the time taken which means lower is
better when you are passing within the
SMT the time is small but as you keep
passing it further and further away it
can be as many times as as many as two
and a half thousand times slower in
passing the lock so to address this we
have built an umma of a locking
algorithm we call it as hierarchical MCS
lock hmcs the insight is that the
passing the lock to a thread in a nearer
Numa domain incurs less overhead so
always pass the lock to waiting thread
in the nearest you
domain when possible so to do so we
organized the lock into a hierarchy of
queueing MCS lock mimicking the
underlying Numa hierarchy and
orchestrate the protocol such that
threads wait in locality domains if here
is how threads were waiting in the
original MCS log in the hierarchical MCS
lock they will wait in their own
locality domains for example if two
threads belong to same innermost domain
they will be lined up in the same queue
here each ello bar is one level of MCS
lock so every thread arrives if you are
the first one you have more overhead to
go further up and acquire the lock at a
higher level having acquired that in
high contention typically you have
waiters either in your domain or
somewhere in the nearest domain and you
keep passing it within for a bounded
number of times and make use of the
locality and once you have done that the
last thread which recognize which hits a
threshold passes it to someone in the
nearest domain and makes use of locality
it makes better use of data locality
Numa architectures high lock throughput
for highly contended locks a given lock
acquisition may incur higher latency
because if there is no contention you
you incur the cost of going up in the
tree we ensure starvation freedom by
bounding the number of times we pass and
we have a relaxed notion of fairness and
one can tailor the design of the lock
either for high-throughput or for high
fairness if you are interested in
fairness you're passing bound is shorter
if you are interested in throughput you
pass it much more number of times and
the bound on the passing is not a random
number it is based on the excess
latencies between two levels in the
memory hierarchy and a analytical model
of the queuing lock to take advantage of
the difference in latency between two
levels of hierarchy on this s giue
thousand machine that I showed you on
x-axis is number of threads y axis is
throughput that is how many times can i
acquire a lock per second so higher is
better MCS locks throughput keeps
falling each time a new numa domain is
added whereas the hierarchical MCS lock
reaches a stable point and detains very
high throughput throughout
this is K means which is an application
I took from mining benchmark and on x
axis is time y axis is how much
improvement it bc-based to using MCS
lock so black color is MCS locks
throughput I mean end-to-end application
time and red color is how much time did
hierarchical MCS locked it in this
application when the contention is low
MCS using hierarchical MCS lock has more
overhead it is about within eighty
percent of the MCS lock but as the
contention Rises hmcs law can do about
2.2 or 2.1 times better than using an
mcs lock you might be wondering how to
deal with this case of no contention we
used a trick of lamp or slow fast path
mechanism where if a thread arrives and
there is no contention instead of going
through the entire tree of locks we can
directly in queue at the root and make
the case of no contention very fast I
did not show the numbers with this
optimization and if that is not the case
you take a slow path and even within
slow path there is a hysteresis
mechanism which says if in the when the
when I came last time if there wasn't
enough contention at this lowest leaf
level let me directly go in and queue at
a level where I had noticed enough
contention and that makes our no
contention case about ninety-five
percent of the throughput of MCS lock
and high contention case about ninety
percent of the best hmcs for a given
configuration to conclude productions of
great system suffer from performance
losses I would think of hvz applications
as using a lot with just my ignorance of
these
education so canny for example the
Caymans I would expect them to be
embarrassingly parallel code with
various in between it's where are the
locks use very good question so what
happened k-means was they were using
atomic operations they were not using
locks and you would expect atomic
operations to do well more than better
than locks right so what happens is this
is on power 7 the way atomic operations
are implemented is you take reservation
you want to make an update and then you
write the location and when you have a
lot of threads you take reservation some
other guy takes reservation you lost
your reservation so they keep in pawning
data and no one makes progress so law so
atomic operation was terrible and what I
did was I replaced atomic operation with
mcs lock that gave about 11 x
improvement so there was this loop and
an inside the there were three atomic
operations so instead i put a lock
around the loop i said once take the
lock and do all your once you have taken
the reservation make changes to all the
locations that worked out much better so
the conventional wisdom i have talking
to folks in the policy folks who said
you know hey lock free era structure is
appended clocked data structures in
performance what you're seeing is that's
not necessarily the case depends is it a
small update or larger plate for short
sections for short critical sections you
may may not benefit from lock free but
if you have a large data to update you
would end up taking locks right yeah
that is really useful yeah awesome so
this is a case where one thought that
fine-grained atomic operations is better
than using locks turned out it was worse
yeah this may work okay on an x86
architecture where you are guaranteed of
some progress but if you are using
loading store conditional you are not
guaranteed of progress and the other
thing is with mcs lock the contention is
only for this tale pointer
with atomic operations everybody is
trying to bang on then there is a lot of
communication going on right in mcs lock
what happens q gets formed in high
contention one thread updates data say
increments of variable and passes it to
the next one and that's the one which
gets in cute so the contention is only
for this to two threads trying to NQ
that works out better than atomic
operations so so I mean again now coming
not from this area then whenever people
right talk about these floppy data
structures they always use as comparison
unlocked version straight Sox and but
what you're claiming is if you just use
your CSS the baseline maybe two dot psi
headed by AP better I don't know what
they compared it with out good they
might have compared with mcs there may
be cases where it might do well depends
on architecture somewhat at this point
it may do well on x86 because of
progress guarantees and then i am
showing you a highly highly contented
case if the contention is not so high
lock overhead maybe more than the
benefits you make it right because if
you have no one to pass the lock to then
you have only overhead of acquiring the
lock which is what happens in log v2
threads simultaneously try to do all
this and one guy wins and on all others
fail so the different variations of the
data structure where things get horrible
arcaded to avoid everybody contenting on
a single memory location
but so so I an interesting thing is if
you're using MCS which automatically
buys you that benefit why is there
somebody pictures and then then do we
need dog feeder structure all right
precisely figuring out when we need of
related structures might be interested
yeah there are trade-offs actually I do
not recommend using hierarchical MCS
lock left and right everywhere because
it takes some memory your tree takes
memory right because internal tree nodes
are pre-allocated you might not have
those memory requirements in locker a
data structures and also in simple locks
okay um production software suffer from
losses as I mentioned by resource
idleness faceful resource consumption
and mismatch between hardware and
software contributions of my work are
novel and efficient tools to pinpoint
and quantify performance losses adaptive
run time to eliminate wasteful resource
consumption and architecture of our
algorithms the impact is a new
perspective on application performance
and insights and improvements in
importance of we're going forward I am
interested in doing hardware software co
design tools to detect energy and power
way stage adoptive run times for undone
times that are aware of contention and
redundancy as well as doing data
analytics on large execution traces or
profiles collected I am interested in
tools for parallel programming tools
that are in the space of performance
correctness or debugging compiler center
times parallel algorithm including
verification and performance modeling
this is a collaborative work I am not
the only person to have done all of this
my advisor professor John malhotra me
Michael Fagan who is a researcher and
several others at Rice University have
been of great help my colleagues at
Lawrence Berkeley National Lab kaushik
sane at UC Berkeley Xu Lu is professor
at College of William and Mary we had a
collaboration with a Virginia Tech and
argonne national lab
Nathan element was X student at Rice
University he is currently at Pacific
Northwest National Lab and we had a
collaboration with NVIDIA that is my
talk I am happy to take more questions
if you have thank you for the real talk
that these tools look very interesting
do any of them work in windows not yet
there is work to be done thank you the
idea should work I like that locking
should be easy to do it right on Windows
our sampling base tools rely heavily on
linux interface of signaling signal
handling and things like that NW chem
works both on Windows and Linux but my
studies where on Linux so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>