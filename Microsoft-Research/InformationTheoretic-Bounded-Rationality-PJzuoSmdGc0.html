<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Information-Theoretic Bounded Rationality | Coder Coacher - Coaching Coders</title><meta content="Information-Theoretic Bounded Rationality - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Information-Theoretic Bounded Rationality</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PJzuoSmdGc0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
Oh perfect okay first of all okay thank
you very much for the what they invite
them I'm really happy to be here and
apologies for changing the talk because
I announced a talk on bounded
rationality but if you've never been
exposed to it then it was going to be
too specific so i decided to change it
with something that tries to encompass a
little bit all the work that have done
and i realized that i could organize the
material around three key concepts that
i'm going to talk about today so first
of all and just want to explain a little
bit what my motivations are so i'm
interested in the mathematical
foundations of intelligent behavior so
perhaps the term is a bit old school but
i'm interested in cybernetics and in
particular i'm interested in the
intersection between three areas a
little bit of psychology and mainly
machine learning and something died here
oops
I have no idea yeah I'll disconnect and
connect again to see whether i think my
laptop just
of course this never happens it happens
to right when I'm giving a talk
okay let's see
you
maybe I should switch to Microsoft
Office okay that was that was strange I
apology for that again okay as I was
saying I am interested in this
intersection and and the two main areas
are machine learning and theoretical
economics so so basically I'm
interesting machine learning but I have
this additional constraint that I if I'm
going to design an algorithm it has to
be somehow justified also from
principles that are taken from
theoretical economics so and this is the
outline of my talk first I'm going to
talk about expected utility theory which
is the like the foundation of of agency
that we use in for example reinforcement
learning then I'm going to talk about
the role of causality in agency this
these are the three key ideas so
causality an agency then adaptation how
do we come up with the with the behavior
that is that is universal over a given
class of behaviors then the final one
will be bounded rationality and then I
will conclude and the set up I have in
mind is the following so I have a two
systems that interact with each other
the first one is the world which is
given and this typically unknown and has
to be discovered and the second one is
the agent which is the one designed by
the engineer and the two of them take
turns interact and exchange symbols so
the agent produces actions the world
replies with observations and they're
drawn from distributions that are
conditioned on the past interactions and
as we observe these interactions they
define at the interactions you find an
interaction sequence for example this
one here which is like you can think of
it as numbers being written on the tape
where these numbers correspond to the
idea of a particular action which can be
drawn from a countable set or if you
want to generalize this you can also
think of an uncountable set now the
world can be any stochastic process so
in particular could be a multi-armed
bandit a markov decision process at Palm
DP or as I said any other controllable
stochastic process so
so it's important to remind ourselves of
were expect utility came from it's
actually expected tility theory is so
entrenched and is so natural to us
nowadays that they'd seen it almost it's
very hard to imagine that there was a
point where it wasn't so obvious okay so
it started basically with Bernoulli who
try to solve the st. Petersburg paradox
and you realize that people don't
optimize return but they optimize some
other function that is personal to them
between called utility won't go into the
details of how this was derived what
this is basically the birth date of the
concept of utility and then 90 in 1921
came up with the following key idea that
actually decision-makers take account
the uncertainty in order to make
rational decisions so someone had to
postulate this and then a major
advancement came from ramsey and
definity who operationalized the notion
of subjective probability so free before
that it was some sort of a vague concept
but they defined it operation in terms
of alpha backing attitude something
similar was done later by phone Norman
and mortgage them and they came up with
the idea of expected utility where you
take a decision maker you present the
decision maker with with choices and
then you tabulate all the answers the
preferences and from these preferences
you can then infer a unique utility
function so importantly here utility
because nowadays we have somehow reify
the concept of utility we start from the
concept of fertility we take it as a as
a given but originally it was defined as
something that you can regress on okay
so something that you derive from
behavior so savage then later came up
with the idea of combining expect
utility fear with subjective probability
and he came up with a different scheme
and in which decision makers and if you
tabulate their choices you can infer to
mathematical objects
on a unique probability measure over the
outcomes of the world and the unique up
to a final transformation unique utility
function in one framework and this is
basically the bedrock of modern
economics okay so when you do for
example entry for smelling yourself so
for the value function and things like
that it all comes from this these are
all theorems of this foundation here
okay I'm not much time passed until
someone realized that actually I'm
expect utility theory doesn't really
scale it works well when you have
microworlds when you have for example 10
different choices but if you have like
in reinforcement learning where you have
you want to plan over a horizon of
perhaps 100 time steps you have an
explosion of possible policies there and
then this theory doesn't work anymore
and so and this was basically the main
motivation of my work and my PhD studies
is there an alternative to this theory
that make sense from a from a
computational point of view so there is
no widely agreed upon theory of bounded
rationality today and but I'm going to
talk about three as I said three ideas
of how to tackle this problem so as I
was saying a consequence of subjective
extract utility theory is that when you
consider an agent that interacts with an
environment in steps and you don't know
the environment then you place a
distribution over all the possible
environment if this gives you a mixture
distribution and this mixer distribution
has sufficient statistics that are
unbounded and so in the end you end up
solving a decision tree like this one
here that grows exponentially in the
planning horizon and to solve this
problem you have to solve the bellman of
solve recursively the bellman optimality
equations which then defines which are
the optimal actions here in each one of
these decision nodes marked in red now
we see immediately that this in theory
should be very easy because you just
choose the policy that maximizes the
subjective expected utility and it
bypasses all the problems by the usual
encounter in reinforcement learning the
soles automatically the exploration
exploitation problem because it's by
definition by construction its optimal
but obviously this is intractable and
it's intractable view to two reasons the
first one is kind of obvious it's
because the because the number of
sufficient statistics is just too large
so it the policy space starts growing
exponentially with the planning horizon
and but the second one is a bit more
subtle it's essentially because the
policy choice has to constantly precede
the interactions so you have to come you
have to find the optimal policy first
you have to unroll all the possible
lives of the agent calculate the optimal
policy and then you start interacting
with the environment it's like it sounds
like the ways of computational resources
how come you come up with the optimal
action for an environment that you have
never seen so three ideas and the first
one is going to be that actions are
causing interventions the second one is
that if you want to achieve universal
behavior how do you build the universal
behavior that's easy you do an M
projection and you get the bayesian
mixture so that's the second point that
basic mixtures are universal over a
given class of environments and the last
one is that utility can be thought of as
an information constraint so let's
review first causality and for this let
me try to motivate this from a simple
example we have a graphical model we
want to have a graphical model over a
stochastic process it's just a simple
sequence X 1 X 2 X 3 and X 4 and so
forth and we don't know the parameter so
we place a prior over the hypothesis
space and this is the likelihood
function and now we make observations
let's say we observe the first two
values so we condition our model on
these values
and and we can calculate the posterior
over there over the hypothesis I just
multiplying the private the like it now
and let's assume that someone asked us
to generate some examples now so I come
and I generate x3 okay so this is my
simulated example that I just evaluate
their draw from this from the procedure
predictive distribution and now the
question is okay I start generating this
data and then suddenly I found myself a
with Amina can i just--can i take this
data that i have just generated and
conditioned my model on this data now
obviously this doesn't work I mean this
is like a machine learning heresy ok but
so in some cases this would would be
something desirable let me explain why
so if i wanted to insist on conditioning
on data that i have just generated from
the very model how would i do that well
if we insist then the first observation
is that if you generate data from your
model and now let's say you want to take
it and condition your model further on
it it should not change your beliefs
because this this data is a reflection
of the current state of knowledge so you
have the prior and this it would be your
posterior where you have the prior again
multiplied by the likelihood this
likelihood is a modified likelihood
because obviously cannot be the original
like look we have to do something to our
model for this to work some modification
so what will be this multiplication well
if the if the posterior and the prior
are proportional to each other then
necessarily the likelihood model has to
be independent of the parameter and once
you condition your model of the fur on
the parameter it doesn't really matter
how this distribution originally looked
like as long as the chosen as long as
the simulated data is in the support of
this distribution so in particular can
choose just the Delta function now this
operation that I just did taking this
piece of the
apical model and this piece here of the
graphical model and replacing it with
this Q and the particular de Stael to
function that's a causal intervention
okay so the council intervention is in a
sense telling the model that the data
doesn't carry evidence so how does this
look like well it's very simple I
generate x3 I do the causal intervention
it removes this incoming arc here but it
has still effects downstream in the
model as long as this graphical model
has causal semantics so out of all the
possible factorizations of your joint
distribution it represents this one is
the one that represents the causal order
or the functional dependencies between
your random variables so how does this
look like from a measure to Redick point
of view so if here we have the random
variable x1 x2 and x3 and we do
conditioning so we conditioned on the
second random variable on the
observation one which means that I now
go I search for all the realizations of
the stochastic process that are
incompatible with this observation I
remove them and I renormalize that's
just conditioning in the castle case
what I do is I remove the incompatible
and realizations but downstream of the
causal intervention and I renormalized
only these pieces that's the difference
okay so as a summary and the problem is
that simulate the data should not create
evidence when we conditioned on it the
key idea here is to use a causal
intervention because the cost of
intervention erases the evidence by
removing the dependency to the
functional preconditions and the
prophecies are well it directly it's an
it's something that operates directly on
the on the probability measure
um and the actions of an agent if you if
you now typically when you when you
design a reportable learning agent you
think of the prediction model as being
let's say a graphical model and the
polish is being something separate that
uses the graphical model now try to
internalize the policy into the
graphical model tool if you do that then
you get that actions are random
variables and that the actions have to
be treated by as as Earth's cause of
interventions and this can also be
extended to measure space but then you
need to put the partial order over the
first of all measure theory is not
enough you need to impose some causal
structure over the Sigma algebra when a
generator of the Sigma algebra okay
second part universality so how do we
learn an optimal behavior well let's
consider em the following problem to
motivate it so we have a sequence
prediction problem we have three
sequences here so the first way is one
three five seven nine eleven which one
will be the next that's kinda easy it
seems to be just the odd numbers no 13
and that's the rule the second sequence
is 14 9 16 25 3649 great the last one 1
2 3 4 29 because it's drawn from this
polynomial you know but what I want to
illustrate with this is that the
induction problem well this is the
induction problem we want to infer loss
from examples but there is no unique
solution to the induction problem so
what would be the principal we want to
use here so this is important in agency
because let's and because when you have
an agent the agent has to be able to
predict the responders of the
environment and it has to understand the
effects of its actions so let's go back
to this example now we have the agent
that is interacting with an environment
it has experienced two observations so
far
these observations since the agent has a
probabilistic model over these
observations it can use them to create
an up to create a code so these are this
is a binary encoding of these two
observations and this binary encoding
can be thought of as a proxy for the
modifications that have to be carried
out in the belief in the data structure
that represents the beliefs of the agent
so whenever it makes an observation this
triggers some changes in the belief
structure which can be the cost at least
can be represented as the length of the
binary sequence or binary encode the
command that you need to execute in
order to reflect the conditioning on on
this last observation and we know from
informations here with that prediction
and compression are essentially the same
thing there's two sides of the same coin
and these experiences imply minimal
changes to the belief structure of the
agent so the question now is can we
minimize these changes and the answer is
we can so the way this works is if you
want to construct an optimal adaptive
and compressor if you if you just want
to create a compressor is kind of easy
you're given a description of the
stochastic process you use the
probabilities to construct an optimal
code but what happens in the case when
you don't know the identity of the
source so you take all the possible
sources in this case there's only three
and you now consider the convex set
spanned by these guys so each one of the
stochastic process is now a vertex of
your convex set and now you minimize the
cross entropy you construct a new
stochastic process that is the minimizer
of the cross entropy to the average
source now the cross entropy has an
incompressible purchase as the entropy
and it has another part which is the
decal the collaborative urgent and so
that's really the part that you want to
optimize so when you do this
you get the end projection so this guy
here is going to be the one that
minimizes the distance to the average
stochastic process and it's actually
very simple because it's just going to
be an average of the original stochastic
processes that's your bayesian mixture
and it has the property that if you
conditioned on data generated by one of
these guys in the vertices then the
posterior predictive will converge to
the to the true distribution so we know
this so I'll skip it so okay that's for
the prediction part but what happens now
in the action part so let's continue
with this story we have the agent again
that has made two observations has
issued an action made an observation
this is the encoding that reflects and
how you the agent had to change it
believes to record this experience it
generates an action this is the encoding
of the command to change your beliefs
and now the question is can we can we
minimize this encoding here can we apply
the same idea that we use for prediction
to the action part two and actually we
can do that and that's when you get this
rule so so the idea again is that you
have a set of stochastic processes that
represent optimal behaviors between an
agent in the environment and you just
take the mixture of them and now you get
experience your condition on this
experience and you look at the posterior
predictive over your actions which
suggests the next action to take so you
just sample from it but there is a
caveat here that the past actions when
you conditioned on them remember that
these past actions now these actions are
drawn from the model itself therefore if
you want to condition your graphical
model say
on this action that you have just
generated you need to treat it as a
causal intervention precisely for the
reasons we saw in the previous section
if you do that so you end up sampling
your action from the posterior
predictive where as I said the past
actions have been treated us cause of
interventions this guy here you can
expand it to make the mixture explicit
and you can be implemented
algorithmically by just first sampling
from the posterior and using the sample
parameter as a plug-in parameter to
generate the action that's Thomson
sampling okay so let's see an example
very simple so we have a scenario where
we have two doors behind one of these
stores there is a price between the
other one between the behind the other
one there is a goat and there's two
possible world and the price is behind
door one or door 2 in world one your
action would be to open the one because
you know that the price is there in
world to you open door to okay so you
have two random variables one for your
action one for the observation now we
assume that we don't know which world we
are so we take the mixture this gives
you this mixture model here how do we
use it well if we assume that the prior
of that optimal actions is is just over
the two words is just one half then we
can just query this model for an action
and it's going to tell us that we can
just flip a coin a fair coin and then
choose that particular action so let's
do that let's say that we flip a coin
and it tells us to open door one so we
go and we open open door one but as we
open door one if we remember if we don't
treat our action as a causal
intervention then we basically trick
ourselves into believing that the price
HP is therefore behind door one
and the causal intervention is precisely
avoiding this so the intervention
essentially is a mechanism that tricks
the agent into believing that it
intended to take that action on long
okay um so applications this can be
applied for two multi-armed bandit I'm
pretty sure you're very familiar with
this so multi on bandits model-free
reinforcement learning I'm I'll cure
this is just a list of the things that I
have done in the past and but obviously
if you look into the literature now if
there is a lot I mean sure is just then
if you look at the dates back then this
was not so obvious um so as a summary
the problem is how do how to model
behavioral universality the key idea
here is to use a basin mixture over
behaviors and treat past actions as
cause of interventions and the
properties are that you in the end you
get a stochastic controller that that
with experience so it doesn't choose the
policy rather the star but it infers the
policy as the data arrives it exploits a
built-in reward mechanism in Bayes rule
because if you think of Bayes rule as
being a prediction strategy and you want
to understand what is the cost function
that is being optimized by it that you
get back to this am protection and so
when you do for example we force me
learning and on one hand you have a
Bayesian model to predict the
environment and the other hand you have
a utility function it's kind of weird
because you're using two different
utility functions to solve a problem the
policy part with you with some ran some
arbitrary utility function and the
prediction part with with the end
projection so why don't we just use the
same for everything that's exactly what
we did here so
we are just optimizing compression and
the maximization of utility is an
afterthought that is embedded into this
into the into Bayes rule well okay and
this is the last part so let's remember
again expect utility all you have to do
is just maximize the expected utility
that's your optimal action okay so you
have two urns here they contain red and
white balls and you have to pick one of
these urns I'm going to then randomly we
draw a ball show you the color and if
it's right you win which one would you
pick here obviously the left one no easy
now right okay so actually this is the
so-called expert paradox it seems kind
of trivial but this was the first big
blow to expect the utility theory most
of the people choose this one here but
if you think about it if you put on the
economist at then you would say well I
don't know what the proportions are here
so let me list all of them it gives you
101 different combinations so let me
place a uniform primer with them let's
marginalize over it and as a result aha
I get 5050 so the two of them are
actually identical okay nevertheless
most of the people choose this and then
you can come and say well they're just
not rational but there is a caveat here
Ellsberg at the time savage was still
alive and at work asked savage the
creator of subjective expected utility
and his answer was well obviously I will
go for this one here because this is not
a risk situation it's not known
probabilities there are unknown
probabilities and they affect your
behavior in a different way these are
the so-called ambiguities these are
probabilities that you have not
rationalized your
action is not a function of these
probabilities so in a sense you can say
that the unknown probabilities have have
an impact on the net value on the net
worth of a particular outcome second
example and pick the largest here yes
you see the uniform distribution over
the number of rebels and the
right-hander but in fact that prior
could be any number of an infinite space
of models not very every day and white
horse that's one the others and
unnoticed just it could become much my
favorite effectively say and sorry I
didn't understand the last part the
space is too big to to merchandise vapor
and yes that could be one explanation
but it's here the main point is that
these probabilities are unspecified so
it's different so they're in economics
there is a distinction between risk and
ambiguities both of them are
probabilities but risk are the ones that
you that you use in your deliberation to
come up with an optimal action
ambiguities are probabilities that
affect our the unknown unknowns
by presenting them with different yeah
so he experiments basically when you
expose people to this urn task
repeatedly they end up converting to the
maximum expect utility principle so it's
as if they somehow internalized the
assuming that let's assume that behind
this there is basically the same earn
that the two of them are identical
actually I didn't okay I didn't explain
why this is a paradox and so basically
people choose this urn here and then
they ask well okay before I draw a ball
here and i'll change the rules of the
game you win for white choose again so
people would switch if the more
consistent we'd expect utility theory
they would switch now because then
choosing the left urn is basically
reflecting the fact that they think that
it's somehow adversarial and here there
are just less red balls and therefore
see they should switch when they ask
them that they tell them that they
actually went for white they still stick
to this one that's what you have sort of
experimental okay pick the largest draw
here easy top one try again so when you
are planning and when you are building
an optimal controller over an unknown
environment it doesn't look like this it
looks like that picking out the optimal
quality is like finding a needle in a
haystack it's practically impossible so
it doesn't it just doesn't make any
sense we need to find a way to to
redefine the problem or find a different
optimization movie otherwise we're
always going to design algorithms that
are approximations so I want to find the
optimum with the the optimal controller
but my algorithm actually approximates
it and then the other atom aprox
it's the other thing you don't want
approximations you want the theory that
reflect what you're doing so and on top
of that in this particular setup and you
can't do better than random search
because it's completely unstructured
there is no symmetry that you can
explore it to define equivalent classes
and then say okay let me explore this
guy let me test this representative of
this class and then move to the other
class it's impossible you cannot do
better than just random search so okay
this brings me to the last part and here
actually there is a mistake here okay
there is mistake it these guys here the
vertices here are three possible actions
not distributions okay these are just
three actions so Delta functions and
this is a prior this is your prior
behavior prior choice before you have
done your deliberation then the
utilities arrived and utilities are
essentially imposing a linear constraint
on this set what you do now is you
compute the e projection the e
projection remember before we had this
guy here at the at the bottom and now we
have it at the top and the e projection
is the one that minimizes the dkl
subject to this constraint so the
difference of the other one because
sometimes people ask you the difference
to the other dkl is that the oddity kale
is like an or operation and this guy
here is an is an end operation it
incorporates a new constraint this is
all from information geometry so so this
is the decision making framework I've
been working on for the past few years
so a decision maker can be thought of as
transforming a prior behavior prior
choice probabilities into posterior
choice probabilities and
and it can be characterized as
maximizing the free energy functional
the free energy functional is just the
expected utility regularized by this
decal which is if you look at it closely
this is the e projection m it is so we
have different components here as I said
this is expect utility this guy here is
just the Lagrange multiplier if you want
or it's just the inverse and temperature
which controls and it controls the
quality of your answer and and this
models large-scale choice basis trust
can trust and risk sensitivity and other
information constraints so if we solve
this the optimal solution to this
optimization program has a closed form
solution which is given by this guy here
so we define the solution as being the
posterior choice which is just going to
be the prior times the likelihood and
the likelihood has this structure here
as I said inverse temperature and
utility if we take the optimal solution
we plug it back into the free energy
functional we get the certainty
equivalent the certainty equivalent is
probably the most important economic
term it when you have an uncertain
choice a distribution over possible
outcomes each one of these outcomes has
a has a different utility how do you
summarize this value in expected utility
theory you summarize it by taking the
expectation over the utilities therefore
the certainty equivalent is the
expectation but in this case we have a
wide range of different operators
depending on the choice of alpha in fact
if we take the certainty equivalent and
we very alpha we get this sigmoid here
when alpha is equal to 0
we recover the expectation when alpha is
positive it starts approximating the max
operator when alpha is negative it
starts approaching the main operator
this is important because remember when
your exam and when you have adversarial
environments or you play go or something
like that then you have you have to and
you have to use different decision rules
depending on the kind of agent you're
interacting with so for example you use
expecting max when you're dealing with
the stochastic environment you use
minimax when you're dealing with an
adversary at cetera under under this
view all of these basically boil down to
the same decision principle with
different inverse temperatures so for
alpha one the the distribution over
choices looks more or less like this
where the decision-maker becomes somehow
anti rational and tends to choose these
are ordered according to utility so
intense the decision-maker tends to
choose back you and how bad choices this
is just the expectation so it falls back
to the prior and this is approximating a
max operation so it puts more
probability mass on the better outcomes
so and the interesting thing is when you
solve this the solution has an
operational interpretation so the way
you can choose is by just obtaining
samples from the prior and then doing a
rejection sample you have some desired
target utility and you end up comparing
it with the with the utility you get
from the random sample and using
rejection sampling stochastically you
will decide whether you keep the sample
or not so you can achieve things like
this so here on this axis we have
utility and here we have the inverse
temperature as you increase the inverse
temperature you see that the performance
in utility starts increasing at the same
time this is the number of inspected
choices
okay so the number of rejected samples
in rejection sampling say as you
increase the inverse temperature this
thing starts increasing too but the
important point that port of thing to
notice here or this looks like this
looks almost linear okay but important
part to notice here that this is this is
actually concave so if you want to
improve upon this value here you need to
do a lot of search effort so the
marginal gain in utility is decreasing
in their search effort so there is no
real optimal solution here it's just a
trade-off you have to decide which trade
of you want between search complexity
and the quality of your answer now let's
get back to the certainty equivalent
value these are the classical operators
that you would find in game tree so this
is the main operator this is the just a
chance node and this is the max operator
which is usually the decision on the
node will reflecting the actions of the
agent this can be approximated now so if
you take a bounded rational choice by
setting an appropriate inverse
temperature you can approximate these
operators here which means that now you
can approximate any game tree with the
bounded rational tree where you were the
dip where you approximate each node by
setting the inverse temperature and
remember that we can solve this tree now
by sampling by forward sampling so you
don't have to apply dynamic programming
but you start sampling generating
rollouts of from this tree until your
rejection sampling Algrim accepts a
sample
and if we now focus on two-step decision
trees and let's assume that the further
this note here corresponds to the agents
action and this corresponds to the
environmental action and this guy here
has an inverse temperature alpha and the
other guy the environment has an inverse
temperature beta and it's interesting to
see that by choosing alpha and beta you
obtain this the decision rules that you
would get in classical economics so am i
running out of time so you have for
example this is mini max this is
expecting Max and here you have some
bounded rational versions of it so for
example here on this axis if you change
alpha you can move from anti rational to
rational or if you move beta you can
move from risk averse to risk-seeking
behavior so there is a whole continuum
of possible of risk attitudes and in
particular the Ellsberg paradox here if
you want to model it we have a first
step where the M where the configuration
of this urn here is wrong there is 101
different configurations we place a
uniform prior over the over them but the
inverse temperature of this particular
node is negative so it means that you
are you think this choice is going to be
adversarial so the posterior
distribution is going to be drawing and
is not going to follow this with the
unique formula what is going to end tend
to pick the worst outcomes and
applications these are not mine and so
for you probably familiar with earth cap
and sub M work path integral control or
to Doris linearly sold the book so
controlled I mean these are related and
and teach people on have worked on MVPs
with information constraints
and a friend of mine evangelos Totoro
has worked on real-time control of quad
copters and then in Daniel Browns group
a colleague of mine he has worked on
spike in your networks and on emergent
abstractions using this framework so as
a summary and the problem here was to
tack a large-scale decision spaces and
the key ideas here were first to
regularize the choices and second to
exploit stochastic this stochastic
computational model and the properties
are the dealers the boundary between
learning and planning and it's massively
parallel I zabal because I didn't speak
about that but you could see that if you
want to generate the sample you can in
principle do this in parallel it's of
controllable complexity by changing the
inverse temperatures you can basically
choose the size of size of your solution
space and it's also sensitive to higher
order moments of utility because if you
regularize with the DK out basically
you're just you're just saying that you
you don't care only about the expected
utility but also the variance and so
forth and it implements a rich class of
decision rules so that would be the end
so as a summary and the problem is that
expect utility is not suitable for a I
it's it's useful as a gold standard but
if we want to characterize what our
algorithms are are doing it's not
suitable anymore I'm not claiming that I
have a solution for it but it's an
approximation from a from a
probabilistic point of view the key
ideas here are the following so first of
all we treat actions and policies as
random variables in your model and
therefore you have to use caution
interventions to distinguish the
internally generated values from the
externally generated values the ones
that carry evidence to achieve
universality in behavior what you do is
you just give yourself a class of
possible behaviors and you mix
so and and finally you treat utility as
an information constraint so this our
collaborators this is the most important
slide of the talk so these are former
collaborator Daniel brown is group Kim
Kim from Christ Danny Lee you pen thai
late-ish be from the Hebrew University
and my supervisor zubin thank you much
yep one of the examples I we showed the
mixture between three stochastic process
yeah yes so and using University within
a restricted class like this find that
mixture I can understand you can recover
any mixture component within our samples
yeah yeah but it seems to me that in
general it would be quite difficult to
place a prior process of a large class
of stochastic processes it's kind of to
really achieve universality yes how does
work and I'm going to uni there for
coming down notable case yeah I'm
comfortable depending on your
observation space in action space you
know the class of stochastic processes
is so large that even placing a prior
process on that classes is potentially
quite quite difficult by de menezes yes
he's arriving in photos AIX I I got to
present on that but basically it's nice
it's almost impossible writing for for
universal computational processes it's
impossible yes while you're talking is
specifically about the Solomon on prior
and while we take the mixture over all
the computable environments essentially
then yeah you get a non computable semi
measure it's true yeah so you have to
get more calm hold an interesting subset
so yes you do that Disick's is it what
do you mean a different thing was
universality maybe
confused you know universality in the
sense that if you give yourself a class
of stochastic processes that you're
interested in and then you construct a
universal predictor of that so the AIX
the Sonoma no predictor is universal
that's true this is I mean theoretically
that's a fact now it's not computable
that's also true so then this deciding
how to choose how to carve out the class
such of the mixtures then result in in a
computable measure I don't know a
general result that tells his
distinction another question is maybe he
isn't alone yep so you freely combine
just utility function squeeze as a kind
of almost like Gibbs energy term
exponential minus you guys but so is it
always possible i mean the you to these
could be on a vastly different scale and
you only have this one parameter which
which can solve shrinks and higher scale
yes so it says this is just a minor
technicality and you can always any
utility function which is negative or
something can be accommodated or is
there some further restrictions on the
type of utilities hmm that's a good
question i i would say in principle I
cannot answer this question because I
think there is no restriction okay but I
have not seen a theorem that would deny
this another way of a sort of a theorem
that tells me that actually that these
shapes are arbitrary yeah yeah I don't
know that precise answer to your
question for all practical purposes you
can do whatever you want yeah
so when you want to link learning with
compression one way I can imagine is
resonance and deal framework or
normalized maxim like you yeah but there
is a big problem and gets really messy
if you want to go from discrete stuff to
continue stuff there so do you have any
idea how it can do for general class of
learning algorithm and like make the
link between or that is not the kind of
thing you're thinking about where their
approach is basically minimax right is
that the kind of think you want to build
between learning and compression or any
other no no and in my case I so I don't
I wouldn't know how to cook there is one
part of my theory that I don't know how
to connect with information theory which
is precisely conditioning on on the
events of measure 20 which is
essentially what you're asking so there
is no clear connection there so and I
wouldn't know how to answer this so all
the events that I'm considering our
events that have nonzero measure so it
would always be over an immeasurable say
let's say Brad set another more general
gage so so in the last part of the talk
you say basically with the agent changes
to believe such a monster work right
let's say kind of work for the agent and
then and then sort of traders off from
one personality to make a good choice
and you might work that's bad right yeah
but in a way maybe the agents are very
different so maybe insect brain is very
different from my brain is very very
different formula piece of silicon
right so so should they not be also a
model of the amount of work required to
do this belief change that is an active
area of research here so I think what
you're trying to point it is like a an
objective function that characterizes
intelligence perhaps like for example
the legs Universal measure this somehow
I may be simpler to just say how much
energy would take to change believes
exactly so this is like this is you know
being investigated now so perhaps
another measure would be to minimize the
mutual information between you predictor
and your policy you know subject to
constraints and then you get you don't
get a universal measure but you get a
family of measures depending on what
their constraints are but as I said this
is being investigated now you already
touched on some areas of related work or
future work sorry could you just briefly
outline kind of what the key questions
are that you would want to follow up on
or that you would want to work on in the
future well actually I'm I'm with
interested in seeing applications of
this yeah so I'm interested in seeing
how this applies to for example
probabilistic programs and the question
that he touched upon right now okay this
the the measure and the measure of
performance or the family of measures or
performance in that's it so there are
many different flavors for about
rationality right yes so some are more
descriptive on how humans act yes since
you say get someone more prescriptive
how we should build systems yes so so
the one other favor going
computational rationality can you just
make it brief comparison tolerate work
ok so the recess or the framework called
computational rationality and which
relies mainly on something called meta
reasoning the idea here is that an agent
when it has when the agent has to choose
an action he is also considering the the
cost of iming deception this costs are
the computational cost ok this is
different the reason why this is
different is as false in this particular
case when you have the agent when the
agent is capable of doing matter
reasoning then by stepping into by Mears
action by the action of stepping into
the meter level you are incurring in the
costs of thinking about the costs of
your choice so to solve this problem you
have to instantiate a new meter level
and then you have an infinite regress
and this cannot be solved unless you
introduce a cut somewhere now if I put
on the engineering hat then i would say
doesn't matter just the prior that's a
full stop no but from a from a
conceptual point of view this is a
problem this is a serious problem
because it requires the introduction of
a new principle in this case there is no
such a thing the the model that this is
records I mean the the sonority this is
modeling is an agent that is trying to
optimize the utility function but runs
out of resources and it cannot reason
about these resources okay so it's a
computer is just running running running
optimizing and family tie it runs out of
resources that's it so it better has an
any time algorithm here thank you very
much for the interesting talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>