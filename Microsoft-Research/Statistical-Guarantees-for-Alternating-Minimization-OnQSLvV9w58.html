<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Guarantees for Alternating Minimization | Coder Coacher - Coaching Coders</title><meta content="Statistical Guarantees for Alternating Minimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Guarantees for Alternating Minimization</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OnQSLvV9w58" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hey everyone hi New York people hey
there and today we have penny
Metropolitan finaid was an undergrad at
IIT and then after that you with your
Goldman Sachs yeah for a little while I
guess you had an outlandish lies down
after a turn of events decided to flee
Goldman come to the US where graduate
school at Austin and he's been doing a
lot of nice work in machine learning
algorithms and theory and recently been
doing some work and visual object
recognition he's going to talk about a
body work is been doing an authoring
minimization thanks Chum I will be
talking about our work establishing
statistical guarantees for alternating
minimization this is joint work with
Alec anima critique sujay andhra sheesh
i will start the top by motivating our
work with an example the example i will
be using is that of matrix completion
save this problem is this problem arises
from say movie recommendation systems
where we have a user movie rating matrix
where we observe the readings of some of
the movies by some of the uses and we
want to use this information to predict
the remaining ratings this matrix has a
lot of structure because the ratings of
the movies by users can usually be
explained by the movie features such as
genre of the movie and actors in the
movie and so on and the user preferences
for the same there has been a lot of
theory on how to solve this problem
using convex relaxation techniques
however in practice these are not the
algorithms that are used in practice
what is used is based on the simple
observation that if we knew the features
of each movie saving you the joiner
actor directors and all those details
about each movie given given the movies
that user likes or dislikes we can deter
we can predict the preferences of a
particular user so we can do this for
all users and once we know the user
preferences we can go back and estimate
the movies
features so we can guess one use this to
guess the other one and then do this
back and forth even though this
algorithm is used a lot in practice and
it seems to predict these rating matrix
quite well there has there have been
very few theoretical guarantees
regarding its performance in particular
it was not well known when this
algorithm works and when it does why it
does so and this sort of situation is
common for many machine learning
problems in particular for many of these
machine learning problems it turns out
that there are probable algorithms that
solve these problems exactly however in
practice we do not use these algorithms
but use efficient heuristics these
heuristics turn turn out to have very
good empirical performance but we do not
know why they do have such good
empirical performance our work tries to
bridge this gap by understanding why
such efficient heuristics which are used
in practice do work so well in practice
the heuristic that our work focuses on
is called alternating minimization
alternating minimization is a
metaheuristic for solving a large class
of optimization problems suppose we want
to minimize a function f where the
variables can be partitioned into two
sets U and V we choose a random view and
we fix this you and optimize over V and
then we fix this free and optimized
overview in many of these machine
learning settings it turns out that even
when the global optimization problem is
hard to solve we can solve each of these
individual problems very efficiently and
because of this efficiency empirically
this algorithm is very widely used for
instance the k-means algorithm for
solving for learning mixtures of mixture
of Gaussian random variables is a
specialization of this approach and
empirically it has also been observed to
have very good performance however on
the theoretical side the own though in
most of these settings the only
guarantees that are known regarding its
performance or that it converges to a
local minimum our work establishes
critical performance guarantees for
alternating minimization in three
different settings the first one is
matrix completion which is what I just
mentioned the second one is phase
retrieval this arises in optics
crystallography and more physics sort of
fields where we can observe magnitudes
of linear measurements but we do not
measure the phase information so we want
to recover a complex vector using linear
measurements but we only observe the
magnitude but not the phases of the
measurements and the third setting is
the sparse coding problem i will explain
this problem in detail later before i
proceed let me illustrate through this
plot the key challenge in analyzing or
understanding alternating minimization
the plot here represents the performance
of alternating minimization with a
random starting point on the phase
retrieval problem the x-axis shows the
iterations alternating minimization is
run for and the y-axis shows the
objective value of the function that we
are trying to minimize we can see that
the performance of alternating
minimization can be broadly separated
into two stages in the first stage the
error decreases but very very slowly
whereas in the second stage the error
decreases at quite a good rate and this
sort of behavior is common for
alternating minimization across many
settings that we have this flat stage
and then they decay stage it seems
plausible that we can say something
about what is happening in the second
stage whereas it seems hard for us to be
able to say anything about the first
stage because at least the error does
not seem to be a good indicator of if
something is going well or not so our
approach in obtaining statistical
guarantees for this algorithm is to come
up with sufficient conditions where this
DK starts happening and once we find
sufficient conditions for this point we
start ordinary minimization right at
that point and we get a decay right from
the beginning so in most of
cases that I will be that we have worked
on it turns out that the sufficient
conditions for decay are essentially
local convergence guarantees if we start
close enough to the global optimum then
alternating minimization does seem to
does a decay the objective value at a
linear rate and the second part is
coming up with this initialization point
from which this is guaranteed so this
turns out to be an approximation
algorithm where we obtain a point which
is close enough to the true optimum and
then we use alternating minimization to
refine this point to converge to the
global optimum the high-level approach I
mentioned is the same across all the
three problems that we have worked on
coming up with a approximation
approximating approximate initialization
algorithm and then showing local
convergence for alternating minimization
however the details of how we prove this
local convergence as well as the
algorithms we need to design for
initialization are quite different in
each of these three settings and so
today I will be talking about our work
on the sparse coding problem so the
sparse coding problem also known as the
problem of learning sparsely use
dictionaries is given the matrix Y to
decompose it as a product a star times X
star where X star is sparse in
applications why is called example
matrix so each column of Y is an example
a star is called the dictionary matrix
each column of a star is a dictionary
element or a dictionary adam and x star
is called the coefficient matrix the
sparsity of each column of x star means
that each example has a succinct
representation as a linear combination
of very few of these dictionary elements
in particular the first example here is
just a three linear combination of these
dictionary elements this decomposition
problem has many applications in
particular in image processing such as
compression denoising and so on I will
briefly talk about one application which
is relevant to machine learning
the application that is relevant to
machine learning is classification in
theory most of the classification
algorithms assume that the underlying
classifier is a very nice function of
the features say linear classifier or
polynomial classifier or something like
that whereas it whereas in practice the
sort of applications we encounter are
say image processing applications where
the input is pixel intensities and if we
try to apply classification algorithms
directly on this input it usually
results in very poor performance and
bridging this gap by first learning good
features from data and using the
classification algorithms on this
representation usually gives much better
performance the sparse coding a
representation is one such approach for
learning good representations from data
and so as to be able to use
classification algorithms on these
representations what is the dictionary
and the coefficients here in this case
ah we have done an experiment in the end
i will show you some dictionaries or
some pictures in toward sin given that
this problem has so many applications
there has been a lot of prior work on
this problem on the empirical side there
have been extensive applications
especially in image processing and
people have used this sparse coding
representation to obtain improved
performance in many of these tasks
people have also proposed several
heuristics such as case VD online
dictionary learning and so on and all of
these are variants of alternating
minimization however on the theoretical
side things are not so great there there
are no guarantees on the performance on
any of these above-mentioned heuristics
and the only result in this area is the
recent one by spielmann Wang and right
where the proposed a convex relaxation
based algorithm which can be solved
using linear programming and show that
it recovers the dictionary in the under
in what is known as under complete
setting I will explain what under
complete setting is later on in the talk
however in the practically more relevant
over complete setting there have been no
recovery algorithms our work presents
the first exact recovery algorithm in
the over complete sitting part of our
results and quite a bit more work
obtained independently and
simultaneously by Aurora game etre so
let us see how alternating minimization
tries to solve the sparse coding problem
so recall that we have a matrix Y and we
want to factorize it as a times X where
X is sparse so we write this
optimization problem subject to the
sparsity constraint on X this is a non
convex problem and so to solve this
alternating minimization does the
following we choose an initially
dictionary initial dictionary estimate a
0 and in each iteration we fix the
dictionary and then estimate the
coefficients XT and in the next step we
fix the coefficient and then estimate
the dictionary 80 I will detail each of
these steps now in the coefficient
estimation step we do sparse recovery so
we minimize the reconstruction error
fixing the dictionary and constraining
the sparsity of X this decouples into
many subproblems one for each column of
Y or each example so first column second
column third column and one and each of
these problems is a compressed sensing
problem compressed sensing problem
meaning we want to solve a system of
linear equations subject to sparsity
constraint on the solution and this
there are many standard compressing
algorithms to solve this problem and for
concreteness in this talk we will assume
that we will solve this using glass oh
so the hydrate will be updated according
to this where we want to minimize the
reconstruction error of that example
regularize by the one norm of the
solution and this one mom is is to
encourage party on the solution
for the dictionary estimation step we
fix the positions that we obtained in
the previous step and then minimize over
the dictionary and this is just a least
squares problem and we can use any
standard least squares algorithm to
solve this problem or going to be used
for the estimated coefficients what you
just need some we need robustness
guarantees on the sparse estimation step
lasso has such guarantees there are
other algorithm for any robust
compressing algorithm will work because
we cannot we do not know how to minimize
if we regularize it with the sparsity of
the solution whereas this is a convex
problem and we know how to optimize it
so we make a slight modification to the
previous algorithm which is that we have
a sequence of numbers epsilon T that are
given to our algorithm and in the K
titration after the coefficient
estimation step with Rishi old all
coefficients that are smaller than
epsilon t20 we can think of this
essentially as our confidence level a
titration t so a titration so as the
iterations increase our confidence gets
bigger is much better and so we can we
can trust the small coefficients as well
whereas in the initial iterations we
cannot turn the small coefficients as
much and so will threshold them to zero
when I go over the proof I will explain
where this exactly matters in experiment
this does not need to be necessary this
does not seem to be necessary so just
not makes no difference does not really
making it so we can see that we have
completely specified the iterative part
of the algorithm but we still have not
specified how we want to choose the
initial a zero before I describe the
initialization algorithm which will be
an approximate recovery algorithm let me
motivate our algorithm by the most
assumption in our work which is that of
incoherence so we we assume that the
examples have a true decomposition a
star x x star so there is a truth true
dictionary a star and we assume that the
true dictionary a star is incoherent
dictionary is said to be incoherent if
any to dictionary element that is any
two columns of a star are almost
orthogonal to each other and this is the
same as saying that the inner product
between them is very small I'll explain
later what small exactly means but for
now let us just say that it is small
this assumption is a natural natural one
to ensure the whelp oh sadness of this
decomposition problem even when we are
given the dictionary matrix a star
because if to dictionary elements were
close to each other then it may be hard
to tell apart if a given example is
using one dictionary element or the
other this is also a well widely used
assumption in a lot of prior work for
instance in establishing the stability
of such sparse decompositions so let us
now see what our initialization
algorithm is we want to find an
approximate version of the true
dictionary so suppose say we want to
find this second dictionary element the
idea we have is that we want to use to
use a lot of examples which share this
particular dictionary element so in
particular these green ones share the
pink dictionary element we put them in a
matrix since all of these here the same
dictionary element if they used all the
other dictionary elements in a random
fashion and since all the other
dictionary elements are almost
orthogonal to this one due to the
incurrence assumption the top similar
vector of this matrix is a good
approximation for this dictionary
element so the problem boils down to
finding a lot of examples sharing a
single dictionary element
re elements to mean so you're given
example yeah and I can well imagine I
mean let's say that the examples that
these four green ones are dependent on
in some old definition to dictionary
elements that you would want to combine
we'll just get a permutation of the
dictionary element the dictionary
elements so so you're looking for the
aid which is incoherent and given your
examples we'll do something like yeah so
our task reduces to finding lots of
dictionary lots of examples that share a
particular dictionary element we will do
this in two steps the first step is to
find a pair of examples that share this
single dictionary element and then we
will use this pair to find lots of other
examples which here this dictionary
element as well I'll explain how we do
this in the next slide so to do this and
arrange them with respect to the the
other one yeah yeah yeah yeah that will
be another assumption on the Korea so to
do this we define a graph known as the
correlation which we call the
correlation graph in this graph each
node corresponds to an example and we
draw an edge between two examples if
they have high inner product so we have
this graph and the first observation we
make on this graph is that if there is
an edge between two examples why I nyj
it means that they share a common
dictionary element this is because if
they do not share any dictionary
elements they have very few dictionary
elements and all of them are incoherent
with each other so the inner product
will be very small and so since the
inner product if large they have a
common dictionary element the second
observation is that each dictionary
element corresponds to a cluster in this
graph we which means to say that if you
have two examples which share a
dictionary element then with high
probability they will have an edge
between them this is because all the
other ones will be in current 50
using these two observations we want to
find we want to take an edge and you
want to say whether it has whether the
corresponding example share exactly one
dictionary element or more than one
dictionary element and we do this by
looking at the edge density among common
neighbors suppose we pick this
particular edge we can see that the
shade they share to dictionary elements
corresponding to s 1 and s 2 and the
edge density among common neighbors is
far away from 1 whereas if you pick a
good edge if you pick this edge which
share exactly one dictionary element
then all their common neighbors also
share the same dictionary element and
hence will have a just among themselves
so the edge density among common
neighbors there will be close to 1 so in
this case in this way we can distinguish
whether an edge has one intersection or
more than one intersection once we find
an edge that has exactly one
intersection we can use all the common
neighbors to get lots of examples that
share exactly this particular dictionary
element and then we use SVD on this
matrix to obtain the approximate version
of the corresponding dictionary element
a similar algorithm was developed also
by Aurora Gabe Mitra they also present a
different algorithm which does find
average instead of taking the top
singular component and it has better
performance guarantees so so once we
take these once you get these examples
instead of taking the matrix and then
the top singer component you average
these according to the signs contributed
by this fixture element that will also
give you an exact approximate recovery
approximate version of this dictionary
element and this has better performance
guarantees done yeah
this is really i graph is you just have
a threshold and you put in energy yeah
so then but you know it's the sign of
the inner products of that so you get
those signs so for our case we do not
really need the signs because we are not
averaging so SVD takes care of the sign
issue whereas if you are averaging you
need to know the sign SVD of the
neighborhood you make a covariance
matrix yeah so so for instance in this
case we take SVD of all the node all the
examples that are in this blue cluster
and then take the top Ziggler component
that will give you an approximate
version of the dictionary element that
corresponds to this cluster and their
algorithm instead of doing this single
er a single er component they they do a
signed average and that also gives you
an approximate version of the dictionary
element okay so now that I have we have
yeah I I think it should in any sort of
clustering algorithm which can find
large enough fluff stuff should work as
if you have the original graph know this
correlation got interesting you're on
the graph you might expect any
questioning ya gettin so now we have
specified our algorithm completely and
before I go on to describe our results
let me quantify the problem with some
parameters d here is the ambient
dimension so each example and each
dictionary element is a D dimensional
vector n is the number of examples so
why has n columns are is the size of the
dictionary or the number of dictionary
elements so a star has our columns and s
is the sparsity of each example in the
dictionary representation so each column
of x star is Esper's
and over complete setting means that the
size of the dictionary r is greater than
T so we have more dictionary elements
than the ambient dimension this
essentially means that the dictionary
elements are linearly dependent and this
is what makes it challenging that you
are a painting basically also because
you want to combine them right right now
right active try to get the independence
or the other ones and they are linearly
dependent so yeah do it yeah so this is
and that is why sparsity is essential in
this problem because without parsley
there are multiple representations in
the dictionary this is what seems to
work much better in practice so if you
want to do compression or something like
that if you do under complete sort of
representations it does not really seem
to get very good reconstructions and
things like that whereas if you do over
complete things we get much better
performance
so the assumptions for our results are
as follows we have a true dictionary a
star and a true coefficient matrix X
star and we make the following
assumptions on the dictionary and the
coefficient matrix on the dictionary
matrix we assume that we have pairwise
incurrence which is what we saw before
and the scaling we assume in particular
is that inner product between any two
distinct dictionary elements is at most
one over square root B so d is the
dimension of each of these vectors so if
these two were random then this is
essentially satisfied and this is the
scaling that predominantly used in Prior
on all prior work on this problem using
incurrence assumptions the second
assumption is on the spectral norm this
assumption is not crucial for our
results to hold in the sense that we
allow you one to scale and our results
capture the scaly the dependence on you
one but i'll use a constant scaling of
mu 1 for illustration since this is also
the scaling suggested by random
dictionaries on the coefficient matrix
we assume that each column of the
coefficient matrix has uniformly
randomly chosen espar support that is
all dictionary elements are uniformly it
shows in uniformly at random in an
example and every nonzero element is
drawn iid from some distribution and
which and this distribution is supported
away from 0 there is a lower bound and
also bounded by a constant capital R so
these are the assumptions for our
results ok so under these assumptions
are result says that if we initialize
the alternating minimization algorithm
using the clustering based algorithm I
mentioned and if the sparsity is at most
D to the one over 9 and we choose the
thresholds appropriately then the sample
complexity of this problem which is the
number of example
we need to see to recover the dictionary
exactly scales at most as R square log R
note that this is under complete fitting
and so r is greater than B and once we
have these many samples we also have
exponential convergence to the truth
that is in each iteration of alternating
minimization the distance between our
current estimate and the global optimum
decreases by a constant factor say half
so if you want an accuracy of epsilon
then in law of one over epsilon
iterations alternating minimization
gives you the epsilon close solution so
there are multiple bottlenecks here so
the first one is that if we use the
averaging algorithm of Aurora game oi
try instead of our initialization
algorithm we get this to be D to the 1
over 6 and that is essentially the
bottleneck of the alternating iterative
part or a minimization click send yeah
and if you have this barn act like 4
matrix completion I compared to the
Madonna horizon those guys yeah yeah
they have some guarantee for this
varsity level and the matrix and is it
slightly worse for minimization to kick
yeah yeah even in matrix completion our
results are suboptimal as compared to
the convex relaxation basting even for
the decay of alternating minimization we
seem to I mean that is where we seem to
lose in matrix completion for the decay
to happen what we can show is that it
needs more sample what we can show
requires more samples but this a result
is still suboptimal even for alternating
minimization I think the initialization
not necessarily the initialization is
just in the iterative part even if you
are arbitrarily close even then the
decay in error seems to have some
bottleneck in terms as I mentioned the
proof of this result is in two parts the
first one is that the initialization
gives you gives us an approximate
solution and the second part is to show
a local convergence guarantee for
alternating minimization I will in the
next slide I will briefly actually
present the proof outline for the local
convergence part of alternating
minimization so we call that in
alternating minimization we have two
steps the first step was sparse
estimation so where we solve la so we
have this equation and this is update
for XT plus 1 and 80 is the current
estimate for the dictionary what we what
we assume here is that 80 is close to a
star if we knew the true dictionary a
star then we would be solving this
problem instead of 80 and by the
correctness of lasso the solution to
this problem will just be the true
coefficient of x star since we know that
ET is close to a star this implies that
X T plus 1 is actually close to X star
by just the continued
of this problem after this if we recall
we do a thresholding step based on our
confidence in our current estimate of
the coefficients so the thresholding
step ensures that the support of our
current iterate XT plus one is actually
contained in the support of X star this
is because suppose an element of X star
was 0 then after the lasso step since XT
plus 1 was close to X star the
corresponding element in XT plus 1 was
very small and after the thresholding
step we would have said it to 0 and so
the support of XT plus 1 is contained in
the support of X star this is very
important for our results because if we
establish something on the support of X
star which is just one random matrix
then we essentially have the same
property for all nitrates in each and
every iteration so we do not need to use
a Union bound or anything of that sort
have the right support yeah but by the
thresholding operator we always ensure
that our support is a subset of the to
support so it's actually growing yeah so
in particular in the beginning
iterations we capture all the large
coefficients and as the iterations
progress we start to capture smaller and
smaller coefficients okay so these are
the two things that we get after and
note that since we are thresholding only
very small elements we really do not
deserve this because we are only
changing the matrix xt plus 1 by very
little numbers because we have to show
three folding only very small elements
to zero and in step two we will be
solving this least squares problem where
we update the dictionary 80 plus one
with this argument and we actually use
these two assumptions to show that 80
plus one is much closer to a star and it
is very important that we have
sort of a structured error in our
estimates if we did not have something
like this and we if we only had XT plus
one was close to X star using the
robustness of least squares we could
again say eighty plus one was close to a
star but then it could have the distance
would have grown as compared to the
distance in the previous iteration and
it is very important for us to show that
the error in fact decreases by a
constant at least a constant number in
each iteration and for that the most
crucial observation is that the errors
are structured and in all of our results
in alternating minimization this is
essentially the crux of the result this
is to understand the structure of the
errors and see that they in fact are not
arbitrary and adversarial so we try to
implement this algorithm in practice and
it turned out that the initialization
algorithm was giving us the same
dictionary elements over and over again
so then we realize that the there is an
issue with our initialization algorithm
which is that if there are dictionary
dictionary elements which are used with
very low coefficient then they do not
turn up in the correlation graph and so
we will not find them no that was
something so if we developed a practical
variant of this algorithm including the
initialization algorithm which is as
follows we suppose we want to find 200
dictionary elements and we use a batch
set of batch size of 5 we take the
examples and in the first iteration the
residuals is the same as the examples we
do this clustering algorithm on the
residuals we get five dictionary
elements instead of 200 we just get five
we use these to do the alternating
minimization then we compute the
residuals and then we do this clustering
algorithm on the residuals received well
graph get five more dictionary elements
get a big ten ten dictionary element
matrix or combining this with the
previous dictionary elements and do
alternating minimization and then
procedure keep doing this until we get
as many dictionary elements as
to get so we implemented this algorithm
on em this data set which has 60,000
images of handwritten digits and here
the original and reconstructed images in
particular here we took each eight cross
eight patch and we coded it using just
55 sparse reconstruction's so so 8 plus
8 is a 64 dimensional vector and we use
200 dictionary elements and reconstruct
each of these patches using just five
parts 30 reconstruction and these are
the reconstruction and the dictionary
elements that we obtained on these
things are looks look like this so we
can observe that these dictionary
elements are localized and oriented and
in particular they are like lines and
blocks and things like that and this is
also one of the reasons why sparse
coding representation has refused so
much attention because this is
apparently how neurons fire in visual
cortex of people and so on okay so to
summarize we saw that alternating
minimization is a popular empirically a
popular empirical empirical approach for
solving many problems and it usually has
good performance in most of these
settings and our work establishes the
first theoretical guarantees for
alternating minimization in various
settings a key component of our result
is the initialization schemes which give
approximate recovery algorithms and
ensure that alternating minimization
when started from this point these
initialization points does actually
converge probably there are a lot of
directions for future work in this area
the first one is that even though we
have results for this algorithm in these
different settings the proofs of all
these things are quite different from
each other apart from the high level
structure
however alternating minimization seems
to work very well in general for many
problems and it is plausible that there
is a general theory behind alternating
minimization and it will be very
interesting to see if we can come up
with a general understanding of when and
why this this heuristic works the second
one is motivated again by my first
picture which is that alternating
minimization has these two phases which
is the flat face and then the decay
phase and in random initialization this
flat face seems unavoidable but it is
not at all clear what is happening in
this flat face is it that it is just
going around and somehow luckily landing
in the right place or is it doing
something that is not really captured by
the error is not completely clear and it
will be very interesting to understand
which of these cases is true here and
finally the goal of any of this work or
all of this work is essentially to use
these intuitions to develop more
efficient and robust algorithms for
practical problems and in this direction
we have been working with Chum on image
classification on image net trying to
come up with the right sort of
representations where which help for
classification and many of these other
tasks as well and we hope that our
intuitions in these optimization
algorithms helps us help us to get a
better idea on how to design algorithms
for obtaining these representations as
well so this yeah this is
just do something a little
counterintuitive so in your algorithm
you you can start the algorithm when
things aren't correct I mean you don't
have the correct support set like it
converges to the right answer but if I
understand correctly wrong you guys
algorithm you basically nail down the
support center at the deyoung y'all so
but but which seems out about this is it
seems like more natural that you would
hope they are rhythm can work without
getting the support side correct at the
beginning but you get a weaker guarantee
because if you initialize it sorta from
that starting point you get a better
guarantee right so that is essentially
the sub optimality of our initialization
algorithm so in particular so they're
averaging sort of algorithm can get you
exactly clear elements if you had
infinite samples whereas this SVD sort
of algorithm well somehow it feels nice
that in practice that you have an
algorithm which so it doesn't need to
help the support set correct right at
the beginning because this is a pretty
strong yeah and you have that here but
somehow it works with weaker I mean you
don't get as good a way you have this
idea to the 19 + 2 16 yeah well so if we
use their an initialization algorithm
with alternating minimization will get D
to the 16 when you starting inserted
with the cracks no it need not be the
correct support even say this not the
correct support because in our world we
have additional assumptions that the
increase in X matrix and not be very
close to 0 so after thresholding you get
the exact support 00 but the interior
case that's very possible that this very
close to zero and then after a special I
see I see so we have a lower bound for
the initialization algorithm so since I
presented both the results together we
have the lower bound assumption but use
the lower bound assumption only in the
initialization algorithm not in
alternating minimization so we do not
need to get the support set exactly in
the first iteration so the support set
could have errors and then alternating
minimization will correct errors as and
when it gets the appropriate levels
maybe three yeah yeah so um if I
understand what you said you can if I
want to run ocean a minimization in
practice I can either just run it from a
random starting point and I tolerate the
initial slope being not that great or i
can use your fancier nogales patients
game so in practice is it worthwhile to
use the so in one of the things so we
have these three things in phase
retrieval on some of the experiments
we're on it turns out that if we
initialize it in a principled manner the
sample complexity is better by a
constant factor this is in experiments
again but apart from that we have not
really been able to come up with a
practical reason for using
initialization algorithm given
especially given the competition accost
of the initialization algorithm for a
random point you just get a random point
and start but as for this you have to
run the initialization algorithm it will
take a lot of time and then do this so I
would say there we have not seen any
real need for the centralization
algorithm in practice right something
you can arise yeah this is something we
can say something about whereas random
initialization is still open it's
amazing there last experiment is
interesting that sort of didn't work
when your honor to be anemic it's
possible that most of the cases that
work in practice are working because
those are the easier cases and these
harder / complete cases like you try it
and it just doesn't work like the first
thing you tried on the stage a thing so
actually on this detecting again random
initialization does faces oh so this was
we did our initialization at once and
that was giving giving those only like
five dictionary element widest
conviction elements so we had to do our
initialization algorithm in phases
whereas if you choose random 200 patches
sort of the image that are far apart
from each other in a reasonable way and
do alternating minimization it runs just
fine I see great okay that's again since
we're k maneesha that's at kimmys plus
plus this is that people do in practice
yeah yeah yeah yeahs is very far from
the random
yeah so if you initialize these things
by random vectors then it will not work
but if you initialize them randomly from
among the patches and ensure that any
two dictionary elements are not close
really close to each other then it just
works fine you have to come up with like
very simple heuristics to avoid bad
cases by intentional ization and then it
seems to work genious first versus is
all the time okay patches and take
things far away from that and start from
that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>