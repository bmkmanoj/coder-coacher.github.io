<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Analysis of Spectral Graph Algorithms through HIgher Eigenvalues | Coder Coacher - Coaching Coders</title><meta content="New Analysis of Spectral Graph Algorithms through HIgher Eigenvalues - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Analysis of Spectral Graph Algorithms through HIgher Eigenvalues</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jJE_TD-5zr8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
good morning were happy to have Shyam a
veteran from Stanford to tell us about
spectral graph algorithms hello hello
everyone
thanks for inviting me here it's very
good to be around so I'm I won't talk
about new analysis of spectral graph
algorithms through higher eigen values
okay all right
suppose we have a set of data points
okay and we want to cluster them in one
way to do the clustering is first to
construct the graph based on these data
points and then use the graph
partitioning algorithm how can we
construct the graph first we put a
vertex for each for each of the data
points now here I connected two vertices
if their distance is less than a
threshold Epsilon
okay so there are also other ways to
construct the graph for example you may
construct a complete graph a complete
weighted graph where the weights between
each pair of vertices is a function of
their distance
for example you may use this function
into the - this NASA squared /
normalized by some constant signal okay
so once we constructed the graph you
want to use you want to partition the
graph so how can we partition this graph
we can use a spectral graph partitioning
algorithms okay so so first we compute
multiple eigenvectors of the graph say
we 1 up to VK then we use this eigen
vectors to embed this graph in a
k-dimensional space so basically what we
do is that we construct this matrix F
where the columns of the matrix are the
vectors V 1 up to VK now the rows of
this matrix would give me the coordinate
of the vertices so the coordinate of
vertex I would be V 1 a 5 V 2 of I up to
VK OVA okay so so this gives me a
mapping of my graph to decide
in space maybe for the graph I showed in
last slide this is the embedding now I
can use you know one of the heuristics
for example came in to partition the
vertices in this high dimensional and
meaning and and that basically gives the
partitioning of my graph so so putting
this all together we get we get the
spectral clustering algorithm okay so
let me summarize it again we start from
data points then we construct the graph
then we look at the eigenvectors of the
graph and we embedded in a high
dimensional space then we apply k-means
partition these vertices and we use that
to get a partitioning of the original
points just map it back to the original
data points okay so the spectral
clustering has been used for over twenty
years it has a lot of applications I
chose this particular paper and Jordan
advise that this is one of the very
famous papers in this direction okay so
it is clear good in general a spectral
graph algorithms or simple heuristics
that explore graph structure using
several eigenvectors of the graph okay
these algorithms are widely used in
practice because of the following
reasons first of all they are very
simple to implement if you have a linear
algebra library you can simply implement
them they provide very high quality
solutions usually and they run very fast
they run usually in linear in near
linear time ok here are some
applications for example I talked our
applications in data clustering you can
also use them in image segmentation
community detection VLSI design and many
other fields so for example in image
segmentation what you do is that you you
put a vertex in a graph for each of the
pixels in the image and that the weight
of that edge between
you know a pair of vertices the you know
function of the distance of the pixels
and how close their colors or the
intensity are and then you you partition
this graph and you get a segmentation of
the image okay now let me tell you what
we know about spectral graph algorithms
in theory classical analysis of the
spectral graph algorithms only exploit
the first or last two eigen values of
the graph organ values or eigenvectors
of the graph okay here are some examples
for example we have found on the
chromatic number of the graph cheer is
inequality the algorithms of finding
edge separator of a graph even the
algorithms finding maximum cut there are
many others I'm gonna say later but all
these algorithms only use the first or
last two eigenvalues okay I should
mention that in some randomized
semi-random models we know algorithms
that use matrix perturbation theory and
might and multiple eigenvectors but here
I do not assume anything about the graph
I mean the results will be unconditional
okay so let me summarize what I said so
far let me put theory and practice
together so so I said in practice we
typically use multiple eigen vectors and
more eigenvectors give better solutions
in theory we can analyze two eigen
vectors or two eigen values and show
that you know may give good results okay
now let me tell you what we do so
basically we analyze or study the
spectral graph algorithms through the
lens of high riding values okay so you
can see our result as a bridge between
theory and practice so let me just give
an overview of the results without
giving too into details at this point so
for example we relate the case eigen
value of a graph to K way partitioning
we can use high writing values to
justify the performance of a spectral
algorithms in
this we can also use higher eigenvalues
to even give faster spectral algorithms
okay now if you want to see the actual
quantitative version of this results you
should give me some time that I set up
some notations and then I'll give I give
the details okay
so here is what I'm gonna do I'm gonna
start by talking about GG's inequality
okay here I'm going to set up some
notations and then I will focus on our
results I will talk about these three
results and finally I will very mean if
we talk about other aspects of my
research and some future direction okay
is there any question at this point
okay so let me start by talking about
chickens in equality so for the next
five minutes I'm gonna define a signal
inequality and set up some notations
okay let L be the BIA normalization of
the adjacency matrix of a graph this is
also known as the normalized laplacian
matrix let me not define it rigorously
but the point is we just normalize the
entry corresponding to each edge of the
graph by the degree of its endpoints we
use this normalization to basically
normalize the eigenvalues to fit in this
constant range between zero and two okay
so this gives us the normalization of
the eigenvalues so so throughout the
talk I'm gonna use lambda 1 up to lambda
n as the eigenvalues of this normalized
laplacian matrix okay number one is
always zero and lambda 2 lambda n is
always at most two okay now there is a
basic fact in in algebraic graph theory
which says that lambda 2 is equal to 0
if and only if the graph is disconnected
now cheers inequality gives the robust
version of this fact ok
you can imagine what it would say right
it would say lambda 2 is very close to 0
if and only if the graph is barely
connected but to give the actual
contracted version I need to define a
robust version of connectivity ok so let
me do that first and then give you the
sugar inequality ok so so what is that
what is a robust version of connectivity
I'm going to use conductance as a robust
version of connectivity okay so the
conductance of a set s of vertices is
the ratio of the number of edges leaving
the set to the sum of the degrees of
vertices in s ok for example in this
picture the conductance of this set is
1/8 because
two edges are leaving the set and the
summation of vertex degrees is 16 now
conductance is always the quantity
between 0 &amp;amp; 1 it can easily show that
and the point is the closer to 0 means
that we have a better cluster for
example if the edges of the graph
represents friendships in a social
network then a set with a small
conductance would represent a community
in a social net or if the edges of the
graph represents the similarity between
data points then a set with small
conductance would represent a cluster
update okay so this part is cut off the
graph is the set with the least the
smallest conductors we're going to
choose it among all of the sets that
have at most half of the vertices or
half of the vertex decrease okay so I'm
going to use this parameter Phi of G as
the set with the smallest conductance in
the graph okay now I'm ready to tell you
the chiggers inequality so cheesin
equality for graphs is proved by Allen
and Melman and it says the following for
every graph G Phi of T is very well
characterized by lambda - it is at least
one half of lambda - it is at most root
2 lambda - okay so let me tell you how
you should read this you should read
this as follows G is barely connected if
and only if lambda two is very close to
zero the importance of this inequality
is that it is it is independent of the
size of the graph so no matter how large
your graph is you still get the same
characterization the proof of this
inequality would give you a simple
linear time algorithm and in fact it
shows that the spectral partitioning
algorithm for graphs would work if K is
equal to 2 if you
wanna partition your graph into two sets
you use the second eigenvector it would
work I should give you a set of
conductance at most school route of fire
G okay good so let me tell you some
applications of this inequality
she carries Nakata is one of the
fundamental results in the spectral
graph theory and has applications in
various fields of counter science for
example in algorithm design many of the
approximation algorithms use this
inequality or algorithms that looks for
a separator of a graph there is a whole
literature in from in probability theory
that use Chi gears in equality to
analyze the mixing time of random walks
and you can use this inequality to
design algorithms that can sample from
very sophisticated distributions there's
also many applications the complexity
theory and cryptography for example in
constructing expand their graphs or in
error correcting codes okay
like now that we all understand Sugar's
inequality let me tell you our results
our contributions so as I said we
analyzed a spectral graph algorithms
with a theme of high riding values in a
joint work with Li and Travis on we
prove the higher order variants of
chicken triggers inequality you
basically relate the conductance of cuts
in K way partitioning x' to lambda K the
K eigen value of the normalized
laplacian this will provide a rigorous
justification of the spectral clustering
algorithm that I described at the
beginning of the talk in a joint work
with quote lilo and Travis on he managed
to improve chiggers inequality using
higher eigenvalues of the graph and this
gives us a rigorous justification of the
performance of the great performance of
a spectral partitioning algorithms in
practice also in a joint work with
Travis on we prove it is not fast local
algorithms for finding small communities
in social networks this basically gives
gives an algorithm the same guarantee as
of the spectral partitioning but with
the advantage that it can be running sub
linear time okay so so I'm gonna
basically talk about these three results
and all they spend most of the time
talking about the first result give you
some ideas and then I'll talk about the
last two okay so next I'm going to talk
about high order chigger inequality
I ordered it so for the next 10 to 15
minutes I'm going to talk about this
result and then talk about last
so here you want to study the following
problem K clustering problem okay so we
are given an undirected graph it can be
weighted but let me assume it's not
weighted to make it simply to make it
make notation simpler we want to
determine K disjoint clusters as 1 up to
a scale of small conductance all right
so for example if this is our graph
maybe we find these clusters the
conductance is as follows now the
quality of this clustering is just the
worst conductance of these sets just a
maximum conductance of this sets
so here it's 2 8 all right so so in
general we if we find a clustering into
s 1 or s K we define its quality as the
maximum conductance and our goal is is
to find a K clustering whose maximum
conductance is as as small as possible
ok so the optimum is is the clustering
that achieves this and I'm going to use
this parameter Phi of K for the optimum
okay so again Phi of K is a K clustering
whose maximum conductance is as small as
possible so let me tell you what we
prove you join work with me and Travis
on we show that Phi of K is very well
characterized by lambda K this proves
the conjecture by me khlo the point is
here similar to chiggers inequality
there is no dependency to the size of
the graph so no matter how large the
graph is I still get the same
characterization there is a dependency
to the size of the cluster in K but not
to the size of the graph ok
let me show you an example to understand
this better say our graph is just a
cycle okay
what is a K clustering of a cycle
basically all you need to do is to find
K disjoint paths each of length about n
over K this is the best weekend and
therefore Phi of K will be K over
because the conductance of all these
sets will be about K over N or two K
over N okay so so this 5k will be our K
over N lambda K will be about K over and
squared for a sec so putting these two
together you see Phi of K is less than
root lambda K you don't even have a
dependency to K in the right answer okay
this is clear so what else do we prove
we show that if you're allowed to use
two case eigen value lambda 2 K instead
of lambda K then we can significantly
improve the dependency to K from K
square to root lucky and if the graph is
low dimensional it's a planar bound
ingenious craft we can completely get
rid of dependency to K okay so the
second result here a weaker version of
the second result is proved by
independently by Lewis ragavendra
Italian van Paula both of the second and
third results are optimal up to constant
factors for a first result we don't know
still is an open problem okay
founded by constant yeah it's a genus is
bounded by a logarithmic it was just
okay so can you replace lambda with you
well it's a love the three happy yeah we
can do that yeah for any constant
greater than one
yeah if you use 1 plus epsilon than you
do some some function others in addition
to this our proof is constructive it
basically gives you an algorithm to find
s 1 up to a scale such that the maximum
conductance is at most order of K
squared Phi K all of these results are
constructed we can so you know the proof
also provides a rigorous justification
for a spectral clustering algorithm for
example shows that you know your data or
your graph has a good K clustering if
and only if lambda K is very close to 0
okay
let me tell you a little bit of the
proof okay so since I have a limited
time I try to give you the main ideas
okay so for the next five to seven
minutes I'm gonna talk about two five
years before saying the ideas that mean
first define a continuous relaxation of
conductors and then tell you their ideas
okay so I'm gonna use Rayleigh quotient
as the continuous relaxation of the
conductance but for a vector X Rayleigh
quotient is this quantity okay you don't
need to understand this the point is if
our vector X is a 0 one function is 0 1
vector then this quantity will be
exactly equal to the conductance of the
support of X can easily check that in in
this case the numerator would be exactly
the number of edges leaving the support
and the denominator is exactly the
summation of the degree of vertices in
the support okay so so basically the
point is if you could for example find
the 0 1 vector minimizing this Rayleigh
quotient that would give us as part the
less parts this cut off a graph so you
cannot do it sending a heart problem so
now let's see what are the optimizers of
this continuous relaxation the nice
thing about this is that the optimizers
the minimizer's are exactly the
eigenvectors of the normal as laplacian
matrix after you need some you need to
do some normalization by a decrease well
after the normalization that optimizers
are exactly the eigenvectors in fact so
vivan is them minimize that of this R of
V 1 is lambda 1 V 2 is the minimizer of
the Rayleigh quotient over all vectors
that are orthogonal to V 1 and so on and
so forth 3 is the one that minimizes
over all vectors that are orthogonal to
V 1 and V 2 and so ok all of u 1 is
lambda 1 R of it is number to our
application on the K so so basically you
can think of V vana up to VK as a
k-dimensional orthonormal basis
minimizing the Rayleigh Kosh
so in other words you can think of our
problem as a rounding problem so this K
dimensional basis gives us a solution to
the continuous relaxation of our k
clustering problem you want to round it
into an integer solution so how are we
going to use this we're gonna use the
continuous relaxation by this by
embedding our graph in a high
dimensional space this is exactly what I
did at the very beginning of the talk
okay so basically I I'm at each vertex I
to this vector V one of IV two of I up
to VK of ah okay so let me show you some
example to better understand this
for example if if if the graph is just
as J connected components then this
spectral embedding Maps each connected
component to a separate point in this
high dimensional embedding
each color component we mapped the same
point and you can see if clustering
would be very easy okay if the graph is
a cycle and K is equal to 3 then this
spectral embedding gives you exactly the
cycle
so so what happens in general in general
this embedding has two important
properties which will be crucial for the
proof the first one is that the mapping
spreads in their space the vertices
cannot be concentrated into three places
they have to spread in the whole space
okay think of the cycle for example the
second one says that adjacent vertices
will map to close points in this high
dimensional image I'm going to say the
quantitative version yeah so so we're
going to use the first statement that
the first property to argue that we can
choose K disjoint clusters because the
points are spread in the space we can
choose K disjoint clusters we use the
second property to basically to choose
our clusters from groups of close points
okay so you see the cycle pizza so the
basically these two make two components
of our proof so what I'm going to do is
that in the next two slides I'm going to
tell you about each of these components
separately okay so let me face the stuff
with the first component and then tell
you about the second so the first
component is called the spreading
property okay so they prove the
following statement we prove that each
narrow cone through the origin has at
most essentially one over k fraction of
the vertices okay it is the exact
quantitative version well do not need to
understand it the point is if you look
at each cone it has only 1 over K
fraction so it cannot have the carrot
with too much vertices in one direction
that we had they have to spread in this
space ok now the way the proof of these
goals is is through an isotropic
property which shows that if you choose
the unit vector in this high dimensional
embedding and project all of the points
to this vector the mass after the
projection is
exactly one of a fraction of the mass
before the projects so basically all
directions look the same in each
direction you see only one work a
fraction of the mats and the proof of
these only use the fact that our
embedding comes from an orthonormal set
of vectors so we don't use anything
special about eigenvectors and any
embedding you give me from an
orthonormal set of vectors would satisfy
this ok so this was the first component
now let me tell you about the second
component which is which is that how we
choose these disjoint clusters K
disjoint clusters okay in the previous
case the previous company I wanted to
argue that the vertices cannot be
concentrated in two three places right
now
if the vertices not now in the in this
case the difficulty is if the vertices
kind of uniformly spread so in fact if
the vertices are concentrated in K
places I can just return those K porque
clouds right and that would give very
good solutions the heart instances in
this case or if you are those when you
have kind of uniform distribution of the
vertices in the whole space because here
you kind of have to you know partition
these vertices and separate the vertices
that are very very close to each other
and that would make the problem hard
okay so in order to argue that we don't
cut too many vertices we use random
partitioning of metric spaces so
basically we randomly partition this
space and because we do it random each
edge will be cut with some very small
probability okay so here we use some
vast literature on random partitioning
of metric spaces using this book the
works of Chari car chick or Igor Guha
Plotkin Gupta crafts commonly
now these random partitioning in care a
loss of that is polynomial function of a
dimension okay since we are in K
dimension we get a loss of phenomenon in
K now if you want to give a better loss
if you want to decrease the loss
the idea is to as you may guess is to is
to do dimension reduction to go from K
dimensional space to lock a dimensional
space
but this wasn't an easy task the reason
is that because K can be very much
smaller than n dimension reduction will
not preserve many of the pairwise
distances okay nonetheless we show that
it preserves the spreading property and
the average edge length with with very
high probability and that is what what
is essential for our proof
okay so before finishing this part of
the talk let me compare what we did with
the chiggers inequality so engineers
inequality as I said we just use the
second eigenvector we basically map the
vertices to the to a line based on the
values in the second eigenvector now
because this is just one-dimensional
everything you can test all cuts okay
and choose the best of them here instead
of choosing one eigenvector we use
multiple eigenvectors and map the
vertices in a high dimensional space now
you can see where the difficulty comes
from we cannot test all of the cuts
there are exponentially many so
basically we use random partitioning to
avoid our cutting too many edges so you
can see our proof as a high dimensional
variant of chiggers quality okay so let
me conclude this part of the talk
Chickie's inequality is one of the
fundamental results in spectral graph
theory it has applications in various
fields of computer science we managed to
generalize this inequality to higher
dimension to do very to do Cayley
partitioning our proof gives the
rigorous justification for the spectral
clustering algorithms that use multiple
eigen eigen vectors in addition to that
our proof injured introduces new
components that can be used and possibly
improve the quality of a spectral
clustering algorithms so here is an
example that I tried so say you want to
cluster these data points okay now I
used the original spectral clustering
algorithm and this was what I get and
then I applied the dimension reduction
and they're significantly improve the
quality okay notice here I'm not
claiming that dimension reduction always
gives better answer I'm just I'm just
saying this may help in some cases to
get better quality solutions
one reason for that is basically that
partitioning in lower dimensional space
is easier
in question okay so I'm done with the
higher-order cheer inequality now let me
talk about improved chiggers inequality
first let me tell you about the
tightness of chiggers inequality okay
turns out that both sides of the
chiggers inequality are tight the left
side is tight for hypercube and Allah
and the right side is tight for a cycle
okay for a cycle for example lambda 2 is
1 over N squared and as far as Scott is
about mono array okay so if it is tight
how can we improve it so let's look at
the case eigen value of a cycle it's
about K squared over N squared yeah now
you can see that K lambda 2 over root
lambda K for a cycle is about root
lambda 2 so although the root lambda 2
was tight this is also tight
so the basically be sure that this
wasn't a coincidence for a cycle if it
actually holds for any graph for every
graph G Phi of G is at most order of K
lambda 2 over root lambda K okay
the importance of this is that this
guarantee is achieved by the spectral
partitioning algorithm that uses only
the second eigenvector this was very
surprising to us you know because we
have the spectra part picture
partitioning knows nothing about higher
in values and higher eigenvectors yeah
but still you can analyze it using the
higher eigen values and and prove
whether guarantees so so so the
interesting thing is falling say in
general you may have graphs where lambda
2 is very close to 0 sake lemma 2 is 1
over N then the original a spectral
partitioning the Regina analysis would
tell you that the solution of a spectral
partitioning can be n times more than
the optimum so it could be very bad ok
but here what we say is that all of the
lambda 2 could be very very small if I
know that lambda K is much much larger I
still can can show you know this
algorithm gives a very good answer so so
this is actually what happens sometimes
in practice so say you want to do image
segmentation okay so typically an image
segmentation you have K or K minus 1
objects now you construct the graph
based on these objects right you put a
vertex for each pixel and connect them
now it turns out that the graph that you
would construct typically will have very
large like the case eigenvalue of the
graph you construct would typically be
very large will be very close to 1 the
reason is that this graph would look
like unions of K expanders now by our
analysis we can argue that Phi of G is
at most order of K lambda 2 therefore
the solution of the spectral
partitioning algorithm is only order of
K times more than the optimum so it's
actually very close to the optimum and
that is exactly why why people use this
algorithm in practice and they
very good results because many of the
practical applications satisfy this
property ok so this was what I wanted to
say about this result basically you can
Sigma in this image get these objects
let me tell you very very little about
this last result like a two minute to
three minutes about finding the small
communities in large social networks
first let me tell you why why I'm
interested in finding small communities
and then tell you what what we do so
basically here are the three results to
three reasons the first one is that
let's go back lang Das Gupta Mahony
show that I mean basically they do a
practical empirical analysis and argue
that best communities in large social
networks are very very small they have
size only about 100 okay the second
reason is that typically a small
communities would represents a group of
people with similar interest whereas
large communities would correspond to
large-scale noun factors such as age or
ethnicity okay and lastly if you find a
partitioning into large communities
sorry if you want if you want to find
partitioning into a small communities
that would itself give a partitioning
into large ones this is because if you
have two community of a small
conductance their Union would have a
small conductance as well so basically
you can just combine the small
communities and give a partitioning into
large communities so what do we prove we
show the following we show that for any
a small community or an target t that
you would like to find you can find a
set which is a slightly larger than T we
we
guarantee that conductance of s is that
most school route of conductance of T
and we can do it very fast in time
proportional to the output size you know
s could be much smaller than the size of
the graph so this basically improves
upon the results of spin melting
Anderson Chung and Lang and there's some
press so the basically these results
show that you can do the same thing but
their guarantee has this dependency to
the size of the graph has a root dog and
dependency to the size of the graph so
we managed to get rid of this dependency
and get you know a guarantee in a sense
very similar to what you get from a
spectral partitioning and chiggers
inequality also a weaker version of this
is proved by Koch and law dependently so
so so so what we show basically implies
that you can get an algorithm that with
almost the same guarantee as of their
spectral partitioning but with the
advantage that it can be run in sub
linear time also our proof gives
improved lower bounds and mixing time of
random walks that I'm not going to give
get into okay so let me let me conclude
what I said during this talk I talked
about spectral algorithms spectral graph
algorithms through the lens of higher
gain values I talked about these three
generalization and strengthening of
chiggers inequality in a spectral
partitioning algorithm our results use
developments in high dimensional
geometry it also develops new techniques
and new tools that has been used
elsewhere to get new results for example
we use these techniques to give
Universal bounds and higher gain values
of graphs give improve approximation
algorithms for max cut mean by section
and so on also prove a new regularity
lemma ok I'm not going to talk about
these results here you can ask me a fly
all right so
I've worked on approximability of
various problems and here I wouldn't
have time remember I can just give one
talk I can I cannot tell you about all
of this so let me tell you about
approximating Traveling Salesman problem
okay so let me remind you of the
Traveling Salesman problem say this may
have happened to many of you you go for
a short visit to a new place maybe you
go for a conference to Seattle and
perhaps in the second day of the
conference you can you get tired so you
decide to visit some places maybe in
this case you want to visit
PACA Street medicine Park University of
Washington Green Lake Space Needle and
we started that fit here it was but so
basically you guys have unlimited amount
of time you want to head back to the to
the conference as soon as possible the
question is what is the fastest route to
visit all these places and maybe in this
case this is the fastest route so this
problem is known as the traveling sleds
my problem so again you have sort of
places you want to visit all of them and
return back to the starting so this
problem has has many variants these are
I would say three three of the most
important ones the most famous ones the
symmetric TSP where we assume that the
distance function is symmetric going
from here to for example you dope has
the same cost of going from you doctor
here then there's a symmetric version of
TSP where we don't assume this symmetric
this symmetric property could the
businesses could be paid friend this is
a generalization of symmetric TSP
another is the Euclidean version well if
we assume the points are embedded in a
plane and a distance between a pair of
points is is the Euclidean distance
between the corresponding points on the
plane okay so let me tell you what we
knew but what was the state of the art
before our work there is a 1.5
approximation algorithm by Christophe
Eady's that has hasn't been solved for
where it hasn't been improved for over
25 years there is a log in approximation
by freeze Cal BRT and Maffeo Lee that
although many people have tried the only
management improved the constant in
front of the log N and there was a pitas
phenomenal approximation scheme for
Euclidean TSP which means that you can
get very close to 1 in polynomial time
okay so so what did we do here is just a
summary we in a joint work with summary
and singing here we managed to improve
crystal feed ease to 1 point 5 minus
epsilon approximation for a canonical
important special case of TSP called
graphic TSP in a joint work with salary
goldman's madri and so like I said
through Gomez merging and salary we
managed to improve the law again barrier
for a CMS 60s yen improve it to log n
over log log n also new joint work with
strawberry we managed together give a
concert factor approximation for
asymmetric TSP on planar graphs ok the
addition to that the pools of this
develop new technique in algorithm
design called rounding by sampling it
has been used in many other for many
other problems with other applications
alright so in the very limited amount of
time remain let me very briefly tell you
about future so let me tell you some
interesting problems so here is a
theoretical problem so this is a
inherent connection between a spectral
algorithms and some very hard
conjectures in theoretical computer
science unique games conjecture so let
me remind you that here I showed that by
higher order she gives inequality we
know FOIAs case that most order of root
K lambda K I told you that this root
kay is necessary you cannot get rid of
it it's tight now
Aurora Brock and Stuart showed that this
route lock' is not necessary unnecessary
if chains very large although for a
small case necessary if K is polynomial
in n it is not necessary you can get rid
of it and you think that they managed to
get some improvement now the point is if
you can improve this resort a little bit
from n to the epsilon to say - to the
route log n then that would refute the
unique games conjecture it could
possibly imply too many advances in
designing approximation algorithms for
for for many problems like max cut
vertex cover and so on okay here is
another problem I'm also very interested
in online optimization and I've done
some works in this direction you can ask
me offline but here is an interesting
problem so let me remind you that in in
online optimization the difficulty is
that we don't have full access to the
input ok the input arrives online and we
have to decide irrevocably once we see a
new element ok so what we do usually is
that we compare ourselves with the
optimum offline algorithm ok I call this
the information God ok this is the
algorithm that has the full knowledge of
the input ok now usually there is the
information theoretic barrier because we
don't have full information of the input
we cannot get very close to this
information God ok now ideally you would
like to compare ourselves to the optimum
online algorithm this is the algorithm
with the same knowledge as us but with
the advantage that it can be run that it
has unlimited computational power I call
this one the computation God ok now it's
very easy to see that computation God is
always less powerful and the information
God because it has less than for me
that's the same information as ours so
you know competing with him is easier
you can you can hope to get past the
information-theoretic barrier let me
tell you some let me be more specific so
in particular let's say you have some on
an optimization problem and you were
given the distribution of the arriving
inputs a priori so you know the
distribution at once you know the
distribution you can compute the optimum
online algorithm in exponential time you
can write a exponential time dynamic
program and compute it now the question
would be how can you approximate this
optimum online algorithm in polynomial
time now the answer to this question can
have a lot of applications in a
stochastic optimization for example in
all advertising or in fly to scheduling
many other fields
okay so let me finish up with us with
this last slide as a theoretician I
usually think how my research can impact
practical applications I try to have
this in mind when I choose my my
direction of research so here are two
ways that I think it's your addition can
be helpful in practice the first one is
to design new tools and new techniques
that can give us you know that can help
other people other algorithms are use
heuristics in practice the second one is
to prove the rigorous justification for
many of the heuristics that people use
in practice but I don't have any idea
about why these are working
okay let me stop here and I would be
happy to have your questions
so we saw information that high-ranking
values like how much time does it take
like if I want to get less it the first
K I did it's key times K times and you
can the return theoretically you need to
use this linear laplacian solvers and
using that you can do we think and yeah
proof sugar inequality in works
where you want to use
so so the point is you can use this to
to get algorithms for to get very fast
algorithms for low threshold ranked
graphs so so these graphs are
generalization of expanders
so in expanders you have the property
that the second eigenvalue is very close
to one here the first K eigen value
could be as small but the k plus one
eigenvalue is very close to 1 now there
was there is many other papers that
study many algorithms on low threshold
trying class we can use sophisticated
SDPs or the lesser hierarchy to study
them but here what we wanted to say is
that you know you can use this very
simple algorithm we just use the second
eigenvalue second eigenvector to give
very good performance for the expect for
the no problems on low threshold right
graphs
Oh spectral clustering and I just tried
to use it in practice if you find all
this a little bit confused about you
know how to pick the right type of
laplacian matrix and how connected
should my craft me before I complete the
I cannot tiny vectors
are there any attorney can you give me
guidance
those kinds of questions or are there
people working on those kinds of
questions so right so so here we use one
particular laplacian like normalized
laplacian used eigenvectors of this in i
mean in the in the literature i've seen
people use different eigenvector
different eigenvectors on different
normalization but there is a very nice
survey by luke's book i think who
basically you know kind of gather
different normalization like we can use
either eigenvalues eigenvectors of
normalized laplacian or the random walk
matrix or in fact their whole adjacency
matrix without doing any normalization
so all three are studied so yeah if you
wanna yeah the man question is how he
constructed across the second one is
once you constructed at which
normalization do you use this was the
question
this is answer to the normalization now
for constructing the graph again there
are many ways i describe two of them in
the first slide one often was that to
connect the vertices that are close to
each other give some threshold and
connect the vertices that are close to
each other the second one is basically
construct a complete weighted graph
where the weight of each edge is you
know some function of the distance for
example e to the exponential function of
minus distance squared
threshold and when it's just
deviation right so so so for example in
this Peck in this paper of anger and
vice they they argue that what you
should do is basically you should try
very different thresholds and see
what-what would give you the best answer
but no they for example if you assume
your data is from some Gaussian
distribution and you may have some prior
knowledge and and bounds on the variance
of the Gaussian then you can use that
yeah you can't you can do that as well
but this takes more time
other questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>