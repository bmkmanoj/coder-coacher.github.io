<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scale-Independent Relational Query Processing with PIQL | Coder Coacher - Coaching Coders</title><meta content="Scale-Independent Relational Query Processing with PIQL - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scale-Independent Relational Query Processing with PIQL</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h2mctWEgDkQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
pleased to introduce to both of you
hahahaha Michael armbrust who's a very
exciting grad student at Berkeley you
finished five years now I will be
finishing in December but he will be
finishing in this era and he's doing a
correctly you're doing a post doc yet on
there for two years your know uh yeah
two years two years yeah and then we'll
be on the market looking for real job
interview when he wants a real job but
in the interim I've invited him to come
and talk about some of the work that he
did with the rad lab versus an elaborate
kind of crossed yeah both of them you
know the project name changes every five
years but the components change
different offset so he's gonna he's
going to tell us about pickle which I
personally really think is a great idea
okay okay thank you very much bill so
I'm here to tell you about pickle which
stands for the performance insightful
query language and basically the idea is
we're going to let you build web
applications that scale to arbitrary
sizes will still guaranteeing response
time so kind of the model though the
world that we're living in is these kind
of interactive web applications think
things like Twitter being Facebook etc
and kind of the thing about these
applications they grow very rapidly so
this graph here is the number of tweets
per day since Twitter started and as you
can see it was growing exponentially and
continues to grow exponentially over
time and so in order to keep up with
this demand it's very difficult for
these systems to scale I kind of what
you what you typically find is is when
you get overloaded with all of these
users because your application becomes
popular you often end up with what we
call a success disaster where because
you became so popular your your
application servers your databases tip
over and you actually start scaring
users away because your application
can't keep up so when you look at what
the problem is when these sort of
success disasters occur it's usually the
database that is the problem so it's
actually thanks to cloud computing
pretty easy to add stateless application
servers so you can kind of scale those
up arbitrarily as demand increases but
it's actually much harder to scale these
stateful storage systems we're kind of
all of the the difficult logic is and
concurrency issues occur and so you know
Twitter actually did an analysis of
all of the problems they were having a
couple of years ago and they found that
most of it was because of these bad
queries that were being generated by
active record causing really slow things
to occur in the database which was then
knocking other queries out and causing
the entire system to slow down so kind
of there's a couple of reasons for this
that why traditional relational
databases are kind of hard to scale on
and one of them is data independence so
you know data independence for those of
you who aren't familiar is kind of the
idea that you write these declarative
queries which specify what data to
retrieve and not necessarily how to
retrieve it the nice thing about this is
it allows optimization and independence
from the physical and logical schema so
you can add columns remove columns add
indexes and you don't have to change
anything in your application
additionally the optimizer is free to
kind of go and decide what the best
plane is you is the the application
developer don't have to worry about that
kind of the con here is these
performance problems are often hidden
until they cause issues in production so
you're running over a small test data
set at small scale and you don't realize
that this could be doing an entire table
scan so kind of the the dirty secret of
traditional relational databases is that
these declarative languages kind of hide
the underlying complexity of the system
from the developer another issue is the
way that this optimization occurs when
you when it's deciding amongst all of
the different plans which one it should
actually use the way it does this is by
looking at the minimum average cost
based on statistics so kind of to giving
you an example of where this is actually
caused problems in production at Twitter
let's take for example the query that
creates this this line of the web page
so when you go to the amp labs web page
it shows you which of the people you're
following are also following the amp lab
so there's kind of two different plans
that the database can choose in order to
execute this it can do a sequential scan
over a single level index or it can do a
bunch of random reads against a
second-level index and so the way the
optimizer is going to decide which of
these plans to use is going to look at
statistics it's going to say you know
how many followers does the average
person have so the amp lab only has 150
the actual average is 126 so on this
graph it would look here and it would
say well clearly the unbounded index can
plan is much faster and so that's that's
what the optimizer would suggest um
however then when you have an outline
like Lady Gaga who has 19 million
followers that the performance is going
to go through the roof and you're going
to have you know 10 10 minute Layton
sees to answer this question and so kind
of the the dirty secret here is that
this minimum cost optimization fails for
the point one percent and you really you
do need to care about those when you
want to keep these high quantile s ellos
glad you you've got that so you know
kind of the the recent trend for these
developers of these large-scale web
applications has been to switch to these
no sequel solutions so these are things
like big table Cassandra and MongoDB
they provide you just simple key value
look up sometimes they also allow range
scans and the reason developers really
like these systems is because it's very
easy to reason about the performance of
these simple operations anytime things
get slow you just throw more machines at
in a partition of the data further gets
inputs returning basically constant time
independent of the size of the under of
the the total database the con here is
that this is really throwing the baby
out with the bathwater you're losing all
of the benefits of relational databases
and declarative languages in order to
get this kind of performance and so by
only giving developers these simple
operations you're forcing them to
reinvent a lot of complex abstractions a
lot of really smart engineers have
thought about for many many years so
they have to kind of do the optimization
by hand they have to maintain indexes by
hand and then when they change when
things change underneath you end up
rewriting your programs instead of
letting the optimizer do it for you so
instead we propose pickle the
performance insightful query language
and the idea is what we want to do is we
want to guarantee scalability even for
sophisticated relational queries we're
going to do this through a number of
techniques first of all we're going to
build on top of these existing scalable
key value stores that we can leverage
all of the engineering work that has
gone into them we're going to however
give you an interface it is slightly
higher than this this get input we're
going to give you a full relational
query language that you can retain those
productivity benefits the optimizer is
going to ensure that all the queries of
your application kind of statically at
compile time are scalable even for
outliers and make sure that you're not
going to have any kinds of scalability
problems and in order to do this it's
automatically going to create any
indexes or materialized views that are
needed but only if they can be proven
scalable and i'll talk more about the
techniques we use in order to do
and then finally I'm going to show you
how we actually have a model that allows
you to take the number of operations
that are performed by any given query
and use that to reason about the SLO
compliance of the overall query so kind
of the the general workflow when you're
using pickle is like this so a developer
writes an application in their favorite
framework it can be Scala can be Ruby on
Rails I could be dotnet the system
actually pulls out all the queries from
it and the pickle optimizer either says
yes you know you have some problems you
need to fix these queries these are not
going to be scalable or it produces
scale independent query plans talk a
little bit more about what that means in
upcoming slides a list of views and
indexes that are required to actually
execute these queries efficiently and
then predictions for the SLO compliance
of all these queries you can see if it's
actually going to be fast enough to meet
the needs of your application so kind of
the the architecture of the system as i
said we build on top of these existing
key value stores pickle is written as a
library that you can link in with your
application basically you right in the
queries and then it decomposes those
queries into key value store operations
we have a couple of requirements of the
underlying store basically we expect it
to have things like debt and get range
where we can kind of do efficient
lookups either by the whole key or by a
prefix of the key we assume that it's
going to give a scalability with
predictable performance so we assume
that it's taking care of kind of
reprovision his load increases on you
know or as data sizes increase and you
know there's a lot of other work that
I'm not going to talk about today that
was done in the lab on ensuring that
that's possible and then finally we're
not going to worry about consistency
we're basically going to give you
whatever consistency model the key value
store has so there's a lot of work on
I'm kind of getting better consistent
guarantees out of key value stores but
really whatever whatever this key value
stores giving you the pic a library is
just going to give to the next layer up
so kind of the rest of the talk is going
to be split into a couple of parts
basically the key idea is I'm going to
define different classes of sequel
queries where we can guarantee them
scale independent based on the resources
that they're going to require for their
execution so we're going to start off by
talking about the queries that can be
executed on demand so we can actually
calculate the answer when the user asks
for it and in order to do this I'm going
to show you how you can take a
relational query and map it to a bounded
number
key value store operations I'm going to
talk about how you need to change sequel
in order to make the static analysis
possible for complex queries and kind of
the extensions that are required and
then finally I'm going to talk about our
model for predicting SLO compliance
since this isn't sufficient for all of
the queries are going to find in these
these large-scale applications I'm also
going to talk about how you can leverage
scale independent pre-computation to
increase the space of queries that can
be answered by shifting some of the
computational work from query time to
insertion time so I'm going to show kind
of an algorithm for transforming a scale
dependent query into a scale independent
one that runs against a pre computed
answer I'm going to talk about how we
can do a static analysis of that
pre-computation to ensure that both the
storage and the maintenance costs are
also bounded so those themselves don't
become scalability bottlenecks and I'm
going to talk about kind of how you can
extend this even further to leverage
parallel execution and data aging and
then finally I'm going to conclude with
some ideas about future work but I'd
love to hear your ideas on you know
what's missing or what what else could
be added to this so let's start off with
with scale independent optimization so
the idea here is we want to be able to
bound the work a priori for all of the
queries in a given application so kind
of to formalize this a little bit say we
have all of the queries in application q
so we've done some static analysis and
we've come up with this set Q of all of
the queries in an application basically
the pickle optimizer is going to ensure
that it can come up with this constant C
sub ops for every query in the system
such that even in the worst case the
number of key value store operations to
execute it will be less than that
constant so I so basically we I mean
it's specified based on the cardinality
constraints you know the constant for
any query is specified by the
cardinality constraints that you've
specified what you can specify is the
SLO that you want we can tell you if
that number of operations can be
performed within that SLO or not yeah
exactly exactly so yeah so that is that
is what the developer tells us is what
they're what there is yeah
yeah exactly exactly so how do you
actually do this for a relational query
let's start with kind of a simple
example so I have a sequel query here
that basically looks up all of the posts
in a given application so this is
something like Twitter or you know some
kind of pub sub system I want to look up
all of the posts by topic ordered by the
time stamp and then I want to take the
top 10 so kind of what the database is
going to do is going to take the sequel
query it's going to mop map it to a
logical plan that says you know take all
of the posts filter out all of the ones
where the topic ID doesn't match the
topic and i'll point out that this is
actually a parameter so well you have to
specify all of your queries ahead of
time you can parameterize them and then
kind of fill in those parameters at
runtime what the optimizer does then is
it kind of analyzes the query for all
possible values of that parameter so
it's going to filter them by the topic
ID specified at runtime sort them and
then you know only return the top 10 so
this is a stop operator which basically
just returns the the top 10 tuples that
come to it then ends the query so these
logical operators can actually be mapped
to a single get range call against the
key value store assuming you have an
index that's ordered by first topic ID
and then time stamp so one key value
store operation can answer this query
given this index to kind of generalize
this basically we have this model for
what an index scan can handle so it's
any ah but these you is the stop okay so
as you can see here the stop is required
yep yeah yeah so that that is why the
staff is not dotted so it the index scan
is you know some relation with some
predicate on some optional number of
other predicates an optional sort and
then yes though the limit is not
optional that is that is required in
order to be scaling about exactly
exactly so you know in general any time
you see the set of logical operators it
can be mapped to a single get range call
I know so the sword is actually also put
into the key right so we're assuming
that our key value stores range
partition and so kind of in the case of
this example here the index would be
over topic ID and then time sting so
when you look up the topic ID then all
of the rows will be sorted by timestamp
next so you don't you don't need to
retrieve them all
um okay so that if you're right if that
if that yeah okay that good good
question yeah so yeah so of course the
expression that you're sorting by needs
to be calculated using only the
information in that single couple and
cannot be based on any joins or the data
that you're going to be sorting needs to
be already bound to do to some other
constraints in the query that's yeah
that is correct okay so so yeah so index
scans and then if we want to generalize
this we can also support kind of joins
as well so in this case we're looking at
a foreign key join this is where the
attributes in the bounded child plan so
we have some number of tuples coming in
and we know that they're bounded due to
kind of the pickle guarantees they
contain the primary key of another
relation that's what these predicates
are representing and because the primary
for any given primary key there can only
be one couple we know that we can answer
this query by doing at worst the number
of entries in the child plan gets
against the key value store to
generalize this further we can also
handle joins that don't necessarily
involve the primary key as long as there
is a stop operator at the top so in this
case we have a bounded child plan some
number of predicates a sort using the
same conditions as we talked about
before and then a stop operator up at
the top absolutely absolutely yes
oh yeah I mean this is not full sequel
this is the subset of sequel that can be
guaranteed scale independent I think
they're the claim that we're gonna try
to make is that this is sufficiently
general to build all of the web
applications that exist today because
basically if you're violating these
you're probably doing something that's
difficult to scale and therefore
Facebook couldn't be doing then I think
that's a good summary so we can also
handle joins which is something that you
know a lot of these other kind of you
look at the google query language that
they have an app engine or you look at
the cassandra query language this is
kind of a level up and i'll talk about
exactly why we're able to do that for a
large number of queries and really what
it comes down to is if you look at some
of these harder queries just standard
sequel isn't enough to specify all the
information that the optimizer needs so
here we have kind of a standard timeline
query this is you know think once again
of a pub sub system where we have some
number of posts and people can subscribe
different topics this is showing for the
current user all of the posts for all of
the things that they're subscribed to
ordered by time and then you know look
to show me a page of that so right now
pickle would reject this because you
know this isn't bounded and this isn't
bounded either so the question is how
can we give this extra information to
the optimizer they're allowed to
understand why twitter is actually able
to do this safely so we're going to
extend the language in two ways first of
all we're going to make it easier to
bound the amount of data returned so
sequel has a limit operator which you
can also use with an offset but using
that is actually N squared work to go to
each next offset so what we've done is
we've built pagination the ability to
look at the the results of an unbounded
query one page at a time as a native
operator into the engine so kind of what
this is doing when you tell a query that
it's going to be paginate 'add you can
turn this unbounded query into an
unbounded set of queries themselves that
are bounded so each interaction with the
database now has about
so by adding a paginate to that query we
actually insert this pagination stop
operator up at the top there's still a
problem with this query in that the
intermediate results of this join could
be unbounded and the way we kind of let
the developer tell tell the database
that's okay is by letting them specify
cardinality constraints on kind of the
natural limitations due to the
relationships that do that we're
modeling so for example on Facebook
you're only allowed to have 5,000
friends which is kind of reasonable
since I don't think anybody really
interacts with more than 5,000 people in
any meaningful way during their lifetime
and and the the unlike sequel we
actually let you express this as part of
the ddl of your application on what the
database does then is when you filter on
any of these these attributes that are
constrained it can actually insert these
new type of stop operator called a data
stop deeper into the plan the reason
we're able to push this stop operator
much deeper into the plan than you can
with standard stop operators is because
it actually promises about the data not
things that were enforcing a query time
these these these cardinality limits can
be enforced at insertion times we know
that there is no more data than that in
the database so they can actually be
pushed past predicate that might be
reductive so kind of now that we've made
these two extensions to the sequel
language possible to take this query
this can be turned into an index scan
and this can be turned into a sorted
index join and now the entire query is
bounded so now that we've got a bound
kind of going back to Bill's question
you know it is any bound acceptable if I
do a billion key value store operations
is that okay and of course it's not so
what we've done is we've actually built
up a model that allows the pickle to
bolsa glut suggest solutions to scale
dependent queries when it can't come up
with a bound and then when it does have
a bound it actually will suggest what
cardinalities are acceptable the way we
do this is by taking empirical
measurements of single operator
performance against a real key value
store running on your real hardware so
kind of in this example what we've done
is we've measured an index scan of
cardinality 100 with records on average
40 bytes long and we've created a
histogram of the response time the nice
thing about this is sends its a real
measurement it takes into account things
like garbage collection or network blips
or other things they're going to cause
the long tail and affect your your 99th
percentile so since we know for each
operator the worst case
cardinalities we can actually then look
up in all of our empirical measurements
for any given query look up the
empirical measurements for each operator
can volve those distributions and come
up with the distribution for the overall
query from that distribution we can then
calculate the 99th percentile response
time so kind of a question how is does
this work is this expressive enough do
our predictions work does this actually
scale so we kind of did an evaluation of
this on two different workloads we use
TP CW which is a standard database
benchmark simulating something like
amazon com and we also use scatter which
is kind of our own homegrown clone of
Twitter we started by doing a
qualitative analysis where we wrote the
all the queries for the application and
introduced to introduce cardinality
constraints where the pickle optimizer
suggested them they all kind of made
sense we did things like limiting the
number of topics at any given person
could follow or limiting the number of
aw bucks that a given author could write
pickle automatically created all of the
indexes needed to answer these queries
and kind of what was really cool as the
prediction model was able to accurately
predict the 99th percentile response
time within tens of milliseconds for all
of these queries in both of these
applications we also wanted to do kind
of a scale analysis of this and so kind
of the methodology here was we ran each
benchmark on ec2 on top of our own key
value store scads we increase both the
data size and the system size you can
think of this is weak scaling for each
point and we measured both the
throughput in the 99th percentile
response time at each data point and so
kind of unsurprisingly due to the
constraints that the optimizer was
enforcing in the way we change the
objective function for optimization
we're able to maintain nearly perfectly
linear scalability as we added nodes
data to the system and nearly perfect
response time even in the 99th
percentile so what we've done here is
we've actually defined two classes of
scale independent queries that can be
executed on demand so si0 kind of the
highest level of scale independence
these are queries that we can answer on
demand using only the primary index on
for some queries however the the cost to
do that will still be dependent it's
going to require a table scan or
something so we'll put these into the
class si1 where these are queries where
you can answer them on demand but we
need to create a secondary in
in order to do that so this is this is
kind of the space of all cruising me run
whenever the user asks them but what
about queries where we need to use
pre-computation so pre computable
queries examples that would not fit into
this this previous model are things like
we want to do a count so here Sergei has
I like billion followers or something a
lot of million followers too many to
count on demand and you wouldn't want to
put a cardinality constraint on this
because you want popular people to be
able to have a lot of followers you know
another example is things that are
shared a lot or kind of a more subtle
example is a query that returns the top
10 tweets that are tagged with two
different tags so basically what we're
going to do is we're going to expand the
pickle optimizer such that it has
another choice now when it finds a scale
dependent query so a query comes in if
its scale independent we do the same
thing we just produce a physical query
plan if its scale dependent we're
instead going to try to select a
materialized view that can be used to
answer the query for all sets of
parameters we're going to look at that
materialized view and make sure that the
storage required for it is going to be
bounded independent of the size of the
data and we're also going to make sure
that the update costs are also bounded
so I'm going to talk about kind of about
each of these parts in the upcoming
slides yeah I say that again ah well so
if if there is no physical plane if we
if the optimizer is unable to find a
physical plan that can answer it on
demand in a scale independent way so
let's look at this example
oh okay so let me let me be clear is
what we're doing is we're not we're not
going to pre computing it for specific
values of the parameters we're computing
it for all possible values of the
parameters based on the data in the
database so it's not really we're pre
computing it for Lady gaga it's that
we're pre computing it for this to tag
query so let me let me walk through how
we do that and if it's not clear slap me
again so this is this is the to tag
query this returns the top 10 documents
ordered by time that are tagged with two
tags and as an example of why there is
no scale independent plan using only
secondary indexes let's look at the
index that the optimizer would create in
order to try and answer this it
basically maps tags document IDs and as
we're kind of talking about in this
example you would have to scan past an
arbitrary number of ladies before you
found one that actually matched up with
tramp instead of the the pop singer so
there's there's no way to do this
without scanning through an arbitrary
number of tuples to find the five or ten
that they've asked for that actually
match however if instead of commuting it
this way we transform this query to
actually be a materialized view that pre
computes all to tag combinations we can
then just do a simple look up into this
so basically what this is looking for is
it all to tag combinations ordered by
time where we kind of join this tags
table with itself and so what you can
see here is now we have this which is
you know in store it in an efficient
index structure such that we can jump
directly to lady in the Tramp then
ordered by time and then get the
document ID so what pickle is going to
do is it's going to automatically create
this view for you and you can use this
to look look up the answer for any
values of the parameters so how do we
actually do this I'm not going to go
into all of the details but it's in the
paper please it's insufficient but I'm
happy to pass around drafts if people
are interested but just a high-level
overview of how this algorithm works
basically we take the query we remove
any predicates that are compared with
parameters and move them up into the
projection of the view such that that's
going to going to define the spatial
locality of the tuples that actually
exists in the materialized view we
include any attributes that are going to
be used in ordering in in the the
projection as well and then we do a
check to make sure that the resulting
tuples that are going to be the answer
to
query are adjacent in the view that's
because we want to be able to look up
the answer by doing a single get rent
contiguous get range against the key
value store they're not adjacent there
could be an arbitrary number of tuples
in the middle that you have to scan past
and we this would not be scale
independent then and then finally based
on past techniques that were invented
back in the 90s uh we add key so that we
can incremental e maintain this view
efficiently so once we have this view we
now need to make sure that it's going to
be safe it's not going to threaten the
scalability of the system and we'll
start by talking about how we bound the
storage of the view so pickle basically
the goal here is to ensure that the view
is going to grow linearly with the size
of the base relations so the invariant
that we're going to hold here is that
for all views that are created in the
system there will exist some constant C
and some relation R such that the size
of the relation times this constant is
bigger than the size of the view in the
worst case so we're going to bound the
size of the view as some linearly
proportional to the size of this this
single relation so how do you actually
do that for for an arbitrary view the
answer is we're going to use an analysis
of expanded functional dependencies so
for those of you who aren't database
people basically a functional dependency
is some promise that knowing some
attributes will let you know the values
of some other some other attributes kind
of the easiest understand example is
think of your social security number if
I know your social security number I
know your first name and there's only
one first name for that Social Security
numbers you can think of that is the the
primary key constraint with pickle and
and viewers we actually expand this to
include two other types of functional
dependencies we have cardinality
constraints that we talked about before
this is just like a primary key
constraint except instead of having an
edge of weight one we have an edge of
weight the cardinality and we also have
a quality predicate since we're not
looking at just the data we're looking
at the data that's going to exist in the
join we add bidirectional edges for any
equality predicates so to go back to our
example to understand how this works for
the two tag view you would construct
this graph of functional dependencies
you then start with the primary key of
one relation which is basically the size
of that relation and then you try to
include edges until you either covered
all of the attributes that exist
the view or until you can't and then you
decide that it's it's not scale
independent so in this case starting
with the with one of the tag tables we
can cover all of the other attributes
and decides that the size is going to be
at worst k times the size of the tags
table if we didn't have a cardinality
constraint on the number of tags per
document these lines would not exist and
therefore there would be no covering of
this graph and basically what that's
telling you to kind of understand how
that can cause a problem think of the
degenerate case where you have a single
document and an arbitrary number of tags
it's going to be the size of the tags
table squared is basically what this
what this analysis is telling you so now
that we've decided that the storage is
going to be bounded we need to make sure
that we can actually incrementally
maintain this view as well so kind of
the this is going to look similar to do
the work that we did in the first part
of this talk but what we're going to do
is we're going to make sure that for all
the relations in the system the amount
of work required to update all of the
the indexes and views given a single
update to that relation there is some
constant C sub ops such that in the
worst case that number of operations is
less than the constant we're going to
we're going to come up with an upper
bound on the amount of work to do all of
the maintenance of all of the indexes in
all of the views
that that is true but when we expand
into the further levels of scale and
depends we're going to let you get rid
of one without the other which is why we
define them separately yeah good good
question now you're ready to jump in a
head so kind of the idea here is the way
we maintain these views is using
standard techniques on Delta queries so
basically given this this view
definition we construct this Delta query
where basically you take the relation
that is being updated and substitute in
the tupple that is being either inserted
or deleted and what this query is going
to do is it's going to give you all of
the tuples need to be there added or
removed from the view and because of the
way that we constructed the view we know
that this is always going to be safe and
maintain the view correctly since this
is now just a pickle query we can
actually pass this back into the work
from the first part of this talk and
it'll tell us if there is a scale
independent plan for doing this which
are going to notice though is is that
this can actually be invoked recursively
so the maintenance of a view might
create another view might create another
view but because of some work that was
done by some guys at at epfl we know
that that's that recursion will always
terminate so what this lets us do is
define a third level of scale
independence a third class which we call
si2 where the work to execute the query
on demand even using secondary indexes
is potentially dependent on the scale
but the work for to incrementally update
and store a materialized view are
independent of the size of the data so
now the question is are we still meeting
our SLO since all of this is about
meeting SLO is and not bounding
operations let's look again at the two
tags query um when you take the two tags
query this is the the response time of
running it on a real cluster on ec2 from
0 to 200 machines and what you see is
that while the rights remain relative
the for rights remain relatively
constant the response time for reading
blows up as the data goes up kind of
because of exactly what we were talking
about you have to scan an arbitrary
number of tuples before you find the
five that match um we just the the tag
data here was distributed as if ian and
this is a log scale so this is this is
really blowing up in terms of response
time um so what you're going to see is
that when we add in the materialized
view
you can actually cut down the read time
significantly and not only cut it down
but this is basically constant you can
see it it basically stagnates at eight
milliseconds here um and kind of more
importantly you know we're also able to
bound the the right time as well so
we're able to cut the response time for
reading down significantly with only a
mild increase to the right time and most
importantly the increase in write
latency is is constant so there's a
couple of ways that we can expand this
one example is sometimes when you look
at the Delta query the Delta query
itself is not bounded but can be split
up and run run in parallel so kind of an
example here is you know so if you have
it when Lady Gaga tweets for instance
she has many many followers and you want
to distribute them to all of her
followers you could have one computer do
that serially or you could have all of
the computers that restoring the
subscriptions table update their section
of the subscriptions table the idea here
is instead of bounding the total size of
the update function we're going to bound
the amount of work that needs to be done
serially by a single computer and then
all of those will be done in parallel so
effectively you're bounding the latency
before this update is visible to all of
the users in the system so kind of the
requirement here is that it's unbounded
over only a single relation because if
it's unbounded over multiple relations
you could require arbitrary amounts of
parallelism and thus not be bounded and
kind of the cost here is that we no
longer have an upper bound on the total
amount of work required by the system so
you might have spiky computational
requirements another example of how we
can kind of leverage this split updates
is when you have a page that's that's
pagin aided by time what we can
guarantee is that the first page is
updated in a bounded amount of time and
then lazily update the later pages and
so for something like Facebook or
Twitter we're ninety-nine percent of the
time users only looking at the first
page that's acceptable but due to these
costs due to the kind of the way this
changes both the behavior of the
application or the computational
requirements of the application we
require explicit authorization from the
developer before invoking either of
these techniques so what this does it
lets us define a third claw or fourth
class of scale independence where the
update cost is unbounded but the storage
is independent of the system we call
this si 3
um and of course there's going to be a
fourth so what do you do with data that
grows over time so the kind of the
fourth extension that we've made to our
model is we allow developers to specify
data retention policies for data that
grows over time and the idea here is
that many applications often use this
more recent data or more often or
exclusively and so the retention
policies allow the system to
automatically either age dated cheaper
storage where we're not going to give
you response time guarantees anymore but
at least it's accessible or just throw
out all data I don't don't materialize
that stuff keep it only in the base
relations and you kind of have to query
it through other other mechanisms so the
developer must specify the policy in the
ordering here but when it's there the
the optimizer will automatically take
advantage of it and this allows us to
specify the fourth class where the cost
of storing updating or executing it is
all dependent but if you allow us to
split and paralyze the updates or an and
age out all data then it's going to be
independent so basically these are
though the five classes of scale
independent queries that the pickle
executor can can recognize and based on
them you can reason about the resource
requirements of your application even as
you you know hit what could be a success
disaster so that's kind of my thesis
basically now let's talk about some kind
of things that I'm looking at working on
in the future so one one issue is
contention right now pickle doesn't
really model contention at all and so
it's possible to create something where
there there's inherent contention in
your data model so for example if you
have some counter that the system is
updating and you have every user is
trying to updated pickles not going to
save you there another kind of more
subtle example is some kind of hot spot
in your index deuda temporal locality so
if you try to construct an index of all
of the tweets ordered by time the server
that's holding the current time is just
going to get completely bombarded so
kind of the challenges I think here are
first of all can you come up with a
model that expressed like insertion
locality so that you can kind of
understand where contention might occur
and then can you detect these
problematic data structures
automatically and suggest to the users
changes to their data model to fix them
and then finally can you do something
really clever and when you see this
happening say
well you probably don't really ever want
to see all of the tweets ordered by time
since it's going to scroll by so quickly
it'll be changing too fast for the user
to even notice but maybe you just want
you know a sample of the tweets from the
last minute so again the system
automatically leverage approximation or
sampling in these cases where contention
could become a problem another example
is expanding the operator space so right
now we only do get put get range but key
value stores are getting smarter than
that there are things like coprocessors
some kind of distributed storage systems
are actually things that are much more
expressive like sequel so you know I not
sure exactly what a sure looks like but
I imagine that there's some sort of
sharded sequel system somewhere and okay
and so say you know now the question is
what operators exist in these richer
systems and can we come up with scale
independent operators there where we can
kind of expand these techniques and
allow greater numbers of queries to be
executed so i think defining those
operators is a challenge as well as as
you add more machines will RS oh look
SLO compliance modeling continue to work
even in the face of stragglers so since
in our current model we're only ever
touching a bounded number of machines
there's going to be a bounded chance of
straggling and therefore it kind of
worked with our modeling but if you're
going to touch more and more machines of
the data grows are you still going to be
able to do this I think that's a that's
an open question another example is
handling partial failures is actually
something I'm working on right now and
would love to talk about more but most
work on doing incremental view
maintenance assume strong consistency
and so that's a big problem if you have
you know even even a distributed system
and especially if you're going to have
multiple data centers you just can't
take locks on things for a long time
otherwise you know update processing get
delayed arbitrarily just because one
server is down does not mean you should
not deliver Lady Gaga's tweets to all of
her other users for example so I think
the question here is what what does
eventually consistent view maintence
look like is there some way to process
an update partially maybe even in
consistently during a failure do some
sort of best up best effort processing
and then do bookkeeping so that you can
realize that there was either a conflict
or you were missing some data and repair
these inconsistencies after the failure
is resolved so I think there's a ton of
challenges here i think the sec this
area is pretty unexplored
what types of crazy ball what types of
bookkeeping do you need is like
optimistic concurrency enough and you
just check read sets later so I would
love to talk more about this if yeah so
I think I think there is and it comes it
comes down to specifically the update
processing itself you want to make sure
that um it's never possible for a right
to just get lost completely oh it's all
right okay so I would say the semantics
are the same but i don't think the
mechanisms you use for eventual
consistency a table will work for a view
because the delta queries are getting
executed a little bit more complex than
doing a single insert or update right
with like the single insert you just
make sure a quorum gets it used Paxos or
something like that that doesn't really
work for executing arbitrary sequel
queries to calculate Delta's so yeah I I
I do not know the answer to this one yet
but this is actually the project that
we're kind of actively working on and
hoping to have a cig mod submission for
and finally I kind of enjoyed playing
around with language integration so as
you guys probably know developers prefer
kind of nice language bindings to these
opaque sequel strings that the compiler
knows nothing about um so currently
pickle kind of copies a lot of the stuff
that link doesn't is implemented as a
DSL inside of Scala so you you specify
your ddl as just defining these classes
and you specify your DML is kind of
these these chained expressions where
you represent the sequel operators um
and you know right now we have a
prototype of this but there's a bunch of
stuff missing we don't have type safety
for the queries and their results and we
don't have anything with UDF so I think
another nice thing about using a
functional language like Scala is you
can specify these closures and be really
cool if these closures could actually
represent nice language integrated udfs
um and and then the question is what UDF
SAR scale independent and I think
there's a lot of work to be done there
as well okay so that's actually all that
I have kind of in conclusion we've
defined these five classes of scale
independent queries odd that work
against the standard relational model
this makes it significantly easier for
developers to build applications that
are going to scale trivially as their
application grows in order to do this we
had to kind of change
the way the optimizer works the only
chose so that you use a different
objective function for optimization and
only chose plans that were stale
independent we had to expand the
language so that the static analysis was
possible for a wide range of queries we
need to allow it to leverage automatic
view materialization so that
pre-computation could make more query as
possible and we also showed how you
could actually model the performance of
this so this was worked I did along with
my advisors Mike Franklin or not to Fox
and David Paterson as well as a bunch of
grad students all the papers are my
website if you want to learn more and
I'm happy to take more questions yeah so
we use our own which was called scads
and basically the reason so this was a
project that happened before pickle
we're basically the goal of scads was to
have a key value store that will
automatically scale up and down as the
workload changes so it has this extra
component called the director which
watches the workload uses something
called model predictive control to
ensure that you get a consistent 99th
percentile response time independent of
the the current workload so really but
really I was kind of more for
convenience it's because I spent a lot
of time engineering and architecting
scads so it was very easy to bring up
and bring down and bulk load for running
experiments at arbitrary scale really
our requirements are we need a range
partition key value store so like hash
partitioning like I think they using
Cassandra would not be acceptable but if
you look at something like big table or
HBase that would work for us yeah so we
need we need range partitioning we need
to get range and then you know the more
features the better if you give us
stronger consistency that's good because
will leverage it but if you don't then
uh then you you get the eventual
consistency the key value stores giving
you know not least not based on my usage
of their application
yeah exactly exactly yeah we use TV see
that view
is it super for twitter using images the
beginning well so we also did a
benchmark based on twitter itself oh
yeah i mean well okay so let me let me
clarify a little bit we only did the
user facing queries of TBC w we did not
do the analytic queries that are in the
back end it's actually something we're
still looking at but for the oltp type
queries in ntpc w absolutely i mean i
think you would want slightly stronger
consistency when you're actually
recording orders but there's no reason
you couldn't have that but for all of
the looking up by author and kind of all
of the other sorts of things they were
doing yeah i think i think it fits well
into our model well yeah i like to think
that it's a very nice compromise between
these two worlds so you get the the
performance predictability of no sequel
but the kind of expressivity and
productivity benefits of using a high
level declarative language i like to
think of it as we took the the full
relational model and all of sequel and
carved out the section that was scalable
leaving all of the things that could get
you into trouble and then was no
security mystical visit we like sauce is
big guys energy MO as the scott members
of the year which served to make some
kind of sequel queries and some light
rock musician but extensive
yeah I mean I think you I I consider
myself a database person I think this is
getting very close to a database now
honestly I think a lot of the work these
other people are doing is very
applicable to pickle like the all of
these kinds of engineering improvements
that they've done I bet that's my view I
honestly think key value stores will be
relegated to storage managers for
higher-level languages because it's just
too painful to write against and if if
you can get the performance benefits of
using the low level gets and puts well
not having to deal with that I don't see
any reason why developers wouldn't want
the the more flexible productive system
yeah but yeah of course it wasn't
because people didn't like really were
dying to write yeah absolutely
absolutely anything yeah sure oh yes
that it did work for yesterday
take care of them I mean it's not like
you also keeping up to the change in the
store in order to like take care of
these views measured wrong yep yeah I
mean yeah so I think there's probably
more work to be done there like a big
part of what people have found with
these these no sequel solutions is that
they're they're just easier to
administer they were written often by
operations people and not database
people um but I think you're seeing that
in a lot of the new database products
that are coming out post post no sequel
stores but I think yeah that's kind of
orthogonal to the the specific problem
that I'm trying to solve but I don't see
any reason why the a lot of the
techniques that have been appearing in
no sequel stores can't be applied to
more traditional database like things
cool well thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>