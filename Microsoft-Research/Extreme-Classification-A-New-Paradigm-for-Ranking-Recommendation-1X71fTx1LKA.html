<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Extreme Classification: A New Paradigm for Ranking &amp; Recommendation | Coder Coacher - Coaching Coders</title><meta content="Extreme Classification: A New Paradigm for Ranking &amp; Recommendation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Extreme Classification: A New Paradigm for Ranking &amp; Recommendation</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1X71fTx1LKA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay welcome everybody thanks for coming
it's our pleasure today to have Manik
Varma give us a talk on extreme
classification a new paradigm for
ranking and recommendation thank you so
for those of you in India that are
watching from your offices I'll be
monitoring questions on the tablet so
feel free to ask you know all related
tonight cool okay good to go so thanks
very much over right so I'm Monica Varma
from Microsoft Research India and I'll
be talking about extreme classification
which might provide a new paradigm for
thinking about core problems and machine
learning such as ranking recommendation
possibly structured output prediction
and so on now many of you might not have
heard the term extreme classification
before so let me start by giving some
context in classification the complexity
of the learning task has grown from
binary classification where we learn to
pick a single item from amongst two
labels to multi-class classification
where we learn to pick an item from
amongst L labels with Al big larger than
2 to multi-label classification where we
learn to pick the most relevant subset
of these L labels at the same time the
complexity of the learning task has also
grown in terms of the number of labels
being considered so we've moved from
working with two labels for binary
classification to tens to hundreds to
thousands of labels for multi-label
learning and if he looked at the
state-of-the-art about three years ago
then the largest multi-link that
multi-label learning data set hired
about 5000 labels so the size of the
output space was 2 to the power of 5
thousand which was considered to be
large and so I was thought that going
beyond that would be very hard then two
years ago we exploded the number of
labels to 10 million the application was
to build a classifier that could predict
the subset of millions of being queries
that might potentially lead to click on
a new ad or on a new webpage
the input to the algorithm would be an
ad such as this ad for Geico car
insurance and the output of the
algorithm was the subset of queries that
might lead to a click on the ad such as
cheap car insurance or dub dub dub
geico.com using a tool such as this an
advertiser could figure out most of the
queries that might lead to a click on
his ad and he could then go to a search
engine such as Bing or Google and say
hey you know anytime somebody asks this
particular query please show them this
ad and if the user clicks on the ad I'll
give you a dollar now as you can well
imagine from the application predicting
phrases from webpages is a very
important problem both from a commercial
and a research perspective and so many
sophisticated NLP techniques have been
developed in the literature however the
way we decided to address the problem
was to bypass all these NLP techniques
and simply said that we're going to take
the top 10 million queries in Bing treat
each of them as a separate label and
learn an multi-label random forest
classifier which I will be referring to
as ml RF for the rest of the talk that
will take this ad as a test point
extract the bag of words features from
the raw HTML that lies behind the side
and then simply classify that feature
vector into these 10 million labels and
predict the corresponding queries so it
took us about two years to build ml RF
but when all the results came in and all
the performance evaluation was carried
out it turned out that ml RF had a
couple of advantages over least eight
about NLP techniques at that point of
time rich this is rich hi why don't you
view it as a binary classification that
goes the other way that tries to
classify so that was the standard
approach to it and if you want I'll come
back to that towards the end of the talk
but as I was just going to say one of
the big advantages we had as compared to
the binary classification problem was
that we managed to push coverage from
somewhere around 60% to 98%
so coverage is the percentage of ads for
which an algorithm makes non-trivial
recommendations and just to show you
over here right so if you take the
binary classifier approach that Rico
which was mentioning what you're
essentially going to do is you train a
binary classifier that's going to have a
sliding window it's going to go over
every phrase in this webpage and try and
predict whether it is a possible
candidate for a bid phrase or not right
so what happens is when you actually run
this on the page you only limit it to
whatever phrases are there on the page
and that turns out to be a problem in
this particular case because all of this
beautiful looking text is actually
embedded images and most ads are very
text impoverished so that binary
classification approach doesn't work all
that well and if you look at all the
predictions that we are making over here
none of them are actually phrases that
are present on the webpage so this was
one of the big advantages we managed to
push coverage up from 60 to 98% and the
product team really cared a lot about
that so there was a big win right over
there the second advantage was that even
if you focused on just the subset of
covered pages which means the subset of
pages ads for which the NLP techniques
could actually make predictions or our
predictions were significantly more
accurate so if you measured some things
such as precision at 10 then ml ahrefs
precision at 10 was about 5% higher so
that was helpful as well sorry by the
way guys I don't see very well so if
there any questions it's good to like
shout out or do a hula hoop dance or
something if you just raise your hands
I'll never see you right well where is
my so it's trying to go beyond relevance
in the sense that it's likely that in
the past we have seen users ask this
query and click on a very similar
so that's what that is trying to capture
that if somebody asks this query and you
show them the side they might click on
it there isn't a more formal definition
at the moment oh I see no so evaluation
is something that I'd like to come back
to towards the end at the moment this
was an advertiser facing tool so the way
you or the product group was looking to
evaluate it would be how many of your
recommendations were actually adopted by
the advertiser these are a subset of the
top 10 million queries in being provided
by device it by Bing yeah you go and
look at the Bing logs you see which of
the pop queries are most frequent or
most popular or generate the most
revenue you sort those take the top 10
million and that's what you train on any
other question yeah so that covered the
bulk of the revenue generating queries
so it covered enough for the group to be
the product rope to be kind of satisfied
you could have gone larger we internally
we actually went much larger than that
the returns are not that great so
anything else okay so where I was going
with this was that ml RF was published
in dub-dub-dub 2013 so two years ago and
since then many interesting research
questions have arisen in this new area
of learning with millions of labels
which we refer to as extreme
classification and I think two of the
most interesting questions are related
to applications and performance
evaluation which is what someone asked
over here so what I'd like to do is
start by discussing applications and
then if there's time towards the end of
the talk I will touch upon performance
valuation okay so ten million is a
really large number and I think one of
the most interesting questions is when
or where in the world do we actually
have 10 million labels to choose from so
I think they are a couple of
applications high-impact applications
that do exist at the scale even though
10 million is very large and one of them
is people so there are millions of
people who are uploading selfies of
themselves every day to Facebook and
there are millions of people who are
standing in front of Kinect cameras so
we could potentially use all this data
to train classifiers to recognize people
and then ask which subset of Facebook
users is present in this selfie and this
might have important applications in
social network analysis security
surveillance and so on
another interesting application could be
Wikipedia so if you browse if you scroll
down to the bottom of any Wikipedia page
you will find a subset of Wikipedia
labels that have been assigned to that
page by Wikipedia editors now the total
number of Wikipedia labels has crossed
over into the millions today and won't
it be great if we could build a
classifier that could take every
document every webpage every tweet every
query every image every video and
annotate it with the subset of relevant
Wikipedia categories there would be so
many applications that would get enabled
if we could do that successfully so in
particular we can think of building
these really massive scale knowledge
graphs with billions of nodes and
millions of properties right you could
go and stamp all the pages on the web
with with the set of Wikipedia
categories and now you know okay this
person is a VP at Microsoft or this is a
person as an AI researcher this person
was born in 1952 and this might help you
in building these massive knowledge
graphs you can also try and use these
for text feature ization so just as we
do with deep learning we take a deep
plant work and chop off the last layer
and now we use the intermediate
representation as features you could
play a very similar games over here as
well so we're trying to see whether we
can use this 4x feature ization but
apart from all of these applications we
figured out that what one can
also do is go back to core problems and
machine learning and reformulate them as
extreme classification tasks in
particular we can think about ranking or
recommending millions of items and think
about whether we can treat that as an
extreme classification task so the way
that would work is we can treat each
label to be sorry each item to be ranked
or recommended as a separate label learn
an extreme multi label classifier and
use it to predict the subset of items
that should be recommended to each user
thinking about ranking or recommendation
in this way might have a significant
impact in terms of performance in some
applications and that's similar to what
we saw with the phrase prediction and
NLP techniques that we saw earlier okay
so let's get into some technical details
about how we can tackle applications
such as Wikipedia or how we might be
able to reformulate problems such as
recommendation and the algorithm is
going to be called fast XML which stands
for a fast accurate and stable tree
classifier for extreme multi-label
learning and it was developed jointly
with my PhD student at IIT Delhi ashot
idea Prabhu okay now before I get into
technical details let me quickly give
you an overview of fast XML so that you
know what's coming for the rest of the
talk so it turns out that at this scale
almost all real world applications will
require us to make predictions in
milliseconds extreme classifiers should
therefore have prediction costs which
grow at most logarithmically
with the number of categories or the
number of labels fast XML ensures this
by employing a tree structured
architecture and learning very highly
balanced trees the second noteworthy
point about first XML is that its
prediction accuracy can be significantly
higher as compared to the
state-of-the-art by up to 20 to 25
percent in some cases fast XML achieves
this by optimizing a rank sensitive loss
function known as NDC G which turns out
to be a more
improvement improvement over traditional
tree growing loss functions such as the
Gini index or entropy or the Hamming
loss finally first XML can also be up to
a thousand times faster to train so two
years ago I needed a large production
cluster with almost a thousand cores in
order to train em lrf today I can train
fast XML on some of the very same
problems on a single core of a standard
desktop and that's thanks to a new
optimization technique based on
alternating minimization which comes
with provable guarantees so let's start
by considering the first bullet point
and seeing how we can architect first
XML so as to make predictions in
milliseconds so the way I'm going to
formulate the problem is that there will
be a space of users X and a space of
items Y and what we'd like to do is
learn a multi label classifier F that is
going to take a point in the space of
feature users and map it to a set of
points in the space of items so that
when a user comes in we can simply apply
our extreme classifier see which labels
get predicted and recommend the
corresponding items now the space of
items might be very large and even if it
takes us just one second in order to
determine whether to recommend an item
or not it will take us almost 12 days to
go through a list of 1 million items and
almost four months to go through a list
of 10 million extreme classifies
therefore face the daunting challenge of
having to reduce prediction time by
almost 10 orders of magnitude so down so
from 4 months down to about 1
millisecond some extreme classifiers
address this challenge by learning a
tree where each child receives only
about half of its parents items so when
a user comes in he starts off at the
root node which contains all the items
but then quickly in logarithmic time
traverses the tree and ends up at the
leaf node which contains only a few
items and these are the items that are
then recommended back to the user
we can also reformulate ranking
in this fashion and return a ranked list
of items to the user by sorting the
items according to their probabilities
so this sounds like a reasonable game
plan but for the fact that learning
hierarchies is notoriously hard and any
single learn tree would very likely have
been suboptimal first xml therefore
learns an entire ensemble of trees and
simply aggregates the individual
predictions in order to return a final
ranked list of items to the user so this
is the very same architecture that ml RF
used two years ago as well the only
thing that's going to be different today
about fast XML is the way that these
trees are going to be learned so let's
move on to the second bullet point and
see how we can formulate the learning
problem so as to make much more accurate
predictions the key technical challenge
that we need to address is we need to
figure out how to take a node and split
it into a left and a right child because
once we know how to do that then we can
start at the root node of all the trees
and keep applying this procedure
recursively until all the trees are
fully grown now note that our training
data comprises historical information
about which users like to which items
and even though there might be millions
of items to choose from
each user will typically like only a
small number of items it is therefore
far more important to print correctly
predict the items that are going to be
liked by a user and to ensure they're
highly ranked than it is to predict the
disliked items this is our key insight
and fast XML therefore learns to
partition a node by optimizing a rank
sensitive loss function known as NDC G
rather than traditional tree growing
loss functions such as the Gini index or
the entropy or the Hamming loss because
these traditional loss functions don't
have any concept of ranking built into
them and they place an equal emphasis on
predicting the light and the disliked
items so let me try and illustrate
what's going on when we start out all
the users will be present in the root
node but what we will do is we will
quickly
partition the users butt into a left and
a right child and the reason we're going
to do that is because a partitioning of
users is going to induce not only a
clustering over the items but also a
ranking over the items so if you look at
all the users in the left they all like
oranges pomegranates and bananas
sorry grapes so all of these three items
should be clustered together and sent to
the left but bananas should not be sent
to the left because nobody likes them
there furthermore if we look at all the
users on the left then we see that for
like oranges for like pomegranates but
only to like grapes
so oranges and pomegranates should be
ranked higher than grapes
now there are many different ways in
which we can partition users and each of
these different partitions will induce a
different ranking over the items and
what we'd like to do is through and DCG
choose that particular partition where
each users items are ranked as highly as
possible so the way we will partition a
node is by learning a hyperplane with
normal W in the space of user features X
such that W transpose X is less than
zero for all the users who've been
assigned to the left and greater than
zero for all the users who have been
assigned to the right and we learn the
sparse aspossible hyperplane that
optimizes n DCG and show that in the
results this leads to significantly more
accurate predictions but before we get
onto the results let me quickly talk
about optimizing and ECG so there going
to be some formula flying around
I had apologize in advance for that but
you can try and ignore that if you like
I'll explain everything intuitively the
key take-home message from the next set
of slides is we have a very efficient
way of optimizing and ECG which will
allow us to grow our fast XML trees in
minutes so coming here and talking about
and ECG is like preaching to the choir
right but there might be one or two of
you who don't know in DCG so please bear
with me the rest of you while I just
explain briefly and then I'm talking
about the optimization so it turns out
that end ECG is incredibly easy to
define what it does is it measures the
quality
of a given ranking for a particular user
it's a number between 0 and 1 and larger
values mean better during rankings so
the way you compute NDC's your defined
end ECG is you take your ranking you
look at the top ranked item and if the
user likes it you add a one to your
score otherwise you do nothing and then
you move on to the second ranked item
and if he likes it you add a 1 by log 2
otherwise you do nothing third item 1 by
log 3 if he likes it otherwise nothing
and so on okay
so end ECG is incredibly easy to define
but it turns out that it's also really
hard to optimize so you can see that
there's a sort function buried here
inside end ECG and this means that not
only is end ECG not convex but it is
also not differentiable which means we
can't directly apply any of the
large-scale gradient based techniques
that we've developed for efficient
optimization furthermore end ECG might
behave also might also behave very
erratically with respect to the
hyperplane that we are trying to learn
so large changes in the hyperplane might
have no impact on the induced rankings
and so end ECG will stay flat in large
regions of space but there will also be
situations when just a small change in
end ECG in the hyperplane will be enough
to send a few users from the left to the
right and this will change the induced
item rankings and so end ECG will
suddenly jump up so end ECG has this
nasty property that it's either flat
everywhere which means you don't know
which direction to go in if you want to
optimize it or wherever every anything
interesting happens it's it's
discontinuous right and this is a very
well-known loss function in the learning
to rank literature and over and Chris
have spent a lot of time optimizing this
in fact Chris at ICML won a test of time
award for all the work he's done on
optimizing and ECG for the last 10 years
so it turns out like based on all this
because end ECG is such a hard function
to optimize there's a very real danger
that fast XML might actually be harder
to Train than ml RF but it turns out
that this something special we can do
over here which at first is actually
going to sound counter
intuitive so what I'm going to do is I'm
going to make the problem even more
complex by adding in an extra 100
million variables into the optimization
now this is going to sound bizarre but
for the fact that each of these extra
variables that I'm going to add can be
optimized in microseconds and then the
remaining problem that I'll have left
will turn out to be a simple binary
classification problem in the hyper
plane which is our bread and butter task
in which we know how to solve very
efficiently yeah so here's what I'm
going to do I'm going to take each user
and add an extra variable for him or her
which I'll call Delta and Delta can be
either minus 1 or plus 1 depending on
whether the users been assigned to the
left partition or to the right partition
at the same time I'll introduce two
extra variables R minus and R plus for
each item and these will specify the
rank of the item on the left and the
right child respectively and at the talk
you can see how I've modified the
objective function to bind together the
hyperplane normal W to the item ranking
variables R minus an R plus to the use
of partition variables Delta now there
might be a hundred million users and ten
million items so we'll have added an
extra one hundred and twenty million
variables but here is how we can
optimize them very efficiently so we'll
start by simply assigning each of the
users to a left or a right partition
completely at random so there's going to
be a random initialization but because I
know which items each user likes this
will induce not only a clustering over
the items but also a ranking over the
items on the left and the right now you
can see that this is not a very good
partition because the induced item
ranking is not compact and so NDC G will
will be low but what we cannot do is
apply iterations of this alternating
minimization procedure where we're first
going to freeze the item rankings and
then optimize over the use of variables
and then freeze the user variables and
optimize over the item rankings and we
want to keep doing this until we
converge okay so the way we are going to
start is we'll freeze the item rankings
and now we going to up
all the user variables by going to each
user in turn let's say we start with
this particular user and we going to go
to each user in turn and ask him hey are
you better off sticking in your current
partition or would you actually like to
switch partitions so if we look at this
user we see he's been assigned to the
right partition based on the random
initialization but his items are
actually ranked higher on the left than
they are on the right so he would be
better off switching partitions and we
can do this for each user in turn and
get a repartition ofthe uh set of users
now note that each of these we
optimizing each delta for the user can
be done in microseconds because all you
need to do is go and see which items the
user Lite that's a couple of table
lookups you see their ranks on the left
on their ranks on the right compute and
ECG and whichever is higher you just
assign the user there so this is done in
microseconds the next step in the
optimization is to freeze the user
variables and recalculate the item
rankings right but again that's very
efficient right because you just need to
make one pass over your ratings matrix
see which users have been assigned to
the right and simply see how many words
were there for each items and sort okay
so just just going through the ratings
matrix and doing a sort so that can be
also done very quickly in microsecond
and now you can see that this ranking is
slightly better than the ranking we
started out with but this still room for
improvement so we can keep applying
iterations of this alternating
minimization first freeze the item
rankings and then optimize the user
variables then freeze the users optimize
item rankings and we can prove that they
will soon come a time where no user will
want to switch partitions any further
at which point of time we will have you
converge to a stable partition and a
stable ranking so I'm not going to go
into details of that proof you can come
and talk to me afterwards or look at our
kdd paper but essentially within a very
small number of iterations you'll have
reached here the only problem that is
left now is to separate the users on the
left from the users on the right but
that's a simple binary classification
problem and you can learn your you can
use your
favorite machine learning toolkit TLC VW
as your ml whatever you like that will
just optimize that l1 law gloss
objective Oh No so the excise other user
feature vector right so the his age his
gender his IP address whatever we know
about him so that's his feature vector
and the ratings matrix is a zero one
encoding of which items he liked so this
entire what I wanted to say was that
this entire operation can be carried out
very efficiently it optimizing each
Delta and R takes only microseconds and
this hyperplane can be learnt in a
matter of seconds and once we've learnt
the hyperplane then we know how to
partition a node into a left and a right
child so now we know how to grow the
tree and we can simply start at the top
and keep applying the procedure
recursively until the entire fast XML
tree has been learnt in minutes so let's
finally get to some results we
benchmarked first XML on a bunch of
small medium and large scale data sets
the advantage of the small data sets is
that they're all publicly available and
we can compare first xml's performance
to a number of techniques that have been
proposed in the literature of the medium
scale data sets we tried Wikipedia so
this is the challenge version of
Wikipedia
it has about three hundred and
twenty-five thousand labels but not many
algorithms will scale to it but it's
publicly available then the ads datasets
are all proprietary to Microsoft and the
largest ads dataset has about 70 million
training points 20 million test points
nine million labels and two million
dimensions so the results I'm going to
show you now are from kdd we just
learned that our nips paper got accepted
so paper on embeddings got accepted to
nips so the complexion of results will
change slightly at nips but for now
let's just go with the kdd results so
I'm going to start by
showing you results on the small data
sets so here is precision at one
precision at three and precision at five
of a bunch of algorithms on the small
data sets now because these data sets
are small we don't really care about
training time so we decided to focus on
prediction accuracy and we did a very
fine sweep over the hyper parameters of
all the algorithms except for fast XML
so first XML also has a few hyper
parameters such as the number of trees
or the maximum depth of a tree but we
decided to keep these fixed and set them
to default values across all data sets
both small and large because ultimately
we will care a lot about fast XML
straining times and I don't want to
spend time retraining fast XML again and
again and again with different hyper
parameter settings when we get to the
large data sets so of course this gives
all the other algorithms a slightly
unfair advantage over fast XML but
that's alright because even with one
hand tied behind the back first XML can
still equal or outperform all the other
algorithms that have been published in
the literature so if we start by looking
at all the low rank matrix factorization
collaborative filtering embedding
techniques that have been published
starting from the compressed sensing or
CS work of John Lankford Shum Kaka day
going all the way up to the absolute
state of the art in the field at least
until the coming nips which is the le ml
algorithm of in the G Dylan pratik Gen
we see that fast XML is much better than
these considerably better even if these
techniques were taken to their limiting
case and we learnt a full rank matrix
rather than low rank matrix as is done
in the one versus all base line these
models might still not be able to
outperform fast XML finally fast XML
might be better than other tree based
methods as well particularly as compared
to MLR F which is the multi labeled
random forest that we proposed two years
ago as well as LPS R which is a
technique that Jason Weston had proposed
when he was still at Google and which
was shown to give lifts in CTR while
recommending videos on YouTube
so it would appear that thanks to end
ECG even untuned fast XML can equal or
outperform all like highly tuned
versions of all the algorithms that have
been presented in the literature however
this is not the scale for which fast XML
was designed so if we move to a slightly
larger dataset this message gets even
more strongly reinforced there are a
couple of things to notice the first is
that most of the algorithms can no
longer scale to Wikipedia if we restrict
training to be something reasonable
let's say up to one full day on a
standard single core of a standard
desktop of the algorithms that do scale
fast XML stopped ranked prediction can
be significantly more accurate as
compared to both le ml and LPS R so as
compared to le ml there's about a 30%
improvement and about 25% as compared to
LPS are the second thing to note is that
two years ago it took me about five
hours to Train MLR F but that was on a
thousand node cluster on cosmos today I
can train fast XML on Wikipedia in in
say it took me four hours earlier now it
takes me about five hours but that's on
a single core of a standard desktop and
that's thanks to the new optimization
based on alternating minimization but
again this is still not the scale at for
which fast XML was designed if he just
wanted to stick at this stage at this
scale there a bunch of other tricks we
could have played which would have
pushed this up to nearly 60% so if we
moved to slightly larger datasets we see
the trend is still the same so fast XML
is more accurate at prediction than both
LEM l @ &amp;amp; LPS are at the ads for 30k
data set and then LEM L can no longer
scale to the ads 1 million data set and
neither LEM L nor LPS are can scale to
the ads 9 million data set and if you
notice throughout our prediction time is
always either less than one millisecond
or around one millisecond in the largest
case so this thing can actually be used
in real world applications if you like
now the reason I was fixating on
training
single-core is because that's what my
student had back home I'd IIT Delhi but
modern-day machines are have more than
one core and fast XML can exploit that
trivially by paralyzing by growing
different trees on different cores so
here is how my training time varies with
the number of course you can see that if
I have access to 16 cores then my
training time is less than half an hour
on both adds 1 million and on Wikipedia
and I can train in on ads for 30 K in
less than 5 minutes my training time on
ads 9 million is still very large it
takes me about 10 hours to train 30
trees and 17 hours to Train 50 trees but
that's much better than the two days it
was taking me two years ago on a
thousand node cluster however if any
experts out there in terms of
large-scale learning on GPUs I would
love to discuss this more with you and
see if we can speed this up even further
the random initialization is different
and then we are using l1 log loss though
it's not strongly convex so depending on
the alleged ization you get different
results for that sorry we tried we tried
the randomly sampling but data points or
the features but that didn't help that
wasn't a very good idea maybe it would
slow your training down a lot and I
really really wanted to make training
very fast over here what happened was my
cluster got taken away from me right
they said I was contributing too much
the global warming so I said what's the
one thing they can't take away from me
right it's my desktop I really really
want to train on one desktop so I was
really fixated on making this thing
train very quickly sure if you had the
luxury you could
do that I get to what appropriate loss
functions might be on which you might
want to compute the gradient so actually
that's going to come just next so let's
have that discussion in in five minutes
if you don't mind understand why it's
faster what's your intuition so more
accurate is compared to what if you look
at all the low rank matrix factorization
work that's been developed right I mean
if you look at recommendation almost
everybody is done collaborative
filtering or low rank factorizations but
when you come to this scale if you look
at the ratings matrix there is just no
way it can be low rank so what will
happen is you'll have a hundred million
users each of these users in the tail
will like two or three items but there
will be a different two or three items
so there'll be a column like one entry
nonzero no.10 nonzero the next guy will
be different three entries nonzero next
guy another different three entry is
nonzero so there's no way any row can be
written as a linear combination of the
other rows it's not really that much
better than when you the real lift comes
here at Wikipedia and when we went to
Wikipedia and when we looked at the low
rank matrix we did a SVD into the top
500 eigen vectors and we saw that that
captured only 10% of the matrix and 500
is a number that's much larger than what
everybody else uses everybody else goes
between 20 to 50 embedding dimensions
because otherwise the computational cost
is just too high
prediction training both right so only
10% of the matrix is being captured so
that's what our nips paper is about
we're trying to break free of the low
rank assumption and see whether we can
do something which has a nonlinear
embedding that reserves local distances
and that apparently works much better so
that's the reason for the performance
improvement over low rank methods as
compared to other tree based methods
well I think we had much better than LPS
are because all Jason does is he does
k-means and and clusters this
two sets of users and goes with that
right so there's no regard for the loss
function there
let's compare to MLR if I think the main
reason way better is because we're not
taking single features and and learning
splits on that way learning a full
hyperplane and what was happening with
ml RF is that when you have 10 million
features you're 2 million features your
features become super selective which
means that if you were to take a split
on any single feature then even for the
best features you would have let's say
ten thousand or a hundred thousand
documents going left and then the rest
of the hundred million going right so
you learn these imbalance trees and you
don't get very good generalization you
can try and force ml RF to use multiple
features or learn balance trees and then
your training time goes up even further
and then your accuracy in some cases
comes down because of the regularizer
so I think that's why this is better and
then end ECG is kind of you want to do
some kind of ranking in any case that's
how you going to measure performance
right these are more like ranking and
recommendation tasks so it turns out to
be a better thing to do the only thing
I'm not sure about is which I would love
to get some help from you guys is I
don't know what the objective function
should be at the root node because
ultimately the predictions are going to
be made by the leaves right so if I'm
taking something like procession at K I
want to optimize procession at K at the
leaves or I want to optimize and ECG at
K for the leaves but doing that at the
root node would be too myopic right so
if I just want it to optimize let's say
end ECG at five at the root node all
that would says what are the five best
items on the left and what are the five
best items on the right and I don't care
about anything else so if you have
Wikipedia where you have let's say half
a million items there's no way that will
work well so we decided to go with NDC G
over the entire set of items and one of
the things that you pointed out which
was very helpful is that ranking over
the rest of the tale there's a lot of
information over there
so trying to learn a function over the
all L items rather than just the top
five can help you in any case so we
found that not trying to be too myopic
towards the
looking at ranking all the items rather
than just the top K seem to work well
sorry there was a huge monologue so the
conclusion for this part of the talk
well there only take two take-home
messages the first is that extreme
classification is a new area in machine
learning which will allow us not only to
tackle classification problems at web
scale but which might also allow us to
go back to other problems in machine
learning and trying to reformulate them
as extreme classification tasks the
second take-home message is that fast
XML is a new algorithm for extreme
classification which can make
significantly more accurate predictions
as compared to other methods and which
you can train on your desktops so if
you're interested in code just email me
I'd be very happy to share the code or
the data so it's both the objective
function so earlier on I was trying to
optimize Gini index now I'm trying to
optimize NDC G and earlier on I was
doing the standard random forest
optimization which is the brute force
search M thresholding and now I'm doing
the alternating innovation those are the
only two changes and both are important
I'll show you results later on about how
that regice just wait for five minutes
and I'll show you those yes so I'll show
you results in five minutes if you know
me now so I'm just saying five minutes
because I want to just finish of this
last portion of the talk where um what I
want to do is go beyond discussing the
specifics of a particular algorithm and
return to the general area of extreme
classification why did we need to come
up with a new area well why did we need
to come up with a new name why couldn't
we just have done whatever people were
doing earlier on so what I'd like to do
is discuss how extreme classification
could be different from traditional
classification and I think that not only
are the scaling and computation
spec's different but I think the
statistics are also different and this
is perhaps best highlighted by
considering how we might evaluate the
performance of extreme classification
algorithm as compared to a standard
traditional classification algorithm so
this will go back to the performance
evaluation question that somebody asked
right at the beginning so it turns out
that a number of loss or gain functions
have been proposed for evaluating
traditional multi-label learners such as
the hamming loss or the subset zero one
loss or Jaccard distance precision
recall F score and so on and so forth
but I think that these might not
directly apply in the extreme setting so
I've made a toy example to convey that
and the u.s. presidential elections are
very much in the news which is why I
chose this particular example so I am
going to give you the wikipedia pages
for five US presidents and the task is
to label each of these pages with five
wikipedia labels and I'm showing you the
results of three algorithms over here
which attempt this task the first
algorithm is a constant algorithm which
means that no matter what the input is
it's going to output the same list of
labels so it looks at some of the most
popular labels on Wikipedia sorts them
by popularity and this outputs that list
so that's algorithm one algorithm two is
something that does the opposite of one
instead of looking at the tail of head
it completely ignores the head and focus
is just on the tail so it's trying to
predict these labels that are not very
common but because this is a very
difficult task it often makes mistakes
and I'm showing that by these dashed
lines over here so I actually put this
algorithm because of Leon's keynote at
ICML where he was discussing metrics a
lot and one of the metrics that he was
proposing was coverage right so if you
look at coverage I've put this algorithm
for him over here and then the third
algorithm is something that is a mix of
1 &amp;amp; 2 right it has some head labels but
then it also has some tail label so what
I'd like to do is just do a quick show
of hands I want to see which algorithms
you like so people who
thing algorithm one is the best should
raise one hand people who think
algorithm two is better than one and
three should raise both hands and then
people who like algorithm three sure
like two hands and a leg or something
I don't do anything hey I'm gonna do a
hand count for me please see so I think
most people would prefer the third
algorithm consider well here right most
people by and large sorry okay but if
you look at all these fewer thumbs right
algorithm one is the one that maximizes
precision so no other algorithm can hire
higher precision than one but typically
we don't tend to like it very much I
didn't I've been measuring performance
in terms of precision all this while if
you look at algorithm 2 this is the one
that maximizes coverage so as I said
coverage is the number of labels or
categories unique labels or categories
that you got right so this has the most
number of correct predictions across
labels but many people don't tend to
like this as well most people tend to
like 3 but 3 actually doesn't optimize
any of the matrix I'd listed in the
previous page and if you look at why
that is the case then it's very clear
why traditional loss functions might not
be working so well so I think one of the
reasons is something I've already
alluded to and that depends on the
statistics of the positive to the
negative labels so if you look in all of
these data sets on most data points the
average number of positive labels is
completely dwarfed by the number of
negative labels so positive means
relevant and negatives means it relevant
in fact in many cases you have less than
8 positive labels per data point and
then hundreds of thousands or millions
of negatives so traditional loss
functions such as the hamming loss which
place an equal emphasis on predicting
the positive and the negative
labels cannot work well in this setting
now it turns out that there's also
another more subtle reason why loss
functions computed on the negative
labels don't work that well and that's
because many of the negative labels
aren't really negative so traditionally
we in supervised learning we've been
working this paradigm where we we
suppose that there exists an expert
somewhere out there and I annotated on
expert who whenever we give him a test
point can tell us what other labels that
are relevant to that test point and so
performance evaluation is easy however
in the extreme setting there cannot
exist an expert or an annotator who can
go through a list of 10 million labels
and mark out the exact relevant subset
so even if we look at Wikipedia and
Trust Wikipedia editors here's Janet's
page it has some labels but you can see
that many labels are missing right so
for example it doesn't mention that
she's a VP at Microsoft but she was a
director at DARPA it has very little to
do mention about her research and her
contributions there so there's nothing
about type theory and so on right so a
loss function which would penalize you
for predicting that type theory is
relevant would actually be not a very
good loss function over here so I think
we should try and move away from loss
functions that look at both positives
and negatives such as Hamming loss or
other loss functions and look work only
with those loss functions that focus on
the positive labels but that doesn't
solve the entire problem because of
course those will be still biased but it
will also turn out to be the case that
all traditional loss functions that work
on just the positives are treat each
label as being equal however that is
definitely not the case in the extreme
scenario so if you look at the
distribution of labels there are few
labels that have a lot of training data
that occur very frequently and so it's
easy to train on those labels and it's
also relatively easy to predict them
however the vast majority of labels in
an extreme scenario occur not very
frequently in many cases they occur only
once on some of the data sets why I'm
showing you over here
many of the labels occur less than five
times so these data labels they don't
occur very frequently and they're very
hard to train on and they're extremely
difficult to predict correctly however
in many scenarios you can get much more
reward for predicting these rare labels
than the relevant label than the popular
labels so again like in Jeanette's
webpage the single most popular label on
Wikipedia is living person right but
there's very little information gained
in predicting that Jeannette is alive
right it's much better to predict from
the tail and say like predict about type
theory or a VP at Microsoft etc and this
might not be such a problem if all we
had to do was predict all the positive
labels right then then it's fine you
just predict all of them and you're done
but in many cases the number of slots
that we have available for prediction is
limited so in particular if you wanted
to do recommendation then you might have
only five slots on your ads page or only
five spots on your Amazon page and you
want to figure out what are the five
best labels our issue I should show and
particularly in recommendation
recommending items that are popular and
common and that you might already know
about might not be very helpful what
will be what will work the best is is
recommending these rare labels that
really delight and surprise you right I
had no idea this existed but it's
irrelevant to me it's perfect for what I
want and thank you very much for that
recommendation so I think if you were to
design a loss function for the extreme
scale we would need to focus of course
on accuracy right but we should do this
in an unbiased way we should try and
come up with loss functions which as
your test set becomes larger and larger
the value that you compute for your
metric should converge to the value of
the metric had it been computed on the
full observed ground truth and there are
tools and statistics which can help us
do that so in particular propensity
scoring or important sampling and we
started looking at some of those but in
addition to focusing on just accuracy
I think algorithms or sorry loss
functions in the extreme scenario should
also reward predicting rare or normal
items and that will work well in
some applications but in some
applications you might want diversity so
in some applications it's important to
have a little bit of the head in there
as well right because that's where you
generate most of your revenue or in some
situations you might you might want to
have a mix of your head and tail and
then finally at least particularly in
the recommendations in REO you might
want to reward algorithms that are
explainable or that they need if they
can explain why they made a certain set
of recommendations to you you might
trust them more so I think this is a
fascinating research area there are a
lot of open research questions over here
that might not exist in a traditional
setting and I'd love to discuss these
with you or if there's any interest
share code or data or talk to you more
about them so that's my pitch and thank
you very much so I'm happy to take
questions let me just answer on and ins
point quickly about whether the
optimization is more important or the
loss function is more important so one
of these is variance of first XML I
think yeah it's right so if you look at
this right what we did is we can take
NDC G and put it in MLR F right so you
know optimization is not going to be the
alternating minimization it's going to
be the standard random forest imp
optimization where your sample features
randomly do a sweep pick the single best
feature so you can see that in most of
the results ml RF optimized optimizing
and ECG does not do well at the same
time you also said okay what happens if
you take first XML and restrict it to
optimizing precision at 5 or NDC G at 5
which I was mentioning to Chris earlier
that also doesn't do well so I would
think that not only is optimizing and
ECG important rather than Gini index or
entropy but it is also important to
optimize it learn a full hyperplane and
do the alternating minimization other
questions
I had a I mean a question or a comment
or I don't know just talking to them so
I think that so first of all I think the
word is great I think it glanced over
one important thing which is kind of
study and maybe that's why but you kind
of passed over it as if it doesn't exist
so I want to just ask you what you think
about it so when you talk about this
setting you talk about one or two
scenarios one is where you have a very
big but fixed set of labels mmm and now
the training set asymptotes to infinity
so now we can think about it as a
standard machine learning question only
with the constant number of levels being
very very big but all of our Theory all
of our understanding all of our
intuitions still works out well and I
think the example you gave as you said
well let's look at the labels give it a
Wikipedia and let's use those to label
the entire internet so threes we keep it
at one point in time we have many many
labels but it's fixed and now the web
keeps growing and growing and asymptotes
to infinity and we can apply our
standard statistical tools understanding
the machine if you had given a slightly
different example if you would say
listen let's use the limo's in wikipedia
and label new wikipedia pages they come
in now you would have fallen into the
trap because now you have two things
going to infinity one is the number of
labels the other is a desert size they
are going to be the truth in it so every
new wikipedia page appears with new
labels so the label said grows almost
you know perhaps at the same rate is
your training didn't go so again sample
size and number of labels both grow the
same rate and now all existing machine
learning theory goes out the window and
many things that you kind of took for
granted here you know you were focusing
on doing it faster and do get better so
on now all bets are off you have to
start from scratch and it it seems like
I don't know maybe you were saying that
this will work in both scenarios
yes so there is I see no reason why it
should work into that so I'm not a
thoroughly good
so theoretically things might really go
down in the basket
in fact that's a great way for you to
come in and help people some theorems
about what is happening right
practically we've observed two things
one is as new labels come in both in the
tree setting and in the embedding
setting which is for our nips paper you
can always take
new labels and add them to the leaf
nodes of the trees or embed them into
the common space that your the the the
CCA space the low rank matrix
factorizations
in either case new labels as they come
in simply get inserted into the problem
and now you can predict them and the
guys at Google Sammy Ben geo Jason
Westen have been doing this for a number
of years now so if you go back to the
zero short learning setting you can new
a label came in you just propagate it
around your tree figure out which leaf
node is and you update the leaf node
distribution in the extreme multi cloth
setting John Lankford has been working
on these long trees so there's online
learning points are coming in as a
fashion and the trees are grown as the
data points come in and again you can
apply similar kinds of things so John
actually has theorems showing at least
some that is trees will be balanced and
stuff I don't think he has a guarantee
for like statistical guarantees and
that's actually a really interesting
theory question right how do you even
give statistical guarantees at this
scale what does it mean for me to have a
million labels like what is the
complexity of this space I could take a
one label and replicate it a million
times but that doesn't mean I have a
million label problem right and as you
were pointing out the way the label
space goes to infinity might be really
interesting so I think there are really
new theoretical research questions over
here what is the dimensionality of the
ambient label space how do I give
statistical guarantees that I would love
for theory people to work on some Theory
work has recently started coming out so
at ICML there was a workshop on this
which I'm told by the organizers was
very successful they say that it was the
second most attended workshop after deep
learning whether there's a big curve at
600 for deep learning or 100 or 200 for
you so what they were saying was let's
say I'm going to take a Hamming loss and
and optimize that using man versus all
or whatever standard techniques I have
but now suppose I have to do efficient
predictions I'm going to construct the
tree well how much worse will my Hamming
loss be can I give a bound on that as
compared to the Hamming loss over the
entire one versus also so I think there
lots of new questions coming out and she
just put them up on the slide if people
are interested in working on there might
be very happy to chat and discuss
that's what I think</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>