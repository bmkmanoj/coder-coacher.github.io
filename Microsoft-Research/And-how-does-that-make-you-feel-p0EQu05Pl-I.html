<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>And how does that make you feel? | Coder Coacher - Coaching Coders</title><meta content="And how does that make you feel? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>And how does that make you feel?</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p0EQu05Pl-I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thanks everybody for coming to our
session it's the last session of the
microsoft research faculty summit thank
you all for coming to pretty much
everything over the last couple days and
we're going to top today we're going to
be talking about basically effective
computing like looking at people's
emotions and trying to build software
tools that can react to those things and
for a number of different user case use
of scenarios so first to let me
introduce everybody who's going to be
speaking today so my name is Andrew
bagel I'm a senior researcher in the
vibe group at microsoft research in
redmond and i'm going to be speaking
today along with my co speakers this is
aaron salt solavei maybe stand up as you
can see aaron is an assistant professor
at Drexel University and recently
finished a postdoc at MIT and she's
going to be talking to you about some
really cool stuff that might actually
save your life and then our third
speaker is Mary Stravinsky who is a
research manager at MSR Redmond who was
actually my research manager at MSR
Redmond and she'll be speaking to you
about things that you can do to reduce
your stress and become a happier person
so that's why the title of the stock is
the cryptic and how does that make you
feel we're looking at all the kinds of
things that help infer what you're
feeling as well as trying to do
something positive to help you in
response to that so this is really the
field of effects from heating which is
really where we're studying basically
using devices that can recognize
understand and be able to analyze what
your body is implicitly giving off the
emotions the affect cognition and really
it kind of spans a lot of different
areas there's a lot of psychology a lot
of cognitive science and a lot of
computer science and if you are like me
you had to go back to your signals and
systems books that you have in your
electrical engineering classes back in
college and actually remember how to use
matlab again in order to be able to do
this kind of fun research so all this
kind of originated with raz bacardi
in terms of the modern field of
affective computing now basically the
idea is to basically build a system that
can understand how you're feeling are
essentially artificial empathy in a way
call it that way and be able to either
to give you feedback or help modify your
effects so that we can change you in
different ways so the things that we're
using are basically non-invasive sensors
and I'll show you a couple examples of
those sensors to pick up the signals
that your body is putting off just as
you naturally just stand around or do
different tasks different parts your
body is actually reacting like your
heart rate is changing your pupil sizes
are changing this the amount of sweat on
your skin changes and at various time
scales in response to everything you see
everything you do everything you think
about which is really exciting because
then you can build systems that react to
those things so really a lot of this
became very feasible actually only a few
years ago in with the rise of all these
really cheap sensors that people wear on
their wrists a lot of people had Nike
FuelBand zor fitbit's they also had
chest straps I got a heart rate monitor
when I went to the gym for the last
couple months and its really kind of fun
you can see what happens after you do a
particular exercise my heart rate like
jumps really high but apparently it's
good when it comes back down really fast
that means I'm getting better at the gym
and we use a whole bunch of different
kinds of sensors and we put the sensors
on our body we also put them inside our
body there's kind of pills you can
swallow that will watch your intestines
as you as they make their way through
and for a lot of health applications we
also have exoskeletons things you can
wear on the outside your body that will
react or actually show off how you're
feeling to other people so it's even
more you can use it like displays
especially for those who arts are good
at body language or communicating that
way so this was just the intro we're
going to split up into three sections
here first will be Mary talking about
detecting and relieving stress and
anxiety
through sensors in and applications and
then errands elevator will talk about
improving driving especially when you're
driving distracted or being able to
detect when you're distracted and trying
to do something about it and then I'll
wrap up with a discussion about how
we're trying to look at software
programmers and in particular I look at
those at Microsoft to figure out when
they get stuck or confused and maybe
stop them from causing bugs in the
software that you guys use and each of
us will speak for about 20 minutes and
then we have time at the end for
questions so sort of hold for the most
part if you can your questions until the
end we'll have people with microphones
running around so if you do have a
question you'll wait for the mic say
your name and then ask your question
that way all the people who are watching
on the web and apparently according to
twitter at least searching for me there
there were people watching for the last
day or two so that's kind of exciting
but so you've got an audience as well so
thanks very much and I'm going to hand
it over to marry now to speak about her
work thank you thank you Andy hi
everybody welcome to sunny Seattle it's
like this all the time here's an outline
of my talk I've done lots and lots of
projects in the affective computing
space and there is a lot of science
around these projects in terms of user
research longitudinal research for this
talk because I only had 20 minutes I did
want to show you the breadth of what
we've done and what I've got to
sacrifice is there for all the details
of the studies and all the data but I
have a role in backup slides so if after
the talk you want to come up and ask me
any questions feel free to do so but
today it's gonna be a lot of videos ok
so our rich research group really
believes that emotional health is just
as important if not more important than
your physical health in fact a lot of
times when you have a physical ailment
the emotional impact ends up causing
more bodily harm than the actual ailment
insult and you know a lot of
organizations have come to feel this way
there's there been a path ons that have
gone on lately there have been all kinds
of studies of countries that are the
happiest in the world and people have
analyzed Twitter feeds for my country by
country and so it seems like there's a
lot of awareness now
that this is you know beyond fitness we
need to be looking at emotional fitness
and a lot of people ask me you know
three years ago what why is Microsoft
interested in this why would they care
well actually there's a ton of product
interest in this so you know everyone
from xbox to Windows is interested in
how stressed out their users are or how
overjoyed their users are and can we do
more of that that latter one so we
actually feel like this sort of
affective computing research could be
built in baked into our products from
the very beginning so we have focused
first on stress and anxiety as a group
because it's such a pervasive problem I
always tell the joke that the more I
read about stress the more stressed out
I got three summers ago and I'd walk
around like this just crazy because it's
so bad for you you know causes obesity
causes cardiovascular health problems it
causes a lot of people say that they are
stressed out but they say that they
don't actually know how to de-stress and
here I usually have a picture of people
drinking alcohol because that seems to
be one of the ways that we do distress
other people work out some people do
yoga some people meditate but most
people don't have a good handle on their
stress so the first application i'm
going to talk about is effect aura it
was actually the first emotional
prosthetic that automatically logged a
user's emotions and allowed them to
reflect on this over time and I believe
I have a video here just start I hope
note i had to find where the controllers
we present affect Dora I apologize
patient of a user's ashes over time for
reflective purposes how well do you
think you remember your mood over time
we have found that people's moods are
quite volatile and can change rapidly
during the course of a day and that
users tend to forget these mood swings
pretty rapidly we developed at victora
to visually assist users in keeping
track of their emotions over time we
combine multiple streams of sensor data
in order to capture the user's context
and model their emotional state let's
explain effect Dora lays the day out on
a timeline including where the user is
which we obtained through GPS locations
are shown as icons on the timeline
the balloony shape colors indicate
hourly mood patterns positive and pink
tones and negative and blue tones the
size of the balloons indicates the
user's activity levels and we show the
kinds of documents and websites the user
is visiting at that time smooth balloon
shapes suggests that the user is less
engaged in the activity while bursty
shapes indicate high levels of
engagement with the activity at hand the
user can flip through days by using the
navigation arrows to either side of the
timeline let's take a look at Joe's
effect Dora on Monday just starts the
day out on a low or negative tone but
then as he becomes more active he
becomes more engaged in the work he's
doing and his mood picks up by the time
he leaves the office in fact he's in a
better mood let's compare that to his
day on Tuesday Tuesday starts out
extremely busy and he seems happy as he
works however after lunch it appears as
that Joe is working very hard but his
mood has soured Beck Dora allows Joe to
take a look at what happened to trigger
this mood swing looking over his day we
can see that at about 1pm it appears as
though Joe received a nasty email from a
colleague this must have triggered a
negative mood swing that seems to have
impacted the rest of his day effect
erick can be used to go back in time and
reflect on what patterns of behavior
lead to happy or sad outcomes a user
study showed that users found effect
dora to be useful for reflecting on and
remembering mood swings over time and
okay so effect aura was great for giving
users insights into little micro
patterns that's what i like to call them
of daily ups and downs emotionally of
course if something really emotionally
significant or salient happens you're
going to remember that quite well we
know that from psychology it's the
little micro trends that people seem to
not have a very good handle on of Oh
every time I go to that meeting I'm in a
horrible mood or every time I'm with
that person or when I don't work out and
they started to see those qualitatively
and quantitatively and it was really
eye-opening to them and they really
liked it the only thing you
just told us that they didn't like was
they were really worried that there were
going to be false memories instilled
into their system and they wanted the
ability to be able to delete the
system's events when the system got it
wrong which would be very easy to do and
the other thing they really wanted was a
system that was in real-time effect Dora
actually you had to go back and
visualize the users data it wasn't
running in real time so the next system
we built we built in real time we are
presenting some of our work on honest
signals which I'll talk a little bit
more about in a minute and a professor
at u-dub stood up and said you know you
could take all of these signals from a
user's face and from their voice and
actually apply that to doctor's bedside
behavior and we thought oh that's very
interesting we hadn't thought of that
application and it just turns out that
clinical empathy is actually strongly
correlated with patient outcomes so they
leads to higher satisfaction with their
treatment it leads to them in hearing to
their meds better side effects are
reduced and there's less anxiety and
fewer complications however it isn't
really taught in medical school if you
can believe that what doctors get is
they get observed by a paid hand coder
who know who's very expert at coding
empathy and they watched them interact
with a patient for about 10 minutes and
then they get a score on how empathic
they are and that's pretty much it so we
knew doctors time was extremely valuable
we were working with oncologists at the
University of Washington and Seattle
Cancer Care so we needed to design the
system in a different way than normal
because we didn't want to use doctors
from the very beginning and this is just
showing that most of the time empathy
measurements are made by the patient's
self reporting on the doctors or by the
doctors self recording themselves so we
wanted to take this honest signals
theory that sandy Pentland came up to it
actually was inspired by biology sandy
thought that there were these four
features activity consistency influence
and mimicry that all animals all
biological animal is used in nature and
this way we establish dominance and
stuff like that well we wondered if we
could take those same four signals which
we'd already been using in some of our
video conferencing work and kind of map
them to things that matter for empathy
which was which was affiliation control
and
warmth versus coldness and dominant
versus submissive so we we tried to do
that with this system but we had to do
in a special way like I said because we
had to preserve the doctors time so we
did a wizard of oz and so what basically
we ended up doing was in medical studies
what they do is they hire this woman
over here she's actually a trained actor
for medical as a medical patient so
she's hired by the University come in
and have one-on-ones with the doctors
and what she's very good at is we give
her a scenario and we actually pulled
one of the scenarios that they actually
use in medical school we gave her a
scenario and she was supposed to act out
that scenario for the hired health
professional and health professional is
here on the other end and so what was
happening was this is with a room with a
one-way mirror behind the mirror roopa
roopa the one who was the primary on
this project is actually moving the
visualization based on how empathic she
thinks this health care professional is
being so the system isn't really reading
her facial signals or listening to her
voice it's that it's actually Rupa
moving the the system and there's a
picture there of the first visualization
we used which didn't really work it was
a son that got warmer and brighter
bigger the more warm you were and a
teeter-totter that teeters either
towards the healthcare professional or
towards the patient depending on who had
controlled the situation who was being
more dominant and the first design
people thought you know the
visualization was helpful they could
read it they could see it it was behind
the patient so the patient didn't know
what was going on but it was all so
confusing and distracting and we think
primarily that first design was probably
not the best because it had two
different dimensions that they had to
track at the same time while they were
trying to be empathic and talk to the
patient and that didn't really work so
we went and did a redesign and I'm going
to just show you a little video of what
that looked like
clinicians nonverbal communication skill
can make a big difference to patients
our entendre system records the
interactions of patients and clinicians
and provides visual feedback for
clinicians clinicians can choose to
adjust their nonverbal cues in response
to the feedback which could result in
patients feeling less anxious trusting
their clinicians more and potentially
having better treatment results how does
entendre work nonverbal cues are picked
up from live audio and video streams
nonverbal cue sequences from the video
are assigned relative weights of control
and affiliation control and affiliation
our core dimensions of interpersonal
communication the control dimension
ranges from submissive to dominant while
the affiliation dimension ranges from
hostile to friendly we are currently
exploring different visual feedback
designs that display levels of control
and affiliation 1feedback idea comes in
the form of a lotus flower as a new pair
of petals forms the pedal size adjusts
in real time based on the individual's
level of conversational control
the pedal color also changes depending
on the affiliation of the individual
here the pedals change from a cool blue
to a neutral gray to a warm orange as a
clinicians affiliation level increases
each pair of petals reveals information
about nonverbal signals during one
minute of conversation and the entire
flower represents the last five minutes
of conversation we are further examining
the use of entendre with clinicians and
patients in an actual clinical setting
with various designs ok so what we did
was we brought those same healthcare
professionals we had 16 of them in the
first study we brought them back and had
them look at the lotus flower redesign
they actually watched their old videos
while rupa then Wizard of Oz the lotus
flower based on how they behaved in the
first session and this worked quite well
the health care professionals totally
got what was going on all but one of
them said they would use it in a
professional clinical setting the one
person who said she wouldn't use it was
because she was empathic enough already
thank you very much one suggestion we
got was that we could put the lotus
flower in the face of the clock because
the doctor said they all have to keep
looking at that anyways not a very good
thing to hear but it's true so we're
still working on that in fact we are
working with a guy in Australia now
who's implementing the whole system for
us again for real he's an expert in
affective computing and we're actually
going to test it with real doctors
finally it's been a couple years so
we're excited about that ok this is just
kind of a fun one I'm just going to show
the video for this but of course we did
run a user study this is where we
thought maybe the walls could talk to
you oh no I thought we tested all them
in there worked ok I'll just describe
what it was really fast but it's a great
video we had an MIT graduate student
work on this she was from the
architecture Department and what she
designed was and she did many studies
leading up to what this actually looked
like she found fabrics fabric shapes and
forms that could evoke happiness
or calmness and then that could have oak
stress and anger and what she did was
she designed using nitinol wire a wall
piece that would go from really smooth
which was supposed to suggest com2 very
angry he got all scrunched up it turned
red it got really loud and we did bring
people in and have them look at it and
it really really worked it did what it
was supposed to do and when people
watched it calm down they said it
reminded them of a fish bowl you know
someday your whole home might be giving
you feedback about what the attention is
in the room and so that could be very
interesting this is just data that
showed that people when they saw in the
scrunched up mood they're in the upper
left quadrant of the two-by-two
circumplex model which means stressed
angry and then when they saw it calm
down they've all moved down there
self-ratings of stressed down to that
lower right or at least further to the
right so they were happier or calmer
okay i'm going to show you another video
if it works I sure hope this one works
great okay um so the idea here is that
we had a butterfly that we designed
again this went through many many
iterations of design that actually
flapped its rings the harder it's
flapped its wings harder and wider the
more stressed out you got let's see if I
have a better yeah here's a better
picture of it we had users come in and
do a driving simulation the bad part of
the design the graduate student dynamic
Lane was a fantastic graduate student
the bad things she did though or that we
all did together was that we kept making
the driving simulation more and more
stressful so first you're just driving
along and the butterflies kind of barely
flapping its wings and then before you
know it we've got trucks slamming in
front of you and people walking out in
front of your car and all kinds of
construction and by the end of the study
like it's completely hard to drive the
car and the force the butterflies going
like this so now we know we can really
stress you out by showing you how
stressed you are because that's what the
butterfly did and everyone said not only
physiologically were they more stressed
out they were very highly aware of it
but they did drive significantly better
so there is some good uses of stress
obviously we now know that we designed
it backwards that we should have made
the butterfly's wings start two flaps
more slowly as you were more stressed
out because that might have been a
useful way of calming yourself and so
we've moved our designs now to more
calming designs than showing you your
stress because we know we can stress you
out like I didn't want to really go
through all this data I just wanted to
show you the video okay and the last
project I'm going to talk about real
quickly here this is when I'm real proud
of there were a couple that we did this
last summer where we gave users mobile
technology to give them ideas for
interventions that would relieve their
stress or at least remind them how
they're supposed to behave in a
stressful situation in this particular
one we were looking at the general
population in a second study that I'm
not going to talk about today we looked
at parents of ADHD children and we were
monitoring the stress levels using
wearable bracelets and we were able to
alert them to win a stressful event is
about to happen and remind them of how
they were supposed to behave for their
child both of these mobile applications
were very very successful and I'll just
talk about this one so the good idea
Pablo had for this one was he had this
idea of can you take psychology the real
kinds of psychology interventions and
I'll just give you one for example a
mindfulness technique is remind yourself
of three things you're thankful for
today right so the interns idea was can
we take those psychology micro
interventions and actually infuse them
with social media so what are people
going to do on their phones anyway well
one of the first things are going to do
is go to Facebook well we can remind you
to go to facebook and look for three
things you're thankful for on your
timeline so that was a way he mashed up
the psychological interventions with
social media and he was hoping that that
would be more fun and then his second
hypothesis was we asked ourselves could
we use machine learning to actually
personalized interventions for ones that
work for you because we knew it would
not be one size fits all I don't like
playing games but maybe you know maybe
one of you does and so I would probably
tell the system I didn't really enjoy
that and I would never choose that and
the system would start to learn over
time these really finessed decisions and
also what my current context is maybe I
love to play games but I'm
here in the lecture room I can't really
play a loud game right now I have to do
something else so then our question was
if we can do this can we generate
long-term behavioral change and so this
was a very complicated study there's
tons of data I could show you evidence
that we actually did help people over
the course of four weeks not only learn
more positive coping strategies but also
lower their depression the reported
depression which was just fantastic and
I pray this video works so this is Pablo
this is really how stressed-out Pablo is
all the time and by the way he just had
twins so do you even more stressed ugh
he's trying to hook up his this video
so you used as a penny calms down
email made them stressed up
it's going to use his app and he's going
to calm down and I'm just going to roll
this to one more incident because it's
just funny and then I'll stop in turn it
over
I using what happened what nothing's
working now what show me the file what
this yeah how do you think here yeah
come on there is a mirror there well you
told me to do that you know no you fix
it oh dude and so of course he's gonna
go use the app and actually here here
you could kind of see a little bit more
the app but for timing purposes I'm out
of time I'm just gonna go ahead and stop
it there so what have we learned we
learned we can stress users out many of
our designs did stress users out by
showing them their stress levels and
that wasn't necessarily a good thing
especially because users aren't used to
thinking about their emotional state
especially certain cultures or certain
genders I won't say which way get really
upset when you tell them they're
stressed out and they don't think they
are and especially when you tell my
machine has figured out that they're
stressed out and they don't think they
are that's even worse so these designs
have to be very sensitive to that fact
some users dislike any external actuated
awareness given out to the room about
their own internal stress state as you
can imagine other users interestingly
enough really liked it so another
project we did we did crystals and we
put them out of our out in on our admins
desks up on the fourth floor in building
99 and you could walk into that admins
office and see how stressed out they
were and we were wondering if people
would change their behavior based on
that or what not it had a cloaking
mechanism and so what happened was
admins were okay when the other admins
came in and saw what was going on but
when a manager came in they immediately
would cloak the device so some people
like it when they can share and talk
about it with each other some people
really don't like it so we can use
technology to do just in time
interventions that actually do help
people and there are populations out
there that really need it we're
continuing to finesse our designs to
make them more sticky and more fun and
we really think this is especially
encouraging for doctors patients and
parents so thank you very much I'm now
going to turn it over to Aaron
sit on yeah okay hello everyone I'm
Aaron solavei I'm currently an assistant
professor at Drexel University where I
am a professor in computer science and I
just started a HCI lab there in this
past year so my research in general is
on emerging human-computer interaction
techniques and today in this talk I'm
going to talk about brain and body
sensing and I hope you'll have an
understanding of how this is becoming a
realistic tool for human-computer
interaction and how we can do it even
outside of the lab with lots of
different types of people okay and so
I've been studying the next generation
human-computer interaction like brain
computer interfaces and physiological
computing and the reason I think this is
important is because computing power has
gotten in Creek keeps getting better and
better and especially over the last 50
years meanwhile our own perceptual
capabilities have not changed that
dramatically and so it's really becoming
a bottleneck when you have a human
computer system when the person went our
own when we can't perceive all the data
that's coming at us so we have this
increased computing capability which can
sometimes lead to increase demands on
people because you have information
overload your multitasking and then on
the other end of the spectrum if you're
working with a really intelligent system
you might start just your role changes
into from directly controlling things to
more supervising this autonomous system
and that can also lead to low workload
which can be a problem too but then at
the same time we have all this improved
sensing so we saw a slide earlier with
all these different sensors that you can
put on your brain on your body I've been
doing mostly brain and body sensing but
now we have sensors in our homes in the
environment and a lot of this applies to
all of those sensors as well because
really what we're trying to do is expand
the bandwidth between the human and the
computer by identifying the sig
that were naturally an effort
effortlessly giving off as we do our
tasks our main tasks that we're trying
to do and then we can use this
information to have this the technology
adapt appropriately to help us with our
task this has a lot of applications in
medicine education driving really
anything that involves multiple level of
different levels of workload and
multitasking so up here we can see some
of the sensors that i've been using
recently so their skin conductance
electrocardiogram which is heart rate
and then the picture the headband is f
nears brain sensing or functional
near-infrared spectroscopy and all of
these are practical for real-world
settings because they're really quick to
set up their comfortable safe portable
and you can do your normal tasks you can
be sitting on a computer you can be
sitting in the office and you'll see
later you can be in other locations as
well and this really creates
opportunities for human-computer
interaction research so with all of this
non-invasive brain and body sensing if
we can measured in real time then we can
use this to capture these subtle changes
in your cognitive state as you're doing
tasks and we can do this in real time
and use this as a continuous input
stream to the system and then the system
can be more in sync with you and provide
help and support as needed so the
approach that I mostly take in the
systems that I build is to use it to
augment the other traditional input
devices you might be using so not use
your brain as the only input so system
but to as an additional signal and then
this can lead to adaptive context-aware
systems so some of the examples I put up
there are you know if you are working
with autonomous system the autonomy
level of it could adapt based on how
much you can handle at a given time
based on your cognitive load you could
modify how much information you show
someone at a given time based on their
capacity at the time you can transform
the modality of how you present
information or allocate tasks between
different team members and things like
that so that's the general
view that I have okay so I'm now going
to make this more concrete by talking
talking about some recent work that I've
been doing in collaboration with the MIT
age lab looking at driver workload so
has a video okay so why do we care about
driver workload driving is as we know
it's a dynamic task that you're doing it
involves visual cognitive and manual
tasks so by that I mean that you're
you're doing things like figuring out
the route that you're going to take
figure out a detour if there's traffic
and these are high-level strategic goals
that you have in your mind at the same
time you're monitoring the road you're
monitoring the vehicle that you're
driving and this is information
processing and then there's the physical
part of driving actually steering the
wheel and accelerating so we also all
know that they're it safety is a major
concern when we're talking about driving
so in 2011 there were some statistics
released where there were over 300 3000
people killed and three hundred eighty
seven thousand people injured in the US
and crashes that involve distracted
driving and if you're driving and you
just look around you can see it you
might have noticed that people are doing
more and more things while they were
driving and so there's secondary tasks
going on when they're driving whether
it's attending to their mobile devices
tending to their GPS system or and other
things too and plus there's just all
this new technology being built into
cars and so this creates this distracted
driving scenario so understanding the
workload that someone's experiencing
from these secondary tasks while they're
driving can help us prevent accidents
and hazards on the road and then the
other side of it which I'll talk about
is that driving is changing just the
experience of driving is going through
rapid changes right now as well and so
as so I think this is important for
human-computer interaction researchers
and computer scientists to be thinking
about because we're designing and
building these systems that were adding
into the car we have constant changes
where
making new types of user interfaces in
the vehicle we're adding GPU complex GPS
systems and infotainment and even
browsers that are going into the car and
then as I mentioned before people are
also bringing their own technology into
the car which causes distractions and
and then the other advancement is that
all the cars are becoming more
autonomous so there's cars that are
driverless or just semi-autonomous and
all of this can chain is changing the
role of the driver in the vehicle from
directly controlling it to moving to a
more supervisory role and so we need to
be able to evaluate all these
technologies so the approach that I've
been taking and looking at is doing this
passive automatic cognitive workload
detection while people are driving on
the road using body sensing and also
some brain sensing and as well as all
the metrics from the car so their
sensors in your car that you can use and
so then if we have this continuous
measure of workload we can use it to
evaluate the new technologies being
introduced also the autonomous behavior
that the car might have and we can even
use it as input into the intelligent
vehicle that might adapt its behavior to
be more supportive of your state so I'm
going to talk now about two experiments
that I did moving in this direction so
in both of these experiments the goal
was to collect data from people's bodies
and see if we could use it to classify
the workload that they're experiencing
so the first one I'm going to talk about
we were trying to build individual
models for each person so we have in
both of the studies people are actually
driving on the highway and then we gave
them a secondary work secondary task
that's supposed to induce cognitive
workload and that task is just a proxy
for other secondary tasks they might be
doing in the first study we have 20
subjects and then the second one we have
99 subjects and I'll talk about the
differences between them in a minute so
in these studies I'm that I'm talking
about now I'm really just looking at
body sensing so skin conductance an EKG
and then car metrics which are pretty
simple just driving speed
the steering wheel position and the
acceleration data so like I said we were
doing the study on the road so this is a
map of how the experiment went so we had
people start off this is in Cambridge we
gave them some instructions we talked
about the experiment and then we would
get them set up in the car with all the
sensors give them safety training
explain how the anything that they need
to know about the car and then they
would drive for 20 minutes and that
wasn't part of that's not the data we
were analyzing that was just them
getting accustomed to the car and then
we would collect data as they drove on
the highway going south we'd have them
turn around drive north collect more
data and come back and then we do some
other questionnaires and debriefing and
things like that so yes so while they
were doing this driving we gave them a
secondary task to do and it's called a
delayed digit recall task if you're
familiar with n back it's essentially
the same idea so they heard a series of
numbers and they had to respond with the
number that was to previous to the one
that they that they heard um let's see
so I'm going to play it and what's
actually playing is shown up here and so
you can kind of follow along to
understand the tasks that they were
doing because it will help it will help
understand the rest of this next okay so
you hate ate and you just wait and then
77 now when you for or your correct
response is 85 now seven is the correct
row 24 would be the correct response 351
296 109 so that's the correct response
now you can try it there's gonna be
another one without the answers in front
of you next to try it 73
64 you can say anything 05 8192 and this
over there there's more of them but so
if you actually did it you should see
that it it's kind of difficult and we do
some training first to get them up to
speed on it but it does induce cognitive
workload and keep in mind they're also
driving while they're doing this so so
but this this task has been used in a
lot of other studies and so it's a it's
kind of a validated task that we can
that we understand and we just used it
as a calibration tasked with this kind
of known consistent validated amount of
workload and then we can use this in the
future for user interface evaluation so
if you have a calibrated task and then
we give people and we measure their
sensor data when they're doing this and
then we have them do other tasks in the
car we can compare it to does this look
more like when you were just driving or
does it look more like your sensor data
when you were doing this elevated
workload task so in the first experiment
like I said we've had people driving and
doing this task and we had for each of
the participants they would do that task
for 30 seconds and then that would be
followed by 90 seconds of a recovery and
baseline and then they do another 30
seconds of the end back or the two back
and then 90 seconds of recovery this was
really just to gather data that's
labeled so we can look at it later and
so we did it 24 times and then this
became our training data for a
classifier so we had normal workload
which was extracted from the middle of
the recovery period and then we had
elevated workload which was during the
to back task okay so then we collect all
the sensor data and we have to do
something with it and really we were
doing pretty simple things with this we
have all this data it's a time series
you have to break it into windows so we
slide a window across and for each
window we do some calculations so we
take the average the standard deviation
and the the minimum the maximum and the
first derivative of heart rate skin
conductance level velocity and then for
that window we also take the number of
small and large steering wheel reversals
so if you do have really large reversal
than that actually indicates that you
have to correct for something so you
kind of maybe were distracted and not
paying attention so people use this and
so so but there's still a lot of
parameters you have to figure out like
how big of a window we need to use and
how much overlap these windows can have
and so we were collecting this data to
experiment with that so once we had the
data we did some machine learning
classification we use nested
cross-validation to choose the
parameters and then clot and then do
classification and these are the average
accuracies that we're getting and like I
said in this experiment we're just doing
pretty simple things so we were getting
between 69 and seventy-five percent
accuracy when we used all of the
features and it varied depending on the
algorithm and some other things and then
even just using heart rate we were
getting almost the same accuracy and it
was reasonable we were doing pretty
simple things so it just showed that
there's some promise but like I said at
the beginning we had people do 24 trials
of this so we had about 48 minutes of
data per person and we trained on like
43 or 44 minutes of that and then we
classified on the last block and so this
is okay for a proof of concept but it's
not really ideal for the real world
because you'd have to have someone come
into the car do these tasks for 45
minutes before you even have a
classifier for them so in the future
we'd like to have improved methods to
shorten this whether it's better
classification algorithms better signal
processing and then also if we could
classify across people instead of
building individual models then we could
reduce or eliminate the training time
and that's really what I was doing an
experiment to
so experiments he was very similar setup
but this one had 99 participants and it
was balanced between three age groups in
their 20s 40s and 60s and their each of
those age groups are also balanced
between genders so we had a kind of
diverse and very large data set and here
we didn't only do two back which is the
one you guys were doing but we also did
a one back which is a little bit easier
and that you only have to remember one
previous to the one that you're hearing
and then zero back where you repeat the
number you here so this could lead to
more general classifiers without this
individual training okay so from this
experiment what we have here is each
across the bottom is the time for the
entire experiment and each row is a
different participant in the study so
they're almost 100 rows and we have on
the top heart rate and on the bottom
skin conductance and so you should see
that you can almost just from this data
see what the experimental design was
because you can see where we started and
then there was an elevated workload
period followed by the recovery followed
by workload and so and it's more clear
as you can see what the heart rate it
does recover in the rest period whereas
the skin conductance it doesn't always
recover in between so this was just kind
of exploring the data and seeing if this
is worth wild and it pretty much showed
us that we should be able to classify
the state build a classifier for this
data so then we did we tried to build
classifiers and so we did very similar
analysis we did 10 but here we did
10-fold cross-validation to evaluate all
the different approaches we could take
with the data but we didn't choose the
parameters using nested cross-validation
like we did before instead we reported
all the results of all the different
choices we can make just because we were
trying to see what the best way to
actually do this was so we had five
different classifiers we had different
window sizes so that's what the small
things were so there is 10 15 20 25 and
30 second windows and then some of them
you could either overlap them not at all
or by 25 sec
and sore different amounts and what we
found was that first of all we're
getting pretty good accuracy and the
highest was up into the 90s the low 90s
and then there was a trade-off between
the window size and accuracy which has
been shown before to be true but the
overlap didn't matter that much so now
looking at more results we had a lot of
results in soon pulling some of them out
we also tried just putting into the
classifier only subsets of the features
so if we just looked at the driving
features alone and this is the steering
wheel and the acceleration our accuracy
was about sixty percent and then if you
just look at heart rate it jumped up to
eighty percent accuracy and then when
you add all the physiological measures
but not the driving ones who jumped even
higher close to 90 and then the last one
is all just everything and so the future
combinations definitely had an effect on
the results so as I said earlier the
goal of my work was to detect these
signals that the user naturally gives
off and improve the interaction with the
technology and so this these studies
were really a step towards looking at
the feasibility and practical
considerations of doing this in the real
world so not in a lab but while
someone's actually on the highway and
use a large group of people so not just
college students but people of all ages
and and so then we can do this field
study record the body and body sensor
and task data and classify the cognitive
workload and we hope from this because
it's you know it's a much larger data
set than most of the other similar
studies and so we hope this is a
foundation for future applications so we
could use this to build to evaluate in
vehicle user interfaces evaluate the
semi-autonomous behavior in cars and
really just make the vehicle more
supportive of the drivers changing state
and then on top of this we could apply
this to classifying workload in other
contexts so we could use
approaches for game user experience or
using it for passive adaptive user
interfaces and really there we happen to
be doing in a car but this could be used
anywhere so it really has broader
applications for using this in the wild
beyond driver interfaces so some future
stuff that we're doing and so first oh
yeah so first of all it's more likely
that we could use other metrics from
driving we were using very simple ones
so other driving measures other sensors
I'm doing some stuff with rain sensing
now and other classification approaches
we can look at more granular recognition
of workload levels so I said that we
collected data looking at 0 1 and 2 back
so if we could classify all of those
that would give us you know low workload
medium workload and high and maybe even
more granular than that and then move to
more realistic tasks so I don't actually
imagine anyone is doing that to back
task while they're driving but if we
look at things like adjusting the radio
or GPS or or using those calibrated
tasks as the training data and then
classifying the more realistic tasks and
that's what i mean by cross task
classification and then ultimately we
want this to just be ready to go so you
get in the car and you already have a
classifier built for that person and it
will just work and then finally we were
using kind of medical-grade EKG sensors
and stuff like that but some of these
things can be integrated into the car so
you could get heart rate from the
steering's holding onto the steering
wheel or the seat and so that's some of
the stuff that were looking at and with
that I will end and pass it over to Andy
thanks Aaron so my I'm going to be
talking about actually a similar kind
set of experiments except mine probably
don't kill anybody if they you know if
you can't do like three or four back I
don't know I don't know if I could drive
with the two back even that would be
dangerous but mine's about code so how
many people remember how to program you
can raise your hand so I'm showing two
sets of code on the screen which are
pretty similar to each other how many if
you look at the pieces of code one of
them is harder for people to understand
so this code draws a set of shapes how
many people think the one on the left is
more difficult for you to understand you
can just raise your hand and how many
people think the one on the right is
more difficult to understand so a lot
more people think the one on the right
is difficult to understand would anyone
like to venture a guess as to what is
the reason for that and I'll repeat that
for the visitors at home have naming
conventions abstraction so naming is
actually a very key thing here so in the
one on the Left I have a circle name C
and a triangle named tea and a square
named s which is nice and mnemonic so
what we did is we had participants in
our study take a look at each of these
examples and we watch them with an eye
tracker so it's basically taking a look
at where they're looking on the screen
and what we found is that the example on
the right uses obfuscating variable
names so object de object k is the
circle by the time the subject gets down
to this draw command and says you know
graphics draw object X they have
completely forgotten what object X is
and in the eye tracking data you can
watch their eyes go back up to the
definition of object X oh that's a
square ok that draws the square they get
to object day they go back up to the
line where it says that's a circle so
when they do the the Monica one on the
left it turns out
can just remember and they're using some
other part of the brain to make the
Association and they don't actually have
to look anywhere else so it turns out
the more difficult one to understand is
the one not only is it more difficult to
remember we're kind of pushing on your
short-term memory and filling that up
it's also taking longer because it
physically takes longer for your eyes to
go back and forth then it is for your
eyes to just read the each line once so
basically we're kind of looking into the
origins of this question of like why are
some programs harder to understand than
others why are they harder to build and
lots of areas really look into this
obviously this is all about CS education
you're teaching your students had a
program you definitely want another
answer lots of studies have been done in
the 80s on the empirical studies of
programmers workshops to understand sort
of what do students think when they're
looking at a piece of code they get them
to think out loud and tell you what
they're trying to understand and there's
even conferences on program
comprehension try to see like what
happens when you give people large
programs or whole programs like for your
car breaks or something or an airplane
like if somebody were to try to figure
out what's going on inside there what
are they doing and how can we help them
so because people find some code harder
to understand none of those you imagine
that if you're working on software code
with other people you might not
understand what they're doing it and the
code might have a bug in it so maybe you
want to test it just verify whether the
programmer really got it right so we've
got lots of automated testing solutions
you look over the code over their
shoulder and say like oh I think you
screwed up and you forgot your semicolon
or maybe I don't really like the fact
that you've named all your variables
object X object a and object B perhaps
we could have a better naming convention
in our software so that other people
could understand it and there's a whole
conference called mining software
repositories that basically looked dives
deep into sourceforge and to github and
we do this inside Microsoft basically
looking over all of the source code
looking at all the check-ins all the
bugs and trying to figure out our their
correlations between the kind of
activities that are happening in the
code when people when programmers are
writing the code versus what happens to
the bug reports that we
you at Microsoft and we say well where
was the bug fixed let's correlate that
with the bugs that come in and say
anytime you're editing this function in
this file over here in Windows it tends
to cause a bug so maybe we should go and
check that one twice before we ship that
now what we want to do is actually go
back a little bit further because it's
nice to know a correlation that helps
you figure out what where you should be
testing but what if we could actually
stop the developers from causing bugs in
the first place if we could spot what
they're actually doing because they're
the ones actually write the code so
let's go backwards in time to when
there's a programmer at the desk and
let's figure out is there something
going on in their cognitive inside their
cognition maybe they're giving off some
signals with their body that affects
whether they think something is hard or
not that maybe when it's hard maybe
that's not the time that they should be
typing maybe we could stop them in some
way and we've seen this in short term if
you watch a programmer work for a few
minutes you'll actually see them get
confused about what they're doing like
very quickly and it could be just things
like navigation they went go to the
definition go to the definition go to
the definition and that third good of
definition they hop three times they
forgot how to get back and we see them
like just completely pop the navigation
stack that go back to what they did at
the beginning of the day and then they
walk forwards in time because they just
can't think backwards that well and so
what we're trying to figure out as well
is these transient states where you're
confused or maybe you're stuck or maybe
you're frustrated does that have any
long-term effects on the code or is that
just something that affects you in the
short term so what we'd like to do is if
we do detect that lets say a programmer
is in flow state so these are things you
can do with the sensors maybe that's not
the time to interrupt them so maybe you
close the door automatically or you like
put up a little flashier that says don't
come into my office because I'm actually
making progress if lots of people keep
thinking that a particular piece of your
code is really hard to understand maybe
that's the place to refactor the code so
it's a kind of a different metric for
refactoring maybe it's sort of a
cognitive complexity and then
my personal goal would be really awesome
as if we could make a classifier that
works in real time like Aaron's doing
for driving we can detect that you're
getting stuck that you're frustrated
maybe stop you at the point right before
you're about to type in something bad
into the into the code so we did an
experiment and actually this was a joint
experiment with Thomas firts who's
sitting on the right side of the room
over there next to el murfi he's going
to wave his hand in a second he's from
university of zurich so what we did is
we invited in 15 professional software
developers who understood how to do
c-sharp programming and we're local so
they could come in we ended up getting
14 males and one female and they were
professional so they actually do this
for a living and we give them eight
different programming tasks of two
different types little shape drawing
tasks and we we basically attached a
bunch of different sensors to them and
we have them do these tasks / / about an
hour and a half each and rate how hard
the task was for them and at the end
kind of rank all the tests just to see
if we could triangulate whether or not
they were measuring difficulty the same
way all the time so the two kinds of
tests or the first one was an overlap
task we basically had to a couple
functions that would essentially create
two rectangles and then you know draw
them onto the screen so we asked do
these two rectangles overlap and all
your get to see his numbers you don't
get to write anything down you have to
do it in your head pushes on your
spatial relations a little bit and then
we had a whole bunch of tests over about
drawing order so if we create a whole
bunch of shapes and maybe we stuff them
into an array and we loop over the array
and what order are all those shapes
drawn and we played with a lot of things
like the variable names we messed with
those we change the loops around so
instead of for I equals 0 i less than
three we went like I equals 1 I less
than 4 which is a little weird for
people or i equals 0 I less than or
equal to 3 it's a little weird so
there's a lot of unaccounted on expected
parts of the program they slowed down a
bit and they get confused we had like
nested ? colon operators the AFL's
operators we moved field assignments
around so we kind of took a nicely
organized program
when we jumbled all the lines together
so it's still technically correct but
really hard to read we also i also had
one of them i put a bug in it on purpose
but it's something we learned from the
empirical studies programmers if you
make the code look like it's supposed to
be right so we had an array and we had a
swap function with like a temp variable
and it looked like a swap function it
was called swap but I put a bug in it
nobody found the bug nobody spotted the
fact that there was a bug and they all
just went blindly past it and you could
sell with the eye tracking David nobody
actually really read it which was
totally awesome because I mean it sort
of confirmed all these previous results
and the fact that that could cause 15
people to all get at a scrum which was
really neat so anyway so we're pushing
on a lot of different cognitive
abilities try to make people feel
something that something is difficult
and let me tell you show you what one of
the other types of tasks was so this is
the overlapping task in the beginning of
this function it creates a rectangle and
then starts creating points for the top
left top right bottom left bottom right
both of the two rectangles are created
in the same order of points the first
ones first on top and the second ones on
the bottom and then we had kind of
another question which was similar and
we counterbalance the order here where
we just mixed up all the statements and
it's just a bit harder it turns out yet
it's the same thing you know it's the
same kind of question so to a computer
it doesn't care but a person really does
and that's makes people stressed out
which is awesome so you know we're
trying to push on this laboratory design
so it's not like we're having them drive
while they program but
so what we do is we have these 15 people
each seed assassin in different orders
and we attach different sensors to them
so we use something called the neuro sky
mind band which is a really cheap EEG
sensor that basically just attaches to
your head and measures one easy signal
at the left prefrontal cortex on your
forehead and we're pulling out different
ways like if you've ever you know watch
the medical TV show where they talk
about the alpha and beta waves and the
theta waves of your brain it also turns
out when you put a sensor on the
forehead it's really awesome at
detecting when you blink your eyes
because your muscles are a lot make a
lot more strong electrical signal than
your brain does because there's no bone
in the way so turns out when you're
measuring electrical signals on the
forehead and you blink it makes a huge
massive low frequency response that you
can spot so that's kind of neat and the
prefrontal cortex area that it's
measuring is supposed to be able to
detect when some of these actually pay
attention or when they're just kind of
zoning out and calm those are variants
of this mind band that has little cat
yours and and you can wear it and when
you're looking at people that yours kind
of go up or down like anyway so we also
use the Q efectiva sensor which is
measuring electrodermal activity which
is from the pseudo mother neuron that
couldn't rent in your body basically
it's measuring sweat the more you're
sweating the higher the signal goes and
it turns out there's two kinds of
signals in there one is this low
frequency signal that kind of measures
generally arousal like are you paying
attention or are you kind of like calm
maybe you just ate lunch and you're not
really paying attention then there's a
high frequency signal that it flat if I
were to come to you or Thomas which I
did and like smacking the back of the
head when you weren't expecting it then
you would see a little blip and we
didn't see any blips actually in ours
for programming we didn't smack anybody
then we also use an eye tracker on this
is an eye tracker that's about forty
thousand dollars of hardware and like
$20,000 of software there's a new one
I'm waiting for the summer from the same
company that's a hundred dollars that
just clips to the bottom of monitor
which is kind of cool so price
differential two years from 60,000 down
to
so that's pretty nice everybody can use
this you can tell where the person is
looking which is nice and how they move
around the screen and it also tells you
the size of their pupils and the pupil
size is pretty nice because it turns out
when you're reading something difficult
your pupils pop open a little tiny bit
like about point 1 2.5 millimeters
bigger than they were so it turns out in
our pilots we were having them look at a
function that said return x x 13 plus y
and every single person people popped
right open so i made a math detector
which maybe i didn't need a sixty
thousand dollar i track afford but we
used it for other parts too so we asked
them to rate the difficulty these tasks
on a bunch of different scales to kind
of triangulate it I want to show you
what it looks like so you can start to
see the developer working ok so the main
thing is you're going with the source
the red balls where they're looking at
the moment so parentheses attempt to
greater than or equal to or greater than
17 time to 11 that falls so that would
be a square then 17 greater than equal
temp 117 is not greater than or equal to
21 so that's false so we have them do
think aloud thank goodness because their
eyes move a lot faster than them around
us and they look at a lot of things that
we have no idea what exactly they're
thinking if they weren't telling us
that's actually a big major research
challenge but neither Sharif who's also
in the room also on the right side of
the room she was participating
experiment with me about looking trying
to understand what programmers been
thinking when they're going through the
code with when you look at their eye
tracking data so what we do something
similar to what Aaron was doing
basically record lots of data while
they're working spend a huge amount of
time cleaning the data because it's
humans produce really disgusting looking
data add that's what I had to go back
and learn my signal processing back from
college and then we extract a whole
bunch of features that we might think
are relevant and then put it through
some classifiers I had to learn machine
learning to do this and some
statistics but what we found first thing
was nice you don't need to read all the
details here but basically we're looking
at measures of difficulty when they did
the a tasks we looked at how long they
spent on it we we looked at how they
measured how or how they rated kind of
how difficult the problem was and how
they ranked it at the end after they had
seen all the problems and they're all
really really highly correlated so we
definitely believe in their measures of
how they said things were difficult so
we built three machine learning
predictors first thing was by
participants so what this means is if I
took like all of you in the room except
for let's say eight on over there and I
had you do my experiments and I trained
on everything could I predict what a Tom
would think is the difficulty of all the
tasks and the answer was not so much so
everybody's very different or at least
at the centrist that we were using
there's a bunch of bars here for
precision and recall but and for
different combinations of the sensors
either the just the eye tracker the
electrodermal activity or and the EEG or
different combinations none of them
really got much higher than around
sixty-five percent precision and and
pretty terrible recall there and we also
did a sliding window thing which
basically just chopped up the data for
the like previous five seconds all the
way back to the previous 60 seconds and
kind of like you what you do is you were
doing this stuff in real time and again
precision on the top recall on the ball
and that's still pretty crappy so then
we decided okay let's split the other
way if i watch everybody do all the
tasks except for one and then i have you
all come in can I predict how you're
getting you on that last task so are the
task similar enough that I can predict
what the task is going to be like and
that actually does a lot better even
though these tasks are actually pretty
different from one another our precision
rates jump a lot more which is nice and
our sliding windows kind of for the most
part the sliding window size really
doesn't matter it turns out it's pretty
flat then the last thing we did was just
to see it's a well you know last first
you know the order is sort of you know
Post hoc when you write your paper but
the basic thing here was if i watch all
of you do all of the tasks except for
eight on doing the last task could I
predict what a town is going to do or
how he's going to write it and that's
actually you know the most boring looks
if I can't do that nothing's going to
work right and you have no product so it
turns out if you look at all the data in
aggregate of just like the entire task
which is like to the stream and that's
basically for each one we do we still do
actually pretty bad but it turns out if
you start splitting everything apart
into the time windows we actually do
awesome which is what we were hoping for
it's the most useless number just
because this is the one we should have
done really well on which is good that
we did but now we realize the sliding
windows actually are super important you
can't just well in real life obviously
you don't know how long a task was going
to be but in our cases we could do that
and just check between the two now it
turns out the eye tracker kind of gave
the best data and this is really the
pupil size and changes in the pupil size
over time but if you combine it with the
electric dermal activity measure that we
wore on the wrist that gives you a
slightly better set of numbers so I just
wanted to end here to just sort of say
like we are continuing on our work
trying to basically kind of look at
different aspects of the brain that we
can kind of poke at to make it problem
see more difficult or make them seem
like they're taking a lot longer time or
problems that get you frustrated i have
a new device a new keyboard that I have
which is a pressure-sensitive keyboard
so like if you getting really frustrated
probably the bang and the keyboard so I
can tell and we have the microsoft touch
mouse gets back like a little bit map of
your of how much you're touching and
Mary's had an intern who was in one of
those videos ha ba who did a study last
summer who figured out that when people
are more stressed they put more of their
hand on the mouse which is kind of cool
otherwise they'd you know like this
fingertips versus full like palm and you
could tell so it's kind of you can use
these additional sensors to be able to
tell not just comprehension but also now
composition which is kind of cool and we
got a better GSR sensor or a better
electrothermal activity sensor that you
wearing your foot and it turns out you
move your feet less than your hands when
you're typing which is good because when
you move apparently that sensor doesn't
do so well so there's a lot of like
these sort of like let's say educational
lessons that we found out by using
sensors and I'm sure Erin and Mary both
learned a lot of these lessons as well
of the fragility of the sensor values
that are coming out of it the noise that
comes out of it as well as the bizarre
very bizarre sensor values that come out
of humans and very entertaining actually
we got that first the EDA one is
measuring sweat you know it turns out
some people don't sweat so we had to
make them jumping jacks and when the
jumping jacks to the work I made one of
them run down four flights of stairs and
back up then it worked and I apologize
alright so at this point we're going to
move to a discussion thing if you have
questions please wait for the mic there
should be one person in the back who is
running around with a microphone so that
way people outside can hear and we'll go
with that until we finish at four-thirty
we also have some other questions if
everything dies down we have some
talking points we can go with so thanks
very much so maybe you guys can stand up
with me and keep your make sure your mic
is on or you could sit okay either one
I'll sit you that way I don't look
different so my question I think is
basically for for Mary and since I was
trying to remember my question during
the other two speakers I've changed it
several times and so whether or not I've
been proved it or not is another matter
so is there a work being done between
understanding emotional states of an
individual and environmental controls
let's say in the workplace and so I'm
thinking particularly of light and maybe
also audio signals and noise noise being
good or bad so because well we can we
can control
lot of things with light and so anyhow
that's my question that's great question
we have not done that we have definitely
looked at whether as predictive of
emotional state we've been able to do
some correlations there you can
definitely see that weather has its
effect we're actually doing a project
right now looking at using light to
improve mood so I think you can do that
in the workplace noise is a really
interesting one because I don't know you
could be hard to make people wear
headphones or soundproof offices better
or whatnot that's pretty expensive but
we could certainly look at those so um
um it I was really looking at this and
the point of view of the individual so
that the lighting in the room would
actually adjust to them yeah the mood
changed desired for an individual all
right that's a great idea and in the
case of noise or sound sound is a more
neutral term um you know there are all
these sort of products you can buy to
sleep or to stimulate your brain
activity and are all about sound so it
wouldn't have to you wouldn't have to
wear it advice but it would have to be a
way of generating what it is you need
yeah I don't have will use music because
a lot of people want to use music to
help them rotate or whatever so we're
definitely going to offer that good good
points so there's only one person with a
microphone so keep your hand raised no
we have people in the web who are
listening so they can't hear you if you
speak lobby MC Cheryl just a quickie
point of information to the last comment
there's an awful lot of work in
well-being in the space around light and
noise a lot coming out of Denmark we're
doing some work with the National
Foundation for workplace well-being in
Denmark and a project called refresh
inspired by a lot of the stuff that
Mary's already been talking about
looking at very much that property for
workplace and what happens when you're
dealing with more than one person at a
time so they the Danes anything to do
with the workplace environment the Danes
have been there they're the ones who say
you got to have a window in your office
for it to be a legal office too
so there's a lot in that space just in
case you're curious let's come over here
in bartram from Simon Fraser so we've
heard a lot about detecting effect but
it's largely in the arousal access so
let me ask you how you measure effect in
the valence access so you can use lots
of things you can use smiles muscles in
the face you can use in activity on the
phone you can use all kinds of things
about what the user is doing whether
they've had face-to-face or social media
contact can have an influence on valence
so there are lots and lots of sensors we
can use to get at all four quadrants and
I think we can do that pretty well now
so these are these are sort of activity
related though as opposed to direct
reporting or direct detecting no you can
use the eyes the squinting us of the
eyes the lips okay the corrugator
muscles of the face face detection like
a motion detection in the face there's
actually a couple of commercial products
that will do that it's kind of
off-the-shelf now these algorithms
winter Mason Facebook hi Mary so I have
questions about two of your projects one
is the physician one with the lotus
flower is one of the features that you
use a mirror ring between the physician
and the patient because you know there's
a lot of evidence that that shows
rapport and light increases liking and
that sort of that's one of our it's
called mimicry but that's one of our
students yeah yeah yeah just way we're
both shaking our heads right now yeah
and very similar and I think you may be
touched on this with the butterfly one
but to what extent is it better to sort
of go the opposite direction so it can t
mirroring with it with a with a device
or with the the thing rather than like
you know synchrony yeah yeah so more
making making device more of a bio
feedback mechanism and trying to get
trying to use the movement of the device
so the coloring of the device to
actually help you do the opposite of
your current state is how we think you
need to design these things now so good
points
first andreas and then hi this is andrea
stella from salon university at this
actually question for three of you to
which extent does the lab setting or the
fact that your participants know that
their emotions are being recorded
actually affect the results for instance
Andy in your setting when people are
debugging and they actually verbalizing
their thought this already helps them a
lot in their thinking less today
yesterday we had Leslie imports telling
us in order to solve a problem first you
must think and in order to think you
must put it into words and the very
action simply of me thinking aloud
already helps me during debugging it's
not it's not always so it's not always
social me thinking aloud but this is
something that helps or the physician
for instance who knows was being
monitored will certainly not go into an
emotional break out at the very moment
he's talking to it very moment he's
talking to his patient because he know
because he knows he's being monitored so
I'd like to know from the three of you
how do you control for such how do you
control for such a possible
interferences no I'll just say a short
one and then we can ask Aaron as well
one of the things that we noticed was
when participants were doing our tasks
their electric dermal activity sensor
was basically just showing a constant
increase over time for the first few
participants and we were kind of
confused at that it turns out they were
getting more and more nervous as they
SAT there and we're not sure whether
they got the answers right so we
undertook to interventions the first
thing was we told them their answer was
right all the time pretty much just to
make them feel better even if they got
it wrong and the second thing that we
did is in between every task we have
them watch a very calming video from
YouTube of some fish in a fish tank we
also learn to tell them not to count the
fish or follow the fish with their eyes
or that we're not testing them on the
fish that they're just supposed to come
down while watching the fish and for
most of the participants we saw a big
drop in their electrothermal activity
back to baseline after about a minute
and then we had them watch of just for
it another minute just to kind of keep
it calm but it worked pretty well
actually for most of them but we
definitely know this yeah there's
definitely this Hawthorne effect yeah i
mean i'd agree and I think it's not even
particular to having sensors I mean any
study that you're doing that people know
they're in the study and so this is a
problem in general for some things I you
know when you're having your eye
tracking happen think you're still even
though you know that it's happening a
lot of these things just happen
automatically and when we're talking
about brain sensing it's actually pretty
hard to control you but you can do some
things but people we find that after
they are set up with the sensors they
might be a little stressed at first but
then they get used to it and almost
forget about it and especially the task
that they're doing are hard enough that
the last thing they're thinking about is
other way the sensors they're just
trying to complete the tasks but I mean
it is a problem but if they're wearing
them for your entire experiment then
that's kind of a constant and then you
can still compare the different
conditions Mary do you have anything to
add oh yeah the same we use calming
techniques and we use stressing
techniques and we make sure that you
have phases where you calm stress calms
dress and of course you calm before they
go home because we can't get you really
stressed out and it's not a good thing
to have you leave the lab after that we
have another question in front of my
dress actually he asked my question but
I have a different one um I'm curious
about these different kinds of
physiological signals that are coming I
mean they're different Layton sees for
these these things and Andrew I think
you said that the galvanic skin response
was probably less accurate than then he
then certainly the eye tracker I mean
what's your all of your experience if
you only had to buy one of these like
and you really were interested in
cognitive load that physical load which
is the TLX sort of tends toward so it
was good to at least see correlation
there I mean would you have a
recommendation if I had to invest
my limited money you probably have the
same one I would do heart rate heart
rate monitor I don't know what I mean I
think it depends on my I think do you
have something I'm off the Crosby
University of Hawaii and I've actually
been doing this since the 80s and one
thing we've found and we've been getting
very high accuracy by using multiple
sensors and it you know it there's such
individual differences that something's
always happening in the body and we
found it takes three sensors not the
same three for the st. you know not you
know and not even within a person for
the same task but if you have three you
can get ninety percent accuracy you know
with it with the ones we use so i
wouldn't go with just one yeah they're
cheap enough now yeah yeah the sensor
servants are like most of the sensors
we're working with are less than 100 at
this point have a pad amount pressure
mouse that they don't even know it looks
just like a microsoft mouse and we put
three sensors in it and we get there
click signature and that's been really
good you know so the sensors now are so
cheap and some of my first eye tracker
174,000 dollars so yeah and i would
agree i mean-- and I think it also
depends on what you're trying to get up
and there are some things for example I
when I'm doing brain sensing so I don't
recommend using a brain sensor for
everything because if you can get it
just from our heart rate you might as
well use heart rate there's no reason to
put an expensive brain sensor on but
then there are some things if you do
have a brain sensor that are that are
just happening in your brain that are
maybe not happening throughout your body
and so then you need it and so it really
depends on what you're trying to do I
its order to have one answer so the time
the timing of the signal so the electric
verbal response the governing skin
response it takes about its various
between about two to six seconds before
you see anything happen so if when I'm
doing this experiment where I'm watching
what they're looking at the screen and
that's helpful to know like what
triggered them to feel like something
was hard or get scared or something I
mean they're moving their eyes it like
really really fast like we're talking
about like 50 milliseconds between moves
here and
if it takes like six seconds to see any
signal at all everything's just a big
wash now it turns out that the pupil
size reaction is actually within a
millisecond and that's really awesome
because it goes super super fast between
something that has that it's hard to
read so its cognitive load versus low
and as soon as you move your eye off of
it your eyeball will act your your
people actually shrink it back up again
and it reacts super super fast so it's
as good i would say as the EG but a lot
is in a way to clean your signal because
the EEG has trillions of neurons and
there's only two us so I like that
there's a bunch of sensors that are been
used as sort of like clever ways to get
around the noise of the EEG which for
the most part if you're outside the
cerebellum it's just too hard to
understand what's going on in there so
um I guess this question is for you
Aaron so you had this route that people
had to drive and you were measuring them
um but there are variables in that route
that are just changes that can't be
replicated and that they drive different
ways and the car is probably sensing
some of that data or has some of that
data for instance rapid acceleration
rapid the acceleration so that's one
part of my question the other part is
that since both of us live in cities we
know the syndrome of the aggressive
driver you know who-who accelerates fast
to the next stoplight to repaint the
same behavior so so what is it we can
study about the interaction of the car
in the human and then the last part
would be how can we moderate one or the
other or both those are good question so
I mean the first part they were talking
about about the variation that was it
really one of the reasons we did this
big study because um a lot of studies
are doing it in a simulator where you
can fully control all of the
iron mental things and if you and it's
good we show promise that these sensors
work when you're in a driving simulator
to classify workload but if we actually
want to evaluate real workload and real
driving conditions that's why we did the
study so there is variation that's when
we still were able to get good accuracy
which was good that I mean that's what
we were trying to see is if we can still
classify it even with all these
variations we did not include if it was
a thunderstorm or raining or snowing we
didn't do the study so we tried to keep
in general the weather conditions and we
tried to go around the same time of day
we didn't go during rush hour so it's
like steady highway traffic so those are
the thing but other than that you know
yeah there's different traffic patterns
also if some was a dangerous driver we
asked them kindly to pull over and we
drove the car back to think so huh oh
yeah I mean it's not one of them there
myself were like one of the other
researchers so um me and that was for
safety and I mean we have a car we have
all this equipment so you know and that
happens and um no I forget what the rest
of it oh so in in terms of changing
people's behavior I mean that's a big
question and that's more of the
human-computer interaction human factors
a question of if we can recognize these
behaviors how do we help them and so
there's I mean there's some things
already happening that might be that you
could think about so you know we have
cruise control which is standard you
know you keep a certain speed and that
the car will stay at that speed as you
go there's more advanced cruise control
that keeps a certain distance from the
car in front of you so you can imagine
having that distance adapt based on how
aggressive you are with driver or as how
based on your work load level so if or
if you're zoned out or something then
you want to keep the distance bigger so
that might be one way that's not that's
just one thing I can think of other
things might be you know changing the
volume of the music and some of these
secondary things so I don't think that
you should drive your car via your brain
or your uncensored but I think changing
some of these peripheral secondary
things
could could work and I don't know if I
answered all your questions yep so it's
430 now thank you everyone for coming if
you have further questions we're here
now just come on up or we have emails as
well that you can always find and talk
to us thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>