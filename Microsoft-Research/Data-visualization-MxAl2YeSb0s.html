<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data visualization | Coder Coacher - Coaching Coders</title><meta content="Data visualization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data visualization</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MxAl2YeSb0s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so we've had a very exciting time so far
this is our first of our breakout
sessions and I'm glad to see everyone is
here on time that's a great start on
behalf of the my friends at Microsoft
Research I really want to thank for
pesky what a great partner for pets bees
been and putting together this event and
it's very exciting for me our topic
today is data visualization you know
that's fundamental to the entire East
science area I think probably was clear
from some of our discussions this
morning that data visualization is a key
element and our first speaker has robbed
a line from Microsoft Research you know
there's there's something about Rob you
can see something about people about how
they describe their world around them
and in fact the way he describes the
world is from the viewpoint of the
person doing things and that's the way
he approaches his research also if I
understand correctly and so in his talk
he makes a comment a goal is to make it
easier to produce usable reliable
software as a general goal of his group
and of the things he cares about and of
course today's topic is related to that
but in a whole area visualization so
please come
alright thank you everybody I know this
is the talks lot right after lunch so
i'll make you a deal if you'll try to
stay awake i'll try to stay awake too
yes so as Harold was saying let me give
you a brief introduction to how I
approached research because I know we
have a very large variety of backgrounds
in the room so I work at the
intersection of human-computer
interaction and software engineering
which are two fields of computer science
and generally what I do is I study how
people go about doing technical work so
I observed them I interview them and so
on and then I try to improve how they do
that technical work so I try to make it
less frustrating less error-prone easier
for them to do and today I'm going to be
talking about how I've been doing that
with data scientists so about a year ago
actually a little more than a year ago
now a group of colleagues and I started
engaging with data scientists as kind of
a set of users as a set of customers for
us and how we got started with that was
with a set of interviews so we did
interviews with 16 data scientists which
were published about a year ago in
interactions magazine and we
intentionally tried to interview a very
wide variety of data scientists because
this is an emerging role so what is a
data scientist I mean there are lots of
definitions for that right now so some
of the people that we interviewed are
very much traditional scientists so
there are people that are doing say
climate modelling because they're
ecologists or some of them are
biologists some of the people we talked
to were computational social scientists
so they were doing data mining of the
Twitter firehose to try to understand
human behavior by analyzing tweets some
of the people that we talked to were
more from an engineering or commercial
whole point of view so some of them were
software engineers who were trying to
mine the data associated with software
projects in order to make the teams more
efficient or help their error rage or
help their costs that kind of thing and
then some people were just working on
product teams so one of the fun groups
to talk to was a group of data
scientists working on games in fact this
graphic that I put on my title is a
graphic that we got from the halo 3 team
at Microsoft and this was a
visualization they produced for
themselves where they took a map of a
certain game level in Halo 3 and they
started plotting where different players
playing the game were dying and getting
frustrated as a result because they
couldn't make it through that level and
as a result of seeing this map they look
and they said aha we now had this
insight there's this one terrible red
area where everybody is getting shot in
this game and not having fun so we'll
rearrange things in the game a little
bit so they can survive longer and go on
to to play a little bit more so there's
actually quite a wide variety to what
data scientists do but what we found is
that their work practice the day-to-day
activities of how they were gathering
data analyzing it making decisions based
on it it's actually quite similar and so
what does that work practice actually
like today well unfortunately it's like
this so the way that people analyze
large data sets today is very much like
the mainframe era was 30 years ago in
terms of developing software and and I
don't mean to be this to be insulting in
any way I mean that's just where the
state of the tools are right now and
specifically why i say this reminds me
of the mainframe era is that the list of
complaints the data scientists had about
their work were very much the kinds of
complaints that you would have heard 30
years ago so where they were things like
importing data into the system is very
difficult basically you know the system
is in control you have to meet the rule
the system the data doesn't come first
one of the largest complaints they had
is just the fact that the processing
engine they were using things like
Hadoop for example or Azure are geared
to be these kind of big batch operating
systems so people will submit an
analysis job and then they'll wait hours
for the job to actually get scheduled
and then they'll wait even more hours
for their script to run and their
computations to run and and to get some
output and heaven forbid you should have
made a typo in your script because now
you're going to wait for many hours just
to get back an error message that says
whoops or they'll wait many hours to see
the very first graph and they'll realize
oh I men you know inner join not outer
join oh no and then they have to start
over again so it's very slow a lot of
the work now is done very clerical
fashion you have to sort of output files
from one tool and then pass that into a
different tool from a different tool
supplier so it's very kind of careful
clerical work and the worst part about
such clerical work is it's easy to make
mistakes so a lot of people believe in
things like carefully tracking
provenance so keeping very careful
records of how you went from the raw
data through all the different
transformations to how you got the final
result but today there's no help for
that you just have to be extremely
careful and then the last couple of
complaints we heard is that the nature
of the work is very exploratory so
people want to do this in a very
iterative fashion but by having these
kind of long batch kind of tools it's
hard to be interactive and iterative
it's hard to be exploratory and then
finally visualization is really key
people want to see visualization
basically at every single step of their
work but it takes it takes a effort I
mean you basically have to learn
applauding library or you have to learn
a plotting to lure this kind of thing
you have to put a lot of effort into
visualization okay so we heard this long
list of complaints and we thought okay
can work on that we can fix that so for
the past year group very
multidisciplinary research group at
microsoft research has been working on a
tool called big sky and big sky is
intended to be kind of one-stop shopping
for data scientists doing their work so
in the same way that if you're familiar
with programming programmers today will
use something like visual studio or
they'll use eclipse as kind of the one
environment that they do pretty much all
of their work in so we want to build the
equivalent thing for data scientists
that's what big sky is so let me walk
you through some of the features here
and I'll say in advance that I have a
demo tonight at the demo fest so if
you'd like to see it in more detail or
you'd like to ask me to do this that or
the other I'm happy to do that later so
a few things so the first thing to
notice is this is a web browser so the
whole thing is web-hosted there's no
download or anything everything is in
the cloud so all the storage of data is
in the cloud and all the computation
takes place in the cloud all the content
much like any website all of the content
that's on this website is just shared by
default now of course we believe in
security and you authenticate yourself
and there are permissions to see this
data set but not that one but by default
we encourage sharing also it's a
collaborative environment so if two
people go simultaneously to the same web
page they will see the same content in
fact they'll see it updated live so if
you want you can work in a coordinated
fashion with colleagues on the same kind
of workflow and then finally all of your
interactions with with big sky and with
your data are automatically recorded so
this makes things like preserving
provenance kind of come for free so as
soon as you've imported your data every
single step you take with it from that
on is recorded so we know exactly what
you've done with the data and so it
makes it much easier to reproduce in
terms
the overall user experience we're very
much using the metaphor of a research
note book of the kind that researchers
already tend to keep so on the left hand
side here is a set of notebooks the
notebook consists of pages and then sort
of one page is active and shown at a
time each page can have heterogeneous
content very mixed content so this
particular page has a data set which is
I got from kaggle calm about the
passengers on the Titanic and then right
below it is a script that processes that
particular data set and so we want the
notebook page to basically consists of
all the different transformation steps
that you did all the different data sets
that you touched kind of all together in
one place in fact in this morning's
keynote you heard Tony hey mentioned
kind of this new publication model but
you know a published paper should have a
second paper that goes with it that kind
of gives you all the data and explains
exactly what steps were done and would
be interactive and allow you to try
alternatives etc we very much hope that
big sky is a candidate for exactly that
kind of publication mechanism we would
hope that a big sky URL is something you
could publish in your paper and then
readers of it who want to know more can
simply go to that web page and then
finally let me point out a couple of the
design features that we think are
important here so one is that as you can
tell visualization is very much
pervasive in this design basically every
step that you take we try to produce
useful visualizations just for free as
part of doing that interaction so you
can see when I imported my data for
example we just automatically draw a
histogram of the distributions of the
data in that column because that's the
kind of thing a data scientist which is
going to do right away as a first step
anyway so we'll just do it for you for
free so you don't have to worry about it
and there are other kinds of
calculations that we try to do as well
for free so for example if a data
scientist is looking at unknown data
often the first
they want to figure out as well what
kind of measure is this is this is this
a continuous real value is a categorical
data that kind of thing so here you can
see for several of the columns we
figured out that this is categorical
data even though in some cases it
happens to be numeric so things like the
survival rate or the sex and then
finally another very common for step
people do is they look for missing
values and again this is sort of a thing
you get for free i mean as soon as you
import your data into the system you
know you can see the little orange box
there it's probably too small to read
but it says something like 177 missing
values and that's a certain percentage
of the overall data set so let me say so
the intention here is that you'll load
in some data sets and then you'll start
writing some scripts to start processing
that data let me say a little bit about
the scripting so the scripting is very
much like any scripting environment you
might be familiar with like our matlab
that kind of thing so it's got sort of a
read of a print loop but with some
differences so one difference is that
whenever you get a response from the
system that response is not a print
string it's a visualization so here I've
done a sort of query that looks very
much like sequel and I get back a table
so it draws a table the next line of
script that I did is looking at a set of
numbers so here we do a histogram to
show you the distribution of those
numbers and then finally the third
script command that I've done here I've
asked to fit a linear regression model
so the response that I get from the
system is a full scatter plot showing
the line that I get back the confidence
intervals around the line and then we
even draw the residuals for you and then
again this is in a way trying to support
best practices you know the standard
statistical technique is that you know
our advice is that you're supposed to
look at your residuals whenever you do
linear regression plot people often
forget what the command is for that so
they forget to do it or whatever here we
just do it for free we in effect make it
we make it harder to do the wrong thing
than two
the right thing and that's that's kind
of one of the design principles so I
wouldn't really call this a read eval
print loop it's really sort of a read
eval visualized lube but in effect but
in fact we're going to make all these
visualizations interactive because we'd
really like it to be a read eval explore
loop so every data item you produce we'd
like you to be able to like fully
explore it in an interactive direct
manipulation way one quick detail I'm in
fact lying to you it's not a
redevelopment loop at all it's actually
something called a live script so I can
actually edit the script document any
place I like so for example if I go and
switch the mail in my query to female we
just automatically update all results
that depend on that query to keep them
kind of live and up to date so it's
actually a little bit nicer than a
classic read eval print loop now all the
data sets that I've shown you here are
in fact extremely tiny so what about big
data so the way that we deal with big
data is that we deal with it through
what we call progressive streams so
every time you touch a piece of data we
automatically start streaming that data
and show you progressively what's going
on so there I just imported a large data
set 1.67 row million rows just went by
and here i can already start scripting
against that data set even as it's
coming in and again the script that i
write execute progressively so i can see
the results even as they're happening so
now no more of this like batch system
kind of thing where you wait four hours
since I have your attention and since
some of my colleagues couldn't travel
with me to be here I thought I'd give
you just a few quick pointers to other
visualization projects that are relevant
to this community that are going on at
Microsoft Research just to give you a
little taste and then you can visit our
website to find out more so one of these
projects is called sand ants and it's
about doing large-scale visualization
with this really clever technique where
basically they use one pixel for each
data point in the set and one of
reasons besides scalability why that's
interesting is that it allows these very
nice transitions between different kinds
of queries so you can go back and forth
between maps and different kinds of
histograms and so on very easily because
the particles that represent your data
just kind of flow around another project
is called sketch inside and this one is
about trying to make basically data
exploration and data presentation be
much more intuitive and natural
particularly for say business people who
don't necessarily have a great
background in analysis or or other kinds
of end users who don't have much of
analysis so here function is showing off
some of the presentation aspects of this
tool where you can sketch very
personalized kinds of presentations of
the data of the kind that's very popular
in these internet memes you probably see
in your Facebook feed and this kind of
thing this kind of storytelling around
data that people do and then the third
pointer that I will make is to the work
of Natalie rishe where she spent the
last couple of years actually working
side by side with neuro scientists the
neuro scientists are very interested in
the connectivity of brain cells now this
is a huge challenge for information
visualization as an academic field
because traditional info vis you know
works with graphs of say hundreds of
nodes maybe thousands of nodes but of
course the neuro scientists want graphs
of millions or tens of millions of nodes
right so this is several orders of
magnitude more complicated than info
visit strid itional e done with and not
only reaches sort of ambitiously
attacking this problem okay so with that
I will end and just remind you that I
will be giving a big sky demo at demo
fest so please come and see me without
outside questions
my understanding is that it will be
available on the workshops website and
thank you for this interesting talk and
we have some problems with the visualize
more than two objectives or three
objectives and emotionally algorithms
for example and is there some way to to
make them understand to visualize
solutions for genetic algorithms that
has more than for the objectives for
example well let's see it's a very
specific question so that's a
visualization problem that I have not
worked on before but if you like you can
give me some examples of that afterwards
and then I can I can help try to find at
least pointers to other researchers who
have worked on similar things thank you
mm-hmm
thank you for the lecture I would like
to know if the biggest chi software is
open it is not yet so actually this week
we are giving big sky to its first
customer we're really excited about that
I've been spending a lot of the workshop
in a corner fixing bugs actually because
we're trying to ship to the first
customer this week so as we make more
progress and have more experience with
customers eventually we do want to make
this available completely publicly and
open like I said it really is my
ambition that one day people could
publish papers where they have sort of a
big sky URL included as part of their
paper because it's simply you know part
of our infrastructure that people can
share so yeah I very much want to go
public with it yeah no I cannot yeah so
it's a so what we're going to do is over
the next say six months or a year we're
going to be working with Microsoft
product teams as our customers basically
because they're convenient they're
nearby if they have a complaint they can
come you know choke us and we're easy to
find and then after that we're hoping to
go public with it yeah six months or a
year or something like that yeah that's
my that's my estimate
Thanks you mentioned that you'd develop
your own scripting language is that
correct no so the scripting language is
C sharp with its a link feature and link
is actually one of the reasons that we
wanted to use C sharp because it has
this nice sort of sequel like syntax for
doing queries I must also say that I'm
I'm not especially loyal to any
particular scripting language I'm pretty
open to anything that a data scientist
tells me is useful because my data
scientists all are are there in our shop
I'm not going to go to C sharp sorry so
I don't wonder where was a way of I'm
with you a hundred percent so I love and
hate are I think everybody feels that
way about our one of the reasons why I
hate our is because the this is probably
going to bore most people but the
licensing associating with our is an
absolute nightmare are consists of a
million little fragments all separately
licensed some GPL some not etc that
makes it untouchable for me in the
corporate world that's just the reality
one thing I am excited about however is
Julia do you know about Julia it's a
successor language to our so that our
community made up a new language to kind
of try to fix some of the semantic
issues with our and they put it under
the MIT license which makes it
completely open for me so I'm very
excited about Julia okay just the last
question what advice do you have for us
it's a good one I guess my usual advice
so this would be perhaps I don't know
how good a fit this audience or my
advice is for this audience but I'm a
tool maker and I'm used to talking to
audiences of other tool makers and my
advice to any tool maker is be as close
to your customer as possible you know
sit side by side with your customer you
know your user so to speak you know
spend as much time with them as possible
really understand their life because
then they might actually use your tool
because it will be a fit for what they
do all right thank you thanks everybody
okay while she's setting up our next
speaker is Chris Christina Oliveira and
she's coming to talk to us about
challenging multi-dimensional data and
her focus in general her research focus
in general is scientific visualization
and information visualization and it's a
very appropriate topic for a workshop
about turning data into insight hello
I'm from Universal some polo at San
Carlos at icmc and you've been hearing a
lot about science and when I have to
talk what is the science to laypeople i
used to i like this example because i
think they're appealing to all of us
which is well we all know that by now
you know that in science has to do if
they the processing data mining data
visualization data handling in general
but then what we want is actually to be
able to get intelligence from our data
right so for example when you go to the
doctor and you have some symptom and she
say she has an ipod hypothesis that your
thyroid may not be working properly so
she asked for a blood test and she have
your hormones checked your hormone
levels checked and she asked for that
and that's to get an answer on that but
actually there is a lot more data in
that tiny blood sample of course that
she's just ignoring because it's not
something that she was looking for so
ideally we could have a situation in
which as we are taking your blood right
so literally we could say well you just
tell me anything that it might be useful
for me to know about this person based
on the information that is in his or her
blood and then get the data records from
this person which might include all
sorts of data
in different formats like body scan
images sensors that may be attached to
the person at some point or previous
reports and use all this information and
actually tell me if there is something
or anything that could be useful for me
to know at this point considering this
situation right so that is what if
science is about is actually handling
data from heterogeneous sources multiple
sources and analyzing them in contact in
context so that you make the best
possible informed decision on a
particular problem or issue so I'm
focusing on one particular component of
this big picture here which is abstract
data visualization so I i'll try
actually to talk about her experience my
research groups experience on
visualizing this abstract data high
dimensional data i Willa straight those
with one a category of a specific
category of approach for handling high
dimensional data and then I want to talk
a bit about the challenges then actually
I'd like to make you very enthusiastic
about working on visualization so I go
for to a different scenario than in and
this is a current one right so I mean
I'm part of this research network in
Brazil that has many researchers from
several myths domain scientists from
several areas and generally could call
them as material scientists they develop
sensors as may be from biologically
inspired materials so these sensors
actually nano molecular structures very
thin membranes thin films and they they
have taken X to assemble these materials
in different ways and so that they when
they interact with other materials you
can actually measure their response and
from that learn something so there are
many techniques to do
measurements one of them is impedance
spectroscopy which will give you
something like these like these curves
here right so you have a range of
measurements over a spectrum of
frequencies and the behavior of these
spectra will tell them what's happening
that material so their problem is
actually to design these devices these
sensors so sensors for what for example
one of the problems they've been working
on is on this sensor to detect the
presence of antibodies likely to detect
shuttles disease or leishman uses in
blood samples so these are two diseases
that are identified as neglected
diseases because they are typical of
tropical countries and impoverished
areas and they cannot be properly
diagnosed with current methods you'll
get lots of false positives so for both
they get mixed or a sensor to detect the
presence of fatigue acid in very low
concentrations that something for
example you don't want to have nor food
electronic funds sensing devices in
general right so their work is actually
they test a lot of configurations of
these sensors and materials and of the
measurements the way they take the
measurements and it's a very dynamic
scenario and their goal then their
research questions is either to find us
an optimal sensor to solve a particular
problem or to optimize an existing
sensor and of course then they want to
understand why this particular solution
is good and that one is not so good and
so so then we ask them how do you
analyze your data and we learned that
they have a very limited repertoire off
tues dr they resort to a very limited
repertoire of two so for example for
this particular kind of measurements
what they used to do is they try to
guess which is the best frequency in the
spectrum that would give the best
response and they throw away all the
rest of the data and they do like a
principal component analysis on that
particular piece
of the of the beta so actually what they
are doing is similar to what we do if
our blood samples right we are throwing
a lot of data away and it's not that
they can afford to throw the late away
right it's just that they cannot handle
the data as they get it so that's so
then then I change to being what it is
I'm a high-dimensional data right so
this spectra of them is one example of
what we call high dimensional data so
this is data that usually can be
described as these vectors I feel that a
vector of features could be numeric but
it doesn't have to and so you have a
dimensional embedding of the data you
have this vector representation which
eat you of your data objects is
described by a vector and actually you
can get away without talk to an explicit
representation of your data object if
you can actually compute their pairwise
similarities already similarities so you
actually tell how much how similar a
pair of objects are and why why I'm
talking about similarities basically
because this category of techniques we
work on our techniques that generate
this point placements these layouts in a
two-dimensional three-dimensional space
of your data trying to preserve some
having the layout their similarities or
their the similarities likewise
relationships right so you generate
these layouts that are based on the
similarity of the objects we work with
two main classes of techniques which are
projection based techniques or three
based techniques so we have an example
of each so projection based are
basically variations of this approach is
known as multi-dimensional scaling or
some other kind of dimension reduction
and the three base generator are a
hierarchy in which the objects in the
same branch are more similar than
objects in different branches that's
basically the idea
so I won't be able to talk about those
info i'll just talk a bit about the the
projection approaches essentially the
approach is actually to find a mapping
function that does this mapping of your
data defined in an original high
dimensional space to a target low
dimensional space so usually two
dimensional right you need to have some
way to measure similarity in this
original space you have a distance
function defined in this visual space so
usually oak lydon mapping and you enter
a mapping function that tries to
minimize there were between the original
distances are the similarities and the
target distances so basically add there
are many different mathematical
approaches to handle this problem but
the techniques you try to minimize some
sort of normalized stress function
defined in terms of these measures and
these are examples of techniques that
colleagues I am colleagues from my
research group have been working with
that have been proposed recently and so
this is actually not a new idea it's the
idea of generating layout based on
similarities quite old so you might ask
why do you need new techniques for doing
that and I'll come to that in a moment
but as I'm saying our work so this is
the people i work with this is the
visualization an emerging group and I
splitters in the visualization people
which is in pink and the imaging people
but actually although I've segregated us
here we are we work a lot together so we
are a real group and your work on
visualization so as I was mentioning
this I know the idea but would be
proposing new techniques to handle this
problem why basically because we have
nowadays and interact a different
scenario in terms of handling data and
computer capabilities so you were
searching for techniques that can comply
with the requirements imposed by
visualization oriented application
which it could summarizes we need
techniques to be very very fast hand
large amounts of data at interactive
rates handling a large data is not only
a problem in terms of the computational
effort you need but also in terms of the
visual representations how do you
represent this large data properly and
we also want interactivity and in fact
one a distinguishing aspect of these
recent techniques is that they can
actually get information from the user
and you can use this information to
steer the behavior of the technique so
that you can get the most from your data
so these are examples of layouts and in
this case the data objects are
scientific papers well actually
scientific papers in a news feed so just
for you to have an idea of what kind of
layouts you have and also that this is
applicable to two different kinds of
data I'll come back to this later but
then let's get back to our domain
scientists write a material in
engineering so I mentioned you that they
they use an appropriate tool so how can
these tools be useful to them so let's
consider the problem of this sensor to
distinguish leishmaniasis from from
Jacques disease so in one that they've
start using this visualization
techniques to investigate the sensors
they they've been using and I have one
example of a visualization that we've
generated this is being created with a
technique that's not ours actually
Simoes mapping is a very old technique
bits backs from these 60 late late 60s
and so here what we see is each of these
circles correspond to any spectra for a
taken measured for one particular sample
so they have here eight different types
of materials or or or analytes as they
call actually they have eight types and
some of these appear at different
concentrations so they have actually 25
different substance
and these different the different
analytes here they are they are shown in
color so you have three types of
materials here so they have a reference
solution solution which is this buffer
here they have synthetic samples with
antigens for a different kind of disease
they are not interested in they have
samples with antibodies for
leishmaniasis and samples with
antibodies for cruzi these are synthetic
then they have real blood samples with
these combine these different diseases
and then live and antibodies for a leash
money and for cruzi and one that
actually mixture both and bodies for
both and what this visualization tells
them is that this particular sensor
configuration is doing a very good job
of discriminating between all these
different substances because you have
these separated clusters of spectra that
are that the sensor is being able to
clearly separate so to get to this
configuration they certainly tried
several others so it's it's an effort
actually to be varying these I mean the
kind of sensors the kind of the kind of
the type of measurements to find out an
optimal sensor so this particular
solution the good one the best one they
found was achieved with four sensors two
of them were specific for detecting the
antigens for these two diseases and two
of them were not they were sort of
general which was surprising to them so
actually the combination of the four
sensors was actually the best one to
differentiate among all these different
substances so this is my point with
visualization with of using including
visualization and including appropriate
tools in your data analysis right
because in particular visualization can
help a lot in situations where you are
exploring alternate
and you need flexibility you need the
rapid feedback and that's something we
haven't explored in this particular
example but in many situations when you
are handling a lot of data data you also
want to give your users the ability to
to actually teach the technique what you
know about the data so you may not be
you may not know everything but may know
a lot of things that might be useful for
the techniques generating the
visualizations and then for the data
analysis afterwards so you might say
well but I'm a computer scientist and
I'm not particularly interested in
working with like materials or things
like that but in fact this the nice
thing I think about utilization is it's
very multidisciplinary it were applied
so you can actually work with domain
scientists scientists in very
interesting applications but also there
is a lot of room for fundamental
applications or our fundamental aspects
of computer science that have to be
handled so that you have really
effective techniques for for these
problems and so just to illustrate that
you can have other sorts of situations
it can use these ideas so actually this
idea of generating and visualizations
based on global similarities a very
general one right it's a very ineptly
cable in many different situations so in
this case this example here here is the
similarity map right in the circles here
are actually music so this is the work
of my master students audience she just
finished her work and so she generated
these music visualizations this is this
is 1300 songs as use the media
representation the symbolic
representation of the music's in so she
has this similarity based map and here
the colors actually are mapping the
january of the music right so we have
the red is rock pop rock and the few
blue samples here are classical music
and this green ones is
we call him Brazil certain age which is
a very dull kind of music green here in
so how she assesses similary so you see
actually that the map is actually
separating the music's by general and we
that's not information that we used into
the technique what is the so how did the
feature vectors how we are describing
the music so what she did is actually
twist can the media symbolic
representation looking for repetition of
certain pattern so she has compasses and
chord repetitions of different lengths
and she maps discord repetitions into a
signal and then she assessed the
similarity between these signals to
generate them up and actually she also
uses the signal she mapped the signal to
an icon which is are these icons which
you see here so this is a music with a
lot of variation in this structure this
is the classical music and jazz is very
much similar to this the appearance of
the icons these are they pop rock in
disease the certain usual they want a
record though and not you see there is a
difference in these patterns and why is
this useful for well at this moment we
don't know actually that's a good point
to discuss but I think there is a whole
thing that we could learn about the data
or the things that we handle in our
everyday life and for example there is
many interesting by recommender systems
on learning how people like music and
how can you explain the way the personal
faces of people and so on so forth so we
don't know so this is very exploratory
right so we wanted to see to see if we
could see the structure of music and
that's why we bought that's a different
application on text and for texts we
have texts also a challenge this is
actually results from a google search on
the term jaguar features right so we
have actually 64 snippets represented
here in the rectangles the bigger the
rectangle the higher the rank and
actually the layout is grouping these
nibud based on their topic similarity so
here on the left i head
of different models of jaguar cars in
the different colors and here i have
other things about Jacques bars like the
animal there is an old video game there
is an old operating system other things
on Jaguar so it's a different way of
actually seen your Google hits and
actually been working with many other
applications in social networks
biological images volume data there is a
wide range of application so what are
the challenges I actually group them
into categories right so there are the
challenges that are related with the
computational aspect of those techniques
and of having them properly used so for
example handling very large data sets is
still a challenge I mean we do have
techniques that handle very large data
sets but it's still not easy you always
have to find me what you want to
visualize is what you will be able to
visualize the Pens on watch how our
modern your data so however representing
our data and how do you assess
similarity right so finding proper
representations in similarity evaluation
functions is difficult in its very very
domain dependent which technique you
have several techniques and times you
don't actually have many choices of
which technique to use but in many
situations you do so which one do you
use which one do you know that's going
to be the best one for that particular
kind of data and of course we are doing
dimension reduction severe one and this
is a very lossy process so we should be
able to actually evaluate quantify and
qualify how much information we are
losing that would be important so that
goes towards validation from the user
perspective then I think there are other
challenges that are not only for
computer scientists like finding better
metaphors learning how people use these
visualizations and how they learn from
them that's something we should know in
trying to find out these better
solutions because this is all abstract
date and then abstract representations
so what are we how effective are they
actually took give people what they need
in summary many many types of problems
and
potential applications and that's an
area that requires people with many
different skills so this work is
actually involves several collaborators
both in Brazil and abroad and i hope
i've mentioned all of them here i also
have some papers that are some listed
here that are somehow related to these
things i've been i've been doing and you
can also go to our website and as i said
i'd like to make you enthusiastic about
the possibility of working on
visualization so collaborators and
students and postdocs are very welcome
to contact us thank you very much maybe
I would like to make a question about
the music design revision sighs okay so
blue its classical red rock and roll
green centennial what does that mean
exactly as I see it it means that the
verity of rhythms and chords in certain
areas much greater than the very thin
rock and roll is this correct in a sense
yes actually what she did it was huge
because me it is a symbolic description
of the music writer from me she uses
some very basic music theory to identify
compasses and then chord sequences
called repetitions and then she maps
disc or the repetition so they have
certain lengths and she mops one
particular code repetition for color and
so these icons here they will be more
varied when you have deep more a greater
number of different kinds of repetitions
I've gotta have energy so say yours
telling me for instance that the blue
icon has less fewer repetitions no I'm
telling you for example here you have a
selection of these blue musics which we
know are classical music right so
actually she is actually showing a
detailed representation of these new
six that have been selected here so it's
like I mean looking into the music so I
have say I don't know how many maybe 20
musics here so each line corresponds to
a music and the color in the icons maps
these different cause repetitions so you
have more variety of these repetitions
based repetitions here then for example
in okay that's the idea but between the
certain age group there's a difference
yes there is okay NY is the blue so
close to the red because you know there
there's the classical base there's a
famous classical music called canon and
gigue by a hypo it's it's it's it's said
to be the basis of all rock and roll
music you see I mean this place needs
very relative so in a sense it makes I
mean it makes sense to me that it's
closer to the red then to the green but
I mean it I don't really know why it's
so close there are other things actually
that might made stored things here for
example have many more red ones you had
have many more samples of rock band from
from classical so that might be
distorting so maybe use a specific
period of classical music because
they're NPK do you / oh yeah because she
collect the music's from the internet
it's not a very mean it's not a very
structured data set but we did an
experiment in which we added some jazz
music and they overlap with the classic
and if you look at the icons they are
sort of more like these in there like
these okay thank you very much
okay well Rob sets up I have a question
and I think it's relevant to all of us
that you know really it seems like your
area is an area of working with domain
experts in general in this case maybe
you use the internet a little bit as the
domain expert to select your music but
in general you work with individuals
what is some of the advice you can give
us regarding beginning a new
relationship working from your
particular field with somebody in a
different field what kinds of things do
you go through what kinds of things you
ask the domain expert in order to begin
to help them you have you have to learn
how they are doing there I mean what
which is their standard approach and I
think they were the recommendation usual
to my students winter act with them is
be very very patient and very very
willing to listen and not also to
explain and that's that's a challenge I
think it's more in a sense more severe
in Brazil but I have the impression that
our domain scientists in general there
is a big gap in what they are using to
analyze their dating but they could be
using if they were of course I don't
want to imply that it's their fault but
there is a lot of there's lack of
information while on available
techniques and tools for computer
science for many people so sometimes
people just get used to doing things in
one way and they go on and they could
actually be doing it quicker and more
precise with more precision if they so
actually both sides have to be willing
to learn from the order and that
requires I think a lot of patience
because in the beginning even I mean
even though you speak the same language
it's it's a situation in which you you
don't understand what the order is
talking and that lasts some time and
thank you thank you very much thank you
so Rob is setting up I will say
something about Rob I think I should
I've said something about everybody else
first of all he describes his world as a
place where he wants to use information
in service to environmental research and
I think you'll see in his demonstration
or those of you who attended his
tutorial yesterday saw that as well but
that's really his major focuses is how
you apply the information to help
environmentally search and he's truly a
scientist what he's going to talk about
today is data visualization and layer
scape
does that mean
maybe we're okay here okay and that's
working
okay so um hi my name is Rob flatland
I'm with Microsoft Research and as y'all
noted i work on there's a green light on
ceiling I work on technology problems in
relationship to environmental science
yesterday I was asked to give the
morning presentation on using a layer
scape and world wide telescope that took
three hours so for those viewer here and
we're at that then I apologize for a
little bit of redundancy and I'll just
have to try and condense three hours
into 20 minutes so I'd like to start out
by playing tour earlier today we heard
about telling stories around data that's
something that Tony mentioned so well
this is playing I'll make a couple of
comments about where I'm feel like
Harrison Bergeron I'll make a couple of
comments about where I'm coming from and
what we're working on and some of the
themes that we encounter when we talk to
scientists is you know microsoft
research connections tony hayes group is
about reaching out to people in academia
and outside of microsoft and figuring
out where we should be placing our
emphasis based on real world needs and
in our case it's real world
environmental science research needs so
one of the things that i'm going to talk
about is visualization obviously as this
tour is playing out you're seeing some
of the capabilities of our visualization
engine it's called worldwide telescope
and as I work on environmental science
we kind of created a set of tools that
orbit around world wide telescope and
that's collectively referred to as layer
scape so the naming can be confusing but
worldwide telescope was initially
conceived of as an astronomy tool it has
astronomy data built into it and it
talks to external databases of astronomy
data and then there's a viewing mode
switch down here in the lower left and
you can choose to look at planets you
can look at panorama views from taken
from Martian Rovers and you can look at
the solar system and you can look at the
earth so we're going to spend some time
looking at the earth right now it's
looking at the magnetic fields of the
Sun
oops so when you build a data system
like this you sort of build it in two
parts there's what runs locally on your
machine and maybe that's in a browser or
maybe that's a really heavy duty
powerful piece of software which happens
it happens to be the case here and then
you have something that's out in a cloud
that you're sort of talking to and your
your data is sort of distributed amongst
yourself and the cloud but there's
really kind of three sources for data
there's what's built into the system
natively so for example tiles of the
earth these coming from bing maps and
then there's data that you bring in
yourself for example these white
thoughts or earthquakes that's a
distribution of earthquakes over time
and of course the scientists are
interested in the earthquakes because
they represent the tectonic plate
boundaries so there's a model for the
edge of two tectonic plates that are
smashing into each other one of them is
subducting down below the other one and
after the sequence of earthquake
animations it'll go and show the South
America plate and the subduction that's
going on there anyway the third so this
is first is natively what kind of data
you have and then secondly there's the
data that the scientist brings into the
systems they just like what we're seeing
here and then the third type of data
that you can bring in is something
that's provided by another scientist or
by another organization let's say NASA
and so this idea of Confederate data
systems and having data systems and talk
to one another is is a new way of
thinking about how to do research
because in the past the model has been I
have my data and I kind of scoop it
together and maybe I know about one
other guy who's a got data of his own
and he goes off and does his stuff but
then we swap by FTP or by thumb drives
or something like that and that's not
going to keep up with the data deluge
with this data intensive science that
we're era that we're in you have to
really be able to I want to say discover
new types of data that allow you to
reach into domains that aren't your area
of expertise because we're discovering
as we learn more about the earth of how
interconnected it is that we have to be
able to reach off into other domains if
I'm working in hydrology then I'm going
to start to need to know about ecology
and about microbial population dynamics
in the food web and atmospheric
radiative transfer and stuff like that
so as we build our picture of the earth
and gets more and more complicated
there's this drive to build systems that
are Confederated that know how to talk
to each other and that leads into long
discussions about cyber infrastructure
and standards and so forth and then the
other aspect of of how we work on stuff
is whether to introduce things that are
incremental changes to what people are
already used to or to try and build
something completely new and crazy and
out of the blue and the harder it is to
adopt the less adoption you get so you
are motivated on the one hand to build
things that are straightforward and easy
and inspiring to people to adopt but
then again if you have a really good
idea then maybe the thing to do is take
a chance and build it and really go way
out there and ask people to kind of
follow you along and that's how these
big sort of innovative jumpstart made in
the way that we use technology so it
looks like the tour has flown to its end
I'm going to restart it one more time
just to let you take another look at it
and this time I'll sort of be more
narrative about the visualization piece
actually there's there's two things I
wanted to mention we mentioned earlier
the idea of a second paper that
accompanies your first paper that's
executable and we in microsoft research
connections work closely with Microsoft
Research Cambridge that's true purvis
who gave the talk earlier today and
they've generated something called fetch
climate and they've generated something
called distribution modeler and these
are tools that are extremely useful for
people into environmental science and
particularly the distribution model or
something that I want to mention in
terms of provenance because it allows
you to build a processing system that
works on your data and that thing can be
stored in the cloud and then pull down
and share it and so it's something you
can unpack and execute and see how the
data results came out of the source data
okay so now we're coming down into the
solar system so this is in solar system
mode having flown in from the very
distant galaxy view and as we come in
here there's a little comment that's
going flying up to the top of the screen
I'm introducing some
solar magnetic field lines how would you
do this how would you generate this well
first you need a four dimensional engine
that allows you to sort of fly through
time and space like this and then you
just need a data file it describes a
magnetic fields and then there's lastly
that sort of third missing step that you
need to be able to translate your
magnetic field file into these lines and
so there's a little bit of a learning
process there and that's what we
discussed yesterday in the in the
workshop and i'll be at the demo fest
today so if you see anything here that's
of interest to you and you say gosh how
did you do that by the way this is the
Moon moves not lay this color but that's
a false color of the moon's altitude and
that's data provided by nasa so we don't
have that data we're pulling that in as
a service anyway it's a demo fest if you
see something that's of interest here
please come and talk to me and i'll tell
you how how it was done and now we
switch to the earth there goes australia
and again this is a distribution of
earthquakes and i already described
there's a model in here of the plate
boundary and the scientist who generated
that model couldn't look at it on his
own software until I showed him this and
he's like oh this is really great this
allows me to sort of investigate my
hypothesis the insight that he has is he
suspects that when you have an
earthquake and it starts ripping open
and tearing apart the rock and rocks
sliding past itself that earthquake
keeps propagating until it gets reaches
apart on this slab face where it has to
go uphill and it kind of runs out of gas
so he would like to do further analysis
and further visualization to kind of
indicate that hypothesis and show that a
particular event which you can solve for
as a sort of constitutive smaller events
propagates out from an initial point and
then runs out of momentum or runs out of
stops moving when it has to work too
hard to continue Parker mccready at the
University of Washington does fluid
dynamics models so as you see this is
three-dimensional data but it's actually
four dimensional in time and there's a
fifth dimension which is the intrinsic
color simply whether the water is going
north or south and at this point I'll
just sort of take a chance on pausing
this and I'll demonstrate how you would
initially work with this data
as you put it in here and you sort of
let it play and then you would say well
okay let's go down and investigate this
particular part here and I have a little
time controller that I can pop up see if
we can get the right layer highlighted
good okay so that's a bit overwhelming
and the second part of my talk is
actually going to be about the
overwhelming pneus of heterogeneous
oceanographic data if I can get that far
so let's see how we're doing with this
if I back out a little bit further I was
showing this yesterday too at some point
one of my particles of water wanders off
into this Bay over here so it's managed
to become separated from everything else
that sort of seem physically reasonable
that some water is just going to kind of
end up in that Bay after the bait after
all the Bay has water in it and then I
can eventually see that it ends up going
back into the main channel here and as
it does it has this kind of interesting
little corkscrew motion I'm going to
move time forward a little bit further
as it goes along the coastline so that's
an interesting thing to me because it
asks the question does the water going
along this coastline actually corkscrew
or is that a product of the model that I
wrote the software that i wrote to do
this simulation one of the things that's
come out of the work that we've done is
that you can often see errors in your
data if your data has maybe your sensor
was off calibration or your model has
some kind of you know non-linearity munn
linearity in it or something like that
you can kind of investigate this here
and use your sort of human acuity in
your spatial processing abilities to
sort of see what's going on with that
now I've created a narrative story that
we've been watching playback that's
actually a sequence of slides up here at
the top so the the software comes with
that is called an authoring environment
now just go back and right click here
that the thing you do is you always
right click on stuff and roll at tools
telescope so I'm going to preview the
tour from here and we'll
onwards with our with our story by the
way these are swishing back and forth so
you can sort of guessed that the time
scale is about four days those are tidal
fluctuations tide comes in the tide goes
out another problem we generally face in
trying to design systems is that if we
build something for a particular
scientist the scientists in the office
next door won't be able to use it so how
much do you focus on building platforms
generalized platforms versus how much do
you focus on verticals you know specific
applications and that's a balance that
we try to stripe but we try to make
tools that are not only useful for a
particular research project but also
that can be adopted multiple times so
the generic ability to generate and put
in a mesh you know that's going to apply
to a lot of different disk domains
within environmental science we hope and
then the last thing I want to mention
here this tour started out with galaxies
and we're coming down here to a mass
spectrometer data so those dots or
individual molecules and they're being
plotted in two different ways and I'm
sort of animating the transition between
those two vertical axes so it's all the
same six thousand or so molecules but as
they transition from one view to another
those views are in one case representing
the mass of the individual molecules and
in the second case they're representing
probably what we sort of hypothesize is
the isomer of a particular formula than
a number of isomers of a formula always
how many different ways you can build a
molecule out of them and the individual
atoms so this is a research into
dissolved organic matter which is a big
huge part of the Earth's system because
it represents sort of the fundamental
grocery store that's available in the
oceans to the microbes that are living
there the autotrophs in the heterotrophs
so with that I wanted to actually open
up a second tour so I'm going to halt
telescope and I'll mention a couple of
other things about layer scape as this
is happening somewhere in here i have a
browser
across ok so this happens to be my wiki
and it has all the links from the
tutorial yesterday so if you again if
you talk to me you can get this URL on
try and give it to you right now but
it's has a whole bunch of links for the
technology that we work on so our
current whoops sorry i should say the
page is going to come up is the tutorial
page and it has a whole bunch of things
but rather than wait for that i'm going
to show you the lair scape home page so
you can build world wide telescope but
how do you get it adopted and used well
you create maybe a web page that acts
like youtube for worldwide telescope
content and allow scientists and anybody
actually who can get a windows live ID
to come here and look at the content
that we've placed here download it play
it investigated experiment with the data
and the data is wrapped up in those
visualizations so so the visualization
which we call a tour is a little story
around the data but that actually has to
contain the data as well so layer escape
is a website its lair Scavo RG you can
install the world wide telescope client
using this orange button up here and
then you can install an add-in to excel
that enables you to push data into
worldwide telescope found easily so the
layer scape website if you scroll down
here has a featured content and there's
example tours here there's an educator
in Poland who's has a tour here of her
kids using worldwide telescope to
inspire some artwork and it shows them
building that and then there's a
workshop sort of how to hear the fourth
content on the homepage is how you would
get started using Excel to generate data
put it into worldwide telescope and then
and then view it so that's this kind of
the layer scape ecosystem everything I'm
describing is free the only cost really
is the time it takes to learn how to
operate everything at how to get your
data in so the idea is to show the
potential and then to sort of place that
out there and offer that out for for
public use
so I'm just going to take a moment and
open up a second tour and the second the
purpose of the second tour here is to
kind of explore the information to
incite theme of our conference or
workshop do two things at once
so while that's loading i'll just ask
are there any questions and what's going
on so far everything's straight forward
okay so it's a little tricky to sort of
work backwards like this there's another
tour up here but instead of hitting the
play button I'm going to actually just
explore this data using the layer
manager so I'm going to sort of pretend
to be an ocean scientist this is
Monterey Bay off the coast of California
and we work with the mana bay rum
monterey bay aquarium research institute
located there and these people do
experiments that generate very
complicated data set so i'd like to try
and give you a sense of what that means
I hope
yeah it's very calm that's right drifter
no I'm not seeing it
that's very peculiar
time series
alright let's see what happens if i play
it's too good to last
okay there we go something just clicked
and everything's working now
obviously this is going to turn into a
very complicated data set so I'm going
to turn off some of it here
ok
so what I was hoping to show is this
trajectory here and this is a robotic
submarine it's called teff'ith onymous
underwater vehicle and as I can show it
flying by here okay my system is giving
me a little bit of grief so I'm just
going to have to work with it okay so
what you can see if you look down on
this is that this robotic submarine is
kind of flying in or swimming through
the water in circles and as it swims
that's going up and down and up and down
and up and down so that in itself isn't
too bad but that thing has about 10
different sensors on board so it's
creating 10 different signals in time
and for example one of the things that's
calculating is a salinity of the water
so since the water tends to be sort of
layered I'm going to select a particular
salinity value and turn start lighting
up the pixels of water when I measure
that salinity and if I measure a
stronger salinity and change the color
than I can get something like that that
down there so this is official physical
oceanography measuring the structure and
the mixing of the ocean and so far it's
still not too bad now why is it swimming
in circles like that well it's following
an environmental sensor package and that
thing has it just a drifting with the
currents type of a pattern that it moves
in so it's not as it's not self directed
it's not powered it's just drifting
along you can see what the currents are
doing and there's a little title stop
and so forth now that environmental
sensor package has onboard a biology lab
and that in turn is measuring microbial
populations because the ultimate goal of
this research is to understand microbial
population dynamics which is what the
producers and the consumers little
phytoplankton and the zou plankton are
doing out in the water so we can also
turn on those layers and see what's
going on with the phytoplankton so we
have two populations of plankton sinica
caucus and cron archaea and they're
respectively white kind of gold and blue
here and now I'm going to use my time
control and go backwards in time
and i'm going to show how it plays
forward so it's a mess but nevertheless
you can see that those two populations
the gold and the blue ones are doing
something kind of coherent so i'm going
to turn off the salinity ripples turn
off the trajectory of Teth asst and
we'll go down here and just look at the
drifter and look at those two
populations of microbes so when it
starts out the blue ones the Krenn
archaea are very faint back at the
beginning there's not very many of them
I can fly over here and look at that and
then as we move up sorry I'm losing that
too far his move we move along the
corner ki pick up and the Seneca caucus
kind of fade out so that is something
that you can investigate in terms of
those robotic submarines that are
swimming around and gathering data but
of course the other thing that you can
do is you can look to other data sources
and in this case we have surface
chlorophyll from a satellite and surface
elevation so if we look at that
structure here we can say oh look at the
very beginning here we're kind of going
around this big gyre and in that point
that's where the blue ones were kind of
faint and the yellow ones are strong and
then we kind of got into this water over
here it has more chlorophyll in it
that's what the color means and then
suddenly the the blue ones started
picking up in the yellow and started
fading out so there's a clue here as to
what's driving the microbial population
dynamics and you can sort of imagine
understanding this in terms of typical
XY scatter charts and stuff like that
but what we're trying to go for here is
that you can actually have this sort of
three and four dimensional environment
in which you can dump all these
different data sets and play around with
it and turn things on and turn things
off so in fact another thing that you
can do is you can create a reference
frame I'm going to turn off this
chlorophyll map
so I can create a reference frame here
well thank you I can create a reference
frame and I can play back that entire
experiment relative to the center of the
experiment and so this is another sort
of way of giving the scientist a way of
visualizing of looking into the data set
and still it's too complicated still
we'd have to take this apart and the
last thing that I wanted to show in here
in addition to the sort of reference
frame centered view of the world is one
more little data set where we're
calculating sort of a synthetic value
and examining what the water is doing
and that's just one thing here okay so
the last data set in my presentation
today is simply the fluorescence of the
water that's an indication of how much
chlorophyll there is in the water but
it's done at a particular calculation of
salinity so again we have a robotic
submarine that flies along it measures a
temperature and the salinity of the
water and it gets a density so I what I
did was I said okay at a particular
density I want you to paint the water
whenever you reach that density but
paint the water based on a color that's
driven by the amount of chlorophyll in
the water so what we're doing is we're
combining two different data signals
we're actually three different data
signals temperature salinity or
conductivity to get water density so
that water density is then combined with
the amount of chlorophyll at that point
and you can slide this around and
experiment with it but what I did was I
just sort of showed all of that 44 runs
out and back of the submarine so the
submarines just going out off the coast
and coming back and going out and coming
back and then every time it hits that
particular density so think of that Z
track of up and down through the water
column it paints the water the color
based on the amount of chlorophyll now
this is looking at everything at once so
if I now take this layer of data and I
click the tag here for time series and
now it becomes time series data so now
my time slider is active and I can move
backwards and forwards in time so here's
the thing swimming out and there's this
thing swimming back and swimming out and
women back so that's breaking down the
data in terms of time but I wanted to
point out that at this particular track
in the beginning here if I go down and
look at it carefully I can see that I
hit those density values at just one
depth in the water column but if I move
a little bit further in time so I'll go
out and I'll come back here I'm going up
and down through the water column but
I'm hitting the density at two different
places so it's as if there's a lens of
let's say higher density water in
between the lower density that I'm
actually measuring ok and that's
important because water is going to flow
along constant density gradients so if
the water is changing and bifurcating
into sort of like inversion layers and
splitting apart and you can measure that
with these submarines then maybe that's
going to give you insight into what the
microbial population is doing and how
come a population that's maybe initially
monolithic can be split by this density
division and sent off into two different
separate currents and sent off into two
different directions so on the one hand
you have this incredible amount of
energy and intellectual struggle to get
just the equipment out there in the
water and working and generating complex
data sets like this and then the poor
guys who did all the work to do that get
the data back and they're like how do i
how do I deal with this how do I look at
it this is just a gigantic terrific mess
so we're trying to come along with a
solution we're trying to sort of offer
them a way of getting their data into a
visualization environment where they can
say okay what about this and what about
this and why are things together and
explore the data in sort of the way that
a human being would naturally and you
know investigate a three dimensional and
four dimensional object so that's one of
our ongoing projects with the monterey
bay aquarium research institute so again
i'm a demo fest and i'll have this and
the other things that i showed again
everything that we're showing here is
available for free and that we have
materials to learn how to use it and
then i wanted to mention again that the
idea of data system confederation is
very important to us using services and
our current work in environmental
science for example is coupling
ourselves to the work that's done in
cambridge so we can take advantage of
one another's work without having to
sort of
we build and reinvent the wheel and then
lastly if I'm working on a computer and
I'm thinking about stuff in my human
time scale which is sort of the time
scale of a sentence or a word than I
want things to be sitting on my computer
but if I can wait if I don't need to see
it if I don't need to have it
immediately at hand then I want it to be
taken care of somewhere else because one
of the biggest problems in environmental
science and science in general is just
data management and so a big part of our
effort as we're getting into this cyber
infrastructure and the management
organization of data data is to push
stuff into the cloud in our case that
means we use a sure and it's a really
exciting possibility because it gets us
all kinds of things it gets us the
ability to for scientists to get credit
for publishing data it allows them to
aggregate larger data sets data sets as
people contribute data from all over the
world of a particular type and basically
it gets the scientist out of the
business of trying to maintain computer
systems and more time to focus on their
actual research so thank you very much I
appreciate your attention
thanks for the presentation is always
fun to watch the visualizations on this
as more scientists start using this and
more data sets come in I through the
tutorials yesterday you were saying
there's kind of implicit semantics with
your data set but you have any plans to
incorporate explicit semantics so Otto
discovery of web services that might be
out there in the future so scientists
can link up data that they may not know
about yeah that's that's a really good
question what I one of the projects that
we've done in the past didn't actually
use this data you use data collected by
government agencies and it gave you
basically a geospatial search engine so
you could specify an area and a data
type it was called size scope and it's
still operating but basically the idea
of discoverability says that your data
set has to be registered you don't have
to register a data set by copying the
entire data set you can simply describe
it in terms of location time and type
and then it becomes discoverable so I
would like to see further work done in
that direction and NSF would like to see
further work done in that direction so
discoverability is like the first word
out of our mouths when we're talking
about the flow of what happens to data
even though sort of discoverable is at
the far end of it you know you want
people to publish in a discoverable way
so that their data can be reused and
have more value than it was originally
gathered for so yeah very very strongly
agree with that
see any other questions so
I always have a question if there's no
other question I'm standing in between
you and your coffee break or actually
you will be as soon as I ask the
question so you showed us quite a bit of
data you've been working in the field a
long time can you share with us one of
the exciting discoveries that was made
possible by visualization that might not
have been as easily too easy to find
well I would say that the main ones I've
touched on here which are the sort of
aha moment that you have when you see
the structure of your data in a
geospatial context so for example the
plate boundary stuff that I was talking
about and in this case I think we're
just at the tip of the iceberg so to
speak in what we can see with this data
but we're already seeing things that are
falling into two categories number one
oh I didn't realize that my data looked
like that or that the you know the let's
say the structure of the water could
suggest so strongly how it's related to
the structure of the microbial
population but then the second type of
information we're seeing is oh that's a
bug that's a problem that's something
that shouldn't be in there and I
shouldn't be spending time trying to
analyze something that's just plain
wrong so again it's you know
streamlining the process to get the
paper done okay thank you very much and
thank you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>