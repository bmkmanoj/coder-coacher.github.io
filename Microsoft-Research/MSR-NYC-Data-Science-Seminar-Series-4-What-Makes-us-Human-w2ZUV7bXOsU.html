<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR NYC Data Science Seminar Series #4 - What Makes us Human? | Coder Coacher - Coaching Coders</title><meta content="MSR NYC Data Science Seminar Series #4 - What Makes us Human? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR NYC Data Science Seminar Series #4 - What Makes us Human?</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w2ZUV7bXOsU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
oh hi everyone thank you very much for
being here I know it's a really really
cold day so thank you for braving the
cold and joining us here I also want to
thank our staff Pete and and Jeff for
helping us put this together and finally
I would help I would like to thank our
speaker plot a claudia perlick who is a
chief scientist at distillery her
research interests lie in the
intersection of machine learning and
digital advertising she also teaches at
NYU mba mba program she has over 50
published articles lots of awards and
all those whatever technology magazine
that you can think of at some point they
named her one of the smart or smartest
or you know most creative people so
we're very happy to have her here today
she will tell us about the challenges of
machine learning data mining and other
analytical techniques in the context of
digital data advertising so with that
Claudia the floor is yours thank you
very much um so there are a lot of very
familiar faces here I've warned you you
probably have heard part or all of the
story before um so I'm kind of
transitioned from academia over a
research lab now into advertising the
last place I expected myself to be but
that's where i am and i found it to be a
great sandbox to play around with
machine learning and so this is not
going to be a very technical talk this
is more of a interesting overview of the
things you end up wrestling with that
often aren't really of that technical
nature in digital appetizing and you can
do me a favor and make this really
interactive and ask me questions so
because that's a lot more fun for me
then and hopefully also for you so um
I'm one of those people who bother you
all day long and show you ads wherever
you go and whatever you do so in
particular I'm going to be focusing on
display at but
I saying meaning when you use your
computer and you look at websites we
also do mobile and video by I'm not
going to get into much of that stuff so
let's just focus on how the hell did
this ad get on that page when you were
just trying to figure out what's going
on in New York in the morning so i'll
give you initially a bit of a high-level
overview how programmatic advertising
works now what does programmatic mean it
basically means this is all real time
real time auctioning where we are buying
those ads in real time so it's a huge
market place where ads get bought and
sold we are on the side of actually
buying these spots and then filling them
with an ad that we hope it's not too
irrelevant to you and ultimately leads
to conversion alright so that's the
business I'm in now how does it work
that's my mom she is one of the two
million two hundred million people
haven't figured out yet how to disable
third party cookies or install a ad
blocker and when she looks at the
website what happens while the site is
loading is that this spot for display ad
is sold through an ad exchange meaning
there is a request forward into an ad
exchanging there plenty of those
marketplaces and then they come to me
not me personally but our company and we
receive currently about 30 billion
requests per day and that's after
filtering a lot of stuff that we don't
want to look at so 30 billion times a
day I need to decide whether or not to
show my mom and AD and for what or
anybody else for that matter so we have
about 100 milliseconds typically less to
submit a bit price and if we win the
auction and it's like eBay supposedly
it's a second price auction although not
entirely convinced it's really a second
price from what I can tell that's kind
of on the interesting side lines then we
get to serve an ad so then I choose what
camp I campaign I want to advertise to
you and the add loads on
your browser anybody on board with the
triskele delivery side of things I'm not
an ad agency i work on behalf of an ad
agency i might work either for brand on
ad agency but a good point let's
actually try to get to where my money
comes from right so far i'm only paying
i need to learn something too now the
interesting question is what do I know
at that moment to make that decision I
know where the person currently is but I
also know a cookie ID that I have
assigned to the person so I can look up
browsing histories and additional
information that I have collected about
that person and I have to do it in these
100 milliseconds to make this decision
now where does that come from obviously
I get a lot of data directly from this
side because every time i get a bid
request somebody tells me that a person
is on the website we also have data
partners that help us kind of observe
some part of browsing behavior that
people show the rest of the time and
it's we track about 10 million urls the
question is how do you parse them so
don't get hung up on the number here so
what happens there there is a pixel on
the page that again lets us put a cookie
third-party cookie on the computer and
that is kind of the Augmented history
now in reality the cookie really only
contains a 22 at random number which is
the ID that we assigned to you but that
is the key under which we store all the
information that we collect for that
person so far still no money made only
paid okay so how do we make money we
work with brands directly or with
agencies so let's say there is a
marketer citibank nike whoever runs once
to run a campaign with us prior to the
campaign will also put a pixel on that
brand either homepage or checkout page
so now not only do i observe what you do
when you just read but I also see who
goes there and buy something
that information automatically works
also as kind of an outcome the metric
that we are held to what is called a
conversion in this game some people
still measure click but more interesting
these days is after seeing this ad
within a given time period of say seven
days typically does the person who saw
the ad show up at that brands homepage
and take some action that the brand says
is relevant it could be buying something
it could be in case of luxury cars
unfortunately not too many people buy
them online it could be scheduling a
test drive whatever the brand says is
the relevant event they care about all
right questions about that's kind of as
Divas I'm going to get it is kind of the
bidding and the serving is almost the
same decision because I in fact I first
decide that the best thing to show you
its campaign X and then I decide are you
good enough for X even so it's kind of
an integrated decision so this
information has to feed all right here
into that moment now i will be doing a
lot of pre-computation because i can't
do everything down here so the system
pre calculates a lot of things first of
all yes every person is like people get
put into certain levels and that's where
I'm going to talk about the modeling and
the prediction the scoring on top of it
even for the same person in the same
campaign I will be the different price
when you are at kayak versus when you're
reading some random blog I know you're
good prospects a4a travel campaign I'm
running but if on top of Vidya right now
and kayak I might might multiply my bid
price by 10 because I know right now
kayak is a great kind of conversion is
at the marginal effect of being an
entire
me another factor of 10 beyond knowing
that you are anyway a good prospect we
don't and I'll show you in a moment what
data actually collect so the only
information we collect is the literally
the URL string we're not going to scrape
the page and try to figure out what's
going on there we're not even buying
contextual information now we're talking
about one of the ugliest pieces of our
tech it's called attribution it means
chances are before you convert wasn't
just me who showed you an ad the heads
been maybe a search ad in between and 50
other vendors who on the same business
so then when that conversion happens the
question is who gets credit for it and
there are different schemes there's last
touch which is problematic because you
can game it quite not easily but people
game it there's a lot of research right
now in multi-touch retribution which
borders on really asking a causal a
question of who drove the conversion but
it's a very ill defined cause a question
so we can have a whole one hour seminar
on attribution I don't want to go there
it's a bad problem and the solutions are
even worse in a three usually it's last
touch meaning whoever was the last one
to show an ad now I don't know how
different channels like search feature
into this but the vast majority is last
touch ya know excellent question
business model is actually in bulk I'm
paid a buck so city comes to me and says
here on a thousand bucks go have fun by
the way we agree on a certain CPM cost
per thousand that automatically
translates the budget into a number of
ads I have to show the performance at
this point almost doesn't matter until
we get to the renewal and if your
performance wasn't good enough compared
to some other company they hired they
cut you out of the plan most deals that
we have our of that
CPM so the proof the performance is
almost a secondary metric all right so
let's move on that so if I actually care
about performance because the price is
what it is right there nothing to do
with that what in principle I'm supposed
to be estimating I want to calculate the
probability of purchase given who the
person is inventories in this case the
New York Times meaning where the ads
shown or or kayak or blog and for people
who i shouldn at you and this means a
new customer so we don't try to do
retargeting and show you an ad for the
product you already looked at or bored
but we actually trying to get you to buy
something that you haven't bought yet
okay now what's the problem with this if
you followed me I wasn't very specific
we track about 100 million binary URL
indicators so my data has about the
dimensionality of a hundred million and
conversion rate is typically low no
matter what product you look at unless
it's pizza people don't buy it very
large amounts of them and even pizza
people who for the first time by a pizza
a very rare so that doesn't count even
as an example so conversion rates are
really really low I don't have an awful
lot of positive examples to start with
and actually some really picky here
technically picky I have no examples to
start with because I'm sure an ad yet
I'm supposed to spend these hundred
thousand bucks but before I've done so I
haven't shown an ad so I can't estimate
that part so all right here is a step
back let's take a look at the data we
collect what we have and that is to the
question earlier we basically take the
URL and hash it so I get something like
a time series of these odd hash evens
and they're just indicator so I'm not
looking at this text on the page so I'm
completely anonymizing not just
user because the user just gets a number
I also make the actual browsing history
very abstract very anonymous so it's
very hard to actually know what that
person is I can't tell you so it's
almost on purpose that we're trying to
recall this privacy by design we're not
trying to label people based on their
behavior they're not credit card in
tenders but I'm really collecting just
these URL indicators and purchases are
locked separately and so what this means
really as a learning problem is i have a
huge dimensionality array here up to 10
million very sparse very few urls any
given person ever gone to and to make
things worse I don't have many positive
outcomes either I wouldn't call it
convenient but it performs very well in
fact every time we try to add contextual
information then help because the you
are all kind of tells me it's like an
the identifier contains all the
information is just a matter of do I get
enough positives to extract it but
anytime we have tried to change that and
augment it with additional information
it didn't seem to improve our
performance
it doesn't matter at all it's the
irrelevant so it's just a binary
indicator and by the way we also ignore
the time series of it we're not looking
at the stuff you've done more recently
versus although even that didn't seem to
change performance much yeah correctly
I'm I'm completely ignoring any possible
structure that might be sitting behind
it now I'm not saying they are
independent I'm pretending they are
every rose a person is a cookie for
every cookie I have observed that cookie
on a subset of URLs in which case again
one otherwise a zero typically on
average somewhere between 20 and hundred
nonzero not at all I'm going to ignore
that entirely as well and it any attempt
we did on using time hasn't led to major
performance improvements didn't seem to
matter yeah look we don't we ignore that
as well it's slightly different for the
bidding the URL for the bidding the
information we get at bit time there we
use a lot more in terms of augmenting
collecting your browsing history we
ignore it so what I'm in principle
trying to predict is the probability of
a purchase or wherever the client puts
that conversion pixel I almost don't
know it's like every client comes to us
and we agree on this is the outcome that
you are after that's the outcome you
want to measure us against and for
instance like pottery barn yes it's the
track out page at the very end of the
purchase okay
that currently doesn't contain
information that's worthwhile or you
think that you just haven't figured out
yet no it doesn't contain information
given what I already know the
information i already have based on the
IDS on the URLs the contextual
information is not informative
marginally on top of what I already have
that's what I'm saying I'm not saying
you couldn't build a model from the
contextual stuff saying you can't build
a better model than the one I have no
literally it's the it's the URL ID so
here what does the column I don't care
no I know what you do that tells me a
lot more than who you are then anything
you're willing to tell me about who you
are you are what you do exactly that's
my that's my point here we tried adding
demographic information as well and yes
it also didn't work yes we have
consistently failed for the last five
years to build a model that beats this
and we have tried yeah with a different
story that's a different business model
they are there are companies that
specifically have focused on retargeting
video is one of them they play a
different game it's a different game and
it's actually measured hopefully
differently as long my task is I'm
trying to find new people that's what
you hire me for all right let's keep
going because some money so how do you
do that well the reality is we solve a
different problem than the one I told
you I want to be good at so the good
news is a machine-learning cheating is
called transfer learning so you're in
good hands here and there's a long
literature on that and how to do it so
instead of trying to predict this or
find myself a smaller problem no it's
not much smaller but notably smaller I'm
going to say ok I will predict whether
you go to that brands homepage and I
don't care whether they buy or not
forget that I mean whatever the
conversion thing is I'm just going to
check whether they are even interested
in this and i also don't care whether
they seen an ad or not
let's just figure out what kind of the
organic propensity of going to that side
and being interested now the upside is i
can do that pre-campaign so now I no
longer have to show ads right so I can
built this model the moment I have a
pixel on that brands whole page so I
have something to target when i start i
still have the big predictive problem
with a lot of dimensionality so let's
look at this a little bit more when not
as i said looking at new purchases but
we look at any kind of site visitor new
or not it's the first thing um we're
doing the things you really shouldn't be
doing a machine learning never do that
unless you have to we pull the positives
differently from the negative we take
everybody who goes to the website as a
positive and then we pull some rather
some random set of other people that we
haven't seen on the website yet as
negative which is really negative looks
like going in a hospital in getting
yourself a bunch of cancer patient and
then walking down to Washington Square
on the corner and grab a bunch of
students that you run into as a negative
set and then try to build a classifier
for cancer or not I don't recommend it
that actually only works because we have
this kind of funky representation and I
can geek out on that topic they want to
yeah they exclude so we actually we do
track whether they were positives since
we have seen the pixel I don't know if
you really never ever ever in your life
bought pizza but since we have the pixel
up on the pizza place we check that we
haven't seen boo you there yet since
then um negatives are everything else um
we will try to fix it later on so there
is this piece of transfer learning and
stacking that we'll get into now we have
a couple of really brilliant people here
who know how to do this I'm not going to
go into much detail of how to actually
do that modeling on the system we have
built is a logistic equation it just
works with that very sparse
representation and 10 million dimensions
it's using stochastic gradient descent
comes with the usual bells and whistles
l1 l2 constraints depending what you
want so you have fake dimensionality
reduction because we want our models to
be preferably smaller just for kind of
speed of retrieval there's all this
story about learning rates and so on
people in the room I know about this
again stack of literature on this arm we
can initialize models with industry
priors we already have that we have kind
of derived from previous campaigns in
the same domain the models are not
rebuild every time but they have like a
streaming update so every day we just
collect a new set of positives go find a
new set of negatives and then starting
the stochastic gradient descent from the
last known model and just incrementally
updated with the new information that
thing is fully automated we build about
10,000 of these per week and nobody ever
looks at them specifically right I mean
we're like seven people that they
assigned screw like I can't look at any
of these this thing just runs in the
background and constantly updates near a
lot of tests around is the model okay
like there's a lot of validation steps
in between that I'm not going to get
into but this is basically the system
any other questions on just the
targeting yeah
what kind of structure is being imposed
when you say so you have many different
sites for which you're trying to predict
the probability of cycle right we have
our here's the thing if you were to use
the information you got from the Audi
website to run a BMW campaign you only
do this once this is imposed it's a
legal problem even if I thought that it
would be great news to use kind of
competitive information this is entirely
siloed so the outcome meaning the
campaigns were running they're
completely siloed and they're treated
entirely independent and they do not
share any information so that's where
the grace zone starts where we have
pixels on some campaign sites but part
of that data that we buy kind of brings
in information about people going to
brands as well and that stuff we paid
for it belongs to us and that can build
kind of that background in industry I'm
saying for some of them we have so the
hash isn't isn't like necessarily
complete for a subset we have some
information otherwise I can't really
show you anything detail by the way how
much time do I have okay there is after
this first model what we call a stacking
event where we feed the predictions of
the initial model into what's now a
correct sample so a week into the
campaign we have seen a couple of
impressions right so now we can actually
build a model on the people we should
add to and this is a kind of a random
subset as kind of our control group and
we can estimate the same thing where we
feed it the prediction from the previous
model it's like classical stacking
sitting on top and this plot to your
shows if you look at the scores that the
first model gives us and higher is
better here and it's not to calibrate a
probability the people that the model
has the highest confidence that they
have high
probability of buying actually don't do
very well actually not the people this
is a calibration plot who truly convert
very well and that's an artifact of the
spire sampling and the other kind of
less than crucial things that we did
here so there is a second layer that
kind of fixes that part that accounts
for some of the nonlinearities and the
bias sample that we started with by
taking the right sample the other thing
is I can also be much more flexible here
with what I'm optimizing for so the
couple of opportunities all right so
what happens well once these models are
done we have them stored in the system
every time we have a touch point like a
bit request we check when was the last
time we scored that person and if it's
longer than 24 hours they get submitted
to a scoring queue that scoring cube
takes the browser looks up and this is
now not in real time this is a huge
system looks up all the browsing
information and score stead cookie
against every single model we have in
the system across all brands and then it
stores only those brands for which that
cookie was in the top three ugh in the
first presenter so we pre-select who we
consider interesting for any of the
campaigns were having at the bidding
time so when the bid request comes in we
basically now look up not directly the
probability but this kind of
pre-calculated percentile that we have
and we do some adjustments for the
inventory meaning where the person
currently is so Duncan asked early on
saying if you're right now on kayak and
you are in a high percentile for trouble
campaign there's a good chance that the
basic bit price we have for the campaign
gets increased by a factor of 5 10
because we know empirically and this is
again estimated that when you're on
kayak your conversion probability is
much much higher so it's kind of
to the 10,000 inventory ideas that we
have broken out separately so
dimensionality of this thing is smaller
well for the really small ones we just
don't have when we'd go with the base
factor if you get enough information to
make a smart decision do it otherwise go
with default it's kind of how this works
for the scoring well in a perfect world
I would love to score a person every
time I see them every time I get a new
piece of information I want to score
them but we have like 200 million
cookies going through ingesting them
that how many computers you want to own
in order to do that 24 hours is good
enough the marginal benefit of scoring
people more often I'm not sure I mean
you clearly have some kind of tapering
off there
do you find that the same people keep
showing up and that top 1% would you get
simply cover each other
for the most part it's pretty stable so
I would say look either you're our
runner or you're not a runner and if I'm
running a campaign for Nike unless
you're remotely interested in running
there's no point right you don't change
your behavior like whether you're
running or not that vastly so there is
some movements but for the most part we
don't deal with those okay keep keep
going where you're going yeah do you
have enough people oh I'm about 300
million people right maybe so that's
full of milling cookies and you have
probably multiple cookies for me and I
just see a different version of your
personality in the different cookies so
when I say top one percent that leaves
me with three million for most budgets
that's perfectly fine if we have a
bigger campaign what happens I'm saying
okay maybe I need the top 5% if it's a
really huge budget campaign maybe it's
the top 10 so we basically just push
down the cutoff on the percentile as a
function of the size of the campaign
plenty okay overlap of all the one
percent I see how much is left over
there wasn't a name how about I can tell
you this of all the 30 billion bit
requests we only bid five percent of the
time which tells you a little bit of how
much room there is so right now with the
amount that the delivery we have to
provide we only bid five person that
doesn't really answer your question
because it might be they're the same
people in all of the campaign's um the
internal competition we often end up
with you qualifying for two or three
campaigns and if I see your three times
a day then I can show you one for each
so I don't think there's a lot of
cannibalization going on as it stands
eventually if we actually were to grow
the company x 10 x that's a problem then
we have to maybe find more inventory
think there's also
you mention clicks right I think there
is a nice twist and based on the fact
that the actions because they're not
clicks yeah the actions that are taken
are specific to the brand so so the
problem people who visit every right
every brand across so four clicks is
actually worse for clicks his concern is
actually very valid because it's a much
smaller group of people who click on all
ads people either with eyesight problems
or but there aren't that many of them so
there is more overlap here because it's
really product specific I don't see this
right now I mean between the pizza
campaign and the running shoes and the
credit card they're really quite diverse
absolutely possible but there many other
ways to get to the page usually and you
don't distinguish for example between
and had on kayak in the morning or
evening or viewable ad so viewability
goes into the bid strategy we have bit
strategies that again increase the price
when it's viewable but only for
campaigns where the customer cares about
viewability our morning and evening not
specifically I we have actually looked
at that in terms of the incremental
signal you get from that given that you
have a seven-day conversion window
whether it's morning or evening almost
doesn't matter as long as I know that
you convert within the next seven days
again the time scale here makes that
irrelevant unless you really try to game
last touch so for the last touch
retribution it becomes a bigger problem
and maybe we're not doing our due
diligence on the timing in that context
if it's just about the conversion rate
and seven days doesn't matter
something hotel and it's you know
whatever look if the client is hell-bent
on spending that money right now with us
what am I going to say come back next
month that's not my choice to make all
right so how does that work now relative
and so um this is a plot it's kind of on
the more geeky side so here you have
scale in terms of millions of browsers
there you have performance in terms of
conversion rate it's a log scale on both
and what we show here is for these
different buckets that we have
identified the green ones are our
prospecting groups so these are people
that we think are good prospects for
this specific this is one brand here I
don't know which one but if I knew I
probably couldn't tell you the red
points are the retargeting this is where
some of our like other companies we can
do retargeting if we have to but we
haven't specialized or not and what you
see for retargeting on the scale is much
lower because they only that many people
have already bought it conversion rates
much higher the prospecting this is the
the green stuff here some of them is
small but generally the scale much
larger and we can kind of scale this out
as far as we want but for this campaign
the conversion rate is kind of not below
and then here you have what we call a
third party the gray stuff or segments
you can buy like a credit card in
tenders or I don't know soccer moms
whatever your heart desires they tend to
also spend about the same area on skill
in performance because they are not
specific to that product the performance
ends up being notably lower than the
prospecting the yellow are somewhat
generic of these industry models that we
have and there's kind of a mixed bag and
don't want to talk about mobile infusion
it's basically people that we've
identified based on the places they have
gone with their mobile device but that
gives you kind of a relative scaling of
performance versus the number of people
you can reach
with this system um all right this
explains the same thing how well does it
ultimately work in our internal
measurement where we look just how the
model compares to a non targeted group
meaning if I just pick people at random
and show them ads for the same campaign
the median lift is around 5 X now come
there are some campaigns that are really
hard these are products that almost
everybody has so things like credit
cards are very very difficult to get a
lift of two and the credit cards amazing
it's because simply because almost
everybody has a decimal signature a
campaign do you do really really well on
one over here that we didn't show it's a
supplier for scrapbooking material just
close your eyes and imagine the audience
of people who are interested in
scrapbooking of supplies they're very
easily identifiable and in terms of lift
if you compare it to random base rate
get a huge lift easily 100 so this is
just kind of across many different
campaigns what does the list that the
campaign's typically have all right this
is pretty much when I switch gears and
talk about the next topic being fraud
and then finally some causal is there
any more questions about just kind of
the prospecting piece here and how this
whole system works alright and let's go
in the fun stuff um rule number one
every metric that can be faked will be
faked just assume that to be true and
that holds for almost any outcome you
may want to measure the cliq we see
plenty of ads that we show in the US and
the cliq comes from Asia I don't know
how this works but clearly many of these
things are gamed viewability somebody
tells you it's viewable hundred percent
of the time I promise you this will
increasingly be gamed if you achieve
hundred percent viewability for sure
you're not showing it to real people
because you never know whether the
person Scrolls down there's just no way
of knowing
um video completion it's another one
that we see a lot of gaming around it
and today BOTS also starting to get
really really good at filling out forms
and doing other things what they
typically don't do is buy stuff which is
what you can't fake but then it's also
so rare that's very hard to optimize
against so let me tell you this story
spring 2012 has been awhile since but
still kind of the same fundamental
concept here the performance of one of
our internal metrics doubled so the lift
across all campaigns over period of two
weeks the median lift doubled and we
hadn't done anything now usually that's
good news but in this case um maybe not
we started looking into it and this was
data we got from the bid requests where
all of a sudden if a cookie was seen on
women's health base they also then we're
extremely likely to buy pizza look at
luxury cars or download microsoft
products look at credit cards and
potentially visit the homepage of
sheraton now i can't see any possible
explanation of how this ties together
this kind of coincides with a huge
uptick in what people call now
non-intentional traffic and it's not
exactly but it's basically malware
that's sitting on real people browser
software that generates HTTP traffic in
the background so let me say non-human
traffic if you look at this it means you
just didn't want to go down you probably
don't even know that your computer
requested that site what we're going to
look at the traffic patterns where we
look at overlap between websites so in
the next graph the red dots are websites
and there is a connector if at least
fifty percent of the cookies who went to
this website within a short period of
time also show up on the other all right
so
looking at where these cookies go um
this is kind of a graph what the world
looked like in 2010 so let's look inside
this was here the little thing down
there that's the Boston Herald and
Boston search it kind of makes sense
there's an overlap of after up to fifty
percent between those websites right
they're all about Boston somebody who
goes there who wants to read something
so this is probably human behavior this
kind of cool visitation and overlap um
what about women's health space and
these other strange things right there
there's women's health base in the
middle of that cluster imagine how much
time people must have they're looking
not just at women health based but all
these other sites are visited by cookies
who went there I also want to point out
the very diverse interests here try not
on TV wrestling news you take your pick
again a connection means fifty percent
of the cookies from women's health base
also went to however you spell this
thing on wrestling news so this is
basically the bot traffic this is
non-intentional there is some process in
the background that generates these
visitations to these websites and the
websites I argue have been built
specifically to then sell ads on them
through the exchange that are never seen
or at least not seen by any human so
when it comes down to you're trying to
decide when the bid request comes in is
there a person on the other end and that
the person really want to see this side
before you make a decision to bid now I
can't really show everybody in capture
before I show you an ad would be really
annoying so this is the progression into
2012 and I haven't made a new plot since
but it probably doesn't look any better
um now what has this to do with
performance so far I haven't told you
anything that would explain why
performance of our system goes up right
it's just more traffic well the owner of
the butt
who's working with the guy who owns
women's health base figured out that you
can get a lot more money for the same ad
space if before the butt goes to women's
health base it visits a couple of brand
websites why is that because that's
called cookie stuffing now you enter the
retargeting pool where people bid a lot
more money on throwing you an ad because
usually on retargeting the prices are
higher the conversion rates are higher
so that also means that the web
analytics of those sites are probably
questionable what worries me is that now
since I'm modeling against these events
to my models learn how to identify parts
because BOTS go to homepage is much more
often than we are people and it's really
predictable because bots are semi
deterministic as compared to the usual
kind of human behavior so if you look at
conversion events or pixels of people
track to measure the effectiveness of
campaigns more than fifty percent of the
clogs Hispanic home pay home care video
completes were bought of course whether
you actually complete a video is a very
good metric right means that you were
really engaged in the app or that you
were up your butt and you see pretty
much to who who is who on this list in
terms of the what traffic that you see
going to these sites so what we build us
what we call the penalty box where we
exclude anybody that we see on these
sites that have very high overlap
meaning we are not recording any
information so that our models can't
learn from it we don't recall the
outcome and we don't record the sites
that they go to now recently all of a
sudden the New York Times showed up as
having very high overlap which is
disturbing because either they are
buying bot traffic which I doubt or
something is wrong with my algorithm
while it turns out this is spoofing
there's a somebody pretending to be the
New York Times and selling ads in the
exchange under the URL New York I'm is
except that they aren't here you see the
the traffic so these are the number of
bit requests and this is one exchange
this is the regular amount that we get
which is indeed an inventory of the New
York Times and then one exchange all of
a sudden had huge spikes in availability
and this was a spoof website that just
pretended to be the new york times and
again visited by boss don't pay for them
or you actually no no so we work with
aggregators that give us access to these
10 million sites no but yeah I'm not
even sure they know and have the
technology all right last topic for
today unless the animal questions about
the kind of Watson and fraud story last
topic of today I want to touch on is
what does it even matter whether we show
ads if you listen closely to me all I've
basically said so far that I able to
find people who would buy that thing
anyway whatever that thing is right I
haven't had all spoken about the fact
that I'm trying to measure whether i can
bring people to do it or how much i can
influence somebody's willingness to buy
something I'm just good at figuring out
whether you would have bought it anyway
um so we're looking again in this
context of having post view conversion
within seven days now if you just look
at the conversion rate of people that i
should add to and people i didn't that's
obviously not causal if i'm worth
anything I mean if I did anything right
so far of course I can find people who
are more likely to convert that's what I
spent the last 40 minutes convincing
Europe right so this is clearly not a
right measure of the impact what you
need to do is you have to account for
the confounding that is introduced by
the targeting that I've put in place
here so one option is if you really want
to know whether it worked you can hire
Nielsen and Nielsen will assess whether
or not you
was effective right because they will
come up with a comparable group and they
will match it and then they will track
things and this was a campaign for
Clorox nobody buys clogs online anyway
so they are tracking it to offline bias
and stores now let me zoom into this so
the blue line is the number of boxes of
Clorox bought by people we should add to
and the yellow is the number of boxes
clocks bought by people we didn't show
add to that they had matched this is the
memory test do you remember when the
campaign started anybody let's memory or
based on this graph all right if things
worked well where this term this graph
should the campaign have started much
but guess what it didn't so what does it
tell you that Nielsen can't adjust for
the confounding that I have introduced
because I know a lot more about people
than nielsen dies i mean people and
nielsen things it does matching correct
it actually knows less about people and
as a result we knew all the way along
that we had the better audience and so
the seven percent lift that were
attributed to our campaign I feel very
reluctant to take credit for I mean look
the customers happy I'm not going to
complain but maybe the lesson is that
you may not want a higher Nielsen if you
can really care about the answer that's
a different question I'm not going there
yes this was actually okay don't tell
anybody I showed you this is a tech we
got from Nielsen let's literally pulled
out of a news report for our campaign
but they actually sent me this data that
puzzles me
apparently right in line with one
another sort of remark so it could be
possibly that something else but then
why was I able to figure out who else is
in the market who else is advertising
still doesn't explain why that's a
correlation with me unless I'm better
than I think because i don't know that i
think i was saying that maybe they're
targeting for the exact same reasons and
so they end up targeting might be in any
case this is not really the answer to
the question at least i don't think
that's the answer to the question
whether i cost any more clorox sales
whatever is answering so what I don't
have time to go over but what I want to
emphasize here is that a bee testing is
difficult and accounting for things it's
also too expensive for most come most
brands to enter be willing to engage
instead what we looked at this can we
measure causal effect from observational
data during our campaigns with the
targeting and using analytic solution to
adjust for the confounding that's
introduced and I will be very brief on
this basically this is kind of on from
the Berkeley School the outline
typically in terms of baseline variables
then you have an interaction this is the
ad or not and then you try to estimate
how this accounting for the all of the
baseline variables which explain the
confounding hopefully if you have
accounted for everything which you don't
really ever know and how it affects your
outcome and so this is the Beijing
formulation of this what we use is
targeted maximum likelihood in
estimating this I don't have time to go
you into the details if you're
interested please let me know and i'll
send you something first thing whenever
somebody claims that they can go to
causal estimates do a negative test do
pizza add change the rate at which
people sign up for credit cards they
better don't because of your pizza at
change the rate then you clearly haven't
accounted for something or you're
actually measuring some unintended
effect so here actually know what's a
telecommunication on pizza and what you
see in the estimate is the p-value so
the difference in the conversion rate
with adam without
completely negligible so a negative test
works out we did an a/b test for this
particular campaign just a couple of
weeks prior and we actually see that the
observational method that's running
purely on campaign data gets very very
close to the actual effect that the a/b
test showed here you can do that now
once you have the machinery you can
measure for almost any campaign what the
true effect that you had on the campaign
and one of the things that you see here
we did some retargeting that the
creative should be matching the audience
so if you have a retargeting campaign
you should use a different creative you
see a huge impact here and for the
prospecting that creative was completely
useless now we found this thing nothing
going on whatsoever really rare we
deducted for a while let me show you the
creative any idea what we're appetizing
a hotel chain so you might get people to
click on this doesn't mean they have a
book a hotel room with you so this is 15
years of click optimization affecting
creative design which has pretty much no
impact whatsoever other than getting
people to click on it finally and that's
my closing slide how does what we do
finding people who would convert any way
relate to who are the people who are
most impacted by the ad because it's not
obvious what the relationship is what we
look at here is this is the basic
propensity of the people to convert so
high over here Lou over there on the
y-axis you have the marginal effect of
the ad and all those super noisy the
good news is that there's generally a
positive trend which means that people
who are by nature more interested in
your product are also the ones who are
most affected by the ad and if somebody
doesn't care whatsoever about scrapbook
no matter how many scrapbooking as you
show them they will not change their
opinion I'm not going to debate kind of
the incentive problems of the industry
at the end if you want to share this
here are the papers that you can go if
you want any more details on any of the
things we talked about the publications
we have on the topics all right you're
the youth felt that the options were not
second-price actions what do you think
so and do you think there's any way for
you to yeah so the the great news about
second price auction is that you don't
have to worry about it and you can just
bit what you think it's worse to you and
you're good the reality is that's why I
don't think that it is a second price
auction because they are far too many
instances where we pay exactly what we
bid chances of this happening in a
second price auction hello so certain
exchanges and inventories I see eighty
percent of the time I pay exactly what I
bid then with some back like some
engineering you can often see
relationships you always pay ten cents
less than you bid or you pay kind of a
certain midpoint of the distribution you
find spikes in the distribution of what
you end up paying there's no reasonable
explanation for that now they are now
called dynamic floors that's the
official way of saying I'm not doing a
second price auction except and I don't
tell you where that floor comes from or
how I calculated so it's very hard for
me to now do smart bidding decisions if
I don't really understand the mechanism
we do some experimentation around us
right now to understand that better we
don't have the solution i would want but
it's in the works
the last time to sort of discovered the
fraud yeah
ignore that part yeah oh yeah I mean
this is like the spam problem that
people constantly further develop the
way these things are they evolving and
I'm not saying that we have the solution
the advantage of this approach is the
bots require scale you only I mean you
can only really make money with the bots
business and putting up these sites if
you get scale so they have to have high
loads of traffic we keep looking knowing
that somebody is trying i but it might
very well be that I don't know in the
other problem is what's performative oh
if you look at viewability or click rate
or homepage visitation I mean I'm pretty
sure that a lot of people are very very
happy advertising to bots because it
makes you campaign look great probably
as good as mine is all right i mean
there's a huge industry on ad fraud
right and there are there is actually a
huge industry now in dealing with that
fraud so why tops and integral and so on
or dealing with it right so for any of
us who are doing research that's using
online advertising data you better be
really really cognizant of the ad fraud
because it may be the really reason you
get really good results or essentially
because you're modeling something that
is but a much more deterministic that
you would behave
also means I'll be in business for quite
a while do that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>