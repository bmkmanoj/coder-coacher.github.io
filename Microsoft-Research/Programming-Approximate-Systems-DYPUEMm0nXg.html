<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Programming Approximate Systems | Coder Coacher - Coaching Coders</title><meta content="Programming Approximate Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Programming Approximate Systems</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DYPUEMm0nXg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
good morning my name is Karen stranson
here architecture group and in today
it's my pleasure to introduce Adrienne
Samson adrian is a student at a PhD
student at university of washington is
currently working with lisa degrees here
in the audience today and then Grossman
and adrian is no stranger to MSR has
actually done two internships before one
with me and one with Katherine McKinley
and today is going to talk about
programming approximate systems hey Jake
thanks so much again hi everybody yeah
so I'm gonna talk today about
approximate computing which is what I do
and as a PhD student can everyone hear
me okay this is cool good all right um
so I have a do a little bit of
background on what approximate computing
is if you've never heard of it and the
sort of programming language aspect
which i think is one of the organizing
forces between behind how approximate
computing should work in the future and
i'm going to do a sort of high-level
overview of a couple of the the
component projects that we work on
because there's there's there's like
many different ways you can you can
think about approximate computing i'm
just going to talk about two to keep it
short and this is a pretty high level a
talk i've only gotten but the sort of
the the most interesting five percent of
the of the idea but if you but that that
means that i would like you guys to slow
me down if there's any technical stuff
you wanted to talk about in more detail
so just interrupt me if it ever seems
like I'm I'm glossing over the important
things sound good cool great and so um
about Brooks my computing the the idea
beyond approximate computing is it is
that lots of the applications that we
that the most important applications
that we can think of today have
something in common if you think about
applications like robotics or things
like large-scale physical simulation or
them some of the most important
applications of all mobile gaming
there's a there's the common thread
among them is that there is there's not
a hard-and-fast notion of what correct
means is that there's there's a certain
tolerance
a little bit of variation in how the
program behaves if a few small things
change in your climate simulation for
example that doesn't impact the that
doesn't fully compromise the overall
usefulness you can imagine a few bit
flips or a few rounding errors not being
catastrophic when you're talking about a
sprite in a video game being a little
bit off that the game is still useful
and it might even still be playable if
the computer is allowed to be a little
bit wrong when it does the computation
involved with the game and it's not just
these applications you can keep on
thinking of more and more programs that
have this property of error resilience
at the application level and few and if
you're like me and you've been thinking
about approximate computing for a while
like once you have a hammer the entire
world of applications start looking like
nails and that you can you can start to
see this error resilience present in
lots of the applications that you care
about and one of the but it the idea
behind behind introducing errors
intentionally into the way the computers
behave has obvious problems right so we
you might imagine for example an image
render that when you inject errors into
it does something like this in
particular what I mean is that in the
upper left we have a output of a very
simple ray tracer and it's completely
correct I just ran the program on a on a
precise computer and then the other
images show varying levels of error
injections so during the execution of
this program I flipped some bits during
some arithmetic operations and during
some storage and in the lower right you
see a pretty bad outfit from that but in
between there's some there's some
outputs that look okay and the goal of
course is that we want to choose
something that has not that many errors
but can save us a lot of energy what I
mean is that we can what if we could
design a computer that when it's allowed
to be wrong can be way more efficient
than a computer that we that we forced
to be completely correct all the time
blind to the fact that the application
can actually tolerate then so we'd like
to select an output that makes a good
trade-off but in reality you probably
want to yourself like why why does a bit
flip only affect a few pixels in the
output and that's in general that's not
true you can imagine that sometimes when
errors incur
are randomly distributed through a
program the there's no graceful
degradation from completely precise to
not very good it looks the outputs don't
look like that that that slide a while
back they would look something more like
this that is that that bit flips do in
certain parts of a program can cause
catastrophic failures that is that they
even one single bit flip and a pointer
for example might cause the entire
program to go completely wrong which is
why approximate computing is challenging
is it that we want to find a good
trade-off without compromising the
essential reliability of the program and
that motivates a lot of the work that we
do in approximate computing trying to
make it feasible for for people to
reason about how errors can impact a
program to exploit these quality and
efficiency trade-offs that we think are
present at the hardware and software
levels without completely compromising
the way that application works so the
the work that we've been doing at the
University of Washington and some other
people at other universities have
started to look at has as really had to
address this from a whole stack
perspective it doesn't make sense just
to think about hardware for example and
just to say what are some ways I can
make hardware more efficient while
allowing a few errors it doesn't it
doesn't work that way we need to
incorporate properties of the
applications and the way the programming
languages work and the way that
compilers interact with the hardware to
to make good trade-offs and that's what
I'm going to talk about today is a few
points along this this system stack and
some of the things we've worked on in
the passive in programming language
called energy that lets people express
approximate programs and to enforce the
safety properties they sort of that try
to address the catastrophic failure
question that I brought up before and we
were also arrested in ways that
compilers and hardware can get involved
but in particular today I'm going to
talk about a little bit about background
on energy which is the the programming
language that we that sort of ties this
all together and then a little bit more
on more recent work on storage
approximation for storage in phase
change memory which is about to be
presented next week at micro
and after that I'm going to talk about a
compiler based approach to all software
approximation which is unpublished and
super new work so this is the outline
for what I'm going to talk about any
talk about just a little bit of
background energy and then some hardware
stuff and then some software stuff
anyone have any questions at this point
and i left the the gray things on the
side just in case you're interested in
talking later about about cpus doing
approximation in cpus or in accelerators
we have an interesting idea for using
neural networks as accelerators for
approximate programs cool great thanks
so let's talk a little bit about the
programming language of perspective and
the reason that I'm bringing this up now
even though it's it's about three years
old now we publish this in POV I 2011 is
that it really it captures I think I
think a programming language is really
necessary for letting programmers
express approximate programs currently
when you write a program there's no
notion of air resilience in the way that
you write to the program and we need a
way to get around this problem that is
to say we want to relax programs want to
allow in correctness without allowing
this type in correctness we don't want
to allow crashes or arbitrarily bad
behavior we would like to guide the
program to a to a sort of continuous
trade-off space between accuracy and
efficiency any way we do this is to to
ask the programmer to divide the program
into the air resilient and be in the
critical parts of a program as you can
imagine things like jump targets and
pointers on the one hand being needing
to be completely precise and we'd like
to isolate those from sort of a bulk of
the data that the program is working on
for example the pixel values in that
right here sir example that I showed you
or the frames in an image render and we
would live in the ideas we'd like to
keep the precise part of the program
precise while allowing this is the the
bread and butter of the program via the
actual data that's been computed on to
be relaxed and Lisa
it's a less important than the most
significant bits of pixel data say
that's an interesting point this this
distinction is all about what needs to
be completely precise versus what can be
approximated at all right so that's what
I would your question about whether
about certain bits being more important
in other bits as sort of a question of
degrees right so once we decided
something can be a little bit
approximate the question of how much can
it be approximate I'm actually not going
to address that in this project but
there's a few other let's talk a little
bit later about how you might control
that but that's in it that's a very good
point right is it right now what I'm
talking about is finding is this sort of
first cut which i think is is that the
first sort of design decision which is
where can s be allowed at all even a
little bit where even a little bit of
errors can cause precise things to go
very wrong and then the rest we need
some sort of tuning system or something
to decide how approximate that could
possibly be and what I think that a
programming language can help
programmers do in this situation once
we've asked the programmer to make this
distinction between critical and
non-critical parts of a program is to
help isolate those that is we want to to
prevent the programmer from accidentally
shooting themselves in the foot and and
taking the these taking data from the
wild west of the approximate part of the
program and letting it flow into the
precise part of the program causing
arbitrarily bad behavior the second
thing that if that that a programming
language can help do is to provide some
degree of generality lots of people have
lots of ideas about how to do
approximate computing about how to make
efficiency accuracy trade-offs but it'd
be great if we had a single concert
something unifying in the language that
that was that a LED programmers just
express approximate pneus in the
abstract and then the runtime can map
that down to any type of approximation
that you can imagine for example we
we've been interested in ways that you
can approximate the logic in CPUs maybe
by using a lower voltage and allowing
timing errors to happen sometimes I'm
going to talk later about its storing
data approximately allowing a few bit
flips when you read data back after
storing it and also about approximate
proximate algorithms and not nothing
involving the hardware at all but a
different algorithm that is less
accurate than the completely precise
mutation so that's what energy tries to
provide is an ocean of safety that is
protecting critical data while allowing
approximation in the bulk of a program's
data and generality that is a using some
sort of single language construct to map
to express approximation in the abstract
and make sense as a motivation for an
approximate programming language cool
the answer to how we accomplish that is
actually very straightforward the idea
is to add type qualifiers to the
language that mark data elements is
either approximate or precise so here is
a tiny bit of pseudo Java where we have
two variables and you can just mark any
type in your programming language is
either approximate or precise and the
the isolation property the the the flow
invariant that we want to allow precise
data to flow into approximate data but
not the other way around is just
enforced using the type system so if we
can make our subtypes work correctly so
that you can assign precise data into
approximate variables but not the other
way around this prevents you from taking
potentially corrupt data and letting it
flow into data that you marked as
precise to make this a little bit easier
we make the the precise qualifier
implicit meaning that you can take a
normal Java program and compile it as an
energy program and that is a completely
precise energy program with the same
semantics as the Java program but if you
you can incrementally add these
approximate qualifiers to allow to
selectively allow approximation where
you think it might be a might be
possible and the way that usually works
is you look at a program and you find
that bulk of the data that pixel array
or something new market is approximate
and you follow the type errors that the
compiler gives you 2222 mark all the
things associated with pixels as
approximately 0 in this program better
do this that is to say is the program
can to implicitly precise it is yeah and
it's a good sign right now obviously
will be I would always be thrown out if
I said eBay is bigger than 0 right yeah
so but it but the yes written so but
what that means from the friend language
there is that conditionals have to be
precise in our language that's the way
we get around implicit
you can imagine more complicated things
but this actually tends to work okay
yeah use the switch or whatever some
sort of simultaneous switchover
thresholds on a see to it fine providing
you say and some of the A's go to the
left and some go to the right yeah but
that's a really disappoint right where
if you if you if your control flow is if
you if you can show the control flow
only effects approximate data and then
you can imagine a more sophisticated way
of reasoning about how how conditionals
could influence the other approximate
programs but for the moment we don't
even do that right like I guess the
simplest example is you have if
approximate condition assigned to
approximate value right that's probably
fine because it totally makes sense yeah
and and at the moment we don't do that
that would be the solution is obvious
but they but for now we found that it
was fine 02 to just say please don't do
approximate conditions but of course
there's a way to get around that which
is we provide an escape hatch to to let
let programs explicitly mark the points
where this flow invariant can be
violated we put endorsements into the
language which mean that which acts just
as a typecast to say this this value of
approximate type is allowed at this one
point to flow into approximate values
this lets the programmers sort of
isolate the points where things need to
be need to be checked to make sure that
they they make sense as they flow from
the approximate part of the program into
the precise part of the program this
example in the slide is one pattern that
we see a lot which is to say you do
something expensive and then after that
you want to do some sort of cheap
checksum when approximating that
checksum would be bad for two reasons
first of all it doesn't buy you much
because the checksum is cheap and second
of all you probably want that checksum
to be correct even if the input data is
approximate so at that point you want to
endorse the value to make sure that it
can it is allowed to flow so we we
provide this endorsement constructs
which which just
explicitly lets the programmer say this
is fine I know what I'm doing that makes
sense that's a really high level
overview of this of the the programming
language energy that sort of ties the
other two projects that we're talking
about today together when I've left out
a lot of details about how the language
works so if you have any questions now
is the time you're going to get into
examples of the Opera or something like
that that we can see I don't I don't
actually have example output slides here
except for the example you saw earlier
in the program that that ray tracer was
a real energy program running on
simulated approximate hardware and
showing you that they could so V the
examples with crashes those were made up
but the ones with the with the graceful
degradation that's a real energy program
that we annotated to to allow that trail
how much what's a degradation you want
right yeah in this case I was running on
a simulated hardware with a number of
different approximation techniques and
each of them have a parameter the
question of parameter selection that is
how we're in the trade-off space you
want is an interesting one and we'll
actually get to that in a third section
of the stock cool great um so the next
project I want to talk about is is
something we're working on lately from
and this is more from a hardware
perspective which is the the idea of
approximate storage is that there's been
lots of projects about about doing
computation that is is to say take your
functional unit and you run it at a
lower voltage sometimes it can be wrong
but now we're interested in ways you can
exploit storage technologies to store
data in a way that doesn't that that is
not guaranteed to return exactly the
bits that you wrote and we'd like to
sort of exploit the some properties of
the underlying storage mechanism in
order to make that the most efficient
and to to mitigate errors even in
approximate data so what I'm we talking
about in this section is phase change
memory and as a quick review if you
haven't heard of PC
em we like PCM we as the the
architecture community like PCM because
it offers the ability it probably offers
the ability to to scale beyond d Rams
limits and what I mean by that is that
people are worried now that that DRAM is
on the verge of not being able to scale
any further so we need some new memory
technology to come along and save us as
we try to continue scaling memories and
it has these these nice benefits on the
side which are that it's it is quite a
bit faster than flash and it's almost as
fast as diagram that we can that it
might make a good main memory
replacement and on top of that it's
non-volatile it's so it can be used both
for persistent mass storage like flash
or hard disk so used today and for
forming the memory like DRAM is used
today but it has some drawbacks and
that's what a lot of people interested
in PCM try to address the two drawbacks
that I'm going to talk about today are
have to do with multi-level cells which
allow you to increase the density of
storage in PCM which would on but at a
trade-off in the access latency that is
it's a lot slower to write to and read
from multi-level cells a multi-level PCM
than it is single level PCM and it also
has this a troubling aspect where things
we're out relatively quickly compared to
compared to DRAM meaning that that
individual bits in cells in your in your
storage system can fail and become stuck
at a certain value relatively early in
the devices lifetime and these both of
these have to be addressed if we're
going to make PCM reasonable for four
main memory storage or even to replace
flash and the because I'm an
approximation person and again I have a
hammer and everything looks like a nail
when I see these two limitations I
implicitly add something to the end of
each of them which is that multi-level
cells are slow if you want to store
exactly the right value and cells can
wear out and you have to throw away
blocks of data if you want to actually
store the correct data in those cells on
the other hand if you're willing to
compromise on that if you're willing to
store slightly wrong data then you can
you can help address both of these these
quirks of PCM as you might be able to
make multi-level cells faster and you
might be able to use more of your memory
for longer if you're willing to tolerate
a few errors during them so the two
techniques I'm going to talk about right
now are one trying to trade off accuracy
for access efficiency and multi-level
sales and two ways to use blocks that
have dead cells to store a precise data
and is to sort of proximate dedham so
the the first one we talked about is
this multi-level cell idea so as
background let's look at let's look at
how a single level cell in a multi-level
cell in PCM work so a PCM like lots of
other solid state memory technologies a
cell is just some is you can think of as
an analog storage device and we're just
quantizing it to a to a digital value to
make it make sense for the rest of the
of the system and in a single level cell
that there are only two levels that's a
little bit of a misnomer there's
actually two levels in a single level so
but there's a there's a zero level that
is a low analog value in PCM that's a
low resistance set in the cell and then
a high resistance that's a one and the
the right mechanism is relatively
obvious you can either increase or
decrease the resistance to get as you or
one sword in the cell and a multi-level
cell as you might expect has lots of
different levels and here's a two-bit
multi-level cell or four level cell that
we can store that has four different
analog values that correspond to digital
values for the rest of the system and
because of where were you because we
although we're using a digital
abstraction to use these multi-level
cells under underlying that digital
abstraction is still an analog storage
medium and we're conveniently able to
ignore it because the people who have
made this multi-level cell have tuned
that so that you can completely ignore
the fact that that analog style errors
are possible they're just so rare that
you never see them once the value is is
abstracted into the the digital domain
so you can think of it a little bit like
this so they say you want to write a 10
to this cell the
analog value isn't going to be some
precise analog resistance stored in the
material it's there's a going to be a
probability distribution because the
right mechanism and the read mechanisms
are probabilistic there they and they're
they're not they're not reliable like
you like you might expect in the digital
domain like in the analog domain things
can go wrong and the idea behind
approximate storage in these multi-level
cells is just to allow those probably
distributions to be a little bit wider
and if you if you and hopefully we can
find a way to to make it so when you're
writing multi-level cell or reading
multi-level cell that that when you put
less stringent requirements on the width
of that probability distribution you can
make the axis is much faster in
particular the way this works is to
exploit the way that people currently
build the right Andrea mechanisms for
multi-level cells let's talk about
rights and the way that this currently
works is to use a iterative right
process because each individual step is
not accurate enough to if you if you
want to write a resistance to a
multilevel PCM cell you have to heat up
a material and co and cool it down in
such a way that it converges on the
correct resistance and it's not possible
to do that in a completely accurate way
you can't just do that on a one shot and
hope that you get the right answer every
single time people haven't built
something that accurate so what use is
an iterative an iterative process where
you use a series of pulses to get closer
and closer to the analog value you want
to store in the cell and this provides
an opportunity for approximation which
is to say that if you if you're able to
use fewer steps to program yourself
fewer coarser granularity steps and you
do and you allow values to fall farther
from the correct analog value and then
you then you can then those rights are
faster another way of putting that let's
change this graph a little bit to show
the actual probability distributions is
that here are here's some some
probability densities for how rights
might go in a precise cell that is that
these cartoon probably distributions are
showing a zero or negligible overlap in
the probability distributions when
you're trying to write each of these
levels to the cell what we're trying to
do with it
storage is to say will will allow things
to be slightly more distributed from the
correct ideal analog value so that those
probably distributions actually overlap
and then non negligible amount and that
means that writes can go quite a bit
faster well while allowing a little bit
of level confusion among those those
values and what we found when we
evaluated this is were able to
accelerate the the rights that was by
about 1.7 x meaning that there was a
reduction in the number of right steps
meaning that each individual right was
about seventy percent faster than a
precise right over the average of
precise and approximate rights in
approximate programs unless you say with
what probability no no you said
previously is a negligible and now it's
at some probability mind getting long so
yeah so there's in faster to get a ten
percent probability involved that's
actually exactly right yeah so so what
we did is we we took a bunch of programs
and we ran this simulated them on this
type of hardware and we tuned the knobs
for how fast these rights wind and we
took the the highest error run that had
under ten percent output error and we
said and we looked at what the the
average right speed was for those
applications and so this this actually
brings up interesting question which is
what do I mean by output error right in
with no actually for for the program so
so who do I what's it looks completely
diff it is completely different yeah you
know what the also for the device itself
would be yeah so we have that too and I
don't have the number right now but
they're like the the bit error rates are
like on the order like depending on the
program depending on how approximate
this is allowed to be the resilience of
the program it's something between one
percent and like 10 to the minus 9
probability of bit errors then we find
years you prove them up with good
amplified years in a Cell oh cool she
could write yes fuck ah but record
exactly yes that's why we should vote so
both the device i wear as well as you
know what will happen in the pair
output to the probe right so right yeah
good measure here right because right
you should really report somehow the
scalar deviations that's a really good
point yeah so we are bringing up here is
it in this particular error model unlike
a lot of other sort of just raw bit flip
air models that it's like nearby levels
are more likely than far away levels and
that actually brings up an interesting
coding theory question that we haven't
solved yet which is to say if you and
this is like here we have a four level
cell and you can only store two bits in
it but two-bit numbers aren't all that
useful and it's the error model is great
because one value error when you're
storing two-bit numbers is relatively
likely but a three value error is
relatively unlikely but what if you want
to store something anything larger than
than a two-bit number how would you
combine multiple approximate cells to
like to get the ideal error distribution
and we don't have an answer to that it's
an interesting question a great code is
actually not exactly what you want with
we thought about this too right but but
the gray code means that like if you a
multi-level gray code is what you would
have to use right which would mean that
that it means that only one okay so you
have since there you have for
multi-level cells representing an 8-bit
number and each cell has two bits in it
right then a then an increase in one
means that any one of those cells can
change by one right what I mean to say
is it is that if say say any one of
those cells has a one-level air then
that could map to a high bit failure in
the represented number and when you
simulate this it doesn't turn out to be
any better in fact it turns out to be a
little bit worse than the naive take
your bits and divide them up by the
number of cells so again we don't have a
perfect answer but we hit what we do
which is which is a little bit better
than the naive thing is to sort of
interleave the bits so again let's say
four levels than they've been one of the
cells so you map the highest order bits
of the number to the highest sort of
bits of all of the cells so that you'd
be you have an 8-bit number and take the
four highest bits of the number and
distribute them among the high bit of
each of the four cells
you take the four lower bits and
distribute them among the four lower
bits of the cells that this seems to do
a little bit better but that's aligned
all your floats did you make sure that
the sort of least significant bites were
stored in pc ends where you are using a
lot of a lot of levels and the higher on
events were stored in PCMs where you
used a few levels totally yeah yeah we
didn't evaluate that that would be
really nice though yeah totally if you
for if you had the granularity where you
could take the the exponents a and put
that in precise storage or something and
you take the mantissa and put that in
then and in this type of storage that
would be great yeah it's sort of fine
grain like this simply there there has
been research on this we didn't evaluate
this particularly but people have done
this for the precise domain which is to
say that if you if you're willing to
spend all the extra time for for mlc
over slc then you then you try to make
this a parameter right and make the the
level count two parameter and it is
possible we had no one has actually
built this yet so who knows whether it's
actually feasible but it is possible
yeah Harvard today you may be able to do
that the line granularity but within the
line I think you still have to use you
don't think I think deserve if you think
about ways to store an image all right
there's an old fashioned way where you
start at the top left and sort of lay
down bits into memory and each one is it
is either a high significant bit or
those you know right and then there's a
slightly less old-fashioned way which is
a JPEG way which you can think it was
you could think of as dot prodding the
whole image with a bunch of basis
functions right of various cosine eNOS
right some are high frequency some local
what was that but then there's the
compressed sensing way where u dot
product the image with just random
vectors right the great thing about the
bits you get from the random vectors is
they're all the same if you lose anyone
it doesn't really matter right the the
accuracy of the number of bits you Bob
so if you just had a bit for the memory
that would seem like the obvious mating
compressed sensing good foot memory I
think it slightly into it is interesting
to talk about these analog ones and find
a sort of a compressed sensing into two
big quantities so you'd have pairs of
random vectors with with good properties
so I know if that's that is a really
good point yeah is that the JPEG way
seems not right yeah cool so that's
that's the that technique for
multi-level cells so I'm gonna move on
now to talk about em about approximate
about cell failures which is the other
big Bugaboo and using these future
memory technologies like PCM and that is
that we need to be able to because so
because errors can happen like within
days of turning on a PCM array and needs
to be possible to tolerate errors in PCM
and then this is true also of flash so
if you're familiar with flash then you
can think of this as an analog or as in
PCM the the thing that melts the
material that calc ignite inside of each
cell that can break and you're even no
longer able to store your you're stuck
with the value that is currently stored
in the cell and this is you if you're
thinking of flash then you can think of
charge accumulating the cell and you can
no longer effectively change that
without it reverting to its reverting to
a zero very quickly so what that means
is you can think of a block of PCM data
block of flash data as having is having
unusable cells in each block in your in
your in your memory has some number at
any point in time has some number of
cells you can no longer use to store
data reliably and how you how one
normally deals with this is to append to
each block some amount of error
correction this is in the abstract but
some like you append some amount of
extra bits to beach block in your memory
to say I can indicate the bits then I
can correct the bits in the in the main
part of the block that are failed and
what this usually means is that is that
you have a finite amount of correction
resources and then some you
you can't use the entire memory for
Corrections so at some point each block
exceeds its correction budget and by
that I mean that the the air correction
is allocated to correct in this example
two bits that are failed but eventually
you're going to have three bits failed
and at that point all you can do is
throw away the block and so the capacity
of a device decreases over time and
eventually it becomes so small that you
have to throw away the PCM arraign this
is the way that flash works to the of
course when I see something like this I
say this is not necessarily a dead block
this is just an approximate block we'd
like to take blocks like this and
instead of throwing them away entirely
let's try to use them to store the the
program's approximate data so we learned
that programs need both precise and
approximate storage and they have a
relatively large amount of data that can
that can have wrong wrong values so
let's take that and put it into the
blocks that have uncorrectable failures
and see what happens corruption budget
so the data density goes down but that's
unsurprising was a lot of errors yeah so
that would be the traditional way to
deal with this but eventually into a
deadlock they just turn into a less days
oh I see we were saying yes right is to
some sort of flex some sort of way of
flexibly increase the amount of error
correction when errors become more
prevalent that becomes more and more
expensive so so what I've actually drawn
here is is not too much of a cartoon
like the efficient way to deal with this
is to actually co-locate the the error
correction with the block meaning that
there is a finite amount and you can't
go elsewhere to find more correction
there is research of course to sort of
to make this more flexible that is to
use more resources elsewhere but you do
run into limits that is if you if you
had to like if you store it a pointer
for example to the location of the error
correction for for the values that would
quickly get slow and it would also open
you up to errors in the pointer I just
meant you could still collocate it you
could just move the boundary between the
purple and the white but then you lose
oh I see I see everybody will become
less dense right line but you also lose
alignment properties right so we're
talking about real hard worth and this
is this what you really want is each
block to stay for example 500
bits you don't want to it would lead to
efficiency problems if you if we each
block could be a slightly different size
in memory you can imagine how that would
get hairy quickly so what would the what
air correction does for you and is to
take for this to take the lifetime of a
device and extend it so this is showing
the capacity the the blocks that are
usable in a PCM array over time so you
can think of the x axis as the number of
rights that have happened over all the
array and the the y axis as the number
of blocks that are still usable and if
you if you don't have any error
correction at all then the first error
anywhere the first failure in any cell
in your entire array means that you no
longer have a completely usable system
but with error correction on you you're
you're able to extend the lifetime of
the array of to a to a certain point
right so let's let's say you have a ten
percent budget here you like you
allocate ten percent of your blocks to
to be hidden from the user and say like
you have a you have a one terabyte
memory but we're going to add on a
hundred gigabyte of like budget of
reserved blocks that we're going to go
into correct that means that your you no
longer have the advertised capacity once
you reach some amount of uncorrectable
failures on the other hand what all that
approximate as a storage is doing for
you is to say is to say instead of the
y-axis being the number of blocks that
are alive that's the number of blocks
that are precise and we can keep using
the array until one of two things happen
first of all you can run out of precise
blocks necessary to store all the
precise data in your programs so we want
to guarantee to the programming language
layer that some of the data never has
any errors ever and so if you don't have
enough blocks to store all the precise
data then the usefulness of the other
the memory is over the only thing that
can happen is that errors become so
frequent even in the approximate data
that the quality is so bad it's like the
lower right image of
ray tracer that I showed you a while
back and it's no longer useful for doing
computation but any case we we get to
extend the lifetime of the of the PCM
array and there's one other trick that
you can play when you when you're doing
this so beyond the obvious let's use
these these compromise blocks for
approximate storage which is to say that
that you you don't have to correct the
same failures that a precise storage
would correct what I mean by that is
it's a you have a block that has two
failed bits and you have error
correction for two bits in your block so
we've successfully corrected those and
then along comes another failure and
it's in I'd say the the high-order bit
and one of the higher order bits in your
block that's bad say if you're storing
some some values where the where these
bits over here are more important than
those bits and the naive thing to do
would just be to say too bad you you got
a failure in a bad place so we can't
really help you with that but we can do
a little bit better than that which is
to say we will protect prioritize the
error correction as once we see a new
failure we won't just keep correcting
the same failure as we had corrected
before but will actually correct
different bits and will say we'll
prioritize the error correction of those
bits that can be the most harmful and
this lets us extend the the usefulness
of the of the array a little bit longer
in that second failure condition that is
to say that when the the quality of the
stories is degrading too much we can
make it degrade a little bit slower in
this case we're only interested in hard
failures because those are the dominant
source of failures in PCM you said baby
then we just assume everything yes
that's right yeah so that's actually the
that's actually normal failure mode for
pc and by which i mean the the first
time I see a failure means that that
will be failed from here on out and
which it and that in PCM that holds true
but you don't see a lot of soft failures
that make sense you know just surprised
you don't see a lot soft baby because in
main memory
babies at home right and D Ram is there
the error rate is a lot higher but
because this is a system where you're
you're actually crystallizing material
at cosmic rays for example don't have as
much of an effect as they do in DM just
normally higher then that's right um
right so that is that's approximate
storage in failed PCM blocks we see here
is we're able to extend the light useful
lifetime of PCM arrays when running a
sort of average workload of many
different approximate applications by
about twenty-five percent so that's all
about approximate storage and I can move
on now to the sort of software oriented
compiler based approach to approximation
that's unpublished work so if anyone has
any questions about this this storage
business now is the time the complexity
of the error connection surridge it's
quite simple for a standard to bit error
correction it's not like it identifies
the two bits it's gonna save and then
and then you know writes the code it
what what's the increase in complexity
if I've got three bits and I want to
choose to to save that's a really good
question and that thing you said in the
middle about choosing the bits to
correct and correcting them is actually
that would be the air correction that we
use that we extend in this work so
unlike a traditional ECC like a sec 10
type thing which is great for soft
failures it turns out to be more
efficient to directly indicate the
failed bits when you're dealing with
hard failures so there's actually we're
using a scheme that Karan worked on
called error correcting pointers where
you actually store the address with very
small address of the bits but you're in
they want to create the other thing over
to do with vesa with these stock drops
with many stuff bitches start moving
rarely written data to them
so I'm just wondering what are the
applications where you have lots of
approximate data which is also
frequently written because that would
seem to be the match but there's sure
yeah so in a province we looked at it
and the approximate programs would deal
with in general the hottest data tends
to be approximate as if there's these
pointers and stuff there's like some
metadata that is also hot thin that is
precise but for the most part things
like pixel values and audio samples and
like neuron weights in a neural network
those things are relatively hot and
they're read and written relatively
frequently and those can be mapped to
approximate hardware so we actually
don't see a huge amount of cold
approximate data if that makes sense
frequently written the proximate via
there's lots of frequently read but
rarely written data which might also
visit or fitting in ECM like dll
binaries for example sure what's
interesting about them so are you are
you proposing a scheme where you like
find the block with exactly the right
failure is to fit in the data and so
that you know dis ingest things in terms
of reducing we're on the PCM might
choose to store in PCM memory things
which you expect to be rarely written i
see interesting yeah so i'm imagining a
system now that has doesn't have an
alternative afraid everything has to go
in PCM but you could start moving the
rayleigh live in stock to the blocks
that are running out of their
reproduction pointers before they run
out entirely yeah what's a good
so I'm just wondering you know that's
Katie to two different kinds of data
competing for this slightly broken
memory I really like that idea but put
your cold data in the almost failed
cells I haven't heard of anyone
proposing that and exactly I don't know
have you have you cardigan um great is
that all the questions about storage I'm
going to move on to talking about a
project that we call accept which stands
for approximate c compiler for energy
and performance trade-offs i think which
is a little bit of a contrived acronym
but bear with me here for a second so
the idea is that we want to take a
program written in an approximation
aware language and for a long time we've
been interested in mapping that to
special hardware that has special
support for approximation we have this
approximate CPU design and we've
designed approximate accelerators that
are really good at running code that
when the controller a tear is but what
we'd like to do is say what can we do
today can we take an approximate
application map it on to an x86 or an
ARM processor that's a that's on the
market now and what that means is that
we want to take an all software approach
what can we do to to not use special
hardware not use a dedicated approximate
storage arrays what's available at
compile time to transform a program to
be approximate to make the this type of
trade-off has actually been a lot of
proposals about this in the literature
when the more famous ones is from martin
Renard at MIT called loop perforation
and it's almost exactly what it sounds
like you have a loop in your program
let's not execute all of the iterations
let's skip a few of them and see what
happens this sounds really scary but it
turns out that for some programs for
some loops that's actually okay like if
you if you're doing a convergence loop
and some of those iterations only get
partially executed or something that
that actually is okay and this this is
understandably very scary and I think
it's scary too which is why I would like
to bring the notion of safety that we
looked at in energy to this type of
transformation other types of software
based transformations and involve
equally scary things like removing locks
around data that is if occasional races
are okay on some of your data in your
program and let's not lock the accesses
to those you can imagine some parts of
the program being written with floating
point but they only need fixed point for
certain sections of a computation so all
of these these transformations I can see
a few people be missing actually I'm the
audience about how how scary some of
these can be so it would be great if we
had a mechanism for finding out where
these transformations were appropriate
rather than continuing to publish papers
that just say look when i took this part
of this program and took it from float
to flick staff it fixed it still worked
even though it violated the semantics it
seems to work okay we'd like to have
some sort of framework that gives you an
assurance of safety well while guiding
you toward the right trade-off between
approximation and efficiency so the the
way that we want to do this is to is it
starts with a quality metric which is
something I've not talked about a lot
but is actually under underlie all of
the work that you've seen so far which
is that we would like programmers to
provide us with some sort of a notion of
the quality of the output of the program
you can think of this sort of like an
asset acceptance test but instead of
saying yes or no it says this it has a
continuous answer between 0 and 1 this
is it gives you the continuous notion of
the quality of the behavior of an
application given for example a
comparison between the completely
precise output and the degraded output
and you can say like you know ten
percent of the pixels were crazily wrong
in this image or something like this and
so we would like the programmer to
provide us with the original application
and the quality metric that goes along
it along with in and to to do software
approximation means that we need to
automatically evaluate whether some
candidate transformations whether some
some loop perforation or some
synchronization elision is helping the
program to meet that that quality metric
so the high-level idea of course is to
produce lots and lots of candidates
using a heuristic Lee selected subset of
all the possible transformations on a
program and to eliminate those that that
that don't meet the Koala
requirements given by the programmer so
you give us a quality metric and will
and will eliminate those will try to
eliminate this application those
configurations of the program those
transformations that didn't work out
that led to really high error and the
next thing we want to do is like once
we've filtered out things that are
obviously infeasible is to to plot a
bunch of candidates on a sort of
performance and output quality trade-off
curve as we will will if you have a
bunch of Canada applications and we can
run them many many times and try to call
it to try to assess the the output error
we want to select the Pareto frontier of
program configurations that that make
them make an optimal trade-off between
accuracy and efficiency and these these
are the programs that the these are the
configurations of the programs that the
programmer should consider the way we
don't want to prescribe a particular
level of output as being the best but
there is but we can plot for the
programmer this trade-off curve that
exists in the program between efficiency
and accuracy and the way the way we do
this at a compiler level is to analyze
the program to look for four sections of
code that are relaxing it's requires
some sort of a notion of going from the
data center canÃ­t a shins that you saw
in the energy section of this talk which
which all talk about individual
variables and fields and objects that
can be approximate and to and to move
from that to some sort of code centric
notion of what is approximately and the
way this works is how you might expect
we can analyze sections of code
arbitrary sections of code like for
example a loop body or critical section
between two locks to see if it has any
precise side effects and we say that any
region of code that has exclusively
approximate side effects is relax abbud
of Z we can skip loop iterations or we
can lied allied to the lock around that
region and that lets us move from the
the data center domain which makes a lot
of sense when you're talking about
Hardware approximation into a code
centric domain which makes sense when
you're skipping loop iterations or over
a removing block
Lock calls and what we see here is that
depending on the program this this type
of technique is quite a bit more risky
but has higher payoff than the type of
hardware approximations we've seen but
you saw on the rest of the talk is it
for for some programs the that this type
of software approximation doesn't buy
you a lot we looked at a video encoder
for example we're able to speed up even
perforating lots and lots of loops
throughout the program and and alighting
lots of locks throughout the program
only able to speed it up by about ten
percent but for other programs where
they're very loop centric and have lots
of approximate data lots of in the
structured in a way that they affect
only approximate data much of the time
we're able to speed up a by like an
order of magnitude but again this is
unpublished work in trouble with this
loop thing yes I borges you at the
almost complete example so far even if
we know it's in a doing approximate day
two he said well you leave the whole
loop apt right but that probably a bit
to approximate absolutely were you going
to say you just every other situation of
the loop I mean how would you decide
which ones to leave at ya so for the
loop perforation in particular there's
an obvious parameter which is how many
of the iterations are you going to
execute right so you say you have an
image filter right and so it runs on
every pixel and it transforms it a
little bit by looking at the
neighborhood pixels one of the
applications we examined actually did
that and if you transform only some of
the pixels or if you like if you're
taking an average say you take an
average of only some of the pixels and
that's okay but but it's a it's a sort
of application dependent tuning question
how many of those iterations you want to
skip this is sort of the same question
you asked a while back about the the
hardware parameters even if you say ok
for this new cadre this gate some how do
you decide which to skip let me do that
sounds like you might have to do more
work to say oh I'm all the 17th once or
skip this one right yeah can I answer me
oh you have sure on this machine there
are at least three loops which say if
Rand greater the point 99 loop body
either one of them's in a nonlinear
optimizers accumulating
part of a matrix matrix product and it
just drops ninety-nine percent of the
accumulations um because it runs much
faster that way and it doesn't really
matter so what you think you just did
you throw amount of numbers oh you have
in your examples yeah exactly okay Hugh
mandame Lee choose a proportion of the
loop stitch evaluation it was actually
deterministic and we that the the
randomest is generated ahead of time but
the Tim just to make it cheaper but the
be like it but yes if you just it's it's
there's there's no there's not a lot of
work involved in trying to skip this and
in the deterministic case we just do it
with bit masks right in this by saying
well you just have a counter your loop
counter if it's a for loop or we added
add you in a new counter that says if
this is if this is the you know if every
time the counter reaches for then we
skip an iteration and reset the counter
20 an expensive way of correcting code
because the answer would if some of
these doing a thousand loops and they
shared only need to do 500 loops then
the answer should be to just tell them
just do 500 lips yeah absolutely yeah so
one of the the aspects of this is it
does it does it is sort of an
interactive process with the programmer
which is to say that we that we we were
discovering these aspects of programs
and it may be the most efficient thing
to do is that the programmer should go
back and reconsider some decisions the
idea here was to make this button which
then just says everything is well this
way you just end up with a precise
program yeah so if the if the programmer
like sees what we're doing and the the
system except comes back and says this
this loop should only execute five
hundred iterations and the programmer
can say I mean right now that that's
applied automatically and it's and
you're right it's at an efficiency cost
but if the program wants to come back
and say I'll do that manually I myself
will go in and execute only 500 of the
iterations and that's fine too and yes
that becomes precise those are that's
the intent of the program review so
basically if the program
says no I want to keep a thousand then
you shouldn't drop it to a hundred to
five hundred but no it's the opposite
rope I actually have code with random
execution of the loop body if you're
saying you've got some hardware they can
make that code run much faster save me
the doing the rand or something whatever
it is or you can somehow to I've already
annotated the code right you can look
for a SRAM then body and now you were
just going to propose different ways of
doing it approximately that seems good
that's like know that but you find your
intent young your coat yeah yeah that's
fine I've never written if round do
something or not children when you do
actually we have had right so you could
have if you let's say if you had some
assurance they're doing that'll be fine
for your output and see how do we are
declaring how the way that the output is
good or not you can have that done for
you automatically so you don't feel
insane no you just added a range of
options so I can now try advice run
approximate workers tiliranen's
initiative options like this candidates
it tries out automatically as long as
your honor in the output quality you can
apply optimization I suspect it's
because I don't compute the random with
proximate data but I mean if you if you
if you if you closer to the story to me
slightly differently I said to me we
have a method for saying to the
programmer this piece of your program
consumers fifty percent of the Palmer
cost but only contributes three percent
of a policy would you like to consider
me consider whether it ought to be there
or not then I can kind of believe that
but if you say to me while I'm just
going to randomly throw bits of the
program away I can't imagine you know
the director of tests ever signing off
on shipping such a thing
right because it's the case of the
unfortunate user why do I you know
protect myself from the fact that the
person who's who loses the critical
iteration happens to be blood Amir Putin
I think that's really read yet and one
of the pieces of this system that I
didn't show was was how this actually
interacts with the programmer and so
it's a convenient fiction to say this is
going to happen completely automatically
like well we'll just annotate the
program in from there out this auto
tuner will give you the best possible
program but it's true you want some
feedback right which is that the way
this could this this actually works is
to have it there's an annotation step
this sort of as a dialogue between the
compiler and the and the programmer it's
as if you if you have a completely
precise program the the compiler will
come back and say here's all the things
I would really like to do to your
program but you're preventing me from
doing it because you haven't put any
annotations yet here are the annotations
you would have to insert here is the
data that would be affected if i were to
perforate this really juicy loop that I
really want to perforate you know and
and so it's it's sort of in this this
dialog that you the programer right then
get to say okay I think this might be a
good idea what is the data that's
actually going to affect you you
annotate these individual things and the
compiler says thank you great now look
at what this quality is that you got
back so this by this process that you
that that you can the you can think of
it as the programmer being guided
semi-automatically toward a good
solution I agree that's sort of taking
your hands off the wheel and just seeing
where you go might be terrifying great
so that's accept and brings me to the
conclusion of the talk which is to say
that obviously there's a lot more work
to do on on approximate computing that
the to project is so you are just two
facets of what i think is becoming a
bigger and more exciting field every
month you can imagine this this question
of how you do a compression for
approximate storage is a really
interesting one i'm working now on how
to analyze approximate programs to to
see to try to guess and semi statically
how approximation at the hardware level
is going to affect the quality of the
output of a program and there's there's
other places you can you can imagine
using the approximation hammer to to hit
various inefficiencies in computers like
for example how wireless networking
needs to use a lot of error correction
to to to guarantee precise communication
even with communicating things like
images that don't need to be preceded
completely precise so that's all for my
talking if you're interested in where
this here's my website that's got a lot
of all sorts of papers and some blog
posts and stuff about approximation and
then our group's website is sample done
c.s that Washington the ID and there's
all the information you could possibly
want there thank you so much for your
attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>