<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Big Data with Stratosphere | Coder Coacher - Coaching Coders</title><meta content="BDA - Big Data with Stratosphere - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - Big Data with Stratosphere</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iSZ915mi_VU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
thank you thanks for the invitation and
I'll be happy to tell you today about a
research project and an open source a
system that we have been developing a
tubulin but maybe first if this works I
guess it doesn't okay no it does first I
am sure all of you know about data and
analysis becoming increasingly complex
and all have heard about those Rees of
big data but what I'm believing it's
that's really missing an important part
of the equation if we only talk about
data volume velocity variability in
velocity we already have heard in grads
talk before and other Natasha's talk a
lot about this is really about the
analysis and respect to the analysis
there's a lot of existing systems out
there that are pretty good and
essentially conducting relational
operations or some subset thereof like
selection and grouping like mapreduce
relational databases that can do joints
and correlations but also applying
somewhat complex user-defined functions
like you need an ETL or information
integration morrissey the requirement if
you do data analytics that you have to
go into the space of implementing
machine learning algorithms optimization
algorithms who are even more predictive
models and there's a new class of
problems in particularly if you look at
things from a data processing
perspective that those two types of
analysis operations require and that is
iterations and state and that's very
novel are very different from
traditional database systems that
usually follow a data flow that a state
lesson doesn't evolve involve iterations
and the reason for that is because you
want to automatically optimize parallel
eyes and execute the operations you
don't want to program bread in the
program at with those and what one of my
students did is in the space now looking
at what kind of systems that are
emerging in the space so it's missing a
couple of recent one search for instance
missing raffle epi on this chart and
it's also missing a new development for
Microsoft Nyad but essentially what you
see here in this stack you see
and relational databases you see novel
systems classified according to the
Korean scripting languages some higher
level API that ideally has some form of
declarative features as well as what
compared with the compiler technology
lower level API and execution engine is
and the interesting observation is there
substitutional database systems that
usually have some scalability issue and
then there's this very popular MapReduce
paradigm which on my viewpoint is
essentially a very efficient and a very
good implementation of a scalable
parallel swept algorithm and you cannot
obviously do a lot if you have sorted
data you can apply a lot of different
aspects to it then there is a sequel
variants that are implemented on top of
those systems there is also some
scripting languages with centrally
hardcode a query plan and the reason why
I'm saying that is a database system
like in a language like c ql has the big
advantage that you specify your query
declaratively it's independent of your
execution strategy what physical
operates or whether you have and how it
depends on particular data sets it's not
dependent on that there's a component in
that system called a query optimizer
that will automatically select the right
strategy but if you do Korea's plan
scripting then you hard code such a plan
and your data may really influence your
performance very heavily because you
have gear di little to a particular size
of data sets of distribution then
there's column stores that are currently
very much in fashion and there's also
systems that follow I'm a verte
eccentric programming model like like
like prego which essentially also and
we'll see that later there's hard code
app specific query execution strategy no
system like stratosphere and then
there's a breed of new systems will hear
about a couple of those I believe two in
the next couple of days that I'm on the
right side here like like scope and
there's the stratosphere stag then
there's another snake from the
University of Berlin called Asterix and
there's the spark stake in Berkeley so
with that however I want to now focus on
one specific of those Manolo stacks
that's been of being developed and
that's the one that my team a tubulin
together with other researchers in
Berlin and also across Europe a building
and it's called the stratosphere system
which is a layer and flexible
spec for massive repaired Atta
management I'll introduce a programming
model to you how this programming model
is optimized essentially it's
declarative so that's the important part
here and so it can be automatically
optimized and then ever talk about this
important new class of problems that one
has to tackle and data analysis which
are iterative algorithms if I still have
time for that otherwise it will speed
through them so but first let's talk
about the stratosphere system stack so
it's really a layered approach where
there's several entry points into the
system so there's some scripting
languages there's a scala front end so
that you can write your data analysis
programs in a language like Scala and
there's a lower level interface and I
focus more on lower parts of the system
which is there's a specific programming
model called pact which is based on
second what are functions and I will
tell you how this is being implemented
and realized but on the bottom level of
the system there is a parallel data flow
engine that allows for massively
parallel executions on hundreds of nodes
and this peril data for engine it takes
care of resource allocation scheduling
task communication fault tolerance and
execution monitoring on top of that
there's a runtime engine that takes care
of memory management a synchronous i/o
and in particular implements the typical
query execution algorithms like how do
you distracting how do you do hashing
how do you integrate and combine various
data sets of joint operations so this is
what the runtime operator does it really
implements essentially the functionality
that one would expect in a parallel
database that's a big difference if you
look at systems like the very popular
Hadoop stack where you don't have those
kinds of primitives and you don't have
pipelined execution which can be dealt
with in this case as well and then
there's a optimizer which based on the
program specification the data analysis
program specification selects the
physical execution strategies and
partitioning strategies and also the
order in which operate this should be
executed and with that I want to briefly
motivate you a programming model for
specific angles data analysis programs
that goes beyond what the rationale
algebra the prime language for data
analysis is and the reason for why we go
beyond that
as a first complex user defined
functions and later at extended for
iterations but first let's talk about
paralyzation contract so essentially
this was motivated by this big data
programming model called MapReduce the
MapReduce programming popularized by
Google which essentially is a very
simplified language of second water
functional programs it's really second
what a functional programming because
what one does in in this context this
one has a second water function that
will take as an input a first or a
function in a set and the set is usually
in the MapReduce model identified by key
value pairs and in this context have
second what a function named map what
that one does the second water function
in map it will build independent subsets
for each input so that means if you have
a map each key value pair that is part
of the processing can be processed
independently sort of first what a
function can be applied to each key
value pair independently of another one
and what this allows for is for
embarrassingly parallel processing
because it means if you had n processors
and you had n data items every one of
them could theory we processed
independently so embarrassingly parallel
similarly there's another second water
function called reduce and the reduced
second letter function but that one does
it takes a key and it groups everything
around that key everything has the same
key will be processed together by a
second water function so those are the
common second weather function is
popular in the MapReduce programming
model and used in the Hadoop system the
interesting observation is that those
are two certain classes of functions so
the first function map is a record at a
time function so I processes each record
and it's one-dimensional because it only
has one input and reduce processes
processor set at a time namely
everything which has the same key and
again it's only one input and if you
have those two functions you can
efficiently implement grouping and
aggregation and applying a lot of
complex user-defined functions on those
so what we did in the context of
stratosphere we generalize this model by
saying there's not only those two
second water functions need met but
reduce some others that are important
and in particular if we look at what's
needed in order to have at least a
full-blown relational system of
user-defined functions you need second
other functions that take more than one
set as input and there is now some
additional what we call paralyzation
contracts a second word of functions
that can be used for data analysis so
the first one is called a cross and what
that does essentially it builds a
Cartesian product but note it's not a
Cartesian product because it applies a
first order function on each combination
so essentially it's a record at a time
function that's two dimensional because
it takes two inputs it combines
everything and applies a user-defined
function on each input the next one is
co group which essentially is a two
dimensional reduce which is a set at a
time function that takes two inputs and
the group's everything together so what
you see here as an example is so cross
essentially everything that comes
together like this one and this one
forms an independent group this one and
this one forms an independent group with
a co group this one and this one they
are grouped together they're coming from
two different input sets but they group
together where those three this one this
one and this one a group together and
that first what a function is applied on
those there's an additional language we
call match which is not necessarily
needed except for efficiency which
brings together everything that has the
same key and in this case becomes a
distributed equity on which is also a
record time function that's two
dimensional so the interesting thing is
if you have those three additional
paralyzation contracts plus map and
reduce then you can implement the
functions that you already know from the
relational algebra selection projection
grouping aggregation efficiently and
they don't have to hard code you don't
have to hard code any particular
operation like what you would do in a
typical to produce environment you would
have to implement if you want to confirm
a join this would be done by actually
using some tricks like distributed
caches or you have to take the inputs
which doesn't allow for automatic
optimization because what you have to
note here this doesn't tell you anything
about the execution how it's
paralyzed how its processed and we have
different ways how they can be done and
how it is done will depend on data
distribution will depend on the number
of processors the data sizes of various
sets so all of that plays a role and the
power of such an approach is you can do
that in a declarative fashion it's
different however from what it's done in
a database system in a relational system
because each time you do that you have
to apply a second order function so you
cannot use the standard techniques
because you don't really know that a
certain operation is a selection or
projection or a joint because it's only
telling you how things a group together
and then process together so with that
you can now I are together arbitrarily
complex if you wish data flows of those
second whether functions which
essentially could be you have some
sources you apply map then you apply a
match that brings those together and
what you see here is the first order
function that's given and then you have
a reduced and then you have an output so
a pet program is an arbitrary data flow
as you click graph the consists of
operators wage operator has a second
water function signature and the
user-defined first what a function
that's usually written in Java so it's
essentially really a generalization of
the MapReduce programming model and we
have some scripting languages on top of
that like meteors color that automated
compile into this kind of language
that's used by the system now I want to
talk about some specific aspects that we
can do because we have this richer
language and I should maybe point out
before i go there though so there is
some were second to other functions that
you could think of the to define for
instance overlapping partitioning that
you might want to use for metrics
operations of a time series analysis so
you don't have to stop at those paths so
those are the ones that we currently
implemented but we currently exploring
for other ones for different operations
with that I want to talk about one
specific piece of work that we have done
for the system which is one paper that
was published it really be last year and
that is about how we now can
automatically optimize and find the best
strategy for how to paralyze and execute
those functions and end programs and the
overall idea for the stratosphere system
is that there's a cost-based optimizer
it's similar to how things are done in
language compilers but combined compiler
technology from programming languages
with the database career optimization
technology so we have a cost-based
optimizer which essentially tries to
attribute a certain cost with each
operation and this is dependent on data
characteristics but of course also the
specific operations and this cost-based
optimizer will produce a physical
execution plan that will decide on
cashing on broadcasting and data
shipping strategies and it will annotate
the edges of this data flow graph with
distribution patterns like should it be
a broadcast or a partitioned operation
it will determine the physical execution
strategies should be a hash based or a
 based algorithm in this particular
context and it will real what are the
functions if a certain different water
is more cost-effective like a typical
example would be you wanted to really
reduce the data set so that later on you
don't have so much data to handle if so
you want to automatically apply those
rewards when it makes sense and then it
will construct for this nefeli execution
engine this job graph that will
automatically be executed so by the
challenge in this context is now how can
we identify how and when we could
reorder operators because the problem is
in contrast to a well-defined data
analysis language like relational
algebra the semantics of the
user-defined functions is unknown in
this context so the question is really
how can we define or derive correct
transformations and how can we cost
those because it's really about
user-defined functions optimization and
the way that is done is really using
compiler technology so we do static code
analysis in each of those first what are
functions that I like to think of your
map first what a function implementation
will reduce first order function
implementation we extract properties
from those and based on those properties
we derive semantically correct
transformations we enumerate
semantically equivalent plants and in
this way we really show how one can
deeply embed map reduce functions the
query optimizer so that the user doesn't
have to worry about in which order to
write operators and it's actually not
only relevant to the stratosphere system
but it's relevant also apply to
seems like scope or any system that
combines complex data analysis
operations with a MapReduce or
user-defined function context so it's
done by a static code analysis and I'll
give you an example so what you see here
is the code of a specific implementation
of a match operation that takes two
records as an input left in the right
record and well there's some collector
which is essentially the output and the
way that is done using a specific API
this API it has get copy and then and
also set operations and in this way we
can determine using static code analysis
is data being accessed or changed so
it's feasible to do the static code
analysis because we have a record data
model with a fixed API and there is no
control flow between operators so this
is still a data flow model this is
important it's a functional programming
model so it's a second order functions
that call first order functions that
have no side effects and only then you
can do this automatic optimizations
that's also a difference so that's why
you could not easily do those kinds of
optimizations in the inner system like
Hadoop automatically because if you want
to join you very often use some things
like distributed cache which I have side
effects and then you could not apply
those kinds of reordering anymore and
the difficulty comes now of course that
you have different code paths because we
have some if statements that we don't
really know which code path has been
used but we can ensure correctness in
this context through conservatism so
that means we want we determine
basically a union of all possible code
paths and determine which data items are
red which data items are written and
based on that we can determine ery
waterings so what we did there is we
identify an output schema for a record
that tells us how will the output record
look like we determine a reset that
gives us the attributes of the input
records that might influence the output
and the right set that are the
attributes of the output records that
might have different values from
respective input out attributes and thus
we arrive at an omit cardinality which
tells us and we only need to know how
many records do we get as an output of a
map or reduce or whatever function
because if we know that information is a
one more than one or potential as a less
than one we can use that information
so with that we can use our code
analysis algorithm and what that
essentially does and you see that here
we determine what we call a read set and
the right set of attributes the read set
we get from the get statements and also
from the copy statements and the right
set from the set statements and you see
for instance in this particular program
it has a sweet set ABCDEF and there's a
right set B see if the record if the
left record is ABC idea right record has
the schema of d e f so if we have that
information and we do get the image
cardinality also by traversing the
control flow graph backwards which is in
this case image cardinality is one then
we have read and write sets and the
interesting observation is that that's a
very powerful concept to know if we know
those read and write sets we can
actually pick partitioning strategies
just from the pact signature week which
is essentially just knowing what are the
input and output up records and that
means we can automatically decide should
we use a broadcast strategy a
petitioning strategy is symmetric
frequent replicate joins without
physical methods that otherwise if you
write data analysis programs in a system
like a group you kind of hard code those
aspects if you do complex analysis
inside your map or reduce function you
have to hard cut those operations in
your map or reduce functions or you
don't even have that choice well here in
this case we use a fiber which is really
a pipe lining or streaming operation so
you don't have blocking operations and
don't have to materialize data in a
distributed file system so in this way
one can infer the partitioning and one
can also get rid of operations that
would happen if you don't have that like
you can eliminate sweating operations
that otherwise would occur and operators
can be reordered and in this way we can
arrive at what would be an optimal
execution plan of respect to a
particular model of cost the interesting
observation is that in this model we can
do what could be done in typical
relational databases without knowing
that we have relational operations we
can do selection push down a projection
push down
a relic grouping and join reordering all
of those can be determined just using
those properties even though we are not
knowing that this is a joint operation a
selection this is chest first order and
second order functions so it's a very
powerful model and there's some proofs
that I in this really be 2012 paper that
show us that to map operations can be
rewarded if they're udfs have only read
with conflicts and for a map and reduce
if the groups are preserved we can do
reordering and as already mentioned we
can the selection push down join your
during aggregation push down and even
more there's also a possibility to
reorder non-relational reduce functions
so it gives a lot of power to the system
and it relieves the programmer the data
analyst the data scientist from a lots
of burdens when writing data analysis
programs so this is the first part and
now I very briefly go over the second
part so if we have arrived at that we
have a system that can efficiently
process complex user-defined functions
and it does not require the user to
worry about the physical implementations
the person who is a data scientist
doesn't have to be a systems programmer
anymore can really focus on specifying
the code in the prayer and the problem
domain it doesn't worry about joint
strategies join orders and so on now the
second aspect is that a lot of
algorithms in particular machine
learning optimization algebraic
algorithms require iterative queries so
there's essentially an iteration going
on and in stratosphere the system has a
concept of two different kinds of
iterations so the first one we call by
loop iteration so in general the concept
of alliteration is that you recompute
the state at each iteration and you
change it so the concept that how that's
done in stratosphere is by introducing
an higher-order function which is an
iteration function which is on top of
those Map Reduce metrical group and so
on functions and what that means is you
now have not data flow as you click
graphs but those graphs can have certain
cycles which give a feedback that is
defining an iteration that goes on until
the convergence criteria has been met an
example here
you see and hear this would be an
implementation of page rank in the
stratosphere system so you have
essentially your constant data path
which is your pages then you have your
rank vector which is initialized in some
way those two are brought together and
then there's a reduced function going on
that creates this the partial ranks and
then this operation continues until a
convergence criteria is met and this is
the dynamic data path because this
modified rank vector goes into the next
iteration and here's a stopping
condition that has to be checked so the
interesting observation is that in such
a kind of a program you have a static
and the dynamic data path or constant
data path and in just in if you use a
system like Hadoop I mean most other
systems you would write an external
drive a loop that you're writing Java or
so it calls the code for the iteration
regularly the problem is if you do that
first of all you may not choose the
right implementation strategy for the
interior color computation because this
could be a complex program here for
instance right in the constant path and
at the same time you might not be able
to identify the right way of how you
ship the data because in some cases you
don't want to materialize that they
don't to distributed file system between
each iteration you could just pipeline
the data onto the next iteration which
will give you tremendous performance
improvements so and that is a decision
that again can be done by the optimizer
automatically so there's a programmer
you don't have to worry about it you
really write in programs as those kind
of second what of functions and then we
have this iteration construct in the
language so you choose your language
like Scala and different iterates
concentrate on top of that and then the
data scientist doesn't have to worry
about that so the interesting
observation is that this shows us the
power of such a system those here are
two optimizer plans so this is an
internal representation where the
optimizer would do with different ways
of how you in this case here you build a
a hash table and then you pipeline the
results and here in this case you do a
partitioning and sorting those are two
different implementations of how you
compute
pagerank and interesting observation is
one of those is how it is implemented in
a specific system called Pegasus and the
other one which is essentially following
the direct proposal by Google how to
implement page rank which works very
well if one part of the data is fairly
large and the other one is an
implementation that's used in the mout
machine learning rivalry on hope of a
dupe but it's hard coded and that one is
used if the rain quickness fairly small
but as a data scientist you wouldn't
want to worry which of those
implementations you would choose but
this here is really an internal
representation that the system would
come out the first generic specification
of page rank so again the data scientist
doesn't have to worry about that's the
power of having a moi declarative
specification that just requires the
second letter functions there's another
class of data analysis algorithms and I
will go through that very very briefly
only for the sake of time that do not
update the entire data set in place like
page rank with we update the entire
vector but do you actually each time add
some new aspects this is means that you
have some sparse computational
dependencies that you can exploit really
you change the state at each iteration
on parts of the that changed at the
previous iteration the most simple
example would actually be computing a
transitive closure each time you add
some new things right until you don't
add anything anymore a very common
example would be connected components
this is just an example of a connected
components algorithm that I will not go
into detail the important aspect is that
for those kinds of algorithms don't have
to transfer and communicate this this
really complex state at each time at
incremental iterations you can actually
split those up and the questions can the
system do that automatically we are
currently working on that sofa data
analyst would still have to decide
either it's incremental iteration of
alliteration but the goal would be to
automate that decision but what that
means you can split up your computation
into a work set and a solution set and
this work set and the solution set I
used to compute a delta that is the new
additions to the solution or the changes
to the solution that happened during an
iteration
that will happen until there's
conversions reached so this shows
tremendous speed ups all of those
details are in the paper the important
thing I only want to point out in this
case as a very specific system like prio
is then a query plan in a system like
stratosphere so Sprague there's a very
specialized implemented system for graph
analysis in stratosphere it's just a
query plan that the optimizer will come
up with with the specification requires
such a particular solution with that I
conclude so stratosphere zai did really
only give you some internal overview but
it's it's a whole system it's actually
running it's available open source and
the apache license it's a generalization
of hadoop so if you're familiar with
hadoop you can easily use it in
particular if you just would you use the
map and reduce functions it's just the
same but you can do much more with it
because you have there's additional
higher-order functions and you have this
additional capabilities of doing it
erations with that I thank you very much
and look forward if there are any
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>