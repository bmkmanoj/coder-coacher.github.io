<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fourier Sampling and Beyond | Coder Coacher - Coaching Coders</title><meta content="Fourier Sampling and Beyond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fourier Sampling and Beyond</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5vH8snamEI8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
we're really happy to have whoa to have
Eric price here for ei sampling and
beyond by the way he doesn't use he
doesn't even use Adobe Reader to present
this because it's not free he only uses
free software no not as in freedom all
right yeah so I can't talk about some of
the recent research I've been doing
starting with work on the sparse Fourier
transform and then I'm going to move on
to the broader context of this research
which is more general sparse recovery of
compressive sensing so first first
Fourier transform Fourier transforms you
all probably saw them in undergrad at
least so they switch between time domain
and frequency domains and time domain
you have your sound wave has some
amplitude of compression of air and
frequency to make you train you have the
note that you're playing so here what
happens when you press a piano key art
strike and then in time domain it decays
increasing in frequency domain you have
440 Hertz you're paying country a and
then you have some harmonic to that that
give you the timbre of the note you're
playing and that's great it's useful in
all sorts of places
so in compression force a sound wave
compression you with say mp3 you first
take the Fourier transform and then you
do some stuff image compression here's
JPEG video compression especially
beautiful there you can see my office
back or you can see my office because it
can only see what's hidden by this
yellow thing so have a nice view of a
yellow wall right and but it's useful
more more than
compression so did a lot of you mixing
wireless stuff computational tasks such
as convolution you first take a Fourier
transform and then you can do some stuff
and take the inverse Fourier transform
so it's useful in lots and lots of
places mathematically you define it so
we define the Fourier transform of a
vector X to be X hat where the J the
if-- term is the sum over all terms J of
Omega to the IJ times XJ this is a
linear operator
unit rest it as a matrix vector products
so you're saying the Fourier matrix
times X where the IJ entry of the
Fourier matrix it'll make it the I J
again Omega is an nth root of unity the
inverse Fourier transform you just
replace Omega with a mega inverse and
scale appropriately and so it has
basically all the same properties that
regular Fourier transform has and lots
of such properties one one that we're
gonna use in particular is that it
switches between convolution and
multiplication so if you want to
convulse to sequences you can first take
the Fourier transforms multiply them
entry wise and then take the inverse
Fourier transform basic primer on the
DFT now how quickly can you compute this
Fourier transform if you want to do it
in all these places
well naive method would be to multiply
by this matrix matrix as n square
entries it would take N squared time but
in 1965 there was a great result by
cooling to key that showed a simple
recursion that takes n log n time so
this was great in it and so digital
signal processing people really love
this result but actually has a longer
history so in 1805 Gauss was doing them
by hand and said greatly reduces the
tediousness of mechanical calculations
and he wasn't the only one who came up
with the same formula so also in 1942
Danielson and lanczos were doing x-ray
crystallography and this is again before
computers so they were computing Fourier
transforms by hand and it took 42
n log n seconds so how big was the
constant I'm not sure yes the same
formula but they had computers so they
didn't know about these so these are
independent discoveries it's possible
there are more and so nowadays yeah
computer is the constant is a bit better
than 22 seconds but can we do better
because you're using it in so many
places and so there's been a reminder
research try to do this but there's no
really compelling results either way if
we don't know if we could do better in
general but we're not able to do that in
this work so yes so what can we hope to
do well we can hope to do with the next
best thing which is to speed up common
instances of the Fourier transform so
why are you using the story transform
well you're using it precisely because
it goes in in say compression the reason
you use it is because when you take the
Fourier transform the vector becomes
sparse so it was a dense vector with
lots of large coefficients and now it's
sparse vector with relatively few large
coefficients and if you have a lossy
compression then this is really easy to
see because you just only store the
large entries and you store nothing in
the small entries and then that has a
small amount of air and in fact in
lossless compression where you don't
want to have any air it's still true
that the reason that you get compression
is because it concentrates it in fewer
coefficients and that it's when you
represent it as a sequence of bits
it actually takes fewer bits to
represent the decaying shows up in the
slopes around the frequencies
maybe that's all maybe there's a little
bit of noise there hey I'm a left-hand
side he's asking where is that man in
the trailer
well maybe you have to do the redo the
transform every few seconds yeah I think
I think this may actually just be for a
snippet of it I should probably do the
actual computation for the real one yeah
yeah so then the natural question you
have is if your Fourier transform is
sparse then can you compute it faster so
that is a sparse for a travel and and
the base case we can assume that what
we're gonna suppose that X is ax X hat
is actually K sparse
it only has Kate on 0 coefficients and
in that case how quickly could we hope
to compute well we're not gonna get any
better than K log K because we could do
that then it could do better than n log
n in the general and you might think
that we couldn't do better than n time
did anybody Oh No that's why it's in
quotes we're not gonna do any better
because that would be that's right and
then you might think we couldn't do
better than n time because of the input
in the output size being n but that's
not really a lower bound because the
output size is only K cuz the only K
nonzero coordinates and for such vectors
the vector is actually structured enough
you don't need to read in all the input
to get enough information and so we'll
be able to do this in only K log end now
low signals are not going to be actually
sparse they're going to be close to
sparse and have some noise in every
quarter
and then you won't be able to this end
lower bound would apply if you wanted to
compute it exactly but lossy compression
tolerates some error that's proportional
to the size of those small coordinates
and so our goal is to get a result
that's only one plus epsilon times this
air away from the true for a fact and
we're gonna get this with K log n over K
times log n it's the l2 error of all the
coordinates it's a constant so I I
believe it's 1 over epsilon times I'll
give it more formal result later on but
it's your error proportional to the
magnitude of the smaller coordinates
very small case constant or logarithmic
then you get like pony logarithmic time
instead of linear time that's right
that's right
but in fact it's always faster than yeah
yeah so right so we haven't done the
general case what what it ought to be
even though we don't get that it should
not be over log of the ratio between the
large coordinates and small coordinates
and so here we're assuming that the
ratio is infinite in particular more
than polynomial and then okay so that is
the basic result that we're gonna get
and I'm gonna talk about more formally
what we do and what the previous work in
this area is so it has a fairly long
history starting with the work of
crucial David's man sewer building on
goal Drake Levin for Fourier transforms
over the boolean cube and then Stewart
developed this adapter
this too the complexes and then there's
been a fair amount of work just on the
complexes and the facets of these
results with by Gilbert mudra Krishna
and Strauss that got a K log to the
fourth end album and that's pretty good
but it is a fair number of logs it is
not great constants and so if you
implemented it and compare it to F of T
W which is fastest fourier transform in
the West there's a good implementation
of f of T then you need the sparsity
ratio to be more than about 40,000 for n
being about a million and if you look
theoretically you need K to be less than
n over log cubed and so it needs to be
very sparse in order to be better than
just doing a density and so our goal is
to be faster both theoretically and in
practice for a larger parcel eat the
mouse and so as I said we get a K log n
time algorithm for this percent of tea
and a K log n over K time algorithm for
the approximately sparse transform and
there's pointing zesting so we take the
error that we have is you take the true
Fourier transform and you approximate it
by the largest K coordinates you look at
the difference between those two and our
error is at most 1 plus epsilon times
that difference and if your error 0 then
this error will be 0
although the algorithm we have is sorry
they slower then yeah that was for
approximately
and to both of these are faster than the
F of T as long as K over N is less than
some fixed constant and we implemented a
variant of the first algorithm and for
that case the constant is something like
1% although it varies by the by what
side so what happens if you actually won
the first algorithm but on a vector
that's on the approx spouse you will get
C you could check whether it was
actually did satisfy this so you usually
get something that's approximately
sparse I am told by my co-author who is
more applied that the the exact
algorithm actually has better robustness
than you can show in theory and so so
you do get a cover with what I got to
show you but if you do a little bit more
to get a bit more robustness then it's
not as garbage as no it's just that like
you repeat a few times and then you get
so they're both randomized particular
there's a few caveats to our result so
one is that our output only has
logarithmic many bits of precision so
polynomial precision another is that we
acquired n to be a power of two the same
out the same restriction applies to the
Cooley two key recursion although there
are there are other density algorithms
that don't have this restriction that's
a pretty natural scripture and both of
these algorithms only succeed with the
cost of Bob and you could amplify this
probability but that would increase the
run again okay
we don't know you can make 90% 99% and
these statements would still be true can
you can you check it and answer in that
much time or do they take longer okay so
no you can't check it in fact one can
show that if you want to get this
guarantee exactly then you can't hope to
do this in less than any time because
you need to look at all the input you
get this guarantee okay so you can
decrease the error probability so you
can if you repeat it multiple times and
your arrow probably decays exponentially
in the number of times you're like how
do you combine you after yeah I do agree
this is a problem I think it would be
what what we ought to be getting here is
that the air probability is 1 over N to
the constant like one of us N squared or
something and you could increase that
cut that power I don't know how to do
that I think so one one place where you
can use it in practice i okay so there's
a couple answers here one is that so if
you're using something like this for say
an MRI machine where you're doing
samples then you just see what the
result looks good if it's not you retry
and so if we're decreasing the time it
takes and it just means that sometimes
it takes long another answer is that you
can you it so if you're doing it for
video for video compression then there's
sometimes there's two phases one where
you're going to look through your data
and estimate what you're going to do in
the next phase like where we're in the
video you need to have more compression
where you need less compression and so
there it doesn't matter so much if you
screw up on a case
that just worsens your compression rate
and so then that gives you a good
estimate for where to do so even if in
temperature you tell as I pick somebody
and ask do you know when it's garbage
because so so how do you know that the
MRI isn't good or that the compression
right right and to that that's the one I
see I see
so I think for MRI you can know because
it's an image and you can look at the
image and see if it problem is that if
you want to actually guarantee and then
only one over one percent over 30 so you
actually can do something about that
then you actually many so maybe know
like right so I think you can change we
adjusted how many times we did this to
get about 90 percent success rate I
think and that's where the 1% is he
takes it 75 up to 19 the only right the
only thing this affects is whether this
is 1% or something else all the
theoretical results this could be an
arbitrary all right so
to discuss the exactly sparse case for
most of this talk and then give some
suggestion for how the proximities worst
case works so that's a bit more
complicated so our goal is to compute as
far as Fourier transform in kala and
time one can show that most of the
interests most of the hard bits come
from not finding all the K coordinates
but finding most of the K coordinates so
finding some vector such that the
difference rather than being case versus
K over two spots and then you can repeat
on the difference and so then the time
will be K log n plus K over 2 log n plus
K over 4 log n just trying to get weak
sparse recovering and to see how this
works
I think it's useful to consider first
what happens if rather than doing
arbitrary linear measurements you can
rather than choosing only 48 samples you
can choose arbitrary linear measurements
of your back and so this has been
studied in the sketching literature and
there's an inspiration for what we're
going to do so the idea is you have some
vector X with n coordinates some K which
are nonzero and you hash them down to B
being about 4 K box so that each each
bucket has some subset of the
coordinates and then for each bucket you
sum up all the entries of that bucket
and you sum up over all the entries of I
times its value where I is the index and
the idea here is that we don't know what
happens in the buckets with multiple
things very well but if you only have
one coordinate that's nonzero landing in
the bucket then you can just divide
these two values so you compute B prime
over B and that'll give you the index of
the coordinate and you look at B and
that will give you the vet so this gives
you the week's worse recovery guarantee
you need so every coordinate has a 3/4
probability of landing alone in its
bucket if it's alone in this bucket then
they recover it into that
you know it's you don't know whether
it's isolating this bucket but only a
quarter of the entries will they're only
gonna be at most K over four buckets
that are badly recovered and so there's
catered for remaining to find and care
for that are errors that you introduce
and so you've gone from being case bars
decay over to stores is not a subset of
the O's that a coordinate system
something straight junk that you added
so there's because there's most K
buckets that have anything non zero in
them of these K buckets three-fourths of
them are estimated correctly and 1/4 of
them are estimated wrong and so there's
a quarter that remain in a quarter that
you've introduced for K or two total
remainder in residual error yeah so we
do consider that it's so what will be a
problem is that when you do that it
doesn't work very well if you have
pairwise independent hashing you often
need or you need that if you just have
simple pairwise independent having then
you doesn't actually work as well as you
need and we can end in the Fourier
setting we can only implement pairwise
independent hashes so right now what
you're getting is some linear hash from
the top to the bottom but you eventually
can't choose the linear hair yes we
can't we can't we can't implement this
arbitrarily we can only take Fourier
samples of our vector and our goal is
given to you to take some set of Fourier
samples and combine them in a way that
gives us things that are close to B and
V Prime
this is again something that you could I
mean this is still a heuristic you could
actually run right just imagine that the
four-year thing is exactly this and hope
for the best
well it is exactly like a pair wasn't
even hash function and yeah yeah you can
right and so what we're gonna do is
we're gonna we're gonna lose a log n
factor in time and number of it in time
in our measurements to estimate
something that's close to B and B prime
and based on those compute the result
and see how this works let's look at
what we actually see we actually have
time domain access to some vector and we
want to compute its sparse Fourier
transform suppose for for now that we
could we had as much time as we wanted
but we could only look at a small number
of coordinates so you can take this the
first B coordinates and then take their
n-dimensional DFT so it's a lot of time
but it's not that many samples and what
we've done is in the time domain we've
multiplied by the Boxcar film which is
one on the first beat terms and 0
everywhere else and so in frequency
domain
we've convolve dwith the Fourier
transform of the Boxcar Children
that's a sinc function so we've involved
with a sinc function sine T over T and
gotten the results on the right which
relates to the actual frequencies now we
can't actually compute this but we can
do is we can take take this and only
take the B dimensional DFT of the of the
first B term now this is done is it's
aliased this so it's taking the first
three terms plus the second p terms plus
the third B terms the rest are all 0 and
then we take that DFT you get a sub
sampling of the original of the original
and so we've got a sub sampling of the
frequency involved with this thing you
can actually compute this in P log B
time and this kind of relates to the
frequencies we have and it's kind of
like the hashing that we wanted
particular each frequency most of its
mass goes to the bucket associated the
point at its value although there's some
amount of leakage
where we've where the mass has gone to
adjacent the adjacent pockets into the
further way buckets but ignore it that's
the main issue we have that we haven't
actually computed these beads exactly
but if you ignore this issue of leakage
then there's a couple other issues that
are relatively minor so one is that we
don't have any randomness so far
you need to have random hash function
but this was solved in previous work by
yoga mudra Krishna and Strauss well
rather than accessing the first B terms
you access a random subs random
arithmetic sequence of B terms and that
permutes it in frequency domain and so
you get something that's close to
pairwise independence and the other
issue is that we also needed to have
this is B prime which was some of the
index times of that so you divide the
two and get the index for this rather
than accessing the first B terms you
could access a second to be +1 first
term so this is a time shift by one a
shift in by one in the time domain that
corresponds to a phase shift where
you're their new vector the I've term is
omega the I times were used to be and so
then if you had no leakage in the button
the buck would be the sum of Omega to
the I times X hat sub I divided the 2
you get omega to the i- i tells you what
so the main issue we have here is this
issue of me
whereas the leakage come from well it's
because we multiplied in time domain by
a boxcar filter we've involved by this
sinc function sine T over T and sine T
over T is non-trivial everywhere it has
at least one over and everywhere but all
we really cared about in the time domain
was that it was sparse we didn't need it
to be one at all these quarters we could
have multiplied those coordinates by
other values and so you can consider
other filters and particularly consider
you could consider a Gaussian filter you
have a Gaussian filter then that decays
X e to the negative T squared so you can
get down to 1 over poly n with root log
n buckets it's most person time but you
can just cut it off after that well the
point is that if you have if you also
include root log and standard deviations
in the time domain then the it then is
basically negligible it's less than 1
over poly n in everything that you've
thrown out and so that's where you get
this ringing here but it's not
significant and so you get something
like root log n buckets oblique which is
much better than n buckets but is still
not perfect another thing you could do
is you could you could widen your freak
filter in the time domain and you can do
this because it's now you make it now B
be log n sparse you can still look at
that in be log n time and that in
frequency domain narrows it so it
actually won't leak at all because so
the Boxcar filter doesn't decay at all
here so it decayed is 1 over T here the
the Gaussian decays
the negative T squared and so you have
if you include root log n standard
deviations in both time and frequency
domain then your sparse in both domains
up to this trivial air
we want to have a filter that's close to
sparse both in time and frequency domain
well you can you like you could you
could hope to be these sparse and n over
B sparse that's that's the best you
could hope for and we're not able to get
that but we're getting up to get we're
able to get be sparse and enema be log a
nice box with gaussians well the box our
filter doesn't get that there is there
is so so in fact yeah but it does give
you better but only by a factor of two
so for implementation we will use adult
Chevy chef window function which Cape
saves a factor of two over a gap see ya
and so another thing you can do is you
could take this down into a wider and
time domain which makes it narrower in
frequency domain and but the problem
with this is now when we convolve it in
subsample we're likely just miss the
spike that if we have a large frequency
its widened a little bit by this
Gaussian but when we but not enough that
when we sample every end of our beef
term to find it so we just we wouldn't
miss the contribution for this so what
we do is we take this Gaussian with a
very narrow spike and then we convolve
it in the frequency domain by the Boxcar
filter which in time domain means you
multiply by the sinc function but since
the gaussian was sparse this is still
sparse while in the frequency domain it
decays very sharply because of the
gaussian and it's wide enough because of
this but you are me you're supposed to
time to main because we took the
gaussian the Gauss we've taken a sparse
Gaussian we cut off the negative old
terms and multiplying by the sinc
function doesn't deep doesn't make it
less Bar
and this involving the frequency domain
by the vodka trend suggests that way
when you do end up sampling you don't
miss the exact exactly and so this fixes
or leakage problem so that's right
before you got a good bucket so we have
our filter that is in time domain D log
and sparse and in frequency domain looks
kinda similar to a bhaskar filter in
particular its width is n over B of this
90% of it is the pass region where it's
basically one if there's only a 10%
region where it's neither basically zero
or basically and our algorithm becomes
pretty simple we take our vector you can
multiply it by this filter in time
domain because that takes B log n time
and if we could if now if we could take
the full DFT we would get the Green Line
on the right we can't do that but we
can't alias it down to B terms play
something up the first B the second B
and so on and then when we take the B
demential DFT of those feed values we
get a subset of a on the green line
which is these red points so you can
actually compute these red points in B
log n times and what's nice about them
is we know for each bucket around them
that if there's only one coordinate and
that coordinate is in the good region
there's the white then the coordinates
now I know is the value we've computed
so we don't know if it lands in the red
region we don't know if there's two
frequencies in the same bucket but most
frequencies will be alone and in the
white region and we can estimate their
value correct so this lets us estimate
the value of most of the quarter we also
want to state their location so we
dressed time shift by one and repeat so
now their new value is their old value
of times omega to the i divide those two
and you get omega to the I on this
this exact this is these conditions
there's some error that is polynomially
small because we've cut off for
gaussians a true log and standard
deviations that's a 1 over N to the C
okay so you compute I up to 1 over
polynomial error and then you could
round then you get the index so this
gives you the week's press recovery
guarantee you're finding most of the
coordinates and your peep on the
residual there's a there's a caveat here
a complication here which is you know X
in the time domain you know X prime in
the frequency domain if you can't
actually compute their difference but
you can compute the contribution of X
based on the procedure so far and you
can hash X prime directly see exactly
see what's contribution is from the
filters that you have and see you can
because X prime is sparse in because our
filter is sparse so you can do this and
that gives you the K log n time as far
as for a traffic's prime was
approximately correct values or not so
when you subtract X minus X Prime you
don't really get something which is K
over 2 sparse get something which is
approximately here with this bar right
so I Venice so you can make that error
be X be polynomially small into it won't
actually contribute much yeah in general
you're gonna have in the approach in the
in the more general case of having
approximate values approximate first
vectors that will be an issue but it is
doable
the sparse example but as you've seen it
requires having very good precision of
this estimate so you're not gonna get
that if you have nonzero coordinates
everywhere so let's see how to do it in
the approximate case the idea is we
could do the same thing and we'd
estimate Omega to the I but if there's
noise from having values in all the
buckets then we won't get Omega the I
exactly we won't get it to this one over
polynomial error we're actually going to
get a constant amount of error at the
regime that we care about and so we
can't get the full log and bits we need
to identify but we can still get a
constant number of bits of information
about and we need to identify the index
among the N or K different elements of
the bucket so we need to repeat log n
over K time and then we repeat then we
make B prime rather than being shifting
by one you can shift it by C and now X
prop the the we'll be estimating rather
than making the I are gonna be made
estimating Omega to the CI with some
noise and if you repeat this for log n
over K times with different CI in each
stage then that gives you enough
information to do the recovery so you
just repeat and repeat linear equations
about yeah yes it's like that it's not
it's not quite linear equations but it's
like you know basically CI mod n with
some additive error and each of those
gives us gives you a bit of information
and if you do that randomly that you can
think of this as a code and if you do it
randomly it gives you enough information
but we wouldn't know how to do the
recovery in linear time or long
quadratic time really is what we need
here
and so he chews them with a specific
distribution such that we know how to do
the recovery I was more interested in
this you know is any where the errors
not kind of atmosphere that the area
that's added on to say stochastic so I
don't want to reconstruct the signal
like about add that Gaussian noise or
something to my my signal then presume
they could hope for a better so yeah so
if you have Gaussian noise you can't
hope for any better sample complexity
you do you still need K log n over K
samples they guarantee no could you make
a guarantee with their guys to know the
Gaussian noise is gonna matter like sort
of what we do is we hope for the noise
to be as good as a Gaussian would be
well you can't the Gaussian noise
matters it is adversarial but because of
the randomness we're doing you you can
make it be random so your algorithm is
not any easier if instead of adversarial
it was just
joseon's around somehow it actually gets
treated okay so so in fact what you get
is that you have a distribution on error
so this is two standard deviations of
your error and so most of the time it
lands in this location and it and you
can do it with a Shep you should have
gone and looking at the variance of your
air okay another question is what are
the constants here and so we implemented
a variant of our algorithm and compared
to fftw and to a f of T which is the Ann
Arbor fast Fourier transform so here we
have for a constant K being 50 as n
increases what happens for our algorithm
in red for a F of T and blue and fftw in
black GW is basically linear in n both
our algorithm in a t don't depend on n
very much you know is a couple orders of
magnitude faster than T and ours is
faster than DW as long as n is more than
about 80,000 which is corresponds to K
over N being about half a percent here
we have it for n being 4 million what
happens as K increases and so again we
get that VW depends on n are mostly
dependent on K and we're faster for K
being less than about a hundred thousand
this is for random binary Plus random
binary just 8k ones and zeros this is
for the exact case there's no noise what
a more typical value of N and
look like in some applications yes so if
you look at images then it would be
about 7% to have almost all the energy
know that most applications would not be
the exact case but if you have more
specialized domains such as so there's
40 light fields which are these new
cameras where you get light coming in at
every pixel then that has a lot more
structure than regular images because
adjacent pixels have basically the same
view and they're I'm told that it's much
much sparser although I don't know
should be crossing over at some point
the dotted blue is Kellogg to the fourth
and where K log N and the that was Anna
Gilbert's work that top one which is
which was the thing that you were
improving yes yeah so this is previous
births per a transform this is our
sparse for so this is for as K increases
for a constant n and so ours and the
previous Fourier transform depend a lot
on k f VW doesn't care at all and again
it's about the one percent ease with
yeah so here's like have a percent
yeah okay so we gave a kilo again time
algorithm for exact first recovery K log
n over K log n for proximate to W now
question that arises can you do better
is there gonna be another people that
comes along it does better and so K log
K time you can't beat without improving
the density algorithm so K log n it's
pretty good and then this K log n over K
comes up because the number of samples
you need to take to get the sparse
recovery guarantee has to be at least K
log n over K so it's fairly natural that
we take a log factor more in time than
the optimal sample complexity now that
lower bound that we had only applies if
your samples are chosen independent of
the actual values then you get so if you
could choose them adaptively where you
look at some and then choose where to
look at that you could hope to do better
but we showed that you can't do better
than a log log factor and so we're
fairly close to and these lower bounds
they come from a more from a more
general theory of looking at recovering
a sparse vector X from arbitrary linear
measurements that for this Fourier
transform is a special case of where
your matrix has to be a subset of the
universe for EMEA so the rest of this
talk I'm going to us this more general
sparse recovery theory that by
researchers back so maybe easily can we
say that some message of what you're
saying is that we can pretend that the
4-year is kind of like a random
measurement no so so these are for lower
bounds right so they say whatever matrix
you choose you can't do better yes
not just as much unfortunately yeah but
you need you need more measurements but
but I guess I'm saying that the
techniques are strongly related to those
four generals present as like our
inspiration was from our choosing
arbitrary so if you're going to actually
show us the proofs how much the full
proof I didn't finish them at Madhu's
reading your and win there was a lot of
handle a today I'm just curious to know
how much so in the exact case there was
not very much hand waving in the
approximate case there was a lot of
animating in I didn't describe how that
what the error correcting code is the
exact case was actually as all the ideas
and if you tried to go back down and and
they all it down you can do that okay so
the general compressed sensing results
for Fourier yeah you actually need K log
to the fourth and samples so so we show
this many samples as well as sufficient
if you use the general theory you would
we only know that K log to the fourth
end examples so they were a different
method of getting the same and we
simulation but they just didn't know
either there's not really relation now
they're totally different techniques
that lose the same number logs
our so our so that'll be in sambuca
plexi but in time complexity it would be
n poly log n for the general compressed
sensing because all familiar you care
much more about sample complexity than
about time right that's why it's n poly
log and rather than n K so this is
better than was previously known for
sample complexity okay so more questions
that I was expecting even though I do
know this guy all right so I'm going to
talk about sparse recovery generally in
your measurements and this is this
appears whenever you can do linear
measurements easily so sparse Fourier
transform is one example where you can
sample in time MRIs are another example
where you can take for a measurements
because nuclear magnetic resonance is
the technology used to memorize just in
hardware it observes Fourier samples of
the image and so you hear a lot about
the sample complex C which is directly
corresponds to how long you spend the
patient needs to spend lying still in
the machine there's the single pixel
camera which is a prototype Hardware
being developed at Rice which switches
the architecture of a camera so that
rather than having a million different
photo sensors you have a million
different mirrors that can either shine
onto a single photo sensor or not and
this is useful if photo sensors are
expensive and invisible light photo
sensors can be made of silicon really
good at working with silicon if they're
not expensive but if you want a far
infrared camera then you need to use
gallium indium arsenide or something
like that and that's really expensive
and so a megapixel camera will cost
$40,000 and said you can have a mirror
array that they make in projectors and
so this can be done for cheap early
now you're choosing a subset of the
pixels to shine on to your photo sensor
and so you can do that for many
different in sequence for many different
subsets of the pixels and those are
linear measurements of the yeah they're
just they're aluminum and aluminum is
reflective yeah I haven't thought too
carefully about that but it does seem
too so in streaming algorithms if you
have a router and it wants to keep track
of some notion of who is what packets
have come through it it might want to
record say for every IP address how many
packets from that IP address have come
through this router but there's a lot of
IP addresses you can't store a count for
each one and so you can store a
compressed stash where you choose a
matrix a you store a X where X is a
vector and then give in a packet you
have an update Delta to x given your
stored value a X so you can add a delta
to get this the sketch of the song and
do genetic testing you have a bunch of
people and you wanna know which of them
have our carrier for some disease so
wrapt if you could one thing you can do
is you could test each person
individually but then almost all the
tests are gonna come back negative if
you're not getting much information for
tests and so R is you could take the
blood samples of multiple people and you
can mix them together and then you test
that mixture to see how many people in
the mixture had the disease and then you
get much more information Cortez
well the idea is to do perfect right so
if you put your blood into multiple
mixers then you can figure out what show
those people have so I've done work
that's been inspired by all these
applications so Express Fourier
transform describe also they're for MRIs
you care much more of the sample
complexity they have a couple of works
that are reason that try and improve our
sample complexity at the expense of
computational complexity and so so one
of them is for the one dimensional
Fourier transform just for the extension
computational complexity the other one
assumes straw more randomness on the
input for imaging you are observing an
image and so you want your result to be
visually similar to the image the actual
image and standards for us recover used
in l2 metric to measure distance but in
the image retrieval literature people
have thought about other metrics that
might do a better job of representing
visual similarity and one is earthmover
distance a little bit there should be
and so we've shown that you can get
earthmover distance as your norm for
compressive sensing in streaming
algorithms so one classic streaming
algorithm that is implemented in
practice for example in the standard
library of MapReduce at Google is called
cow sketch by Tara Carter and Farah
Colton 2002 and we showed that the same
algorithm that people are using in
practice actually satisfies a stronger
guarantee than this classic analysis
show and then for from deny testing we
were inspired to think about adaptivity
so this is where you
you mix some sets of blood samples to
test the result and then you choose
another set of samples to mix and we
show that this lets you give an
asymptotic improvement on the number of
measurements and I have like three
minutes is that right
all right so I will basically skip what
we got to do here but so the idea is you
have some unknown case for a sector and
you want you can observe linear
observations these are like V dot X or
some vector V and then you can choose
another V and repeat and you want to
satisfy the same price recovery
guarantee that we had before
non-adaptive it requires K log n over K
samples adaptively we can improve this
from K log n over K decay log log n over
K and so it's what can apply to genetic
testing also the single pixel camera it
does them in sequence and so you could
choose the later filter patterns based
on the previous one for routers you
can't hope to do this because the
network isn't gonna replay the data and
it corresponds to multiple paths
streaming out but for things like
MapReduce where you use where you can
use streaming out as a proxy for
external memory data structures then you
can do multiple paths tweeting out its
activity of this I'm going to talk about
one sparse recovery and so so the
problem is you want to find a one first
signal and for simplicity suppose it's
just one at that location and Gaussian
noise everywhere else so we start out
not knowing where it is we can shoot we
can't hope to get much more than one bit
of information we can make our vector B
negative one on the first half positive
one in the second half so we would get a
sample from one Gaussian or the other
guy and depending on which half I is in
based on that sample we can we can
identify which half work now we can make
our vector B zero on the second half of
the input and a constant on the first
half and so our noise have gone
by a factor of two and so you can fit in
twice as many gaussians and get for two
bits of informations and now we can
distinguish sixteen things and get four
bits of information and in general given
B bits of information we can restrict
your set of size n over 2 to the B our
signal noise ratio grows by a factor of
2 to the B and you get B bits of
information and so to get log n bits of
information takes log log n and we
showed in fact this is optimal you can't
do better than log log n and we
implemented our algorithm compared so
this is blue is if you use Gaussian
measurements Green is if you use our
algorithm and as a function of log n
ours is about log log n dashing
measurements take here you can also if
you look at the amount of noise for our
algorithm you can start in a later round
and so as the amount of noise increase
decreases so the signal noise ratio
increases our algorithm takes three
measurements you use random Gaussian
measurements then in fact you don't
benefit after so basically my talk so
gave results for this first Fourier
transform and then talked about the
broader implication the broader aspects
of the new your measurements there's
lots more I want to do so I think the
Fourier sample complexity is extremely
important thing because MRIs do that
because you can because you can also
sample a lot of vectors are sparse in
the Fourier domain and you care about
the sample complexity and we have some
partial results that can prove this but
there is a ways to go another thing that
may be useful is Varsity is one form of
structure but there's more to structure
than just sparsity and in the general
sparse recovery literature there is this
idea of model-based suppress recovery
which is useful if we don't know how to
incorporate that into Fourier
measurement
but even beyond that there's more kind
of structure you could hope to do and
finally I'd like to get a better
constant on the number of measurements I
think it's unlikely we're gonna get a
type constant on that in time but a
number of measurements there's a lot of
in logs between sparse recovery
comparative sentencing and coding theory
and in coding theory you care what the
constant in the rate in the capacity is
and you want your rate to be one minus
epsilon times capacity and our lower
bound techniques being similar to those
of coding theory are probably strong
enough to get tight constants but our
upper bounds need some work before we
can get so the model-based compressive
sensing you say you're sparsity pattern
the the set of K coordinates comes from
some family and rather than being all
entries K things where you get require
log of n choose K being K log n over K
measurements you say it's a tree or it's
from its in block so they tend to be
next to each other and then the number
of measurements to the log of the family
size but that also isn't really all you
care about because you may know that
this coordinate is going to be large and
this coordinate is going to be small or
like medium sized or there is a
correlation between core to coordinates
and we don't really understand how</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>