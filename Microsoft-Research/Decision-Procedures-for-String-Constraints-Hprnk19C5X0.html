<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Decision Procedures for String Constraints | Coder Coacher - Coaching Coders</title><meta content="Decision Procedures for String Constraints - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Decision Procedures for String Constraints</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hprnk19C5X0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
good morning everybody this is Peter's
candidate talk and a pleasure to welcome
him back so Peter homier is from the
University of Virginia has been working
with Wesley Weimer on various problems
related to strings and that's also the
topic of his talk today so personally I
know Peter for roughly two years he or
actually no even two and a half or three
years he did his first internship here
with me in 2010 which was work related
to strings and then last year he
continued the internship kind of was the
second internship following that which
was on the same topic and some of the
stuff that he will talk about today will
touch upon these these topics or these
things that he did hear that time so
with this and most let me say that peter
has been working with other stuff other
areas like empirical software
engineering and sensor networks so he
has a pretty broad scope from from other
areas as well so with that I think I'll
give Peter start people let Peter stop
all right Thank You Marcus thanks
everyone for coming so i'll be talking
about strings the working title of this
talk is Peter talks engagingly about a
single data type for one hour so in
order to make that happen I figured I
tie this to the audience by saying you
know imagine you are currently using
your laptop so in fact you know you
might have a lenovo which looks a lot
like this imagine you're browsing a
website let's say you're looking at
Stack Overflow to scout out your
competition turns out jon skeet has some
impossible number of points you will
never be able to beat him but you know
you're looking at his profile and the
developer who implemented this page
decided would be a good idea to allow
users to provide their own profile
picture so in this case they can provide
a address that is the address of the
image file so in this case I have a very
bare-bones image tag on the slide and
the
is that will treat the source attribute
in this case as untrusted input and as a
developer let's say I'm quite confident
but not super security savvy I might ask
what could possibly go wrong and of
course the answer is it depends it
depends on what we do with this
untrusted input so the address is user
provided we should make sure that it
doesn't do anything bad to our page and
let's imagine that we don't do anything
in that case an attacker might provide a
specially crafted attack string in this
case an address that contains a single
quote and allows the attacker to escape
from the source attribute and enter
let's say their own attributes for
example an onload attributes that loads
arbitrary JavaScript into the page so
this is a call site scripting attack you
may have heard of them this is very
common in dis perhaps not the most
exciting example of one but nevertheless
and the problem here is that you know
the attacker can run code whenever
someone visits this profile page and in
addition to Annoying alert boxes they
might execute website functions with the
privileges of the user currently viewing
the page and this is problematic if
you're a bank or if you really care
about your score on stackoverflow
alright so there's a lot of research on
cross-site scripting I imagine going on
microsoft academic certain looking for
this you'll turn up hundreds of papers
for some reason in a variety of areas
not related to computer science as well
but I'll asserts that the majority of
results come from computer science and
they tend to try to mitigate this
problem if it happens or prevent
cross-site scripting all together
through some constructive means either
way the point is there's a lot of work
on this and part of my research aims to
generalize some of the insights gleaned
from the existing research on that says
cross-site scripting or SQL injection in
short vulnerabilities related to string
manipulation so with that I'll give you
a dark slide the talk will be two parts
so I'll talk about my swing constraint
solving work which is roughly my
dissertation
for the first let's say half and after
that I'll briefly touch on the deck
project which is my internship work with
with Margaret from the last two years a
few other things so let's talk about
string constraint solving sort of what
it is why I care and why you should care
so the structure of this will be roughly
like this I'll go into some background
to sort of explain what I mean by string
constraint solving and why you should
care and also following that I'll talk
about sort of building an interesting
constraint solver in this case sort of
my first paper on the topic and then
i'll talk about tuning which is the
topic of a few other papers trying to
make this sufficiently fast for people
to actually be able to use this stuff so
roughly background definitions
evaluation or performance tuning all
right start with background so there's a
number of constraint solvers out there
you may have heard of some of them there
is this notion of you know doing program
analysis using a constraint solver this
is very common it is prevalent for in a
variety of contexts so everything from
automated testing to static analysis
doing model checking and so on you
typically end up using one of these so
this is sort of the MSR potentially
redundant slide there's a number of
different implementations one key point
to keep in mind is that there's a
standard input format so at least in
principle if you don't use terribly
esoteric features you might interchange
these tools and this also allows for you
know doing annual competitions and so on
so this is the state of the art in terms
of mathematical constraints typically
involving let's say integers bit vectors
data structures and so on so a very
simple example if i have x squared is 25
i want to know what X might be I can
solve for x using an smt solver looks
something like this the declarations
roughly match what we have and sort of
strict math notation and in this case my
my result is X can be five in this case
there might be other solutions but we
just have one particular example here
and what's most important
is that we know that it is possible to
satisfy these constraints so what about
strings it turns out existing SMT
solvers have not traditionally had a
theory of string constraints so bit
vectors are well represented like I
mentioned sort of various types of
arithmetic on integers some stuff
involving quantifiers even but not
necessarily strings so I've said here on
the slide that reasoning about strings
is difficult and I mean that in sort of
an informal way for programmers it is
apparently difficult because they write
websites that have errors in them like
cost side scripting vulnerabilities and
it is difficult for automated tools sort
of for other reasons which I'll go into
so in that case I guess the definition
of difficult is a little bit more formal
so there's a number of existing
approaches or rather approaches that
exist now that did not exist previously
and these are the names of four tools I
will feature in the talk and each of
these tools is essentially a
domain-specific smt solver let's say a
string constraint solver for just string
constraints so d pearl is a tool that i
presented at PLD io9 Pompey was a month
later had a style 09 and I'll assert
that all of these tools were published
let's say in the last three years yeah
although that goes way back or a shorter
it's yeah so this how do these so mmm so
the question is what about Mona yeah yep
so in the mid-90s there's a bunch of
research on writing dedicated solvers
for monadic second-order logic using
either multitrack automata or treat
Amida depending on which fragment and
those are in many ways theoretically
similar so they solve in the Atomic Age
multi-track odometer case constraints
that are equivalent to a regular
language but they are not sort of
specified they're not specifically
designed for string constraint solving
in the way that we do here so there's
one paper I forget where I forget the
venue there's a recent paper that uses
Mona to solve string constraints and it
turns out this is possible and you can
express regular languages and so on this
is well known however it is not
particularly performance so they're
still sort of space for domain-specific
tools and so on and I think you know be
very interesting to look at sort of the
deeper implications for let's say Damona
implementation very highly tuned and so
on and we have some of the same stuff
showing up in stream constraint solvers
that they did for Mona so the use of V
DD s for example yeah these tools
domains they are domain-specific so
question is what makes them
domain-specific the tools are
domain-specific in the sense that you
know very sort of bluntly foot and they
are presented as starts in the in the
papers that sort of published them they
are domain-specific in the sense that
they solve in this case and regular
language constraints so if you do
symbolic execution on some code and it
generates some string constraints so
stuff like this string should match this
regular expression then you can use one
of these tools to solve those
constraints does that answer your
question generally but in terms of logic
I mean what
I mean compared to Mona for example part
of these languages like logically
formally more restrictive and so that's
an interesting question and the short
answer is I don't know the sort of full
specifics at least not well enough to
you know state them on the record but I
think so a fragment of monadic
second-order logic is definitely
representable using automata many of
these constraints are representable
using automata with their structure it
slightly differently so my guess is that
there would be very similar so there's a
notion of repeated or iterated automata
intersection being pspace complete and I
believe that is the case for sort of
both things in this case but as the best
I can do in terms of specific upper
bound let's say are you satisfied chance
thanks so yeah so for different tools
all published in the last few years and
the basic notion is that will generate
constraints that look an awful lot like
string manipulating code in this case
shapes to look like C sharp and we'll
pass them to sort of any of these tools
they have roughly equivalent input
languages sort of sufficiently similar
to be engineering feasible and we'll get
out some answer so where previously we
got out of a response that says X should
be 5 now we get a response that says
string variable a should get concrete
string value a be um so very briefly
I've shown a very short example
constraint talked about different
solvers and what the shape of a solution
looks like so an assignment to string
variables but what that doesn't tell you
is sort of where the constraints come
from so I've already touched on this but
the basic notion is that will sort of
punt on this so we'll say stuff like you
can use standard techniques to generate
constraints that include string
operations like in this example let's
say symbolic execution so if I want to
exercise the if statement in this code I
can generate a constraint system that
instead of branching on r dot is match
of a asserts that that's true and then I
can solve those constraints and find
inputs that exercise this path through
the code
but in general we separate constraint
generation as a problem from constraint
solving and this is sort of common to
all of the tools and presentations that
I have shown and we'll focus at least
for purposes of this talk on constraint
solving so we'll assume there's a
reasonable way to generate stinking
stains without really going into the
details of how to better generate
constraints for Strings in practice we
evaluate whether our our assumptions
about stream constraint solving or our
assumptions about string manipulating
code are correct empirically so we'll
use standard techniques and see if they
if we can solve the constraints that
result all right so so far I've talked
mostly in terms of examples and I'll
sort of continue to do so but in general
I think this raises a question of scope
so what is a string constraint as you've
already sort of asked and what is not so
one example might be you know md5 is a
cryptographic hash or at least a one-way
hash and it is technically a function
that takes a string and outputs history
so if we wanted to do constraint solving
and we sort of didn't narrower scope
that would be fair game except there's
you know sort of separate papers on you
reversing cryptographic hashes and it's
a very domain specific problem that we
may not want to solve so of a different
example which if you've seen the slide
you are prohibited from answering but
you know how hard is it to do a simple
one string one regular expression match
in Perl so some of you may have seen
this slide at vm chi or maybe last year
and the answer is you can't see that
because it's a color blue very close to
the black it's NP hard and the reduction
is from three-set so this is a 5-line
piece of pro code that given a string or
rather given an array representation of
a three-set problem turns it into a
single string and a single specially
crafted to reg ex that use
is back references in order to solve SAT
so if you ever need a five-line SAT
solver there you go so this is not my
result this is something that was sort
of floating around on the pearl mailing
lists some number of years ago so this
still doesn't answer our question of
scope but in general it does answer our
question of can we model the sort of
anything that shows up in a wild and the
answer is probably not so md5 is an
obvious example regular expressions in
Perl perhaps not so obvious so it turns
out that the exact formal language class
of pro regular expressions is sort of
nebulous so we'll focus in this talk on
constraints that we do know how to solve
and in practice that usually means we
will go with the sort of strictly
regular component of real-world regular
expressions all right so let's talk
about my first attempt at building one
of these tools also sort of see the
implications so I'm pldi 09 I provided
some definitions of basic string
constraints and the little logo on there
is meant to indicate that we define this
stuff in so we went through the
trouble of defining sort of strings from
first principles string constraints of
interest from first principles and then
showing that our core solving algorithm
is actually sound and complete relative
to those definitions and we also provide
a implementation and this is one of
those stories where the implementation
is strictly not related to the formal
proof but you know it solves string
constraints in practice and we use it on
a existing benchmark to show that we can
generate attack inputs for 17 known SQL
vulnerabilities so we look at some
corpus of PHP code we use off-the-shelf
techniques to generate constraints and
then we solve them using our tool
measuring let's say running time and
whether or not it works so rather than
defining formally what the constraints
of this particular presentation look
like I will focus on
a quick demo so there's a online web
version of this tool available if you're
currently on your laptop the URL is
something long @ virginia.edu and if you
happen to be playing around with your
phone it turns out i wrote a quick
script in touch develop so if you're
following along and look for the one to
follow along look for Easter solve
script in touch develop and it is the
shortest starts to develop script you
have ever seen it just links you to the
website which I will show you now
alright so for this short demonstration
I want to emphasize a few things and
this for this particular tool variables
represent regular languages and I took
that very literally in the tool
implementation so they literally are
utama definitions concrete ones so
there's two sort of aspects to this one
is regular inclusion constraints and the
other is concatenation so it turns out
if you support those two operations you
can do the majority of you can model the
majority of constraints that originated
from the symbolic execution of string
manipulating code in some way or another
sometimes less direct sometimes more so
on the slide is a short example so I
apologize for the sort of limited
visibility of black and colors I guess
the the main ocean is one that Tom
already raised is that there is an
equivalence here between other logics
and what we call swing constraint
solving so the example I've seen here
I've I have on the slide is something
where I want to do modular arithmetic so
I have to automata twos and threes that
represent in unary the set of numbers
that are multiples of two and three and
what I'll do is I'll define nine as a
single number that's a little tedious in
an automaton representation but let's
imagine we can also do Araxes and what I
want to find out is how many ways I can
use to put two and three together as
multiples to
m9 so this is truly the arithmetic
example from middle school let's say so
the way I express that constraint is by
saying to concatenate of 9 which is a
single string so if i want all solutions
i use solve all in this case and then
there's some additional machinery
required to select the two solutions and
display them so let's hit submit on that
and hope that i have internet so looks
like it ran successfully so again this
is a tool implemented in let's say 2008
so it's been a while the result is two
distinctive solutions the first looks
like this tues it looks like we
multiplied by three so i end up with 6 /
0 for 9 switch is 1 and 13 so in other
words 6 plus 3 and my second solution
looks like 0 to s and exactly three
threes so I've done a very basic modular
arithmetic example to show let's say a
decomposition of the number 9 so this is
an example meant to illustrate sort of
interesting properties most notably the
use of concatenation in this case to
separate the twos and threes and get
separate solutions for them the other is
that it is not sufficient to just say
here's a regular language for each
variable and I have two disjunctive
solutions it does not work for me to put
together 0 to s and six taken out of
three so in other words these solutions
are entire inherently disjunctive I
can't just sort of merge them together
into single regular language so if I
have multiple solutions they will be
sort of strictly separate so in practice
we probably won't be doing modular
arithmetic but I figured it would be a
sort of interesting example to show
own equivalents using a new tool in this
case so i do not i mean it's it's really
variables are regular languages we have
grounded regular inclusion constraints
so there's a single constant reg ex that
is the right hand side of this subset
constraint and concatenation so i can
say stuff like variable one concat
variable two should be a subset of this
regular set which is really all the core
constraint language handles and this is
also this is equivalent to constraints
in other papers so the hampi core
language is essentially the same except
that we bound the length of each string
because it is a reduction to set and
this in this case the length is
unbounded so moving on the evaluation as
I mentioned was on a existing corpus
produced by wasserman ensue at PLD io7
they do a static analysis that finds SQL
injection vulnerabilities and the sort
of main motivator at least for me
personally at the time was the output of
this tool which looked a lot like there
might be a bug on line 58 of your code
and nothing else so we wanted to add
indicative inputs to this static
analysis in order to do so we had to put
together some additional machinery so we
had to find a path to this particular
potential bug and symbolically evaluate
that path to find constraints so we use
running time as a metric like I said
it's 17 vulnerabilities so it's not the
biggest corpus for doing sort of
statistically significant performance
results but we wanted to know if this is
feasible just as a first cut and the
results are that yes we can generate
successful attack inputs for these
constraint systems and our running time
is between about one hundredth of a
second and about ten minutes
so that's quite a big range even across
a very limited sample set so more on
that later in fact i'll talk about that
now my next sort of step in this process
was finding out faster ways of doing
this so i'll go into the context in a
little bit but in general the idea at
the time was there's two competing
approaches one was the Humvee tool which
i was a co-author on which uses a
reduction from string constraints to bit
vector constraints which then get turned
into a SAT problem and that tool was
relatively fast in practice faster than
little I just showed you in fact
nevertheless we have this feeling that
automata based constraint solving would
be faster and it seemed sort of obvious
in hindsight but at the time it wasn't
really clear immediately why does
bounded strangling yeah but string and
coding right yeah cuz that sense it's
not operable to what you are doing yep
large am so we have a yeah so the hump a
problem is np-complete and ours is not
believed to be but I'll go so like i
said i believe it is pspace complete but
i have not formally evaluated that i am
have focused more on empirical
performance evaluation instead all right
so there's two papers on this one is a
vm chi paper which i wrote with marcus
during my first internship in redmond
and it does basically data structure
selection so we be implemented a bunch
of different techniques from Nona
tammana libraries in the same context
and use them to do some stream
constraint solving problems on one
variable which reduces to a single
otamatone intersection or single
automaton determine ization and
computing inverse so i won't talk about
that paper in a lot of detail for lack
of time but i think the key factor or
the key point there is that we did a
sort of rigorous of an evaluation as is
reasonable in the sense that we fixed
pretty much everything down to you know
the front end parser the language of
implementation and so on at the time
existing work had
the more usual structure of we have a
tool it works better on some other two
on some benchmarks than some other tools
are therefore we win and this is sort of
a V implementation of existing
techniques not a novel technique to see
which data structures work best for
example for representing large character
sets and intersecting them efficiently
what I'll focus on instead is the ASC
2010 paper which essentially takes some
of the results from vm chi and
implements them in a real solver in this
case i sat down and decided to code this
stuff in c++ so it has some engineering
benefits as well as some of the sort of
data structure algorithm selection and
that we gleaned from this other paper so
the approach is just like the solver you
saw before except when whenever we
relied on sort of fullmetal operations
previously will now do stuff as lazily
as possible so if we're finding a single
string an automaton that means that we
find a path to a final state without
looking at other parts of the automaton
if we can avoid it if we do intersection
it's essentially the same thing and it
turns out for multivariate string
constraints and that becomes a little
trickier in some cases we may have to
find paths that are essentially circular
in nature so if I have a constraint a
follows be in some regular language and
then I have a different constraint be
follows a and some other regular
language it's only becomes very tricky
even to sort of figure out where I
should start in the hypothetical search
space which may be quite large so I
won't go into detail about the algorithm
too much it's basically a wall of
pseudocode in one of my papers as well
as my dissertation with some examples
and so on so that interests you please
have a look and I'd be happy to talk
about that offline as well so instead
I'll focus on the evaluation sort of the
hard numbers in this case so we do a
bunch of different experiments and I
will sort of skim over the two middle
ones of these so I'll do a comparison
with humpy which is like I said a
different tool that I was involved with
and I'll do the long strings experiment
to sort of designed to illustrate some
of the limitations of other string
constraint solvers in this case so let's
talk about humpy I'll do a little bit of
background in terms of the humpy
architecture so like I said where I went
the route of using automata for this
stuff which made stuff easy to prove but
not particularly efficient for humpy my
co-authors came up with a separate all
algorithm which is a reduction to bit
vector constraints so for a given
regular language it sort of enumerators
of characters appearing in a particular
position in a single big big vector that
represents the entire constrain system
and like i said this approach worked
well in practice but there's some
question of how much can we improve this
so humpy internally uses the SDP bit
vector constraint solver which
internally uses mini set so there's
several layers here already and that
makes the performance at least somewhat
unpredictable so for different problem
sizes you might get sort of non
continuous function of performance
results and what we wanted to find out
for this particular experiment is how
much could humpy be improved if we were
to have faster bit vector solvers so one
of the main advantages of doing a VN
coding like this instead of relying on
ad-hoc algorithms is that if SDP were to
become twice as fast then Hampi can
directly benefit from that performance
improvement so in this experiment will
assume that will replace hampi with a
zero time oracle that answers bit vector
constraint solvers bit vector
constraints so the task for this
experiment will be to do 100 instances
of regular set difference so I have 10
regular expressions taken from
real-world code they vary in size and so
on and we'll do a set minus B for each
pair so leading to 100 data points for
each length bound that we give humpy and
our metric will be on the proportion
that humpy spends solving constraints
versus encoding them in the first place
so our idea will be that
can eliminate the solving time if bit
vector constraints offers were perfect
but we want to see how much that
improvements gave the center proves what
you ask about it is it empty or not so
the goal the minimal requirement is
finding a single string if it's not
empty and reporting that it is empty if
it is so here are some results for
length bounds 1 through 15 on the
vertical axis with the proportion
running time on the horizontal axis as a
stacked bar so in this case solving is
in gold on the left it takes relatively
little of the time in fact most of the
time is spent in the exponential it
turns out encoding step so these results
in these results I have sort of light
gray as everything else which you might
consider like parsing the constraints
and returning the solution and so on
what's actually happening in each
individual bar is sort of not clear in
this graph right so this is aggregate
based on 100 results per bar so just to
reinforce this a little bit more let's
look at let's say just N equals 15 and
it turns out I have sort of the
correlation or the scatter plot of total
running time on the horizontal axis and
the proportion of encoding and solving
time ignoring everything else on the
vertical axis so in general we see sort
of a vague trend where the longer the
running time is the clearer the
difference between solving and encoding
becomes with encoding almost always
dominating sure so you imagine that how
to be solving in a problem in it be yeah
but here you said that the encoding time
is exponential yes so what is going on
there's two things that are going on one
is what is the size of a red X so for a
linear size red X represents potentially
large number of strings and here we've
defined stuff strictly in terms of the
foot length more generally speaking the
proof of Pompey's NP completeness is
unrelated to its implement
I see so the problem that zombie solves
is np-complete the implementation does
not take the polynomial time reduction
to an amp you complete problem solution
so was that never implemented I mean why
is why and why are they doing that that
is a great question which I volley for
later thanks so in short we find that
even if we were to replace humpies
solving time or with 20 let's say so
eliminate the gold portion of each bar
it is still orders of magnitude slower
than our fastest automata based
implementation all right so let's look
at long strings this is a benchmark
that's sort of designed to show
performance of stream constraint solving
tools relative to the output size so I
have to read exes I want to intersect
them in other words find a single string
that matches both of them at once and
the goal will be to do that
parameterised on n so in this case the
curly brace notation n plus 1 and n
represents the repetition of the most
recent element in the reg ex so in this
case a to see some number of times
exactly so notable is that we'll need
some string that I guess contains the
sub-clause a be somewhere and if you do
this incorrectly you'll spend a long
time looking if you do this correctly
you can do this in sort of linear time
in terms of the output if you're very
lucky let's say so for this experiment
we ran for different tools Saudi pearl
which you've seen before and humpy and
Rex which is based out of MSR and stir
solve which will call on our new
prototype implemented in C++ and so on
so the graph shows n from 0 to 1000 and
the time on the vertical axis is a log
scale we see deep yardley very sort of
slow compared to other tools in this
case that's because of its eager Tom it
on implementation humpy actually fares
reasonably well in fact it is a little
slower than d PR le for let's say the
first n equals 50 up to there after that
maybe Steve really sort of confirming
what we already had as a informal
impression Rex is very horizontal and
around a tenth of a second and then our
implementation is sort of pushing the
boundaries of how precisely we can
measure running time in this case
because as it so happens it makes the
right choice when implementing automaton
intersection so the takeaway from this I
guess is that in addition to being a
little bit slower hampi for example also
exhibits a lot of these so Sun bumps in
running time which on a log graph don't
show up that well if you look at sort of
the non log version of this graph it
sort of fans out so as you go from
bigger to bigger n the optimizations
that SDP and in turn mini SAT make to
solve these constraints sort of certain
matter so for some reason the encoding
time dominates but then the solving time
sort of fluctuates so in some sense this
is undesirable because it makes
performance a little unpredictable pea
Oh in general so if you look at let's
say n equal 750 through 1000 you can
sort of go from let's say sixty hours
worth of solving for DP r le to a couple
of seconds for our tool sort of in the
worst case all right so I've talked
about string constraint solving and I
guess the main punt there was that we
didn't really talk about constraint
generation so I talked a lot about
solving solving some constraints and
this will be good for everyone and so on
and what I haven't talked about is sort
of an end to end tool that implements
this for a particular class of program
say and that's pretty much exactly what
we did with the back project so it is a
complementary approach to what I've
presented so far so this is the work of
two internships and all sort of very
briefly skim over the details so let's
return to our earlier example and web
developer implements a profile page and
let's say that we throw in a sanitizer
at this time so previously we were
vulnerable because we didn't include any
sanitizer
but there's all these libraries that
claim to help mitigate cross-site
scripting attacks so we can just call
HTML encode and this will save us right
and it turns out the answer in this case
yeah might well work so in this case the
single quote in the attackers input
string is actually encoded to ampersand
hash 39 semicolon and now there's no way
actually to escape the source attribute
and include code so this will just be an
image that shows up as well URL does not
exist so we've successfully avoided this
attack as a developer I might ask what
could possibly go wrong in other words
did I just fix all my problems
permanently and the answer is well it
depends on which library you used so
let's say library a it has a function
called HTML encode and it has been
available for C sharp developers for
some number of years and sort of
well-regarded and now I have a library B
which through a being searched I find to
be sort of equally prevalent published
by the same people and also you know
let's say the exact same credentials and
it turns out that in this case if I use
library a I win my single quote gets
escaped and it turns out library be
takes the more formal route and since
the HTML standard says that I shouldn't
be using single quotes for my source
attributes in the first place it does
not escape single quotes because it's
not deemed necessary so now I still have
the same vulnerability in spider of fact
that I call the function called HTML
encode so it turns out libraries amb
correspond exactly to microsoft's anti
XSS library the implementation for HTML
encode and Microsoft's dot net web
utility library so it turns out
Microsoft is not the only entity that
has concerned itself with what the exact
semantics of HTML code should be in fact
if you look at the source code for the
PHP interpreter the portion that does
HTML and code has been updated 151 times
in the last decade so let's say every
three weeks on average in that time span
across you know 200 300 thousand svn
revisions it has grown from 135
of code which I'll sort is sort of
relatively easy to inspect and manually
verify to about a tenfold increase 1693
lines so a little bit more challenging
to look at as a programmer and figure
out what's going on so in the meantime
they've added all sorts of flags for
example to see if this function is
idempotent and so on to make sure that
it doesn't double encode things and all
sorts of other subtle changes in
behavior which if you think about it is
kind of bad so if you have millions of
web apps out there that rely on the
exact behavior of HTML codes for their
security then it is not necessarily a
good thing that HTML and code apparently
changes its semantics in subtle ways
once every three weeks all right so I am
sort of limited in terms of time I'll go
into the background of the back the back
project as well as a very high level
overview of our approach if you're
interested in our both external and
internal valuation I'm going to refer
you to the papers and just a bit all
right so let's talk about background and
the key idea is that we want to create
essentially a regular expression
language for string transformations so
we're a classical reg ex is a
well-defined formal entity that has sort
of properties like I can convert it to
an automaton which I can then use to
match strings for Beck we want
essentially the same thing about for
transducer so in a Tom and Tom that
takes inputs but also produces a set of
outputs potentially empty potentially
many so the key idea is to create a
domain-specific language that
programmers can actually use to write
sanitizers and then convert it to this
formal model so we can do interesting
analyses at a higher level let's say
there's this gap between code and the
formal model we would like to use this
is a problem that comes up a lot rather
than punting and doing a separation
between constraint solving and
constrained generation like we did
before will actually try to fully close
this gap for a limited class of programs
so we'll create a domain-specific
language in this case to make the code
anibal to translation into this formal
model and on the right hand side I have
replaced model with finite state
transducers the second step will be to
make those sufficiently expressive so
that they easily capture the types of
transducers we would want to build based
on our own domain specific language and
these two approaches correspond pretty
much 12 12 hour using security paper
which presents the language and some
practical applications in terms of
modeling real worlds an atty zehrs and
the pope'll paper which goes into the
detail of symbolic finite state
transducers which are an abstraction
that's what we show to be sort of
strictly more expressive than classical
transducers and yet somehow still very
analyzable and very useful so let's talk
a little bit about the approach and I
guess I'll start with them a short Beck
program so this is a program I won't go
into the details but it escapes quotes
so double quotes and single quotes get a
slash in front of them unless they are
already escaped so the key sort of parts
of this program are an iteration over
some input string s and a boolean
variable which will update across
iterations which captures whether the
last thing we've seen is a slash or not
so to avoid double escaping so I won't
go into detail about what this code
looks like as a transducer and so on it
turns out you can try that on rice for
funcom so there's an online demo which
includes examples very similar to this
some sort of allows you to visualize the
transducer and perform analyses and so
on but one thing that we might want to
prove about this transducer is that we
did it correctly so if I apply this to
the same string twice will it end up
double escaping quotes or will it do the
right thing and sort of escape them
exactly once all right so slight
transition to the formal definition of
symbolic finite state transducers I'll
go over this sort of at a very high
level the basic definition is something
you might have seen before it's a four
tuple with States a start state a set of
final States and a transition relation
so all of the state related parts are
pretty much as you would expect the main
differences in the transition relation
so each transition is of the form Q to R
where the edge has two annotations in
this case a formula fee and let's say a
bold F which is an output so a sequence
of output formula in this case so the
phpbb it goes from the input alphabet to
the boolean's this decides if the edge
is reversible on a given character and
the f portion of the show is an output
sequence so I can provide some number of
functions that take the input so
basically as a lambda and turn it into
an output and the star here means that
we can have a sequence of those and I
guess the only really notable thing
about this formal definition is that the
star in this case is outside of the sort
of function scope so normally i would
say goes to a list of characters in this
case and we explicitly require a
sequence of functions for some number
that's bounded so this helps make the
algorithms even more desirable than they
already are what this definition buys us
is a clean separation between state
space related operations on the one end
and background theory related operations
on the other hand so this work is an
extension of Marcus's work on symbolic
finite state finite state machines
taking that into the symbolic finite
state transducer space where we have
outputs and what's nice about this is
that it works for any decidable
background theory so anything that can
do undecided satisfiability and witness
generation in z3 let's say so in
practice we use the theory of bit
vectors to represent characters and we
can do that quite efficiently for large
alphabets I including utf-16 and so on
and so for further details I would refer
you to the pope'll paper at least in
regards to what we can do with symbolic
finite state transducers the high-level
ideas that we define an informal algebra
of operations that are closed on
symbolic finite automata that represent
values and
symbolic finite-state transducers that
represent transformations and based on
that algebra you can implement a lot of
different interesting analyses so it can
do relational sub sumption and if you
use that in to weigh in two directions
you get relational equivalence and you
can use that to test for let's say item
putin's can i apply to sanitizer twice
and will a double encode stuff or not
and commutativity and soul alright so
that concludes my my portion of the show
on deck like i mentioned it is available
as a demo on rice for fun it's i think a
great online demonstration so in
conclusion i've presented two
complementary approaches to dealing with
code that manipulates strings from a
security perspective or more generally
from a validation or test case
generation perspective string constraint
solving there's a number of different
concretely available tools that are open
source and so on and they all solve a
sort of similar set of constraints and
in sort of contrast to that the back
project is sort of a singleton at the
moment and models a subset of importance
during manipulating functions more
directly so in terms of that same i work
i would say that the bulk of it has
focused on swing constraint solving a
relatively little portion of it so far
has focused on back to summer so far and
i would love to sort of continue working
on this so in terms of future work i
have a couple of stray bullets which I
will cover very briefly basically we're
looking into the closer integration of
string constraint solving into SMT
solvers so there's a draft for a
standard that would add this to the SMT
lib sort of set of theories that we know
how to deal with and potentially
generate benchmarks that would allow
future tools to participate in sm decomp
while solving string constraints
specifically and then other potential
uses for beck for example i think would
be a great use for an educational tool
so there are some of this available at
MSR already but basically you know let's
say I learned to program in qbasic in
1992 or some sort qbasic had as its sort
of main points in favor a very good
documentation very good debugger and so
on what if we can extend something along
those lines on to a more domain specific
tool like using back to teach basic
string manipulation or this manipulation
in general and do automatic checking of
student submitted code by using the fact
that we can check for program
equivalence so if there is a gold
standard correct version of a function
we can always tell you exactly why
you're wrong if you're wrong the other
point I briefly wanted to mention is
back for other domains so I've done some
work on wireless sensor networks using
different models to I guess model a
distributed system in this case it is a
common approach to analyze distributed
systems using automata I think it'd be
very interesting to see if there's a
front-end language extension tyvek that
would be amenable to use as an easy tool
to write wireless sensor network
applications that run on distributed
heterogeneous hardware and sort of
guarantee properties of our programs of
communicating looks so that's it for my
talk and I'll be very happy to take any
questions
well maybe a plan for questions I can
probably half an hour please
so actually I didn't quite understand
what what what do you do with these
transducers mm-hmm how are they related
to the security of web pages sure and so
the basic notion is that a function like
HTML and code is written in a certain
style so we've looked at a bunch of
different implementations for the usenix
paper and they all follow this sort of
you know loop over string keeps on
boolean state and look at a sliding
window essentially of the input string
converting characters into other
characters as needed it turns out the
translation from let's say D Java owasp
implementation of HTML code to back in
this case as the front-end input
language to our tool is fairly direct so
the purpose of the language is to mimic
the style in which programmers already
right low-level string manipulating code
such as HTML encode the transducer is
the formal model which we can do which
we can use to analyze stuff like
equivalence so they are single valued
symbolic finite state transducers we
show that equivalence checking of those
transducers is decidable which means
that if you give me two different
versions of HTML code both implemented
in our language then we convert them
both to a transducer and then check
whether they are equivalent if they are
equivalent we say yes if they're not
equivalent we say no here is a
differentiating input that looks
different if you run it through on the
two transducers so that's the sort of
core of the deck project basically a
language that describes transducer in
the same way a classic reg ex will
describe a set of strings
first what are your thoughts about the
in in c programming you might might do a
lot of this by hand just
for you if you put assistant
sanitization shorter so if you if you
extended the C language or any other
programming language with these features
means that isn't is it broken for
various illogically think about the
meaning essentially putting back into
into language yeah is it is it there
yeah i mean would that be using for
languages or is it just could be a
handful of people yeah i think what's
that's a great question and so the I
guess the basics is what about like ad
hoc sanitization I can programmers
actually use this in real life and the
answer is yes there's an ongoing effort
to turn back back into real-world code
so in the same way you might use lexing
yak to write a specification which then
gets turned into relatively up to
looking c code we have a similar project
underway or at least Marcus has been
working on this I believe that's
basically the translation from back into
various mainstream languages so then it
actually becomes quite feasible if you
recognize that your current task is one
of sanitizing a string using it that's a
single pass to write it in back instead
so you can do some analysis before
generating the code i will then link
into your application um but jeez I was
just wearing today what's the main
source of strength sun's rays ready to
go sure I'm so the question is why I do
swing constrains come from uh-huh
literally yeah sure from from a slide I
think the answer is so if you look at
modern web development and there's a lot
of templating languages out there I
would say that if you were to analyze
them in the same way we analyzed PHP
code which represents a slightly older
class of web application in my opinion
then you might look at stuff like so
these templating frameworks they might
apply some auto sanitization in order to
reason about their correctness and you
would need to symbolically execute
through that library code or develop
some model of that library code so in
that way the string constraint solving
approach is
is almost exactly sort of complementary
to the back project we might use Beck to
essentially summarize different parts of
library code we might use string
constraint solving for executing or
evaluating single paths through library
boundaries and does that answer your
question
more questions
I have a question about what early
experiments she did mm-hmm they
experiment about the long strings yeah
so the example reg ex is that you gave
or no please classic examples of races
that if you try to criminal they blow up
yeah stepping up mm-hmm recognizing some
character curve like the mpr people yeah
but the experiment was on intersection
and of difference yeah so none of these
tools try to determine eyes this is true
yeah that would make all of the tools
slower especially the automata based
ones yeah beauty and it might actually
like yeah let's we're doing this this
probable yeah that's a good suggestion
the difference in scale right so I'm
doing difference instead of intersection
it's generally slower I wonder if that's
also the case for hampi in this case so
maybe home p comes out looking a little
better it relieves that was the rex use
desk in this case using accidents it was
the more recent implementation that we
had a license for so the rex
implementation that used concrete ranges
i believe so it's not actually the
fastest rex implementation and BDDs
would be faster but BDDs tend to consume
a lot of memory and run out in some
cases at least that particular version
of the code did so we went with the
version that returned results for all
inputs so i guess i should qualify that
graph by saying that the rex results
could have been a little faster if we
had used BDDs but then there would be
sort of fewer data points to look at tom
so one of the things that makes approach
viable in general is that
these general-purpose languages you're
analyzing have a demain specific sub
language that have closely matches sort
of your domain right yeah right so if
you wanted to extend this for example
two trees in the way people construct
trees and match over trees difference
might not be as regular shall we say as
it gets encoded in the general-purpose
language sure what would you if you had
to sort of try to generalize your
approach to to treat manipulate yeah so
the question is what about data
structures with multiple follows instead
of just one so trees instead of lists
and I think you know that's a very
interesting area of research so I think
Marcus is looking into trees in this
upcoming summer in particular two
transducers and this goes back to your
earlier question about Mona as well so
Mona has specialized support for this
this is something that I've been meaning
to look into sort of on and off over
time I think in general it'd be very
useful but like you said we do benefit a
lot from the fact that high-level
library operations over strings look a
lot like let's say string constraint
solving combined with some back so I'm
not totally sure that many tree
manipulations take the same form
so any more questions
Peter once more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>