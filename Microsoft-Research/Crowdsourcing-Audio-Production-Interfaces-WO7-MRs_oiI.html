<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Crowdsourcing Audio Production Interfaces | Coder Coacher - Coaching Coders</title><meta content="Crowdsourcing Audio Production Interfaces - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Crowdsourcing Audio Production Interfaces</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WO7-MRs_oiI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon everyone those who are in
the room in those who are watching us
remotely it's my pleasure to hear
professor bumper here he is the head of
not Western University interactive audio
lab and his talk will be about
crowdsourcing crowdsourcing audio
production interfaces without further
ado bran you have the floor okay thank
you so here we are crowdsourcing audio
production interfaces and let me just
tell you about where I work I work in
the interactive audio lab and the
interactive audio lab is at Northwestern
University and northwestern university
is in chicago illinois and i felt
required to put in the next slide given
that i am in the Pacific Northwest for
some of you here at Microsoft may be
wondering how is it that northwestern
university is in Chicago instead of in
Seattle and so for historical reasons
northwestern is called northwestern this
specifically back in the 1790s there was
a territory called the Northwest
Territory long before Seattle was even
this area was even part of the United
States and northwestern university was
named after the Northwest Territory so
we had the name before you guys did and
we're not letting it go without a fight
but you're not here to get a history
lesson on American geography you're here
to find out about what I'm doing and
what goes on in my lab so I work in
computer audition and computer audition
in particular is something that combines
a lot of stuff and some of the things
that it combines are crowdsourcing
linguistics music cognition or at least
the way I do it and we also bring in
machine learning information retrieval
and signal processing and we take all of
these things together and and apply them
to problems in computer audition and
you'll see a lot of that today
so I want to start out with a question
which is how do we humans communicate
ideas about sound and I'm going to
propose to you that a composer named
Mark Applebaum really kind of nailed it
in this piece that he wrote called pre
composition and this is a sort of tape
piece about his process of making a
piece of a sound art so let me play this
for you let's get back to this where we
were okay so we got this there's this
like nasal pulsing thing does anyone
remember that and then suddenly there's
going to be like this kind of wind sound
like
and then I don't know what do you think
should happen next then um it could get
like distorted and staticky like oh yeah
oh yeah and then we could like send it
around like okay so what was that that
was actually a pretty apt kind of
description ER or snippet of the kinds
of conversations now here he had the
conversation with himself essentially
but the kind of conversations that
happen between artists and producers
when they are making music and some of
the things I want to point out here are
the approaches that they were using to
communicate or he was using to
communicate in his own head one is a use
of natural language terms such as
distorted or staticky another is
examples in this case made with his
mouth and finally evaluative feedback
like oh yeah I like that at no point did
you hear people talk about say poles and
zeros of filters about constant Q about
decibels at no point did anybody use
those terms and that is typical when
people are discussing their artistic
creative process when dealing with sound
now i'm going to give you an example
task which is a real task so here we go
i have a recording made in the 21st
century it sounds like this
now this is specifically in an older
style of American music and I would like
it to sound tinny like it's coming out
of an old style radio something more
like this as opposed to the original so
how do I do this now of course your
first answer might be why don't you just
play it out of a 1940's radio and
rerecord it and that is an excellent
solution but not necessarily one that I
can implement if it is say midnight and
I have inspiration in my home and I
don't happen to own a 1940's radio but
what I do have is Pro Tools the industry
standard tool for mixing mastering
editing your software or editing your
audio I should say so great let's let's
take a look at a Pro Tools session what
you see on the screen is a typical Pro
Tools session and up on the screen right
now is the tool that I would use to make
it sound tinny and my first question is
is it immediately obvious to you which
tool that is have a look think quietly
to yourself which one you would pick ok
I'm going to make it easier on you
that's the tool now I'm going to blow it
up for you this is a parametric
equalizer and a parametric equalizer is
not necessarily something labeled with
words like tinny or staticky or
distorted or warm it's got a bunch of
things with that that help you control
gain Center frequencies maybe with the
filters and then there's something over
here that if you're not a someone
steeped in audio you may not know what
this display is all about this is
actually frequency and this is boost and
cut
and if you think about this as a search
problem in an n-dimensional space you
have n knobs somewhere in here is a
region that we would call tinny all you
have to do is turn the knobs until you
discover the region look at how many
knobs there are and ask yourself if
there were even ten knobs with pen
settings each do the numbers that's ten
to the ten combinations you would need
to try but there's more than ten knobs
here now this interface bears some
thought why is it like this and the
answer is actually a cultural one before
we had software tools to do this we had
hardware tools and as you can see this
is really an emulations of a hardware
tool with this is kind of a new feature
but in general they are trying to stick
to an interface that would have made a
1970s music producer happy someone used
to the knobs and switches that were on
their analog equipment in 1972 and when
the transition to digital happened these
plugins these audio plugins were
expensive they might be five hundred or
a thousand dollars so their only market
was going to be people who are already
professional engineers who already
understood this interface so when the
time came they duplicated the interface
so that there was seamless transition
from their point of view and that made
sense for that generation of people but
it is not 1979 it is not even 2009 it is
2016 and why are we continuing to
emulate this hardware the hardware was
built this way because of the
constraints that they were dealing with
when building physical hardware but we
no longer have these constraints so what
is the first thing that people did to
make this easier to use well presets so
there'd be a drop-down menu that you
could click on and see presets that are
going to solve your problem so okay
great here this isn't an equalizer now
this is a reverberation tool something
that makes echoes
this I i just went this morning and i
opened up a standard reverberate ER and
clicked on its drop-down list of presets
and some of these words make sense to me
like maybe under the bridge i might have
some vague idea of what an echo of under
the bridge might be like but what about
memory space or a corner verb a shin and
what is a bitter hallway presets have
historically been selected by the tool
builders with no actual thought about
whether or not anyone other than the
original maker of the tool has any idea
what these words mean and as the number
this is something with a small number of
presets you can find tools that'll have
50 or 100 presets with descriptive names
like space laser frisbee and when you
have that you are back to random search
through a space this time a random
search through presets it's not just eq
it's not just reverberate errs here's a
traditional here's a simplified a
traditional synthesizer controller again
lots of knobs now they've given up the
the whole trying to emulate the exact
look but the same underlying thought I'm
going to have knobs that can correspond
to features and i'm going to set myself
in the feature space we tried some user
experiments where we would generate a
synthesizer sound using this interface
then we would have the interface to a
novice a musician but who wasn't one
experience with synthesizers and say we
promise you that this sound was
generated with this synthesizer and this
interface all you have to do is set the
knobs until you can get that sound again
here's a typical response i give up it's
as if you put me in the control room of
an airplane and i can't take off this
has effect this choice of interfaces has
effects here's a long time musician on
trying to learn to produce music I've
been playing guitar for 30 years i
bought the interface software I'm 48
work as a carpenter and I'm just too
tired all the time to learn this stuff
there is so
much to learn at the same time I don't
know the terminology I have given up for
now sad because I have lots of ideas so
for all of you who are watching this
right now and think to yourself I have
no problem with these interfaces I don't
see what the big deal is I bit the
bullet and learn them for every one of
you there are many other people who
tried became overwhelmed and just gave
up what we're going to work on to do
what I'm going to talk about today is
how we've been using crowdsourcing and
and a new sort of way of thinking about
the issue to provide an alternative set
of interfaces that the kind of person
who doesn't naturally think in these
terms of these existing tools can use I
am NOT trying to take away your existing
knobs I'm just trying to provide an
alternative you know put another way
let's say the world was all pianos and i
introduced the trombone the trombone can
do things that pianos can't do pianos
can do things trombones can't do I'm not
saying the piano has to go away I'm just
providing you an alternate kind of
interface a different way of dealing
with the issue specifically we want to
build interfaces that support natural
ways of communicating ideas and we've
defined natural as the way people talk
to each other about it natural language
terms examples evaluation things I gave
you in that other example the mark
Applebaum piece okay so here's the first
interface we called it IQ now I see at
least two people in three many people in
the audience with glasses and if you
think about the interface for selecting
the right tool the right settings for
your glasses you do not necessarily the
traditional approach has not been you
walk into an eyeglasses store and say I
would like something minus 1.75
spherical diopters in my right eye minus
2.5 on my left eye with an axis of
astigmatism of 75 degrees off the the
verdict
many of you probably don't even know
your exact prescription what happens is
you go to the eye doctor and the eye
doctor says okay what do you think of
this is it better A or B you are doing
an evaluative paradigm and you quickly
get to the eyeglasses setting that you
want we decided to do this with
equalization and notice you don't have
to know terms like diopter to get your
glasses prescription you just have to be
able to say whether you like one thing
more than another thing so that's the
idea so how do we make it go well simply
put we take a sound and we know that
you're going to want to change it we're
you're going to want to re eq it you're
going to want to raise some frequencies
or lower some frequencies to make a
sound more along the lines of what you
want ok so what we do is we play you
different EQ curves these are different
EQ curves here's boost and cut here's
high frequency to low frequency we take
the sound we run it through an EQ curve
you're thinking of the effect you want
and then you rate it how much do I like
this is this pretty tinny or not and so
let me show you this in action
I use dark and i'm going to teach it the
word dark on a piano sound
uh maybe i'll use to me just cuz tinny
was the example i've been using as a
running example nevermind anyway st.
piano sound tinny so this is the
original sound and I sincerely hope that
this works i'm i'm downloading this is
all a web demo this one's running in
flash the next one will be nodejs hears
the sound do I think it's tinny I don't
I don't think that one's didn't either
it's closer to what I think of his teeny
so what i'm doing here is i'm as
something is closer to my idea of tinny
on going over here and pulling it to the
right which is where it says very tinny
and if it's further let's see what I get
I got an EQ curve well that's more of a
bright now what have I done this is the
EQ curve I learned and just for fun
we've got lots and lots of eq curbs
other people have taught it and over
here I've got the five nearest words and
the five farthest words and so I can
take a look and say somebody taught it
aggravating or sharp or bright
so I'm in a certain space of words
and so this first with so many let me
say what's happening here this first
interface we don't need to see that this
first interface was one where we wanted
to see whether we could use evaluative
interaction and now I what I did was I
just did it with eight if I do it with
25 or 30 i get a very nice capturing of
my concept but then how did i do
anything at all with eight well the
issue here why and why am i just
choosing to do eight well when i went
ahead to do this even though we've done
some studies that show that when you
hand somebody that first interface with
all the knobs that they get lost and
they never even finish but when we
handed this something with evaluative
interfaces where they had to answer
25-30 questions before we gave them
something they became annoyed that we
asked them 30 questions even though we
could show that if we just handed them
the original interface they'd never
actually get to the end so so the
problem was now we were solving the
problem but we were still annoying
people how could we annoy people less
you ask too many questions well one way
is perhaps to use prior user data here
are three EQ settings I have a much
larger set that we would we would
normally play to someone if we didn't
have prior user data here are three
people sue jim and bob and each of them
had some concept in their head that they
were going for and if we play when we
treat train the system if we play a
manipulated audio example with a certain
EQ curve and say well how warm was that
sue she gives an answer how dark was
that Jim hmm Jim liked it one is the
strongest positive thing you can say and
maybe down here this other example Bob
who was going for something called fat
really didn't like it now we have a
space where these eq curves could be
looked at in different ways they could
be looked at in the space of how similar
the curves themselves are
we could also look at them in the space
of how similar different user ratings of
the curves are for certain curves may
look a bit different but may be
effectively people think of them as
roughly equivalent so if we take these
eq curves and let's say I'm going to
take the responses Sue's evaluation of
curve two and curve three where her goal
was to make it sound warm I could take
her concept now and put it in a space of
evaluation so the concept is Sue's warm
and in the space of what she rated those
two examples as it's here and these
other ones could be placed in this space
as well so I've just done a
transformation from which space i'm
looking at things in before i had eq
curves now i have concepts user concepts
and now at the EQ curve all my audio
data has disappeared and now I just
talked about the ratings of the EQ
curves these underlying things which
could be anything now if I have some new
concept I'm teaching the system I can
say well in this space of user ratings
prior user ratings Who am I closest to
and instead of having you evaluate lots
and lots of eq curves to try and figure
out what exactly I should do to
manipulate all the different frequencies
I ask you to evaluate a few eq curves
and then compare your answers to prior
users answers on those eq curbs prior
users who did answer all the questions
who did evaluate all of the different EQ
settings and then I can say well rather
than just hand you something arbitrary
or hand you something that required lots
of answers you answer a few questions we
figure out who your most liked and we
use a weighted average weighted by your
distance to this new concept in this
space and instead of 25 questions maybe
I could take it down to a much smaller
and in fact in this case I showed you an
example where we did it with eight
questions now you might want to ask well
which a questions do I ask if i have a
bunch of examples that people had to
rate in the past before they figured out
what they were doing maybe what I should
do is figure out what the most
informative question is and in this case
we decided that the most informative
question or the most informative eq
curved ask would be the one that caused
the largest disagreement amongst prior
Raiders and if you think about this
let's say that I was trying to let's
pick something dumb I was trying to
determine your political party and one
question that I have on the on the
questionnaire is do I like ice cream and
the other one is do I like Obamacare now
probably most people will say they like
ice cream and so it doesn't have a lot
of discriminative power but maybe do I
like Obamacare would have a lot of
discriminative power and I would want to
use that question first in this case
this example turned out to have the
largest variance in user ratings and so
if I was going to have to pick one
dimension along which to rate you or two
to judge you I think I should ask this
question first and amongst these three i
would say i'll ask this one then this
one then that one and the result is
socially q which is the thing that I
just demoed for you where instead of
answering 25 30 questions i answer about
eight and these questions are really
designed to locate me in a space of
prior users who have answered these
questions before and then say okay we
think you're most like a weighted
combination of these individuals let's
give you something like that and that
was pretty good
then we and now you might be asking how
did we get this prior user data and the
answer is as it is all for all
crowdsourcing Amazon Mechanical Turk if
it were about five years ago I'd be
telling you a story about how we were
going to game a file all of this problem
but as many of you who work in
crowdsourcing know gamification and it
turns out those of us who are good
researchers aren't necessarily good game
designers and therefore when you make a
really great game the problem of making
a really great game that people will
come back to and do over and over again
is really different from the problem of
doing good research and machine learning
or interface design well closer to
interface design and if you think you
can get the answer by paying a thousand
dollars to get in on Mechanical Turk you
should do that rather than spend a year
designing a game to get as much data as
you would have by paying a thousand
dollars from mechanical term which is
what we did so we got that prior set of
people by going on the mechanical turk
and having them teach us a word in which
case we were paid between a dollar and a
dollar fifty depending on how reliable
they were to teach us a word and words
were selected by the contributors so we
didn't tell them what word to choose we
said we just want to learn a word that's
a adjective about sound you pick the
word and then show us what it means and
in this case we have them do 40 rated
examples / word and we had some
inclusion criteria words existed in an
English or Spanish dictionary one thing
I should mention is we did this both in
English and Spanish and that they self
reports on listening on good speakers in
a quiet room and that they took longer
than one minute to complete the task of
rating 40 different examples in light of
a particular goal if you could rate 40
examples in less than a minute we
figured you were probably just after the
money and finally we wanted people with
consistency greater than 1 standard
deviation below the mean consistency
what do I mean by consistency we
actually had 15 repeated examples in our
set of 40
so that means that if you heard an
example and said yeah that's a one and
then the next time you heard the example
and said that's a negative one you're
not being consistent so we wanted to
make sure that independent of the
context in which it was heard you would
still say this EQ setting was warm or
whatever word you were going for after
the inclusion criterion we ended up with
932 English words 676 Spanish words of
which 388 and 344 were unique and here's
the thing to mention the learning curve
so what is this learning curve about
here's the number of user ratings and
what do I mean by that I have a sound
that I would like to have ich you'd
somehow now the machine hands me an EQ
curve and I rate it that's one user
rating based on this it's going to try
and figure out what my rating of the
next curve will be ok I rate the next
one sees how far off it is now I'm in a
to space I've got two ratings and we can
try and build a model of me based on
prior users on who answered questions
like I did on two examples we build a
new model we try and predict your answer
to question three and that is machine to
user correlation the better the machine
is at predicting your answer the more
correlated the user and the Machine are
by the time we get to about 25 we can
predict your answer and we're kind of
done this is our training curve without
prior data this is our training curve in
the end with prior data and so we moved
we moved that learning curve up which
means that prior users are in fact
helpful and a thing I don't actually
have on this slide is of course if we if
we constrain it once we have enough data
if we constrain it to prior users who
used a word like your word that is if I
was training tinnie we only use prior
users who were teeny or a synonym drawn
from wordnet like bright or sharp or
something like that well those aren't
synonyms from wordnet but we know
they're synonyms um you can do even
better I mentioned English and Spanish
and one of the things that I think is
interesting about this work is we're
actually doing a translation problem and
we started out thinking of it as a
translation problem between the control
parameter space our knobs our feature
space which is sort of a perceptual
thing and perhaps a word space I want to
I'm leery about saying a semantic space
because a lot of times engineers throw
around the word semantics without
knowing what it means which is great
because semantics is about meeting but
we were doing this translation we
started to think about well what could
this translation be useful for it
initially started out as a translation
between this complicated base of the
tool and the user it turned out this
translation was a great one between
users and audio engineers and by user
and I now mean acoustic musicians and
audio engineers because we we had this
we were actually reviewed and sound on
sound magazine by a famous audio
engineer for the initial version of this
tool who said that this solved a problem
that audio engineers have been having
since time immemorial which is you got
to use you get a customer a client who
comes in and says make my sound buttery
and the and the audio engineer says
there's no buttery knob I have no idea
what you're talking about you go through
this teaching process and we can
discover whether buttery is in fact
related to eq at all or some other tool
and if it is related to eq what the EQ
curve would be then we got to thinking
about translation in another sense which
is what's the right word call mostly say
a warm sound how do you say a warm sound
in Spanish
would it be a sound that's gallina is is
that the right term and it occurred to
us we have this really interesting
possibility here rather than do the kind
of machine translation that we're all
used to which is you have paired texts
and you find statistical correlations
we're going to do a mapping between the
perceptual space and the word space as
follows you have people trained here's
an example rate how warm it is they give
a rating now you got someone else in
another language here's an example rate
how Kylie though it is they give a
rating examples where lots of people
give high ratings to this word in
English and that word in Spanish that
might be a good translation for that
descriptive term because warm is out
start yeah it's not temperature wise for
what about in Spanish for the same sound
they use a different abstract exactly
that's a problem more than yeah so so
what we do is this we don't actually
translate from the word to the word we
have words that get associated eq curves
so I have a word in English and a word
in Spanish and each of them has an EQ
curve that has been taught to it by the
users then if I take an english word
chunky which has an EQ curve associated
with it I go looking for the EQ curve
from the Spanish users that looks most
similar to the EQ curve / chunky and I
call that a likely translation and of
course these translations are different
from paired texts and at least for audio
maybe better descriptions so putting
this in more concrete terms here we go
what this is now is an EQ curve but it's
sort of also a probability distribution
and don't worry about the units of this
basically this is boostin that's cut
here 0 and the brighter it is statistic
speaking for this set of people the more
likely it is that this amount of boost
or cut was what people would say it
should have to teach it the word tinny
that's a spectral curve for tinny here's
a spectral curve for light again this
has learned from six people in this case
but you can kind of see this sort of and
one of the things we discovered is you
might have a word that you wouldn't
expect to be for example the antonym the
opposite of this here's hard there's
light there's hard whoop here's warm
here's profundo it's not golly though
it's more like deep
so now we we've got this interesting
thing you can translate between
non-experts and experts using a tool
like this what does a buttery sound
what's a whatever sound you can also
translate between different linguistic
groups like this and this obviously
isn't limited to audio per se I mean I'm
an audio guy so that's what I did we
could do this the same game could be
played with color words for example or
maybe flavors said all that we also we
didn't once we did this and we thought
well it's not just equalization of
course again I'm a sound guy so I just
said color words and flavors but I'm not
interested in that I'm interested in
sound so I went ahead and did it again
for reverberation and we went out and
collected a lot of examples of
reverberation and so then we had this
big data set prior data set of
reverberation effects and the words
associated with them equalization
effects and the words associated with
them and we had this philosophy which
was the less teaching a human has to do
the better now we've taken it from our
interfaces from the starting point which
was those hard to understand knobs hard
to understand by someone who isn't an
expert who already understands the
signal processing or whatever into
here's a teaching paradigm essentially I
evaluate it asks me questions what about
this that's pretty good what about that
better but that interaction you know we
took it from maybe 25 30 examples down
to about eight that was still pretty
good but there's a reason that we use
language as humans it's a shortcut right
when I say the word dog you all picture
something in your heads it's probably
you know this tall it's got four legs
this whole body is covered with hair
it's got a wet nose imagine if I had to
teach you what a dog was every time we
interacted that would be very slow so
since we've been collecting
this vocabulary why don't we use this
vocabulary as a starting point and then
only fine-tuned if the word that we use
isn't quite working out which gets us to
the interface for audio lies what you
have here is an interface for I'm not
sure if you can see this on your tiny
video screens with for those of you who
are watching at home but eq + reverb
equalization and reverb we've learned
vocabularies for both of them these but
the slides oh okay great so now the eq +
reverb we've got vocabularies for both
and we even have a reverberation setting
and an eq setting that somehow we have
some confidence in for each one now we
can take these words and place them in a
space in the case of reverberation
there's five control parameters that
we're using here so you can imagine each
of these words is well n 6 is again but
we'll save five-dimensional you can
imagine this these each of these words
being a reverb setting in a five
dimensional space and we're projecting
this down into 2d to provide you a word
map so what this means is that words
that are closer to each other probably
have reverb effects that are more
similar sounding in words that are
further from each other more different
sounding now your first thought might be
wait aren't I just getting back to that
preset list I thought you said that
presets were a problem and they were a
problem there were a problem when the
presets used a vocabulary that was
arbitrary defined by the tool builder
with no real thought about whether or
not anyone else would understand what
the words are the presets lists were a
problem when they were organized in some
way that's not intuitive like wood why
were they in the order they were in it's
hard to know how to search that list
here close things sound like each other
bar things sound different from each
other
so let us say and oh and the other thing
is these things are social construct in
some sense let me tell you what I mean
when I was describing this work I was
talking to the bass player who had gone
on tour with liz phair who is a Chicago
she came up out of Chicago in in the 90s
and she's a relatively well-known indie
rock artist and she was talking to her
engineer he remembered a particular
conversation where she said I want you
to make my guitar sound underwater this
was an electric guitar now if you think
about it for just an instant if I walked
over to Lake Michigan and took my
electric guitar and dropped it in the
water the sound that would happen would
be a splash maybe if it was sorted out
maybe if it was you know turned on there
would be a shorting out of something
here's what underwater sounds like
here's a guitar first without underwater
here's underwater
now I'm not sure if you can see this but
down here it tells you for each word
underwater in this case was learned from
95 contributors down here for each word
it tells you how many people it learned
an example from yes so a certain on a
certain reverberation setting was
labeled as underwater by 95 people and
therefore it made its way to be one of
the bigger words on our map or you could
have coming back to this maybe that's
too much and I want something that's
like risk or clean maybe I can't find
the word on the map so I type it into
this search box
got cave
this interface if you compare exploring
this interface to exploring that first
interface with all the knobs on it and
just again don't think like an engineer
for a moment think like you're an
acoustic musician you'd like to make
some it's it's midnight you're at home
you're playing stuff into a garage band
or whatever it is and now you'd like to
make it sound like I'm playing my guitar
and then I plunge underwater it would be
nice to know what reverb effect would
not just be underwater to me but give an
impression a general impression to an
average person of underwater this kind
of gives you that gets you in the
ballpark but let's say that you don't
exactly like what you heard maybe
underwater is overdone ooh let's get
that going you can always go back to the
traditional interface
we provide you the controls if you want
them you can search around in this space
you can go back to the controls in the
old parameter base space or if you want
to go back to that evaluative thing
maybe there's a word like a fit cyl
plink which is not going to be on my map
it tells me to try teaching it and then
we drop back to the teaching interface
that you saw earlier so the idea here
now is that we started from interfaces
coming way back wherever it is that
looked like this right the interface
that comes ships with this standard a
equalizer and parametric equalizer and
we've turned it into an interface that
looks more like this here's the EQ tab
where you think of the word that would
describe what you're after and then you
click on it and if you want to see and
you want to get an idea for generally
what would happen if I went from here to
over here you very easily just drag
across if you want to get back to the
old interface it's down there but you
don't have to use it and if you like
just teaching it there it is so now we
we've managed to cover two of our
approaches so one of the approach that I
mentioned was an evaluative interface
and we got that the next one was natural
language using words that regular people
know we went out and asked lots of
regular people on Mechanical Turk what
word does this sound make you think of
and we got that answer and we created an
interface like this but there was one
other kind of interface that I'd
mentioned we'd use or we wanted to
explore which was the example based
sometimes I you know I say it's wind and
you go wind like this and we use our
example it's like no and then I try and
go through the learning and now it's
still no could you give me an example
yourself so mark Cartwright a doctoral
student in in my group became very
interested in that and so he came up
with something called synthesist and let
me tell you what the problem was you
might remember back earlier I said that
we gave an example problem to some
novices which was here's the sound that
a music synthesizer made here's the knob
based interface that someone used to
program the synthesizer all we want from
you is to get that sound back and this
is a typical problem that you have with
synthesizers if you're a musician maybe
you this is something a friend of mine
had to do a lot she worked in a wedding
band as a keyboard player and so you've
got your keyboard and the bride has
asked you to do the latest Katy Perry
hit so you need to make it sound like
the Katy Perry so you listen to the Katy
Perry hear that keyboard sound and then
you go looking for it on your interface
ah if only there would be something that
would help me with this search so mark
Cartwright went ahead and made a thing
and now this one I don't have a live
demo for because it actually you have to
hook it in with his in his synthesizer
and stuff but mark was kind enough to
make me a movie to show you so let me
show you this movie synthesist is an
audio production tool that allows users
to interact with synthesizers in a more
natural way instead of navigating the
synthesis space using knobs and sliders
that control difficult to understand
synthesis parameters users can search
using examples like existing recordings
/ vocal imitations
they then listen and rate the machines
suggestions to give feedback during the
interactive search process let's see an
example using a synthesizer database
with 10,000 randomly generated sounds
let me just pause it there so what
happened that first sound was what he
has in mind the second sound was the
best he could do imitating with his
mouth you so we record that imitation
not what he has in mind but the
invitation because you can't actually
get what he has in mind give it to the
system
two
let me pause it for a moment and tell
you what's happening here the system has
produced a bunch of examples in each of
these examples along some dimension it's
measuring is a highly similar example
now what the user is doing is clicking
on them and if you click on them and
drag it towards the center towards the
target you're telling it that this
example is in some way similar if you
right-click to delete you're telling it
it's completely off so dragging it
towards the center means yeah this is
something good away means something not
so good and click to delete means
terrible synthesis look is an audio
production to let inventory stars out to
interact is there a way for me to
restart it with out nope and I have to
do this see an example using a
synthesizer database with 10,000
randomly generated sounds will do it
that way
so it searches here's his initial
example that he provided it's close but
not that close he thinks and so now he's
done that now he wants to rerun the
search you have to rerun the search now
that you've done this to give it an idea
of what are the important features
and he found his sound
now
you
synthesis is a naughty let's get past
this okay so how does this thing work
you have some examples you could load up
whether it's your vocalization or
something pre-recorded features are
extracted you rate some results based on
this it's going to update its features
and we're going to have this sort of
loop through the data set each time
you're updating what's closer what's
further and is it's learning the
distance metric basically it's learning
the importance of various dimensions
until presumably we find something close
enough now you say to yourself this is
similar to something like you flick
which is in some sense true one of the
things that's sort of innovative about
what we're doing is in the audio domain
well there's a couple of differences in
the audio domain we work under certain
constraints that they don't in the
visual domain one of which is when
you're comparing individual pictures
it's very very fast humans are just very
good at very quickly rating into
pictures boom you're done you've already
seen it with sound one of the issues is
we always have to deal with this
real-time issue we can't present you ten
sounds at once like one of the things
you can do in a lot of visual comparison
tasks is we present you ten pictures at
once and say find the one that sticks
out and you can just grab it we can
grasp so much more at once and and
anything time-based you have to listen
to it the other thing is and a lot of
prior work when you're talking about
what features are in here a lot of times
there's been this tendency to use what
is called a bag of frames and what I
mean by a bag of frames is that they
don't think too hard about the timing
relationships the sequence of features
and there's a big difference to a human
between
and but to a bag of frames
representation these two things are
identical a last thing I'll mention
relative versus absolute now maybe you
can imagine that there's a sound that
goes maybe I can't whistle so I go rule
obviously in absolute terms these are
very different sounds but they both have
this characteristic along a certain
dimension that if you measured the
change over time they both have this
it's going like this even though
absolute terms if we grabbed the
frequency the center frequency of what I
was doing these would not be the same so
we know that the humans are going to
produce some sound the range of features
they have to work with our limited we
may remap onto we may translate down we
may remap onto a different feature but
still somehow get this change so when we
started looking at things we grabbed
features which were typical pitch
loudness how harmonic is it clarity
spectral centroid how peaky is it etc
and we're actually using dynamic time
warping distance calculated on each a
14-time series independently and by that
we mean both the absolute and the
relative versions of them and for those
of you don't know what dynamic time
warping is dynamic time warping is an
algorithm that is used to take sequence
one and sequence two and find the
mapping between them this time
stretching between them that would make
them best aligned so we do a time
dynamic time warping between two
sequences in each of the different
dimensions and then from this we get a
similarity measure in each of the
different dimensions both the absolute
and the relative versions of them then
we give you some top candidates to show
you you rate the top candidates along
each of the dimensions and we learn
whether it's
absolute pitch that matters or maybe
it's the relative pitch contour that
matters and as I said initially we wait
all dimensions equally and as we move
along we change the ratings so here's a
preliminary experiment we're going to be
doing a bigger one a little later this
is the underlying interface for a
synthesizer and the question is can a
user find a specified sound faster with
synthesis the interface we just showed
you or a traditional synthesizer
interface in fact the interface that was
used to make the sound in the first
place and the task was as follows here's
the sound play it as much as you want
once you start you have five minutes to
match the sound as closely as you can
with this interface then we switch
interfaces here's a new sound as much as
you want to listen to as much as you
want then you got five minutes to try
and match the sound excellent question
this in this preliminary thing it was
actually straightforward the user is
actually rating themselves how close
they think it is now why are we doing
that well if we used an absolute measure
the problem is if we already knew the
right absolute measure to use we
wouldn't need to go through this whole
waiting procedure we would know exactly
which was the closest thing and
synthesis would just hand it to them so
we turned back to the users and said
okay well you hear the sound you hear
the result of what you're doing you tell
us how close it is
now what are we looking at here each red
dot okay so there were three trials per
user so this is just this is very
preliminary just so you know three
trials per user um here's the red line
is the experienced user the blue line is
the novice every minute we ask them to
rate how well they're doing how close if
they'd gotten and then what you see here
are on well actually I said three but
there's clearly four dots here so maybe
there was poor trials breeze I'll have
to go back and double-check in any event
you know there's four here as well so
let me back up for trials per user the
important thing here is to notice this
down here is the traditional interface
and by experienced user we went out and
sought someone who was used to
programming synthesizers with the knobs
and by novice we went and found a
musician who was an acoustic musician
and not programming synthesizers for
their living and we asked them to rate
their progress as they went and the
reason we don't have dots at the end for
some for many of the trials is because
the user gave up up here so take a look
at these lines these two yeah this would
be the reason for the for the
experienced one but I know for a fact
that the that the novice gave up on at
least one of the examples and I think
maybe two at the enemy sorry you quote I
quoted him earlier yeah again this is
very preliminary and this is this is me
waving my hands in the air and going
believe me from just looking at two
users but we really feel like there is
something here that if you've got
there's the blue line for the novice in
the red line for the experience this is
our synthesis interface how close they
thought they were getting over time this
is the traditional interface and
basically what you're seeing here is the
on average the experienced person went
from 0 on a Likert seven-point
scale up to you know what is that for
five and a half ish and up here you know
one two three four five and a half in
the experienced one we can't really talk
about statistics but they weren't doing
a whole lot worse with synthesist but
the novice suddenly went from I give up
or I'm doing terribly too I got
somewhere this critical they are up to
the results till the experience excuses
well I don't know young down here they
were very critical of their own results
and again so few data point this is
really me this is something that
happened we you know we ran this in the
lab we don't we are not talking like all
the other results I've been showing you
are we have 500 users we had 75 users we
had big for psychological experiments
big numbers of users this is a couple of
users so this is me basically just
trying to convince you that we think
we're onto something but we don't
actually have the data yet to back that
up but what I will say is we're trying
to do something a little bit different
and this is the punch line I want you to
think about when you walk out of here
we're not rethinking sound interfaces
we're rethinking we're teaching
interfaces to rethink themselves with
the first interface the evaluative
interface social EQ or IQ as it started
out all we do is we hand you examples
and have you rate them the result comes
out like the result comes out the second
one we went to the web and we asked lots
of people to give us examples we have
lots of people to do ratings we
collected the vocabulary from them
without trying to constrain the
vocabulary or seed the vocabulary we got
back the vocabulary made audio lies so
we could use natural language vocabulary
in fact idealize is running up on the
web now and if you want to teach it
words if enough people teach it the same
word it runs each night processes and
thinks about what it's learned from you
interactions and if you know a dozen of
you got on and and decided to collude
and teaches the word phys gibbet but
you'd have to give it the same audio
concept if you manage to do that visit
if it will end up on the word map and
then synthesist is the thing I was just
describing again this is this combines
two ideas which was the user provides an
example and then there's an interactive
evaluative paradigm that goes with it my
student mark had a we had a
philosophical disagreement about whether
using words would be a good idea and so
mark leaned towards a word free
interface and I sort of pushed towards a
word heavy interface and again you know
I think of this as two alternatives one
could have gone the other route with
synthesis and done like we did with the
other stuff and had lots of people named
things to give them meaningful names
here he wanted to say some things maybe
we don't have a good language to
describe there's nothing even remotely
universal about bore you do we but maybe
you'd want to make that sound and so he
thought examples and evaluation was a
better way to go with this which I guess
I agree with so if the stuff you think
you saw with stuff you think you saw if
you think the stuff you saw is
interesting then please go check it out
at music eecs northwestern edu I need to
acknowledge the people who actually did
all the hard work of building stuff
because of course I'm a university
professor I very lightly touched the
code if at all and it's the students who
really do the heavy lifting so far Rafi
worked on on having something learn to
learn reverberation effects from user
interactions bong Joon Kim is the person
that sped up the equalization learning
from having to use 25 or 30 examples
down to eight by using a transfer
learning from prior you
there's an active learning prime seat
the ramen is the one who came up with
the idea lies word map mark Cartwright
did synthesist that you just heard about
and uh by the way Mark's graduating this
year so if anyone out in the audience is
looking for a really excellent person
with a PhD who knows about machine
learning audio processing and interfaces
Brad right here Andy Saban was the one
who did the initial IQ that was the
first evaluative interface and these two
are both now working in industry these
three are still with me but of course
mark is looking for that job so anyone
on the web want to reach out to me and
I'll put you in touch with mark that's
the talk so thank you very much there
was a VT k we are Knoxville become the
direction but question please I'm
requesting that the through all of these
there's a mapping of perception to some
parameters based on a very specific
implementation of EQ or reverb or
whatever it is so you guys have done a
bunch of learning already of how
perception maps to those parameters
let's say that I were what i wrenched in
this technology and I wanted to make a
new product is your idea that hey you'll
have them you may have a very different
implementation your own implementation
of a reverb is your idea that you will
be able to use the training you've
already done or is it that hey here is
the model and design of the interfaces
if you implement it in this way then you
can do your own training with your own
users right okay that is an excellent
question so what Jesse Bowman can I just
brag and say also former student in my
lab I asked a very insightful question
so there's this idea of the control
space which is the parameters that
control aspesi
tool like the reverberate ER that we use
actually a bit but here's a little
diagram describing the reverberate earth
at of the underlining underlying
reverberate ER that we used and so he's
asking the question well there's a bunch
of control parameters for this
reverberate er that's great you learn
that but now I don't want to use your
reverb rater I want to use a different
reverberate ER maybe one that's an
impulse response based reverberate ER do
I have to start by learning over from
scratch and the answer is going to be
that depends because there's two
different ways that we can do this thing
one ways we learn a mapping onto the
control feature space in which case yes
you would have to learn it over another
thing that I didn't talk about some work
that savar rafi did was we also did a
version which did not end up getting
incorporated to Audie lies but we can I
can send a paper on it where you use
descriptive statistics on the resulting
audio on the resulting impulse response
function such as RT 60 or spectral
centroid blah blah blah and in this case
if you learn the mapping between these
descriptive parameters of an impulse
response function and the words then you
can happily swap out any other
reverberate ER as long as you know when
you turn the knobs this way it'll end up
with this kind of impulse response
function so you would have to learn that
mapping yourself but then all of our
user learning could be dropped in with
nope no problem so it depends how you
did it now the way we did the equalizer
we can do that because we describe eq
curves and it's very straightforward to
map between an EQ curve and a parametric
equalizer or a graphical equalizer with
the reverberate ER we went straight to
the control parameter space which kind
of ties us to this reverberate ER but of
course you could do this again mapped on
to this feature space and then then
you're free
with the synthesizer as well you know it
was a single oscillator amplitude
envelope but you know what if I have six
lfos and in awe well synthesist actually
was not a mapping into the future space
first sorry not a mapping into the
control space you it was a mapping into
a feature space that described the
output these are the features that we
were using these these seven plus the
absolute version of them absolute pitch
and relative pitch as in change of pitch
over time and in harmony City change of
in harmony city over time so synthesis
is one where you could in fact swap out
the underlying synthesizer with a
sampler or a granular synthesizer or
anything you want Oh Christian Bob
descent assists in the example it was
what you were searching for us like a
very short sound and it's time bomb so
could this similar technique we use to
be able to select something maybe
distinguish between a strange sound and
the Prophet sound if I'm if I have a
same plan I want to you know give it an
example what kind of instrument I want
to want to find and I use my voice to
give an example in it could it help me
find that well this is kind of an
interesting question we're trying to do
is make something that could go ideally
either way ideally what we would hope is
this thing would let you find either
something about the underlying tambor or
and this is somewhere we were a little
more focused the overall shape of the
notes so maybe you can imagine a string
going you or a trumpet doing a follow-up
and we would like it if what would
happen this is our ideal is that when I
go boom let's let's say that that was a
great imitation of a trumpet falling off
then what we would want is that top set
of examples one of them might be the
string sound that also did the falling
off and the other would be a trumpet and
then in that first round of ratings you
would tell it well what matters was it
the underlying Tambor or was it more
this court sort of pitch contour and so
our hope is that this technology when
appropriately used will allow you to do
either kind of search but our medium to
long term with this is now we talked
about evaluative interfaces we've talked
about examples and we talked about words
what we haven't shown you is an
interface that incorporates all of them
and and we're hoping that the search for
the evaluative interface could be
constrained if I said it's a string
sound that kind of goes boom then it
goes great I'm going to search in my set
of strings i'm going to give you things
that have some sort of well he's saying
kind of low so we'll give him low
strings he had sort of a falling away
maybe we'll give him a couple of violins
that are high but they kind of had this
you know they did this and and the
overall thing will allow you to use all
of the tools that you would use to
describe it to another person if you
didn't happen to have a violin in your
hand that's that's our goal I'm not
claiming where we've integrated them all
yet but that's what we're hoping for how
much we can do is leave the broad read
the words on the inscription or other
areas which are also subjective in it's
difficult it's a emotion sound quality
spatiality our stuff like this we have
our experience with labeling emotions
and it is really a painful either the
colloquial normal people who are using
slightly different meaning than what we
have clinic discredit in the literature
but the data were enormously noisy what
is it better to let them use their own
words the judges intently to mark to the
scientific mini corps it's a cobra stink
well I think we all know this about
language colloquial there's a reason
that math was invented there's a reason
that specific technical terms were
invented it's because we don't all
necessarily exactly agree when I say the
word love I know just what I mean but
you probably have a slightly different
idea of what love is my hope would be
the following I haven't shown you a
thing but i will show it to you now
where is it Shh ah this is we did
multi-dimensional scaling do not worry
about the what these dimensions mean we
took higher dimensional space and shoved
it no lower dimensional one what I want
you to see is this everywhere it says
the word underwater in this feature
space somebody labeled a reverberation
setting at least once that reverberation
setting with underwater the bigger the
word is the more often that
reverberation setting was labeled with
underwater the reason I say this the
reason I'm showing this to you is this
is what from our data a tight
distribution looks like this is a word
we can count on more or less more than
average this is the word warm done the
same way again these were reverberation
settings and what happens is we play a
reverberation setting to someone in for
this test what we did was we played a
reverberation setting and we said what
word would you use to describe that
someone says warm and then we took all
of the events where a reverberation was
setting setting was labeled warm each
point is a reverberation setting again
size is how many people labeled that
setting warm
but what you can see about this
distribution compared to that
distribution I could sure take the
average of these I would get a point
here which is of course a place that
nobody nobody would call warm what this
says to me is that there might be
multiple regions each of which there are
some group of people who think yeah
that's what I mean when I say warm our
current interface does not we decided
for the interface we were building for
this that we would sort of hide that
complexity we didn't want to give them
here's a warm and here's a warm and
here's a warm we could have instead we
decided to kind of go with the most
frequent one and say uh how we're going
to have to pick now but knowing the
shape of these distributions gives us
some possibilities we can at if you have
enough data from enough people we can
answer questions for particular words is
this a technical term that most people
seem to agree with the experts on is
this a technical term where you know the
technical term that generated the most
will call it disagreement but more maybe
generality is one for the reverberation
problem every single reverberation of
fact got labeled by at least one person
with the word echo which makes echo a
useless word for describing
reverberation why because by definition
all reverberations are echoes if you
would seen the echo distribution it's
everywhere but now what we have is we
can we could do the following we can
have if you if if you gave me enough
money to hire enough experts and and
pull them we could get their definitions
for words we can crowdsource our
definitions and ask how much overlap
there is and then do something but let
me as since you've asked these questions
I saved a couple of slides synthesis
now for the moment now please grant me
that the person who wrote say adobe
audition or the or one of the people who
wrote adobe audition this is why I was
hoping certain somebody would be here oh
let's grant that they're experts and
that the words they use of the words
experts would use and now we can talk
about why that's probably not true for
their drop-down menus what we did was
this is kind of an interesting thing
here's the vocabulary here's the size of
a circle is correlates with the size of
vocabulary this is in ableton live there
eq's had this many drop-down menu preset
labels audition had that many audio lies
the ones we learned from the crowd that
we decided we trusted 365 words this
actually we trusted more than 365 it's
365 plus 18 plus five plus four okay
these are the overlapping regions and so
from this what do we gather it turns out
between the preset list in this and the
preset list in that and the crowd there
are exactly five words that overlapped
amongst all three Wow here's one of them
clear and this is the preset that got
labeled with clear for audition this is
the preset that got late or sorry for
ableton this is the audition one and
here's the one we learned from the crop
the blue is the one we learned from the
crowd I should just date how he came up
with
well at this point who knows maybe his
presets were replaced by somebody else's
yeah and between these two I would say
the crowd pretty much agrees with the
Ableton one like these have
qualitatively if similar shapes in
pretty strong disagreement with the
adobe audition one so here's an example
what may be the experts don't exactly
agree on what the word clear for example
means and so it turns out to be a
linguistic problem all around and there
was actually a study and you would
expect this between England in the
United States there was a study done on
organ sounds where they asked people how
I'm forgetting the descriptive word now
I think it was warm they took audio
engineers in New Jersey that's in the
United States and audio engineers in
London and by that I mean London England
not London Ontario and said make this
organ sound warm so they did eq things
and there was broad agreement amongst
the London audio engineers and broad
agreement amongst the New Jersey audio
engineers but if you looked at the EQ
curves between London and New Jersey
they were not the same so this language
stuff gets tough right so there's oh the
on this particular word the crowd agrees
with ableton on another word maybe they
agree with the definition in audition
maybe the person who wrote Ableton or
the person who did this preset has a
whole community that all agrees with
them and maybe this person also has a
whole community that agrees with them
you're only going to know if you go out
and gather enough data which a company
like Microsoft can do more easily than I
can you know I can I can do this to get
a thousand people and then I kind of run
out of both research dollars and and and
you know time and all that other stuff
and a company can do something at a
scale that I never could so maybe one
day you you will find this out and tell
me which words are actually truly
broadly agreed upon by experts and by
non-experts or we could work together
and you can spend spend a few thousand
dollars and find out together so I see
it's almost five so i figured militias
time also oh you're four minutes got one
more quick thing have you considered you
know the dependency of the meanings of
words like here for example for
equalizers on the sound you abaya to one
of the things that I didn't talk to you
about how does certain words end up on
the map in other words not let's pick an
obvious pair I have a tuba which plays
really low notes and I have a piccolo
which plays really high notes now if I
ask people to evaluate what should I do
to the equalization of 100 Hertz for a
piccolo note well there's no energy at
100 Hertz so obviously their responses
will only be meaningful in the
frequencies in which the piccolo is
actually making noise and if it's not
making noise at a frequency whatever
evaluations or things we learn from
people will be meaningless to get on to
the map we also considered the frequency
bands which were represented in the
underlying audio upon which they did
they're the people did their labeling so
we had multiple audio files drums voices
guitars etc and only things that were
the word to get to be a big word on the
map we also took into account the
underlining underlying distribution of
spectral energy amongst the sounds so if
you're a big word like underwater or
clear that means that
we had a variety of sounds that covered
the entire not the entire but a large
part of the frequency range of
frequencies we expect someone to have
sounds in and that the word was
consistently used among them and that's
what makes it on there's there's a lot
of details in the paper that I have to
sort of gloss over but it's a very very
important point and you know we could do
it another way which is I said frequency
what happens when someone hands me a
sound they'd like to apply reverb to
that already has reverb on it this is
one we haven't been able to deal well
with with the frequency one we can
obviously look at the frequencies and
say there's no energy here but when we
start to allow people as we do now with
idealize to upload their own sound and
teach it a word we can play these games
and from quality control with
equalization but we don't really know a
good way of telling that there's reverb
already on the sound or we're also
starting to work in compression that the
sound is already compressed and that
will obviously change a lot of things
about how they perceive when we add more
reverb you already you're already able
to tell whether or not people agree on
on the words right and then you know if
the word is generally agreed on what
what kind of eq for example belongs in
words it would be interesting to also be
able to tell whether or not the word is
independent my opinion of the songs
source sound or if it's specific to
certain hmm she'll kill us we have not
looked it we have not looked for words
that seem to only be used to describe
say animal cries versus machine sounds
or something like this all of the data
that I've described was collected on
isolated instrumental and vocal sounds
recorded in a studio that were dry that
means no reverb added and this is true
for both equalization and reverb
we do have data sets at we do have some
data sets available on the web if if you
wanted to download some of this data and
have a look at it and see if you saw any
correlations I would be very happy for
you to do so and get back to me and that
would be I'd be excellent so let me know
this thing goes to go again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>