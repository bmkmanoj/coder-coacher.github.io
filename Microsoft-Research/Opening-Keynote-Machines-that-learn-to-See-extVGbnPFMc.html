<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Opening Keynote: Machines (that learn) to See | Coder Coacher - Coaching Coders</title><meta content="Opening Keynote: Machines (that learn) to See - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Opening Keynote: Machines (that learn) to See</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/extVGbnPFMc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
well it's it is a great pleasure to be
here this is a pretty impressive
audience I'm going to talk about
computer vision from a particular
perspective there are quite a number of
my peers in the audience I think in the
spirit of of Chris's and Evelyn's event
I have maximum opportunity to irritate
you and annoy you and get you get you
talking about how very very wrong I am
so let's um let's get her get on with
that so first of all I think I want to
say just one general thing which is I
think in studying computer vision we
have an immense privileged view of the
study of intelligence because there are
many scientists trying to study
intelligence from different points of
view some of them have set themselves
the task of understanding in in detail
what is the neural circuitry of the
brain we see the connectome project the
u.s. brain project the European human
brain project and so on people really
want to understand how the brain works
on the other hand adopting those
constraints accepting those constraints
of a fully biological explanation of
intelligence is a tremendously heavy
burden and so what we have the in
computational studies we have the
opportunity to do this from a completely
different perspective where we study
intelligence in some sense in an
abstract from an abstract viewpoint and
unburdened by some of those details and
I think there is a sense in which people
who division are really getting a deep
insight into some of the the nature of
intelligence and how that might work so
my talk what I'm not good of course
talking about the whole of vision what I
want to focus on is a trade-off and a
balance between two different styles
doing vision one that I'm referring to
as very cool detection which is
characterized rather as Rick said by
using big data and learning at
substantial scale and this is something
which you know we can now do and that's
why it's a rig mentioned that's why it's
taking off now and the other style of
competition I want to think about is
analysis by synthesis where the the
emphasis is on explaining in some
mechanistic and probabilistic sense were
the data that you just saw came from how
did it emerge what processes when were
gone through in order to arrive at that
data and then inverting that
relationship so which of these
challenges will win is a kind of a naive
question and perhaps a more subtle way
of putting it is how will these two
styles of computation work together just
under sullied about vision for those of
you that are not so familiar with vision
it's curious to some that vision has
turned out to be so difficult back in 66
in the MIT AI lab pass Seymour Papert
started a project on vision and fully
expected that by the end of July the
summer interns would have made quite a
bit of progress with simple objects and
that by August they'd be able to go on
to more complex some objects not just
blocks but also things with curved
surfaces and so on so i think it was it
came as a bit of a surprise that vision
is in fact quite a lot harder than that
and in a one perspective on why vision
is so difficult is depicted here that
suppose you want to recognize and
analyze this picture of a hand what
you'd really want to emerge is this
outline here which is what we have
seemed to see when we look at a hand
effortlessly and unaware of the
tremendous computing resources that are
mobilized inside our heads but if we
reasonably honestly mobilize the
reasonable signal processing
capabilities on the same data what we
get is something much more like what you
see on the left there
in which the contours that you would
hope to find and that would characterize
the hand are present but with so many
other confounding factors shadows text
remarks that somehow didn't seem
significant to us um I guess another
thing that I should point out about the
the picture on the left is how much
ambiguity there is so there's not only
is it not immediately clear which of
these contours are the salient ones but
also the contours if you look carefully
are broken in a way that doesn't bother
us at all as humans but which upsets
computers and so the there's a
considerable degree of ambiguity already
inherent in the visual world and in the
in the data that we have to analyze and
this has been recognized in different
disciplines in physiology in psychology
in mathematics and the consensus is and
I don't think I find much dissent from
that in this audience that we need the
calculus of uncertainty to deal with the
ambiguity of the visual world and what
is the calculus uncertainty of course
it's probability so we do expect to be
doing probabilistic analysis of the data
and actually computer vision recognize
this quite early and imported ideas from
different fields that here are some of
the the main epochs of technologies that
came into vision from from various
disciplines that were able to deal with
uncertainty I'm in the beginning with a
variational optimization didn't look
immediately like probability but it soon
became clear that there was a deep
relation market friend of fields and so
on the last one it's quite remarkable
that we have models complex models of
probability that describe the world and
yet oh models in which we can still do
exact inference this is something that
doesn't happen too often that you have a
model of interesting complexity but in
which the inference problem can still be
solved exactly so these are important
landmarks that have
progressively mobilised our ability to
deal with the visual world in
probabilistic terms so the two of my
branches were empirical detectors an
analysis by synthesis I want to talk
about empirical detectors a little bit
first and so what's what's happening
there where we have an image I to
analyze which is a pile of pixels and
some kind of a state X that we want to
infer from the image and what the
empirical detector does is it directly
via a tuned black box if you like
absorbs the image I and spits out a
interpretation of the state X and this
may be ideally it would be in a
probabilistic interpretation of the
state that is the posterior for that
state given the image and if this is
still sounding a little bit abstract
there's plenty of concreteness coming
but the characteristics of this way of
doing things is as I said earlier
massive learning a detection mechanism
which is somewhat closed so it's quite
hard as to get into the black box and
analyze what's happening take it apart
so for example if the detector isn't
working probably doesn't help too much
to go into the black box and try and
tweak things that's not not where you're
supposed to go much more likely you'll
look at your massive data set and try to
add more data to it that covers the
critical cases and I think envision
particularly what really surprised
people when empirical detectors became
mainstream and I'm going to talk about a
couple of exams in a minute is that it
was sufficient to do one shot detection
on individual frames without necessarily
putting a lot of work into tracking the
history and evolution of the state over
multiple frames and up to that time the
perceived wisdom had been that you did
need to track that that evolution in
history so it was quite surprising to
see that one shop detection could be
both accurate enough and fast enough to
process and analyze video at video rate
so one of the famous examples a
a series of pieces of work on face
detection initially coming out of
Carnegie Mellon but I think the thing
that really grab people's attention was
the viola and Jones paper at the turn of
the millennium from Mitsubishi where
they showed that they could build one of
these one-shot detectors for faces using
a boost of cascade and bike carefully
designing the the inference and learning
structure they were able to get
something that learned accurately and
could run in real time and the I think
another interesting thing which I'm sure
will come up a lot during the the week
during the conference is where the
representations come from that enable
interpretation of complex data to happen
and with the V Aaron Jones development
the representations are almost insulting
ly simple almost deliberately to upset
the field as it were they used
representations which were simply sums
of pixels over a rectangle here
subtracted from a summer pixel a
rectangle here whereas the previous
wisdom had been that you might use Gabor
filters or other ideas coming from
signal processing that needed to be
carefully tuned and carefully sculpted
so this idea that it really in your face
it really didn't matter what the
features were provided the learning
machinery was sufficiently heavyweight
and the data was sufficiently copious
and so was possible to learn detectors
like these from example faces on the one
side positive examples and non faces on
the other side and I was founded
intriguing that in certain architectural
cyclists support vector machines that
what you needed was non faces that were
almost faces the critical examples were
the most the most interesting one so
little you know bits of the facade of a
cathedral or something where you could
hallucinate a face in the Gothic
groaning you know that would make a
great critical example of course there
might be a gargoyle as well in which are
in trouble because that's a it's a real
face by the way one of the things I want
to do as we go along discussing this
sort of two pronged approach division
also to celebrate the fact that these
days vision really works and this is
something which over 10 years has
completely changed so i think there's
remarkable that the mitsubishi work
resulted in consumer devices within two
or three years so it was really not very
long before we saw on our Canon cameras
the ability to group up shots with face
detection all in a put the lightweight
processing that that was available on
board the camera at that time and that i
think was quite breathtaking that that
happened so quickly and that it worked
so well and even more remarkable i think
was the same technology worked on moving
people so the wisdom up to up to 2003
was that what you would need to do to
track a moving person was very much in
the analysis by synthesis para diamond
we of course will get to that in detail
but just to say that it would be
something like constructing an avatar
inside the machine and bringing the
avatar and articulated avatar up to the
data and somehow maximizing some
correspondence between the avatar and
the data and you know the very early
work of hog did that and filament and
Gabriela later which I'm going to come
back to did that but what viola Jones
and snow did was again breathtakingly
cheeky that it um there was no internal
structure it was the same trick
essentially as the the face detect of
the boosted cascade but taking account
of motion now by applying boosting not
just to the image but also to a map of
motion and you here we are you see a
depiction of those shockingly simple
features that I talked about before and
it really was a shock to the field I
think that you could get so much
intelligence out of such simple features
and that it would work so well
other task which has become of great
interest recently is the image net
classification challenge this is
interesting because it's a challenge
really on a scale that we haven't seen
before there are actually several
challenges but the one I want to talk
about is the one in which the job is to
classify images into one of a thousand
carrick categories and there are over a
million training images provided the
algorithms are tasked with producing the
five most probable classes the topics
that describe the the image and here is
a map of the possible tasks and you win
if you get your class in the true class
in your top five and of course some I
feel confident there's going to be a lot
of discussion of deep belief networks at
the the conference it would be amazing
if that you know this is not every day
that a piece of academic research really
makes the news but this is you know
disseminated through the New York Times
as well as through the user usual
academic channels and this is the work
from the Hinton lab where they took the
image net challenge and quite remarkably
and i have to say upsettingly aced the
challenge reducing the error rate on
this very difficult task by almost a
factor of two I think this is quite
breathtaking this is a map of the neural
network from the data end to the to the
output end one of a thousand classes
popping out in this box here the image
going in here and we don't need to say
too much about what the intermediate
levels are except just to say that there
are quite a lot of them so around ten
levels and secondly to say that it has
this property that the representations
that are Indian to mediate levels are
not programmed in but are entirely
emergent and this is because of advances
in learning technologies if you look in
one of the intermediate boxes you may
see some representations emerged that
look image like reminding you of the
blobs and bars that have occurred in
previous theories but nobody actually
put those there and they don't provide
knobs for you to tune the process you
wouldn't go in to those intermediate
boxes and adjust them in order to to get
better performance you're the norms as I
said before are what you put in the data
set you might want to add more critical
examples and hear a lot of the success
is the sheer size of the data set which
is actually inflated by reproducing the
data via various transformations to
which the resulting classification is
supposed to be invariant so this is um
quite a breathtaking piece of work that
automatically classifies the topic of an
image and and does so pretty
successfully but this time I want to
sort of step back and ask what is vision
really for deciding the topic of an
image is one thing but what do we
actually want vision to do well a sort
of old-fashioned view of what vision
should do is that it compiles a
description of the visible scene so
David Marr for example had the
two-and-a-half you sketch back in
nineteen eighty which was a rather
static passive database of the
three-dimensional properties of a scene
compiled by gazing around it and Bert
Old Hall a little bit later put forward
the idea that well vision is really
inverse graphics you have a set of
symbols inside a computer from which you
could simulate a scene and if that that
if your system was sufficiently complete
that you could simulate the scene
somewhat realistically from from the
pile of of symbols then the job of
vision is to invert that whole process
and to take the the scene and working
out what symbols would have generated
that scene but a more modern view is
that vision is active actually an active
sense that it's all to do with its
functionality within the organism and
Daniel wolpert puts this very
beautifully telling the story of a
certain sea cucumber this is not exactly
the right sea cucumber but ask a close
cousin which spends its life its early
life moving around and needs its brain
for moving and then it attaches itself
to a rock
and having attached yourself to a rocket
to no longer needs its brain so it
consumes it and I should have the Daniel
wolpert ads at this point that this is a
pretty good model of the tenure process
so what is your brain for according to
wolpert your brain is for controlling
movement and it's all to do with the way
you interact with the world and of
course we have highly practical examples
of those and again in the spirit of
celebrating some of the things that now
work in vision I wanted to have a look
at some of those here's some work from
Sadie's this is actually seven years old
now where the poor unfortunate guy on
the skateboard has escaped near death
you know the umpteenth time that morning
they even did it with real research
assistants when I was driving in the car
and from that from that stage 12 sorry
seven years ago we now have at the
announcement from Mercedes of a new
product where vision is on the the top
of the range Mercedes it actually works
in concert with radar and it's
sufficiently reliable at the time that I
tried this out in 2005 we drove round
nuneaton in the middle of the UK
somewhere and every two minutes or so a
ghost pedestrian would jump out into the
street and the brakes would have jammed
on if they've been connected up so that
clearly wasn't enough but in the seven
years from there till now the vision has
improved the integration with other
sensors has been crucial and now we have
a car equipped with sensors which in a
few cases will avoid hitting a
pedestrian altogether in more cases will
mitigate the effect of the of the
collision by slowing down the car much
faster and with not shorter reaction
time than the driver could and the
result is that they expect up to half of
all accidents with pedestrians to be
mitigated another beautiful example of
vision interacting closely with the with
the environment this is the
robot car being developed in Oxford here
it is they're testing track in they beg
Brook and I think the distinctive thing
about this project is that relies so
heavily on vision so one can rely on GPS
but GPS is not is not reliable in cities
for example where the skyscrapers at all
and the GPS is shadowed and so the
alternative which is working rather
beautifully here is to use recognition
of landmarks in the scene and to having
a deep enough analysis of the visual
texture of the scene to be able to
detect landmarks when they reoccur and
that's what's producing the rather
remarkable ability that we see here for
for self-driving and one last sort of
discussion of why we might not be
interested in much more detail in what
is going on in the in in the image then
simply detecting a topic would suggest
is this famous figure from llaves who
back in 67 tracks I movements and showed
I movements in response to various
questions about what was happening in a
scene so if the subject is asked what
are the ages of the people they're not
surprising in their eye movements track
back and forth across the faces in the
scene because those are the the most
informative but on the other hand free
examination of the scene produces the a
much broader distribution estimate the
material circumstances of the family
well I suppose the faces might be useful
but the clothes would also be useful and
the furniture would would be interesting
and so there's a much broader
examination of the scene so you know one
can think of all of the pixels of the
the scene as being potentially
information bearing and the the
necessity of being able at least to
examine them all even if you don't
examine every pixel every time and we
can think of this in probabilistic terms
as well that we're trying to detect
foreground objects against a background
and so foreground object insanity
plotted against a particular feature a
probability distribution plotted against
the feature could be pixel color or it
could be a some composite feature is
typically going to be a narrower
distribution whereas the the background
the expectations of what is in the
background are less highly tuned if you
like and so the distribution is is
broader and so observing something which
really is on an object the weight of
probabilities is very much in favor of
the object versus the background whereas
conversely observing something which is
in fact in the background the background
distribution may explain better actually
that pixel then here it is than the
foreground distribution and in many ways
the best background distribution you
could have is the union of all possible
foreground all of the objects that you
you know about that would make the most
acutely tuned distribution one would
give you the best chance of explaining
away a particular pixel as having
negative evidence that is not actually
part of the object that you're looking
for so there we are we I talked about
empirical detectors in which the image
directly leads to an interpretation of
the state but now I want to talk about
analysis by synthesis and so in this
style of analysis things are turned on
its head and the we begin with the state
and we ask well what kind of an image
would have been produced under the
assumption of a particular state and
then the probabilistic counterpart of
that is the probability distribution for
the image conditioned on the state and
the remarkable thing is that a little
bit like other um declarative miracles I
guess in the history of computer science
and I think of logic programming as
being an older miracle where you get to
describe things in a particular language
and you gave no clue as to what what
kind of a mechanism you might use to
detect one of those things but somehow
magically and automatically a detector
emerges and pops out from from that
description and in the in the with
analysis by synthesis something similar
happens
that all we have to do is state how a
particular image came to be and how it
was generated from a particular state
and via the algorithms that are
available the posterior can be can be
inferred from that and of course that
conceals a whole raft of complexity
exactly how those algorithms are
generated and envision often what people
do is they particularly interested in
the the mode of the posterior and use
that as an estimate of state and in the
special case that the image is supposed
to have been generated by taking the
state applying some function to it and
adding noise then this estimate of the
state that you get is obtained by as it
were simulating an image from a putative
state and then forcing that simulation
to be closer and closer to the image
that is actually observed and so this
very much motivates the term analysis by
synthesis in the more general statement
it's perhaps not quite so clear that
that's what's happening but we can think
of this as being in some sense
generalized analysis by synthesis so I
just want to kind of show a few
synthetic models the simplest
conceivable synthetic modern you could
have is a bag of pixels I suppose the
task is to peel the larmor away from
Machu Picchu where it lives and so you
might model the contents of the of the
foreground object simply as a as a bag
of pixels that is to say not spatially
differentiated but all of the colors
piled together into one probability
density function for color and similarly
with the background so here we are the
image is a pile of pixels and the
foreground distribution for any given
pixel drawn randomly from the foreground
is a particular distribution theater for
the parameters of those distribution and
the same same for the background so we
have a set of parameters now the theater
FM theta B which described in this very
simplified world how the image is
generated from state and the state
itself is a pile of in
k two variables one for each pixel that
simply switch on for being in the
foreground and off for being the
background here's a another view of that
that mobile here are all the pixels of
an image drop down in a color space and
here they are modeled in terms of
foreground distributions and background
distributions the blue components are
Gaussian components of the foreground
and so on and so now what we have in
principle is a data likelihood that
scores how probable it is that a
particular image was produced from a
particular state and accounting for all
of the pixels in the image both the ones
that give positive evidence and the ones
that give negative evidence and I've got
a simulation of what this model I've
done a sort of sample simulation from
the from the model you should brace
yourself for this because it's a little
bit shocking perhaps so here's the just
limbering you up here here's what we
have to do is supposing I'm given the
foreground of the background and I'm
asking now what does this model imply
for what the appearance of the object
would be under this very simple
hypothesis of image formation well
that's what it looks like it's there is
the kind of ghost of al armor in there
it doesn't seem to quite do justice to
the textural detail of alarma so first
of all I a I guess I could i should say
what we need to do better it's clear
that for some purposes we'll need to do
better but secondly I want to say that
for some purposes you don't need to do
better it's quite surprising despite the
fact that this is a terrible simulation
that if you use this inverted to find
the posterior the way that I was saying
actually you can get interesting
inferences even out of a model as simple
as this but supposing I go one step
further now and spatially differentiate
the model so I'm going to make the model
a little more elaborate instead of just
one color distribution for the entire
interior of the alarma object it's going
to be spatially differentiated so that
each region gets its own foreground
model and now I can simulate from that
somewhat spatially differentiated model
and you get something like this which is
there's a bit of a relief that's a
little bit more plausibly capturing some
of the the texture of the object and of
course one may or
be able to make better inferences with
that and we can now take that model the
insultingly simple one and install it in
an inference system and this is just
depicting really the same the same model
and where the state variables were being
used to explain the image but now we
want to kind of run this backwards in
the classic manner of a probabilistic
inference and infer state variables from
the observed pixels and knowing the
background and foreground models and
when you do this I'm going to
demonstrate it where the region of
uncertainty is restricted to this gray
region so we know the white stuff we
know the black stuff and we're asking
questions about what is foreground and
what is background in the gray region
and this is what you get it's not really
very convincing as a kind of a mess of
texture that has not resolved properly
so clearly we need to do a little bit
more than that it turns out that
building in a connection between the
indicator variables that simply capture
the statistical regularity of the world
in its most basic form that adjacent
pixels are likely to be on together or
off together imagine moving across a
raster line in the image as you make
your tour along the raster line
occasionally you'll switch from
background to foreground case of the
alarma that's only going to happen once
in each direction so really these
transitions are expected to be
relatively rare and you can capture that
with an Ising model making a markov
random field out of that by tying as it
were with suitable probabilistic waiting
the fates of these two indicator
variables and when you do that you get
something much cleaner now you know if
you're a connoisseur of segmentations
you look at this very closely and you'll
see Angela there are still a few
artifacts and you know squared off bits
and so on it's so you know it's in the
right ballpark but it's still not
actually quite good enough and so
there's more elaboration that people do
that I'm not going to go into that gets
you results that really are workable now
I'm going to show you
this momentarily so here is my
presentation it's the same presentation
but now it's in PowerPoint so these days
this Markov random field inference is
actually in PowerPoint so let me see yes
remove background I do that it's already
pretty much done it not quite what I
wanted so give a little more a hint
that's getting pretty good now there are
probably a few touch-ups that I could go
around and do and there are some brushes
for that but even the brushes don't
simply touch pixels that the brushes
rerun the inference process so that you
get a power assist from from doing that
okay now I'm gonna hide that away again
something sort of self-referential about
this isn't there so another another
thing that you know from computer vision
that works and and can be sold and just
one last thing I wanted to show with
bags of pixels here's a demonstration
from the Oxford vision group which i
think is quite remarkable because it
does just use that very simple bag of
pixels model the simplest possible
explanation of the data in the scene but
it uses also a strong notion of
coherence even stronger than that icing
term that I talked about before and also
does it over over time and so this I
think is a wonderful demonstration of
agile tracking it uses something called
level sets to keep the coherence and
what you think is that this is a picture
of somebody jumping around in the world
but actually she's staying still in the
room is rotating at high speed around
her I think it's a pretty impressive
demonstration so the fine art of making
these synthetic models is is highly
developed and I can't of course going to
the details of it in the talk excited to
say that I think it is an amazing
then if you like with with amazing power
someone can do things like adding layers
of hidden variables which are neither
inputs nor outputs but the described
intermediate properties of the data so
here's an example from the so-called
unwrap mosaic work which enables you to
process whole videos as a piece so for
example if you wanted to edit an entire
video you could go and edit one frame
and the result propagates through out
the entire video and in order to do that
there need to be some hidden
representations here's one of them which
is a sort of pseudo 3d representation of
the the face which is not known
beforehand but gets built up in the
course of analyzing the video and other
hidden representations that are there
include motion maps and other things
which you didn't actually want to know
what they were they are sort of
intermediate steps on the way to the on
the way to the result another beautiful
thing that people do with these
synthetic models is access incredible
levels of detail so that the work from
Manchester and more recently from Basel
is capable of simulating faces in such
detail that they can encounter an
unknown face and show you the properties
of that face from a view that has never
been seen before and then I guess the
last body of work which i think is is
also quite beautiful gets back to that
idea of negative evidence and the
background distribution in an image and
how do you model in generality the stuff
that is in an image on I said earlier
that of course the best model you could
have in some sense would be to take all
of the foreground objects you know and
combine those models into one grand
model of the scene but of course there's
inevitably a residue of stuff out in the
scene that you never got the opportunity
to model in generality and there's this
wonderful body of work ending with the
larue at our paper which is takes this
idea to a highly computable form and the
essence of this is that the visual world
is quite unlike other worlds of signals
so for example with audio signals what
happens is that when two signals arrive
at one spot they
overlap and they are additive but in the
visual world because it's a projection
from three dimensions on to two
dimensions signals combine in quite a
different way and obscure one another
occlusion as its referred to in envision
and so that you can think if you like of
the the dead leaves on a forest floor
that somehow one needs to capture the
statistics of the dead leaves and it's
amazing how this can be done to make
convincing models of generalized
backgrounds and images also i've talked
about empirical recognizes i've talked
about analysis by synthesis they each
have their characteristics and of course
you know this is like the conclusion of
a concerto or something you know where
was never really a fight between the two
there's going to be just a harmonious
bringing together in the end and you
know we can't afford to have arguments
about which is the most powerful model
because it turns out that we we need
both the connect challenge that came to
us I guess it's four years ago now is
points this up very clearly so giving
the computers the ability to analyze the
motion of people and their limbs and
interpret those in three dimensions is
quite a challenge and previously I was
involved in one piece of work trying to
do this and it was very much as I
mentioned earlier the analysis by
synthesis approach with an avatar in the
computer trying to explain the data
actually this is another kind of
variation on the explanation of data
theme but rather than explaining every
pixel sometimes it's more interesting to
explain the most salient parts of an
image for example the edges of things
and that's what this work did so you
know what we might have thought that
that would be the kind of thing we need
to do to make connect work we were past
some analysis by synthesis models of
this kind and it's interesting to see
what they did here's one run of the the
model you see the the real person in the
background and as soon as they move into
the shape of the model the model can
lock onto them and analysis by synthesis
kicks into action and you see it does
work but it
necessitated a high degree of
cooperation on the part of the subject
you know to a level that you wouldn't
expect if you were handing this off to a
third party to use in their living room
another kind of bad behavior which is
really symptomatic of the same
underlying cause is that once tracking
is underway things can go well for a
while analysis by synthesis is
proceeding beautifully but now the
subject gets some gets a desire to be
more than usual usually agile jumping up
and down and you see the the underlying
skeleton it's kind of reduced to
spaghetti so that clearly isn't enough
on its own so happened that we'd also
been working in the lab on object
recognition just for the hell of it and
out of interest using building various
versions of black box detectors to do it
and so I have to say very much to my
surprise it turned out that to be that
this turned out to be the missing piece
in making analysis by by synthesis work
and so this was a very considerable
undertaking necessitated generating a
vast data set too big in fact to collect
in the wild and so had to be generated
synthetically which again is I think
quite breathtaking because it's it was
not commonplace at that point in
computer vision for training to be done
from synthetic data and it's something
that's been argued about a lot but the
wisdom of the time was well you'd miss
some details of a phenomena in the scene
if you tried to replace real data by
synthetic data but with the range data
that was that's used here range data is
just sufficiently predictable that it
becomes possible to do this and so this
is the result that we have moving moving
objects seen in range data labeled
automatically using the black box
detectors and we're now the objects are
parts of the body and from the inferred
body part parts one can go on to infer
somewhat directly where the joints of
the body might be
and so what you end up with is a
pipeline of analysis starting with the
depth map and followed by the inferred
body parts which of course you see there
the inference is a little bit noisy
doesn't have to be perfect inference
because subsequent steps will refine
that to infer the positions of positions
of joints and so it's the the combined
effect of the pipeline the marrying up
the black box recognizer with the
analysis by synthesis that the Xbox team
did at the far end that finally made
something that worked well enough and
this kind of trade-off happens in other
areas as well here's a picture of a
medical image registration challenge
from criminy sees team in in Cambridge
and that the problem here is to take the
two scans of the same person some days
apart and map them on to one another
because without registering the two
scans it's impossible to make any long
due to dental interpretation of what's
happened between the time the scans were
taken and what you see here is
successful registration of the of the
scans there well overlapped even despite
the fact that the initial miss
registration was quite substantial and
what the graphs here show is the effect
of doing things the way that's depicted
here in which the kidneys in this case
our first detected in the image and that
prior detection step using black box
detection is sufficient of a queue then
to allow analysis by synthesis to kick
in and work satisfactorily and these two
error curves here show the relative
robustness of the two approaches the
level of error without using the the
preparatory phase of black box detection
and the very much reduced level of era
when the black box detection is used and
this is also this strategy is also
familiar in other areas of vision for
example the very successful Buechele
system that's used in in
movie analysis the task there is to
match successive views of of a camera so
that virtual reality can be done
effectively in those views and again
analysis by synthesis is involved but
it's much too difficult to do from coal
as it were so the ransack algorithm but
very popular in vision is used to do the
initial detection and propose initial
alignments and Photosynth that microsoft
produced some years ago to blend
together in a pseudo 3d fashion your
your tourist photographs works in very
much the same way in this preparatory
stage so that we are we have pipeline
processing in which detection is
followed by synthesis but what we want
to be able to do is to get outputs in
terms of interpretations of that
posterior which are totally consistent
so I I'm not sure on my computer vision
colleagues here we'll all agree with me
but I think that's something slightly
awkward about having a pipeline with two
such disparate pieces that don't exactly
work together and however the the field
of vision is really looking very hard at
this problem and coming up with
fascinating solutions one solution which
is from a few years back which I
particularly love in it again I think
this will be controversial with my
colleagues is this work from UCLA in
2005 where they they use the ideas of
important sampling to kind of have their
cake and eat it so this is a way of
taking a detector and taking a
generative model and combining the two
to make inferences from the posterior
the detectors are kind of prompting the
the system with some good initial
initial places to to look for objects
and fragments of objects but by
accepting those prompts somehow the
probability district
the posterior has been has been biased
has been skewed and that seems to be a
bad thing but the miracle of important
sampling sampling is that there's a
systematic way to compensate for that
bias get back on track and sample fairly
again from the posterior distribution
and that's what to a towel manage to do
and here's an illustration of their
system working interpreting this this
picture of sports people and here are
some of the intermediate steps detecting
various kinds of a special object in the
scene with tuned detectors that are just
there to do that prompting and then
finally here is actually a completely
synthetic image which is which i think
is remarkable in a way which is using
this as a synthetic model which was
actually built for for analysis but
here's a kind of five finger exercise
showing that you can actually
resynthesizer like the initial data and
really capturing quite a lot of of what
was happening there well suffice it to
say there's a lot of very interesting
work going on fusing detection and
analysis by synthesis in many different
ways there's a whole body of work here
which is actually doing it is fusing the
two but in a way completely different
from what I had in mind that is to say
doing that doing it in reverse that is
starting with a generative model
replacing some of the components of the
generative models with detectors this is
beautiful work in its own way but
completely different from what I'm
aching to see the vision field do some
other interesting pieces of work are
dealing with the fact that the priming
of analysis by synthesis is potentially
exhaustive and too time-consuming by
making that phase of the of the priming
more efficient and this this well-known
paper on sliding windows does that and I
guess most recently the work of felton
Schwab on hierarchical models also is
addressing this complexity issue that
you have so much work to do
to look around in an image to to find
the the initial configuration to apply
analysis by synthesis so there we are
i'm i'm hoping for a more intimate
analysis by synthesis I think there's
there's more that the field can do and
i'm sure more that the field will do and
i just want to end by celebrating that
while we think about these problems and
how to solve them in the most systematic
way there are so many things in vision
now that really work whether it's
commodity cameras commodity software
medical imaging cars that drive
themselves new user interfaces it's
wonderful i think that vision has become
so mature hey I won't i'll try not to
give you you know the mercury delay line
story of you know how 30 years ago when
i was doing my thesis it took 24 hours
to process each image there we are I've
done it now but it's the field has come
a fantastic long way that now we can
think seriously about mainstream
consumer applications that rely on
vision thanks very much
oh thank you very much Andrew very
thought-provoking talk I realize that in
my my enthusiasm to truncate the
enormous list of accolades and honors
and awards you have I did at the MIT to
mention one thing which of course that
andrew is now the director of Microsoft
Research Cambridge which is the European
basic research lab of Microsoft so
Andrew heads up research for Microsoft
in Europe so we have a little bit of
time for some questions because we're
streaming this live and also we're
recording the event for the post-event
website we'd like to capture the
questions as well as the answers so we
have some microphones going around if
you'd like to ask a question please just
raise your hand we'll bring you a
microphone and once you have the
microphone then then you may ask your
question so if you like to ask a
question just raise your hand perhaps as
a chairman I can take the liberty of
asking the belfry Chris if I if I were a
skeptic and I have to say I'm not a
skeptic and analysis by thin to this but
if I try to imagine that I were and I
might point to the the sort of Moore's
law of data the fact that data is sort
of doubling in size every 18 months to
two years there's there's dangerous
growing at least as fast as computer
speed is it not the case that a low
analysis by synthesis is very elegant
well I North always just want to just
throw more date Rijn at it instead if
I'm limited by computer power when I
just throw more data and get the same
results well there is an interest
question I don't know what's going to
work best I have to say for me the the
issue is not what's going to be most
powerful but what is most beautiful so
this is this is you know not very
industrial of me is it but I can't help
being seduced by the beauty of analysis
biosynthesis and particularly in the
probabilistic form as a as a paradigm
and I guess what might come back to bite
the massive data approach if done on its
own is this business of transparency so
you can go into a synthetic model and
look at the I mean I didn't show any
synthetic models of great complexity but
you know if you think about a synthetic
model with multiple stages
if your software isn't working what do
you do with the massive data approach
you're pretty much restricted to trying
to collect the critical data with the
synthetic approach you can go in and
look at the modeling assumptions and
look at the the structure of the model
and make changes which kind of seem
rational so I think there's much more
transparency in the in the gym in the
analysis by synthesis approach excellent
so questions just put your hand up if
you'd like to ask your question one
towards the back there alright so your
analysis by synthesis is in some sense a
little bit of a fib because of course
some of the models that you're talking
about I rather complicated and have
multiple levels of reality in them it
seems to me that the real game here is
choosing these intermediate levels to be
appropriate and your idea of beauty is a
little bit of a Ferb as well because in
fact the levels that are appropriate are
going to be rather empirical and
data-driven at least to some extent so
it seems the game here is going to be to
choose intermediate levels that are both
empirically useful and also in some
sense beautiful I don't really know how
you put those two things together so
you're going to allow me a modicum of
beauty to that's that's good but I guess
it also depends what you're trying to
model sometimes the synthetic models
that are used in analysis by synthesis
are highly physical depends what the
process is so sometimes you can look at
the intermediate levels of the of the of
the models and actually see how the
physics of what you're trying to to
capture maps onto those those levels so
I think the analysis by synthesis
paradigm is at its best in that context
for example if you want to track a ball
that bounces I mean you might have
something about parabolic arcs in there
and something about reversal of velocity
when you hit a surface all of those
things can be plugged in in the right
place in the in the synthetic model and
inform the structure of the model
whereas as we see with the black box
models you simply have to pile in the
data and let those intermediate levels
and representations emerge thank you who
would yes so there can you just press
the button on your microphone okay this
is better ah that's good we like that
say so the message from especially
towards the end was that you seem to
trust the analysis with this is much
better in using the the empirical
detectors don't need to kind of shortcut
the search problem so only assist in the
search aspect of the problem where's for
accuracy you really trust the analysis
place in synthesis as wondering if this
is really représentant or do you think
that the the data apartment the
empirical detectors the database
empirical detectors can actually also be
used to actually improve accuracy not
just ate and search for these problems
or if you really think search for
accuracy really just one synthesis well
you know people are certainly pressing
on the level of detail that you can get
from the feed-forward predicted kinds of
models and I don't know how far that
will go but the the kind of the logic if
you like of accounting with them
precision for what you see in an image
is somewhat irresistible and you can
even account for you know what's called
sub pixel accuracy where you actually
model the fact that one pixel is a
mixture of a piece of some foreground
and background because the division
between foreground or background
actually subdivided the pixel so there
are some commercial systems that get an
order of magnitude of precision like the
Vikon body measurement system for
example get an extra order of magnitude
of precision by doing the analysis of
synthesis so carefully I think we'll
take just one more question over at the
side there
so the predictive models that you you
mentioned the votes are they the
predictive model the feed-forward
predictive models like the convolutional
nets that yeah you mentioned you're
basically are composed of a linear
operator a couple non linear operators
and then it's just gradient descent from
there how is that not beautiful depends
what your criteria of beauty are of
course i also want to mention a
interesting piece of history which is
that in your list of papers on face
detection it on thought there was a lot
of work on face detection from the
machine learning community back in the
early 90s that never gets mentioned by
the computer vision community because at
the time there was no interaction
between the what was going on that's
then and it's an key provision it only
started to happen in the late 90s or so
because of part of the works that you
that you mentioned i think i was right
that the two communities were somewhat
separate for a while but i think you may
forgive us because in the last you know
since the turn of the millennium we've
more than made up for it and i think the
the richness that has come in in
computing computer vision with new
methods that's come from machine
learning is is staggering actually and
ripe ncc you comes for a circle because
you know the the best method we know we
know of for for object recognition is
convolutional nets and those are the
papers for face detection use
convolutional nets back in 93 and 92
yeah okay thank you very much in a
moment we'll take a break for coffee
we'll reconvene at eleven o'clock with
the the first of the breakout sessions
we have three sessions you can choose
which one you want to go to hopefully
choosing between the sessions will be
the toughest problem that you face at
this event just before we break though
perhaps I could ask you to join me in
thanking Elaine Rick and Andrew for
superb presentations</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>