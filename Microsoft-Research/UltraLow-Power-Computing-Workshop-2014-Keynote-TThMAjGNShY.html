<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ultra-Low Power Computing Workshop 2014 - Keynote | Coder Coacher - Coaching Coders</title><meta content="Ultra-Low Power Computing Workshop 2014 - Keynote - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ultra-Low Power Computing Workshop 2014 - Keynote</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TThMAjGNShY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I would like to introduce our keynote
speaker dock burger many of you know him
already it's a director of client and
cloud applications I think that's the
name of the organization in Microsoft
economic rasam research technology side
and in his previous life has been a
professor at UT Austin and I've been
working on computer architectures and
silicon scaling and a lot of his recent
work and previous when recent works are
about power computing so be a pleasure
and to introduce doc with his keynote ok
G thank you for the introduction so I
guess this is by your taxonomy this is a
teaser keynote right it's a solo energy
a low-power session and you know the
faculty summit is pretty pretty grueling
for everyone with the amount of amazing
people so maybe this will be a
low-energy keynote since it's coming up
at the end of the week so I have a what
I set out to do is to give a computer
architects perspective another you know
people like David and Ross who you know
intersect with or cross intersects with
computer architecture here but and a lot
of you are working in circuits and very
very low power devices and energy
scavenging and sensors so i thought i'd
give a computer architects perspective
on what's coming with silicon scaling
and what I think some of the directions
are afford to get programmable systems
that will be able to run a much lower
power okay so I think we all know that
we're facing a bunch of challenges Eric
Chung you know came up with this really
nice slide showing a showing a large
impact event that that the end of CMOS
scaling with nothing really sitting
behind it to replace the levels of
progress and scaling that we've gotten
used to is a pretty major event and
we're now in that end game and we don't
know exactly where because we don't know
how it will all play out
but we've all become used to a 50-year
exponential going reducing energy / / op
reducing cost per transistor allowing
for higher performance designs and so
now that we're in the midst of this
Twilight and this transition we're going
to start seeing ripple effects that that
have progressively larger and larger
effects on the industry and and so in
the low power space I think the
implications are pretty profound because
we've come to rely on consistent
improvements in energy efficiency per
operation and if you don't get those for
free anymore you have to do a lot more
work to get them and I think that's
really the effect that this community is
going to see so I check our shared this
slide with me a number of years ago and
it's interesting because at the time
this was around 2012 we were saying ok
he we know how to scale up to about
twenty fourteen to sixteen nanometers
and then check our said then things get
really really tough after that and
what's interesting is that if you look
at the this is the the CMOS roadmap so
this is your year obviously then
technology noted nanometers and the
number of billions of transistors you
can put on a chip you know we were
expecting to get to 11 nanometers around
2016 and what we're seeing is that it's
so hard that the cadence is starting to
slow right so in some sense Moore's Law
is already cracking and that you know it
was supposed to be generating a regular
regular cadence and that the pace of
that Keynes has shifted sometimes slower
sometimes faster over the decades but
we're we're now starting to see this
you're somewhere in here is where we run
into economic limits to scaling if not
physical limits and I personally believe
that it'll be economics that bring this
thing to a screeching halt as opposed to
you know we're not capable of
manufacturing smaller devices with even
with the lithography they have the
question is do we get enough gains to
edit economically justify going forward
and I think increasingly the answer is
no so I have two other slides on that
just to give some or three of the slides
just to give some context so and I think
most people know this pretty well I I
thought that the visual of it was pretty
striking that here we have year
the corresponding to process node and
the start is even a couple of years old
and then on the y axis here we have
number of companies building
leading-edge fabs so who is who is at
the leading edge for CMOS process
manufacturing and obviously people have
been dropping out and so we're down to
about five that's five is certainly
plenty but it's it's because of the
costs and that has had the costs in
particular has had a number of effects
so the number of asics starts each
generation has been dropping fairly
precipitously because the costs to do a
high-end ASIC go up so if you're going
to build a large aggressive ASIC you
know the entry bit that barrier to entry
now is a hundred to two hundred million
dollars to do something aggressive and
that's driven a lot of the startups that
are doing chips out of business that's
not say there are no chip startups a lot
of people are doing designs on you know
trailing edge designs older designs or
doing fabulous designs you're coming up
with an idea and then evaluating our
FPGA and then trying to find a partner
who will put in the money to take it to
production but the number of companies
doing high-end chips has gotten so low
that the venture community doesn't even
really track it anymore although there
are still it's not a zero number but as
the end the ASIC costs just have been
going up and up and up and that's in
addition to the fab cost of the
equipment okay so now if you look at at
why people drop out you know there's a
there's a really interesting ratio which
is the revenue of your you get from
silicon to the cost of the fab okay and
what we can see is that over history
there's this there's this dropout zone
with with a range of that ratio and when
companies enter that zone they tend to
drop out okay again it's not one hundred
percent correlation because as you can
see globalfoundries has been you know
kind of kicking underneath that zone for
some time they're more of a consortium
you know they have national sovereign
wealth funds investing so and there's a
lot of reasons to keep them alive so but
they're certainly economically a lot
more challenged if they were independent
business without any subsidies they'd be
really
really challenged and then you can see
the TSMC has you know has been skirting
along the upper edge of that zone for a
while they also make a lot of their
money off of older fabs and then you
know we have more subsidies but the two
that are really way above the zone are
Samsung and Intel but you can certainly
see that they're on an intercept course
and so I think the the natural
conclusion to draw is that you know in a
generation or two it'll just get harder
and harder there and there won't be more
than two companies at the fabs and
that's even assuming that they can make
the the gains economic okay and this is
the this is the chart that really
floored me which is if you look and this
is for TSMC but if you look at the cost
per transistor the argument is that it
has largely flattened and in fact it's
even it's even projected to go up a
little bit to the 14 or 16 nanometer
node okay and that's really what turns
this whole thing around because we've
had we've had you know exponential drops
in the cost per transistor for many
decades and so you can double the number
of transistors on a part and it costs
about the same okay and and that's and
the cost both to build the fabs and the
wafers and the double patterning the
number of design rules the yields its
and it's it's all gotten so hard that
it's actually driving the costs up I've
had personally I've talked about this
with the CEOs of both both altera and
xilinx as you might imagine they're
particularly interested in that trend
both of them have basically said yeah
this is true this is what we're seeing
and right now we're making up for it
with architecture and tweaking our
designs to get more logic with less cost
but we're not seeing out just a raw
transistor costs drop okay and obviously
tweaking the architecture you know you
don't you can't just keep doing that
doing that and getting big gains year
after year if you go to this you stay
with the same model okay so that's a
really big deal now I've talked to other
people and you know in the industry and
they said well the cost at 16
is going to drop a little TSMC is
basically building with you know 416
nanometers you know 20 nanometer area
transistors with 16 nanometer like
performance you know because they're
going to try Gator finfets and so the
you know they may they may resume we
don't know what will happen with Intel
until may squeeze another generation or
two out and again you hear mixed things
but we're clearly you know at a point
where the easy stuff is over and
individual companies may fall off more
quickly but we're not seeing the big
drops and the easy drops even with a lot
of I mean it was hard doing it here but
it's gotten it's gotten really hard I
guess one other comment is that the way
I see this playing out at the end is
that we're going to overshoot by one
generation because you don't know a
priori if the cost if you'll be able to
bring the cost down because once you
start deploying them and you start
manufacturing at scale you're able to
tune the process you keep improving
things you improved your yields and the
cost curves come down the really scary
thing is that I think what's going to
happen is we're going to move into some
node we're all going to be counting on
it we're all going to be you know it's
very excited for the number of
floating-point unit sand and the number
of you know the energy performance and
the level of integration we can do and
with the products road maps are
specified and people are have built the
asics and they're designing them and
we're waiting for them to come and then
at some point the company's the fab
companies will say we can't afford this
and they'll either drive the prices up
or they'll roll back to the previous
generation and and that will be like you
know in 2004 when intel cancelled its
high frequency part and and made a hard
right hand turn i mean it will send
shockwaves through the industry so I do
think we'll end up there's just too much
momentum for for them just to stop
beforehand because it's the business
model they'll build they'll push ahead
they'll hope that they can bring the
cost down and then we'll say whoops and
then we won't know that until we're well
into it okay so how does this play out
and this will be my last slide on the
you know the kind of market implications
then we'll talk to start talking more
about technical things so
so the semiconductor companies are going
to have to change business models to to
a large part and so Intel I think will
become a foundry there they've already
started you know Intel manufacturer and
other people's designs ten years ago was
unusual to say the least they're not
partnering with several FPGA companies
and as many of you know altera will be
manufacturing their high-end parts on
Intel's 14 nanometer process starting
next year personally I think the FPGA
companies will have to become service
companies you know they won't be able to
drive large volumes of silicon people
upgrading to the next parts so they're
going to have to start building designs
and SOC s and building solutions which
will be a huge cultural change for them
we'll start to see licensing models like
arm has proliferate so you'll build some
IP and you'll try to license it to lots
of people obviously you know not a very
large number of players will be
successful there will start to see rapid
ASIC services proliferate because for
because of the need for energy
efficiency and specialization people
will want to build a six but right now
it's too slow it's too long it's too
expensive and so people will provide
lots of IP blocks even more than today
in the arm ecosystem you'll be able to
cobble them together through
well-defined interfaces and bang them
out much more quickly than today you
know right now we're moving in the
opposite direction where it takes longer
to build an ASIC than it did at the
leading edge you know a generation ago
we need to turn that around and make it
shorter over time you know if the
process stays stable the tools will
mature and I expect much more industry
industrial activity around those sorts
of services but when you do that you
know in all of these business models
your margins are much lower and so we'll
see the semiconductor industry as a
whole become a lot smaller that's not to
say by parts you know we may still see
many more parts being sold but it'll be
a less profitable business and so a lot
of these a lot of these new models that
people will move to will be much less
profitable than today the other thing
we'll see is that the industry will
vertical eyes a lot as the silicon
becomes more of a commodity so you know
all the people at the bottom will start
running up the stack and we're already
starting to see that consolidation with
acquisitions in the fab space and the
tool space
and so they'll be running up as the big
players will be running down into more
specialized silicon and there'll be a
lot of tension on that interface and
it's going to be really interesting to
see that play out and when these when
everyone's trying to run up the stack
and down the stack it's going to be very
disruptive to business models especially
for the fabulous Emmys you know the the
AMD s and the invidious and the FPGA
companies and others because they have
to move to new business models you can't
do that on a dime but this this hard
stop will be a surprise when it happens
so so we'll see a lot of churn there and
then some of you may have seen the slide
before that using a few times in talks
but if you look at the way the the
industry used to look we had a much more
horizontal ecosystem ecosystem and this
is well known it's becoming increasingly
well known that that you know we had
device manufacturers Apple of course had
their own OS devices Microsoft heads and
productivity apps but then people would
build apps on top of it but then the
hardware space things were largely
stratified you had the same thing in the
cloud where people were building you
know servers and chips and then Google
of course started you know building
their own data centers fairly early on
but then what's happened you know in
over the last you know some number of
years oops my animation didn't make it
so i'll just keep clicking at all it
does five six seven eight nine ten there
we go 11 how did that happen somehow i
got 10 dummy animations put in there
we've seen the industry start to
vertical eyes on a number of integrated
players and I think we expect this trend
to continue and so you've you know
recently over the last four or five
years you've seen these companies start
to push much much more aggressively down
into first devices and then chips and on
the top end you know building their data
centers but then building their own
servers and then increasingly and we're
starting to see people there's all these
rumors about people building their own
ships for the data centers and of course
there's I didn't draw the fabs on top of
that so I think what what you're going
to see as this trend continues and
accelerates with the changes we see in
technology scaling is these pillars will
eventually be end-to-end from chips in
the in the clients all the way up to
chips in the cloud and they may even
start pushing down into tools and fat
okay because these and now these players
are going to be really commodified and
be trying to push up to create value and
it's on this boundary that we're going
to see a lot of interesting churn okay
but this this trend will continue and
even you know even Microsoft it
purchased Nokia and and and got access
to huge menu international manufacturing
and distribution facilities for devices
I won't comment on chips you know but
Google and and apples well known to be
building lots of their own ships there
are even rumors that they were buying
fab capacity go ahead yep the pendulum
swings I think it's sure that the
question is is this thing's used to look
this way with IBM and then we went to a
more horizontal ecosystem I mean IBM has
always been vertical if I go to the
enterprise space you know the chart
would look a little bit different this
was the consumer space that was perhaps
less mature I think at a given time when
you see a horizontal ecosystem or
vertical ecosystem it's a function of
both the the level of investment
requiring a technology and the business
scale and where people are able to build
the moats to preserve monetization and
so for example 10 years ago Intel was
able to build processors with enormous
capital investment in nres that would go
to thousands of customers the same part
could proliferate across many segments
of the market and they might turn you
know an IBM used to do the same thing
with their mainframes and they might
turn some functionality on or off or use
them in different packages but they
would design a small number of parts and
they would put 100
millions to a billion dollars into these
parts and so in 2001 I mean sure they
had some competitors like AMD but the
thought of one of these companies trying
to out Intel Intel at its own game was
crazy right because how could you
possibly do it better than until they
have they have their integrated process
technology they have all of this stuff
but they you know but as as that as the
gains have slowed and the need for
differentiation has increased its now
it's now profitable perhaps for some of
these companies to start investing you
know because because there's mean the
cadence is not as fast and and and
they're at their now had enough scale
with with some of their markets you know
advertising you know consumer phones
distribution networks which is really
what Amazon is to first order they there
yet you have enough revenue that they
can actually afford to build their own
and specialized and get away from the
rest of the market so i think it's where
you see these monitors eight monetizable
moats bring up coupled with how much
investment you can make to how many
players and and the the thing that's
interesting about that is as its
segments more you know if when Apple
builds own chips it slices off a big
chunk of the volume for the the phone
industry so that the integrated
providers now have a smaller volume to
sell to and if another company slices
that off eventually you know if you have
if you have a horizontal provider and
you're their only customer right you
know you just acquire them and cut out
the margins so process I'll repeat the
question so the question is what to what
extent is there the need for
differentiation versus the need to
remove abstraction layers I would argue
that that there's an overlap between
those two so if you if you want to
differentiate and you have a very stable
stack on top you might need to actually
remove abstraction layers or work across
and that frees you to differentiate more
quickly and if you can't change x86
a limited number of things you can do
under the covers but if you have the
whole stack and so as we've seen
general-purpose processing cap off it's
a lot of people are arguing now for
specialization but it's hard and it's
inefficient I mean in terms of design no
one really knows how to do it but it's
hard to do if you're a horizontal player
because you don't know what to
specialize but once you have these
integrated stacks you can now you can
now specialise across many layers and
you're completely free to blow things
open I mean Apple has been a master that
they've changed is a as many times in
their career because of their model so I
but I think we're going to see that
cadence increase because everyone's
competing and now increasingly everyone
can specialize and differentiate and
this is necessary for aggressive
specialization to happen okay so sorry
for the bug on the slide but I would
argue that from an architect's
perspective from my perspective
architecture sort of is everything when
you don't get processed scaling gains
anymore now of course from a circuit
designers perspective you might say we
still need low-power circuits and from
an energy harvester researchers
perspective you might say we need to do
better energy harvesting and those are
all true right but but we don't get we
don't get free gains from the process so
if we think about architectural II you
know if you're if you're not getting
things from the process and you want to
build something that's fairly
programmable general purpose what do you
get from what you get from the design
you know there's lots of options and you
know this is a variant of a very
well-known spectrum where you have fully
general systems on one end you know
single CPUs that are optimized for
single thread performance but there are
thousand X less efficient than a six or
ten thousand X depending on you know the
CPU and the ASIC and then there's lots
of points in between where you get
increasingly specialized or harder to
program and here you know with a six you
can't program them but you get
programmed them but you get much more
efficient and so what I want to talk
about a little bit for the rest of the
talk is what are the architectural
directions we should think about for
ultra-low power computing or is it we
just have commodity you know cortex a9
ARM cores and they don't get much more
efficient that's what we use and I don't
think that's the case
I think there's actually still a lot of
fat left to be cut giving us fairly
general low low power and highly
energy-efficient computing for all these
very deeply embedded you know you bik
witness devices and wearables and an
even mobile and wait where should we be
investing in this space and and what are
the sources of the overhead that we can
cut out ok so if I look at the world
today and this again this is an
architect's perspective and I think this
is complementary to you know better
batteries and low-power circuits and and
all of that we have in the phone space
we have you know the big little approach
so I've got some you know one or more
high-end cores and then I've got some
small in order arm cores that I can
offload things for doing continuous
sensing and continuous monitoring when I
want to have something that's running
more continuously as opposed to run hard
and stop I put it on the little so
that's pretty pretty common I've got
mobile GPUs for doing simdi sorts of
computations and the the value of doing
simdi is of course I cut out some of my
front end overhead because I can do one
front-end operation and then many data
operations so I'm amortized over heads
of my instruction fetch and decode and
all that in again in the in the lower
even lower power space we're seeing
customized cores and 10 silica is a good
example of this especially for the DSP
space where you can start to customize
specific instructions that are key to
that that flow and so you might have a
many core array of 10 silica parts to
handle a lot of the fairly fixed
functions that you want in deeply
embedded devices but of course those
can't be you still need programmability
because you might change the algorithms
but but you want to get them pretty
close and then of course on mobile
phones in other words we have lots of
asics okay and those are those are being
used more more and more because the need
for efficiency is greater of course as
you again as you go down along the
spectrum you get less general and it
gets harder to design these things and
you're less agile and that's a real
tension that we have you know do you
completely redesign your your SOC every
time you want to deploy a new device
it's very expensive and very hard
okay so if we start thinking about
specialized accelerators of which the
asics are one version I'm going to talk
a little bit about the cloud for two
slides that's the only time I'll talk
ISM focus on devices here but some of
the constraints are similar so one nice
thing is when you have CPUs you're fully
general you write high level software
you can compile it down you know you can
run of course if you want to maximize
efficiency you do a six and then
accelerators sit somewhere in between
where you want to you want to be perhaps
domain-specific so your general within a
domain and you don't get all the
efficiency but you get some so the
accelerators sit there and especially in
the in the mobile space and and the
client space there's something between
the CPUs and the asics that I call the
uncanny valley of death for accelerators
and this is it's a play on the the HCI
com concept of the uncanny valley where
things look you know almost lifelike but
not close enough and they're really
profoundly uncomfortable if it's a
cartoon it's fine if it's almost human
but a little bit off in the eyes it's
really creepy okay we have a similar
problem with accelerators where which is
if we have some some workload blocks
that are really really important you're
going to you're going to do them all the
time you stick them in a six and you do
maybe your top five and your top six and
a six so in you know mobile phones we
have lots of basic blocks all over the
SOC and then and then your CPUs are you
know fairly power efficient you know
they're you especially if you do the big
little thing so your ace your your
accelerators need to be more power
efficient than a little course but the
value proposition is that they can run a
bunch of things but you've already taken
the 10 most important things off the
table and so they have to accelerate
whatever is left in the value
proposition very often does isn't it
usually isn't high enough to justify
building those in addition to the little
core and the mobile GPU okay and so I've
we've actually seen this you know over
and over and over in the in the phone
industry where people are pushing down
some accelerator path and you know it
looks great until they decide to harden
you know these five things and then
and then game over so these are all the
debt acceleration sitting at the bottom
ok and now in the cloud and and this is
just you know another constraint we have
and there this is related some things on
the client space is that if you think
about what you actually want for a for a
specialized accelerator in the cloud
there this is the life time you need it
to be viable and this is the percent of
servers that it needs to run it okay and
so I the reason I have five years here
is if you're going to build an
accelerator an ASIC accelerate or
something specialized to be very ultra
low power for its workload you want it
to be working for about five years
because it's going to take you two years
to design it get it on the road map get
it into the server design and then the
server is going to be you know have a
lifetime of about three years two plus
three equals five so you want that you
want that thing to be stable and useful
even if the software and the workload
mix changes and then of course because
of the overhead of manageability for
managing large scale data centers at
scale you really want this thing to be
useful in almost a hundred percent of
your servers okay so what you really
want is an accelerator that sits in
every server because you don't want to
specialize them so that workloads can't
migrate around is demand shifts and you
want it to be viable for five years so
this is sort of the bar that would
justify a fixed accelerator in the cloud
and this is what we actually see right
so you know any1 property is not going
to be running on more than you know two
two three four percent of your servers
you know at the scale that these large
companies operate even the biggest work
single workloads first-party workloads
like search ranking for example only
runs on a few percent of your servers
and the software's changing monthly okay
so it's really hard to get ahead of this
so free you know so if if this is what
you want and this is what you have it's
actually really tough okay so that
really comes into ubiquity you want this
thing to run everywhere and be
ubiquitous / / space and time and so in
the cloud we've made a very big bed as a
lot of you know on fpgas to try to
bridge that gap but that doesn't really
work for the mobile space you know some
some phones have had very small fpga is
in it but the FPGAs since the spatial
parallelism you 10 didn't need a lot of
silicon real estate to do interesting
clothes and that so look at that high
silicon real estate is too expensive for
for these small mobile devices okay so
in the data center we dealt with this by
building this fabric where you can
offload services and you've probably
heard some about that in this you know
in this faculty summit but this doesn't
really solve the problem in the mobile
mobile space so I think we have the
cloud story pretty well in hand but what
are we going to do about the client and
and and by client I mean even smaller
devices or wearables or deeply embedded
devices so when I was thinking about
this after G invited me to give this
talk and I'm trying to think about
general computing so programmable
devices not a six because those you know
we know how to do those they're just
hard and expensive there are really four
targets and Mark next door thank you
thank you all by the way for coming to
my talk so you know we didn't have a
hundred percent of people in one room
and zero percent the other although you
might be better served to go next door
mark is actually talking about this
today so I'm stealing a little bit of
his his content so you you're not bereft
and and really there's four four areas
where we we see opportunities to harvest
energy and I think the architects in
this space and maybe many others of you
will say this is obvious and well-known
and that's fine we'll get into some more
specifics so the first is the front end
when you're when you're fetching an
instruction and decoding it and then
dispatching it to issue logic that
actually takes a fair amount of energy
you're hitting instruction caches it's
pure overhead the really nice thing it
makes you is it makes you fully general
GPUs use their sim d model and and the
warps and you know nvidia parlance to
cut to amortize this and cut this way
down and as long as you're doing the
same operation many times on different
pieces of data close enough together you
can amortize some of that but there's
actually a lot of overhead hear of a lot
of opportunity to remove it computations
are often very regular code is reused
you know instruction locality is high so
how do we map things to get rid of that
front end logic out of order issue logic
is the second one you know app would out
of order cores are much less energy
efficient than in order course but they
provide a very nice performance boost
and that's one reason they're still
widely
use it even in the mobile space so you
know you have big little you have tend
to have out of order in order least for
higher end devices in the phone space
but this is a this is a real real
opportunity if you can get rid of this
but get the benefits you've saved a lot
of energy and gotten a lot more
performance you know run hard and stop
becomes a lot more attractive than ok
then fixed al use is another source of
of overhead you know you you provide
some set of arithmetic units but there's
a lot of times that there's other
operations or you could fuse them and
you'd actually save a fair amount of
energy and this includes both the you
know the the network connecting them as
well as the ALU operations themselves
and then data movement is a really big
one ok so this is you know not just not
just going out to DRAM which is hugely
expensive but even touching your l1
caches and going through your cache
hierarchy and your loan store cues and
all of that so these are for fat targets
and you know if we can get rid of these
or cut them way way down we can get much
more energy efficient computing for eat
more and more deeply embedded devices ok
so so number one through three you know
we've had a project running in MSR for
some time called et tu using an edge
architecture which is built on some of
the work that I did at UT this stuff
takes a long time and so you know to
really get the right abstractions and
the right targets what we found is that
we're actually able to significantly
reduce the energy of one through three
even below what you'd be able to do on a
simple lightweight in order arm core and
so this this has become our primary
strategy for greatly reducing energy for
for deeply embedded or ultra low power
devices while still getting reasonable
performance in a programmable way
obviously if you can do a six that makes
sense to do and so I'll talk a little
bit about how how we get how we reduce
these chunks of energy and I'll talk a
little bit more about opportunities to
reduce data movement and then the nice
thing about the architecture that we've
that we've I think matured fairly well
now is that it's also very extensible to
specialized architectures so it's not
just fully general but if you say hey I
have something a little bit more custom
I want to push on it allows
specialization
in a fairly automated fashion and we
have a full tool chain built to support
this ok so the architects near know I've
been working on this stuff for a long
time but we're getting now getting very
very good results the and so with
apologies for the people that have seen
this many times before you know if you
have edge stands for explicit data graph
execution and there are really two key
components in NH architecture that are
different from risk or sisk the first is
that we take blocks of instructions and
we treat them as an atomic unit so they
might be fetched atomically they may
execute in some you know data flow
obeying order but then committed as an
atomic unit you know so it's really like
a programmer invisible transaction and
then within those blocks we actually
build an explosive data flow graph or
instructions point to succeeding
instructions so so when I was briefing
some of our executives on this not too
long ago chaloo asked a really good
question which is you know you're
getting something for free here I don't
get it you know what are you really
giving up because nothing is free right
and it really was a great question and
it got me thinking and that what what
really the trade-off we're making is you
know in the modern era energy is a
critical resource and code size used to
be really driving almost all of your
architectural decisions so if you go
back to when sisk architectures were
defined in the late 50s and early 60s
memory was so expensive that they did
every single possible bit of encoding to
reduce your code size and minimize that
and then and then more recently you know
in the 70s and early 80s when the RISC
architectures were being defined the
goal there was to reduce the control
logic so you could actually fit the
whole thing on one chip okay now there
were other benefits like easy pipeline
ability that came later ability to have
fewer instructions so that the compiler
could schedule them but that was really
designed to drive down the microcode of
control and and and the risk the risk
designs were able to be integrated onto
single chips first you know so in some
sense both of those were storage
efficient architectures one was
instruction encoding and one was control
logic and that what we've actually done
is taken whips is taken an opposite
approach which is just say we're
actually going to increase the code
sighs and bloat the binaries by thirty
percent and in some sense cases as much
by fifty percent and in deeply deeply
embedded devices where you still have
tiny amounts of memory this can
certainly be a problem but in the phone
and wearable space we think it's
actually an appropriate trade-off and so
what you do with that bloating is you're
actually conveying a lot more
information to the from the compiler
that allows you to save energy at
runtime you know so so an out-of-order
core will discover all of these
relationships between instructions
dynamically using expensive hardware
we're actually doing that analysis well
the compiler already knows that but
conveying it through the ISA to the
hardware and the cost is an increased
binary size the benefits are that you
can run efficiently with much much less
energy and and so I'll show you how we
tackle those three things so just you
know to get us all on the same page with
with the various overhead so what about
an order and out of order core does and
I think most of you know this will all
be quick you know you have a this is
your list of dynamic instructions your
design and construction order that gets
executed and say a risk is a and you'll
be fetching some number of them at a
time let's say for so you fetch for you
know you fetch another for you fetch
another for and then you start doing
register renaming to identify the
dependences among instructions and you
know register renaming requires a fair
amount of analysis and you have to save
the rename maps to roll back in case of
a branch misprediction okay so you're
doing these the all of this analysis and
then and then building this graph which
you're maintaining at runtime in the
hardware and then of course when you so
as you're fetching instructions you're
adding new instructions the graph you're
connecting them with your register
renaming and then you're taking them off
the top with commit and you have a
window rolling down over the program you
know that maintaining the fraction the
data flow graph for the fraction of the
program is in the window and then when
you actually want an issue instruction
you take the tag associated with that
instruction and you broadcast it to a
fairly hefty cam and it's that cam goes
grows very rapidly with issue with and
that's how you actually traverse one of
these arcs you take a tab you broadcast
it to a cam so you're broadcasting it to
multiple operands for all of these
instructions and then I 2 and I for will
say yep where the
ones that need this the outputs of i0
and i won and now you're traversing that
arc and you know if it's other input
dependences are satisfied you can issue
I too so all of that overhead of
renaming to form the form the arcs cam
broadcast to remove the arc or to
traverse the Ark is runtime overhead to
rediscover stuff that's largely known by
the compiler not completely because you
don't know what control flow path is
going to be taken and then of course
what instructions commit they're removed
from the graph and like I said this is a
rolling window on a fraction of the
program's dynamic instructions okay and
so as an alternative what we actually do
in an edge I say is we actually build
this graph up priori and expose that
through the instruction set and so here
I have three basic blocks which so I
have a branch here that's jumping here
conditionally so I might fall through
and go here and then go to here and you
can see that we've got these here's the
graph that's built and this you know the
overheads a little higher here of course
because it's a small example but I have
the same graph here where instructions
now like let's say this ad which
corresponds to this ad here rather than
having to input source registers r2 and
r3 that right the resultant r r1 this ad
is getting sent its inputs from
somewhere actually it's getting sent its
inputs from this register read in this
register read so these Reed's go and
pull operates out of the register file
and send them to this instruction i5 it
will receive these two operands it will
fire and then we'll send its results
explicitly to I ate and I see okay and
so this whole graph that previously in
an out of order core we had to build
dynamically and hardware we actually
encode in the compiler through the ISA
and just hand it off to the compiler in
one chunk okay again this is this is not
new stuff that you know we've been
working on this a long time but now in a
micro architecture you know you have
your caches fairly this is a fairly
standard thing now you have an
instruction window but rather than a
centralized register file you have one
for you know the begin and exit to a
block we can actually you now have
first to hold the operands that are
being produced for each instruction and
so on instruction issues and as a
pointer to say this buffer slot and this
buffer slot it will just write the
results in there and there okay and I
don't have time to go into this in great
detail there's lots of stuff to read if
you're interested but one of the really
nice things is that when you have any
recurrence in the program okay and so if
you have a streaming app or you have a
loop and you know some enormous fraction
of many programs is actually just in
recurring loops this this this data flow
graph and a block that's been mapped to
part of the window of course you can map
multiple blocks is actually resident
there and the way you can re-execute
that loop is simply by clearing the
valid bits for that block and then a
number of you know the independent
instructions like the reeds will reissue
go to the register file and you'll start
traversing that graph again so since
that graph is pre-constructed you don't
need to change anything you can just
sort of reinitiate it and now if I have
a loop that's consisting of multiple
multiple blocks you know one two three
four for example and I can fit it in the
window I can hold it there too and as
each block finishes it will just refresh
and so what you've done that with with
that is you've actually cut out all of
the front end you're no longer fetching
decoding dispatching instructions all
that energy is gone so while you're in
that loop mode the only energy you're
expending aside for leakage is issuing
instructions so you have you know
instructions select and wake up running
to me al use and forwarding them back
okay you still have data movement you
still have the memory system okay but
you've actually cut out that front end
logic and net and then as you provide
more and more of these resources and a
larger and larger fabric you could
actually capture the working set of
larger larger recurrences so perhaps an
inner loop and an outer loop okay so
this this this is a first step towards
actually spatially mapping large regions
of code and and and just being able to
keep going and refreshing refreshing
them and this is similar in spirit to
what pipe wrench tried to do and a much
finer granularity but but pinning the
code and being able to refresh without
reef etching when you have something
that maps
well is is really really nice okay and
then as as you know the architects your
know well we we actually put the support
in to be able to dispatch different
blocks to different you know from one
threat on a multiple-course so if you
have eight core an eight-core array here
you can of course run eight separate
threads which one you have the threads
gives you good energy efficiency because
each one is running on a very
energy-efficient core especially when
your recurring so each of these cores
has already cut out all of that energy
overhead for out of order execution with
the exception of the wake up and select
logic and and then when you're in a
recurrence all of the energy overhead
for doing the front-end fetch and decode
but now what you can also do is fuse
cores and dispatch blocks different
blocks to different cores and treat
those multiple courses single few
diffuse entity so I can run one thread
on four cores and say for example you
know two threads on two cores each and
that allows me to have a larger spatial
mapping for doing this recurrences so in
some sense I could say hey I want to
allocate five cores because that's
enough to fill up this this loop that I
have and now I can cut out the the
front-end energy it also allows me
although to dynamically shift between
big medium and little depending on the
needs of the workload okay so i can be
running some number of course just doing
continuous you know sensor processing
and analysis to say hey is there
something i should be paying attention
here too and those are typically loops
so i can just be sitting issuing
instructions and I can dial down the
clock right just to you know be able to
ingest that that input data fast enough
and then when something interesting
happens I can actually spin up fuse all
the cores and then run out it very hard
and this is a this is a very high
performance but also energy-efficient
substrate and this would be a 16 wide
issue sort of megacorp so I don't want
to spend much more time on this but so
if you think about this block atomic
execution model that we've been working
on for a long time and now I think
making very very good headway in the
energy efficient space so the the intra
block data flow exposed through the ISA
gets rid of most of your out of order
energy so you can actually get higher
formance much higher performance than a
than in order core for similar levels of
energy you can pin the blocks in the
processor core to cut out your your
front end energy you can now issue
blocks to more than one core allowing
yourself to sort of grab more or less
energy you know depending on your
workload mix and the needs of your
system at the time and then finally
these blocks also form you know they can
run from for 128 instructions they
actually form an interesting
configuration that you can use to
synthesize into an accelerator so if you
have a block that's something that's
very common that you're going to be
doing a lot let's say in your in your
phone you're processing some sensor
stream and it's in a block or two what
you can actually do is take that block
and synthesized it into an ASIC but have
the same interface that you would have
to software and so when the software
reaches that block it can go out to the
synthesized unit and this provides you
an automated way to take code regions
and synthesize them into specialized
accelerators and then in the next
generation if you if you you know
deprioritize that piece of code and
prioritize something else you actually
go and turn this one back into software
in the sense that you'll run it in code
and and synthesize the next one so you
can kind of move back and forth
automatically in different
implementations between hardware and
software and as kernels get pinned it
gets much more efficient now in terms of
data movement which as you remember was
the fort Rozz go ahead it's actually
global it's across as many blocks as you
have in the window okay now that being
said so that's sorry the question was
how global is you're out of order pneus
that's actually an implementation choice
because you could decide to save energy
to enforce sequential instruction
semantics between blocks and run blocks
in order or you might map a bunch of
them and then pre issue the loads but
then wait until the last block was
finished before moving on to the next
that would actually simplify some of the
logic at the load/store queue interface
and save you some energy so that's
really a policy decision but in general
I think if you if you want a big window
and to get that latency
tolerance we don't actually add a lot of
energy overhead to do that there's some
at the load/store queue that actually
buys you a fair amount of performance if
you just want to run block sequentially
you would want a much smaller core that
only runs one block at a time you and
then you have to do renaming between the
register edges I mean any intra block
values so the question was you need to
do renaming between blocks and that's
absolutely right any inch interblock
operands or Val or data flow arcs
whether the through registers or through
memory need to be treated the same way
they are in a conventional out of order
core with some associate of logic the
good news is that that number is
actually much smaller than you'll
typically see because if you have a
block of 64 instructions there aren't
that many temporaries going going from
one to the other you know so you don't
get rid of it but you you you take it
down you know by a significant fraction
it's a great question thank you so in
terms of data movement now we think
about how on a silicon substrate that we
want to be highly energy-efficient we
can make things more efficient so we all
know that you can block things and
that's actually very useful this is what
you know Mark is talking about next door
as a way to try to block things for
synthesizing accelerators more
automatically certainly you can stream
things through and the idea that you you
you lay out a series of kernels and then
one fords directly dunno there's been a
lot of this at Berkeley and Stanford and
MIT and other places over the years
stream it things like that you know
systolic arrays are very synchronous
special case of that where you're
mapping out graphs of accelerators that
communicate point-to-point Joe Lemer and
his group had a really nice paper last
year called triggered instructions that
tries to take a tiled architecture and
turn it into and have that sort of
streaming behavior you know where tiles
you know have generalized instruction
graphs which are a little bit like ours
and they will send you know operands
through five phones directly to other
tiles bypassing the caches and they
showed an order of magnitude increase in
efficiency over fairly efficient cores
by doing that and then of course you can
tackle this technologically like
bringing dram in package and that's
there's a lot of work going on there
that i think is going to take off I mean
we're definitely looking at in package
DRAM and you know right now
you think about those two you can say
milliwatts per megabytes per second or
pico joules per bit how much energy does
it take to support a given rate of
bandwidth you know we we tend to think
in milliwatts per megabytes per second
you know ddr3 is several hundred 300 to
400 and you know 40 is sort of the limit
bounded by the the energy digit access
the DRAM row but there are solutions
that are you bringing it down to 120 and
80 by putting it in package and they
look very very efficient so that'll help
a lot and and of course you can
eliminate memory memory accesses with
techniques like you know the streaming
people do with triggered instructions so
we expect to see those come down but
that's the the fourth category of fat
we're working very hard to cut out and
then as an example you know going back
to the cloud in our big accelerator that
we we built on on our fpga fabric you
know in the front end of the pipeline
extracts features from a document and
there are thousands of features that you
extract and so to get to make this very
energy efficient and high performance on
an fpga what we did is we just built
this large array of finite state
machines each one in which its tracks a
feature of families you have a document
coming in over PCIe and then we actually
you know turn that into a stream of
tokens that's just a conversion and then
we broadcast that stream of tokens to
every of these fsms in parallel and
they're all in parallel extracting you
know the their features from that flow
so this is an example of a you know one
of these graphs of kernels where you
know there's there's no explicit data
dependence from form of the other it's
just a broadcast network but it's one
topology you can do many other things
like this and this saves enormous energy
because you're only doing the data
movement once and extracting thousands
of features were in software you're just
doing it over and over and over again so
even the CPU even though the CPU is less
efficient to begin with you're doing
much more computation okay and I'm out
of time so I was going to talk a little
bit about how you know some of you saw
hotties talk but by adding a little bit
of error and approximation you know you
can actually then trance do algorithmic
transformations on the program and the
recent work we've done on NP use will
transform a general code segment into a
neural network
and that allows you to run on a it takes
diverse code and then transforms it into
a fixed representation ie neural network
that you can run on accelerators and we
see very large energy efficiency gains
by doing that so for this is the range
of applications that we transformed in
the paper and it's fairly diverse and
you know the results in terms of the
quality of the output we're good in all
of the cases that the you know
perception didn't say this is much worse
than this and then when we did a digital
version which was in micro 2012 we saw
six point nine times energy delay gain
and then iska this year we had an analog
mixed-signal analog neurons and mixed
signal circuit that actually bumped that
up to about a 23 x and needle a game and
so you know i think this is another good
good way to think about for ultra low
power can you actually transform the
code into something that will run well
on an accelerator that might be gentle
might not be general purpose but which
will be much more energy efficient and
the npu actually has a lot of the
characteristics that we talked about
earlier it eliminates all of the front
end you know code because you're turning
a code stream into a fixed invocation of
a neural network where the network is
already embedded on the hardware so you
get rid of most of the instruction fetch
and decode energy you know the out of
order energy is sort of gone because
you're just you have a highly parallel
thing so you're you know you don't need
out of order issue to get good
parallelism and and so it cuts out
several of those and the data movement
you know is largely local so it makes
that pretty efficient okay so i think
you know given what's happening with
Moore's law and what we're going to get
out of the process which is basically
nothing we're going to need much more
efficient execution and we see
tremendous you all know very well we see
tremendous pressure in the wearables and
sensor spaces we just need enormous
amounts more energy efficiency to do the
sorts of things that we want to do
especially in wearables but we're not
going to get much more for process
scaling and if you're willing to run
very slow there's a lot of stuff you can
do like your thresholds computing but a
lot of the cases in a lot of the cases
we need much more performance ok so i
think in addition to all the good break
we're going on in energy harvesting and
circuits we need much more efficient
architectures that are still somewhat
programmable and of course will be
do you to be a lot of asic work and I
expect that that will increase as the
tools get better but you know that's
expensive and slow and things change so
we still need still need programmable
things and then you know I said I said
here the Holy Grails programmable cg Ras
means uh is more like you know core
screen reconfigurable architectures or
hardened al use with some configurable
fabric and those haven't really taken
off they're really hard no one has
really nailed that yet but many of the
things that I was describing you know
the triggered instructions the stuff
we're doing with e2 they have a cg ra
like feel and the goal is really can you
get it down so what you're really just
doing is running code at high
performance with good utilization of
your al use but just beating through
these al used in spending the energy
there rather than all in all the
bookkeeping logic the control logic the
front-end logic and so I think whoever
really nails this is going to have a
great great run in this deeply embedded
ultra efficient energy space and we're
taking one crack at it other peoples are
taking others and so the two directions
we've really looked at our e to internal
transformation so that's my talk thank
you very much for attending I'm happy to
take a few questions yep
or 14 I think that's exactly what the
question was do we see a time when will
when we're at that last node where the
cost of basic gets reduced over time I
think that's exactly what we'll see
it'll be a commodity and the tools will
just get better and better the companies
that support doing that will proliferate
the subject but it will be slow right
it's really hard the design rules are
growing exponentially as you know right
so we have to turn that around and bring
them down or at least find a way to hide
them you know so so we'll just see a
slow declination over time and so 10
years from now will be much better than
it is today every year we'll probably
get a little bit better but it's not
going to be a fast you know I think
we're I think we're at an aider and I
think once that technology stabilizes
and people are really desperate for
performance I think architecture will
see your Renaissance because we have to
do things different and so a lot of
things that were previously unthinkable
or unnatural will become natural
unthinkable and I think you'll start to
see more are a sick startups coming back
in fact I'm already starting to see see
that move a little bit so yep Brendan
yep so the question was it's a great
question will we have to change the
programming models to exploit these new
architectures that will come to mine out
those those things that I talked about
as sources of overhead I think what
you'll see is a little bit of both so
there's certainly there's there's
already a lot of work in domain specific
areas and we see we see it in catapult
you see it elsewhere where there's a
class of things that you're interested
in and you're willing to have a more
restricted language or special language
and those will proliferate I don't think
is any question about that I think that
as language the language community
doesn't for obvious reasons doesn't like
to target weird architectures that
aren't in widespread use just like the
architecture community really doesn't
like to plan on devices that aren't
really ready for primetime yet I mean
people write papers but you don't know
what's going to happen so I think we're
likely to see that happen from the come
from the bottom up we'll come up with
new architectures will be program them
by hand and twist and turn and contort
to be as productive as possible and then
that will go up to the compiler and
eventually the language and the
languages may shift a little bit right I
don't think we'll be jumping too radical
languages that looked nothing like we
have today they will be looking to keep
them all fairly similar but we may you
know we may evolve away and I know who
might take a big step function to
functional languages the only way of
doing this but I would be surprised if
that happened
sure alright the question is are we
going to have a clean abstraction or a
mess I think we're going to start with a
mess and we're going to just work to
evolve it to be clean over time I mean
one reason I've been willing to stick
with the e2 stuff for so long as I see I
see lots of ways that we could take many
different programming models and
actually map them onto the same
substrate so I think we could actually
rather having a heterogeneous mess you
actually have a fairly homogenous
substrate that does pretty well on most
of the things you know so I've been
willing to continue that investment
because I think that would be really
good if we can pull it off but you know
we don't know but I think initially like
what we see in catapult as we're
building a lot of these services is
you're doing a lot of it by hand you're
using you're grabbing different things
you know it's it's kind of a mess and
you're just trying to get through it and
then and then we want to evolve towards
cleaner models so we're in for a messy
time no question thank you
that's right
yep yep tiny yep yes
you
right
so so so there's a role for customizing
through and let me summarize the
question you know what what are the
general lessons we learn from the client
and cloud work okay put a short version
of it and and just for the the camera so
for for the client work I think the rule
is that programs are intrinsically data
flow graphs with with uncertainty about
edges and we do a very poor job in
mapping those out efficiently I don't
just mean within one of our instruction
blocks I mean across a substrate and
then and of course there's recurrences
and so the data flow graphs also capture
instruction movement you know Joel's
work Joe lemmers recent work is a really
different take on how to manage those
graphs when you can contain a whole
kernel on within the silicon when the
whole thing is mapped so you don't need
to do you know cash I cache misses right
and so I think we're going to I think
the general rules we should be thinking
about these things as graphs and how to
layout the graphs on hardware that says
homogeneous as possible and we might
have to give up on some of our
programming model abstractions like do
you want you know like terrazas question
do you want out of order across blocks
you know so by weakening or
strengthening those you can make your
hardware more or less complex but we
really want to think about substrates
that can map graphs to minimize all the
overheads as well as data movement okay
I mean we haven't we haven't gone very
far into the data movement and explicit
communication between load and store
space that's something that Joel jumped
out ahead on it we're thinking very hard
about on the catapult side on the cloud
I don't think it's I think the projects
still too young to have any strong
conclusions I mean parallelism is good
reuses good data movements bad I mean
it's kind of the obvious stuff but I
haven't I don't think you might ask
Andrew Eric or Adrian or others on my
team but I don't I don't have yet a you
we're still building the services and
it's still early in the architecture is
shifting very fast
yep
can I can I answer that then okay so I
think you I think you do two things you
try to provide a more efficient
architecture without changing the stack
too much that's your that's your auntie
right that's that's what gets you in the
door but then you want to provide a
bunch of opportunities so that people
can mine out more efficiency by then
disrupting the stack all the way up so
that's exactly the strategy we've taken
with the client work let's try to make
things more efficient but not really
mess up the stack but then now I see all
these opportunities for doing direct you
know produce your consumer communication
mapping whole kernels and pinning them
things like that and and that that will
change the stack but we haven't pushed
down very far because I don't think
we've gotten the buy-in to the point
that I'd like it
yep
right thank you thank you for the
comment thanks thanks thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>