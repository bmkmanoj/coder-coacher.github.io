<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Extracting Knowledge from Informal Text | Coder Coacher - Coaching Coders</title><meta content="Extracting Knowledge from Informal Text - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Extracting Knowledge from Informal Text</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/juc49_N_Sfo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so I'm really thrilled today to welcome
Alan Ritter who is visiting from all the
way across the lake of University of
Washington where he's a student with
Oren Etzioni he'll be graduating this
June I think with his PhD Alan is well
known to many of you from his two since
here as an intern first in the machine
learning group back in 2008 working with
Simon Basu and then again in 2009 in the
NLP group here with us
Alan's work with sumit in 2008 won best
student paper award this was working on
interactive machine learning techniques
at IUI Alan's work is focused as its
thesis work is really focused at the
intersection of natural language
processing social media machine learning
and he's got some incredibly cool
results to show you I think today he has
a Twitter specific toolkit he's worked a
lot with with big raw nosey data like
Twitter trying to extract signal from at
trying to extract structured information
that can be used to enable new
applications and his toolkit has been
used by many people he's got it
distributed on the web I'll turn it over
to Ellen and let him talk about his
exciting research agenda as he finishes
up Thanks all right yeah thanks Bill um
yeah I'm really excited to be here and
please feel free to ask questions okay
so the Internet's really changed the way
that people communicate and this has led
to a huge amount of informal text that's
available in electronic format so this
includes things like email SMS messages
Twitter and people are also writing
informal texts and professional
environments for example physicians
notes and military field reports okay so
from my perspective the really exciting
thing about informal text as compared to
formal text say books or newspaper
articles is that the amount of it that's
written each day is simply much larger
so just to make this a little bit more
concrete if we're to look specifically
at Twitter if we take all the tweets
that are written in just
des and print them into books we'd have
a pile of books that's as high as some
of the tallest buildings in the world
and so clearly no person can read
through all this text and this is really
why we need some sort of automatic text
processing techniques to extract and
aggregate and organize this information
and so there is actually a lot of
important information that shows up
first on Twitter so one famous example
of this is this Twitter user who
happened to be in a bot Abad and live
tweeted the raid that killed Osama bin
Laden and so of course this user really
didn't know what was going on but it
turns out that also the first news that
Osama bin Laden had been killed showed
up on Twitter so there was this guy who
was a high-ranking official at the
Pentagon at the time who leaked the
information on Twitter before it showed
up in the press so people were talking
about it on Twitter first okay so of
course there's been a ton of previous
work on natural language processing and
information extraction which is focused
on on processing news articles and I
think this actually makes a lot of sense
because historically news has really
been the best source of information on
current events and current events are a
really good application area for
information extraction so if our goal is
just to extract some historical
information or kind of encyclopedic
knowledge it's really difficult to
compete with these structured data
sources like Wikipedia and freebase but
news is also challenging for NLP
applications for other reasons and part
because it's just already pretty well
organized to begin with it's it's not
that hard to just sit down with the
newspaper and get a good overall view of
what's going on so in the meantime
social media has become a really
important competing source of
information on current events and so the
status message is people are writing on
these social networking websites have
very different characteristics from
traditional news articles so they're
short they're easy for anyone to write
and they're instantly and widely
distributed so because of these reasons
they often contain fresher information
on a wider variety of topics than news
articles but of course this lowering of
the barrier to publication is kind of a
double-edged sword so because these
messages are so easy to rate we get a
lot
irrelevant information people talking
about what they ate for breakfast and
there's also a lot of redundancy so we
get many people all talking about the
same thing
and again this leads to a situation of
information overload and motivates the
you know why we need automatic text
processing techniques to extract and
aggregate information from this big
noisy text data set okay so when we look
at applying NLP and information
extraction techniques to Twitter there's
a number of challenges that come up so
for instance there's a huge amount of
lexical variation so Twitter users are
really creative in their use of spelling
and abbreviations and just to give an
example of this I ran a distributional
word clustering algorithm on a large
corpus of tweets and so here you can see
there's over 50 different ways that
Twitter users can refer to the word
tomorrow okay so secondly tweets have
really unreliable capitalization yeah
you know that's a good question so we're
basically just looking at the the
context the words co-occur in so they
tend to occur in similar contexts that's
what the distributional clustering
algorithm tells us but I think these
these look pretty reasonable yeah right
so in terms of capitalization you know
users will capitalize words just for
emphasis or they'll often leave the
whole message lowercase and this is
pretty challenging for this named entity
recognition task you know which is a
standard NLP task and for information
extraction you know it you know
traditionally for news articles at least
named entity recognition relies heavily
on on capitalization which isn't as
reliable here and then finally tweets
tend to have a unique grammar so users
will often drop personal pronouns for
example assuming that the subject of the
spent sentence refers to the speaker and
you just don't see these same kind of
sentences and news articles okay so you
might be wondering at this point you
know how would you off-the-shelf and LP
tools do when we apply them to Twitter
so I'm just gonna walk through kind of a
standard NLP pipeline here going through
this example and show where some errors
come up so first of all the part of
speech tagger thinks that this word yes
is at
renowned which is a pretty big mistake
and you can kind of see why it's you
know capitalized and it's this funny
spelling so it's probably out of
vocabulary then the chunker miss
segments it's official Nintendo is a
noun phrase which is another big mistake
and the named entity recognizer miss
segments America is a location whereas
if you look carefully you'll notice it
should have really been North America
and so I'm not even highlighting all the
mistakes here but the the point is is
that Twitter has this noisy and unique
style which these tools designed to work
on grammatical text we're just never
meant to handle oh so this is a state of
the art and like off-the-shelf taggers
so these are from the UI EC group
actually yes we also great so of course
yes I mean to deal with this we've
rebuilt an NLP pipeline which is trained
on in domain Twitter data and so you
know the main approach we're taking here
is a supervised learning approach so I
basically just went and annotated a
bunch of tweets with part of speech tags
Shella parser chunk tags named entities
and events we're also using some
semi-supervised techniques for example
using these unsupervised word clusters
as features and you know I think there's
a lot of interesting work on
unsupervised learning for syntax but
we're really trying to take a practical
approach here and get something working
on Twitter so that's where why we're
taking this supervised de proach so I
think there's actually a lot of room for
interesting work and unsupervised
learning for these more semantic level
tasks and I'm going to talk about these
later in the talk
so we've done some work on you know
named any categorization classifying the
events and also unsupervised relation
extraction okay so here I'm showing the
performance of our NLP tools the the
shallow syntactic annotation tools
compared against off-the-shelf tools
which are tuned to work on on newswire
text and you can see that in each case
we're doing
much better than the off-the-shelf tools
we've made these tools available on
github so you can go download them and
use them if you're interested and we've
actually found that there's a relatively
large number of people that are finding
them useful okay so given that we have
access to these tools which are tuned to
work on this noisy in domain Twitter
data a natural question is you know what
can we actually do with them
so to give one answer to that I've built
a system which is automatically
extracting a calendar of popular events
coming up in the near future so to do
that we're continuously processing a
stream of about two million tweets per
day running our NLP tools on them and
extracting for example named entities
and events in addition we're also
extracting and resolving temporal
expressions so for instance if we see a
phrase like next Friday we can actually
figure out the calendar day that's
referring to so then we can just count
the number of times that each named
entity co-occurs with a reference to
each date and use a statistical test to
determine whether there's a strong
association there and plot the most
strongly associated events on a calendar
okay and so I also want to highlight
briefly here there's a number of systems
building issues that come up when we try
to do this so you know because we're
processing so much data here we found
that just using a standard relational
database just couldn't keep up with the
number of inserts instead we had to move
to using this distributed no sequel
database okay so I'll go ahead and show
a demo of this and this is available
online I'd invite you to go check it out
so today is you know April 2nd so
looking ahead to Sunday you can see
there's a lot of people mentioning
something about the FAA we can click on
that to drill down and get some more
detail and so here I'm just showing all
the tweets that mention FAA in addition
to April 7th and you can see that
they're basically shutting down 173 air
traffic control towers this Sunday and
of course this is due to the the
sequestration so looking at a bit
further ahead
you can see that next week on Saturday
the 13th Mubarak who's the former
president of Egypt has a retrial
scheduled and then on the 14th Venezuela
has new presidential elections and so
there's a lot of other stuff on here as
well I'd invite you to go and kind of
yeah it's it's from all the categories
so we do do some language identification
to filter out only the the English
tweets but yeah so this is kind of open
domain it's on kind of all different
types of events and sources yeah very
rate so this is these are these event
words that were extracting so we've
annotated some data to train an event
event event phrase extractor so this is
kind of similar to the time bank corpus
if you're familiar with so we're doing
sort of a similar extraction task on on
Twitter yeah so that's the statistical
test right so we basically look at like
how frequent is the entity and how
frequent is the date and then how often
do they co-occur together right so if we
just go straight directly by frequency
then like Justin Bieber would be the
most frequent thing for every day
basically right yeah so that kind of
comes out naturally from the statistical
test so there has to be like something
that's happening really strongly
associated with this day so if people
mention what they're eating for
breakfast every day
it'll have to you'll have to see it
mentioned very frequently to kind of
overcome the you know the baseline right
okay
all right so I think this just provides
some motivation for why we want to do
information extraction and natural
language processing in Twitter but so we
were actually pretty surprised to see
that this works so well so if you try to
do something similar in with newspaper
articles it's actually a pretty
difficult task so just to kind of
explain why that is for example in this
instance to figure out when the bomb
attack mentioned in the first sentence
takes place we have to recognize that
the blasts mentioned in the second
sentence is referring to the same event
and also that the blast takes place on
Saturday and then to make things even
more complicated we have to further
recognize that this other bomb attack
which is mentioned later in the article
is referring to a totally separate event
which happened on a different date so in
order to kind of link together all the
information and news articles we have to
solve these discourse level processing
tasks which link information together
across sentences and these are some of
the more difficult and I would say
unsolved problems in NLP there's a lot
of interesting research going on in this
area but we don't really know how to
solve these problems in the same way we
know how to take an individual sentence
and process it so in contrast tweets
tend to have really simple discourse
structure so users on Twitter sort of
say things and really straightforward
and compact ways and to kind of
understand why this is if you imagine a
user writing a message on Twitter they
typically assume that it's going to get
mixed up in the feeds of all their
followers and so they don't assume any
context that it's going to be understood
in and contrast the sentence in a news
article is really meant to be understood
within the discourse context of the
article okay and so the point to take
away here is that by working with these
short and formal messages on Twitter
we're able to sidestep some of these
complicated discourse issues okay so
given that we can do a pretty good job
of extracting open domain events from
Twitter a natural question for us was
whether we can categorize them into
high-level types for example sports
events political events product releases
and so on and so this would have a
number of benefits probably the most
obvious thing here is it would allow
users to browse more customized
calendars which match their interests
so there's a number of challenges that
come up when we look at categorizing
events on Twitter so the main thing is
that there's just a huge number of
different types of events that people
can talk about and in advance we're
really not even sure what the right set
of tight right sort of event categories
is so furthermore the set of types
important types might actually change
over time as new topics become more or
less important or if we want to focus in
on a specific group of users there might
be a different set of categories which
best describes the data okay so to
address these challenges we're proposing
an unsupervised approach to event type
induction which is based on on topic
models and this is actually based on
some work we've done on modeling
selectional preferences with topic
models and so this approach has a number
of advantages it allows us to
automatically discover an appropriate
set of event types which match the data
we don't need to annotate any individual
events in context and we don't need to
commit to a specific set of event types
in advance like we'd have to do before
annotating data okay so how these
generative probabilistic models work is
we first make up a story about how our
data was generated which involves hidden
variables and probabilities and then
given a fixed data set we're going to
apply Bayesian inference techniques to
infer values for the hidden variables
which will then tell us the category of
each event in context so I'm just gonna
walk through a really high-level
explanation of the generative story for
our model here just to kind of make it
clear what's going on so we'll start out
by grouping together all of the tweets
which mentioned the same event phrase so
in this case all the tweets mentioned
the word announced okay and so then
we're gonna have a set of event types so
each type will have an Associated
probability distribution over named
entities so for example for product
releases we might see entities like
Microsoft Samsung and iPhone and then
similarly we'll have a type for you know
politics and sports so then each event
phrase will have a distribution over
these types for example you can see that
an announcement could be part of a
product
release or a political event or a
sporting event and then to generate the
named entities and our data will just
repeatedly draw types from this
distribution and then generate the named
entities and our data based on the you
know from The Associated types okay so
this just oh yeah by the event so you
yeah so right so one of the part of the
NLP tools that I kind of glossed over a
little bit was extracting phrases which
refer to events right
so we basically annotated data using the
same guidelines as like the time bank
corpus and then so it's like you know
the great so it could be like verbs or
nouns can refer to events you know you
could have sort of like attack or you
know attacked right but are they free
are they prescribed event types or are
you clustering to create event types
yeah we're clustering to create the
event types right yeah okay anyway so
this just kind of describes the the
generative story here in practice what
we'll actually do is to apply Bayesian
inference techniques which will then
give us reasonable values for the hidden
variables which will then tell us what
the types are and also tell us the type
of each event in context okay so we
gathered about 65 million of these
entity event date tuples which are
basically the same thing we're showing
on the calendar and for inference for
using collapsed Gibbs sampling which is
actually a pretty standard approach to
inference in these types of models in
practice we actually use this
parallelized approach to Gibbs sampling
so I should mention that Gibbs sampling
is really an inherently sequential
procedure but there is some theory that
explains that the the parallelization
can be understood as an approximation to
the sequential sampling and we actually
found this to work really well in
practice and it what's the scale up to
much larger data sets
you have change yeah yeah you run a
separate chain on each machine and then
at the end of each iteration basically
sink you know synchronize the counts
yeah that's a good question so that's
basically right so the the runtime here
is sort of like order of the size of the
corpus times the number of types so this
is kind of as many as we could do in a
reasonable amount of time yeah so of
course there's you know there's work on
like nonparametric models that try to
find the right set of types I think for
these if we're just trying to find event
types that the number of types kind of
doesn't matter that much if you're
looking at something like coreference
resolution then getting the right number
of types is really important and maybe
the nonparametric approaches or more
more appropriate in that situation but
this part you mean yeah right totally
yes so this is I've kind of gave a
little bit of a simplification there so
basically right so here we have the the
for each type it has a distribution over
named entities and then also a
distribution over dates on which events
about type occur and then the theta up
there is the the distribution over over
types for each event phrase right if
that makes sense so maybe I can use the
laser pointer be a little easier to yeah
so basically it's right so these hidden
variables here when we if we infer
values for these these will basically
tell us the category of each event in
context right and it'll also tell us you
know from if we just can basically in
the inference we're you know we do this
collapsing so we integrate out these
parameters and then also these
parameters so all we're really doing is
is inferring values for these which then
you can kind of read off you know what
are the
yeah so the observations are basically
the event phrases which we've so we're
using a linear chain CRF to extract
event phrases and then also to extract
named entities so this is kind of on top
of that oh right yes so these are the
dates yes so this rate so basically
right so we also extract and resolve
these temporal expressions so I kind of
I'm skipping this in in this kind of
higher-level story but so basically each
each type of event has a distribution
over dates on which events that that
type happen to so this kind of helps to
group together tweets which are
referring to the same event which of
course have the same type of servation
really just not the work is destructible
exactly yeah
so yeah so we just went with just days
right so you could imagine someone says
like Oh at 8 o'clock on Thursday I'm
gonna do this but for the Twitter data
it seems like most events that people
are talking about how just they just
give you the specific date on which it
happened
yeah or just all more than 365 so I mean
it could be it in any time yes so this
right so this is a little bit
counterintuitive and we tried doing it
with and without this but basically the
effect that ends up having is grouping
together events that happen on the same
day so maybe on this day there's like a
really big yeah I'm doing it separately
I think it would be interesting for
future work to look at a joint model
here so I think the the advantage of
this approach is that the inference
remains pretty simple and so because
we're working with a lot of data it lets
us scale up to it to a lot of data I
mean I I think that it's it's
interesting for future work to try and
do an end-to-end joint approach that and
you make the model and you infer the
distributions of entities and whatever
over the event types or the clustering
happy during the yeah the clustering is
happening during the inference yeah yes
yeah so basically right so basically we
just find for each named entity sort of
like what event type does it have and
then at the end we can kind of read off
the clusters from that right you are
limiting it to like to a hundred so
there's somewhere in the inference and
learning process for it sorry is new
clustering sort of on every it's doing
like an iterative
yeah yeah right so basically the Gibbs
sampling procedure how it works is we
basically go through all the data and
then sample a new for each hidden
variable we just sample it go you know
as we're going through the data resample
a new value for it and then yeah do that
through the whole data a couple times
basically or you know a thousand times
yeah okay all right okay so anyway so
these are some of the the event types
that are automatically inferred by our
model and I think these look pretty good
so for example we have like a sports
type here where we see event words like
tailgate scrimmage tailgating homecoming
and regular season and then entities
like ESPN NC double-a Tigers and Eagles
oh and it should mention by the way that
these labels are just kind of my
interpretation of what the events are
these aren't automatically generated
then we also get a nice TV event here
where we see event words like you know
new season season finale new episode and
then we're seeing some TV shows like
Jersey Shore True Blood and Glee and
also TV networks like HBO ok and so we
also did an evaluation where we looked
at the ability of our model to actually
categorize events in context and so I
manually annotated some data with the
event types which were automatically
discovered by our model and so here
we're comparing against supervised
classifier as a baseline and basically
the point here is that by using large
amounts of unlabeled data we're able to
do better than the supervised baseline
to prevent cookie because these were the
infrared categories yes you're trying to
make your own interpretation yeah yeah
so basically we we run inference in the
model it'll give us it'll automatically
infer some categories right and then
basically I'll use those same categories
that the model kind of automatically
inferred to annotate some separate data
if that makes sense so how well you
understood the underlying categories
that referred yeah that's true it's a
little bit I mean I'll agree that it's a
little bit it's a right it's it's a
little bit weird I mean I see your point
but I mean it's I think there's some
advantage to like just automatically
finding the right set of categories for
the data and I mean I think right so
that's a right so I mean I think it's
kind of it is a little bit odd I'll say
to say like oh we have this unsupervised
model that's doing better and I think
part of the reason why people have you
know found that unsupervised that
supervised models tend to do better is
because that what the unsupervised model
is finding doesn't really match up with
your ideas so this is kind of like I'm
gonna like let the unsupervised model
find something and then all like use
that to annotate the data with yeah
that's basically what's going on here
okay um right so this unsupervised
approach to information extraction has a
number of advantages so it lets us scale
up to large unlabeled data sets we don't
need to specify the the right event
categories in advance but I think
there's an interesting question here
which is you know what to do in the
situation where we have access to large
amounts of structured data for example
from you know freebase or Wikipedia and
this is actually the case in this next
task that I'm going to talk about which
is named entity categorization so here
it's it's pretty easy to get large lists
of named entities and their types from
these structured data sources okay so
there's a number of challenges that come
up in named entity categorization and
Twitter so there's a huge number of
different types of named entities that
people are talking about they're talking
about you know bands movies products and
so on and many of these are going to be
relatively infrequent in the data so
even in a really large manually
annotated data set there's going to be
few examples of these infrequent
categories so because of this I think we
can't simply rely alone on unsupervised
learning alone here okay so the second
thing that's challenging is that tweets
are very terse often so for example in
this instance it's really hard to know
what type of entity kkt and why is
referring to without some additional
background information okay
so to address these challenges we're
proposing a weekly supervised approach
to named entity categorization which
uses large lists of named entities and
their types gathered from freebase as a
distant source of supervision okay so of
course we can't simply just look up a
named entity to figure out its type in
context so for instance if we look up
China in freebase we see that it could
refer to either a country there's also a
band called China there's a number of
different people whose name happens to
be China so we need some way to
disambiguate between these different
possibilities okay so to do that we're
proposing a new approach to distant
supervision which is based on
constrained topic models and so like I
mentioned or like I kind of alluded to
in the previous slide just applying
distance supervision direct
to this task doesn't work because the
the training data is just too ambiguous
so instead we're proposing a latent
variable model for named entity
categorization which uses the freebase
dictionaries as constraints in the model
okay I'll try and make this a little bit
more clear on the next slide so again
here I'm kind of showing the high level
version of the the generative story for
our data so in this case we're grouping
together all the tweets which mentioned
the same named entity and then each
entity type has a distribution over
context words which co-occur with
mentions of that entity in context ok so
right so the key difference here is that
these these type distributions for each
entity or constrained based on the
freebase dictionaries so for instance if
we look up JFK in freebase we might see
that it could refer to either a person
or an airport and then we'll constrain
its possible distribution over types
based on these possibilities okay and
then like before will repeatedly draw
types from this distribution and then to
generate the context words and our data
will pick them from the associated
entity type type distributions ok and so
again this is just a description of the
generative story in practice we apply a
Bayesian inference techniques and four
values for the hidden variables which
then tell us the category of each named
entity in context so basically right so
we just use the the previous and
following three words and then we also
use the words in the entity as well
right okay so here I'm showing some
example type lists which are
automatically generated by this model
and this is just for three out of the
ten types that we're working with and
these I should mention these are also
some of the more difficult types so the
easy things are like person and location
but I think these actually look pretty
good so for example rule rate so this is
basically these are the top 20 entities
which weren't found in any of the
freebase dictionaries so these are like
words that our model was able to
categorize automatically so for products
we're seeing things like Nintendo DS
late
well iPod there's some segmentation
errors in here as well but you know iPod
Nano and so on but I think what's really
cool here is that we're able to
correctly categorize some of these
Twitter specific abbreviations which you
just wouldn't expect to find in freebase
oh yeah these are so these are TV shows
I don't I don't know I hear about all
this stuff I don't I don't actually
watch these shows though yeah that's
part of the fun of working with Twitter
data okay anyways so we also looked at
how well our model can actually
categorize named entities and contexts
so I annotated a large corpus of tweets
with named entities and their types and
here I'm showing our performance
compared against a bunch of baselines
including a supervised baseline which
actually does really well on the more
frequent types like person and location
but does poorly on these infrequent
types where there's few examples in the
training data we also compared against
the code training approach to weekly
supervised named entity categorization
proposed by Collins and singer and you
can see that we're actually doing quite
a bit better here okay so why is it that
Lda is winning in this case so I think
there's a couple of reasons for this so
the first is that it's able to share
information about an entity's type
across mentions in a really nice way so
these so basically we can you know
figure out the right type of the entity
in these highly ambiguous cases by
looking at the same entity and in other
contexts so the other thing is that you
know because we're using these freebase
dictionaries as constraints in the model
we're just better able to take advantage
of this highly ambiguous training data
so we don't have to just rely on these
unambiguous cases to learn how to
categorize the entities okay yeah
fine what did you do for that yes so I
annotated about 2,400 tweets with named
entities and their types and we
basically just use like a maximum
entropy classifier did it use freebase
yeah no I think it didn't look at
freebase at all so we did also have a
freebase baseline where we basically
look up the entity and freebase and if
it's unambiguous you know make that
prediction and this actually has a
really high precision but the recall is
pretty low ok all right so yeah so I
just talked about this new approach to
distance supervision based on topic
models constrained topic models which is
appropriate for the situation where you
have highly ambiguous training data like
this named entity categorization task
and so there was a natural question that
came up while we were working on this
which is what happens when there's
missing information in either the text
or the database and so the answer is
this leads to errors in the training
data and this is a really general
approach eneral problem that affects
distance supervision both for this
weekly supervised named entity
recognition task I talked about and also
for the more common application of
distance supervision which is extracting
binary relations okay so for the sake of
comparison to previous work so here
we're looking at the case of binary
relations and so it turns out that most
of the work in relation extraction uses
a huge number of features which are
highly correlated and overlapping and so
these generative models that have been
talking about are kind of not a very
good fit for this data because they make
really strong independence assumptions
so instead at this point in the talk I'm
going to move on and talk about
conditionally training models okay so
this is kind of like what the the set up
looks like for the for extracting binary
relations using distance supervision so
we'll start out by
having a relation in this case the born
in relation and then freebase is going
to give us a large list of people in the
locations where they're born okay so
basically for each pair of entities here
or we can go and search through a large
text corpus and find all the sentences
which mention the entity pair so Barack
Obama and Honolulu in this case and then
we can basically just treat these as
positive examples of the born in
relation and extract features from these
sentences and train a supervised
classifier okay so this is great but the
problem is is that you know what happens
if there's some information missing from
freebase so in this case all of these
sentences function is now a negative
training example for the born in
relation which I think you can see as a
problem and this is actually a pretty
common scenario so there's actually a
lot of information missing from these
databases and that's kind of the whole
reason why we want to extract
information from text in the first place
okay
so before I get into the the solution of
how we're gonna deal with this I'd like
to just walk through a model for
distance supervision in the context of
extracting binary relations so we'll
start out with a pair of entities Barack
Obama and Honolulu in this case and then
we're gonna get to observe all of the
sentences which mention this entity pair
okay so now we're gonna have a
classifier which is going to predict for
each sentence what relation it mentions
between these pair of entities so unlike
the standard supervised learning setup
we're not gonna actually get to observe
these variables during training instead
we only get to see these aggregate level
variables which tell us which relations
hold between Barack Obama and Honolulu
and aggregate so the question here is
how do we relate these aggregate level
variables that we get to observe with
the hidden sentence level relation
mentioned variables and so one answer to
this question is a simple deterministic
oral function and so basically what this
is saying is if any of the sentences
mentioned that Barack Obama was born in
Honolulu
then this fact is true if none of the
sentences mentions it then it's false
okay and so we can then tune
parameters of this model by just
maximizing the conditional likelihood of
the observed facts and freebase
conditioned on the observed sentences
and the data okay so for learning here
we're taking an approach based on the
structured perceptron which is an
iterative gradient based update to the
weights in addition we're taking an
online approach to learning which just
means we update the parameters after
seeing each pair of entities rather than
going through all the data and doing
batch updates okay so this is what the
gradient looks like it's just the
difference between these two
expectations over the features and so in
practice these expectations are too
difficult to compute so instead we
approximate them with maximizations so
basically what's going on here is we
have two inference problems one where we
want to find the best assignment to the
sentence level hidden variables
conditioned on the observed sentences
and facts and freebase and in the other
case we just want to find the best
assignment to the sentence level hidden
variables given the current parameters
but ignoring freebase okay and so the
unconstrained inference problem is
totally trivial but the the constrained
problem is a little bit more complicated
but it turns out that it reduces to this
weighted edge cover problem which we can
solve exactly in polynesia
polynomial time so this works out pretty
nicely okay so there's two assumptions
that are being made here so if a fact
isn't in freebase we can't extract it
from any of the sentences whereas if a
fact is in freebase we have to extract
it from at least one sentence and so
these assumptions are good because they
help to drive the learning but in the
case of missing information and either
that the text or the database they lead
to errors in the training data okay so
how might we modify this model to more
gracefully handle the situation of
missing data so what we're proposing to
do here is to take these aggregate level
hidden variables and split them into two
parts one which represents whether a
fact is mentioned in the text and the
other which represents whether it's
mentioned in the database and so then
these factors between the two variables
gonna encourage but not require that
they agree with each other so now you
can see that the facts in freebase are
acting like soft constraints
whereas before they're like hard
constraints so for example now it's
possible to extract a fact that's not in
freebase if the local classifier is
highly confident but of course we're
gonna have to pay some penalty for doing
that okay so the learning is pretty
similar to before so the only difference
here is that we're now maximizing over
these additional aggregate level hidden
variables that we've introduced and it
turns out this doesn't make any
difference for the unconstrained
inference problem but the constraint
inference problem gets a little bit more
difficult so it no longer reduces to
this weighted edge cover problem in a
nice way like like we had before okay so
of course the the question here is how
are we going to solve this inference
problem so again the goal is to find the
best assignment to the sentence level
hidden variables conditioned on the
observed sentences and facts and
freebase and like I mentioned this is
just kind of an optimization problem
with with soft constraints okay so um
basically what we found here so great so
we looked at a couple different
approaches so we looked at some exact
inference approach approaches like a
star which are you know time and memory
intensive and so don't really scale up
to these really large data sets we're
working with but we found that a local
search almost always finds an optimal
solution so long as we use a carefully
chose instead of search operators that
are designed so that it doesn't get
stuck in a local maximum and so to
verify that we're finding optimal
solutions we can compare it against the
solution and found using a star and we
found that in over a hundred thousand
problems from our actual data we only
missed an optimal solution three times
using this approach and so this is nice
because it's fast and in memory
efficient and it almost always finds an
optimal solution okay
so of course the real question is how
does this affect the the performance in
terms of precision and recall and so the
answer is it actually makes a big
difference so here I'm showing precision
and recall curves on the sentence level
extraction task
comparing against human annotations from
the data and so the red curve here is
the the system which is using hard
constraints by simply relaxing those as
soft constraints and setting to handset
parameters in the model we're able to
get the black curve which is actually a
huge improvement and then by
incorporating some additional
information in the form of a missing
data model we're able to do even better
which is the the green curve here and so
I think people realize that these
distance supervision models are making
some bad assumptions about the data I
mean all models have to make assumptions
right but I don't think they realize
there's this much room for improvement
by better modeling the data and the this
distance supervision problem okay so I'd
like to just pause for a minute here and
summarize what I've talked about so far
so I presented an analysis of the
challenges in applying information
extraction too noisy text I talked about
our NLP tools we've adapted to Twitter
and these are available online you're
welcome to go and use them
I showed this demonstration of a system
I've built which is automatically
extracting a calendar of popular events
coming up in the near future and then I
presented three different probabilistic
models for unsupervised information
extraction one which is doing
unsupervised event categorization I also
talked about this new model for distance
supervision using topic models which is
appropriate for the case of highly
ambiguous training data and the nei also
showed this recent work we've been doing
on on modeling missing data and distant
supervision right so I'd like to spend
just a little bit of time mentioning
some other work that I've been doing
during my PhD so I've been collaborating
with some folks here at Microsoft
Research bill and Colin who's now at NRC
on modeling conversations in social
media so in addition to talking about
popular events users of these social
networking websites are having
conversations on a really large scale
and I think this opens up all kinds of
new opportunities for data-driven
conversation modeling so for example
we've done some work on unsupervised
modeling of dialogue acts and also
automatically generating responses to
Twitter status messages and so elaborate
just a little bit on the second point
so the approach we're taking here is
based on a statistical machine
translation so in machine translation
the task is given some foreign text we
want to translate it into English and in
order to learn a model to do this we
have access to large parallel corpora of
paired foreign and English sentences so
I think in some sense this
conversational task is is actually kind
of similar so you know given an
arbitrary user utterance we want to
generate an appropriate response to this
and to learn a model to do this we have
access to millions of naturally
occurring conversations from Twitter
okay and so at a high level how phrase
based translation works is you know
given an input sentence we first segment
it into phrases and then we translate
each phrase in the input into a phrase
in the response and so this is a little
bit different than the machine
translation case but you know so there's
potentially some reordering here and to
to find a good translation we want both
good translations at the phrase level
and they'll also a high score according
to a language model yeah that's a good
point yeah so conversation and
translation are two very different
problems so yeah yeah right so it's
right so we can't have kind of very deep
intellectual conversation it's basically
we're learning these kind of high
frequency response patterns like you
know if I see airport in the status
message maybe say safe flight you know
or like I translates as you or like you
know dinner or translates as young so
these aren't very like deep you know
kinds of conversations but yeah but
right so we have a demo of this
available online you can go play around
with so for example I'm feeling sick
translates this feel better soon and
this is like an best output so you can
see the other things
yeah and right so I think right so this
is kind of cute and all but I think
there are actually some interesting
applications here so one might be you
know conversationally aware predictive
text input or speech recognition so
assuming that a user just you know
assuming that your friend just sent you
a text message and you're typing a
response to it using some noisy input
mechanism I think we can actually do a
better job of predicting what you're
trying to type by taking the message
that you just received into account so
for instance if your friend you know
texted you saying I'm feeling sick we
should be able to do a pretty good job
of predicting how you might respond to
that without even seeing any input from
the user yeah that's a good question
right so I I mean right so the the hope
here is that by you know combining
information you know actually generating
a customized response we can do better
than or handle sort of a wider range of
different things I mean it's a you know
I haven't done this experiment rate so
it's it's hard to say I mean template
matching is probably a pretty strong
baseline for this for sure
exactly yeah yeah so you could probably
get like the sort of translation a lot s
out of this or something and combine
that with the you know lattice from you
know speech recognition or something
okay so then I've also been doing some
work recently in collaboration with
folks at New York University and
actually also with Bill
so but on paraphrasing between different
language styles so for instance you know
we've been looking at paraphrasing
Shakespeare's plays into modern English
and also you know modern English into
kind of a Shakespearean style so the
approach we're taking here is to we
basically found we've scraped a bunch of
modern translations of Shakespeare's
plays off the web which we can then use
as parallel text to build translation
models and so like for example one thing
we can do is paraphrase lines from
modern movies into a Shakespearean style
so for example if you will not be turned
you will be destroyed gets translated as
if you'll not be turned you'll be undone
and Father please help me is translate
it as father I pray you help me and so
there's a demo of this online as well
and again these are kind of some fun
examples but I think there are actual
applications here as well so one thing
would be educational applications so it
turns out there's only modern
translations for 17 out of the 38 plays
that Shakespeare wrote so by translating
these other plays into modern English
maybe we can actually make them more
available to to students and of course
there's a ton of other authors from the
same time period as well I think looking
at paraphrasing between language style
and other domains is also interesting so
for instance paraphrasing you know
technical documents into more easily
understandable English or paraphrasing
between you know noisy and formal texts
on Twitter and more formal texts okay so
for future work
one thing I'm interested in looking at
is extracting richer semantic
representations of events from
microblogs so you know I think people
have spent a lot of time working on
information extraction and news articles
but you know there's still a lot I think
there's still a lot of opportunity to
sort of
you know do a better job of extracting
events from Twitter so one problem here
is is sort of solving this event
reference resolution problem which is to
group together all of the tweets which
are referring to the same event so we
haven't really solved this yet so I
think the the representation that we're
currently extracting is nice because
it's open domain but I think there's
there's sort of more opportunity here to
you know extract a richer representation
and so also related to that is this
problem of schema discovery which is you
know given you know these automatically
discovered event categories can we
automatically cut it you know extract
schemas for them so for example for a
concert you know we might expect to see
like a music artist that's performing at
the concert also the venue in the city
where it's taking place and then ideally
we'd be able to autumn-like to
automatically extract and fill out these
templates and if we can do this
unsupervised then I think we can do it
in an open domain way which isn't
restricted to a specific type of an
event okay so a little bit more of a
longer-term agenda for future work here
I'd like to look at scaling up grounded
language acquisition to more realistic
and open world domains so I think we're
in a really exciting position right now
so we have access to all kinds of
real-time text in different languages
and in addition we have all kinds of
real-time sensor data so for example
real-time data about the weather you
know traffic you know seismographic data
and so I think there's an interesting
question here of whether we can link
events that people are talking about in
text with you know events that are
showing up in sensor data and they're
kind of temporally correlated with each
other to give us some kind of signal
there okay so one possible approach to
this would be to extend some of the
latent variable models that I've been
talking about to incorporate both
real-time text and sensor data and maybe
by doing this we can ground the meaning
of these distributional semantic
representations that we've been learning
in real-world sensor data at scale okay
so I'd like to wrap up and and just
thank my collaborators and thanks for
coming to my talk
yeah some I don't always like possibly
take
I want to know if this is a pipeline by
so they're definitely relations but a
deposit is that what this thing about
three days at their block duplication
Brennan to it so basically how important
it is to have improve your pattern
between
I was taken yeah I know there's some
some good points you're making there
right so I think right so in the
calendar at least we are kind of
exploiting a little bit this affective
redundancy so if we see lots of people
saying the same thing in different ways
we that helps to improve the precision
right yeah so rate so definitely I think
there's room for improving the
performance that part of speech tagging
and named entity recognition so the you
know one thing is just annotating more
data like you're saying some kind of
joint model that looks across different
tweets right is also really interesting
I think yeah you guys have been doing
some work in this direction too which i
think is great but yeah right I think I
guess my feeling is is that probably the
performance on these shallow syntactic
annotation tasks like part of speech
tagging and named entity recognition is
sort of always going to be lower on
Twitter than what we see in news
articles just because it's so diverse
and noisy right so it's kind of more
challenging from that aspect but I think
once we get past these noisy text issues
there's actually other things that
become easier raid because it has this
really simple discourse structure like I
mentioned so it's kind of just this
interesting domain with different
characteristics than and what people
have mostly focused on you know
yeah conversations that look like you
were focusing on sort of the stimulus
response pair conversations can go on
yeah so we did in a little bit different
of a context though so we also had some
work on unsupervised induction of
dialogue acts so these would be things
like you know trying to classify each
post is like it is it a question or an
answer status post and things like that
so yeah so there we're looking at the
sequence and kind of longer
conversations yeah for the response
generation task we just picked like the
first message in a conversation and then
the response to that just because that
kind of constrains the problem a little
bit right but yeah doing that in the
context of longer conversations I think
is interesting but we haven't looked at
that yet yeah eating breakfast example
having set a baseline across these high
frequency low frequency
yes we basically just count in our
corpus like for each entity how many
times has it been mentioned you know as
long as you know we have data for
basically right
so we basically count the number of
times each entity is mentioned and the
number of times each date is mentioned
and then we can look at the number of
tweets that mentioned both of them and
then just apply a standard statistical
test like you know chi-square we use
like a G test I mean ideally what you do
is Fisher's exact test but with the
amount of data we're working with that
it sort of it's it's you know there's
some like floating point overflow I
guess basically that happens that's a
good question yeah so we haven't really
I've just kept the same
I haven't reset it but you probably
could it might actually be worth doing I
haven't really looked into that though
good question for the calendar you mean
yeah yeah so there's I mean there's a
lot of segmentation errors to be totally
honest
so like movie names specifically are
really hard because they're often sort
of short phrases right so often you'll
see I mean gosh I'm having a hard time
like if you know how you have sort of
dumb and dumber or something as a movie
title rate you might just get like dumb
is an entity or something rate instead
of you know I mean it's they're really
hard to distinguish
yeah I mean it's so basically in that
case for example you would see just like
I mean just the name that you're
displaying would be incorrect but still
you can kind of click on it and drill
down and kind of see what happened yeah
I mean I should you know the calendar
application I think there's a lot of you
know I could have spent a lot more time
engineering this and making it better
it's just kind of a but there certainly
are some errors there for sure yeah
here's the so Cooter's a domain where
people do some amount of manuals hanging
with hashtags and things like that bring
it and on the one hand you could treat
those just another word which happens to
you know claps across a lot of things
but maybe you want to treat them
especially because there's some you know
content use your intent going into yeah
I know that's a great question I'm so I
haven't I mean I've just treated them
just like another word so far but I
think there is some definitely something
to get out of them for sure so I mean
you know some of them are right so in
some cases they're really useful and
they sort of you know really give you an
anchor on the event and some cases you
see things like hash I like bacon or
something like that right but so they're
I don't know I mean I think there's
definitely something interesting to be
done with the hash tags I haven't really
figured out what it is yet though I mean
I've seen some interesting work I'm
trying to segment them into words and
things like that right but yeah I mean
definitely like for conferences and
things you know it'll sort of like give
you a nice focused group of all the
tweets on this specific event I think in
some sense I think they're almost kind
of more useful for people just to you
know sometimes you're trying to extract
like a human readable label
yeah distribution amongst hashtag
totally tweeted they're not even a bit a
perfect coverage might be helpful to
people because they're kind of
engineered by people or even if you just
take all the hashtags that are taught
you know if you tell you I want to know
about this particular hashtag and then
just get all the tweets that mention
that and then you know one interesting
question is sort of how can you
summarize all the information that
people are talking about there in a
short easily readable way kind of if you
know great great to find other tweets
talking about the same event that aren't
with the hashtag yeah no that's a good
point</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>