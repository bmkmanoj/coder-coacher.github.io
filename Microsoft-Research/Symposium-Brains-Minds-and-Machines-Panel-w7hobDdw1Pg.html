<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Panel | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Panel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Panel</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w7hobDdw1Pg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um we have two new panelists basically
on namely Gary Marcus interest in osky
arm or we were lucky to get to
participate in a panel discussion and of
both of you um could you please just
like quickly introduce yourself and how
your work is relevant to the symposium
sure I mean you asked me to prepare
slides so that maybe it gives an answer
to that I'm CEO of the company that Sue
being Gary money mentioned on the first
day geometric intelligence which was
just launched and professor of
psychology and neuroscience at NYU and I
want to start if the animation would
work and terrible moment for it not to
with a email that I got from Jurgen
several of us got from jurgen
schmidhuber this morning which he more
or less repeated for demos he said it
seems to me that the last time
neuroscience contributed something
useful to air was many decades ago since
I've lost most of my voice I'll let you
read the rest but you get the idea the
argument is neuroscience isn't really
contributing to a I I think well there's
some room to quibble and I think dennis
made some good points and i would say
call david i bet a lot of other people
make good points in this symposium
earlier in the day I think Schmidt
hubris at least partly right I don't
think neurosciences so far carried its
weight in AI there were a lot of the
main borrowed ideas hierarchical feature
detection as may be better pointed out
in his email and that's over 50 years
old and a lot of the other things that
people are calling neuroscience ideas
are really ideas from cognitive
psychology they're also over 50 years
old about memory attention navigation
and so forth and so I think it's
actually hard to make a really sharp
argument though I hope Terry will
disagree with me because I'm a fan of
neuroscience but I think it's hard to
make a sharp argument that neurosciences
really fundamentally changed hey I with
any recent ideas but the fact that no
science hasn't yet delivered is partly a
matter of attitude in sociology and not
an inherent limitation I think it's
really important to remember that I once
heard a neuroscientist say data talks in
theory walks and I'm afraid too many
neuro scientists believe that they think
it's all about collecting the data
getting better measurements and so forth
and there's less I think encouragement
for people to develop theories i think
there's less funding for it and i think
young people are discouraged from doing
it and that's not a good thing still
this is a transitory problem and then
surely I think neuroscience will
contribute today I again that doesn't
mean if neuroscience isn't contributing
that we're left with what your gun said
at the end of his quote which was that
we're left with math and engineering I
think there's another place which is
cognitive psychology Wow you'll be able
to see my visuals really well now hmm I
don't know how much I had to pay for
that that's great so Josh made I think a
case for semantic net where somebody in
the room is leaning against the lights
that's my prior and maybe we'll find out
who that's my condo entrance I'll take
for example semantic networks that's an
idea from cognitive psychology that's
actually been revived in AI in Google's
knowledge graph and a bunch of other
places so they're lots of ideas from
cognitive psychology that are actually
starting to become popular again they
are maybe nobody's noticing or the show
attend tell network is using visual
attention it's again an idea from
cognitive psychology still I'm not going
to argue though I have a card-carrying
psychologist of a sort that psychology
really carried its weight either but
it's pretty obvious where we should look
which is little children this is where I
want it to be dark so you can see my
kids who are on that side of the room
two and a half and one year old I call
them my late model in vivo learning
machines and they I think offer a lot of
insight could compare them to current
approaches to machine learning which
largely try to dispense with trying to
infer the inner representations that are
used by people so computation is instead
of having a tree structure like a
linguist would spend years trying to
discern you have just if you're doing
machine translation you've got some
words that are paired based on some
human created data you just do it from
what I would say is a shadow of the real
data the real data are the
representations inside the brain you're
using this shadow sort of like trying to
reconstruct the 3d world from two
dimensional data and it gives reasonably
good results in the more data that
better you may have seen this graph from
the first paper on google translate it
show basically the more data that better
your blue score is but I'm a
psychologist and irie graph the axis and
if you go from zero to 100 yet any more
data doesn't necessarily help you that
much here's an example I cooked up in
Dublin last year was sort of honoring
Noam Chomsky you feed into google either
the translation of sentences with
complex sentence structures into celtic
languages remains remarkably difficult
or doesn't it goes
in to gaelic if anyone here wants to
pronounce it afterwards and teaching me
that would be great we don't have time
now and it comes back either continue
the translation of sentences with
complex sentence structures Celtic
language is remarkably difficult or does
it it's like sort of vaguely related to
the thing that we're trying to translate
but it's what a linguist would call word
salad its mess a decade later after the
first papers on google translate it
still frequently produces word salad
which is incoherent both at the sentence
level and at the discourse levels you if
you feed in a whole new story you didn't
get something incoherent it might give
you the gist but you would never say put
a legal contract into it and even on its
best days google translate still can't
answer questions about the past such as
translates it's not even set up to do
that it's no match for my two-year-old
so here's my two-year-old one actually
win his two and a half with what he
called a finger berry he looked he
coined this title in one trial and one
day while I was in California his mom
asked which of your animal friends will
come to school today and I want to go
through the inferences that he made he
said big bunny is going to come to
school today because he doesn't say that
word aloud Baron platypus are eating so
my wife goes into his room and finds
that he set up this diorama and indeed
Baron platypus were in fact eating as
she points out and he did this all by
himself which was pretty impressive for
two and a half year old so what does
that show well he understood complex
syntax is what a linguist would call a
wh question it's fairly complicated
logical reasoning so he can figure out
if one of these things is true then the
other things aren't possible he can make
novel answers depending on recent
updates to the world so he doesn't just
have a store cash answer but he can
update based on new things he can do
pragmatics you compare this to a system
they couldn't do novel updates it might
be confused about Barack Obama last year
he said he was 53 this year he says he's
54 which is it Obama so I'm almost done
my very quick talk here's one shot
learning and invention if I can get the
animation to play oh it's sad without
the sound the sound is not actually
working I don't know if we can are we
waking up he says up says my daughter uh
says my son mama says my daughter mama
says my son and they start giggling and
copying each other they've invented in
this happened after I came back from
another trip they invented a game that
my
called the wake up game one trial
learning if that and it's really like
they're simultaneously like a jazz
orchestra creating something you know
the younger ones a year and a half the
older ones not quite three years old we
don't have any idea how to do machine
learning that can do that level of
sophistication analysis from from such
small amounts of data until we dive
deeper into the mind we may be stuck
between AI systems with limited
flexibility where you have to have a
little card that tells you what you can
ask it to systems like Facebook's em
which are not limited you can ask it
anything but the really driven by humans
so studying human cognition might be the
best way forward studying the
correlations between these shadows the
output data rather than the thing itself
humans internal models probably can only
take us so far last thing I'll say is
big data isn't everything this is my
science paper from a long time ago 1999
showing the infants could learn rules
from two minutes of data and I since
then I've been trying to argue that
understanding how children acquire
patterns from small amount of data is
essential I still believe that that's
true I finally decided to put my money
or my time we're both where my mouth is
and I formed a company that's trying to
learn more powerful abstraction from
small amounts of data you can come talk
to zubeen or I about it if you'd like to
join and we'd love for you to join thank
you very much thank you Gary um Terry
put you from well says you so gave me
the opportunity here to first thank the
organizers this has been a great
symposium I really enjoyed it so I think
that we it's the wrong model to think
that either neuroscience is going to
make a big breakthrough for machine
learning or machine learning to make a
big breakthrough for neuroscience
systems neuroscience especially that's
not how collaborations work if you've
ever worked with somebody very closely
from a very different part of science
you know engineering versus biology you
find that there's there's a fantastic
cultural difference and it has to do
with their training and the way it is
and the way their conceptual framework
and what I've you know felt over the
years because I've worked on both sides
of the fence that it's a translation
problem and it takes time you know when
it first started out you know the inch
ever go up and talk about some very
sophisticated mathematical framework
control theory or something and the
neuroscientist did not understand one
word and similarly the neuroscientist go
up and use this arcane vocabulary used
for an ear anatomy and of course you
know it's like one of these other
languages here it's basically
understandable unless you've been
immersed in it but what's I think
happening now is the younger people
growing up and hear a lot of mere here
at the meeting they they go freely back
and forth between these two worlds and
where the advance is going to come is at
that intersection is it's at the being
able to the same time have a conceptual
framework that can map onto both really
different bodies of of science but let
me give one example a concrete example
and here I have I have a prepared to
talk here no I'm not going to give it
but I mean I was gonna say that if you
want me to I will know so in so this
happened in my own lab it's um it was
really remarkable so I came back from
Germany and there's a bee neuroscientist
there by name of mental Randolph Bensel
who studies learning in bees any case he
had a postdoc who is recording from a be
one of the neurons in the b fo mix one
by the way so it has a million neurons
it's a champion learner can go out it
can recognize flowers by the color their
shape and their odor and bring up and
with the waggle dance actually tell the
other bees were to go kilometers away
amazing that nature can do that with a
million neurons but in kc a postdoc was
recording from this one neuron and it
was an amazing neuron that would really
carry the information you need when you
pair a sensory stimulus with the reward
it was a vomits one midline neuron well
I took this back to the lab Peter Diane
and read Montague were postdocs in my
lab at the time and Peter Diane says
that's temporal difference learning
reinforcement learning and so we quickly
within a couple weeks had a model
together that reproduced a whole rap
of papers in be psychology by the way I
don't distinguish neuroscience and
psychology i think behavior basically is
a part of neuroscience so i wouldn't
draw a line there okay so in any case uh
this led to a resurgence of interest
within and because it there was a real
direct map actually to the human brain
because of the dopamine system had very
similar properties that this one neuron
had and Wolfram Schultz then took this
concept and turned it into an experiment
as a Claudia killer experiment and what
you do is instead of giving the word you
withhold the reward and you see that
what previously look like it was zero
was actually negative because it was
subtracting two things the expected
reward and the actual award so this is a
fantastic advance for neuroscience but
now what's happening is that it you know
the psychology literature has exploded
neuro economics a whole new field has
started and you know it keep snowballing
and now what's happening is we're
learning that different parts of the
basal ganglia the back part is more of
looking for simple of sr matches whereas
the prefrontal part the VTA projects the
prefrontal cortex is looking more for
model-based and so as psychologists
refined their experiments and as the
neuroscientist actually go in with brain
imaging and test them where I think it's
going to help a lot when you start doing
the kind of planning that is going on a
deep mine in terms of more conceptual
more abstract so I think that this is a
perfect time right now for these two
communities to work really closely
together and it's really happening I can
see it happening it happened in my lab
and it can happen in your lap too great
thanks a lot um maybe let me home kind
of some stuff they're also one of the
mantra of this symposium was like today
science tomorrow's engineering on in the
idea that we are interested in
understanding human intelligence to get
to create better engineering solutions
for interaction so um to the panel Mary
um
what scientific advancement do you think
I'm the most likely be important to get
a better engineering solutions or better
understanding of human intelligence I'd
be happy to start that I think something
that I don't hear much about at this
conference is symbol manipulation I
think it's it's an unpopular topic here
although I loved hearing about the
neural Turing machine stuff which
approaches that of course josh is
sympathetic but by and large at this
conference people don't talk about some
mu manipulation I was it another panel I
was tempted to ask how many people have
heard of Doug Lenin psych system can we
have a show of hands many people know
Leonard psych system it's all of it very
small smattering I wouldn't say that it
works but it's the best effort I think
that's out there to try to codify common
sense knowledge I think we need
something like that maybe it works
differently but works better and
nobody's really taking on that kind of
problem of large symbolic power symbol
like information and how we integrate
that with the neural network stuff and I
think when we get some better handle
either through computation or through
neuroscience or through psychology about
how the symbols fit together with the
pattern learning and so forth will be
doing much better symbols are vectors
well there's a line out there which is
you can't stick the whole sentence into
a vector and there's a lot of problems
with capturing things like
quantification and negation trying to
find the right place to put them right
let me let me let me get in the middle
of you guys I would say vectors are
symbols but not all symbols or vectors
and the fact that you can understand the
difference between those two sentences
or those two phrases really it's just
the beginning of what I think Gary's
talking about it's also just to go back
to the question that you asked me which
because this is also relevant you said
how do probabilistic programs work in
the brain you could also ask where you
know how does symbols work in the brain
I mean we know something maybe about how
vectors work in the brain and it's a
legitimate question to ask you know
probably programs in the sense it's the
same kind of question I think that's a
really interesting scientific question
but we shouldn't let it constrain the
science we do or the or the kind of
engineering that we imagine Mike Jordan
gave a great example of this at his
rumelhart prized talk over the summer at
cogsci where somebody
and Gary Cottrell asked him you know
well you know I think we should be
exploring some more biologically
plausible brain things and he said
looking to deep networks he said look if
if people were mean people have a
worried since the beginning about
whether back prop was biologically
plausible and how back prop works in the
brain people are still worrying about it
these guys are worrying about it and
they're really interested in it right
well okay Andrew told me he all right so
they're still debating it Andrews
interested in it Surrey thinks it's
something else but the point is if we
had if we had let if we had said well we
don't know how back pop works in the
brain then none of deep contents would
have happened right so I think it's
important to simultaneously hold these
kinds of questions as interesting
scientific questions let them inspire
engineering but not hold it back if I
could maybe play the devil's advocate
and be unkind to every single
self-discipline and characterize them in
the most unkind way possible and then
ask how can we yeah that that's what we
do but okay so this this is what we
could say right in large-scale
neuroscience the end product is some big
flashy movie of flying through a
connectome huge datasets you show the
movie you put the image of the paper
you're done no conceptual understanding
all right I'm being I think these movies
are very interesting and we will get
there I'm not saying we shouldn't do
that but that's an unkind
characterization of it machine learning
it's an engineering problem you just
want to win the competition you want to
get x % better on something no
conceptual understanding of what's going
on pure Theory conceptual understanding
of simple systems the understanding may
be completely irrelevant to the brain
and sophisticated machines all right if
you believe in any percentage as to any
degree of unkindness in any of these
characterizations how do we bridge the
gaps how do we extract conceptual
understandings from the movies and data
sets at allen institutes for example as
collecting and other people are
collecting how do we bring theory into
machine learning and create a science of
machine learning as opposed to an
engineering of machine learning and then
how do we bridge engineer
in the brain that those are the things
we have to do I think you're confusing
the product of neuroscience with with
actual neuroscience neuroscience is
about understanding mechanism like
hodgkin-huxley that we understand in all
of us in all of us present here even in
the basement that adds and the
cockroaches here that you know we
understand how electrical activity is
stored and propagated right that sort of
the principal yeah and it shows up in
the way in the forms of movies but let's
not let's not confuse the two inciting
well physicist all they do is generate
equations that nobody can understand
except a few experts but that's all what
physics is it's not really understanding
so I agree that that's the highest form
understanding hodgkin-huxley all that
but I think here were at a premature
point where technology development is
driving in your science and we're
collecting large scale data sets but I
think there's a lot more to go into turn
turning those large scale data sets into
a conceptual understanding of how that
brain region solves a particular problem
and indeed what problem is that range
and solving and neuroscience has always
been to redmond in the 19th century it
was died technology then it was a
discovery of electrodes in the 40s 50s
and 60s then was discovery of FM I now
to discovery of large scale connect
omics in optogenetics like any signs we
are always driven by new technology but
that leads to new conceptual question
that we couldn't have asked the 20 years
earlier I I think serious that's fair
enough but how do we get there now like
what do you see as the pathway now no no
it's no listen that this is what what
neuroscience is going through right now
it's going to take a while to settle out
so what I think you're reacting to is
the first blush of data used to be data
poor in fact you know almost i would say
night for the last 50 years it's
basically up until recently it's been
painfully recording from wandering out
of time and cataloging them in the
hundreds and you know that's a look two
theories like the grandmother cell
theory which is that you know there's a
single cell in your brain that recognize
your grandmother jennifer aniston cell
which you know it's basically it could
you be called the micro electro theory
of the brain but but but congratulations
now that we can see what the vector
looks like and how far as it is and how
its transformed from layer to layer and
in one of the I think really important
discoveries has been made in in terms of
the actual what's really going on in
deep human learning in terms of the
hierarchy is that it's not just a the
same thing repeated the same layer
repeated as you go higher and higher
especially into the prefrontal cortex
the time scales with which you integrate
the information gets longer and longer
and that means when you're saying
sentences for example the auditory
cortex is doing it really rapidly one
word at a time but by time you get to
the prefrontal cortex it's being
organized in a very geometrical way and
it sticks around that's what the memory
but breakthrough in memory was with with
with the LST ms so I think that there's
you know a whole new conceptual
framework now that we're just building
and I think these networks are helping
us understand those concepts and I think
that serious analysis is providing us
with the beginnings of a real theory I
think that the single best thing that
people in this room could do would be to
expand the repertoire of models that we
can compare this onslaught of data with
in particular I'm interested in the fact
that there's enormous diversity in the
brain so at Allen Institute Christoph
just told me about the magic number 42
which is a certain you know number of
neuron types in a certain spot probably
the number will go higher why did there
42 neurons in one particular spot or 70
in the retina why are there hundreds of
different molecules as stephen smith
also an Ellen brain institute is
documented the models that we have are
very simple compared to the diversity
and complexity that we're seeing in the
brain and I would hope that some people
in this room would take us up on that
and say well what can you do with
diversity if you've got it why would you
have it so so I think I'm a lot of the
comments here you know there have been a
bit sort of missing the point in the
sense of you know building trying to
build AI is incredibly hard problem so
we should be using every single source
of information we can and throw it to
throw at it so it's not neuroscience
versus psychology or neuroscience versus
engineering it's all of the above which
we think all of the above together in an
integrated way like Terry was saying and
we can do that now and moreover just to
make sure people are clear on this is
that we use system and neurosciences
inspiration but because we're primarily
interested in building AI we also didn't
understand the mind but primarily been
building aya we don't constrain
ourselves to only biologically plausible
solutions but it's one of the earth or
no pieces of information and and as we
know if you get a thug inul evidence
from two sources about one thing then
that's incredibly strong evidence for
that right like the TD learning that
Aereo's talking heads that's that's
absolutely seminal that work and that
was really key for us to making a very
big bet on our well of course it was RL
going on before but we know it will
scale we know it will because because of
this work by short centurion Peter Diane
who I did my postdoc with Peter Dan so I
know that work very well so i just don't
really understand this sorted by which
one's better what one should we should
do all of them and integrate them all
together and try and mine that
information from all these things and
we're in the meeting and they're also
varying in their own right of course as
well so i just want to insert one sense
i don't disagree with that at all i
think we in fact one or that unification
I just think psychology gets a bit
neglected gets the short stick I think
Josh probably agrees I do think
psychology is neuroscience at some level
but I think it doesn't get the press and
I wanted to give it a rest and I agree
with child psychology is extremely
important but you know to answer yo Gunz
question radio conservative you know why
would you want to ignore some source of
you know incredible pieces of non
promotion you know nobody up here is but
many maybe there's other people in the
owners with that view and I just never
understand that point you've got this is
the other workshop yeah I think but I
mean I Syria um it was interested that
you kind of left out psychology or
cognitive science from your critique
yeah yeah yeah please so so I don't
believe this i love Josh's stuff but
here's an unkind characterization of it
he is doing probabilistic inference of
her programs it's combinatorial large
spaces it'll never scale up the brain
can't be doing it and must be doing
something else simples are patterns of
vectors of patterns of activity in the
brain there's no way around it okay done
okay so obviously symbols if there's
some if there's something thank you and
I love yours very much if they're
seriously and I love that what one of
your main examples was inspired by
cognitive science like stuff that I've
worked on to it and okay so
yeah but I mean but but the kind of but
let's listen to the the different things
people are saying here cuz the the
subtle differences are I think really
important right of course nobody could
argue that if symbols are in the mind
and they must be in the brain somehow
and there's nothing in the brain except
patterns of neural activity probably I
mean there's something did something
like that so obviously symbols must be
some kind of pattern of activity whether
they're just vectors I mean I think you
know here's a place where gary has
argued that symbols in language and
cognition need to go beyond just simple
kinds of things like that if you guys
are interested in this come to the
workshop tomorrow on symbols and neurons
where we're going to be debating these
things I'll show for example a very
simple kind of challenge data set in
very simple kind of language
understanding that to date none of the
best recurrent neural net models for
language and we've we think we've
actually tested the state-of-the-art are
able to address a very simple aspect of
language understanding it's I bet they
could I hope one of you guys will figure
out how to do it so I'm not trying to be
you know not trying to be a negative
critic I'm just trying to say there are
real challenges here your brains will do
with exactly the thing i'll show you
tomorrow and then let's get something
like a neural circuit to do it Tommy you
wanna yeah I think this has been said
already several times I think the brain
is a great problem this also means a
very difficult problem I think we need
hope all we have all we can get our
hands on in order to make progress on it
this means cognitive science
neuroscience computer science
engineering no questions about it you
know AI in the 50s in a sense faith
because it was computer science and just
common sense he knows I just personal
feelings of what was going on in the
brain of Marvin or other people involved
at the time so this time work this to be
different they think as Terry mentioned
it's a great time to collaborate we'll
need each other not easy to organize it
same problem the deep mind has you know
to organize
is not just the Manhattan Project is
more than that because the Manhattan
Project was science but a lot of
engineering people knew that a bomb was
possible and so on we really don't know
whether you know in artificial
intelligence is possible you don't know
whether we can solve consciousness or
the brain and personally think so but we
have just try but you know just to this
is very general very kind of
metaphysical let me mention one example
very specific of something that comes
from from neuroscience cognitive science
everybody knows and I think will have
profound effects in understanding human
vision and developing different systems
for computer vision and this is the
simple fact that our acquity depends
strongly on eccentricity very strongly
you know we have in v1 probably simple
cells that are as small as about one
cone excitatory calls and maybe a couple
of inhibitory cones nearby and they may
be just a that resolution covering only
around half a degree of your vision
right half a degree is a fraction of my
thumb by this length this is one or two
degrees so that's very small and
conversely there are simple cells either
if one other areas that are 10 or 20
times larger so and they cover around
five degrees or so all of this data that
I mentioned are reasonable estimates
plus or minus a factor too but that
means that when we look around at high
resolution we can most see in half a
degree that's very little five degrees
and in each one of these levels of
resolution as many as five or so you
have only 5 40 by 40 units simple sets
so I think all of this has a lot of
implications for instance that when you
recognize an image the recognition is
based on fragments that a satellite
solution are pretty small less than 40
by 40 and this the respect of physics
that shipment Norman has done on this
milks minimal minimally recognizable
images that you know support this and I
think the design of a system that works
this way will require fixation for
instance the old design of this system
means that scaling variance is much more
important than translation invariance
and the evolutionary reason is probably
that it's easy to move your eyes if you
want to correct for translation it's
very difficult to move yourself if you
want to correct for scales we want to
have scale invariance more than
translation invariance so anyway I'm not
saying the computer vision systems that
you get out using this framework could
be better than say the system based on
the image night systems I think it will
be different thats related out were
saying about you know there is human
vision you can solve it there are other
forms of intelligence in this case
visual intelligence that I'm not you can
say they're better worse they have
different properties for instance wine
image night you need to test with you
know the response has to be among five
choices that the reason is that the
systems as they are don't fixate that's
a quite different from what we are doing
anyway just that's my personal bet we
may be interesting to ask other people
i'm not saying this is the only one I'm
saying that this this one of just
eccentricity depended size or receptive
field inventors three
we'll have a profound effect in you know
the the outline of the vision system the
fact that you will have to keep a kind
of hippocampal type of map of pointers
or places where ever looked
corresponding to the illusion that all
of us have the technique given time I
see all the room when in fact I'm seeing
with reasonable resolution on the very
small part of it but I can move our eyes
where I want so this implies the
existence of some map that has not be
found yet anyway so to be interesting to
to hear other bats about you know
scientific data or research direction in
envision in the brain that could be
important for the future engineering
okay here's my bed so um I i think you
know we've had these amazing successes
and a lot of perceptual tasks but to me
I think worthy that the engineering
systems are most blocked is still on
action selection so we have these
amazing systems from demonstrate how
deep q-learning but it's still using a
deep network i would say essentially
perceptual role so it's using deep
learning but it's not using what i would
say is the central intuition from deep
learning which is you should compose
together simple things into more complex
things so to me a truly deep learning
system in the actions case should be
some form of hierarchical action system
so you have sub actions that you compose
together to get more complex actions
it's actually this action hierarchy so i
think we're kind of stuck here on the
engineering side we have some ideas you
know there's there are a variety of
hierarchical reinforcement learning
schemes but i don't think any of them
are fully satisfactory and yet there is
some interesting evidence for example
Matt bought Phoenix work I guess it's
now it's a deep mind maybe that's a hint
who you know is as a studied
hierarchical reinforcement learning
right so to me maybe that's one place
where you could look at the brain and
learn some more and just to say a little
bit more about why I think action is so
important is I think it runs you
straight into causal
and causal knowledge so when I see
Josh's examples i actually think common
denominator among a lot of those
impressive examples is action selection
right so you're drawing the actual
characters so incorporating that and
understanding how the brain does that
mean can I make a comment on that or do
you want to follow up on that I have a
direct follow yeah follow up in then
I'll follow okay so so is very this is
really interested Pat churchland I wrote
a book chapter which has been cited
something like you know two thousand
times and the the title I possibly
because of the title it's a critique of
pure vision and the the point that we
try to make in it was that we our
intuition about vision is really miss
lettuce in many ways and I think Tommy's
point is exactly right which is that it
looks as if you know we have clear view
of everything around us but the reality
is we know that you have a phobia and
you're only getting snippets to the
world and somehow we pasted them
together right that's the mystery and
they're all these fantastic experiments
that have been done by psychologists
beautiful experiments where that what
they do is they flash a picture complex
picture magazine and then it goes off
for a second it comes back and something
has changed change blindness and you
cannot for the life of you figure out
what it is unless you have to be looking
at the object when it disappears it's
really dramatic when you see it and
what's amazing is that after you see it
you can't not see it miss your brain has
changed it's not the same brain anymore
so i think i think that i think one of
the reasons why good old AI went off the
rails was they trusted their intuition
about what the problem was they were
trying to solve or what what the
solution was and there's no reason why
biology or evolution should ever have
given us any insight into how the brain
works i mean you know it's survival
that's important you know you don't
really want to know what's going on in
your visual system you want to see so i
think that's why it says it's a tough
problem because it wasn't built by human
engineer it was evolved and it may be
very different from what we imagined it
might be
um yep so just also follow up and I
think it's a great point to make a
connection between action and actionable
representations and causal models I
think they're very connected I think
it's part of why we've been interested
in studying what we did and it's also
part of how I think to go back to want
the other set of questions how we might
close the gap to the brain right I think
in in both like the in the handwritten
character stuff one of the things that
brendan is currently doing Brendan lake
is actually trying to study these kinds
of representations in the motor cortex
and there's good reasons from prior work
to suggest that even when you just
passively view a character you see some
you have some promoter representations
and he's looking to see if you can see
that in a much more interesting fine
grain structure which his models can
help to decode on the stuff the stuff I
briefly talked about with the physics
engines right with the you know imagine
the blocks falling over some really nice
work by my MIT colleagues in Nancy
chemistries lab and Jason Fisher they've
actually found you know some something
of the neural circuits that seem to
underlie performance in those and other
kinds of visual intuitive physics task
and they also combine combination of
parietal and promote or sort of
actionable representation so it does
seem like there's a link between the
kinds of causal models that we're
talking about here and the way we plan
our actions in the world but I want to
also turn this into a challenge a place
where again coming from basic psychology
although though there have been decades
of experiments along these lines you can
just do it as a thought experiment to a
challenge for really all the people
interested in RL including demouth
because I think this is this is really
interesting in Atari video games
actually right I mean it's it basically
goes back to the idea of model model
based learning right and Tolman's latent
learning and learning models that are
not just depend on the reward signal
we're getting or the task we think we're
doing right now right it's one of the
most foundational results in cognitive
psychology is that we can learn a model
and then use it for new things that we
didn't know we were going to do when we
learn the model and I think you can see
this in a really neat way in video games
right think remember when you you know
some of you probably are playing a lot
of video games for me I have to go back
to my misspent youth but when you
learned say how to play an Atari video
game you didn't just learn how to
maximize score think about all the
different goals you could and often kids
do adapt all sorts of different goal
that they put their knowledge to like
for example sometimes when you're
playing a video game your goal is to die
as quickly as possible all right like
you know you say time to go you say okay
let me just died so then you use your
knowledge to do like the worst possible
thing right kill yourself immediately
right or sometimes your goal is you know
you're playing with a friend and you
just want to you don't want to embarrass
them you want to beat them but like not
by too much right that's another goal or
sometimes your goal is like get a
certain score so that it will spell out
a really cool thing when you when you
take a picture and turn it upside down
or like some swear word or something in
Atari digital or sometimes your goal is
just to get to a new level right it's
not like you when you're exploring the
game your goal isn't just to get score
you just like okay let me get to this
level as quick as possible so I can get
to a new place and start doing that and
so I think it's mean I bet you guys have
something to say about that but it's
also a question for RL systems in the
brain how does rich model based learning
work in the brain because like we have
every reason in the world to believe
that has got to be there basal ganglia
and the this yeah it's well we're about
cell be obviously we're hugely
interested in her RL and i agree with
andrew that that's one of the main you
know things that we need to have a
breakthrough in also we've raised it in
sub gold generation there's also issues
of intrinsic motivation there that
coming in you know that novelty seeking
these kinds of things that are driving
behavior not just the top level goal of
score so we're looking we're researching
all those areas too and in fact we need
some of those solutions to solve some of
the games even some of the Atari games
like our Mount Everest is Montezuma's
Revenge which doesn't have much school
and is incredibly dangerous well we're
almost everything kills you so initially
the agents just learn to sit still on
the start platform because they had
never seen any reward in the world and
all later how all that happens then was
they got killed instantly so it seems
like quite a reasonable thing to do at
least that's our own interpretation of
what's going on there so yeah so we're
working on number approaches to try and
solve it I would say it's one of our
most active research areas by the way I
never gave the punchline for a book
chapter which was a critique of pure
vision
in number one was that you shouldn't
separate vision from the rest of the
brain the sensory systems because
there's so much feedback and number two
we at the end of the day when you and
this has been an iterated a couple of
times just want to make it really
something you remember which is you
really want to start from the motor
system because without the motor system
you can't move and that's really the
purpose of the brain is to move you
around so that you're out of danger and
you can find the food and and and that's
really what the all the representations
are funneling in there and if you don't
have motor system and your deep network
you then you know you're going in the
wrong direction yeah yeah and because so
another comment is going backwards yeah
in Josh's presentation he kind of went
easy on you in the sense that he gave
you these sort of simple physics stuff
is just maybe a little bit beyond what
you can imagine your deep networks doing
but he could have gone way further right
there's things like theory of mind where
you're interpreting other agents as
rational thinking beings and
interpreting their goals and how they're
acting is like okay you know the dog is
running after the frisbee because it
likes frisbees and things like this and
I think that's all again about action
about interpreting other agents acting
agents and so that it's actually the
space of problems is incredibly rich
here you can learn all kinds of things
from observing an intentional agent
action acting as well I mean we think
actions massively important which is why
we chose a agent base or a setting for
all our research so that it emphasized
action from the beginning and I think
you know it's you can't have
intelligence without
I want to emphasize something demo said
earlier that relates to what Josh was
talking about which is transfer so I
think to do transfer learning well you
need better generative models I think
it's one thing to memorize a simple
Atari game but to really go from one
game to another one first person shooter
to another you want some more abstract
excuse me representation I'm so via
already over time um but we promised to
open the panel discussion to the
audience so if anyone has interesting
questions you would like to ask the pen
oh okay so surprising who's there first
somebody cited you York and I'm sure
somebody my first name is you again
because it because it sounded easier to
pronounce then oh no not you again
however tomorrow I just wanted to point
out tomorrow at the deep reinforcement
learning workshop which is organized by
Santa nursing and I think which Sutton
and colleagues we will address all these
issues that came up hierarchical
hierarchically reinforcement learning
transfer learning from one task to the
to the other program learning where the
program's involves subproblems at a
solution to previously learn programs
and and and tasks and so on so all of
that has a long history at least 25
years or something and progress has been
made all the time and there's a lot to
say about that tomorrow at the deploying
deeper enforcement lying workshop so
okay um question yes I have a crisp so
listen to you all talk i hear that
there's a big need for diversity i see
very different perspectives and i keep
thinking that there's a need to have
people that are good at bridging
disciplines and as I do that I think
well women tend to be very good at that
and I'm wondering why there's not a
woman on the panel and that's a great
question yeah just pointing it out so I
mean I just think I'm it is it's really
something that women are very very good
at and it would be something that would
be um I think it would it looks like
it's something that's needed in the
field and I just wanted to say that I
have a question
take your time phone more yep despite
all of your friendly disagreements the
thread through most of what you've said
as far as I can tell is a focus on tasks
that almost all humans can do even young
humans may be very young humans and
obviously there are also domains of
expertise of tasks that are difficult
for humans to do mathematics and music
and programming and I'm curious whether
you believe solving the tasks that you
focused on tonight will be prerequisites
to further automation of these expert
human domains and there will be just a
continuum from one to the other or
whether you expect that entirely
different techniques might be necessary
or might be accessible now in some of
these more let's say advanced domains my
answer would be that it's a very
multi-dimensional thing it's not yoona
dimensional so already we can have
expert level chess without solving any
of these other problems and it's going
to depend on what domain you're talking
about in the abstract it's hard to say
there are things like where you want to
understand human language in order to do
the next task where there's some kind of
prerequisites structure but it really
depends on the task yeah um I certainly
wouldn't disagree with that i guess i
would say that for me the focus on these
things that basically all humans do well
even young humans is really motivated by
the goal of reverse engineering right
that's very much what we're all
committed to hear the sub this is a
engineering-based science we're trying
to reverse engineer how the mind and
brain work and your best bet is to do
that for systems that were actually
engineered right i mean even we can talk
about exactly how we think brains got to
be where they are but and gary and i
have recently debated about this but i
don't think we really fundamentally
disagree you know evolution is a kind of
design process it's a kind of
engineering it's not the most efficient
it takes a long long time but look at
what it did and um in some sense as
pedra might say it is the master
algorithm right so okay so if we have
something with looking at the capacities
that in some sense our minds and brains
were engineered to do I think that's
just the best target and then I think
that you know you point to chess right
so well there are human level really
world champion level chess-playing
programs everybody will tell you that
they don't do it the way humans do it
and I do think that our best bet to
understanding you know all those other
kinds of things including mathematics
including chess including also the
flexibility of you know well we have a
human level expert chess-playing system
you change the rules in some way you
know you you can you can stump them just
like you can stop any current AI system
I think that but by following this
reversion reverse engineering path where
you start with the most basic cognitive
abilities and then you know try to get
follow along the path that led to the
more advanced ones that we're likely to
come up with more interesting solutions
to those more advanced kinds of
cognition well the mathematics is a
particularly good example because I
don't think there was any drive in
evolution for us to do mathematics right
I mean did it you know did it make you
stronger faster in some ways it makes
you weaker but somehow it we were able
to use the machinery the evolution gave
us to do something which is really
amazing you know abstract mathematics
many different sorts and it that means
that what near the cortex and all the
basal ganglia all the machinery there is
is in some sense of a general purpose of
learner and creator of conceptual
frameworks and I think that gives me a
little bit of hope that in fact so you
know if we understand the first
envisioned the best understood part of
the cortex then we'll be able to
generalize that to some of the other
parts that are not as well known you can
make an argument if you look at the
brain part that we think are involved in
mathematics for example from the hanes
work later three at the structural level
it looks very very similar to all the
other cortical areas so you know it's an
evolutionary adaptation of pre-existing
abilities like anywhere else in
evolution so once we understand the
basic principle of computations I think
these other ones will will will fall out
relatively easy okay let's take two
through more questions hi I so there was
a lot of Awesome talk tonight about
collaboration between various
disciplines I'm kind of curious what the
your opinions are on the implications of
this on kind of the structure of
academia what a PhD should look like
should a PhD be a deep dive into one
very tiny thing or should there already
be an attempt early on in people's
careers to go across disciplines what
does this mean about yeah I'll start
with that one but i can say that i think
i think is an undergraduate it's
incredibly important to do a deep dive
into something and even as a graduate
student but you know take classes in
other fields talk to people in the
fields I mean my trajectory is I did a
deep dive in a string theory completely
irrelevant to in detail to anything that
I'm doing now but the deep dive into the
mathematics and thinking and all that
which is very useful to me in general so
I think it's incredibly important to
obtain a very very deep education you
know but then at the same time just
talked to a bunch of people and take
classes outside your domain and then as
a postdoc start to bridge to other
domains and then as a faculty you know
really start bridging and then when your
tenured you can do whatever the hell you
want yeah in it I can tell you what our
Center for brains minds and machine is
doing subtly trying to form educate a
new generation of students and faculty
members that are proficient in all these
areas of neuroscience cognitive science
and machine learning computer science
for instance we have a summer school in
boots all and I think if you should
perhaps have a look at our website
because we had it for the last two years
it's free weeks and the feedback we got
from students was fantastic so it's
trying to get you know experts in
computer science machine learning up to
speed on the cognitive neuroscience side
and vice versa
and I'm not saying this should be what
you do as an undergraduate because my
personal feeling is you should just
learn mathematics that's it but but but
but it it's at some point you you want
to know where to look and you want to
know with whom you should speak if some
problem comes up means having a general
knowledge of all this area's you know I
did the same thing you guys did so I
mean I'm not going to say it's a the
wrong thing to do however it's a lot
easier to absorb conceptual structures
when you're young if you wait until
you're a postdoc to even begin to
understand something about biology it's
it's almost too late in the sense
because it's a very different type of
science that requires you know you have
a system or you have uncontrollable
variables very high-dimensional I mean
and the brain makes it even more
difficult but but it really i think if
you think you want to go into
neuroscience you should along with your
math courses you know start just start
talking to people who are working on the
brain and you'll discover that by
osmosis you'll be absorbing that but is
i think that you don't want to be too
narrow I think deep dive but you also
want to you know spread yourself out and
so that you have a little bit of a
little bit of cross-disciplinary of
thinking if you want to get a lot of
grant money study something really
narrow if you want to change science get
that interested interdisciplinary
training early and push it hard um I
think petrol you run an accident yeah so
I think more than half of the speakers
you know mentioned the you know Hubel
and Wiesel new cognate Ron you know
continents deep learning as a great
example of how progress can happen in
this in this area and it is on the other
hand one salient feature of it is that
it took 50 years to happen and we don't
want the next ones to take 50
years because if that's the case then we
will take hundreds or thousands of years
to get to AI so my question for the
panelists is how do we make this loop
faster how they make the math the next
thing happened in five years instead of
50 so two years ago they the president
announced the brain initiative and the
stated goal was to get engineers
computer scientists mathematicians
physicists to work directly with neuro
scientists in order to accelerate
progress by making devices and you
Christophe gave beautiful talk about
where we are in order to accelerate what
we know about the brain so that we
wouldn't have to wait a hundred years so
that we could do it over the next 10
years that's why I set up this is a
fantastic time because over the next 10
years we're gonna figure it out
Christopher so I've given a lot of I've
been given a lot of money and a 10 year
plan to understand the brain however i
also have a donut Paul Allen who's asked
how many Nobel prizes is it going to
take to understand the brain is it going
to be 50 or 100 and they said if now I'm
not talking about the AI I'm trying to
understand this plane that s terrex has
characterized by a very very large
degree of freedom and a system that's
very far away from equilibrium and so
most of the standards that mech just
simply doesn't apply and there's less me
if you look at the war on cancer the war
on cancer was started on a Richard Nixon
1970 1971 immediately led to a quadruple
of the federal budget of the National
Cancer Institute not like the brain
initiative that led to an increase of
point one two percent of the total nih
budget and it's only now 45 years later
like 45 years after this so-called war
on cancer stuff that we understand
enough about about the basic biology of
cancer that we have things like gleevec
and there we are things like
immunotherapy that a really rational
design therapies where we really begin
to understand the mechanism of cancer
and the brain probably is vastly more
complex because the brain are only
consist of cells like any cancer cell
but it's also this network thing and so
by that measure we you know the vote is
going to be it's exciting but the hold
is going to be a long one to understand
brains and I mean personally I think
we'll have
I way before we have understanding of of
our own brains yeah I mean I think one
answer to that you know what we're
trying to do it deep mind is that I
think we should be looking at the ways
we structure how to do science so
everyone's talking about into this
prescience is a great idea and I think
you know that's been the sort of lip
service buzzword for a long time but
actually how much interdisciplinary
science goes on at the really
fundamental level in many academic
institutes and I think it as a company
we've tried to organize it specifically
around that and sort of touching what
some other people said about we have
very fluid dynamics between our groups
so there's very little siloing you know
quickly small teams reform and and and
sort of unform and then you know new
types of teams come together around a
certain project and very short time
scales and when we do have one way we
get around this problem of going deep
and and being general is we have
obviously if you've got a company or big
organization you could have both types
of people and they're essential living
at a deep world-class experts in certain
areas and then equally valuable amazing
generalists that can actually connect
between those world-class experts and
you need a mixture of both of those and
with both of those types of people being
in power to do their bits and in a very
collaborative open environment where one
thing I think problem I thing that
happens in academia is it's hard for the
generalists to get the same respect as
the deep specialist because of the way
you know with this behold about evening
debate on its own but because of the way
you get grants and and and Wade
committee's work you know it's hard for
them to judge a a true generalist but in
a company you can change all of that so
I think that you know one ounce of that
is innovating on the way that we do big
science not enough I mean we yeah so you
know please apply the lady that's
earlier I agree you know we need me and
we actually don't have enough generalist
they're the hardest people to find
people who are deep enough but but have
taken the harder path in my opinion of
being general and I was a greedy Terry
actually I think you should do earlier
you do it you want to get some core
skills like coding and maths you know a
basic core skills and put you know
problem-solving skills but then beyond
that I think you should be quite broad
in terms of the content that you're
trying to learn so Tommy wanted absolute
just now I'm always surprised how
unrealistic people are if we say that
the problem of intelligence is you know
one of the great problems in science may
be the greatest one and the other
problems are the origin of the universe
the structure of time and space the
origin of life I mean and then we say
that 50 years for solving the problem is
too short my goodness so you know and or
does it mean to solve any one of these
problems it's kind of a we can say rate
of convergence we can say that there
will be a lot of byproducts and progress
done on the way and I'm sure of that I
also think we'll get there in terms of
making machines that are amazing but I
think it would be quite some time before
we solve the problem of intelligence um
there's a wonderful book written by my I
actually two advisors one of my was
Tommy paja my other by that was Valentin
Battenberg he put a wonderful book
called vehicle the law of a pillar
analysis in download synthesis in which
he tried to demonstrate that if you if
you use simple neural networks and put
them together in a particular way in a
relatively easy way you can synthesize
very complex behavior that from the that
that if you wanted to analyze it from
the outside is very very difficult and
that's the test that we knew a scientist
phase we trying to understand that the
brain was out knowing its rules while a
is try to emulate something which but
this argument should be much easier but
they also sell it early lessons in in
biology so for instance if you look at
this big data analysis of cell lineage
like they do for example here at the
board Institute they make these
complicated Network for their thousands
of of so-called lineage factors that all
interact in order to try to understand
the sort of the lineage of one
particular cell and then yamanaka four
years ago goes in and uses just four of
those two thousand conscription fact
that's okay and it takes any say takes
yourself from first in miles and two
years later in a I'm assuming any cell
you put it in
you ad for transcription factors the
right one and suddenly you get inducible
till we put in stem cell which now you
can turn into any parcel cells so turn
out you didn't need big data you didn't
need 2,000 factors you just needed to
have some intuition of what the hype
factors so it's difficult but not
hopeless I think this is a good tunt
stuff actually so thanks a lot for all
the speakers and finalists for the
really interesting symposium
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>