<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>3D Vision in a Changing World | Coder Coacher - Coaching Coders</title><meta content="3D Vision in a Changing World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>3D Vision in a Changing World</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jlZE28lDnzI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello just to calibrate me how many of
you have taken a linear algebra course
great how many you remember anything
from it do it good and how many have
done any sort of 3d graphics about
quarter maybe cool okay right so I'm
going to talk about a long sequence of
work I've been involved with I'm
actually going to start with soda my
very air some of the work i worked in
very early on and it's all leading to
the idea of recovering 3d information
from the world and in particular no
lasers will be harmed during this
presentation so i'm specifically trying
to recover 3d information from RGB
images from sort of standard images
rather than what's a the kinect does
which is inject structured light into
the world to simplify the problem so
it's it's RGB only the bulk of the work
I'm talking about was done in
collaboration with some with interns
here in fact so all of these people who
are now at various other places as
listed were interns here at some point
and did some of the work that that I'm
going to be telling you about it's an
amalgamation of all their their work so
a very long time ago are in 1998 I was
interested in the problem of taking an
object like a dinosaur or whatever this
thing is it's Godzilla thing and given
multiple views of this object i would
like to recover a 3d model so these are
the input images we used they are about
vga resolution so about 480 lines high
and we had 36 such images as the object
rotated on a turntable and this is a
zoom in on the hand I think it's the
wrong way around so I think it's this
hand of the dinosaur showing the polygon
model that we recovered so what I want
you to remember here is that if you
calibrate your system really really well
you can get
sort of extreme super resolution so this
hand was never larger than 50 pixels in
any of the input images so there it is
there there it is there but yet we have
several thousand polygons modeling it
here to a reasonable degree of accuracy
and that's just doing everything as
accurately as you can and just remember
that you can get about a tenth of a
pixel accuracy should be or your
ballpark when you're doing anything with
images so we did this work we were very
pleased with ourselves we try to solve
longer and longer sequences and so
eventually we got to the problem the
structure and motion problem which is
take a video like this a standard RGB
video we're going to do some processing
on that video to recover 3d information
you're going to see that the 3d
information isn't actually very
impressive but then I'll show you how we
did something impressive with it which
gave rise to this thing which is a Emmy
Award so what do you do you take the
video and i'm going to show you this
pipeline because it's still alive and a
lot of the work we do today you take the
image you run some image processing
operator which will fire at those red
crosses so the red crosses have been
inserted by the image processing
operator which is looking for
essentially high contrast points points
that are dissimilar from their neighbors
and that's a completely blind low-level
operation that you can do and then you
can track those points over time giving
this or yellow trajectories so that
everywhere in the image some 3d point in
the world is now associated with a
trajectory at the yellow trajectory a
sequence of 2d positions and from those
positions you can reduce the points to
the 3d so you don't want to solve for
the combination of 3d structure of the
world and the motion of the camera so
each of those tracks or many of those
tracks were converted into these blue
and yellow dots please ignore the
difference between blue and yellow and
those dots themselves represent a 3d
view of the world which we can view from
from this sort of you so you'll see that
the reconstruction we got is is very low
res
okay even though we have some points
this is the long building on the
background this is showing the
trajectory of the camera that was
recovered as it as it moved through the
scene so it's very low res you couldn't
really use it to build a 3d model of
anything interesting but what's valuable
it turns out is this thing the camera
trajectory because if you know the
camera trajectory you can modify the
scene by placing objects within it so
here you can see that we've placed a
computer graphics ladybird in such a way
that it appears to be are following the
movement of the camera and you can
possibly imagine that if we used a bit
more realistic lighting and actually got
some professional animators to draw the
ladybird that you could pretend that it
was really there and indeed this this
exact piece of software is basically
used in any movie you see today that is
not made by industrial light and magic
so basically everybody except them
bought the software they have their own
and it's used to do all sorts of stuff
including you know obvious things like
castles on the hill in Lord of the Rings
less obvious things like extra snow in
bridget jones's diary so that's a that's
a sort of a big practical use of this of
this technology and just to remind you
what people used to have to do if I play
this again I'm just going to play a
version where I turn off the background
and you'll notice that the lady wrote
appears to move around much more
animatedly when you can't see the
background so you as a human look at
this problem and you think oh it's not
too hard right just put it in position
every frame and shifted around before
they had this software the animators had
to make quite significant movements from
frame to frame its mate so just
demonstrating that it was actually a
quite a difficult problem so that's all
very well but what was special about the
world in which we did this 3d
reconstruction was that nothing moved
you may have seen a cyclist walking
driving through the scene but the whole
cleverness and the algorithms is how to
ignore those moving objects the cyclist
is treated as
as outlier data so what I want to do is
do the same sort of thing but in a world
where there's more animation more what
we call non-rigid motion so let me just
set these to play okay so here's a video
where the camera is moving across the
background and we have some moving
animals I'd like to be able to somehow
recover information about these animals
I'm not going to get good 3d information
because we're only seeing the animals
from one side but I'd like to recover
something that behaves like 3d here's
another image where the motion is
apparently not very significant right
he's just the guy is talking his mouth
is moving each we should be able to deal
with this these sorts of images and
generate some 3d information when I say
we should generate some 3d information
sometimes I won't show you the 3d model
that we recovered I showed you for the
Buju problem that I recovered a 3d model
and it was a sort of an uninteresting
sparse cloud of points well the first
work we did on this non-rigid work a few
years ago set as the goal not to recover
the 3d information but to paint the
videos as if there were 3d information
there so you can see that as this guy's
head moves around his artificial 1970s
mustache is correctly hidden so it looks
as if the left side of his face is now
occluding the mustache so we're
demonstrating it moves as if it's in 3d
it follows his face as if it's in 3d and
it secluded as if in 3d but i'll show
you that we never actually did any 3d
reasoning here and i'll try to argue
that was a good idea okay so how do you
do this this 3d non-rigid problem well
are some a great line of work started
about 14 years ago now is by these guys
Greg lyrtle asylum Torresani at Al they
follow the same principle
of tracking interesting points so you'll
notice that here they have the left side
of somebody with very textured trousers
so that they're able to track 2d points
on that person and then they can turn
that into a per frame 3d reconstruction
of what's going on so that's pretty
impressive right so it's taken the 3d
problem and solved it it's it's kind of
much harder with non-rigid because it's
a different 3d model in every frame so
it's it's quite deeply harder so that's
great if you've got dents point
correspondences you can get a 3d model
so we would apply something like that to
this sequence of our actor talking and
I'm going to go into this in a little
bit of detail because you can see the
kind of the kind of developments and
hopefully it will point out some of the
difficulties so how do we do it what is
the actual maths behind this well this
is representing my point tracking so I
found some interesting points I've
tracked them just as I would in the
rigid structure in motion case that's my
same starting point for non-rigid ah and
each of these tracks i do something
pretty simple right so this is the guys
I in you can't see that so here's some
interesting point detected in frame one
I simply take the XY coordinates of the
point and stick them into a column
vector as the first two entries I go to
the next frame I'm actually going to
move quite a few frames further on so
this is the trajectory that we had some
time further on we see the eye again and
we copy it into our vector its X Y value
is just too d ok we do that again and
then when for example the eye is not
visible in this frame or we haven't
detected it we put some entries in the
vector that tell us we have no data
there ok so there's one 3d point the
person's I represented as a tutti
dimensional vector with some missing
entries right we do the same with
another point we get another column in a
matrix which is now to t by 2 matrix
right and you can imagine we keep doing
this and we build up the
these measurements and if i zoom out and
do it for a long time this is I've
turned that video of a person moving his
head and talking into a representation
which is just a giant matrix of 2t times
n numbers okay and all of this you can
do automatically reasonably reliably you
know that's fine we can easily build
this matrix so what do are so what are
we going to do we've got a matrix full
of numbers what we're going to end up
doing is something that looks like
principal components analysis who has
heard of principal components analysis
fewer than have done the neurons are
interesting okay that analogy won't help
me so I want to just take you through
how this matrix relates to the 3d
structure okay so summarizing everything
we know we have on the top of the bottom
right here I represent that matrix I'm
going to call that matrix M and let's
take the second camera position and the
enth 3d point and let's see what entry
is in the matrix right so so here's my
giant matrix M cameras are down this way
and points are out this way so I'm going
to say 3d point xn camera p 2 and
basically the value the 2 by 1 vector at
this point of the matrix is equal to
camera two times 3d point xn sorry
shouldn't use n no use I ok so this
matrix is full of entries where this
index changes with the rose and this
index changes with the columns we can
write it a different way we can write
the matrix matrix as its first column is
a big matrix with all the cameras in it
p 2
x another big matrix with all the points
in it okay so this matrix is to t by i'm
going to say four and this one is for by
n right why is it for here because i'm
using homogeneous coordinates don't
worry about it if you know computer
graphics you recognize it otherwise
don't worry about it why is it only to
hear if you know anything about this you
will say that should be a three by four
matrix I'm looking in this special case
at the case where the camera is very far
from the scene orthographic projection
again don't worry about it all the
nonlinearities will disappear anyway so
I had this giant matrix M this is M and
I know it has this special form it's a
very low rank every column of n is at
most a combination of the four columns
of this of this matrix here so it looks
like a very simple job it's like doing
principal components analysis on these
on this track data okay so what did I
say I said I had this giant matrix m and
every column of em was a tutti
dimensional vector each of those tootsie
dimensional vectors i obtained so this
is what this is this is the all rows I'd
column of em each of those is obtained
by taking a 3d point living in or four
or are three don't really worry about it
and lifting that point up to 2t
dimensions so on the sequence we were
looking at we had approximately 250
frames so let's say we're lifting this
3d point to a 500d point right twice 250
frames the simple case I told you is a
linear embedding it says take those
points that live in 500d and find a
linear mapping down to our four and you
can turn that into 3d structure really
easily to do perspective projection is
actually not that much harder this
function pie this function which map
from our to the some low rank place 2 to
t is a little bit more complicated and
what you have to do is you have to say
find me the function pi which minimizes
the difference between the matrix M that
I have and another matrix which is the
projection of the first 3d point
projection of the last 3d point all
right so the big norm between the
difference between this matrix and all
these projections and I going to
minimize that function / / they over the
parameters of Pi apologies for my
terrible writing so this minimization
problem in the first orthographic case
was an easy minimization problem you
could do it with linear algebra but I
would say don't just shove it into the
nearest nonlinear optimizer you can find
providing that one uses second
derivatives or first derivatives wisely
and just solve it why am I going to sit
why am I saying that because all these
other steps down here are nonlinear
optimization problems and that's the
only way we know how to solve them
there's no real advantage in finding the
easy case this case orthographic is easy
completely uninteresting okay because
for almost exactly the same amount of
effort you can solve the perspective
case you can do the non rigid motion so
this is that Torresani the the leg of
the matador can be solved by instead of
embedding into our three or are for
embedding in two or three K where K is
some parameter you set by inspection so
all of these problems you can solve what
did we do we said the stuff that we're
looking at in the world doesn't really
live in r3 we don't look at a 3d we
think we're looking at a 3d world
certainly my face appears to live in r3
but any surface in r3 is really a
mapping from r2 to r3 ok so a surface
maps from r2 to r3 and then I look at it
in this video at which point I mapped it
again to
are Tootsie are 250 and are 500 in our
example so what we did was just learn
the mapping directly from r2 to r2 t and
no loads of algorithms that will tell
you how to find embeddings linear or
non-linear from low dimensional spaces
high dimensional spaces what happens
when you perform this embedding on the
data from the face so here just playing
on the left the input tracks here on the
right you can see that what we get is
the natural way for those points to live
in r2 is to sort of unwrap the face ok
so for those points to live in our to
you kind of need you know the right ear
to be close to the right cheek to be
close to the right right hand side of
the nose own you'll have to can see I'm
getting all of these wrong etc etc so
simply forcing the points to live down
in r2 gives them a very natural
coordinate system in which to live and
once you're in this coordinate system so
now we've got a very sparse
reconstruction but because we're in this
nice clean natural coordinate system
it's easy to go back through the
original video and drop down the pixels
so that we have a sort of a dense
reconstruction of what's happening and
then once you've got a dense
reconstruction you can play with it
because this represents now the entire
video this one image so if I paint on
this one image and then replay the
painted image back onto the original
sequence you get the effect that we had
so we have an effect that looks as if
we've got a 3d reconstruction but in
fact it's achieved without ever going to
3d here's another example so we have a
boy walking through the woods and you
know as you can see he turns slightly to
the right this is showing the on the
right hand side of the screen we show
the reconstruction we paint on the
reconstruction the 2d reconstruction and
then again we could play it through and
it looks as if we've painted on the 3d
surface of the of the of the subject
okay so that's fine so don't go to our
three you're trying to celibacy the
problem went down to our to you know
that that is we claim works better so
now I want to do to look at a slightly
harder problem everything we've done so
far is based on the idea that well has
been based on two ideas number one
that's been video ie images close
together in time and number two there
have been these point tracks that you
can easily obtain so as you probably
know if you've ever seen inside a
computer vision lab computer vision labs
tend to be covered in posters highly
textured posters at jaunty angles all
around the walls so that it's easy to
detect interesting points in the world
but you'll notice that the average
object we see so it turns out I didn't
wear any textured clothing today that
wasn't intentional most of us are not
wearing highly textured clothing most of
the world is not full of these
interesting matador trousers and easily
detected points so I've been interested
in a while in what we can do in the real
world the pointless real world as
opposed to the highly textured world of
computer vision so to set a nice hard
problem what I would like to do is type
into a search engine an image search
engine the word dolphins and it would
give me some data that looks like this
and what I would like to do with this
data is somehow figure out what is the
common 3d principle that generated these
images okay now i'm not going to deal
with i'm just going to deal with the
silhouettes of the objects so i'm not
going to deal with the background so i'm
basically going to say that in this
image i have a bunch of shapes which are
sort of like silhouettes so here's a
silhouette here see you absolutely sure
what is the the 3d model class which
could have given rise to these images
you can't do this with a connect for
several reasons number one they're
outdoors no air so and they generally I
guess they bounce around in night time
as well but let's say that they only
bounce around the daytime they're very
big they are very shiny which is not
good you could probably solve this
problem by setting up this big multi
camera rig in some dolphinarium but then
suppose you want to watch dolphins in
the real world or underwater I'm going
to try to argue that it's much much
easier to have a set of 2d images than
it is to have any sort of big 3d break
and you know dolphins are nice and well
behaved but what if it was a
hippopotamus which are notoriously
cantankerous animals you probably
wouldn't want to be near them so but we
know also the humans can do this right
because humans can build statues of
these objects so humans can obviously
work out the 3d shape from presumably
only long-range 2d views although they
may have looked at their dolphins and
humans of course make a classic
something you often find in 3d
reconstructions where you've got
accidental ancillary information on the
shape so humans seem to think that
dolphins are attached to two mermaid
swimming for some reason but I'm sure
they can recover that okay what do I
actually mean what do I want as an
output well what I would like is to be
able to process the images that I just
showed you so in fact I showed you about
8 images for this experiment we had
about thirty three images and I want to
build some sort of 3d model which has a
sort of a central average model that is
pretty easy to get but which will
provide me with a range of sliders that
will allow me to sort of explore the
modes of variation of the shape of the
of the class the object class that I'm
looking at one thing you'd expect this
model to be able to do a good job of is
to superimpose so given a new image of
objects in this class to be able to
recover the shape that best matches the
silhouettes that we have in that image
what we're seeing here where we take the
image determine the slider parameters
for the new image and now we can sort of
recover a 3d representation from a new
2d image of what's happened ok so that's
what i want what i want is to build that
3d parametrized shape thing and i'm
going to try and give you a little bit
of a insight into how exactly we did it
so first of all what is this 3d
parameterised shape thing going to look
like well there's something that works
surprisingly well what I have here is
here is some arbitrary 3d shape this is
from an experiment we did on pigeons not
on dolphins as you probably might have
figured out that 3d shape is represented
by some list of 3d points right so I'm
just going to say there's some vector x1
x2 x3 right and they're arranged in some
topology that I know so I know the
triangle I know the that the triangle
123 is part of the model but essentially
the model script xn is just a big
concatenation of 3d points x1 down to X
sorry X V where V is the number of
vertices so let's say it lives in or I
don't know 300 so maybe we have a
hundred vertices on our control mesh
that's the thing i'm going to try and
recover is these X's i'm going to have a
different 3d mesh for each input image
the problem would be massively under
constraint if that were that were really
all I were looking for so what I'm going
to say is that for any given instance n
n for instance that's how you can
remember it it's going to be the model
we've got is just some linear
combination of vectors which are exactly
the same so that's also in our 300 the
first of those vectors if we insist that
this parameter equals one will look
quite like something that you might
think of as a mean shape so you might
think that b0 is a template me average
pigeon the other elements b1 and b2 will
tend to be
offsets of that mean shape so be tues
nose is red which means this is the
dimension which makes the nose longer
and shorter I'm track b b1 and b2 are
both doing the job here of making the
nose longer and shorter so these are
offsets that we would apply to the basic
shape to generate a new instance from
the model and I'll sort of compact that
up here to say that model instance xn is
a linear combination of basis models and
that's what the sliders on the dolphin
video were changing they were changing
these values alpha and generating us new
new dolphin models as they did so we
don't know these a pro I right we don't
know the bees we don't know the alphas
so they're going to emerge from the
process yeah but good point we will
initialize b0 with something that looks
a bit dolphin-like but but they
formulation it can start from a sphere
it just doesn't work very well but yeah
thank you a good point we don't know any
of this it's trivial if you know the
bees first right so that's the type of
model we're going to use linear
combination of basis models ah so let me
run way ok so that's what I'd like to do
I'm not going to take you all the way
there yet because it turns out to be
fairly complicated so we're going to do
one step that we worked on in between
which kind of gave us intuitions about
how how to how to get to the the final
problem and the in-between model that we
looked at is this thing we call wire
frame or armature models and an armature
model is just some sort of 3d model that
I can treat as a collection of 3d curves
so not as a surface but as a collection
of curve so here we have this this shape
on the edge here where the essential
three dienes is these curves do such
things exist in reality well it's a bit
of a stretch you imagine this is the
part of the research where we know we
want to deal with the real world but
we're going to
with some special case because that'll
give us some intuition so do they exist
in reality well yes we're going to model
the variations of a lily petal purely by
modeling those three curves on the lily
so the the three curves drawn in black
there and we're going to be found this
class of animals the clownfish where we
would say that the clownfish is well
modeled by the wireframe of these
internal curves and the reasonably sharp
external ages here so we're going to try
and build a 3d model just of those
curves and then fill that in to render
it so for the lily example we would have
training images that look a bit like
this this is just some collection of
lilies if you ask a computer vision
person they'll tell you yeah they could
probably build an automatic lily outline
extractor we didn't we just drew them in
by hand and so we sort of taken the
images and just turn them into sets of
of wireframes by drawing around them by
hand like that I'm trying to argue that
that's not very much effort because I'm
doing it in front of you of course you
will easily find a computer vision
person who will automate that fear what
is going to be our 3d representation
well as I told you we have this x.x
calyx or script X is always going to be
the current 3d model a 3d model of the
current view here the 3d model is
represented in a funny way I told you
there were three curves but in fact the
model is represented as a
two-dimensional grid of values and the
curves that we're interested in or just
these curves on the model parameter
space so for the moment again the model
of this image on the left is just some
set of 3d points just think of it as a
set of 3d points which will ultimately
correspond to 3d points as I move along
the contour here that will be those
three points if we knew right so suppose
i'm going to set up the problem that we
have many images like the ones on the
were left and suppose that we always
knew that this point was this point on
the model okay so suppose somebody had
labeled our images with exact
correspondences between points on each
of the of the contours then we would
have to solve a problem which is
actually really quite easy okay it would
be exactly the same as the matador
trousers because we have points and
correspondence so specifically this
number here is a point in our two it
represents for the enth instance ie the
enth training example the 2d measurement
corresponding to some point in this UV
space let's say this one so the 2d
measurement here corresponds to a known
point in the space and what we want to
do is explain it by saying the camera
for the enth instance viewed that linear
combination of basis models index to
point UV okay so so this little point in
here BK UV is a 3d point in the in the
rectangle that I've just circled goes
through a camera this little function pi
says it could be nonlinear don't worry
about it and then it lands on some 2d
point this Phi here is a value which is
0-1 saying I don't know any values in
here okay so i added up as if I knew the
correspondence of every point but in
practice I can't see some of them okay
no problem this is you know a few
thousand variables you can pop this into
any optimizer you don't even need to go
to lunch it'll be ready in a few
milliseconds okay now just to show you
the sort of essential message that I
want to get across here I'm going to
simplify the problem all the stuff that
we already know is packed up in this
function wnuv the stuff that we don't
know is represented in this vector of
parameters theta so this vector just
represents the shape the basis shapes
they the blending functions all of that
software just packed up in here because
I want you to look at the essential form
of this problem there are some
measurements that I know there's a
function that I know there are some
parameters that I don't know and these
measurements have precise correspondence
UV so this point corresponds to this
point so what's going to happen when in
practice all I know is that this curve
corresponds to this curve right what's
going to happen in practice is that I
can still pick one of the points on the
surface but I don't know where it is
along here okay I need to introduce a
new value which i'm going to call let's
call that point the UV point I'm gonna
have to include introduce well what I'm
going to say is that this point should
match to the closest point on the image
curve so going to change my energy
function very slightly it used to say
exact correspondence between an nu V and
a predicted nu v now there's a minimum
inside the energy all right so I've got
to do a min here over a function which
includes a min in it generally very
nasty right these functions these min
functions that I have in here generally
don't behave very well so let me show
you a really easy trick to solve these
men problems problems so I'm going to
again abstract out the problem we're
looking at you are told to optimize a
function which is the sum of minimum
minimum minimum problems here's the
thing you can do for free rename this
variable because no one can see it
anyway outside this song so rename that
variable to TN we have done absolutely
nothing this is a strict equality but
once you've renamed it you're allowed to
take all the TNS out here just because
to the sum of two independent
minimization problems is equal to the
min of the some of the problems because
they're completely independent because X
is not at all visible in here in fact I
can drive it for you easily here's the
initial minimum problem I want to look
at this function is completely
independent of X so i can do that
grouping without changing anything and
this term is completely independent of Y
so I can put that in here without
doing anything giving me this okay so
men some of mins of independent means as
a min of songs so instead of solving
this problem which was typically let's
say a few thousand parameters I'm going
to solve this problem which is typically
a few tens of thousand parameters and
this one's going to be way faster than
this one okay so let me just give you a
even simpler example of how this works
so this is a problem I worked on in the
early 90s when we all thought it was
very interesting given a bunch of points
like these black points here which were
drawn from a little shape and ellipse
find the ellipse which best explains
those points so you're only given the
black points and you're supposed to
recover the ellipse and back in those
days we were very pleased because we had
fast linear algorithms so order and
algorithms which would take the black
points and return the gray curve and say
so the black curve is the answer we give
the ground truth answer we could return
the grey curve using a one piece of
linear algebra an order n algorithm
where n is the number of points in your
data set it was known that you could get
a much better solution so the green
curve here is what became known as the
gold standard solution and you would
obtain the gold standard solution by
finding the ellipse which minimized a
sum of minimum distance of the points to
the ellipse so the the green curve is
the one which has the lowest some of
minimum distances to the ellipse so
there was an algorithm that you could
build to do that right so here's the sum
it looks just like the one we had some
over minima and the algorithm that any
fool would think of when they first
looked at this problem is first of all
we'll take this initial closed form
solution oh sorry you can't see there we
take the closed form solution and then
for each of my data points I'll find the
closest point to the closed form
solution and then I can do another none
in your optimization still with only in
this case five parameters
that's so running yes it's still running
um and I'll alternate so and you can you
know if you're writing an early paper on
this you can show a proof you can show
that at each point or reduce the energy
right it can't get bigger because I've
just searched over a subset of the space
I make it smaller the problem is it's
unbelievably slow right it's really
really slow because the optimizer can
only look at a subset of the variables
at each time so now if you do the trick
whereby you simply pull all the t's
outside so it's exactly the same thing I
just pulled the t's outside and fed them
into matlab's default nonlinear
least-squares optimizer you'll notice
that it minimizes exactly the same
objective it'll find exactly the same
answer it's just that whoops I had to
slow this video down by a factor of 10
so you could see what was happening so
the previous slide is the algorithm any
fool would think of we can play that
does okay early on you know just a
little bit of motion early on but you'll
notice the closer it gets the true
answer sorry not the true answer the
true closer it gets to the nearest local
optimum which is the dotted curve the
slower it gets second order algorithm
gets there so fast you have to slow it
down plotting the convergence as a
function of time so here is like you
know the function we're trying to
minimize it on the y axis log of time is
on the x-axis you'll see classic so
called linear convergence behavior
that's the slow algorithm that's the
fast algorithm this is about what's that
that's like 200 milliseconds or
something let's say point two seconds by
the time we're anywhere near there we're
at around ten seconds and what's worse
if we inspect more carefully what's
happening the optimizer the black curves
don't reach the any useful point until
much later about here so you might say
oh well nothing interesting is happening
because look that's all
was flat well you can test that by
looking at what's called a test error or
something that's analogous to this test
error which are the solid black curves
you'll notice that even though the
dotted black curve is almost completely
flat as it moves along out here the test
error is still dropping fairly
significantly so you do if you want if
you like the black algorithm if you'd
like the alternation algorithm and you
don't like the one that is simply chop
it into MATLAB and go for lunch you do
need to wait a long time it's no good to
say oh it did okay for the first few
iterations right okay good let's see
what we did so we did that with our Lily
problem and we got some reconstruction's
which are you know in red we have the
reconstructions that we got for these
shapes and in blue you have the
reconstructions you get if you assume
the correspondences so I'm going to
argue the red is better we did the same
for clownfish here there are some more
difficulties you have to deal with
because the clownfish is so an image
curve like this one this green one is
not we can't see all of the image curve
it doesn't matter we're chucking it into
a non-linear optimizer anyway you just
set some of the weights 20 it doesn't
care all the closed form etc solutions
go completely wrong with missing data so
you can't adjust them to work well you
can but you'll end up with some stupid
algorithm which is a variant of that
alternation algorithm in nine times out
of ten there are ten times you can you
can look for better algorithms there
will be some out there so from matrix
calc matrix factorization which this is
an instance of you can look at the bburg
algorithm with viborg algorithm viborg
algorithm probably better than the
non-linear optimizer right but most
other there does a massive literature on
this most of which are variants of
alternation and not very good ok so for
fishes are we can do okay
so you know six correspondences on the
left our fish is on the right fixed is
bad ours is good just in case you're
having difficulty interpreting the
results incredibly low res etc etc but
you know theirs is worse fine ok so
we're going to do some fish they work ok
let's go back to the Dolphins ok so we
have a few minutes um I'll tell you
about dolphins here are the input images
here's the data we're going to use from
the input images the most important so
I'm going to say that there are no point
correspondences at least with the
armature models we had the idea that the
clownfish ribs were considered whitwer
something that we could hope to model
across the family on the dolphin there
are some point correspondences right
nose tip of various fins but they're
incredibly few certainly no way we could
hope to build a non-rigid model from the
point correspondences from the dolphin
so the main queue we're going to get us
to the 3d shape is going to be the
dolphin silhouette and that's the red
curves superimposed on these images
which I obtained using PowerPoint at an
average of 1.5 mouse strokes or mouse
drags per image so a sort of
semi-automatic algorithm to recover the
silhouettes I'm sure you could find a
computer vision person who would recover
them automatically I'm not interested
but I'm very defensive as you can tell
so you know we did it manually get over
it we do have key points as I said you
can get maybe five or six key points on
every image of the dolphin nowhere near
enough to do the point based algorithms
but really really handy for initializing
the model and getting roughly in the
right place so we're very pleased we
have those key points I click them
manually I'm sure you could find a
computer vision person who would claim
to detect them automatically there's a
bit of kind of messy geometry to do with
silhouettes are actually kind of a pain
to deal with so in the image here
I have the the silhouette SI so this is
like the i'th image the the SI is a
curve the curve on the image came from
some unknown point on the model so this
point on the image came from some
unknown point on the model we also don't
know that April all right so the point
on the model the set of points which
gave rise to the image silhouette called
the contour generator needs to be part
of our set of unknowns but they're
actually okay they're a bit like those
tees when I didn't know where I was a
long low along the rim of the sorry the
rib of the lily so we have some 2d
variables essentially 2d variables you
which are other stuff we don't know
which is for each silhouette point what
was the UI j SI j what's the the contour
pre image point and they've got little
halos on them because they live in a
kind of messy looking parameter space to
do with subdivision surfaces but let's
not worry about it and then the model is
going to be a bunch of energies of the
form J silhouette point in I finish
should be the projection through the
i'th camera of the UI j the contour
preimage given by the current guesses to
the 3d model push through some more
nonlinear mangling and there are a bunch
of terms like this so this says points
on the silhouette should be explained by
the model there's another term which
says that the surface normal ie they
normal to the curve in the image should
be like the normal to the model and is
some function we've been given which
computes normals to the model each of
these functions is probably 10 lines of
matlab I keep saying they're not linear
they're not that bad it's fairly easy to
compute derivatives just to frighten you
there's a load of terms from the model
they're all fairly okay anyway it
doesn't matter because we're going to
chuck them into MATLAB let's see if it
works Oh as I said in answer to your
question we do need to initialize this
with a rough model this is when we
downloaded from turbosquid you can also
make one using sketching tools so this
is a tool where you can draw a little
boundaries in 2d and it gives you a
messy model like this this model works
ok
I could also I could also be a little
bit cheeky I could show you this model
that I showed you it turns out we also
converge if you inflate this model or
keep the triangle topology the same so
actually what I need for the initial
estimate is not so much the model itself
but the fact that that vertex has three
neighbors and that vertex has six
neighbors it's the model topology that
actually matters but the only sensible
way to build a model of the quite
correct topology is to make it look like
a dolphin so really you know I could
pretend to you that it converges from
this near sphere but that would be
pointless fine so we initialized it
let's let it run okay so so let's look
at it first here I have six of the
images in each case I have that rigid
model my initial estimate for be zero
it's not going to stay the same it's
going to move and I've used the five or
six control points to work out roughly
the rotation and translation did the
orientation of the model so you can see
that we have the same rigid shape is
approximately overlaid on the input
images the red dots on the input shapes
represent the current guess as to what
the contour generator is for this image
remember the contra generators the set
of points on the surface that give rise
to the silhouette so that is initialized
in a fairly clumsy way to be a sort of
closest point algorithm so I'm going to
iterate your closest point algorithm the
way I told you not to and then hopefully
I'm going to explain why that was okay
so let's watch the thing in operation so
what we're going to see here is
iterations of the nonlinear optimizer
and then when it pauses it's doing a big
discrete optimization to to realign the
silhouette the contour generator but
very importantly the contour generator
is also moving on the surface of the
model during the optimization so I'm not
fixing contra generator solving from
model that would be unbelievably slow
there's a smooth movement of the contra
generator points as I optimize
and things I want you to see it just
that really horribly mangled shapes like
the one in the bottom center of the one
the in the bottom right are fairly
easily sort of pushed back into shape
because another trick whenever you add a
load of dimensions to your model you can
have a rough feeling that optimization
in a much higher dimensional space is
often somehow easier somehow if there's
a look what looks like a local optimum
here and what looks like a local optimum
here in high dimensions I may be able to
find a way around the side so so the
thing the message is that reasonably
okay initialization decent nonlinear
optimizer will actually get you out of
trouble a lot more often than you might
have guessed some soft to say was worth
doing loads of stuff um some stuff to
show that we could make it work for
pigeons in this case we just manually
remove the legs I mean digitally remove
the legs because we don't know how to
deal with those so the pigeon model has
fewer parameters than the dolphin in a
sense there are fewer ways the pigeons
can change their shape as a fat pigeon
or something and it doesn't work when
your object class is articulated so
wouldn't work very well on humans
because something like or on the polar
bear because when he bends his his limbs
that really doesn't fit in well with
this linear combination of basis shaped
models so we're kind of getting these
really horrible you know it roughly
explains the silhouette but it doesn't
really give you any feeling that it
knows what it's like to be a polar bear
oh this is just a it works on bananas
here you'll notice there's only one
slider because essentially the bananas
don't change very much the reason this
is interesting is this we want to
emphasize that the state of the art
before we did this work was didn't even
work on bananas
so it was just essentially nothing for
non-rigid from silhouettes so so we were
very pleased with ourselves blah blah
blah it's not that sensitive too well
there are a bunch of parameters in the
energy most of them are the first boxes
parameters that you can set by just
knowing about images the second box is
parameters with things without
parameters the third box is stuff that
matters so there are two main parameters
to the algorithm and what we're showing
here for each class that we looked at as
you vary these two important parameters
what happens to your reconstruction so
oops sorry so too low a value up here
gives you a horrible mess that's the set
of values that we liked for showing our
answers again something you'll often see
in well-written computer vision papers
is we use the same parameter settings
for all the models in our data set
whereas i'm showing you here that we
didn't use the same parameters we use
the ones in the red boxes for the three
different classes why do I say that well
I say why are we doing this the only
reason we're doing this we're doing this
for a few reasons number one
scientifically is there enough
information in the silhouettes to get
some shapes out number two you might
actually want to use these shapes you
might want to explain something about
images you might want to be able to
re-render there's a bunch of stuff you
might want to do with these shapes once
you have them almost everything you want
to do with them is not about actually
measuring the tip of the dolphin's nose
to the tip of its tail or whatever you
know you might think that there's some
science you could do here and you could
certainly do it in the sense that you
could form those measurements but a lot
of the time you just want to look at the
answers and the way you should do that
is run nine different optimizations on a
cluster generate nine different
presentations of the results for the
Dolphins here and you should pick the
one that looks best for your job I'm
sure you could find a computer vision
researcher who would devise you a
principle whereby it would find the best
one blah blah blah right don't do it
just generate lots and pick the right
one conversely i could just as easily
have picked this pigeon
this pigeon actually looks fine compared
to this one we like this one slightly
better but this one's really fine so I
could have shown you the results with
where all three are taken with this
parameter setting you know I could have
said oh it's fine there's no sensitivity
the parameter is just you know we use
the same settings don't don't bother
just look at the answers and pick the
one that's right fine okay um I'll stop
there thank you thank you Andrew
questions fine drew your brief someone
working in related field perhaps it's to
take the opportunity questions everyone
stunned thank you well maybe you
mentioned that but that of the
parameters come from which parameters
are so for example with a dolphin
example you had a number of sliders that
you could Oh modify modify the shape so
they are just part of that so
effectively we have a huge bunch of
parameters which explain how this image
could have got there right one set of
parameters for each image we have these
basis shapes and they showed you them
for the pigeon every image has its own
different combination of the basis
shapes okay and we solve for those those
are part of the model but then once
you're finished once you've got the
model you can make take a set of sliders
and you can say what would happen if i
mix basis shape zero basis shape 12 in
some different way right so what would
happen if i try to generate shapes that
are not in the original set so where do
the sliders come from they come from
they were all the parameters for the
sliders were part of the model and are
optimized at the same time and then the
sliders are just another way of kind of
sanity checking you know did did the
shapes that you got simply explain the
image data or are they useful in more
other ways as well but otherwise we can
do that I just leave one out tests or
you know they
they were all parts of the optimization
yeah thanks what kind of masa to you
used to get to find the correspondence
between the point cloud in a first
example oh sorry the first dolphin
example the first know the augmented
reality oh sorry okay uh oh I see oh I
don't know can't remember um I don't
know you know brief sift or something
like that push me what do we use
tracking point tracking in in unwrapped
mosaics I can't remember some patch
correlation yeah yeah okay sinks some
computer vision thing um what happens if
you eat from the base mesh or the
initial the main makes for example you
get a laplacian graph from the
connectivity and then you can recall
shock new models by paramore trying or
by using the laplacian with the points
on the curve to obtain something that
fits the one image yes good question
it's in the paper or something pretty
closely equivalences in the grave paper
using a finite element mesh it's quite
interesting because you can do that on
each image independently so
independently on each image you could
say just give me a smooth surface that
has this silhouette and then you could
say to get the sliders to get the you
know the combination you would then have
to put those measures in correspondence
it turns out to putting the meshes in
correspondence is actually not that easy
you know so it's like I stick this hand
into a model I get some polygon mesh
stick this hand I get some other bunt
mesh it's not actually that easy to put
them in correspondence you have another
correspondence problem so our argument
was it's just as easy to solve that
correspondence problem at the same time
or are in fact better to solve it at the
same time rather than first reconstruct
then then
but yeah good last question perhaps
everyone happy thank you very much thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>