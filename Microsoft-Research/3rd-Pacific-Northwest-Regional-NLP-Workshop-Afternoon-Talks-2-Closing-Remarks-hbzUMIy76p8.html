<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>3rd Pacific Northwest Regional NLP Workshop: Afternoon Talks 2 &amp; Closing Remarks | Coder Coacher - Coaching Coders</title><meta content="3rd Pacific Northwest Regional NLP Workshop: Afternoon Talks 2 &amp; Closing Remarks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>3rd Pacific Northwest Regional NLP Workshop: Afternoon Talks 2 &amp; Closing Remarks</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hbzUMIy76p8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay I everyone we're going to start
with the last session of talks three
talks and I'm sure you still have some
mental energy to kind of listen and
mainly like asking questions at the end
so the the first talk is unsupervised
dependency parsing with transferring
distribution via parallel guidance and
entropy regularization and the presenter
is max it's jus drama and then from
Department of linguistics University of
Washington and our work is about
dependency padding for result program
images which have no label Union data
and our idea is to use parallel parallel
and label data to transfer distributions
from the party model of resource rich
language and they say is the groundwork
with my advisor Lisa this is the after
land first I will introduce our task and
I learn have you describe our approach
and through our experiment and at the
last have you get our conclusion and
discuss some possible future world first
let's see what our top case here is an
example of dependency tree it is
protective meaning that there are no
crossing edges in this street sometimes
we can use them some labels on the add
edges to represent the dependency types
but in our work we do not use at labels
in our scenario we assume that we have
three kinds of resources first we have a
resource rich language with a labeled
monolingual treebank we denote this
language as sauce language and second we
have parallel text for a parallel text
between resource rich and result /
languages and third we have a label text
for
resource / languages and we expect to
develop a monolingual pattern here
modeling go means that the test data are
in only one language do not have to be
begging go text and mmm and this the
last gives some related measures we have
no time to go into the details if you
compare our systems with all these x18
see terms you following able to describe
our approach our master can be
summarized as estimating the
transferring dispersion from the from an
English patent model here the
transferring dispersion is the
distribution protected from and English
class model review tsk this review
described this in the future and by
using the transform dispersion we can
transfer cross-lingual knowledge between
the target and the source languages the
passing model we use is the ID factor
factored packing model which is a kind
of log linear model here X is input
sentence there is a valid pass TX is the
set of all possible dependency path
passes for input X F G other fixed
functions and lambda are the parameters
of the passing model that we need to
learn which function for the rich
function 48 h e it first some some or
other features belongs to the ed with
the current parameters lambda and then
it takes the exponent function by using
the weight function the conditional
distribution can be written as the
multiplication of of each edge each
edges wait wait and here the X is a
normal normalization factor in
supervised case a common method for
model cleaning
to minimize the net negative likelihood
function with respect to a set of labor
union data but in our scenario we do not
have labor looking in data so the way we
used to serve this problem is to
minimize the expected next negative
likelihood function where the tilt p is
the transparent its future that reflects
our intensity and certainty about the
true labels as mentioned above we assume
that we have a set of parallel theta P
and a set of label sentence you so we
can divided our we can divide our
objective function in 220 ohms k p and q
where KP is the contribution of the
parallel data and key you is a
contribution of unlabeled it here we
introduce a parameter gamma as a
trade-off between K P and Q the next
problem we need to stir is how to defend
their transferring disputed for a label
data we takes the transference its
fusion to be the electric current
current dispute in our passing model
which is P lambda it is reasonable since
that we know nothing about the
dispersion of the unlabeled data but for
parallel data we defend the transferring
dispersion bed defending the
transferring weight function here tails
w is the transforming wave function and
w.e is the weight function of English
patent model P lambda e + ET is IH in
their target parts so if ed is a land to
IPS in the source said then it's
transferring based on width is equal to
the weight of the bait of is
corresponding at yes in English patent
model otherwise the transforming bait
for
he is equals to is the electrolyte form
so here is an example to interpret the
definition of the transfer material
suppose that we have a pair of sentence
and here German is their target language
and the lens the lens between the two
sentence he knows the word alignments
since the world lemons are generated
automatically sold they are not perfect
now suppose that we have let's suppose
that there are two edges in their target
part according to the water lineman's
they are land to two corresponding edges
in their English sentence so according
to our definition they're transferring
with equal to the weight of their
corresponding ideas in the English
patent model so here we use different
colors to distinguish the different
edges and addicted the different
equations but for the to read the two
red edges there they are they are they
are land they are not to learn to any
edges in the English side so for these
two edges the transferring base function
equals to the weight of their the
electrolyte form the turkey legs for
example for this edge we only we keep
only their part of speech tags and drop
other lexical information so the the
optimization algorithms we use to
optimize our object function is the
limit in every bfgs and we calculate to
calculate K prime function its gradient
we use the inside outside of resume the
complexity is 0 and Q
your next station will show the result
of our experiments the tree banks using
our experiments for governor with Audrey
banks both watching band version 2 and
Lori banks from conocer tasks for
parallel text we use europea corpers
version 7 for European languages and use
the kissed corporate for Korean the end
the a liberal detail come from the
training portion of entry bank forward
alignments we use is a proud + + 4 part
of speech we use the universal pad part
of speech text cite proposed in Petrov
2011 and we used the central part of
Seville tiger this last gave the systems
we ran in our experiments for comparison
PTP is the direct transfer passer
proposing to download subnet 2011 we
reemployment this system and the result
of May of our explanation marked as dtp
tag PTP is the protected transfer passer
and we also reemployment safety term and
the notice it as PTP tag the negative
you and positive you are two orphans of
our approach for the neck to view our
approach cleaning only parallel data and
the positive you and training on both
parallel and unlimited or is there fully
super wet pattern which can be regarded
as oracle here are the results on google
universe or two banks version van the
left three columns are the be selected
terms the two columns in a limit ok the
two columns in the middle the two
versions of our approach and the ORR
Oracle's you can see that our approach a
significant delay up from all the beta
line three terms across other five
target language
and here are the results on the Google
nervous or Ebanks watching band and we
can get the same thing with the the
result on version on version one we also
do an experiment on aunty different
amount of parallel parallel text this
figure is the uas on German here the x
axis is the number of parallel sentence
and x axis is ues and produced blue bar
other results of project project a
transfer pastor and orange bar and group
are other two versions of our approach
for for other target languages to be
obtained the scene observations we also
run experiments on the cue banks from
Hoonah share tasks here other result for
8 target languages here DMB is a release
unsupervised depend upon model and the
four columns in the middle other piece
and I see terms here is our result and
the ORS Oracle you can see that our
without hours in term or gives the best
result across older 8 target languages
and we under experiments or photo
experiments on who knows true banks we
compare our results our compare our
seeds terms with more comparisons in
terms and we compared with the big lead
supervisor see terms the filler filler
genetics grammar inducted action model
and the unsupervised passing model with
non-parallel motoring or guidance and
the posterior regulate regularisation
approach table for other us on sentence
of last 10 or lives without punctuation
is from kenosha at events
here the four columns in the middle
other new compression systems and next
to you is our approach we can see that
our approach achieves the past three
without our most target languages
inclusion thicker work so in our work we
propose you know and probably proposed
approach for the dependence patent for
visa / languages which do not have a
label the training data and our our
approach can be used you know completely
model you know Santa sighting Anna our
approach to achieve batteries past
parking promise on 3d SS so prefer for
future work first when we can you can
extend our approach to none protect
people passing by replace the Shanghai
truTV's really entropy and another way
is to extend the transferring ways to
hair other parts by transport by
defending the transferring wait to hair
to the parts that involve small ideas
and another way is to replace the
parallel test with edited data so that's
it thank you there's any questions
okay I've actually have one question you
mentioned at the beginning this
parameter gamma that is trading off
between two probabilities so I was
wondering how you are the truth the
camera are you saying that or are you
set all right yes for the perimeter
karma since that we assume we have no
label label data so we do not want to
ruin the gamma using the developments
that so in our work we choose the team
according to the ratio between the
number of tokens of their parallels
detailed unlabeled data so for example
if the parallel data has 10,000 tokens
and the unlabeled data has has for
example 10 times of the parallel data
than the gamma which was a gamble point
worth
very interesting work I was wondering if
you happen to look in the amount so the
amount of improvement you get is a
function of the specific target
languages you used in terms of how free
or order they are compared to the
English which you probably used there
any correlation between some
characteristics of the target languages
like like a free order or not reorder or
the extent of the freer order they have
compared to English sorry if the
question is the serve I don't understand
your meaning of the order so English for
example it's it's it's very the order of
language of the word order is very rigid
this is how you know basically the
subject versus object because subject
side would come before the verb and
object would come after in other
languages you might have other cues like
morphological inflections and so on
maybe it doesn't matter for your
approach maybe does I was just curious
if you add I think mmm i think you know
we have the experiments on Korean I
think Korean and English have different
with others so and you can see our
results and cream here so for cream can
see we also we can see that for Korean
if you use the direct transfer pattern
then the partner Kristi is pretty low
that be if we use our approach we got
significant improvement so I think so i
think since that the world may affect
the accuracy of the water linemen so
maybe less
vantec to let me lo our accuracy so but
I think for our approach is not be it
not be a big big actor super quick one
so if you make the word alignment worse
with them Brian McDonald model do better
because they have a sec subsequent
training that doesn't entirely trust the
word alignment right if I remember it
correctly but in your case you mean the
metal outward Portilla interpreter yeah
I think I create I have not think about
this but you know McDonald host work is
a kind of weekly supervised learning so
if the woodline meant I was worser than
now I think it yes of course it will
lower the accuracy of the blow departing
accuracy but I don't think that our
approach for you I don't think this will
be a bigger factor for our floor since
that
so let's thank our speaker so the second
paper for this session is graph-based
posterior regularization for
semi-supervised structure prediction and
is presented by Lou hanga Hey ok you
hear me ok yes it's working is there
Mike it's all oh ok yeah hi i'm luhan
i'm a student a lead up today i'm going
to tell you about how to use graph
obligation to help semi-supervised part
speech tagging and especially how we did
it by simply optimizing a drunk
objective function this is joint work
with Jennifer Gila water and Ventus car
and this work has originally be
presented econo 2030 here's an overview
of my talk today first I'm going to talk
about structure prediction in particular
conditional random pho with its
application to part speech tagging and
then i'll talk about grad propagation
which is an alternative approach to the
parts with chugging it's an exam you
supervise and instant instance based
method and finally i will talk about how
we successfully combine these two very
different methods into a single joint
objective function that can be
efficiently optimized so i guess most of
you are familiar with part speech
tagging basically for each word in a
sentence we want to assign to it a
syntactic category such as a determiner
along a verb so for each instance we
have the input X which is the simply the
sentence and the output structure why is
the sequence of parts which tags
conditional random pho is a standard
technique for doing part speech tagging
and its tagging prediction is based on
the feature function f that is defined
on each of the local factors in the
for structure why it usually contains
the current tag YP the previous tag YT
minus 1 and the input X and also we have
a set of parameters theta which are the
future awaits so u theta and Fisher
function f we can compute the potential
scores for each of the local factors
therefore we can model the conditional
distribution and for up houseboat a
consequence given an input sequence X in
particular p theta of Y given X is the
conditional disability of a particular
tagging sequence white given an input X
it is computed by taking the product of
other local factor scores and normalized
to sum up to 1 so therefore learning the
conditional random film model can be
formulated as an optimization problem
where we want to choose the bad
parameters theta to minimize the
negative log likelihood of other label
sentences from x1 through XL and
different from conditional random felt
graph obligation is an instance based
method and it tries to predict one tag
at a time without considering an
instruction information in our
particular setting each input is a
trigram in the sentence and the output
is the parts of each type of the center
word in the char Graham the basic
assumption is that if two words
frequently co-occur in similar context
then they should be assigned with
similar parts which tags so to apply
this assumption we start by building a
sparse k-nearest neighbor graph using
other trigrams that occur in the corpus
as notes and for each pairs of trigram
we compare the distribution of
similarity using the words that's around
in the context of the trigram and inside
the trigram and for trial m star similar
enough we add an edge between them with
edge scores representing the similarity
scores so if a trigram ever occurred in
a labeled sentence we consider the
corresponding node as labeled otherwise
it's allowable then we can propagate
information from those labeled notes to
the allowable note and this gives gives
us a tagging distribution of the center
word for each unlabeled trigram and
formally this tagging distribution is
determined by choosing the one that
minimizes its difference with its
neighbors a final step is to sum sum
over the entire graph and write a
quadratic penalty term we call it a
graph laplacian regularizer this regular
rider forces the tagging distribution
for similar trigrams to agree with each
other so again graph obligation can be
formulated as another optimization
problem where we want to choose the best
tagging distribution to minimize this
graph laplacian regularizer term now
take a step back we talked about two
very different methods to do part speech
tagging the first one is conditional
random pho which is a supervised method
that learn from labeled data and models
the conditional distribution of a
possible taggings given an input
sequence and its parameterised by theta
and we also talked about graph
propagation which builds a key nearest
neighbor similarity graph from labeled
and unlabeled data and propagates
information across that similarity graph
it models the tagging distribution for
the center word for each unlabeled
trigram in next few slides I will talk
about how we can combine these two
methods there is some prior work that
tries to do this the work that most
related to us is by subramanya at all at
a MLP 2010 the years and iterating
method that first and trying a
conditional random filled from the label
data then use the CF prediction to
initialize for the graph propagation and
then use the graph propagations results
to provide additional training data for
the CRF and this goes on for many
iterations so this work is empirically
successful but it also raises several
questions such as is it optimizing some
joint objective or does it have
guaranteed to converge and this question
actually motivated our work to build a
joint framework that has nice guarantees
a first step to do this is to introduce
a set of observed variables q you can
consider it as a copy of the conditional
distribution as P theta so in particular
qi y malos the conditional probability
of a particular tagging sequence why
give an input X I and we make to further
assumptions on the conditional
distribution q1 that because Q is a
probability distribution it should be
normalized to sum up to 1 for each
sentence and second we assume that Q can
be decomposed exactly the same way as P
theta does so we can write Q each qi y
as a product of a bunch of local factor
scores we call them are and later we'll
see that these assumptions are crucial
to our efficient optimization now with
the auxiliary distribution q can write
our joint objective function it consists
of three parts the first part is the
graph laplacian regularizer that's
defined on the distribution q and the
tagging distribution for each trigram is
computed by normalizing q with respect
to each trigram in each tag and the
second part is simply the objective
function of the conditional random felt
defined over p theta so if we optimize
the first two terms separately will get
two very different distributions that
capture different information that's
helpful for part speech tagging so we
have the final term that minimizes the
payout divergence between the two
distributions q and p theta basically
this term forces Q and P theta to agree
with each other so that the final
prediction can benefit from both the
graph propagation information and the
conditional random fill information
so the final question remains is that
how we can optimize this joint objective
efficiently we have two sets of
parameters q and theta theta remember
are the future weights in the
conditional random Phil it's completely
unconstrained so it can take any
positive or negative value so optimizing
for theta is very easy we just do a
straightforward gradient descent method
however then updating Q is more
complicated for two reasons the first
one is that you should q is constrained
to sum up to 1 and second there's no
compact representation for Q remember
that for each sentence X i we the number
of our passport Agins y equals to the
number of our tax rates the power of
sentence length so basically there are
exponential number of components in the
distribution q and it's simply
impossible to memorize or update that
many variables however we still want to
do this remember we made an assumption
that Q can be decomposed as a product of
local factors are so it's natural to
think that what if instead of updating
those kill variables we want to maintain
and update those are variables and
Durham much less of those variables but
consider if we do an additive gradient
update upon that Q then what happens is
that the resulting Q prime will no
longer be able to be be able to written
as a product of local factors therefore
we need to use an algorithm called
exponential gradient descent so instead
of an additive gradient we do an its
multiplicative gradient update which
means we take the gradient term and take
its exponential and multiply it to the
original q and resulting the updated q
and when written as this form we can see
that both the original q and the
gradient term can be
decompose into a set of local factors we
are mental the details here but
basically we can then we can write the
gradient trim into a bunch of local
factors and distribute those local
factors on to each of their are
variables this means that instead of
updating other q variables we can we
only need to maintain and update those
are variables and then compute the Q
using a forward backward algorithm so to
summarize we have two sets of
conditional distributions Q and P
freedom and we have this joint objective
function we can optimize it using an en
style algorithm we're at m-step we
update theta and at each step we update
each local factors in queue which are
there are variables and we can normalize
Q by doing a forward backward and this
algorithm is guaranteed to converge the
local optimum now we come to the
experiments we tested our algorithm on
10 different languages from the cano
dataset and we tested under a weekly
supervised setting which means for each
language we use only 100 labeled
examples we use Universal part speech
tagging set so it's the same tag set for
our languages and for the base model we
use a certain order conditional random
field here's our results on the x-axis
are the 10 different language starting
from English and on the rightmost bar is
the average of other languages and the
birth show here are part speech tagging
errors so the lower the better the first
base line we're comparing against is the
grad propagation results which is simply
by minimizing the first term in our
objective function and the second base
line is computed by in training learning
a conditional random field from the
graph propagation result for one single
iteration it has some improvements but
not much and the third baseline is
simply by training attendance
random filled using the 100 label
examples so it actually outperformed the
previous baselines for some language it
tells us two things one is that
structural informations really important
the other is that graph propagation
provides additional information as well
as additional noise so finally we have
our drunk objective which combines the
power of the previous two mouths words
it outperforms previous baselines for
other languages consistently it has in
twenty eight percent relative error
error reduction on average so too as a
conclusion we want to use square
propagation to help weekly supervised
structured prediction in particular we
formulated this crop rotation method
into a regularizer term and encoded it
in a joint objective function and we
also propose an efficient algorithm for
optimizing the strong objective and it
is more interesting interesting to think
about that what other kinds of posterior
constraint that we can use to help
structure prediction we can use it as
long as it can be written as any convex
and differentiable rag the right to trim
our code can be found at the link below
you can try it out and fight with it
thank you for attention and I'm happy to
answer questions
any question
so am i right in thinking that you are
forcing the graph to do a prediction on
every single I um the chain it's not
exactly it's all the trigram so if two
I'd positions have the same trigrams
they are combined into the same note and
but in your in your joint objective
mm-hmm could you use the CRF for known
words so I guess another would ask the
question is that the graph in the
subramanya work would be sparser doesn't
have to predict on everything right
because I dig the average they sort of
do self training in order to combine the
prediction of the graph and the CRF but
as you have a full joint model yeah but
the graph the graph burglarized is still
based on the tagging distribution for
each note and the note is marginalized
and normalized basically taking an
average over other label data and am i
understanding right so I mean you could
you could use the subramanya method
without forcing the graph to label every
single word in your training data could
use it for let's say unknown words but
in this one you have to do it on all
every single yeah the graph contains
information about all the tokens okay i
just wanted to click it's just a
question about the model I just wanted
to understand it better thank you
there
yep
so I just wondering you already how fast
where the e/m steps can work we ran it
for 28 for Asians okay it's a different
by languages 04 from larger language
slower of course yet time for a quick
question okay otherwise let's let's
thank our speaker again all right okay
our next and final talk for today kept
the best for the for the end is multi
matric optimization ensamble during and
sarkar is presenting so you know that
two out of the three quarters are my
students because they put me as a last
talked this is their way of getting
revenge on me so I'm gonna talk about
this work that was done by primarily by
my student baskaran who couldn't be here
today so I'm presenting on his behalf
and also on of Kevin who's in Nara Japan
he's a collaborator in this work and
it's about multi metric optimization so
I'm going to start with a caricature of
how we do discriminative training in Mt
and this has two objectives if you
already know how it works then it will
introduce you to the caricature i'm
going to use to explain what we did and
if you don't know how empty tuning works
then this will give you a distorted view
of what it does and at least enough to
hopefully understand
and our contribution so we start with a
friend sentence there and English
sentence here which is the translation
of their friend sentence this is a
machine translation decoder which is
going to produce what the machine
translation system currently thinks is
the translation of that French sentence
so you can get an N best output for this
friend sentence and we are trying to
tune the parameters of the model so what
we want to do is to use these invest
list in order to train the parameters of
a model so we have a bunch of knobs that
we want to turn in order to make sure
that the translations we get in that
invest list matches the actual
translation of F right so we might get
something wrong and we want to tune
these knobs so that in the next
iteration we use a better way vector and
that better weight vector goes back into
a decoder and hopefully we get better so
this is our tuning loop retraining our
system to produce better machine
translation outputs so the question is
how to choose this weight well this
weight is something that's going to
match an extrinsic score so you know a
loss function of some kind and that loss
is based on some metrics so some way of
saying that this is a good translation
this is a bad translation and machine
wash is notoriously difficult because
there's more than one way to translate a
sentence so you don't get a loss that's
quite nice like you usually do in
machine translation so let's look at
this weight optimization step we want to
say if the model likes it and the metric
likes it that's great you know a model
score is good and it also performs well
according to the metric if our model
score is really high and our metric says
this is bad
then we should be changing our weights
if in this case maybe they're both good
in some cases the model score is bad and
the metric says it's bad that's good too
right so we are sort of predicting that
it should be bad so here's one
particular metric is called blue it just
basically matches in grams in the output
with the reference and they might be and
this is an the x-axis is a model score
so you can see that this guy and this
guy are kind of evil twins right so this
has a high model score and really good
on our metric so higher the better and
this is really good bottle score but
terrible on a metric so what we really
want to do is say you know here's two
things that are getting high model score
prefer this guy over this guy so you can
kind of replace this complicated machine
translation loss function into something
that is basically binary classification
right so this is the idea behind pro and
so the question is you know is this
metric the most suitable one so you go
to it's happening less often for some
reason but every time you go to an ACL
or an echo somebody in the audience will
get up and say you know I hate the blue
score you know this when is the blue
score going to end and a lots of people
have tried many many replacements for
the blue score you can take your pick
there's a whole bunch of different
metrics so each of them can be used to
choose a weight vector that matches one
of these extremes excursion machine
translation you optimize using blue and
it's the Highlander of machine
translation matrix they can be only one
it has killed everything else
but and it is true it does work better
so like people have tried other metrics
it just seems to be the best one but in
this work we want to say maybe you don't
have to choose one maybe we can be sort
of anti Highlander and have many people
live so so in this work we actually
going to look at different metrics and
see how you can use them all in order to
tell us what's a good translation what's
a bad translation all at the same time
and so we have the blue score it's a
Engram matches with a meteor Ellen
Louise favorite one and it's like glue
but unlike so blue is essentially
captures recall in a funny way so meteor
captures precision and recall and also
has some synonym instead matching
there's a one that you might not have
heard of it's called ibis and riper say
is really good at figuring out whether
things are out of order or not so it's
very popular for Japanese English
translation because it measures out of
order matches does it using a Spearman's
correlation coefficient and finally
there's a good old one from the speech
record days which is whatever so this is
translation error rate so it insertion
deletion and block shifts okay so these
are all different reasonable ways to
measure how different is your hypothesis
from the reference so what we're going
to do is use them all at the same time
so let's define what that means we want
to find the weight vector that's good
according to multiple metrics okay so
this is kind of a hand wavy way to say
that so find 1w that simultaneously
optimizes many different objectives each
metric is treated as an objective and
you treat the hypotheses that we produce
for each metric and G is a way to
combine them so one way to do it is to
simply
at this point you know when you choose a
weight vector just take two things like
tur- blue / to just combine them
together and now you have a linear
combination of two different metrics
okay and this has been done before this
is going to be our baseline there's one
problem with this is if you want to
combine them and you want to say I want
to actually have more weight on the blue
score at less weight on the error rate
you have to do that by hand there's no
automatic way to do it so we're going to
address that issue and we're going to do
things a bit differently so what is a
reasonable way to combine different
metrics well one reasonable way to do
that is a notion of Pareto efficiency
which says you have optimality with
respect to different metrics then you
should look at these objective a and
objective be and look at this frontier
right so these points here don't matter
the worst according to both matrix so
these points on the frontier are the
ones you should be looking at so this is
the idea we're going to use this
previous work by Kevin that actually
just modifies pro to push the points in
the frontier but one problem that was in
that previous work was there was no way
to actually take advice from all the
different objectives at the same time
and that's another problem we solve in
this work okay so i'm not going to
present a three of these introduce four
different ways of doing it and I'm just
going to present our main result due to
lack of time and I'm going to explain to
you how what it does using my caricature
of tuning and we call it ensemble tuning
and the idea is the same as before you
get your n bass output nothing different
here now what if you have for example
blue as one of your objectives you do
blue weight optimization
so you turn the knob so that you get the
best weight that is going to optimize
blue and let's say the other metric is
meteor you do that as well you get
another weight vector so you get two
different weight vectors w1 and w2 and
they both have different views on what
the weight vector should be right and
now what we do is we combine these using
these met awaits the meta weights are
saying how much should I trust blue how
much of it was meteor and the matter
weights are trained so that the the
points that we pick that is the
combination of these two lie on the
puter frontier okay so it will reward
things that are on the Pareto frontier
and everything under that frontier will
get penalized okay now there's one issue
here which is when I combine these two I
still want one model I don't want a blue
model and a meteor model because now I
have two problems not one right before I
had at least one problem so I I need a
way to combine the predictions of the
blue model with the error rate model a
meteor model in this case okay so how do
we deal with these multiple components
so that's the second problem we solve is
you don't have to pick and choose so
usually when you do Pareto optimality
sorry the usual way people use Pareto
optimality as they say well somebody
else is going to pick one of these
depending on me what do you want so if
you really want blue scores you would
pick this one if you really want meteors
cause maybe we'll pick this one so we
don't actually have to make that choice
we combine the weights using ensemble
decoding and the idea be an ensemble
decoding is very simple we have wait one
and wait to write one is the blue one is
a meteor and whenever we make a
prediction we actually combine them
together
to make a prediction while we're
decoding so basically we can take as
many weight vectors as we want and
produce something that will be the
combination of these and we just do that
all the way up until we get a
translation and each case is going to be
a combination of these two weight
vectors ok so we implemented this this
ensemble decoding idea as well as a
burrito weight training in our favorite
decoder which we wrote which is
available on github if you want to use
it if you're brave enough we use a
fairly standard large scale corporal
arabic english and chinese english and
this is this is going to be two
dimensional so this one is ribose and
blue and i'll show you results for other
metrics but let's just look at blue on
one axis and riboswitch is another empty
metric on the other axis and LC so blue
by itself is over their rivals by itself
is over there so you can see that if you
optimize towards blue you do better and
blue if you optimize on rivals you do
better right base that's LC points are
the linear combination points this is
linear combination plus Ensemble and
that point up on the right is blue ryba
center all together and this point over
here I guess I hear this point over here
is blue and ribose together so you can
see that what we want is points that are
on the upper right corner right things
that are good according to both metrics
and ideally better than both met you
know either doing one or the other
individually here's Chinese English and
you can see that as you add more metrics
you can get a bit better it's better on
blue then just doing blue it's
off it's not so in this case it's not as
good as doing meteor by itself but you
can see that meteor scores really badly
on you right so the question is are
those points up there good because you
get a good blue score so can you always
improve the blue score so here actually
we show that if you have a single
objective this just blue gets better if
you have to matrix a blue and rapist
does better this is blue meteorite base
blue mediator are both better than just
doing blue and then we did some crazy
stuff over here with like b3 is like
blue 3 gram into the program and so on
so you can invent as many metrics as you
want and so you can see that that infant
in fact does better so we also try to
look at whether those points on the
upper right quadrant are actually better
you know do they actually read better so
we did a human evaluation which was a
post editing task and we saw a six
percent gain in the post editing error
so it it was six percent easier to post
edit multiple metrics than one metric
and that's it
okay give any question I just wonder if
you find any language difference in
preserving one minute it's over other so
kevin has done work on Japanese English
and ribose for Japanese English just
because of the reordering and we haven't
done anything other than arabic english
and chinese english yet convenience your
module me it's conceivable that you
could have some metrics that help more
with languages that are quite different
I don't know if we had a really good
metric for morphologically complex
languages which doesn't exist but if you
had one then we could use that in order
to improve the blue score in that case
and then you combine multiple matrix and
there you have to take that yeah yeah
but I mean it's an interesting challenge
at least it's it's it's not just
mindless sometimes I think that's an
interesting problem to have
I didn't 100% understand why you needed
to combine the weights on the fly during
inference as opposed to combining them
offline into a single model oh yeah it
does better that's what we claim we have
a lot of me I agree it's not a given but
we have compared it to linear
combination all like offline and
ensemble decoding allows you to do
different things so it allows you to for
example do switching which you can oh i
see i see so you're not just okay yeah
it allows you to do different kinds of
voting so we have different ways of
combining the models that you can sort
of turn off and on in the Dakota okay so
you look you're learning combination man
yes so maggid thesis is is on that so
make sure we have a lot of we have a lot
of papers on ensembl decoding as our
hammer that improves over linear
combination
so I'm surprised that when you optimize
both blue and meteor for example that
you can do better on blue than just
optimizing a blue alone yeah well it's
not surprising it's not surprising well
you know you can think of blue as being
very very sensitive to length but
actually maybe it shouldn't be right you
know like in some cases it might
penalize it really badly because you hit
against the brevity penalty and that
leads you into a bad space so I can see
why optimizing both metric same time
would do better for humans but just on
the blue metric yeah it just stops blue
for making a mistake on I mean that's a
story you could tell I don't know we
have tried to analyze it in different
ways so baskin has done some things
where he sort of on purpose side of
damages one metric and then sees what
you can recover from the other metric
it's kind of interesting to see but
those are still artificially we can't
you can't tell us there's an interesting
paper by the BBM guys I think about
putting together a bunch of different
features to sort of get blue on the
right track so as it were and and this
might be what's happening here as well
Thanks okay so let's thank our speaker
again
and I think now we have some closing
remarks hi I'm Anne Clifton I'm a PhD
student at Simon Fraser with the new
been dumb one of your co-chairs today so
just really quickly I wanted to thank
all of you for participating today and
making this the most successful North
ascend LP to date and in particular I
would like to take a moment to thank our
hosts here at Microsoft particularly
well Lewis who's set all this up for us
so helpful so am that's all thanks for
coming hope to see you in twenty sixteen
relief i do want to mention that
actually and i'm getting almost too much
credit here and a new Mariama and yesh
are sorry yeah sure did an enormous
amount of work and this honestly is the
most successful by number NW NLP that
we've had if we continue at this rate
we're going to have to move too much
bigger venue next time so hopefully we
don't double yet again next time let's
try to keep it around 300 next time
maybe in there will be ok ok thank you
everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>