<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The 31st UW/MS Symposium in Computational Linguistics | Coder Coacher - Coaching Coders</title><meta content="The 31st UW/MS Symposium in Computational Linguistics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The 31st UW/MS Symposium in Computational Linguistics</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GeA07yw_jrY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
and so today we're it's the 31st
symposium and Emily actually pointed out
so we have two interesting connections
to history today the first connection is
the first talk because Emily gave a talk
ten exactly ten years ago at the first
symposium and I think she's already
working on the slides for 2023 so and of
course now we're gonna exactly you know
do the same speakers again for the next
ten years in sequence with the exception
of Meg who's taking the role of Pablo
today who was the first speaker from for
Microsoft ten years ago but Meg also has
a great connection to the program
because she's graduate so you can
actually see that people who go through
this program and turn out to be
perfectly well-adjusted people which may
be encouraging and Faye is gonna
introduce Emily and Woodley for the
first talk and then I'm gonna introduce
make for the second talk and where as
usual we're gonna do about thirty
minutes for each talk including
questions okay so I guess I don't really
need to introduce Emily everything all
Emily is she's an associate professor a
linguist department she also has a joint
position at a CSU and I guess one thing
I she's faculty director for the CMS
program and I can see maybe 50% or more
of the audience actually come from that
program water into that program she has
a new book coming out 100 the essentials
are from accordion syntax that's her
second book and she's interested in
grammar engineering or conversational
semantics and a relationship between
complain and linguistics and our second
speaker now the second speaker that will
give this joint walk so what the Packard
is 17 years here and
student actually he got he saw better
and he masked her from Stanford and I
guess she's calmed
he's keep getting her sorry about this
he's gonna get
I guess graduate from our program maybe
no pressure right I'm just so I guess
Emily you'll start thanks BAE we've made
this a little confusing Woodley is the
first author on this work but the way we
decided to divide up the presentation I
get to go first this is a paper that
also represents joint work with some
colleagues of ours at multiple different
institutions in Europe and the title is
predicting the scope of negation using
minimal recursion semantics I'm sure
you've all had a chance to read that and
so I'm going to start with some just
motivation for the topic who cares about
negation well we should all care about
negation so here's a cute example from
machine translation that another
colleague of ours Frances bond turned up
this Japanese sentence Hank Alamosa
Becky David Dale a knight should be
translated by a human as we shouldn't
have any prejudice but if you put it
through a fairly standard SMT system a
configuration of Moses you get you
should have a bias all right and this
work that Francis was reporting on found
that Moses lost the negation two thirds
of the time and that's pretty important
for you know you can just without
getting negation what's this about but
not What's it saying another example
comes from Gmail so there's these
add-ins that helpfully try to connect
your calendar so actually let's not have
a meeting next thursday pencil it it
okay well no thank you I'd rather not
see that and and also and perhaps more
seriously there's a lot of interest in
detecting negation in the scope of
negation in the biomedical NLP field so
the Bioscope corpus was the first
large-scale effort to provide an
annotated resource here and this is to
support applications like coding
clinical notes for insurance claims
right so if you miss negation and you
code something that was mentioned but
actually isn't there you can get in
trouble because you're charging the
insurer too much and but also
epidemiological studies other uses of
text mining and then so Bioscope
annotates things like a small amount of
I don't know how pronounce that and
adeno pathy cannot be completely
excluded did I get it right
adenopathy all right so this is this is
a particular interesting example because
we have cannot which is sort of a
grammatical clausal negation but then
excluded is also a lexical
representation of the ation and so if
you really want to know what this
sentence is saying you have to be able
to handle both of those markers of
negation which leads into this slide
here so there's many different ways to
mark negation there's clausal negation
which in english is a separate word so
she decided not to go home whereas the
embedded clause go home that's negated
with that not and we also have
constituent negation on noun phrases
where it shows up as a determiner so he
had no friends in Seattle and but
there's also negative a fixes so a
prefix space flight was thought
impossible suffix Kim was clueless and
an infix at least this is according to
the annotated data that we're working
with this was considered an infix we got
hopelessly lost they called an infix
because the list is in the middle of the
string a linguist would call that a
suffix that happens to have another
suffix after it read my book all right
so what was this data I'm talking about
there was a shared task at the stars' M
so the joint workshop our conference on
lexical and compositional semantics in
2012 on detecting the scope and focus of
negation and so the organizers of that
shared task formalize the notion of
negation Q and a notion of the scope of
negation and also focus which is what
within the scope is primarily negated
and then negated events so there's lots
of different subtasks and developed
careful annotation guidelines and then
annotated a bunch of Sherlock Holmes
data for training and testing alright so
you have annotated data where the Q is
marked and then the scope according to
that Q and then further annotations that
are not what we're focusing on here and
there was good interest strong interest
in this problem so there were 12
submissions from seven different
institutions and to give you a sense of
the task here are some of the data so
you have sentences like the German was
sent for but professed to know nothing
of the matter it gives you a little bit
of the flavor of the
I love this text too which is fun to
work with it may be that you are not
yourself luminous but you are a
conductor of light this is way more fun
than Wall Street Journal text and I
trust that there is nothing of
consequence which I have overlooked so
we have nothing and not showing up in
these examples there's also the FX's
that they've annotated in a similar way
and what's particularly interesting I
think is that the scope is based on the
semantic dependencies and so in the
first example the German was sent for
but professed to know nothing of the
matter
we have no being negated because it has
a negated argument and then the rest of
its arguments are also negated so also
in the scope so the German is counted as
in scope so of those previous approaches
the winning submission came from the
University of Oslo and was submitted by
our co-authors humpin others so we've
teamed up with the winners here and they
used an SVM to detect the cues and then
to rank the syntactic constituents for
scopes so the training data included
believe it was charniak parser data so
automatically provided parses for all
the sentences and then the system use
that to come up with what might be or
might not be in the scope there was also
a submission from University of
Washington by Jim white I don't see Jim
here today that approach used regular
expressions for cue detection and then a
CRF sequence labeling for the scopes so
not working with semantic structures in
this case actually not even really
working with the syntactic structures
just looking at the surface strings and
that wasn't too far behind the winner
there was one system that actually
approached this semantic problem from
the point of view of explicit semantic
representation and that was the
University of honing in submission and
where they use DRS which is discourse
representation structures as produced by
the CNC parser and the boxer system and
that gives a fairly explicit notion of
cue and scope so the only real
modifications they made was to change
some of the lexical entries in boxer so
that more things gave the negation
symbol and the semantics and then they
just read the scope off of their
semantic representations and then sort
of filled in the blank
to try to handle the semantically empty
words and they did not do well in the
competition somewhat disappointingly it
seems like that's a principled approach
it should have worked
um so in the paper describing this
corpus amarante and de la Mons
characterized negation is what they call
an extra propositional aspect of meaning
what struck me is very strange because
what I know about semantics is that
negation is actually a core piece of
compositionally constructed semantic
representations and that's what I think
propositional semantics means their
notion of scope of negation is not quite
the same as the way we use scope in
mainstream under specified semantics it
is more tied to predicate argument
structures and uninterested in scope
ambiguities with quantifiers but looking
at the annotations and looking at the
minimal recursion semantic structures
are going to tell you about in a minute
here we actually notice that there was a
nice alignment and so the MRA structures
give us a good starting point for trying
to model the task specific notion of
scope of negation so very quickly on
what men will recursion semantics is
it's a flat under specified
representation of formulae in predicate
logic with generalized quantifiers it
makes explicit the predicate argument
links and also the links between scope
will argue 'men
the entities that fill those argument
positions some of those are fixed so the
negation takes a scope argument but it's
fixed by its position in the syntax
that's in contrast with quantifiers that
are also a scope of things but they can
float around and so in mrs is an under
specified representation that can be
monotonically further refined to get
fully scoped representations when we
first started working on this we thought
that we actually wanted those fully
scoped representations and quickly
discovered that actually that's not the
notion of scope and so we're working
directly with the under specified
representations the main reason we're
using mrs is that it can be produced
automatically at scale for english by
the english resource grammar so the
english resource grammar and will give
us a structure a syntactic structure
roughly like this this is very
simplified in the sense that what's
actually behind each of those node
labels is an enormous feature structure
with hundreds of future value pairs all
right but that gives you a sense of the
constituent structure and part of the
big feature structure at the S note at
the top is this semantic representation
for the German was sent
for but profess to know nothing of the
matter and it's this representation that
we're going to be crawling around in to
pick out the scope of negation and
here's where I hand it over to Woodley
to tell you how we actually do that
thanks Emily
all right so this structure is you can
see that it's it's pretty machine
readable it's got a little bit more
information that we actually need a bit
then the critical things are it's got
you should use my laser pointer here
instead of putting my hand up there the
character positions that link these
predicates back to the string and these
argument positions that describe how the
different predicates relate to each
other it's also got these in curly
braces here properties are the variables
and we're actually gonna totally ignore
those for this purpose but you can see
for instance how this works that the
thing that's known by this no relation
here is its Arg 2 which is X 18 and
that's a thing so the sentence here was
it was one of our examples from before
it's the German was sent for but
professed to know nothing of the matter
and that's how that works out in the
argument structure so I'm going to show
you a slightly different way of looking
at this picture that throws away some of
the information we don't care about in
this project but makes it easier to see
the graph structure here so there's more
or less a graph underlying this and it
looks like that so now the different
links here are the argument and the
nodes are the predicates so the link I
just showed you a moment ago was from no
two thing so it goes from no to the
quantifier and also no to the thing all
right so to attack the problem of scope
resolution the first thing we do is find
out what the Q is and we assume that the
Q is actually one of these predicates
and our system assumes that somebody
else has done the Q identification for
us at the string level and then we
project that onto the mrs using those
standoffs that i showed you so this
shows us where in the string that no was
and we call that predicate in scope and
then based on
that we can say how are we going to look
around the rest of this graph to figure
out what else is in scope because what
we thought we would do is figure out
what parts of this graph are in scope
and then project those back using those
string standoffs onto the the surface so
it's a basic idea is to just crawl
around these links and the trick is
which one should be crown which one
should be not crawl so since this is an
NP negation with a quantifier here the
first step was well first step was dead
I've had the Q the second step was to
activate verbs that take it as an
argument so you can see these green
links are links from some other
predicate back into an activated node
and there's one from of and there's one
from no that knows the only verb so we
activate that and then once we've done
that that was called we've got a couple
different kind of crawling here that we
give different names we called that
functor crawling because it's crawling
backwards on the links from then on we
only crawl forward I would call that
argument crawling and label crawling we
just did the furniture crawling - as an
initialization step so looking from here
we can color in the different arcs that
are accessible from these in scope nodes
and the red arcs are argument crawling
big green arcs are what we could do if
we were willing to flick their crawl and
you can see we want to be able to get
from no to its arguments and well these
are but that may be a bad example
because the most pronounced the same way
bit from Konoe to its arguments and also
from the no quantifier to its argument
so those are both argument crawling and
we just go ahead and do that and then
label crawling so from here what's
available there's more green arcs now if
we were willing to deflect your crawling
and there's a few instances where we do
allow that for modal's and certain types
of subordinating conjunctions
but ferret any of those in this example
I colored the label arc there blue so
there's there's a couple different types
of arcs here the main most interesting
ones are the arguments but
the label arcs relate to the scope
structure that's defined by the under
specified representation of scope here
and the important thing is that Co
modifiers a lot of modifiers of a noun
or verb will share the label with that
so this label crawling link here in blue
so we're always willing to crawl across
that there's a couple of exceptions but
they're not important here so you can
see we've already crawled all of the red
links now we're going to call this blue
link to get two of and then that enables
us to get down a couple more red links
to those and then there's nothing more
that can be done so at this point we've
crawled around the mr-s graph and
activated everything that we could reach
with these rules from No and turns out
these are exactly the words that we
wanted to have in scope according to the
annotations so project that back with
the string standoffs to be a surface
string and we find the German know
nothing of the matter is what we
predicted to be in scope and that's
correct and that's basically the way the
system works in general there's a
wrinkle which is these so-called
semantically empty words so the English
resource grammar in its semantic
representations that it produces there's
in most cases one predication one
predicate one node in that graph per
word but there's some cases where a word
just doesn't contribute anything to the
semantics a good example as a
complementizer like to or that and there
are some cases where a word has some
meaning but it's not totally
compositional so that it's it's best to
encode that as part of a predicate from
another word so send for was an example
I'll love that one I think was out of
scope so it didn't matter here yeah so
so actually I think for is probably
considered semantically vacuous in this
analysis but it didn't matter to us
because that was
scope um then I'm gonna give you another
example here that there are quite a few
semantically vacuous words the ones in
blue here so I trust that there is
nothing of consequence which I have
overlooked the curly braces there show
the scope that we want to get that the
annotators marked and the red shows what
the crawling rules that I just described
will get for us so is thing of
consequence overlooked actually I should
also be red I think you can't see that
but which have and that and there are
vacuous and in that they don't
contribute an explicit predication to
the graph so most of those we want so we
have to figure out how to do that since
they don't show up in the mrs we have to
back off to the syntax tree
they almost showed you before the tree
structure and the mrs structure that are
produced with the ERG so here's the tree
structure for that sentence I trust that
there's nothing of consequence which I
have overlooked and what we do is
initially we annotate the words the
crawling system marked is in scope and
we walk up the lexical head paths from
those so something like nothing is the
head of this entire noun phrase that is
not the head once you get to the verb
phrase so wait what the wrong direction
there we mark from nothing as in scope
all the way up to here and from is which
we marked it some scope up to there and
from consequence up to here and so forth
so that a large portion of the tree
around the area that we marked is in
scope is is flagged so then from there
each of these semantically vacuous words
is relatively close to information about
whether that section of the string is in
scope or not and then each of the types
of semantically vacuous word relative
pronoun what else
complementizer 'he's helping verbs
things like that
they each have there's about four or
five different classes of them in the
each have sets of rules describing where
in the tree to look to see whether
you're in scope or not so for instance
have says I'm in scope if my compliment
isn't scope and which says I'm in scope
if I fill a gap in in in scope sentence
there's something like that so you
iterate these rules a few times because
you can see which is not actually
filling a gap in in in scope sentence
yet because the head of that sentence is
have submitted syntactically so after
one step we can mark the half as in
scope because it took an in scope
compliment and after two steps so so
then that you know its head path went up
to the s because it was the head of that
constituent and then which now fills a
gap in that and we can mark a dozen
scope to and you can see that that out
here doesn't yeah the rule for
complimentizer is that they're in scope
if they are the compliment of an inch
gap verb and there's some idiosyncrasies
in the way the annotation guidelines
worked alright so we have to evaluate
this we used the Sherlock Holmes data
and we used girl the gold Q's it comes
with a test dev train split that was
used for the shared task so we used that
and we divided we designed these rules
using the training data we looked at the
development data once and we applied to
the one best analysis Mayer G so the air
G has lots of different analyses for any
particular string ambiguity but there's
a statistical model that'll tell you
which one was probably the right one and
there's actually a confidence metric
that goes with that so we did this and
the results were kind of ok you can see
it's a high precision low recall system
and one of the reasons that the recall
is low is because the grammar doesn't
have full coverage frequently there's no
parse or the statistical purse selection
gets the wrong parse there's a few cases
where there's rare cues that we didn't
know what to do with so there's a lot of
cases where the system not puts out the
wrong answer per se but says I
don't know so the obvious thing to do is
let's combine with another system that's
a high recall system and the winners
from the competition work certainly that
in we're good friends with them so we
said okay let's do a system combination
with you guys and this system
combination is if we use our results
whenever they're available and use their
results when our results aren't
available and you can see the recall
pops way up it was 67 for token level
recall on the development set pops up to
83 and all the other numbers are better
- well the precision drops a little bit
but not much so that's a lot better it's
not quite consistently beating the
winner of the competition yet but it's
actually relatively close and there's
one more game we can play which is that
sometimes we put out an analysis a guess
about the Scopes even when we have a
very low confidence parse so we have
this confidence metric from the it's a
maximum entropy parse election model and
we can just say all right what was our
confidence what was the probability that
that model assigned to this tree it's a
conditioned on the input string so if we
say well if our probability of having
the right parse was at least 50% then
we'll use our system otherwise we'll
fall back and by now we're only
producing guesses for about 25% of the
of the negation queues but it's a still
a meaningful portion and using that we
can actually outperform the published
results with the system combination here
you can see that the published results
down here from the winners of the
competition last year are here and we're
able to outperform them in almost every
box they they win and recall in one box
there that it's not a big win that it is
you know 1% 5% error reduction 10%
reduction something like that
so so we feel like that's kind of
exciting I can't tell you
statistical tests about whether those
are significant or not seems like at
least the
anyway so the size of the test set is
you remember the number on that I think
that there's I think that there's 200
negation cues that there's more there's
more sentences but we're not doing the
task of identifying which ones are cute
or not and the number of tokens in it is
it's got to be 3,000 tokens or so of
which several hundred are in scope in
conclusion the Mrs Bay system is high
precision but low recall and in a system
combination it seems to be able to
perform at least as well probably a
little bit better than the best
published results we also did an Oracle
experiment where instead of using the
confidence metric to decide which system
to use we used whichever one performed
best according to the gold and that was
able to perform even quite a bit better
than the system combination actually the
numbers there I think on the test set
this went up to 90 point something and
we wanted to know that we implemented
our rules looking at the data but
actually if you look at the guidelines
and compare them to our rules you can
see that they line up pretty well the
nice thing about a rule-based system is
of course you can interpret the rules
and it looked like the rules did just
about what the guidelines said the fact
that we were we were able to converge on
those rules and that they picked rules
that matched up with the semantic
representations that the ERG produced we
think kind of validates each other in a
way so we think it's it's neat that
explicitly semantic approach to this
problem was able to build on a purely
machine learning approach thank you
questions
so among sentences where the correct
parse was chosen my selection model and
it was coverage do you find that mr.
recolor is disappeared yes
so it's hard to say exactly I mean we
did some tree banking to determine what
this so in general we don't know when
the right Parkes was found or not found
you have to go in manually and say or
what was the right parse did we get it
or not a lot of the cases in error
analysis where we looked through to see
what was going wrong we found that it
was the wrong parse on the other hand we
did some manual tree making and produced
gold analyses for a bunch of sentences
and when we ran those through our rules
they didn't perform that well it looked
like the reason was that our gold
analyses had a lot more of these rare
cues in them things that the statistical
model just wasn't picking up on because
they were low frequency and so our rules
didn't have them either for the
confidence that was just I mean we did
play with pushing that back and forth
fifty percent was actually the first
thing we tried and it turned out to be
not much difference between doing that
and say 25 percent or 75 percent yeah
no so we haven't done that you actually
he hasn't released the software that I
know of to make that possible yeah so so
to fill in some context there is a pcfg
approximation to the ERG that lets you
produce a derivation tree at least in
cases where the grammar wouldn't be able
to parse and there's a method for trying
to produce in mrs from those derivation
trees even when the unifications
according to the unification based
grammar wouldn't actually succeed and
you can get a graph that looks like our
graphs out of that that's relatively new
research and it's not a software that we
had access to so what we'd like to try
that I think the fact that the lower
scoring trees yielded noticeably worse
results suggest that maybe trees coming
out of that wouldn't work well either
bet we don't know he'll be worth trying
any other questions
that's my my pleasure to introduce Meg
Mitchell and we're very lucky she joined
our group recently Meg as you know I
mentioned before I actually got got the
masters through through the u-dub
program got a PhD at the University of
Aberdeen and was also a visiting scholar
at Oregon Health and Science University
and Meg is particularly interested in
the connections between language and and
vision which is a really sort of
interesting and I think sort of emerging
field that that's drawing more and more
attention and today she's going to be
talking about generating human reference
to visible objects can you guys hear me
okay right so I'm Meg I'm gonna be
giving sort of a general talk on
generating human reference to visible
objects in particular this is something
I've worked on from different angles for
a while
including collaboration with a bunch of
people I kind of keep the same slide
deck and keep switching in slides but
then I'm not sure what names to take in
and out so this includes a lot of input
from a lot of people spanning a lot of
universities and I thought it would be
kind of fun to look at this from the
perspective of when I started in the clm
a program now it's called C LMS but then
it was called C LM a and just sort of
pick up from the last presentation I
gave there um when I was doing my thesis
proposal and then start sort of picking
out the space on and the problems that
fell out from that beginning so C okay
um so my plan is to briefly cover some
psycholinguistics studies i've run to
tease out what is useful to model when
you're trying to look at human reference
to visible objects and some models that
i've developed in pursuit of that goal I
hope it's clear from this talk that this
is a wide-open space where there's a lot
of future work that can be done
we have some sort of nice starts okay so
back to 2008 um this is a slide from my
thesis proposal in the CMA program Mike
nicely made maybe is pretty Spears where
I was trying to dive into the task
looking specifically at referring
expression generation which is a subtask
in natural language generation and I was
concerned with how a system could
produce referring expressions that sound
natural so how can we produce
expressions that uniquely pick out items
in a scene in a way that sounds
human-like that sort of brought up the
question what does natural mean what the
input could possibly be to get to that
point and then if we actually do get to
the point of outputting something that
sounds natural how do we evaluate it so
this can be sort of seen within the
general context of natural language
generation which deals with taking non
linguistic input associating it to some
sort of syntactic and semantic
structures and from there figuring out
how to output sentences that sound
cohesive and fluent so referring
expression generation can be seen as
just a sub task within this larger set
where you're looking at generating
expression that can identify a reference
to a listener or here so the state of
the art in 2008 was basically given some
sort of computer graphic representation
of an image and some very clear sort of
simple semantics of that object so
something like color gray size large
orientation front find the best way to
refer to that one thing in context so
say something like the gray desk moving
forward in my master's thesis at CL MA I
was I was starting to wonder what
happens when you move outside of the
computer graphic world and moved to
actual objects that people are talking
about so I ran this mechanical turk
study when I had sort of blank on the
left is tied with blank different sort
of configurations of objects and tried
to tease out what kind of things people
were saying when they were referring to
real objects one of the things that came
out of this was the fact that there's an
enormous amount of
variation in the way that people talk
about visual objects so I had 64
different participants and 32 different
expressions to refer to this orange Bob
Lee thing I didn't have some underlying
semantic representation right I just had
the image and so it was sort of up to me
to figure out how to get from this image
to some sort of set of possible ways to
talk about the object and it's it's sort
of like a dual problem where I'm trying
to figure out what sort of things are
worth mentioning in the object and then
how it should be realized in a surface
form so maybe spiked and spiky are from
the same sort of underlying semantics
but these are two separate problems to
tease out right so going from this sort
of traditional task of trying to find
the best expression that you can use to
refer to an item given some predefined
set of attributes and values
I started spinning the task as trying to
figure out from some visual input how do
we get the set of possible things that
people might say to talk about it one
thing that's interesting about this is
that the task of trying to figure out
how to talk about one specific object is
similar to the task of figuring out how
to describe that object right so
something like the bright bumpy ball
thing randomly floating on the right
side of the slide is something you could
say about it given a visual input and
once you start to look at it in terms of
description you realize that it's not a
big jump to go from talking about an
object to saying full sentences right so
this is just a copular transition from a
noun phrase to ascendance the bright
bump people think is randomly floating
on the right side of the slide and for
some reason I like blue my thesis
advisors mind when I made this point but
the idea is that by looking at the
object you can start to reason about the
semantic representations and move this
to full-on sentence descriptions of
scenes okay along with a you know sort
of interesting variation so randomly
floats is randomly floating right so
while look at the object reference why
is this interesting at all um
there's a bunch of reasons
well I find this interesting I kind of
changed around the reasons depending on
the audience because I don't have a ton
of time I'll just say here's a few one
is that it anchors visual language so as
I mentioned before being able to reason
about the semantics of an object in a
visual scene gives you a way to reason
about visual scenes as a whole so
they're nice like little bits of meaning
that you can start adding further since
syntactic structure around it's useful
and things like assistive technology so
at the University of Aberdeen I was
involved in this project called how was
school today or the idea was kids with
cerebral balls palsy who have difficulty
talking about the world might be able to
have a camera that takes pictures of
things and then that could be used to
generate descriptions of objects that
they could talk about that they could
play with and things like that it's
useful for devices that can scan the
world and interact with humans as an
assistant so this is kind of like the AI
problem but it gets right in there it's
also useful for automatic summarization
of videos so by being able to analyze
videos attach them to linguistic
structures we can start getting at how
people are going to search through
videos and find them without having to
watch the videos themselves and it's
useful for creative tasks just like
generating descriptions of things for
the fun of it okay um it also kind of
makes sense to look at the object within
within reference because it's sort of
it's how we've sort of evolved to be
able to talk about the world right so if
we're going towards a model that can
talk about the visual world it makes
some sense to kind of borrow from how
humans have evolved to start learning
about how to talk in general so we know
that the object is the principal unit in
early language learning we know that
some of the earliest vocabulary items
for children are usually nouns that
refer to concrete objects and these are
the kinds of visual things you're
interacting with in the world there's
been a lot of work showing that
referring nouns seem to be conceptually
more basic than concepts referred to by
verbs and prepositions so it is clearly
sort of a nice starting space and here's
the early reference I try and have like
an early reference and all my and
nouns also provide a way to communicate
about the natural world's variety as
distinct entities right so like when
pigeons start up people are usually just
referring to nouns and basic verbs in
order to talk about the world and
something that is also sort of handy
this is just sort of a happy coincidence
is that computer vision which is the
obvious sort of input if you're trying
to automatically talk about the visual
world is centered around the object so
action detection doesn't work very well
pose detection doesn't work very well
but object detection works reasonably
well and action and pose given object is
much better than action and pose alone
okay so um I ran a bunch of studies to
try and start teasing out what it means
to refer to objects in these real visual
domains previous corpora was sort of
impoverished for my needs
so the tuna corpus was this corpus that
looked at these things like gray desks
in certain configurations the GRE 3 D 3
corpus is a similar thing but with cubes
and and spheres and I was trying to sort
of get at what people are going to do
not when they're asked specifically to
refer to this thing but when they're
given a bunch of objects and they're
within a broader task and they're
talking about them and how can we sort
of model that sort of stuff so this is a
there's further like details on each of
these one and I know G 2010 when in cog
side 2013 at the same time it makes
sense to think about what the input
would be to eventually get to that point
right so I was simultaneously working
with some computer vision people to try
and think about what computer vision can
actually provide when I'm trying to get
this sort of naturalistic output so
things like object detection these are
bounding boxes around the object can
work reasonably well in small domains
okay so there's no one corpus there's no
one study you can do to sort of
understand how people talk about the
visual world and get a you know
appropriate model for that but if you do
a bunch of studies and you do corpus
based work
you can start to see sort of general
tendencies that come out of that and
know what it's useful to focus your
research towards so that's sort of what
I did I did this one study where I had
craft objects which I chose because
those are visual manifestations of his
visible properties right that's sort of
their whole point is just to be these
masses of visible properties visual
properties and I had people describe how
to use the objects on the board to get
to to put together pieces sorry to put
together a face and then I ran another
study with sort of like office objects
kind of things I found around the house
and I had them tell an assistant how to
place them in these sort of grid like
patterns in order to replicate some
images that they saw on a screen and
what you find sort of going through all
these things is a bunch of things that
sort of seem obvious but there hasn't
been a lot of work done on them so it's
important to kind of focus on them in
particular so one thing is that
different people refer differently so
this I mentioned in my master's thesis
work but it's obviously come out again
and again and again
one shot descriptions don't make sense
when you're trying to talk about the
visual world that's true for generating
referring expressions that's true for
generating full descriptions different
people will refer in different ways and
a single person will refer differently
in the same context so unless you're
trying to build an agents that is very
repetitive it makes sense to start
modeling the distribution over both the
sort of semantic content that people
will pick when they talk about an object
as well as the surface forms that that
semantic content can be used that that
semantic content can be used to realize
okay it also becomes clear that ordering
really matters and this is true both for
modifiers and pronouns and for a bunch
of other things so this sort of led to a
bunch of projects where people I noticed
that people were comfortable saying
things like comfortable red chair but
things like red comfortable chair was
just preferred and this is true for like
even longer phrases right big
rectangular green Chinese silk carpet is
awkward but sounds somehow more correct
than Chinese big silk green rectangular
carpets there were sort of no exist
models for this kind of thing this
further details on this in a CL 2011 but
the sort of punchline from this work is
if you automatically parse a ton of
corpora extract the noun phrases and
then just train a language model on
those noun phrases you can do really
well at doing this automatically if you
want to do a class based approach it
works pretty well with a hidden Markov
model which you can update using a.m. to
sort of learn these latent classes that
give rise to the surface forms there's a
sort of another problem that comes out
of this that still there's still like a
lot of work to be done in this I don't
think anyone's touched it and that's
given some sort of semantically
representation when are things
post-nominal and when are things
pre-nominal and how do you make that
decision ordering also matters for nouns
so within within a description of a
scene there is a tendency to put animate
things at the front of the sentence and
inanimate things near the end of the
sentence and people are actually kind of
predictable about what they choose a
subject and object so I did some work on
this using word now hyper names where
you could see like object positions in
some three noun descriptions so a
description that had three nouns in it
and animate things tended to be in first
position
things like structure box tended to be
in the second position and you can
actually use this in a generative model
to order a set of given order sorry two
orders that have given objects okay so
this also gives you some sense about
which attributes are important to focus
on so throughout these studies you see
that color tends to be massively
preferred also size sort of regardless
of the task material in shape as well
and analogies and part-whole relations
which I don't think has had any work for
generation so there's a lot of research
to be done there so in the study with a
craft objects I found there's a dominant
preference for color followed by size
material and shape and these are also
material and shape realized as head
nouns which reflects people's tendency
to use shape when they're when they're
giving
object categories so things of the same
object category tend to have roughly the
same shape and you can see that in the
way people talk about objects this is
also true for the for the study with the
officee objects again color size
material shape you see part-whole
relations and you see fun things like
analogies sea urchin whatever that is it
seems to be sort of like this there's
like sort of a wealth of problems that
you can work on here and we've just
started to scratch the surface so
keeping in mind what the actual possible
visual input is it becomes clear that
although we can get some sort of
understanding about the objects in the
scene and some of the attributes that
apply to them there are some things that
referring expression generation has
assumed would just be available that
really aren't so in particular size
right so you can't look at a scene and
just know small that actually takes some
sort of reasoning about the x and y and
z dimensions of the objects in the scene
so so it kind of opens up the sort of
problems that are available within the
sort of vision to language to connection
when you're trying to generate
human-like reference vision provides
things like color shape and material but
the relative properties that are common
for people to use things like size
location and spatial relationships
require requires some reasoning about
the object coordinates and segmentation
of the image and there's still a lot of
work to be done there I did some work on
thighs in particular because it was
particularly a common it's kind of
boring to work on size but you know it
sort of needs to be solved right if you
want to if you want to start looking at
this problem so like you know if you
have an object on the right here this is
a bit smaller than the object on the
left here they're sort of at some point
where you start to say okay that's still
small but maybe it's thin now it's maybe
thin now it's no it's just fit it's not
small now it's tall and thin or maybe
just tall there
seems to be something about the height
width and depth that you can reason
about in order to figure out what kind
of science language people will use in a
visual scene and you can actually do
pretty well on this using a
discriminative model that just uses
these sort of object coordinates in
different ways to try and predict some
basic size types that people will tend
to use I distinguish six different basic
size types that refer to like small and
big as well as things like tall thin
short fat and we can actually get pretty
close to approaching how well an Oracle
method does at understanding the kind of
size language that people use okay so
with all these sort of ideas although
nothing is completely solved you can
still put them together in a proof of
concept to start actually generating
descriptions of images so this is some
work I did with with computer vision
people back in 2011 where I was trying
to understand how I can take on when I
know about the tendencies of what people
tend to mention pre-nominal modifier
ordering and put them together with a
given sort of computer vision input to
create descriptions of images that sound
flew in to natural and what I found that
by collecting the object nouns so again
the computer vision is just providing
the nouns the sort of objects you can
collect the sort of sub trees that these
tend to occur in using just a basic penn
treebank syntax you can build likely sub
trees around them during generation and
you can put them together to make sort
of reasonably well-formed syntactic
structures that make sense so things
like duck on grass and duck by grass and
using just this sort of this method that
uses both the syntax and the computer
vision input on four outperformed other
recent methods on that sort of a task
given a computer vision input there's
still more projects to be done here I
haven't touched analogies I haven't
touched part-whole relations these are
all things that that you all should
solve and then let me like use your code
so future directions here I think that a
lot of work can be done sort of blur
earning a better semantic space to map
between objects attributes and the sub
phrases there everything has been kind
of coarse-grained and I think that we
can do a lot better the syntax that I've
used so far has largely been based on
penn treebank syntax and i think that's
a bit impoverished especially for noun
phrases I think we could do a lot better
I think it's useful to start making some
better decisions about when to refer to
an object by a description when to just
list its attributes when to make
analogies sort of just choosing between
these different ways of talking about
objects um there needs to be better
there needs to be more work on capturing
relative attributes and we have very
little knowledge base building that
we're using currently throughout these
models and I think that there can be a
lot more to guide what's useful to say
and what's not okay thanks seems like a
difficult thing to control for is the
sort of meta purpose to which the
descriptions are ultimately going to be
used and this could affect a human study
where the people don't maybe know why
you're asking this and then also it can
affect just sort of computer purposes
where there's not a sort of well-defined
way to to define the problem I see you
covering a lot of ground here in terms
of different application all tomatoe
applications but I guess could you
comment on zeroing in a little bit on
one particular application where this
type of thing shows up you know when the
data is when the description is targeted
for this for example on the school bus
you know some people could just say well
the bus and that's sufficient right yeah
in your description there's a bus with
the blue sky right yeah
why is that motivated yeah so I think
that I mean that's a really good point
and I think that part of what I've been
trying to do by looking at a bunch of
different a bunch of different corpora
and a bunch of different experiments is
just try and try and zero in on what it
is about visual language in general that
people are picking out so I can have
some generalizations that will hold
across the different domains right so
I'm still going to need to do like
pre-nominal modifier ordering no matter
what domain and and it's still important
to talk about color and size basically
no matter what time and I'm in in terms
of the larger structures that you
generate I think that goes to what
corpus you should be training on right
so if you're doing some sort of
conversational agent and you have you
have them talking about visual visual
objects in the worlds like for Direction
giving then you want to be training on a
corpus that has that kind of language in
it the same problem sort of fall out
with actual syntax changes yeah oh
there's so much I can tell you about
this so yeah so there's been a lot of
work on this and I think that this is
something that's really nice to go
towards less supervised computer vision
right so if you can actually put if you
just parse out a sentence pull out the
objects and then you can actually start
training detectors on the on the scene
as a whole in order to learn the sort of
objects there you don't have to have
people manually figuring out the sort of
object categories you need to NLP to in
order to enable right yeah so so that
goes into parsing and NP chunking and I
mean it depends on how sophisticated you
want to get you want to get I guess but
my preference is generally to start at
the par structure
yeah love the talk it's fun to see
what's happened in the intervening right
so I was intrigued by your statement
that it did one things to learn is what
goes pre nominally and what goes post
family because unlike the order of
predominance there's actually some
pretty hard constraints in English
grammar about what no before that yeah
yeah yeah so is it is it more like which
kind of expression are you going to use
I mean yeah that as well as just the
basic learning and mapping between the
sort of given attribute values and the
pre-nominal and post-nominal forms so
like I'm given things like circular and
I want to know do I say that this is a
circular cup or do I want to say the cup
that's in a circle and perhaps it's
trivial to find out but there's no
models that do it yet so it's still open
you sorry so it seems like you know in a
lot of cases the modifiers chosen might
be chosen specifically to distinguish an
object from like other similar five yeah
and that's the state of the art for
referring expression generation is
selecting modifiers face solely on
whether or not they rule out other items
in the same no it's not the state of the
art and it was a state of the art until
recently until this year yeah a similar
question did you look at I guess to
pursue that so for the experiments you
did where you had people you know moving
around their craft objects and things do
you look at all and how
the descriptors that they chose relate
to like the entropy of the south in
general and like whether they're
choosing descriptors that maximize every
cuz depending on the side yeah you're
kind of set of things that are all the
same color yeah totally yeah so um I can
actually for the crash for the craft
stuff that's an that would be a really
cool thing to do actually trying to pull
out the visual attributes and seeing if
there's something there some sort of
information gained in their selection I
think that would be awesome for the
office supply things I was actually I'm
reporting on the fillers but I was
running a larger study where I was
trying to tease out if typicality of
shape and material affected how people
named things and I found that things
that are an atypical shape tend to be
called by their atypical shape but
things that are an atypical material
tend not to be so there seems to be some
sort of interesting interplay between
some stored knowledge sort of
prototyping thing and and the given
visual scene is also another type of
context is over time right if people
refer to something something probably
the best time they're gonna have for
yeah yeah so that gets into dialogue
stuff and and you do like lexical and
Trainmen so you start to actually agree
on some sort of object name and then
when someone else comes in you actually
start with that object name again and
then you realize that there's been some
really nice Studies on how this works
and I have not implemented them so this
like this work has been largely a first
pass at when you're initially describing
an object and then yeah the clear next
extension is figuring out how those are
shortened and changed their dialogue so
when I was looking at the typicality
study this was in particular something
that was an issue I didn't think it
would be an issue so I conducted this
study in the US and in the UK and using
sort of semantics or using feature
production norms I don't know if you're
familiar with them but sort of listing
of normal attributes on from from people
in Canada and I found that although the
u.s. people use the sort of language I
expected the UK people would randomly
say things that made no sense to me at
all and okay
well yeah so I mean there are cultural
differences and they kept saying like
the pencil with a rubber on it you know
like things like that like yeah of
course it has a rubber on it it's a
tongue I don't know it's a racer but
yeah so so this is this is an issue it's
a tuning for specifically what kind of a
group you're working with definitely and
not just culturally just you know
smaller groups for groups of friends
will tend to use sort of different
language than other groups of friends
right yeah doesn't go back to the same
lodge right yeah yeah you kind of
mentioned the order of modifiers it's
some work a kind of a different domain
theoretical syntax that works on the
order of adjectives on the order of
advert as a Cinque that's pretty much
the king of this he does all that stuff
so I'm interested if you're you know if
you're looking to incorporate that kind
of stuff so the order of projections he
just explains it and I know sometimes
it's what it is yeah yeah explain that
distribution I'm wondering up here yeah
so what so it's the clm a program I was
actually starting to do this in terms of
the underlying semantic so like you
learn like red or a color tends to be
closer to the noun followed by size and
that seems like something I could learn
automatically frustratingly what worked
the best was just an Engram language
model so I it's like do I keep playing
with this stuff or I do I do the thing
that works the best you know so yeah I
kind of left it there like okay a
language model works really well that's
frustrating I'll move on to another
problem but I think yeah I think that it
would be really cool to be able to
incorporate that kind of stuff yeah
other languages oh yeah which one
radiant
yeah but presumably if you could get a
corpus you can train on that harvest in
that language and learning automatically
is is is the preference for selecting
pay maybe descriptor based on water
should be it should be possible
yeah so you mentioned early on the
concept of natural producing natural
speech and we've been talking a lot
about variation do you have any sort of
measure for something being really
unnatural so this seems like there's a
huge range of things that could be
potentially considered natural and maybe
we want the range to be narrower over
specific domains but that's all the more
reason to learn a distribution so you'd
want to have sort of a trailing
likelihood for less and less natural
construction so you're not just trying
to learn my a natural thing you're
trying to learn the set of natural
things that people might say but the
very low probability space for things
that people don't say
I'm curious
I can imagine cases like we're similar
to a bird did you find that context
where there was a shared or lack of
shared knowledge I'm like you're mostly
working with pretty basic right basic
level category two areas where so in a
lot of the sort of traditional referring
expression general generation work they
have a function that they call called
user knows which sort of starts at a
base level class and if the user is not
familiar with that base level class they
move up and if the user is familiar with
that base level class you have an option
of moving down this is still within the
realm of sort of handwritten stuff so I
haven't looked at doing something more
statistical with it but there's
definitely a precedence for looking at
that I haven't</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>