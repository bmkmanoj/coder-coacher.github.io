<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Extended Set Theory for High Performance Database Management | Coder Coacher - Coaching Coders</title><meta content="Using Extended Set Theory for High Performance Database Management - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Extended Set Theory for High Performance Database Management</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vvlULu-KiSM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's a real pleasure today for me to be
introducing Dave child's I mean that's
sort of the usual introduction that this
one is is a special pleasure I've first
met Dave over 30 years ago when I was a
graduate student and he came to give a
talk at University of Toronto um and
it's hard for most of you to think back
that far but it's some since many of you
were not around the butt but at that
time it was very much an open question
as to what the right way was to access
data the relational model had just been
um begun to be promoted by Ted Cod the
every database system pretty much had
its own way of representing data and
people really struggling to figure out
what was a really good way of
understanding data and representing it
and what would be a good mathematical
formalism in which to to understand it
and um dave was was one of the first
really to look at this question on even
before Ted cod in fact his paper on set
theory on set theoretic data model is
actually referenced in the first of the
Cod's papers on on the relational model
the
so he gave he gave a talk and as a grad
student I was always interested in new
ideas in fact looking for a thesis topic
to I think at that time and worked very
hard to understand Dave stuff but could
not penetrate all of it it was too deep
too hard for me to different and over
the years on periodically we've gotten
back together to talk and recently two
years ago at the Toronto vldb he stopped
by to see if there was actually in town
doing other things but he stopped by to
see if there any old friends around and
we I was there and we struck up a
conversation and he was telling me about
his latest views on these things and
this led to another round of
conversations where I I think I finally
um started to understand better what
what what the whole approach was about
um I think it's an important approach
it's not um much discussed these days
but I think it's something really worth
considering um dave is going to talk to
us today about his his model of datas um
which is based on his extended set
theory um he's going to be talking about
the work mostly from a conceptual level
he's not here to present an engineering
solution to a particular problem but
rather to give you a different way of
thinking about data which you may find
useful to apply to your own problems um
and really trying to give you obviously
giving a broad-brush overview of 40
years of work is not really possible so
he's going to try to hit the main the
main points um and i'm sure would be
delighted to have some discussion if you
have questions along the way um the key
point in all of this I've come to learn
his mathematical identity which I
thought I understood before hearing
hearing Dave talk about it but it's
gaining a new new appreciation for the
concept and looking forward to to his
discussion of that and related topic so
with that let me turn the mic over to
Dave
thank you feel I appreciate it I think
thank you very much for coming I like to
say one thing up front and I think one
of the problems that I have had over the
last four years is in talking about a
subject which I probably have no
business talking about and that's math
I'm not a mathematician I'm not a
programmer I'm a researcher I have
dabbled in math I dabble in programming
and for some reason they seem to work
and nobody's more surprised than I am I
think one of the problems is that people
are looking for more than is really here
it's really very simple stuff by serious
it really is and I think the problems
that I have its what people ask me
questions that are far beyond the scope
of what's actually happening imagine if
you will and just as kind of a nice to
set the framework of what we're going to
talk about imagine someone who takes two
integers and puts a decimal point in
between them let's not real mathematics
but now it gives us a handle
mathematical machinery for identifying
mathematical objects that we couldn't
identify before now i have the nothing
more than that i haven't done anything
mathematically interesting I really have
what I've done is taken set theory and I
just combined two sets by putting one as
an exponent of the other and that's
basically it so what we're talking about
here is a mathematical notation
mathematical machinery as you'll see in
the slide presentation I started back
and let's go through the slide
presentations because this is probably
going to be a surprise to both of us the
main topic here is data as a
mathematical object two things have to
be defined what do we mean by data and
what is the mathematical object this is
the whole essence of the presentation
today though I have been able to do some
things with being able to identify data
as a mathematical object that's a
separate issue which we can discuss but
that's not the main issue right now
there is no purposeful intent here the
only thing is mathematics allows us we
were able to capture data with a certain
notation that we couldn't before and so
that's the whole message which I hope
will come through there's going to be
two parts to this presentation one of
the theoretical foundations and one is
kind of hand waving the practical
applications we have done some I say we
because I had a company in 90
71 through 1985 where we actually used
some of the results to form a company
called set theoretic information systems
and we built back-end database machines
not only did we build them they ran we
sold them which was kind of interesting
so the theoretical foundations are going
to address extends just to set theory
and they're not really extensions of
just modifications of the notation and
you'll see why that was necessary and
then data as a mathematical object and
then transactions as set operations so
that's what we're going to try to
address today things that we're familiar
with like data we're just going to give
them a new mechanism for being able to
manipulate them and then transactions
data transactions which will define in a
Broadway so at least we'll have a
uniform basis for discussing the
terminology so the practical
applications are adapt transaction adapt
transactions to date I'm not sure what
that means oh yes oh I don't know what
that means today when we have a
transaction or a query we generally have
the data already organized and then we
try to optimize the execution of the
transaction of the query to bounce off
the data this in place the other
alternative is to adapt the data to the
query er transaction and so that means
just letting the data hang out there
look at some property of the transaction
namely the membership condition on what
that would be you have input which is
successful hide the output and what
actually goes on in between is not
relevant and though is how it's actually
expressed whether it's expressed in SQL
or anything else it's the input and the
output that we're interested in so the
function is a black box that gives us an
advantage if we can do it because now if
anybody specifies an input and output
that's all we need and then we can do
the function separately e which gives us
a separation functionality from
performance which is kinda a nice idea
it was called data independence biped
card but he did it the higher level
between the user and machine we're going
to do it between machine and storage so
that's our objective now there's also a
live demonstration which maintain might
not be live depending on whether we have
time to do it but we actually have on
the laptop we have raw data from books
from the tpc transaction processing
council there it's a fairly familiar
collection of data tpc h and they have
some well rehearsed queries one of which
is query nine which is a killer query
along with 18 and 21 the two of those
constitute half the time it takes the
process the suite of 22 queries I have
on my little laptop at one gigabyte two
four eight and ten gigabytes of raw
storage which we can run in real time so
hopefully later on if people are
interested we can actually look at that
as I said earlier this got started back
in nineteen sixty-five when I was a cop
the it was part of the Ark of the
Advanced Research Projects Agency which
then became DARPA now I think they're
back to arp again but in 1965 their
Charter was to have mathematically sound
systems mathematically sound has certain
attributes like generality consistency
reliability of all the things that
mathematics and viewers with a
particular system so they want to have
systems build systems software systems
hardware systems not ones that worked
but ones that would not fail so if you
have a logically tight expression and
then you can capture that in a machine
your software is going to be more
reliable in the hardware it runs on this
was the objective 1965 as far as I know
this has never been achieved so the idea
was to manage data mathematically give
it a mathematical handle so that we
could manipulate it so to do that we
need a mathematical identity for data
and we need a mathematical expression
for data behavior well we still need to
know what data is and then again what
data behaviors so we'll give some
definitions what are the identifying
characteristics of data so we'll know
whether we succeeded or not buy whatever
notation we use to catch your data its
content so it's just whatever relations
are represented by the data we could
determine what they are the structure
the form of the representation and the
behavior how the data happens to respond
to manipulation
either under some kind of a transaction
so we'll look at what those are let's
start with an example we have a simple
graph which is very familiar to
everybody and everybody understands how
this graph works how do we catch what's
the notation that we use well set
theoretically will call this h1 the idea
of the ordered pair is kind of
ubiquitous throughout math and computer
science it's very nice it's very
user-friendly it shows that a comes
before B and the ordered pair a B is not
the same as ba now these things look as
if they might be sets are represented by
sets this and let's assume they are for
the moment a goes to B is reflected by
this ordered pair CTE is reflected by
that ordered pair etc those are familiar
let's take the same graph and do a
little different interpretation of it
you haven't really changed the ground
but we've grouped this collection that
collection and this collection so a
points to be and eight points to see we
still got that but we need a little more
machinery a little more mechanism we
want to have a as the origin be is on
the left side and our C is on the right
side because that's the orientation if c
were where b is and b was over there
then it'd be a different orientation
capturing rightness and leftist if that
we're interesting at least we can do it
because now we have this group which is
now reflected by this collection it
looks like a set because it has set
brackets it looks kind of strange in
between because it has exponents on the
values but we'll ignore that for the
moment because it seems to make sense so
it may not make mathematical sense but
right now it does allow us to capture
the picture so if you look at the
picture this group B D represents that
circle there where B is the origin and D
is on the left and clearly here is
another one where we have the origin e
and g is on the right so the notation
seems to least capture two ative lee of
what we'd like it's a user-friendly
notation again we have another grouping
where we have the whole thing inside a
set brackets which reflects the circle
we have the origin which is a which has
a little o on top so that tells us the
origin and then we have this group in
between which is this group and it's on
the left now we can introduce things
like middle or more on the left or
on the left in the middle or up in fact
we can actually put if you will a
coordinate system in place of the
superscript so there's nothing very
challenging about that it's very
user-friendly the challenging part of it
makes the mathematical sense and so will
try to make mathematical sense of it in
order to do that we have to make these
things mathematical objects so again
content what is the content it is the
representation of the relationships
that's the content is the represented
relations what relationships from
wherever they are what's being
represented that's the content the
structure is the form of the
representation and the behavior is a
response to manipulation so data in our
definition is a representation we'd like
to use set notation so the first obvious
candidate is let's use set theory to
express the ordered pairs Scola min 1957
published an article which I believe has
been mostly ignored but excerpts from
that article the second of the two
remarks in the middle of it if a
position is in the literature he has
found no answer to the general question
how define ordered a triples is set and
he goes through the article defining
different approaches obviously the curt
ascii approach and others and why they
don't work in certain cases why they're
not totally desirable or some other
things he ends so the conclusion of this
article is I shall not pursue these
considerations here but only emphasize
it is still in 1957 a problem how they
ordered and tuple and title it's I have
no idea how to order one of those things
but and tuple cannot be defined in the
most suitable way and I believe that
this is still the case that is not well
scrotum is not around so he put in to
fund the other thing is that n tuples
behave very bad I don't know if you can
see this but if I have the ordered pair
a B and I intersect that with the
ordered pair AC in some sense I should
get a as the first element well set
theoretically it's well defined and the
answer is the ordered pair AAA
which is not only non-obvious it's
counterintuitive and both are useless so
doing set operations with certain sets
namely tuples is not done and one answer
is we don't do it we don't need to do it
so we'd like to worry about it I wanted
to do it because I wanted to use the
tuple notation for identifying the
mathematical identity for the object if
we're going to treat data is an object
so let's use those superscripts we had
before and use them to actually be place
holders which is not intellectually
challenging I mean we just put a 1 a 2
and a 3 on top to the superscripts and
this is not the square of beer the cube
of see it just means that this is the
order so now we have any suitable
notation these things are mathematically
equivalent they have the same underlying
mathematical object that they refer to
whatever that might be and xby again is
like that now look what happens when we
take the three tuple which in set theory
is not really well-defined because it's
always defined as a codification of
ordered pair we can now do the
intersection normally the intersection
of this would be undefined completely I
believe now the intersection is B to the
two which is no longer a tuple but at
least it's mathematically well-defined
well so far this is user-friendly but
it's not justifiable mathematically so
now we jump in go back to set theory
this is the research I did before and it
seemed easier to modify set theory than
it did to do anything else so taking the
cervelo frankel axioms and just by
modifying the membership condition and
making it a compound condition instead
of a single condition we now are able to
find membership as a two-part condition
instead of just a one part condition and
the bottom part is called scopes there
is a handout we're now this is explained
but now the justification mathematically
is there in that it does have a
membership condition at least according
to the axioms that are followed on here
now we can talk about subsets we can
talk about the usual boolean operations
usual set operations but inter
city enough we can now make up some of
our operations at our own that have no
meanings that theoretically and
interestingly not these things can
actually pre-programmed they were
programmed they ran and they seemed to
give some rather be a bizarre behavior
they run fast and so we'll look at some
of these operations and how they apply
to things like the relational data model
and how we can actually implement
relations as physical entities not as
user friendly views so here we have the
Twelve Tables that caught introduced in
this paper and in the paper it points
out that these tables are not part of
the relational model they're a visual
aid for users to effectively understand
that these things with a certain amount
of hand waving and understanding of the
notation actually refer to a
relationship now a relation the cousins
is actually the equivalence class of
relations which are n tuples so it is
really not a single set or single
relation or a single n tuple but in
equivalence class now these things are
all according to the relational data
model the same in that they all reflect
exactly the same relation in the card
sense but if we look at them this is
obviously a different structure than
this whatever those little things up
above they're not in the same order here
or down here or over here so we have 12
different entities which we should be
able to capture mathematically and call
them SQL tables or anything else you
might want I call them r sub 7 11 at the
line but they all are different but they
all have the same mathematical identity
when they're treated as representations
of relations so now let's look at the
notation we just put together and we can
see for all I so r sub I where is
between an equal to one or 12 this
notation falls out of every one of these
a to the a over there a has a little a
up above it or big a up above it little
B little B has a big be up above it w
has a see so go over to W
all the w's have ceased so in some sense
this actually reflects the underlying
content if you will from our standpoint
of data so these are data items and if
they have content which is a
representation of a relationship this is
the relationship that they reflect we
can have more bizarre examples and
annotation still holds even though they
might be less meaningful here is one
where we have a repeating group now one
of the difficulties with the relational
data model from a mathematical
standpoint is that we want to do a lot
of normalization fine we know what a
normalization is how do we
mathematically justify write a program
that normalizes something well the
output is well defined but the input is
not because the relational data model
does not allow us to effectively define
what a repeating group is well here is
one so this is not a legitimate relation
but it actually can be made into a
legitimate relation and everybody can
see that is actually reflected by the
fact that it has bc and CC and then we
can have even more bizarre things we can
actually have relations inside a
relation nested relations and that's
reflected over here we can see that we
do have this relation which is
illegitimate relation satisfies all the
criterion of exponents and or elements
and scopes let's start talking about
these things as elements and scopes this
the membership talks about an entity
being a scope component so here X is an
a component of this little group and
this whole group right here is a f
component of this group now all of these
things if we look at them can be
justified visually by connecting with
the groupings and the sets here's
another one where notice we have nulls
well instead of falling into the trap
where we have to stick something in to
represent nothing we don't we just don't
put anything there and so the net set
theory of the set theoretic notation has
a few holes in it there's nothing here
we have AQ and a G and a queue and here
we have a G in the queue the G has an e
the queue has a pee but there's nothing
in here that says there's nothing that
has
I'm on it we don't need that because
it's not there so now we can yes very
I'll try to say in my water how they
understand you so my question is do you
think about maps now one element from
their Philip call it a matte lip haha a
matte lip is if X goes to Y then X comma
Y semantics well mark Oh could we
discuss that I'd like to discuss that
with you but I'm already going to run
ten minutes short so there's only runs
through this and then discuss it because
right now I'm interested really in
getting across the idea of the notation
I'm not suggesting that this thing is
mathematically sound in any way we can
discuss that we can discuss maps and
other things as a side dish it I very
much like to I don't mean to cut you off
it's just that I'd like to run through
the rest of these slides before I forget
what they mean and so here's the another
more complex collection again this has a
notation and this is the notation and
all this is supposed to do is reflect
this now has the same properties as this
so whatever we want to do with that
thing from a user-friendly standpoint we
could do from a rather rigorous
notational standpoint and that's all
it's trying to be conveyed here that's
all that's it that this collection of
symbolism and this collection of
collections of symbols in somehow
reflect the same kind of thing whatever
that might be now here we have at the
scopes if you will are now relations
themselves as is reflected by this
notation all we're talking about here
now is notation it may be totally
ridiculous from a mathematical
standpoint but it has some utility as
far as writing programs that are maybe a
little less unreliable and some of the
things we're by the way after looking at
this I would feel a lot safer riding on
an Airbus if they use extended set
theory their programming which they
don't right now XML structures can also
be captured because they also can have a
set theoretic representation as we can
see down here so we can have an XML
structure where we have Alan with an n
which is reflected by this so there's an
easy
transformation from any XML structure to
a extended set notation so in some sense
the XML structures are actually
mathematically sound if one accepts
extended set theory as being a
representative of something that's
mathematically sound we have working
definitions now if we're going to give
data a definition as a mathematical
object so data is a representation of
relationships we've covered this briefly
before now we're going to try to make it
more rigorous data transaction is any
transformation of data from one state to
another so any representation going from
another from one representation to
another there's a data transaction it
could be the identity transaction
logical data is representations friendly
to humans used to specify enterprise
transactions physical data are
representations friendly to machines use
to support execution of enterprise
transactions so what we're trying to do
is couple the user environment with the
machine environment and show how to map
between the two what kind of transaction
types do we have logical to logical
logical to physical physical to physical
and physical theological another way of
looking at that is we have a computer
environment so this is a visual summary
where we have logic up here we have
brains we have specification called an
enterprise this environment is totally
disjoint from this environment brains
bytes logic physical ambiguities rigger
ambiguities program interrupt so this is
educated execution this is specification
looking at an extended view of this we
can now see that in the view up here we
go from logic with illogical given that
we have the idea of a logical data we
have physical to physical and what we
really want to do is have this bottom
part animate what we've specified up
above so if we can have a transformation
that captures are preserved whatever it
is
we want the content to be here down in
the p1 that's a transaction we talked
about before we go from p1 to p2 that's
a transaction physical to physical and
then we go from p2 up tail too if this
works then we have F of l1 which is
there equals B of F of a l1 equals L 2
so this all works then we have a certain
amount of rigor underneath that we can
rely on expanding this shows that we can
take in the enterprise environment any
specification of functionality and then
dynamically pick that collection
underneath that goes to support the
functionality the functionality is
constant the behavior is different so
now we can pick the behavior depending
on the state of the machine the state of
the load anything else we want this is
the ideal way of an idealized view of
what we'll call operation centric data
model it's where we don't rely on the
representation we don't move the
container around and consider the
container of reflection of the content
so if i have the same thing in two
different containers like we do in xml
structures and someone wants to know if
they're mad are they're identical if
they have the same content we can't tell
because if we don't if we separate if we
don't separate the structure from an xml
document look at the content we don't
know that the content is going to be the
same we can now do that by pulling the
structure of looking at this thing in a
global way to represent an environment
and we're set theory and the enterprise
and the machine comes in is what I've
been calling a gamma interface
architecture this is the specification
right here between the brain world and
the byte world and the only criteria for
that is anything you want as long as it
has an extended set definition that's it
the language you pick to do it the
mapping depict makes no difference there
are certain collection of operations
that should be in here to make it fairly
utilitarian by today's standards namely
it should support the relational data
operations
there are a number of implementations of
this I have done four of them and three
other people i know have also
implemented a what we would call and
what do we call it a set processor all
right so now the idea is to define the
functionality in terms of X ops or
extended set operations define a
function that embeds the EVF into the
machine environment and operation down
there called an implementation that
gives a result which in turn maps up
this is a specification of functionality
I can show where we can take SQL tables
and SQL description of the functionality
not the actual SQL layout of the
function but if you look at the sequel
statements as part of SQL it specifies
membership condition it specifies it by
how one gets it but once one understands
the input and the output ones can throw
away the particular choice of doing it
so we're not burdened by how some user
decides to write the query just extract
the membership condition out of the SQL
query grab the membership condition and
go down below this line because remember
in a game interface all we do is pass
membership condition nothing more so the
concept of indices if they use them
ignore them because we don't know what
that is because it doesn't cross the
barrier so even though the users might
want to be helpful in the execution and
help us design what we have down here
they may or may not know the state of
the world of the machine yes I'm
Felicity bch so that we kind of
understand English
better yes okay thanks i will give you
one so that you can whether you're going
to understand any better after my
example i can't guarantee one of the
difficulties in getting from an
environment which is not well ordered
into a environment which is well ordered
is that we're stuck down below at the
machine level with bits we may have very
interesting sets up a bubble when we get
down below we got bits nothing but bits
a great big long sequence of ones and
zeros how do you make sense out of
something interesting map down there
obviously something has to be changed we
have to get down into that world sets
don't exist down there now the example
we're going to use here it's called
factored sets and let me explain what a
tuple set is first of all we couldn't
build tuple sets we didn't have a
definition of tuple so we need a
definition of tuple which we now have
now we can have two poles that are also
the scopes and this is going to be value
in a minute valuable a minute I'll show
you why at least from the notational
standpoint imagine if we have the
situation where an environment the e
environment we have the number 24 in the
M environment we only have digits 0
through 9 how do we represent 24 down
below well we can do it not just with a
structure but with a structure in
operation like 6 times 4 8 times 3 both
of which have the same mathematical
identity mainly 24 I think that's right
yeah both at the same mathematical
identity of 24 so we're not stuck by
taking 24 and jamming it down here and
saying well we don't have a 24 down here
the same idea of factoring into a
collection of structures and operations
on and between those structures allows
us to mimic if you will the same
mathematical identity below as is up
above how do we do that we're stuck
effectively with an n-tuple environment
in the machine world so things which are
not in tuples up they're not codified
this into it without look like an tables
have nothing to do then tuples have to
map down to an end to presenting that
preserves the membership condition
that's the data the relationship has
been
the content this is called a tuple set
the tuple set has a very rigorous
definition all the elements in there are
effectively tuples that have the same
degree on the top and the bottom that's
a tuple set so here is an example of a
tuple set a be with 69 c with a it
doesn't say that all of the tuples have
to share is just that the particular
tuple pair has to have the same if you
will degree so this has degree one
degree one degree 2 degree to this whole
thing has degree 2 3 1 2 3 and the scope
again has degree 3 i'll show why this is
relevant or can be relevant in a minute
this is another one which i assume is
correct i can't figure it out I think
there's it's a 2 yeah it's a this this
set goes with a and this 12 goes with
this ordered pair yeah I think that's
right now array relations we are going
to use the combination of two n tuples
to effectively reflect the membership
condition that has nothing to do with
intervals now this operation will take
and associate this bottom and tuple with
this n tuple this operation effectively
is defined to give this set this
operation here is the same operation it
gives us set same operation gives a sec
these three things are different now if
we have a storage environment we can
store store n tuples if we have a set of
these guys all of which have the same if
you will domains we could set the store
this as a big table and this is kind of
a domain contribution what we do now is
grab any end to poulette it with this
with this operation and we get that set
which means that we have this set up
above in
the enterprise environment we can now
reflect that down below in a machine
environment where we have nothing but
tools so we have this operation which
only exists in the context of extended
set notation but it does allow us to
effectively map back and forth between
membership which has no ordering and
tuples which in fact do so we have this
operation down below which allows to do
it so above the scope inversion it's
actually defined in a paper that we were
going to hand out but we ran out of time
so the paper is available all these
definitions and the axioms are available
through filler for me through me now let
us look at a real world example where I
have as this reflecting maybe people's
names where its income age name age
income name and name age income again
all of these effectively reflect the
same set containing the people's name
their age and their income even though
down below in the machine environment
they have three separate representations
this I'm calling factored setting for
lack of something better to call we call
anything we like but I'm calling it
factored sets now what we can do is take
an SQL table namely this t1 which looks
like this and this is a tuple set if we
effectively assume that a is a a tuple
and we take T 12 which is this set right
here they both have a set theoretic read
lips they both have a set theoretic
representation in fact all 12 of them if
we look at the either an alpha Rho Sigma
I don't remember which it is but when
I'm writing I don't have to know so is
this Alfred Sigma Sigma is everything
Sigma so sigma sub 1 and sigma through
12 is the effectively the call of the
scope operator on a sub I and you'll
notice if I take any a sub I along with
the appropriate Sigma I get the same
relationship that's where we were before
when we started that's where we had the
SQL tables so now SQL tables with SQL
tables have a legitimate representation
on their own
and they have a legitimate mapping into
a storage environment there's two
information access strategies one has
adapt query to the data and the others
adapt our data to the query I'd like to
show how the whoops this is supposed to
go before this this this slide is my
attempt at doing something mathematical
or at least trying to do something
that's radical this is supposed to
represent a category with a k as opposed
to endanger to be associated with
categories of the sea but all it was is
trying to use the notation to be more
rigorous and to provide a representation
on this this is still in the
experimental stage I'm waiting on my
head here but it was a pretty picture so
I thought I'd included here is the
transaction processing councils an
example of the tpc benchmark and I think
this is yeah this uses the sequel server
I don't use a sequel server the
interesting thing about this and we're
knowing that the performance end of the
promised part of the lecture if you look
at query you'll have to take my word for
it this is query 9 and this is query 18
and this is query 21 notice those three
take the longest time to execute now
unfortunately the way the tpc benchmark
was set up if I have a situation where I
take something like this and say it runs
for an hour which it doesn't here but if
I cut it down and it only runs a half an
hour and someone else takes a query
that's very very short and makes that
even shorter the overall effect
improvement reflects the same on both
systems well this is a no value to a
customer who let me move ahead to a
customer who wants to get performance
forget about the cause forget about the
weight of the machine forget about
anything else I just want the answer
before my competition according to the
benchmark query these ten systems
have this kind of ranking if you look at
the way they actually produce raw data
to output in the shortest possible time
they now have this ranking which is a
little bit different if you look at
repeated quick this is starting from raw
data to completed answer if you look at
starting once we have the data organized
and all we want to do is get repeated
queries through the same group now look
what happens no no no it does but when
you mean the point about the the change
in the no that's because of the
geometric mean that they used actually a
major that's the only planet right there
but I do have another point of puffy the
time yes but thank you for bringing that
up but this is um this is the kind of
performance that the user at least the
people that I deal with actually
experience when they're buying a machine
and using it they're not interested in
the geometric mean they don't go there
customer and say yeah but it did take
you two days and you did get it from
someone else in half a day but and if
you look at other certain reports on
this particular system by the winter
corporation that explains why this is
the most valuable system to buy I
strongly disagree with that analysis by
the way here is query 9 I'll going back
to James's point if you look down here
which you can't see the total amount of
storage used for this this is a hundred
gigabytes the total amount of stories
used for this is a 4.1 terabytes 4.1
terabytes in order to get an answer to a
hundred gigabytes of raw data now that
may be very nice and very productive and
very interesting I don't seem to
understand why that's the case and I
have on this little laptop which has 50
gigabytes of available storage I have
one two four eight and ten gigabytes of
raw data all which can run on that
system and strangely enough faster then
it can run then these systems are
actually running now this test has been
done by information builders rel stuff
and that's what I wanted there's also a
handout that refers to that here is the
actual query the SQL query query nine
and as we said earlier all this does is
specify given the input which are the
domains of the tables being used we can
call these scopes because I refers on
the scopes and ABNA scopes these are
user to find operations on values
depending what the scopes are so these
are given this is given and the
membership if you look at this in
totality is also given here is the
actual program that i run this is part
of the game interface these operations
right here are at the game interface
this actually works on storage p here is
actually assigned to a physical file l
is actually a fine to a physical file
these are the scopes that are going to
be used for scope driven operations as
we said earlier we have something down
in the mathematic and then your machine
environment we need two things to
represent a set we need the actual
content itself and then we need that
little guy up above that allows
operating on it and extract the
membership condition yeah we say operate
on the files just tell us a little bit
about how you're physically organizing
the data
well okay James I will in fact I'll show
it but according to this it doesn't
matter in other words see I don't use
indices now it turns out I have been
told that if i did use indices this
thing would be even faster i double
buffer and I'm told that's a no-no I do
a lot of writing which is I don't need
to do I don't do I overlap which I'm
told is a good programming technique and
I don't use caching so I don't do
anything right from a programming
standpoint but it still runs 40 times
faster than doing something right my
point is not that one should say hey
this is great stuff let's throw indices
away no I'd like to put this tool in the
hands of people who know what they're
doing and instead of just running 40
times faster maybe 160 times faster now
this is actually roust out who did the
evaluation for ib i looked at my code
the point of the following so up
on let's just leave like that ok can we
do that after I've done with the lecture
I just like to get through just wrap it
up I'm just trying to give a
presentation here it may not answer all
questions it may raise more than it does
i really don't want to defend this right
now but i will in private ok and if you
can convince me to get out of the
business I'd be glad to because I've
been wrestling with a stupid thing for
40 years no I don't mean to be you know
I'm not the meaning your question I
would like to answer it I would like to
understand it right now this is just
supposed to be a summary overview and
quite honestly and quite honestly what
I'm trying to do is get your attention
get your interest because in 40 years
the only thing I've been able to do is
clear a room with my presentations
this is an example of the test that
Ralph stout rant and by the way I'd like
to put you in contact with Ralph stout
because he is a longtime database person
he knows databases I don't he was one of
the principal designers the inane idid
BMS ID ID ID ms for GE when they were in
France in fact he had they had to go to
France in order to do it because it's so
radical at the time but he knows
databases inside and out and your
questions probably I could make a lot of
sense to me but I'd like you to beat up
on Ralph and he's a very interesting
guide very tolerant and he's the one who
actually ran these tests and this this
is the results of the tests that he ran
and this was I think about four years
ago but it did use a db2 machine and
this is actually 1 2 4 8 1 2 4 8 and 16
10 gigabyte yeah attend gigabyte raw
data input now this is on a machine
where there is only 32 32 gigabytes of
available storage only 32 gigabytes the
IBM took an hour and six minutes to load
the data and to do query 9 once so this
is the load time and the query time four
years ago on the same machine Oracle
took an hour and 54 minutes to load and
do query 9 the set processor the little
thing that's on the laptop took a total
of seven minutes now there's something
wrong here obviously and there's
something that needs to be explained and
Ralph was the one that actually did this
and I am not prepared to justify it I
just wrote the operations to see if they
worked they did work why they're faster
i really don't know quite honestly
I have some suspicions I've been told
that it's because we do by doing it this
way you're able to have information
against iOS and that may in fact be true
the point is that even if this is true
it doesn't violate any of the
characteristics or any of the properties
or any of the techniques that people are
now using they can be incorporated with
this this is not a replacement approach
this is an alternative nothing has to be
changed just can be added on and we can
go over that I'm looking forward to the
discussion really because I hope I can
answer a lot better than I happened so
far this is the two hours and 16 minutes
for a 2 gigabyte input database for
hours and 21 13 minutes for hours and 10
for a 4 gigabyte raw database for hours
10 minutes 7 hours 26 minutes obviously
this is a not acceptable and it gets is
not believable these are the results
that Ralph got using my software I had
absolutely nothing to do with it I
wasn't involved and here it fell off the
machine because the stories required in
order to do this was more than the 32
gigabytes would allow well this silly
little program wasn't concerned and just
kept rolling along and came out with
some answers so obviously this is
fictitious but it had something had to
be put there number of subsequent
reports this is after everything was
already loaded after the data was
already there and the collection of data
was geared for repeated access to query
9 yeah please I'm sorry did you run
let's say I went to run and mix up
queries concurrent yes I mean we can
talk about that yeah this does not
happen to address that particular thing
go this is just a collection of say
somebody had a a bread-and-butter query
that they want to keep running and
running and running and running and
running let's say it's query 9 and let's
say they use this particular machine
configuration they would able to be able
to get a hundred and fifteen reports
into 24-hour period versus one point
versus 7.8 not even a full report to
reports 66 reports there's a certain
competitive advantage here that
businesses might be interested in even
if it doesn't really work the way
database people think it's supposed to
there is a way of making money off of
this as people have done they haven't
been concerned with the fact that it ran
on set theory fact we generally didn't
tell them we just did this ah yeah here
at the end this is again to show that
adaptive query to data is the generally
traditional way of doing things you take
the data you organize it you analyze the
queries that are supposed to be bounced
off of it and then you adapt the query
to the data and I'll turn if not a
replacement is to be able to adapt the
data to the query by doing dynamic data
restructuring underneath by using set
operations at the GAM interface that's
it we get questions now questions are
fine thank you Yuri
don't know anything about databases but
but they think the mathematics was
crystal clear it helps
suggestion how can you oh sure can I
here's your cup tuples in the very
beginning kooples
two please okay no is applied you want
yeah this way okay yeah pictures I think
you think music about 20 that's okay
okay then which has to be something I
call the madness one matter if he is
able to what happened
and map is a second moment so here you
have a goes to B goes to those would be
so set of three methods
you in person what month is it makes
so here also temperature obviously you
have people able or an applet vigoss be
so you have each each tuple becomes a
set of numbers the pre-market risky
methods that
can I ask you a question okay let's say
we have this situation right here now
what would your map let's be here when
you head tables inside tables inside a
young but this is the one that I think
is critical at least for mapping for
implementations because here we have I
can say what they didn't understand okay
no this is in the Machine environment
because I can represent that as a record
in the machine okay but this is your
representation but you have to
mountainous you're able to see people's
enough as opposed to this whole thing
going to that whole thing can you show
which one bar I don't know imitation poo
okay in the beginning and was there were
the stables already put this one right
here yes yes ok ok so here it's very
very clear what goes on also let's see
you have
here is
map it's T goes along w goes Jerry
so there is set of map picture now the
whole set forms the map is set theory
that's fine i mean that's an alternative
way of doing it that doesn't have to be
the way I chose I mean the way I chose
is the way it's easier for me because I
don't I don't argue okay I just want to
explain that it is very rigorous and
simple so all good boo so this is
starting with map of asthma as the base
concepts yosef I'm may elementary
message so the society said so there are
two things it first and also the map it
not is composed of something called the
mattress and then mark becomes a set of
magnets
and if you think in these terms this is
all completely legal okay first so you
have map let's then you may have set of
Matloff as a beginning as a local
housing next part of another man's
and so you have this is built on
recursively yes start that's all over oh
my god yes dickham to party I think the
first part I think I got where it's a
rich no to that rich mathematical
notation to model rich data so you can
model sparse table s in tables repeating
managers and you can model
transformation so you can say this is
what's supposed to happen any illegal
exploitation has to produce this result
I think I get the second part was i can
bring the data to the query rather
according to the data the second part i
don't think i've got yet i'm guessing
that you're reorganizing the data
dynamically in other words you're
partitioning and reorganizing
if you could talk a little before but
what you're actually doing the
interesting gonna cost you drink address
it's not kind of coming to what you were
saying the idea was the same of you like
this superscript or appear this goes to
that of until but I would be careful
about using the working up here because
in what you've got it's entirely
possible for the same elements to be
there with with two different scopes and
that's not what you would normally wrong
map so it's more like what you know may
have a relation because you have the
same element with different scopes the
same scope with different elements on
mix together
I didn't you were mad why didn't you run
functionality just you have a multi set
of matters
address you mind if I point out to the
audience that you've been keeping me
honest for about 10 years now okay what
are you giving operators that you used
to kind of man this okay I came up with
one collection someone else they've come
up with other like I said I can show you
the ones that I have come up with but
any collection that works is a
legitimate collection i will show you
the ones i have in fact i have a number
of papers not be glad to send you the
papers that i have it no tell me that if
i give you let's take to PCH you know
yeah at the execution level you still
have the notion of an execution plan in
order general eric operators what is
those operators what do they do um
keeping yeah they get so yeah so yeah
step we go through that so the so let me
understand how that actually equivalent
to the sequel oh oh I know what you mean
my right here so many things most people
your atmosphere in the next little sound
level oh ok oh good yeah this guy got a
pirated this directly from God if you'll
notice that this right here looks very
much like the like or I returned from
sequel so this is actually a like with a
restriction on it nothing more the
operation here this right here is the
n-tuple offset so this is something that
I know the scopes from up above and this
is the fifth bite I take five bites so
thats a scope representation of the way
p was effectively organized the scope
start the step step back so you've
loaded from the file into these sets oh
yeah okay are you okay cutest Angie
those physical objects as mathematical
sense right so what we did here was in
NP p is actually seven different domains
query nine only uses domain one and
domain 2 so the first thing we do is go
in and do what's called a vertical
partition throw away everything that's
not going to be used a as Phil would say
it's kind of a superset me dudes it's a
projection yeah you start off by
projecting the raw data to do all the
fields that are relevant for whatever
queries sweet you want to run is that
part of the ghetto where that comes no
that was that was before that was this
is this is does not have the load the
load was done prior to this this is just
the execution of query nine the load
actually gave these uh 1 2 3 4 5 6 sets
so there's a load operation prior to
this that did nothing more than a
project on the original tables so just
by looking at an SQL query let's go back
here just by looking at the SQL query
you know what files are going to be
necessary what tables are necessary and
you know what domain so these are the
scopes so if you're looking at any other
scopes but the ones are included in the
system and they'll all be going to clean
water to wash you load everything
in fact you take all yeah oh sure yeah
so I mean konami squares on the
continent but then what you do see is
you put in vertical partitions in
addition to vacuum at least redundant
really comforting has to hear when my
point is wear helmets I think it has to
be but I I'm not sure okay no so you'll
be taking then that data set I'm just
taking down redundancy and as a new
concurrent that's it yeah no that's
that's what that was the revelation that
Phil made you mean you're writing and
sorting guess you think of it as a fire
hose yeah I do nothing more a lot of
people are concerned with access times
i'm not i'm concerned with data transfer
because i just did one access and fire
the whole bloody thing through yanking
out which is going to be necessary and
that's it and then I store that and
never go back to the original table
again so that's fine but I think the key
difference it really understands how the
if I just restrict to the slicing of the
data that I need I mean I think this is
well understood that's not new what I
think I'd be making is a performance
game which is that you can keep only one
copy in other words you are not creating
it per plate per se an uncorrectable way
comes and the vertical positions
completely will change underneath with
all the concurrency I use them I use
vertical partitions from the vertical
partitions so you take the largest you
take the addition hi dude I know that
the query time it's part of a query
cause then you really need to give them
their thing because other well I don't
know if it's going to go you have a
collection in other words let's take the
the sweet the 21 queries where the gun
zombies in base but all you take the
union of all the domains all the Scopes
that are going to be used in every one
of the two hour 22 now that happens to
be every scope in the thing then you
don't do anything turns out it's only
ten percent for the whole 22 queries for
instance we put together a learned the
information systems for General Motors
and very seldom when they were looking
the windshield wipers do they need to
look at axle housings so the a partition
the group's off into collections of
these things but the collections
constitute on a joint the full database
so instead of having one table we ended
up with 200 but all of it but all the
data was still there all the data nobody
the partitions yes team said I'm well
under students are some level yes all
the commercial patients we're not
implemented by this understood what I
mean beyond that point what leverage is
your algebra or your formulism giving
that's less clear ok we will go over
that I also would like to I have a
little floppy disk I have a floppy disk
that we can run on a if you have a
sequel server and we can run these
things Intendant just have some fun
what's happening oh ok yeah but I do
have the the things that we can run hey
I'm going to be here for the next couple
of days if we could get together we can
play on the machine and i'll i'll show
you everything that I'm doing
the impression I get is your partition
every column all the information
now actually I've taken the whole
database and bifurcated the things into
ones are going to be used and ones that
are actually archive for the moment
they're still available but they're not
part of the current query but they're
still sitting there so instead of having
eight files I now have 16 but instead of
hitting you know the thing where I throw
away ninety percent of the data I only
hit the one thing now what i can do is
have more of those where the groupings
are in fact overlapping and redundant
there's nothing to say you can't have
overlapping redundancy the reason I said
no earlier was because this particular
case was not necessary but yes you can
have redundancy there's nothing wrong
with that at all in fact there's a good
way of improving performance and there's
no way that you can use up four
terabytes of storage doing it that way
sounds like your petitioner and then
partitioning after you do the vertical
partition because you know you do the
vertical partitioning but then you
partition that on the fly which keeps
giving you a smaller and smaller
collection of the data it's like an I
was told it's like an ice skater
spinning around with their arms up and
you're going through the core you bring
your arms in the query just accelerates
faster and faster because unlike indices
instead of tripping through the same
maze going for smaller and smaller
amounts of data the major goes who get
smaller so t PCH is a special case where
there's a fixed and relatively small
number of queries but
if this was a system that was getting
random queries coming all the time they
keep getting different wouldn't your
partitions keep getting smaller and
smaller and smaller as a consequence
here an example of that was the mornings
born information system will be put
together for General Motors and that was
where they had all five divisions and
all their data and they wanted to do ad
hoc query of any question at the time so
the first thing we did was group them by
Cadillac Pontiac and this thing yeah you
know and so we did a partitioning by
getting rid of a 1-bit field well if you
have you know 60 million records and you
get with everyone bit you just saved
yourself 60 million bits which can use
for other things so we did not just
vertical partitioning but we did some
other kinds of more set theoretically
complexing but the point is that the raw
database instead of one file now is over
200 finals and the first thing you did
was the data in the metadata you asked a
query and the first thing you do is say
well what clumps of data out there talk
about windshield wipers why look at any
that don't if that your question has to
do with windchill wipers so there's I
asked a query that you there was not you
were unaware of at the time you did
database design with the system with the
system
dition the data again do anything these
are tools you have addition if you add
three to six you necessarily have to
multiply 7 by 4 know depends on what
your objective is this is a hammer
nothing more I have no strategy at all
you say if if I had a strategy you can
you do whatever you want with if I had a
strategy for doing further partition you
could do it look at this notation and
operators do that make it a lot easier
and a lot less software probably in
between it gives you a mathematical
control and this to me this is it gives
you control give you no strategy no
purpose nothing it just give you control
the winter say is in the numbers you
show that the disappearing not and in
DBQ has like 10 minutes and now you run
like seven minutes this is because you
pre partition the data for this
particular curve and only the boolean
data this week barca that seven minutes
included the load time but this is a
premium include an audition time yes
that was in no way ahead he's got 40
times less data than I yes that included
the lowest i realized views that that
seven minutes included everything from
the raw data we started in the same
place from leave me again data DB gents
list included the total elapsed time not
what the PPC include which is okay the
low time is a gimme it said we start
with a warm day base no we start a cold
in both cases impressive because you
need to read the data loss right you
need to polish my own and we did you
show us that comparison chart too yeah
see look the thing is if i had 100
gigabyte add something like seventy two
megabytes per second how long does it
take me it doesn't take me three days to
read and any balls are not doing random
give us 15 Commission of what is the
original state of the table don't you
see the table DB gen DB gen the thing
that no no I DB jen is the to deceive a
benchmarks way of generating the raw
data IBM starts there Microsoft starts
there everybody starts there they just
don't include the time it takes to get
to the object before the query we
included the time that seven minutes
included the time from the raw data
everything you put the chart up for just
a second ah which we should forward stop
back up right right here right so the
tot see the performance difference even
with the total lapse time is on the
order of 10 to 20 but he's got 40 times
less data but this oh this is quite
credible that you're even including the
load time of me because he's not
generating all that extra all that extra
data just loading it once and then and
then running against it against
no a custom partitioning of course so he
had to learn that gold data SBB do yes
and that's included here leave access to
that data gets the whole time off and so
on the load time is here any queers for
that specific query has set of vertical
partitions right he cannot get away when
loading the regular data because he will
need it for some other thing funniest
reloading again for that query from
scratch and I don't know how you want to
do this bad I don't know if I can borrow
one if we brief like staff because this
is you can be loaded they filled all the
indexes I just I don't but if this
bottom number here is where both systems
have loaded the data for query nine so
both systems are loaded and the low time
is not included here this is a time it
takes IBM this is a number of queries
that they could run in a 24-hour period
with the database already loaded this is
the number of queries that we can run on
the same machine with the same data
already loaded for query nine but these
this is only loaded for corey nine they
were both only loaded for query nine
this is only loaded for query nine
realms down did these tests it was only
loaded for Prairie nine mrs. Dawson mean
the index equals not measurable for
query nine in both cases with Oracle
watch of the miser 49 IBM optimized for
query nine and all we did here was
pulling the scopes that were required
for query nine no optimization much less
violent yeah here's the key as was
pointed out that if you look at a
regular system today the i/o buffer is
only fifty percent full of data and of
that day to only ten percent of it is
informational II dense so your own your
iOS in a regular system are anywhere
from one to two percent informational II
dense these are about ninety-eight
percent information against much fewer
iOS
I didn't evenyou education I haven't
done anything intelligent yeah yeah yeah
jhansi I was told that if a program
correctly using caching not using double
buffering and night and using
overlapping I owe that we could do maybe
24 times this because IBM is already
doing quote good programming this thing
wasn't all of which falls under
mathematical control all the predictions
be showing a meta index and that you
only have to look at the data it's
interesting exactly yeah but it's all in
the mathematical control you can rebuild
everything back from where you were so
you haven't lost anything so the essence
here is these things are mathematical
objects before up to
I'll be around for the next couple of
days that maybe your maybe we'll have
fun go out of one-on-one anybody else
want some fun reserve it let me know
Justin schedule thanks again you can do
two or three on one if you like thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>