<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Improving Configuration Troubleshooting with Dynamic Information Flow Analysis | Coder Coacher - Coaching Coders</title><meta content="Improving Configuration Troubleshooting with Dynamic Information Flow Analysis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Improving Configuration Troubleshooting with Dynamic Information Flow Analysis</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Buti990D3CM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright well it's my pleasure to welcome
moaning atariya here and moan is getting
her PhD at the University of Michigan
working under jason flynn and many of
you probably know about it already from
the recent internship here so a lot to
go ahead and start a talking about it's
about troubleshooting and information
flow ok thank you very much as a
pleasure to be here it's great to see
all these familiar faces so today I'm
going to talk about a soft and
configuration troubleshooting and I'm
going to tell you how we use dynamic
information flow analysis to improve
this problem so software systems are
very complex I want my stuff software to
run faster to give me more features most
importantly I want to be able to
personalize my software so you're
constantly pushing her software to be
faster and bigger and better and that
has made our software fundamentally
complex so now the problem is that when
something goes wrong the troubleshooting
becomes very difficult so the problem
troubleshooting it's very time-consuming
it's very tedious it usually requires a
lot of expertise it's also very costly
to corporations so let's see how what
causes software to have problems in the
first place um so here I show you in a
study that was published in 1985 it's
the classic a study by grey and he
looked at the outages of the root causes
of our outages in a commercial system so
I want to draw your attention to this
big red part about forty-two percent of
their outages we're due to
administration problems and that's
basically configuration and maintenance
so 26 years later this is another study
those published just last year less OSP
2011 and again so they looked at you
know the root causes of severe problems
reported by the customers of a company
that provides the storage systems and
again you can see that the biggest root
cause is configuration about thirty one
percent so there is many other studies
during this year's that actually suggest
the same idea that Miss configurations
are the dominant cause of problems in
deployed systems and these are severe
problems these are problems that lead to
performance degradation they
your system to be partially on foot or
fully unavailable let me give you two
more stories on the impact of Miss
configuration problems Facebook went
down what couple years ago went down for
about two and a half hours and it wasn't
reachable to millions of users so many
people didn't know how to waste their
time anymore and at the problem turned
out to be an incorrect configuration
value that got propagated another story
the entire day is a domain for country
Sweden went down for about one hour it
affected thousands of hosts and millions
of users and the problem turned out to
be a DNS miss configuration so these
kind of problems happen a lot in
software systems and when it happen
they're very difficult to fix and as I
mentioned they're also very costly so
reports your dad for instance technical
support contributes about seventeen
percent of a total cost of ownership for
today desktop computers and the majority
of that is just troubleshooting
configuration problem and there many
other studies that suggest the same
result so in the past couple of years
I've been focusing on the problem
improving configuration troubleshooting
I've worked in several different
projects and have developed several
different tools I've worked on a project
called auto bash it was published at SSB
07 and it basically provides a set of
tools to the users to help them fix
their configuration problems more easily
I developed a tool caustic humph that
diagnosis misconfigurations using a bug
database and my most recent tools are
confident x-ray and they both are
diagnosed misconfigurations that come
from configuration files conflate does
that for Miss configurations that lead
to failures and incorrect output and
x-ray does not for performance
misconfigurations and I'm going to talk
about these two today alright so the
goal of my research is to help two types
of users first is end users who might
just be having a problem with their you
know an application on your personal
computer and the second is the admin
sergers who might be in charge of
maintaining other system Nani's users
aren't necessarily you know necessarily
have access to the source code of the
application or they might just simply
not be interested in looking at the
source code or might not have the
expertise to look at the source code so
when they have a problem what happens is
that they usually you know try different
things that they know try to fix a
problem on their own and if that doesn't
help the next step is usually to go
online look at the forums and you know
look at the manuals and try to see if
other people had similar problems and
you know basically using your trial and
error process to just try a bunch of
different things that people suggested
and see if that helps that fixes a
problem otherwise you try something else
I personally find this to be very
frustrating and very tedious and I think
this is why people hate computers so
much because when something goes wrong
it's just so difficult to fix it so
would it be great if we had a fix-it
button and what it does is that when
your programs I'm working you just say
fix it and it would magically start
working so you're not there yet i'm not
going to give you that but you can still
do much better than we are doing right
now so how about I give you an easy
button and what is that it does is that
when your program is not working it
would give you a list of potential root
causes and what is almost always the
first couple of root causes that gives
you are actually accurate but are
actually correct that's exactly what
come on fate an x-ray to confid use you
this list for Miss configurations that
are lead to failures and x-ray does that
for Miss configuration that lead to
performance problems yes we're going to
when you type pro
yes but you have to go through it and
most of the time it's you know you read
this it's completely incorrect you try
it your system is even worse than where
it was this can be incorrect here it's
just heavy rain to see later I'll show
you how we rank them it's better than
google result are you um practically but
the thing is that and you know the
problem with you know searching for a
problem like that is that it's all and
you know it's all in English you you you
basically come up with a description of
your problem and then hopefully someone
else describe the problem the same way
that you did and hopefully it will show
up you know most of the time you just
read the forum and at the end nobody
really came up with a solution so it's
it's it's a lot it's a lot more
difficulty that sense and what we try to
do is to you know give you okay this one
and this one go look at these and these
are hopefully your your answers so okay
so let me out tell you a little bit
about the core idea behind confident
x-ray here's our observation we have a
program it reads from configuration
sources it does something and then
generates an outcome that's incorrect so
the program is like a black box we don't
know how it use the configuration
sources to generate the the article
however the application itself knows how
it got there so our if we open up this
black box and if we analyze how the
application is running yes in your world
like typically programs have other
dependencies besides their own
configuration
we're gonna take those into out or not
what do you mean you be like employer
you mean and things like dll's on the
computer for instance or intra somebody
using some it's a program using some
other libraries then those libraries not
being compatible so it's not that
programs configuration is compatibility
issues between programs that's a good
point so in general so most of that
actually goes into under the
configuration the general term of
configuration like using different
libraries maybe and so so that is part
of the configuration I don't
specifically look at the problem of not
you know how to using the wrong yellow
or not being compatible but in general
that falls into the category of
configuration so usually say okay this
is a configuration of the system and
this is the actual input of the system
so are these are you know the main two
inputs that go into the system I look I
look at specifically at the
configuration of the application but
that's definitely another source of I'd
say configuration again okay so um so if
we open up this black box and we look at
how the application runs how it uses
these sources and how it generated uses
this order to generate the outcome then
we might be able to infer which one of
these configuration sources are causing
the outcome to be incorrect so basically
you know if we analyze the program as it
runs then we might be able to to infer
what's going on so that's the that's the
main idea behind both confident and
x-ray okay so this is the outline of my
talk I'm going to first talk about
conveyed I'm going to give you some
details on the algorithms that we used
to do the analysis they'll just
described and I'm going to talk about
some of the heuristics that we used to
make it more practical and then go
switch gears and talk about x-ray and
then I'll spend some time and talk about
these research directions of like to
pursue in the future and I'll conclude
all right
so um so let's see if you have an
application it is something from a
configuration file ignores for a while
and an error happens you'll like to know
what parameter in the configuration file
is causing the error to happen at the
end so I stake a look at this very
simple our code the application reads
the token the token is equal to X 60 g
references in this case therefore the
variable execute cgi is going to be set
to one later on because the variable
execute CGI's is equal to one the error
happens so you can see there are causal
relationships in the execution that
basically connects the XX tgr a variable
to the error that's happening at the end
and confidence based interested in these
kind of causal relationships and we use
ten tracking which is a common
techniques used in security to find
these causal relationships so here's
what we do whenever a token for instance
here XX CGI is read into the application
we assign a specific taint or mark to
that and then I the application ruins we
use data flowing control flow to
propagate this taint and then when we
get to the error we can use this taint
to link it back to the configuration
token that caused it so the goal here is
to avoid the earth of your sing at the
end and also not lead to new earth so
the goal is to find a successful path so
here here is a simple example we hang
out we have an application it has three
if conditionals each of them is
dependent on a configuration parameter
and we have an error that's happening
again we would like to know which one of
these configuration parameters blue or
red or green can be the root cause of
the problem that you're saying at the
end so the blue option cannot be the
root cause because even if you change
that if you manage to get the
application to take the other path you
wouldn't still merge before the error so
we get to the same error at the end so
it cannot be the root cause the
read-option cannot be the root cause
either because if you change it and you
get the application to go the other way
you would avoid the original error but
then you would lead to new errors on d
path so the read-option cannot be the
root cause either the green option
however can potentially be root the root
cause because if you change it you would
avoid the original error and then it
seems to be successfully continuing and
not leaning to new errors and that's
exactly what what conveyed returns a
list of root causes that have changed a
list of configuration options that have
changed would avoid the original error
and what wouldn't them solidly to new
errors all right so now I'm going to
talk about the algorithms that we use
for tracking gasbuddy question these are
great maybe the new cast butter wouldn't
be like a combination rather fast queen
that will lead to that pass yeah so this
is a very simple case for instance let's
say you know if you have you know an
option here that's dependent on both of
them of course but this is a very simple
case where you're assuming that this one
is only dependent on degree of course
you have you can have cases with green
and red and we can we can tell you that
but this is just very simple simplified
case on the ranch right in the second
conditional yeah it goes left then you
wouldn't trigger it right yeah so if it
goes so the idea is that if you change
red and it goes left then it's not good
because it you'd see a new problem and
that's not what you want so so if you
change red then you you won't see that
error that you were saying but you see a
new error and that's not good either
values and there's a third branch it
doesn't lead to another error if so if
it's possible that it doesn't go here
then we consider that this is the case
where we know we would get go there
okay so okay so I'm going to talk about
the algorithms that we use for taint
tracking now so before I get to the
details I'm going to talk about why we
decided to do 10 tracking so information
flow analysis in general it can be
implemented in multiple different ways
you can do it a statically and
dynamically you can use symbolic
execution just to name a few why did you
decide to go with 10 tracking so we had
several design principles in mind and
that kind of loss to this decision first
of all before that a practical tool
cannot rely on the source code of the
application simply because for many of
the application they've use every day we
don't have the source code so it has to
rely only on the binary the second point
is that a practical tool has to be able
to analyze complex applications because
these are the kind of application of
usually have problems with so you have
to be able to analyze complex data
structures has to support multi-threaded
inter-process communication and things
like that and the third point is that we
need to have a reasonable reasonable
performance so the good thing about
would think about troubleshooting is
that you're competing against human so
we don't have to be extremely fast you
probably won't mind you know waiting a
minute or two for this pathology to
finish but you probably do mind waiting
20 hours you might just as well go and
Google as you don't find find the answer
so we need to have reasonable
performance other implementations of
information flow analysis that we had at
the time fell short in meeting at least
one of these criteria so we decided to
go between tracking and k tracking as I
mentioned is actually a pretty popular
insecurity so um so here I want to
suggest that it actually might be a
better suit for for troubleshooting
problem compared to security and here
are a couple of reasons first of all our
environment is non-adversarial the
developer of the application unlucky
hacker does not have an incentive to
bypass our system you know it worse
they're going to be agnostic your system
and at best they're going to actually
write the program in such a way that
lends itself better the type of
heuristics that views and also um I
think that performance is probably less
and less
an issue for us because you know again a
couple of minutes might be okay for
troubleshooting but if you have to wait
a couple minutes every time you want to
load a webpage that's probably a problem
so for these reasons I think time
tracking might be a better fit for our
problem compared to security security
people if they object okay so let's get
you details up here so me I introduce
our notation T of X is a taint set of
variable X and it includes all the
configuration tokens that if changed the
value of x might change and I used color
triangles here to show up different
configuration tokens tens propagates via
data flow and control flow here's a very
simple example of data flow we have X is
X equal y plus see the text of Y is red
and blue configuration tokens and the
tents of Z is green and blue so of
course if any of these tokens change
potentially the value of x might change
as well so the taste of X is going to
have in all of them the union of two
sets take also propagates for your
control flow many of the systems that
implement a tracking actually ignore
control for because it's it's expensive
and it's more difficult however we
realize that for our purpose is actually
pretty essential because most of our
teen gets propagated your control flow
so here's a simple example we have you
know a conditional that stated here see
and we would like to know what could
cause the value of x to be a different
at the end so of course the value of x
could be different because it is
different and that's your data fall as
it as I just explained the value of x
could be different because of the value
of C because if you change see you might
get the program to not run this and then
therefore the value of x could be
different there's another subtle way
that we could change the value of x and
that's by changing the previous value of
x and at the same time changing see to
make an application to take up the else
part with this other part but note that
both of these need to change at the same
time to give us a different value for x
so i'll let me introduce to you our
first heuristic coffee currently does
not follow joints
figuring Jones root causes basically it
won't tell you that these two need to
change at the same time it will tell you
that these are both potential root
causes but it won't tell you that you
have to change them at the same time so
we're basically not following this final
final term that'll just be able to
triangle you know we would just not not
have that
we basically say that okay either value
of x is going to change because of a or
we would change see and evaluate X is
going to be what it was before so if T
so if so if the actual root cause is the
blue triangle what you do is that you
first you'll see red and green you
change green and then you would see blue
in the next round because then you
wouldn't you wouldn't run this and then
the blue with would show up in the next
room so you might have to run your
application multiple times to see to see
all the possible root causes it really
depends on the structure of the program
though
okay so there is another cell away that
control flow can propagate ain't and
that's where the code that doesn't run
so let's take a look at this example
here sees painted and the application
takes this point the value of y is
technically dependent on the value of C
though because if if the value of C is
different then the application could
potentially take the else part and then
that would change the value of y
conflict is interested in signing these
kind of dependencies as well however as
I explained confiden alice's as the
application runs and the application
doesn't do the else part so how do we
find out about these kind of
dependencies here's what we do the
veneer on the application if we see a
conditional dissipated we take a check
point we flip the conditional we make
the application or to visually go the
other way we run it and you find out
about assignments like y equal a on the
only call this alternate path on the
alternate path and when it merges we
basically roll back everything that we
did your your store the checkpoint and
we continue so let me tell you about our
second here is sick if we only run the
alternate path up to a certain threshold
and that's basically to prevent conflict
from being stuck in really long
alternate path so what happens is that
conflict runs the alternate path if it
hits the maximum number instructions it
would just say ok I couldn't see the
merge point I run enough I'm just going
to roll back and continue so this may
cause false positives and false
negatives but it has a big performance
gains for us so we decided to to do this
all right so usually at this point
people ask well how about false
positives don't get a lot of pots false
positives once you see a case where you
know you see that the error is dependent
on all configuration options and the
answer is actually yes we did see
something like that and the problem was
that we basically treated all kinds of
taints propagators equally so data flow
was basically equal to control flow and
also between it ain't like a binary
value variable was either tainted by an
option or not tainted by an option and
we realize that that's not actually
sufficient for
since we saw that the conditionals that
are closer to the error are usually much
more relevant to the error compared to
the conditional that are very far from
there and also we saw that data flow
usually introduces its stronger
dependency compared to control flow
however we couldn't capture this with
the regular paint analysis so we
introduced our next year slick which is
we call waiting here is thick and what
it does is that is sign weights to the
tanks as they propagate and these
weights are basically numerical values
that indicate how conveyed thinks that
taint is a strong so the way to be
assigned these things is that the
conditional that's closer to error get
to propagate a bigger weight compared to
the conditional that's farther and they
can end the 10th that's coming from a
data flow is going to have a bigger way
to compare compared to the one that's
coming up from a control flow so now
with these weights comp it's able to
actually rank the root cause for you
that's how we get the ranked list the
condition the configuration values that
get a higher rate become ranked first
and then it goes from there so that's
how we get direct list yes how do you
deal with changing pointers
would you mean like attain a pointer out
which would follow a text and a but not
the content is tainted so there are two
ways to do with espresso for some
absolute for some table lookups and
stuff you know if the index system date
and then you know you although the
content is not tainted use for some
applications you treat that the content
should be detention propagate get these
little cups for some other cases you
know that
currently we do if the address is
tainted we are we take the tent
currently that's how we do it we do take
the time so we basically say that if
your address is tainted out whatever you
you're taking is going to have that
taint as well so for instance if let's
say you are looking at you know you're
traversing an array and index is tainted
that's going to taint whatever you read
but if the base of the table is Kane
right so the address right then then you
make may cause a lot of false positives
yes but if you don't do that right if
you don't do that it causes false
negatives yeah so V so what we did was
that we basically felt that okay if you
can deal with the false positive part we
better do that then have you know a list
that does not contain the actual root
cause so we actually had cases where we
didn't do that and we saw false
negatives so we just decided to do that
it causes false positives but the good
thing is that you're doing the waiting
it might just get fade away and I mean
you know we just disappears in the case
that we help to reduce false positives
caused by
address
did you take from the address em so so
we did it and then we did the waiting so
I'm not quite sure which one of the
cases if we didn't have the weight would
cause that false positives because of
the address I don't know specifically
which one of the cases will be worse but
the rate if you don't have the waiting
we're going to have a lot of false
positive in general ok so the analysis
that I just described is actually pretty
expensive the slowdown is in the order
of two you know towards of magnitude
slow down so it's actually a pretty
extensive analysis that would be okay
for you know the application maybe if
you're just running your application on
desktop but every now and then you need
to troubleshoot but it's not okay if you
want to troubleshoot maybe a server in a
production environment and also
sometimes the symptoms of the problem
are time dependent so if you are
perturbing the timing a lot you might
not be see the symptoms again symptoms
might change and also we're kind of
relying here on the user to reproduce
the problem for us so you see a problem
now you want to analyze it you reproduce
the problem and then we would analyze it
for you however solve problems
especially performance problems are
really hard to produce you might not be
able to you know right away created
again to address all of these problems
we decided to develop a very lightweight
deterministic record and replay system
and we run the applications all on top
of this this is all internal and
deterministic record and replay system
basically what is does is that when the
application is running it record all the
non-deterministic events for instance
regenerative system calls signals and
all the timing information and it
recorded all of that in a log and later
views this log to recreate the exact
same execution and then we run all the
analysis on the replay of the execution
so basically get rid of all this
overhead on the online system so as I
mentioned views the log we replay the
execution later and then we run the
heavy analysis on the replay I'm not
going to go into too much details on
this system but the main difference
between our system and other
mystic record and replay projects out
there is in the fidelity of a replay so
the fidelity of our replay needs to be
strict enough to create the same
execution as the record however because
we do analysis in the record we
basically instrument the replay and then
we do the analysis inside our fidelity
should be loose enough to allow this
extra code to run within it and we
achieve this via on a careful co-design
our replay system is instrumentation
aware it can differentiate between the
replay code and the analysis code that
we're running so I'm not going to talk
too much about this I'll be happy to
talk to you offline about this if you're
interested alright so let me show you
some results so we are used conveyed to
uh troubleshoot three applications
openssh apache web server and postfix
mail server we looked online looked at
the forms and manuals and found 18
misconfigurations that people reported
for these three applications we
recreated these and then we ran them
with conflict to see see if we could
find out the correct route cause in
conflict is very successful it would
correctly find the correct route cause
rank it first or second in all of these
cases and these are the total number of
configuration options that were
available in the configuration files so
in seventy-two percent of the cases the
correct root causes was ranked first in
five cases it was ranked second and we
never acted worse than second guess it
worked right girl you know these shall
finish
that's
how is that fit for it does that make a
difference on the complexity
it does make a difference i'm going to
show you another set of evaluation to
right after this these are mostly very
deep configuration problems where people
actually tried a couple different things
they couldn't figure out what it was and
they actually you know posted in the
floor waited a couple of days so so
we're actually when you look at it in
the code is actually pretty deep it's
usually it takes a while now I have
another set a data so that i'm going to
show you after this that creates more
shallow cases and definitely for the
shell ones it's easier and they you know
on the results are actually going to
show that as well and there is another
question yes so if we're using come
convey to how do i specify the failure
point oh it's a very good question so um
there's different types of failure
there's some failures in an obvious you
know like you know asserts or you know
crash something like that their failures
they're not obvious and we're relying on
the user to tell us for instance you see
a message and it's you just don't like
it you say you tell us that this is an
error or you just it might not even be
like message like that you might you
know for instance run apache and get it
you know why from apache and just tell
us that this is wrong so we basically
rely on the user to tell us what is
wrong and what is right right now we
have a very simple way of doing that so
you user trust whenever you are telling
me you're printing something with this
message on the on the screen or you're
giving me something with this content
over the network this is wrong it may
not be easy to use because you assume
user doesn't look at the source code
into the null the source code
we really require the source code for
that because you only see God so you
have a way of figuring out that
something is wrong so you see a message
you say okay this is an error to me you
see you know a Content that that seems
wrong to you so you just you just you
need to know about this or just see what
is you know the application is printing
or giving you so if you just specify
that to us that would be that would be
these areas but relying on the user is
like multiple multiple root causes might
generate the same user visible ever sure
sure that the pub is conflict know right
from wrong so we are looking at that
specific execution that caused that that
message so of course there could be
other ways that could lead to the same
problem but we are analyzing data
specific executive execution that
happened see what I'm saying so there
might be multiple ways to get there if
you're not analyzing those your only
analyzing the actual execution that you
saw that led to that error so we're
recreating the error we are using the
record and replay to recreate the exact
same error and if you're analyzing that
execution path and then we see which one
of the options are affecting that
execution path does
is the answer to question I think so i
think there's some gas alone this time
that's fine yeah okay this point
compensation when you were when you were
building the algorithm that prioritizes
which configuration settings what was
your training set like do you have bugs
from native from use three applications
or were they from different applications
we do not have a box from so we we tried
openssh first and then we saw that okay
there are a lot of false positive seems
there are a lot of false positives and
then we saw that you know most of the
time the conditional that's closer is
relevant and sometimes we r you know
reporting some things that's very far so
if you know we realize that that might
be that might be something that we
should be looking at and then we added
that to the code and then we ran a
partying post pics in later postgres and
they all seem to be uh you know deep
seemed to be pretty good are you use the
openssh so firstly the openssh and then
it ran fine for a couple of the bugs and
then for a couple more we saw a lot of
false positives and then we decided to
fix the false positive problem and then
we introduce the waiting here is thick
but then afterwards your training and
testing on the same pose there's not
much training it's just it gave us the
idea that you know maybe we should have
a way of specifying which conditionals
are more important which taints are more
important you're not really you're not
using any statistical method to figure
out we basically have a simple heuristic
that says the conditional disclose there
is more important but you came up with a
heuristic based on keys but she's not
the openness to search was okay yes and
then we and never use that and we ran
partying postfix and they both run great
and then we ran the other stat that I'm
gonna show you after this and they rank
rates you shouldn't change your
heuristics at all afterwards after a
spoken association County
anything after not your question and
then we also did x-ray on our site so
you have a bargain
windows furnishings wrong share
therefore is is incorporated which has
happened
does that show up as a token here as a
system called failure or how does that
show up in your system I mean those
permissions is it kind of you know
something close maybe to unix wide
permissions francis taken create files
or ships are fo and fail ticular so here
I'm looking at the you know parameters
in the configuration file so it won't
show as you know it prominent I'm
looking at but it's this technique can
be extended to also follow those kind of
configuration values as well so it can
be easily extended to actually we're
doing it exciting you to also follow the
entire stay in life machine because the
permissions check is in the kernel it's
not in my is
or let me show you an assassin so you're
trying to read something and it says
that the you don't have the permission
right that's pretty much enough for us
because we say that okay permission here
so for instance for your next you you um
you know um you perform a system call it
gives you a code that says permission
denied something like that so we just
use that that's a okay permission was
wrong for this file so we don't really
need to go into kernel and see or how
exactly does it but then you just use
the result is you don't have the
information flow in from the actual
permission setting on the slide through
to the failure of the football no we
have it if you just see the end of it
that says okay you don't you don't have
permission we certainly don't follow
kernel as well maybe have a system call
we don't go into the car
Andrew or you'll be the output of this
analysis is the name of the
configuration variable that is incorrect
but do you know what the right value is
or you know how to fix the problem we
don't so we basically tell you that
these are the options that most likely
causing your problem we don't tell you
how to fix it easy in these cases like
was it easy to fix the problem once so
here's the thing here's why we don't
tell you how to fix it sometimes
changing that option is not necessarily
the right fix for instance you see that
okay I can't access this because of
authentication problem you don't want to
remove authentication you don't want
change authentication right so basically
suggests the user that this is causing
your problem it's up to the user to
decide whether they want to add
something really they want to change the
value of course you can change it and
run run the tool again to see if there
are any new root causes but you don't
necessarily make that decision of
whether you should change other or not
enter I guess another way for studying
so she'd ever a sister
potentially the configuration of the
system is not just a configuration part
with the permissions on every Parliament
all the libraries that's great how does
this scale when the size of the
configuration set this vehicle questions
to be huge that's a very good question
you know configuration in general is it
is you know it actually play fuzzy it
can be a lot of things as I mentioned
earlier you know your library is any
file maybe you know in your system the
you know variables are environment
variables all of these are considered to
be configuration and also configuration
file itself can be you know maybe
extremely huge you didn't really try you
know we tribe is like in order of
hundred configuration tokens but I can
imagine systems that might have
thousands and how does it a scale I
can't tell you for sure because I didn't
do it but there is overhead in terms of
you know the amount of memory that we
use first of all and also as you run and
get the art the way that we're doing if
you're you know copying change for these
configuration options over and over in
memory so as your state gets larger and
larger you need to do more when you're
doing the analysis doesn't make sense
yeah well I guess my concern is that if
everything in the system is in
figuration which are potential
that everything ends up getting painted
so your application everything in the
system can be configuration but your
application my nuns with everything in
the system right everything education
greens is sort of all the external
inputs your application or potential
sources for sure so your application
might start with reading a lot of things
from the system the good thing is that
it doesn't use all of that to go down
down a certain path you might use all of
that to go down all the paths in its
lifetime but you're only looking at one
single execution path and it doesn't use
all of that for making decisions
decisions for one single execution path
that's a good thing about our system is
that when you're writing your code
you're not using all of these little
pieces it's actually they're actually
studies that shows that usually there's
only one or two options that are causing
a problem it's not like 10 million
different options that are causing it to
be a problem Oh question so how big is
your taint
so it's my eight bits or
so right now we have one boy /
configuration options if you're tracking
and then so it gets a little bit too
much detail but the way that we do it is
that the that shows us the weight so so
we have you know that so we have that
much that much maximum that much weight
so each configuration option gets the
bites and if you increase the normal
configuration configuration options were
available this is going to increase but
each configuration options gets one bite
so if variable is gonna if you have 50k
like 50 different conviction options if
variable is going to have 50 bytes one
each for each yes such thing yes so the
good thing is that and not all your
memory becomes dependent on
configuration options so for instance
you're an Apache and at the end maybe
like a you know 70 k of it was dependent
on configuration options so the kind of
there's a big overhead in terms of what
they keep but the good thing is that not
all of your memory needs to have that
much overhead so now i can't imagine
cases where you know all of a sudden you
have you joined pieces of memory become
dependent on a lot of tint so we need to
kind we haven't seen that case yet but
we need to what we need to do is to kind
of maybe get rid of some of this tank
maybe make the smaller fade away some of
the team basically kind of compromise to
be able to to keep more but there's a
memory overhead lookup table for each
variable that thank you to
yes it's a true level booking table kind
of like a page table goth instructor yes
do you use I malaysia our banner
instrumentation to my tent analysis we
do we do I namak Lee that's what you
mean we do we used to use binary
instrumentation and the add all the
analysis as a binary runs so done it
feels like this is very very useful for
the developers then definitely another
approach is to use
sage project is this an approach to do
too fuzzy
David those hours Campion
definitely it's it can definitely be
useful for developers although we try to
kind of man we try to not use the source
code so it's also useful for you know
end users and others but if if you have
a source of it's actually going to be
much easier so it can also be used by
developers is about the need to execute
multiple times so so if there are
tendencies you said you were going to
maybe change some parameters so let's
say you have a let's say you have a
problem where you need to change two
things to get it fixed depending on the
structure of the application we might
give you both of them it really depends
on how the application checks for them
we might give you both of them like
first and second and then you you fix
the first one we don't tell you you have
to change them at the same time so it
fix the first one it doesn't go away you
run again and then you see the second
one coming up and you know fix the
second there are cases where we might
miss the second one and you see the
first one you change that you run again
and then you see the second or so we may
or may not show you all all our
configuration it really depends on how
the application our checks for this so
if for instance if if you change this
and now you go to the alternate path we
kind of when we were exploring the
alternate path we may be aborted early
they didn't see the second one then we
might be might miss it like that I
wonder if you if you're going to sort of
allows for multiple
your experiment and observe the outcomes
then there are other approaches so you
can
and it's sort of analogous to sage but
you don't need symbolic execution that
meaning the other than could file or
your configuration file and you have a
hypothesis that some bite is the cause
so you change that to some other value
and then you run again and you observe
and you can do things like full coverage
or pass profile things that are very
cheap approximations to tanked all right
so instead of tracking tank what you say
hmm you know most likely if there's a
change this one bite and I look at the
code coverage before and after the
differences in the code coverage are
only if everything else is deterministic
right then if that's if that's the only
change I've made then if I see these
differences and the code coverage those
are likely very related
so I think that approach works very well
for fuzzing testing because you changed
something and you see it here we all
know what to change necessarily you have
hundred different options which one are
you going to change that's the problem
here what we are trying to give you is
that we tell you okay these three maybe
are the most important one so now you
want to go use something like that like
change them a little bit and see how it
goes then it narrows down what you need
to look at a whole lot simply by yeah
once they do that then you might be so
let's say you want to fix your problem
you see that okay this option is my
problem you want to automatically fix it
now maybe you can go change change it
you know bit by bit and see what it
where it goes yes so if you know since
you know they
failure point right
and backward slice in to see what
options are in that code equity whether
you're using a snap can narrow that much
other than
so we can do backwards I think the main
reason is that backwards slicing for and
for really long execution path is not
very successful so here we have cases
where you usually it happens that you
read the configuration at the very
beginning you run a long path sometimes
you go through processes and then you
get to the air for postfix for example
there are five processes before you get
to the error so the configuration is in
this process the error is in other
processed back for slicing cannot really
go that far and it kind of has problem
and going up for really long executions
so that's one of the reason that we
decided to not do program slicing in the
first place I'm just gonna propose
inside 24 is very interesting 25 beyond
my photo that's my question
okay so this is the other that James
wanted to see ya so um yeah so we use
the tool it's called comforter it's
velvet PEP FL and what it does is that
it randomly generates bugs in
configuration files that looks like
human errors oh wait a lot better he's
actually pretty good for testing if you
want to see your application you know if
it fails horribly if your configuration
values are not going with you know toys
gracefully that's the tool that you use
so it was very useful for us because
then we generated 60 bugs using it and
we didn't change any of our heuristics
as steward ask and we veer on cough it
again and in 55 cases confidus to rank
the correct was first or second so again
eighty-five percent it was actually
ranked first and you know seven
percentage was not in second and there
were five cases where we didn't rank
well worse than second so three cases
yes it introduces in postfix add the
correct root cause was a missing
configuration option and that's
something that conveyed doesn't
currently support so you had to add a
new configuration option to fix the
problem so that was the treaty postfix
ones the Apache ones are the configure
to correct configuration option was
ranked ninth and that was the direct
result of a rating here as sick and the
openness of such one didn't finish we
needed some more support for one of our
system calls so didn't complete yes so
how quickly I'm going to show you some
and now performance results for the real
world bugs the average time is 1 meter
than 32 seconds for the troubleshooting
to finish again I want to emphasize that
this runs on the replay execution and
not online and for the randomly
generated ones it took 23 seconds going
back to s question these turned out to
be shallower bugs you know for instance
you had you know I'm it comes
is an option that only accepted you know
one to ten and you gave it 12 of course
you know right away it failed the rare
ones the very kind of deeper but usually
they were kind of easier the real world
bugs turn out to be much harder right
okay so kind of final note on conf eight
people usually ask you know so y con
fate is successful and here is my
thought on it so usually the
configuration problems dvc usually once
you find the root cause it's kind of
obvious most of the time there is one or
two configuration options that are
causing the problem and of course you
can have you know in case for your
having a 20 different configuration
option causing problem but that's very
rare they're actually studies that were
published it actually support this that
usually one or two configuration options
are are causing a problem so yes this
seems like a big claim however if you
actually talk to a little administrators
or something that you can show the
output of copied and then they would
actually go and fix it immediately so
you're asking whether it's arm and the
answers are correct or whether the one
or two configuration option problem is
used ok so I think it d'vega the
evaluative ility ultimately useful or
not is just by looking at whether it was
the correct configuration option that
was causing the problem whether you go
and change it and then fit that would
fix the problem is a different question
I think so we recreated the problems and
we saw that ok this is telling me that
this is your root cause and this is a
correct route cause that's done they
actually use it a couple of times for
our problems as well so if you found it
useful we didn't ask any administrators
to use it though the problem mostly was
they didn't want to run our
deterministic recorder do play there
colonel so we need to convince them to
do that but i think it is useful we
found we found it in all the cases that
we tried we found it to be able to
narrow down the options a lot we found
that very useful
the output I mean I'm done some
diagnostic okay it was like so the
output appears very different to
somebody who's building the tool to
somebody who's writing that program
versus somebody who just managing and
running that compressor it'll be kind of
interesting to see if you take this
output admins and show them the symptoms
and the output see if they actually can
fix it we actually feel that our tool is
probably most useful for people who may
not have written the code may not be
familiar with the code and they're just
using it because what it gives you is a
very general like is very a high level
result it's not going to tell you that
variable X it's going to tell you that
this configuration they actually have
access to and you can't change there's a
missing part in their argument where you
haven't closed them whether it's you see
again yeah definitely I mean is it any
system if you can give it to people if
they come back and tell you that we use
it it was great of course it's going to
be awesome we unfortunately didn't have
the time to do that V fault that you
know within our group we used it and
found it interesting we are making it
available actually the source code and
it would be interesting to see if people
find it useful as well as in college I
think you know I mean the program user
runs on desktop is more complicated than
unity than the server programs like over
an SSH and when they have a particular
windows right here is a open a program
there's a large number registry be
accessed
open they act involved with many many
VLPs poorest County in sick sir are you
clear on me in the example he evaluated
although openssh good the three sweet
have you seen postfix so i am and i
believe that there are many applications
on oh that's not so let me reformulate
to my question watch how large is
attained source how large is the tapes
how big is the configuration file in
order of hundreds of configuration
tokens I mean look at I'll try over the
URL for a look at how many registry they
read sure I don't think I don't think
that necessarily is gonna you know
translate into bad results of course if
you have you know much larger set it
might you know it certainly affects the
performance it would be interesting to
see if it's going to result in like
worse output too I don't necessarily
think that it translates directly to
worse there's also more false positives
I do believe that some of the server
applications pretty complex postfix is a
nightmare and we also did postgres
obvious database that also was pretty
complex too so we didn't just try simple
application alright yes there's a killer
my soul industry r and outsourcing IT
you know hiring someone
just thinking we might want to
texted saying you've got expert he's got
to go you're on
alright so just to finish my thought
here and so finding a root cause is
basically like finding a needle in a
haystack there's a lot of work that you
need to do but once you find it it's
it's obvious and the good news is that
computers are actually good at finding
needles in haystacks and I think that's
why confident turned out to be so
successful alright so moving on quickly
I'm going to talk about x-ray so so far
I talked about configuration problems
that lead to incorrect output and there
is another big category of configuration
problems that cost performance issues
and don't necessarily cause incorrect
outcomes and x-ray deals with those kind
of miss configurations so what do you do
when you have a performance problem
usually people use monitoring tools you
know profilers tracing logging to see
what's going on in the system the
problem with all these tools is that
they tell you what events are happening
in your system what you really want to
know is that why those events are
happening in your system so now you need
to manually infer why and that's the
part that needs a lot of expertise so
wouldn't be great if you could
automatically enjoy y as well and I mean
will be even greater if you could have
that rank the list of root causes I see
smiles and and that's exactly what X ray
tries to do so extra currently analyzes
latency CPU disk and network you can use
x-ray to analyze at the granularity of
one single request for instance for
applications like servers that handle
requests or you can analyze over time
interval and extra also gives you this
power tool where you can analyze to or
multiple different requests that you
think they should have similar
performance but they don't so hear about
your questions that you can ask x-ray
for instance you can say I have a server
voice this request being handled so
slowly or a wise cpu usage so high over
this time interval or have these two
different requests I think they should
be similar why are there different so
let's talk about the idea of x-ray so
the color performance summarization so
at conflict as I explained we
we're basically interested in finding
out why a certain piece of code for
instance an error ran this bad red block
of code why did dad run in extra the
problem is that we don't know where this
red block is but nothing really prevents
us from treating the entire code like
red blocks of code and and determining
why all events in the code rat that's
exactly what we do so from a really high
level this is how extra works we assign
a cost basically the performance cost to
different events of the execution those
are instructions and system calls and
then we determine using a conf lately
analysis why each of those ran and then
we associate this performance cost to
the root causes of our the root causes
we just determined and then we aggregate
over the entire execution and then we
rank the results so I'm going to use one
of the examples that I am told you in
the last two slides I'm going to walk
you to x-ray I tell you how exactly it
works so I say if you have a server its
handler request and one thing request
this particular is slow we want to know
why so first step as i mentioned x-ray
analyzes execution but here we are
interested in the execution thats
related to that single request not the
entire execution and that is not always
a straightforward sometimes you have in
applications that use multiple processes
to handle the request you know the
request comes in it runs for a while in
one process it then goes to another
process and it continues we are
basically interested in all these blue
pieces all of these are relevant to our
request that's exactly what extra it
does as the request travels between
processes it collects all these
executions that are relevant and then
once it's done it basically says okay
these are all of the execution pieces
within all of these processes that I
care about so once we have that then we
do the cost assignment as i mentioned
obvious point across to the events and
events on instructions and system calls
here we want to see why is certain
requesters is slow so we want to look at
latency and the latency is basically for
system calls his execution time of the
system call that vehicle
online as part of the record and for the
instructions we do we approximate the
execution time of each instruction and
the assignment to that instruction yes a
cool texture a long time in this POS
context switch from somebody else how
does that play into this also good point
and if you analyze that single request
it might be misleading because that
single request wasn't running with just
sitting so you want to do is to look at
a time interval because that would
include other processes that are
actually running at the same time and so
here so here's here's the point we give
you a bunch of different tools and then
you can you know you're running this on
replay so we can do it multiple times
with different you know different types
of analysis you can analyze a request
you can analyze over time interval you
can you can do different things to see
to figure out what's going on in the
system this is something that we kind of
um rely on you as the admin to to fick
to figure out to basically use the tools
the best you can do okay and yes so the
timings are all collected online so the
you know of the analysis is not going to
perturb the timings and then once we
have the cost so for instance let's say
we have like a small block of code we
assign you know 10 micro second maybe to
it and then we have a long block of code
or maybe it has a really long system
call and it took you know did it cost of
100 microseconds let me determine why
each of those ran for instance very
simple case may be the first one round
because of configuration option a the
second one run because if B be assigned
a cost to the root causes and then we
aggregate over the entire execution and
we rank what does this mean this tells
us that actually things that B is a
bigger contributor to the performance of
the problem compared to a so if you're
the admin you want to see why it was
slow go look at be because that's
causing a larger performance cost for
you and then go look at a so maybe you
can't remove be but this tells you why
it was happening okay
so as I mentioned actually also gives
you this powerful tool where you can
compare two different requests and see
why the performance of these two
requests are different from each other
we call a differential performance
summarization and here's how it works
you have two requests we extract the
execution pieces of the of both of them
as I'm the great that I mentioned and
then we compare them and find the points
where the execution diverges we call it
call them divergence points and then
what we do is that we calculate the cost
for each part of this execution and the
difference is the cost of the divergence
point and then we basically do the same
thing we think we find out why the
divergence point happens so it's like I
don't know maybe if conditional one of
them took the if part one of them to the
else part and that is the difference
between the cost we assign the cost to
the root cause and then we do do that
for all the divergence points finally we
give you the list it tells you that a is
the biggest contributor to the
divergence between the two requests not
necessarily the performance of each one
but the divergence between the two so
now you might ask ok I have thousands of
requests how do I know which two it's a
hard thing to do so we decided to do
that for multiple requests as well so
you can tell us that ok i have 100
requests tell me why these are having
different performances what is causing
different performances and what is the
cost so we do is that we kind of compare
all of them we find the shortest path
from the beginning to the end and note
that the shorter shortest paths is not
necessarily a single request can be
combination of some of the requests and
then we find all the divergences from
the shortest path we determine the cost
we determine the root causes and we
basically give you a visual in a kind of
explanation of these are deleverage
points these are the costliest
and a control flow through and if being
previously on side
good points um give me what were you
having Michael here's your question
comes into the file filename egg either
dev open it's imran finally made people
attend open it's an iron mountain in
utah yeah yeah yeah um so the there are
cases especially when you go to the
system call part where because we don't
follow that part where the input to the
system call can cause divergences
outside that and definitely don't follow
but we should maybe you have that in the
future future kind of direction but um
because we don't follow k if we weren't
following the colonel then we would see
that eventually yet as a divergence in
control there could be no I've never be
suggested right that's why it's taking
longer would you should after that we
are trying to look at the figuration
reasons of course you can have you know
my discus is slow because my disk is
broken or my network is slow so what we
are kind of expecting here is that you
as the admin kind of look at different
potential problems you're my hardware
not being correct my network being slow
or I I'm having a configuration problem
the good thing is that finding out that
for instance you know my hardware is
broken or my network is congested for
some reason and there are many good
tools that allow you to to you know
explore that and find out about that we
try to focus on the configuration file
where we thought that they're not that
many good tools pic of it they're like
there's a common performance profiling
tool is like all this function being
cada more 29 I mean 20 for some time and
I other function and the good so what's
advantage at this is
is what your talkin provide compared to
that kind of performance profile that's
a great question our tool gives you a
much higher level idea of what you can
do so if I mean use their FM using you
know an application and you tell me that
this function is called a lot of times I
can't do anything I mean if I'm not a
developer if I'm not you know looking at
the source code telling me that this
function specifically is like running a
lot is not helping me and I in the
overall you know solution of the problem
what we are trying to give you here is
that ok this option this option that you
can go and change is causing you trouble
see what I'm saying so if you're a
developer that might be a good thing
because then you can go to that option
and then that function and do something
but if you're just using it giving a
very low level detail of what is going
on it's going to be useless it easier
sure that goes right here so this is
actually working progress we were still
doing somewhere evaluation but this is a
preliminary result we did approach a
postfix in Postgres we found 14 test
cases of performance problems that
people are such a found online and you
know reported and um we recreated them
in a mere an x-ray and in 12 cases the
first option that extra return was
actually the biggest contributor today
today performance problem and in two
cases it was the third option that did
return yes Andrew given community
example test cases whatever the cash
isn't big enough sure yes okay so let me
give you an example of maybe the
postgres so posters for instance it has
a writer log that as it does the
transactions it writes into a lot log
and then later commits so now if you
have it if and then it basically does
snapshots or check points of the log so
that if you crash you can come back so
if your system is under a lot of load
and you do a lot of checkpoints it's
going to have its going to have its
going to put them even more load on your
disk
so the problem that that person
described was that I'm having you know
my disc is under a lot of load and then
people suggested that okay go look at
how frequently you're doing your
checkpoints and then person came back
and said okay maybe I'm doing
checkpoints too often or something like
that in the case of the thing like that
you're told you would say here's one
request that when normally here as well
request that went really slowly because
it had to make a checkpoint well it was
doing a request okay so let me first
mention these streets in these 14 cases
we have some of them that we did a
prayer request gnosis some of them we
did time interval analysis some of them
we did comparison for the one that I
just described we do the time interval
analysis where we said okay we look at
this then this minute to this minute and
then we saw that there's a lot of disk
usage and then we said okay the option
of checkpoint interval that is in the
configuration file the postgres is
causing a lot of that disk usage now we
had for postfix an apology we had cases
where we look at requests specifically
for instance for Apache the resist
request that was specifically long and
then we figured out that it was doing um
you know extra dns lookups how does the
user interact with us
like if you're what there was no or how
does it do the actual master if you're
running at all over over time interval
and something's going plug because a lot
of a disk access hmm what do why is the
user give to the system or what to think
about so you tell the system I want to
look at a disk over this time interval
and the system gives you the oh this is
descanses so as the user you'd say okay
I seem to my this seems to have a lot of
a lot of load when network seems to have
a lot of load so you so the thing is
that you detect the problem as a user
first and then you tell us what you want
to look at you can of course do you know
okay over this time interval tell me
about my disk tell me about my network
tell me about you know my latency you
can do that as well the good thing is
that you can run multiple times on the
replay and then you're all fine but as
the user you need to first detect the
problem and then try to diagnose the
problem the part is that we don't tell
you that look at your disk is doing that
yes yes it might be that you know your
distance broken we're done you know
you're not really the candy
configuration problem but if you are
then it tells you okay
okay so we have a few minutes I'm going
to talk about some of the future
research directions I'd like to pursue
so with software systems becoming more
and more complex the a very difficult
problem of software reliability and
troubleshooting seems to just be getting
more challenging and I believe that
software reliability is going to be one
of the most important research topics in
the future and I very much like to
pursue a couple different directions in
this field more specifically I like the
problems troubleshooting the software
that runs in larger scale and also
troubleshooting software that runs on a
platform with limited hardware resources
so larger scale NASA's today we have
software that runs scales larger never
we have very complicated distributed
systems and troubleshooting is
specifically if you're called on these
environments even before you get to the
diagnosis and solution you need to
detect this other words explaining arm
to the previous question Andrew you need
to detach that a problem exists and
detecting abnormalities is not very
straightforward in these in these cases
usually so today it's basically left to
the admin and then how they do it is
that they basically look at the logs and
try to see if they find any abnormality
and this is really difficult because
they keep the log to the minimum so the
question I love to answer is is it
possible to automatically find these
kind of abnormalities in the system and
once you find them maybe you can collect
more diagnostic information and then you
can do a better troubleshooting analysis
in the future I also like to look at
troubleshooting us for software that
runs on a platform that has limited
hardware so mobile computing is greater
than ever we have smart appliances
everywhere these platforms run very
complex applications but there is still
very limited in terms of computational
resources and in terms of energy and
battery life so when you're designing
solutions for troubleshooting for these
kind environments we should take into
consideration all these constraints that
they have so for instance is it possible
to maybe offload some of this
troubleshooting to the cloud in a secure
and efficient manner so that we're not
using that much of the precious resource
is that we have on the platform and also
for desktop computers we've done a good
job of making our applications more
user-friendly but when it comes to
troubleshooting this still have a long
way to go and for desktop computers any
impact that we have on the
troubleshooting is going to be huge
simply because of the number of people
were going to be affected by that I have
a bunch of ideas of what we can do for
making trouble shooting easier and
desktop computers I'm going to share
with you two of them the first one is
that the configuration estate is usually
shared for instance you have Windows
registry things like that and when you
are configured one of the applications
that means that you might be breaking
another application so is it possible to
detect proactively that this is this
thing that you are doing is going to
break something else and then let the
user know so they are aware of the
consequences the actions that they're
taking another idea is that you know
usually when you're configuring
something new a new feature you might
need to change multiple things and
modify multiple different things what
people usually do is that they do half
of it and then you know there's a
problem at the end is it possible to
automatically figure out what are all
the possible configuration options that
you need to change at the same time and
then tell the user so they can configure
the system correctly to begin with all
right so conclusion
so problems unfortunately are inevitable
in as complex software systems I showed
you that Miss configurations are
dominant cause of problems in deployed
systems these days and I showed you that
execution analysis can greatly improve
diagnosis of these kind of problems I
talked about confident x-ray they both
use dynamic information flow analysis to
do this and I show you that they can be
actually pretty successful and that
concludes my talk and I'd be happy to
take more questions one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>