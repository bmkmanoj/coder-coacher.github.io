<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Climate from the Ground Up: Trying to Use all the World’s Webcams | Coder Coacher - Coaching Coders</title><meta content="The Climate from the Ground Up: Trying to Use all the World’s Webcams - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>The Climate from the Ground Up: Trying to Use all the World’s Webcams</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vJw56Tp9HRU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
my plan for today is i'll share sort of
the highlights of a collection of
stories and then i'm around all day and
so if you want the details of any of
them find a way to get in touch with me
here at cvpr or any other way so for the
last couple years I've actually focused
a little bit more away from card core
computer vision questions and tried to
understand how we can use computer
vision to help understand more about
climate change and so there's a project
I'm working on that's looking at very
large scale monitoring of phenological
responses to climate change phenological
responses are things like wind trees
become green when they bud when they
lose their leaves natural annual life
cycles of the climate and I promise
we're going to get back to image data
before too long but the kinds of
questions that we care about in
large-scale monitoring of climate is how
do various parts of the environment like
the length of how much light a
particular area gets the temperature or
rain govern the timings of different
biological cycles and then how will that
respond to changing temperatures and
other parts of climate change and those
are important things to understand in
terms of trying to understand how the
bio bio sphere will adapt to climate
change and that's an important part of
the feedback cycles and so the way this
is done right now is mostly with either
large-scale satellite imagery this is
the moderate resolution imaging
spectrometer which constantly circles
the globe and gives about once a week
data products that say how much biomass
is at every location around the globe at
things like pixel resolutions of about
250 meters per per measurement and it's
actually going overhead all the time but
it gives weekly data products because
often like perhaps here things are under
clouds all the time so you have to wait
at least a week in order to expect to
see the ground the alternative is a
small collection of point measurements
and so this is these are cameras from
the pheno cam datasets which a guy named
andrew richardson helped set up and who
I'm working with now and these are
locations that are measured usually with
small towers and the towers are just
high enough to get over
whatever the plants on the ground are so
sometimes they're quite 12 other times
especially in the American prairie they
can be like 10 feet tall and they have
crazy amounts of sensors on the tops
they might have gas chromatographs they
might have a sensor which is essentially
a piece of scotch tape that slowly
rotates through and collects the
particulates that are in the air and
then once every six months somebody goes
and and collects the scotch tape and
cuts it up into the about the spot that
was each day and does more formal
chemical analysis and so about six years
ago I read a paper by andrew richardson
who says we are looking at climate
change by mounting webcams on these
towers on the 16 towers and I said well
16 towers is kind of amateur this was my
first email to him it was two lines long
I said would you like to try 24,000 and
he wrote back three days later and said
I think I would and so we have started
to use this which is sort of my pet
project the archive of many outdoor
scenes which is a log or a data set of
images captured every half hour from
every publicly available webcam that we
found on the world and most of those
cameras go back about three years and
the earliest ones go back to march 2006
and so these green dots are
approximately the locations of these
cameras we have a few more in Africa and
Asia now but overall it's very dense
across the u.s. and northern Europe very
dense in Japan and somewhat sparser
elsewhere in the world and I collect
this data set intentionally as a
resource for any purpose so it's
publicly available at Amos archive of
many outdoor scenes that CS e dot was
still for Washington University in st.
Louis edu the front page has the most
recent picture from 20 cameras you can
click on next many many many times to
see all 24 thousand cameras and my my
goal today is to tell you how we managed
to organize this set of imagery into
something that's useful for a collection
of either computer vision or climate
change problems one thing i want to
emphasize is when I say webcam I mean
camera that's mounted outdoors not the
webcam that's on somebody's computer so
all of these webcams or cameras that
create an image that goes on a web page
somewhere we have a little java program
that right clicks and say
that image into our database and the
second thing I want to emphasize is
these are not our cameras so they were
put up by random other people for
whatever reason and sometimes those
cameras are often those cameras have
interesting failure modes because the
people that put them up may not care
that much about them so maybe they're
highway cams or randomly put on the side
of your building cams or whatever and we
have lots and lots of interesting
failure modes so many cameras are often
just in cloudy areas this camera has
been looking at the brick wall literally
for the last six years I imagine it was
mounted and looking someplace sensible
first but then since then it just has
fallen off and nobody scared there's
often failure modes in terms of the
imaging geometry with cameras that are
looking through water drops that is sort
of an interesting image geometry that we
could exploit perhaps but it's hard to
calibrate as the drop is rolling down
the the image but some of the cameras
are sort of more sensible so relative to
satellite images they're down low and
underneath the clouds so they can see
things even when it's cloudy they're
relatively well scattered across the
Earth's this is one at Scott Air Force
Base in Antarctica they look sideways
instead of up and down which means that
if you are trying to understand
something about atmospheric particulates
and what the atmosphere looks like you
get resolution in terms of the altitude
of different layers of particulates that
which you wouldn't get from satellites
looking straight down and they see
individual trees and while the biosphere
is sort of a global phenomena and we're
mostly interested in changes in climate
at a global phenomena those changes
happen at the scale of individual trees
responses or individual organism
responses so it's very useful to be able
to see individual responses over time
and from each of these cameras we have a
many year archive and we can show that
archive in various ways and so this is a
visualization of one year's worth of
data that's organized by time of day and
time of year and it shows things like
days are longer in the summer than they
are in the winter and it shows
variations in the appearance of a scene
across the day which often comes from
lighting variations and then variations
in the scene over time ah which might be
interesting either biological or human
changes and so this is the data set that
we have if there's one thing you want to
take away today like fine
ways to use this data set we're
perfectly happy to share it you can
download the data there's Python scripts
to download different slices of data in
different cameras and so on and that's
part of the reason that I'm doing this
is to share this but in order to make
you understand the pain that we went
through to do this I want to talk a
little bit about what it took to build
this data set and the sort of five main
tasks that we thought were important in
order to be able to organize this data
so people could use it for different
projects I first define the webcams then
geo locate where in the world they are
then visualize the data from them then
as much as possible calibrate what the
cameras say feel the view and Direction
is looking in and then annotate those
scenes that if you have a particular
type of query like I want to find where
there are trees be able to make a
searchable database so you can find all
the cameras that have trees so I'm going
to talk about mostly finding
geo-locating and visualizing the data
and then a little bit at the end about
some applications so the way we found
them is it turns out that you can do
clever web searches to find cameras
things like view / index shtml is a URL
string that occurs largely in web pages
that are automatically set up when you
buy a webcam stick the CD into your
computer and say make me a web page and
so sort of a set of 20 or 30 clever
searches like this that can bind maybe
10,000 webcams across the world that
wasn't enough so if you think any of
your interns here and you think your
intern job is boring this summer I hired
two undergrads to spend the whole summer
looking for webcams we went to every
state in the United States and every
country in the world and searched for
department of natural resources and
Department of Parks and Department of
Transportation and everything we could
think of in order to find archives of
webcams and scrape the the metadata from
them so sometimes the pages that a
webcam has have interesting metadata
other times a page looks something like
this where it doesn't really tell you
very much about the camera so maybe you
know that CH is the Switzerland country
code and so you might imagine that this
is in Switzerland that's actually a a
cue that is sometimes wrong because
often there's aggregator sites that
aggregate webcams from around the world
and and so the the location the IP
address where the cameras may be wrong
and otherwise you just get a pic
like this and so the first thing you
want to do in order to understand
anything about sort of any global
question is understand where the camera
is so we're going to get to that in a
second sorry out of order the second
thing you might want to do once you have
a lot of data is find ways to visualize
that data and the most simple
visualization is just win do I have data
for this camera so this is another
example of the summer image and so this
is time of day by time of year and each
pixel in this summary image is the
average color of the image at that time
of day and time of year and the red
streaks or places where the data is
missing so this is a sort of data
availability plot if you go to the mo
site this is live on the mo site you can
click at any spot and get the picture
from that time of day so you can click
along this boundary and see for instance
the time lapse of dawn every day in this
camera so that's one visualization that
visualization is showing the average
color of the entire image you might want
to show a little bit richer of a
description of what's happening in the
image and so we use principal component
analysis applied to image data as a way
of projecting all the images down onto a
lower dimensional space to get a richer
visualization so everybody knows
principal components I imagine at this
place but um it's interesting when you
do principal component analysis on data
that is from a fixed camera because you
get richer components than you usually
do for unstructured imagery so we take
images we've been column vectors we
decompose that into a set of basis
images and a set of coefficients so
basis images have a lot of structure
because since the image is static then
the types of things the coefficients
code for are things like lighting
variations across the scene so even
something like the second component here
says this wall is brighter this well as
brighter make the other wall darker and
that's sort of interesting structural
information about the scene and then we
also have coefficient trajectories and
those coefficients are coefficients of
images that were captured through time
so it makes sense to plot them over time
and you get very characteristic
coefficients of essentially all outdoor
scenes where the first coefficient is
often the day versus night variation
and the second one is something about
morning versus evening and this is
already enough to give you lots of
structural information about a set of
images so if we look at an image at
night it has it's in this case low one
coefficient one if we look at an image
in the day a clear day tends to be an
image where this component varies a lot
from morning to evening a cloudy day
isn't is an image where these are sort
of murky or differences between morning
and evening and so I find this
compelling because something as simple
as PCA can give you a really rich
decomposition of what's happening in an
image in terms of important weather
conditions so this lets us make a richer
visualization so this is the same time
of day by time of year plot and now
instead of each pixel summarizing the
average color of an image we can set the
first three coefficients to reconstruct
that picture as the red green and blue
channel and so in this summary image
these colors don't have any intrinsic
meaning but pictures pixels that have
similar color are likely to be pictures
that are similar and so you can read off
essentially the cloudy versus sunny days
in this so the the the days that are
orange are sunny in the days that have
sort of this murky purple in this image
are cloudy and if we flip between two we
can see sunny versus cloudy so you get
this really nice picture of even what is
the weather at this scene over the
course of this year all just from
computing PCA on sets of images that
might be a reframe keep computing PCA
offensive sometimes the this summary
image isn't that has has these sort of
discontinuities this turns out to be
because these webcams were not mounted
by us and we don't control them at all
and so if the camera moves then
principal component analysis since it's
a linear technique has to use sort of
one of its components in order to try to
characterize the difference in the in
the viewpoints and so this summary image
is also useful to tell sort of mundane
things like when the camera moves
anybody has questions adorn questions so
shut them out anytime so all right so
this is our data set we have ways of
visualizing it
principal component analysis already
does an interesting job of decomposing
parts of the variation of that scene in
interesting ways when we wanted to think
more carefully about how the outdoor
images might vary over time we use what
we call the Geo temporal image formation
model which is something like a
generative model that tries to relate
what an image looks like to all the
factors that might affect it and those
factors start off with if you go to a
particular location in a particular time
and take a picture like you'll get one
picture at that location in that time
and that picture varies due to a couple
things some affect the overall
appearance of that scene so what is the
scene structure what are the transient
objects at that moment what is the
weather like is it cloudy what direction
is a Sun coming from and that creates
what we think of as the view sphere
which is if you had a panoramic image
taken with some normalized camera at
that location that panoramic image is
defined by these things like scene
structure and weather in lighting and
then there's the particular image that
you captured because you probably didn't
take a panoramic image with a normalized
camera so there's some orientation and
calibration of the camera that you used
there's some way that the camera lies
about the color that it's measuring
there might be some imaging noise and
all of those combined to define the
image that a camera actually takes so we
don't think of this formally as a
probabilistic graphical model but we
think of this sort of informally as a
way of understanding what are the the
component parts of the scene and Stefano
Sawat Oh over the last couple years has
been developing an image information
theory which sort of is his way of
justifying to the people that have
funded his research why computer vision
is so hard and the way his justification
is that all of these types of things end
up causing or many different components
in this model end up causing similar
image changes which is why it's so
difficult to decompose them so for
instance slightly different amounts of
atmospheric particulates can change the
color of the sky just a little bit in
almost the same way that various color
calibration parameters can change the
color of the sky so many of these types
of changes have that flavor and
therefore they're difficult to decompose
but if you want to take the positive
view of the same thing it says images
are affected by all of these different
components so if you have images
especially
archive of images you might be able to
use them to go back and measure any
particular part of this and so for the
rest of this talk I'm going to go
through sort of different things that we
can try to use this model in order to
understand about a camera so the first
one is geolocation so we're going to say
imagine that we have lots of images and
we have no their capture time we want to
understand that geolocation and we want
to use lighting variations to understand
that and so things like the summary
plots by time of day and time of year
and the average color of the image
actually essentially uniquely define
where in the world a particular camera
is so this camera is near the South Pole
there's a day of the year when it
becomes light all day and that trick
that that this sort of track of when
when is night and Wednesday is unique to
a particular location in the world you
can think of that as just knowing when
dawn is tells you the longitude and when
how long the day is tells you the
latitude and with this is a good enough
q across all our cameras to get within
about 60 miles of the geolocation across
the world and I think it's only that
slow that that bad of accuracy because
we're capturing images only every half
hour we had a richer denser data set it
would it would work better some of our
cameras are mounted on cruise ships so
they have particularly compelling time
of day by time of your brightness plots
including this one which must have gone
around the world over the course of the
year at some point because when night is
relative to greenwich mean time has
wrapped and so this cruise ship went
around the world which i think is
compelling to see a slightly odd richard
geolocation q is to say let's continue
in the model where we have the image and
the capture time let's uh try to solve
the geolocation let's try to use weather
variations to solve the geolocation and
the key part that might be factoring out
the lighting variation so to factor out
the lighting variation we can do
something as naive as just take pictures
every day at noon so let's take pictures
from all our cameras every day at noon
run PCA just on the noon images say from
a month of those that data and then sort
the image
is by their first principal component by
the coefficient of their first principal
component and for almost all outdoor
cameras that gives us a variation of
sunny to cloudy sunny to cloudy sunny to
cloudy and every once in a while there's
a scene where human variations dominate
so this is almost certainly a weekday to
weekend at a parking lot but that means
that from each camera from something as
simple as computing its PCA components
for images at noon we get a signature of
cloudy cloudy sunny cloudy sunny sunny
cloudy cloudy sunny cloudy cloudy we can
go to a satellite map and then say which
pixel on the satellite map has the same
signature of cloudy cloudy sunny cloudy
cloudy and that gets us geo locations
and my favorite one is this one so this
is actually our closest one it's within
like three tenths of a mile which is way
more accurate than we have any right to
expect because that's smaller than the
satellite pixel resolution but you know
sometimes the rounding error it goes to
your favor and what's compelling about
this is this camera doesn't actually
look up at the sky even though it's
using cloudy or not cloudy that first
principal component codes for shadows
that this building casts onto the onto
the ground so even though it's not
looking up the sky it's still sensitive
to these cues and overall we get more
than eighty percent of the cameras
within 50 miles using one month's worth
of data on this okay how many can so
some of our cameras we scraped from
sites that provide metadata that we
chose to trust so some of the highway
cameras and a lot of our cameras came
from something called weatherbug which
was a box that a company convinced
elementary schools in the u.s. to buy
that computed weather data where they
were and they sort of stuck a camera on
for no apparent reason and so we trusted
those and that was our ground truth for
that so we had a couple hundred cameras
that were ground truth
okay so I started off this talk by
talking about trying to think about
large-scale phenology changes so once we
have cameras that we believe we know
where they are we can start looking at
things like the changing albedo and the
scenes and so these are the types of
seasonal variations that our cameras
view this camera is particularly good
because it sees lots and lots of trees
and the variations are really vibrant it
turns out that the current state of the
art in the biology community for
measuring fino changes from images is to
measure a greenness score which is how
green the pixel is divided by how red
plus green + blue the pixel is so that
gives you a number between 0 and 1 of
how green a pixel is and here are
various plots from our webcams that were
not put up with the purpose of doing
phenology testing and there's sort of a
sigmoid model that is fixed to the
greenest score to get the onset midpoint
and end of the phenology change and so
if I take so we managed to do this for
hundreds of cameras if I take one of
them to make it bigger this was the
region of interest that a mechanical
turk or drew for us nicely on this and
here is our fit for the for the fee node
and these three points correspond to the
three pictures and what we found that
was compelling about this is in this
camera and in most cameras that are in
suburban areas there's a substantial
difference between when we saw the
phenology change and when the satellite
estimate of the phenology changes and
it's not just the difference so it's a
difference that it's about three weeks
on average and it's not just the three
week error it's always a bias there the
satellite estimate is always early in
these suburban areas and the reason it's
always early is because these suburban
areas have a lot of grass and so as soon
as the grass becomes visible and green
in the spring the satellite looks down
and that pixel is that as green as it's
ever going to get and so we found a
systematic bias across a large part of
the u.s. in suburban areas in terms of
estimating these greenest scores and at
first we just thought there was a
difference but the nice thing about our
webcam date is we can go back and
validate you can look at this picture
and say well these trees it's really not
green it's starting to be green and here
there's leaves and so we're convinced
that's this
is not measurement dare on our part but
we're really seeing something that the
satellites aren't earlier cities you'd
expected me earlier in the cities
because the cities are warmer is that
the yeah a little bit war he island
effect also potentially street license
probably doing dude greening up and
that's having less of an effect but yeah
there's games beyond one looks slightly
earlier that's right so we have tried to
fit a model that's based on the local
GIS data like the type of terrain that
there is and the latitude we haven't had
a model that I mean we can predict it
somewhat but we haven't had a model yet
that we think is that we believe like
the put that that corrects the error
substantial but that's sort of one of
our active things that we're working on
yeah I would love to say like you know
we tried sort of regression model like
learning the regression model from the
local GIS data and that didn't give
really satisfactory results yet titles
ran a bit of tree thanks if you ask that
right angles around a bit across would
you get the other mom um you we did not
do that um so we did that we did that um
like by I on like 10 or 20 cameras but
we haven't I think part of the issue is
something we don't see the grass and
that many cameras like a lot of the
times the cameras are sort of pointed a
little bit up so we didn't I thought
that's why we didn't ask the Turkish to
do that but that might be something to
do more rigorously I like that idea yeah
that's something that I've said I bought
that my biology collaborators have not
yet discovered but it's really typical
that there's the maybe this fits your
model of in the spring it seems really
green right especially green right when
and and that's that's that's very
consistently observed and it's not well
understood what that is to the point
that they've taken individual leaves and
put them in spectrophotometers every day
and they don't see that there so maybe
it's something about the when the leaves
start off they're a little bit more
rigid and they
point more straight up and then over
time they sag a little bit or its dust
that's accumulating on the leaves in the
actual right so there's different ways
that this ratio could go down I don't
know what the cause of the whiteness
decreasing would be like it's not that
it's darker later in the summer it's not
that it's brighter later in the summer
either really change the leaf defense
and things like that over time right dgc
flowering I mean the trees are clearly
following in that right so the other
places is that so we do see flowering we
and we have less of a specific model
about how to fit the sigmoid like to the
flowering and and that doesn't always
show up so well like flour the flours
and different colors right and this plot
just shows that when we do see flowering
and it's relatively easy to detect any
time we are looking for a particular
color change alright so that was an
example of something we can do just by
knowing essentially where the cameras
are and then using the green this
measure as a way to factor out lighting
changes and factor out overall intensity
and so on there's also may be richer
things that we can understand about the
scene and we're interested in applying
these to understanding what the shape
and size of the tree canopy is but today
we haven't gotten to surfaces that are
as complicated to model as tree canopies
but we're interested in scene structure
and so here the model is we're going to
assume we know the location of the
camera we're going to assume we know the
capture time of every image we're going
to have lots of images we're going to
use variations in lighting to try to
understand scene structure and so this
is a picture of my lab in downtown from
my lab in downtown st. Louis we took a
time lapse of images over the course of
an afternoon I haven't gotten movies to
work you yet so you just have to imagine
a time lapse of a partly cloudy day with
clouds going overhead
we ran principal component analysis on
that set of images and we get a mean
image and these 15 most important modes
of variation to help think about these
we took three of them and made them a
red green and blue channel of a false
color image and we get this picture and
what's compelling about this picture is
it captures the first law of geography
by waldo Tobler everything is related to
everything else but near things are more
related than far things in particular in
this picture all the pixels that are in
blue which means they have a particular
set of values in the first second and
third principle component image they're
all downtown and st. Louis about 10
miles away from the camera the pixels in
dark green are at st. Louis University
about six miles from the camera the
pixels in light green or yellow are at
the Wes you med school about three miles
away from the camera and these are
different sort of distances to trees in
the big park in st. Louis and the reason
that principal component alisis creates
these overlays is because all of these
pixels in these different color channels
are either under-shadow or not at the
same time so as shadows overhead st.
Louis is kind of a dead City they really
aren't buildings why is my mouth that
they really aren't built like this or
those three chunks of tall buildings in
st. Louis so there aren't things
otherwise so it's a nicely segmented
city and as the clouds would go overhead
all of the med school would be in their
clouds are not or all of st. Louis
University we're going to clouds or not
or all of downtown will be under clouds
or not and so that why that's why it
makes a lot of sense for PCA to choose a
components to shade all of downtown at
the same time and so it gives us
brilliant way of again the world simple
statistical tool giving you a depth
segmentation of a scene so we wanted to
try to formalize this and so we took a
this is a time lapse that I also can't
show you from somebody who studies
atmospheric science at the Czech
Republic and we built an algorithm to
try two more formally understand this
relationship between correlation of
pixels over time and the 3d scene
structure and so that relationship
starts by saying I want to learn a
mapping between correlation and distance
and
ground-level views make that difficult
to reason about formally so we took some
satellite pictures and just said let's
look at pairs of pixels let's look at
the distance between the pairs of pixels
in geographic space or in this case in
the image because these are satellite
pictures and let's look at the time
sequence of each pixel and look at the
correlation between the time sequence
and so the correlation between the time
sequence is likely to be high for two
pixels that are close to each other
because the same clouds and the same
fronts are going over them at the same
time and the correlation will be lower
the farther away you are and because
clouds have a scale invariant property
that tends to be like some small wispy
clouds and then big fronts that go
across pixels at different distances may
still have correlations even if they're
not really close to each other and so
this is the distance the correlation map
for a couple weeks worth of satellite
images across the Atlantic and then we
can make those same distance to
correlation maps in our scenes in
particular so we can take one landmark
pixel look at its intensity plot over
time and take every other pixel look at
its intensity plot over time and look at
the and then color code every pixel by
that correlation and what's interesting
about this picture I think of this as
God's flashlights because it lights up
the scene as if you had a flashlight
looking straight down at your landmark
pixel because the closer pixels are in
real life in the real world to each
other then the clen the more correlated
those are and so in order to take this Q
and translate it into actual 3d models
then we need to find the mapping between
correlation and distance and then we
need to build a 3d model that's
consistent with that set of distances
between those pixels we are right so we
are assuming that the clouds are created
by some sort of flake and some sort of
isotropic like plus all model of clouds
over so we are not dealing with that at
all and that means it would might fail
in places like looking at mountains
where there's like always a sort of
cloud right off the mountain ridges on
the sort we're completely ignoring that
and we're assuming that the distance the
correlation map is the same everywhere
the same so we need to solve for this
distance the correlation map and so we
have an algorithm overview that first
takes their initial data computes the
correlation of the time sequences
between every pairs of points that's
never going to change that's driven by
the data and second we create initial
depth map by assuming the world is
planar and fitting that planar world to
our correlation map and that let that
lets us create a plot of distance to
correlation once we have a scatter plot
of distance the correlation we can fit a
model that tries to give a particular
estimate of the distance for every
correlation that gives us an estimate of
the distance between all pairs of points
and there is an eigenvalue problem you
can solve called multi-dimensional
scaling that converts a set of distances
between points into a plausible 3d model
of where those points are you have to
modify that slightly because we know our
points come from pixels on a camera so
instead of solving for a 3d position of
all the points we stall for the 1d depth
along each ray of each pixel that is
most consistent with the distances
between all followed and so then you can
do this iteration where you use the
current depth map to compute the
distances between the points to get this
that lets you solve for this green line
which is our mapping between correlation
to distance then we solve the
multi-dimensional scaling problem to
re-estimate a depth map and then we can
that gives us that will move the
distance of every one of these points
because now the distances are based on
an updated depth map instead of the
initial depth map and over time this
converges to give a model and I have a
movie that doesn't work here but a
prettier scene gives us a depth map and
one of the nice things about this depth
map is because we're not inferring any
smoothness across boundaries we're
computing a depth at every pixel we get
very very sort of precise and rich exact
models of the depth at each pixel that
don't suffer from blurring over
boundaries and the other thing to note
is in this case our final depth map or
our final distance the correlation map
is very flat and that's because this is
a scene that's relatively small compared
to the size of the clouds that were
casting shadows whereas
in this scene oh I don't have the final
death weapon in this scene or in the
satellite picture you get this sort of
exponential decay because only the
nearby points were often under the same
sets of clouds and so that motivated the
use of a non parametric model for
solving for this distance the
correlation mapping instead of a
parametric model so here's one last seen
prettier clouds God's flashlight
pictures and the depth map that we get
it gives you a 3d month so we're showing
the depth map that is the 3d model of
landscape that's rates so that 3d model
is is expressed in terms of the three
like the distances between points one of
the interesting things about that is the
location of the camera almost doesn't
come into the optimization at all it's
really just constraints about where the
points are in the world although we do
like we know that the points all are
constrained to lie along pixels that
have to come from rays from some camera
but it's unlike a lot of cues is not
related to depth from the camera it's
really related to points and their
relationships to each other we mask the
sky out you can actually sometimes see
interesting relationships like if the
Sun is somewhat in front of the camera
you can see where the shad at the cloud
is that would shadow a particular part
of the landscape right and so you get
that correlation between the location in
the sky and the shadow we haven't used
that in any coherent way yet the river
was black does that mean it thinks the
river is really far we also masked the
river out because that reflected up into
the sky and confused everything do you
have any comparisons to ground truth
food there computation um we have in
this scene where is the same in this
scene we we computed our 3d we computed
a 3d model and measure the distance
between the bases and we found the bases
where 99 feet
between each other instead of 90 feet
but we reasoned that this is actually in
the Czech Republic and maybe they built
their baseball field outside exactly so
the so there's actually chook there's 2
q's this is from a cvpr paper or two
years ago the queue that I showed here
is actually there's still a scale
ambiguity in the queue this there's a
related q that you can use that looks at
the time sequence of pixels the image
and the time lag that is consistent with
the highest correlation and then if you
have an accurate estimate of the wind
velocity maybe from some local weather
sensor you can use the wind velocity in
the time lag to give you a metric
response okay so 3d scene structure is
one variation or 3d depth the depth
model is one variation of 3d scene
structure another type of structure you
can have is to look at the surface
normals and so in this case we this is a
ECC V paper called Helio metric stereo
as opposed to photometric stereo so it's
making photometric stereo work with
varying directions of illumination from
the Sun in outdoor cameras and so we had
sequence of images sequences of images
maybe like this as from the same scene
from a fat static camera where the Sun
angle is is varying over time and so the
question is what does it take to
organize this uncontrolled webcam data
into something that is accurate enough
to estimate the surface normals and so
we largely use the Lamberson lighting
model which has a particular pixel
location the intensity of a pixel
location X at time T is the surface
normal of that pixel x the lighting
direction x 0 1 shadow mask which we
have a crappy heuristic to make up times
the albedo at that pixels that would be
sort of the normal lamberson lighting
model and so the albedo is the raw
surface color
the surface normal we're going to show
with this kind of color coding the
shadow mask is a 01 mask and then to
make this more realistic or to the terms
we found that we needed in order to
solve for this on outdoor images set a
time varying but every pixel everywhere
the same al contribution of ambient
lights and exposure because webcams
often change either the exposure the
aperture in order to give you sensible
looking images at all the times and then
a color response curve and so that is
our imaging model for outdoor imaging so
the color response curve sometimes we
call this a gamma function it is the
number of photons in and how it relates
to the 0 to 255 or 021 brightness that
the camera returns and so we set this up
as an optimization problem or all we
know to begin with are the lighting
direction which we assume we know
because we know the camera geolocation
and the time the camera was captured we
have a heuristic testament the shadow
mask and we're given the images and then
we want to estimate everything else and
what's nice as it turns out that
everything else can be solved as a back
and forth between two linear equations
so first we can solve for the surface
normal and the albedo at each pixel and
then given the surface normal in the
albedo we can solve for the exposure and
the ambient lights and then this looks
like a nonlinear term but Sri Nayar I
guess now almost ten years ago computed
the camera response functions of the 200
most popular cameras and created a pc a
basis for what that camera response
function is and the first five
components that pc a basis capture
ninety percent ninety-eight percent of
the variation of the response function
of all 200 these cameras so this is also
solving for those five non linear
coefficients and then we can use that to
solve for the surface normal and the
albedo and we're going to color code the
surface normals with with these with
this color and what's interesting about
this is even though we may not know
exactly what direction the camera is
looking we know the direction the
lighting comes from in geographic
coordinates so our color coding is
actually relative to geographic chord
it's not relative to direction to the
camera so this purple color is always a
wall that's facing exactly west for
example and so here our example results
and again this all of this is done
separately at every pixel which allows
us to get really fine resolution in
terms of the microstructure at these
different surfaces so here is the albedo
that we solve for and the surface normal
map and the camera response function and
for another scene we tried to ground
truth this and I would love to have
suggestions of better ways to ground
through this our initial last way was to
use the Google Earth geometry but Google
Earth geometry is actually really poor
at building structures so it doesn't
have any of the micro texture along here
it doesn't have any of the trees along
here and so we struggle to find a really
good way to to evaluate this but one
interesting thing was that you can
explore how accurate or how plausible
the surface normal maps were for
different amounts of different amount
different lengths of time that we
captured images and so if you could take
images just from one week then you get
this as your output 3d model and this
comes from the fact that the fact that
this is bad is due to the fact that the
Sun is illuminating the scene over the
course of a day from different
viewpoints but from day to day the
viewpoints don't change very much and if
you have illuminations that all lie in
one plane then the photometric stereo
problem is not well constrained and so
there become many possible solutions
something like the bow relief ambiguity
so there's many possible solutions so it
really takes something like several
months worth of data in order to get
what looked to be plausible 3d
reconstructions which is nice if you're
trying to justify capturing six years
worth of data archives did we correlate
so we did not correlate this with the
other feeding construction in part
because this seems to work best for
scenes that are close by and the
one like you need the time lapse of
intensities to vary over time we so so
one plan was to write the paper that
combine these 2 q's because this gives
us beautiful surface normal q and then
we can you can't really integrate across
the surface normal maps very well
because there's discontinuities and so
you could might be able to integrate the
3d model of this region for instance
like in here but then to get across
these discontinuities it just doesn't
work because of the discontinuity so you
can't integrate over it so we had hoped
to try to build a joint model of both
and we didn't find very often that we
get great surface normals at depths that
were sufficient to use the cloud-based q
so far all right so here is our geo
temporal image formation model I have
24,000 webcams but I'm not satisfied
with that and so recently I've become
interested in citizen science and
crowdsourcing and so this is just a
blatant pitch for it but we're to share
so we have an app that we call reef otoe
stands for repeat photography it's the
world's simplest app you take a picture
and then if you or somebody else wants
to come back later to take another
picture of exactly the same object you
see the previous picture half sifu as
you're lining up the new picture so you
can wander around it's actually harder
than it seems that you can wander around
and take the same picture and then if
you take pictures of things at different
times you can sort of use that to
document changes and so this is a free
app on iphone and android and we are
using it now in I think seven different
places across so there's a project in
Zurich looking at rooftop gardens and
what their phenology changes are
although that's very anthro like like
people affect rooftop gardens lots right
as they should because their rooftop
gardens we are using it to take pictures
of turtles in the Galapagos and in a
park in st. Louis and we're finding that
in that case the previous picture that
we're aligning new pictures to is just
an outline
the turtle but that really helps sort of
untrained volunteers take pictures at
the right distance from the turtle we're
finding you can use sift features on the
turtle shell sift plus ransack to
identify that it's exactly the same
turtle so we can do capture recapture
experiments without actually tagging the
same turtle and then our biggest project
is I think street trees in New York and
so new york city and washington DC and
several other places in the US have a
mandate to have less rain water
pollution run into various ecologically
important areas like the East River in
New York City and so either they have to
create a 1 billion dollar rainwater
runoff treatment plants or they have to
increase the tree canopy coverage in New
York City from something like twenty
eight percent to thirty four percent
because if there's more leaves that
intercept the rain as it falls down its
first of all funnels more water
underground and second is a buffer and
lets the work stay that's the current
system be able to treat the rain water
on with vases and so that new york city
has no plan for how they could monitor
the health of street trees a million
extra street trees so the idea is that
we can use rifo tow as a way of allowing
people to adopt a tree and so here are a
couple of the blocks that we did a small
test case on this is also a completely
free app so like that that i'm promoting
for you so if you ever have a project
that you would like many pictures of
volunteers around the world to take like
we can set up pins for anybody and it's
almost going to be an automated process
alright so maybe that's a good place to
stop so i have time for questions but
i'd like to thank some of the funders
and then all the students that put love
into these projects and they're
fantastic thank you for listening
I like your your daylight and nighttime
pictures that you should capture two
positions of cameras and I think you
said that you get them down to 50
kilometers or something or I wasn't
completely sure about that or the other
method so that one is the median error
is 50 miles but you know it's just such
a lovely regression problem with loads
of redundancy in any two parameters so
you know have you squeeze the last ounce
of accuracy out of that or is it 50 gold
is the best of their miles the best it
could be done right so I don't don't
know if we've squeezed the last out of
that but those reasons why it's not so
the reasons that i know of why we didn't
do better so far is the our resolution
for capturing images is one image per
half hour although we have so many
images over the course of the year that
that shouldn't be that much of a
limiting factor but the other thing is
depending on the local surface the local
geography when the dawn occurs like dawn
usually occurs some some fraction of
time before the Sun actually rises the
sky gets significantly brighter but that
depends a little bit on local geography
and a little bit on how much cloud cover
there is so I think that's like if it
was just a pure measurement of wind dawn
was and we had a binary mask a visit you
know has the Sun risen yet we should be
able to do better but it does vary from
location to location that's right so
already doing a little bit better than
400 measurements during ESU divided by
square root of 400 those are sort of
ballpark for Hank you should be get up
to do and that comes to about 50 miles
or maybe half okay and so then I guess
the only other place you could squeeze
any more as if you interpolated the
interpolated lead pixels and it wasn't
actually a half hour error but something
more delicate right so you do get
intermediate intensities right so there
is something there is some possible
value there but that's
better explanation than I've heard of
for the 15 yeah I was wondering at they
have you managed to use the
characterization phonological pairings
over the US across the globe from those
webcams in actual published studies of
you know phenological parent around the
world is not working progress still um
there is a paper by andrew richardson
that is appearing this year and i can i
can pass along the reference to you I
think like I think the best goal is to
try to correct the satellite create the
better regression model to correct the
satellite estimates and we haven't done
that yet and so we've done a few things
here I think one of the big places where
this is most useful is in predicting
future responses to climate change you
can use the annual variability of
climate factors in each particular year
to get statistics of how different
species of trees respond if it tends to
be warmer more warm in the future right
and so these natural variations are
maybe better ways than trying to go from
first principles of tree physiology to
see how that their fina cycle is going
to change us yeah there's a huge signal
that we we uh we have a we have a
visualization tool that we're building
called a tale of two years that shows
this year's picture in last year's
picture unless you wander through the
like visual and it's amazing how
different the like just how yellow the
trees are across a lot of its yeah
looking at crops we have fewer than I
expected so we've like I've personally
labeled like a dozens we had a
mechanical turk job that went and asked
people to label like every camera in
terms of 30 or so categories that had
some of the problems that Mechanical
Turk jobs often have that you know so
and I don't know if we actually had farm
as a category I don't think we
polluted farm as a cat away so maybe
that there's just a couple dozen cameras
that are label this farm I don't think
there's that many camera I don't think
we've not labeled them they just aren't
that many of farms there's a growing
movement in the u.s. of small vegetable
organic farmers feels like the US
farmers are dramatic about things I
think I have a lot of drama so that
drama instantiates in itself and then
putting up cameras to show how pretty
their crops are this is an example where
to for doing agricultural modeling out
something we're interested in doing you
know the most interesting cameras for
that will by themselves be incredibly
boring you know glad you're just
pointing a camera a you know a giant
field of calling Kansas or something
it's just not the kind of stuff people
do that given we can kick off these
projects now because you might be able
to get people excited about that by
saying look if we did have a network of
those cameras across the state we'd have
a dramatic improvement in our
understanding of you know responsive
crops to drought or something so this
beer like it wouldn't surprise if you
don't have any of the bowmen and even
the ones you have might be in a more
interesting bit for farm like a policy
piece around it rather than a bit you
really want which is just the big field
of corn okay although I think I mean so
a you're right that every camera
individual is it's really quite boring
but there's also growing movement to try
to do very how you treat different parts
of one particular field and you know
like even which seeds you plant in
different parts and the time course of
how the crops are growing at the time
course of the moisture if you can tell
that like how much like different parts
of field actually get different amounts
of rain and so people are trying to
track that and webcams are one promising
approach to do that the reinsurance here
in the US really cover the farmers for
that thirty percent are not covered for
crop loss by the government and big area
right now I wonder if it would be in the
insurance company's best interest gave
the farmers a webcam to monitor the
crops to say yes you definitely do have
losses due to direct here when we have a
model which I reliably tells us whether
has
directing juice so the profit model to
the validation of the farmers claim or
is that the insurance company can plan
ahead because they'll know ahead of time
if it's going to give you validating
farmers claims for i hided night yeah
and also anticipating yeah future losses
yeah I got it also be able to
characterize the difference between
trees and grasses favorable right well
just pick by seeing at its tree or grass
sir yeah yeah hmm so much observation
thank you again thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>