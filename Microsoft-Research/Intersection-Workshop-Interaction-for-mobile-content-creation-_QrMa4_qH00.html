<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intersection Workshop - Interaction for mobile content creation | Coder Coacher - Coaching Coders</title><meta content="Intersection Workshop - Interaction for mobile content creation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intersection Workshop - Interaction for mobile content creation</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_QrMa4_qH00" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
I'm lucky that I had already speakers
before introducing augmented reality if
so that's really perfect and also
talking about content creation a lot
like why it is actually difficult and
why we are interested in this one thing
that I want to add at this point is
probably mobile as well and we think
that that for us mobiles are a great
platform to now to work materiality they
provide technically a very round package
that has everything that we could dream
of basically nowadays but also they are
probably the one computer human user
interface that is now used most widely
by the world and probably forever in the
in the future in some sense so we are
really interested in what what we can do
on this kind of platform and what we can
do in terms of commented reality user
interfaces and so we are not necessarily
focusing on a specific technical problem
but on all of that surrounding this this
platform how to realize that and then
some work we're doing on the one hand
side is more technical oriented like 3d
vision based tracking and this is
running a kind of 3d model based tracker
on on a smartphone directly filmed off
and you see a bit of augmentation in
there which tries to indicate how how
well it's tracking and how stable it is
and and other performance of it is other
areas are visualization so this was a
topic that Steve also mentioned
yesterday how do we present information
of mandatory ality and here we are
trying to example show you the structure
of a machine or of an object or
something like this and and one way of
doing this in illustrative visualization
is explosion diagrams so we could take
the real object and explode in some form
and then you get into another problem
when you have a lot of bits in your
field and then this might be cluttered
and very complex and you can't see the
individual parts anymore so you need to
organize that you as well a bit so how
do you do that automatically and how do
you react to changes in the viewpoint
and in the scene and
riders and finally it also goes down to
two actually user interfaces so this is
an example where we try to use all this
technology to provide a navigation user
interface for a pedestrian in an
industry so we don't really have GPS or
something like this it might be much
more difficult to actually establish our
position at this point and so we are
using things like tracking of known
targets step counters from the
accelerometer data and so to provide an
integrated experience and look at how do
you need to use that or audio can you
integrate the two given user interface
at that point and so we have at some
point we have fixed locations where you
get an overview map and you select where
you want to go and then you get an
instruction set with individual steps
and turns and these are then matched
again in the step counting against Ted
to forward the user interface and give
you a kind of a turn by turn navigation
in the building and this might sound a
trivial use case but Sven yesterday
showed the examples of our buildings
these gray boxes and they all look the
same and it takes you two weeks to be
there to find your room again if you
start so for asset er it's a real
problem one the topic for today that I'm
interested in is is creating content and
models forward materiality and we need
content as of course as the information
that we want to present which is very
application specific but we also need a
lot of information about the environment
to be able to implement actually our
mental energy applications we often need
models for visual tracking we need them
for rendering to be able to have correct
illusions to have lighting situations we
also need them for interaction we often
need to know that if I'm pointing at
something in the scene what is that
object that I'm eating there how far it
is away how does the normal look there
and so on and we want to extend this of
course and particularly use it on mobile
platform so we often have not prepared
environments we don't have a virtual
reality lab setup or similar finish
internal the motivation for us in this
kind of work is this idea of an AR
browser as an ultimate location-based
app everywhere we are we can look at
information we can retrieve
location-based information and it should
be possible to register this
not only based on that GPS position or
something like that but really attach it
to an object attached it graphically as
well to an object and if we look at kind
of what we can do there was a typical
idea of original described for example
in the Worldport paper that you have a
global reference system you put things
at a location and this is how it works
nowadays with browsers applications that
you can get already and install they
literally use a database of GPS
reference information and then show this
to your arm we fold it advantages that
you have a global coordinate system
which is easy then to use and to define
but also disadvantages it ends a lot on
this one sensor you have this very
indirect registration problem of putting
your content into a certain location the
other ways of actually putting models
creating models and so on this is again
this problem of really freely modeling
which is very complex refer dr mention
this before as well it's really worth to
look at how can how can non-expert users
and users customers really add to that
in a simple way in some form and we also
motivated by this idea by making it so
simple that we can kind of generate
maybe a kind of user generated content
driven environment for this kind of
information as well so extending this
from taking snapshots which is directly
supported by the device and is very
simple operation to all the more
information that is particularly
registered in the environment so this is
kind of like just a concept again that i
mentioned before that with they are this
registration is is a big issue and it
basically means that we need to capture
when somebody creates a piece of
information wants to put it at a certain
location we need to capture something
about the environment often to be able
to actually make this link happen again
and so we need to capture of the scene
environment as well as what the user
actually intended to put there and and
how to put this there and we think that
this particularly the same capture is
something that the system should
basically do on the fly and
automatically without your intervention
it should not be something that requires
to use explicitly to go out and do
certain
a lot of things and this is also of
course it connects back to a lot of work
in envision in the last years
particularly online reconstruction and
slamming zone which provides us a lot of
the technical input into this problem
and we get something of an idea about
the sister about the environment we get
some idea about what's happening with
our device what how it is moving what
the user is kind of doing in this
indirect way but we don't get a lot of
semantic information yet it's it's all
about simple or a simple representation
of geometry that's put it like that so
we we don't have structure of the object
of the environment we don't have it
segmented into the individual pieces we
don't know how they behave together how
they operate together as we have heard
yesterday in talks where there's really
a problem of analyzing this and then
this is its own task and this is for
modeled objects already and here we're
talking really about the freeze in here
so on the other hand really modeling as
we heard as well as I said for it's also
difficult and and we don't want to we
have this now on a mobile device and the
screens will not get that much bigger or
something like this so we can't really
put even all these complex functionality
on there in that sense so what we are
looking at is trying to build around
this concept of capturing the scene
somehow automatically but then edit
having the user add to it or use it in a
simple way to create something like that
and we want to have kind of with this
stable content registration and simple
ways to do that and then slowly extend
this to more and more complex
environments and complex interactions
and scenario and so working on mobile we
wanted to have something like p.m. in a
very robust way that works outdoors in a
lot of environment and the first
approach here was to just have a kind of
online panoramic checking mapping
approach which basically means that you
created panorama s a user pans around
which is a typical behavior when you're
standing somewhere when you're using one
of these applications and we can do this
in a very efficient way on the phone
which allows us to really do this even
on the phone in the very real time
operation and this was worked by Daniel
you
we sort of present one of the key
concept is that we are not actually
trying to have some global estimation
for example between individual images or
so but we are just incrementally
accumulating the whole information in a
big panorama and we do this on this tile
based based organization which allows us
to only look at new information when it
kind of touches a tile that is not yet
filled filled tiles in this panorama are
basically just not looked at anymore but
these field tiles are kind of put into
the next step of whatever you need to do
for example first of all to provide some
tracking we need some kind of
referencing between the life camera
frame and the panorama so whenever one
of these tiles is filled then actually
we run on this feature detection to get
interest points and then use those
interest points again in the tracking of
the panorama so we kind of have this we
avoid all the redundancy of processing
the whole image at the all the time we
just process it as we build it up and
that works accurate enough in this
closed parameter space that we can
basically do this to this well and of
course it creates a system then it is
very cheap to operate to make it robust
as well there are some additional steps
that allow us to relocalization against
other data quickly in this way and well
the basic output is a panorama but this
was not really the main focus here
panoramas from higher resolution images
taken with proper stitching and planning
and so on provide higher quality but we
also get this interactive orientation of
the camera which is the second important
information for us now this is just
again coming back to this principle that
is basically just taken from the PTM
approach of course that when we split
this up we have these two modes of
operation and we can attach more and
more operation to this second part here
as well a simple way is just to use
these panoramas as a canvas on which you
actually put your annotation this this
kind of canvas allows you to have much
more accurate registration and if you
just have GPS position or something
similar approach to reference this and
so we have a kind of simple browsing
annotation
application where based on your current
location of database of available
annotations and then when you move into
the AR fu you actually recreate the
panoramas on the fly again while you're
tracking and we are now using this
panorama to actually have fast or let a
high-performance matching within that
created panorama so in the panorama we
are then able to look for any of these
reference points to where we have stored
some visual information and are just
able to detect them in the panorama and
because the paramus build up we just
need to do this again once and we don't
need to do this per frame so we can save
a lot of computational performance there
we can also add of course invitation and
this will basically trigger storing some
more information so the principle behind
this is again that all we really are
doing here is for one of these locations
in the current setup when you record
this we take a patch we store it with
the annotation and then we just search
in the current environment that you're
building for the patch again and that's
where we apply the panorama in the end
and because we don't we are building up
the panorama again in this step by step
way we just need to do this per tile we
don't need to do is in on the whole
video image all the time so this gives
us a very good amortization of our work
over over the video frames as they're
coming in there is more to it you can of
course use multiple of these references
to estimate your orientation better I
want to be robust against individual
outliers and similar things but this is
kind of a again a simple principle how
we can make these things work on a
rather limited platform we also extended
is to get a bit of depth in our
environment by using the same input
again in a basic being a structure for
motion reconstruction pipeline and again
with the idea that that we want to kind
of limit the work a bit for the user
here we wanted to make it simpler
instead of taking a lot of images to
cover a certain area we're just
basically having three four locations
where we capture this wide angle to very
wide panorama images and then run
basically a basic reconstruction
pipeline
there's a trade-off in there of course
and that we don't capture so much
redundant information between different
viewpoints we are kind of limited to you
too less few points but we can kind of
get it to work in and have reasonable
reconstructions and so these are kind of
the principal again behind it you you
just go to three or four locations and
you capture these from the different
areas where you want to be and then you
really just have a standard structure
from motion pipeline where feature
extraction is happening on each of these
panoramas matching then pose recovery
bundle adjustment and so on and we kind
of get a get a measure out of this there
are some more to this that this mesh is
actually treated in a kind of volumetric
fashion and then carved a way to get to
get some of the usability more correct
so and these are kind of a simple
results that show you that the qualities
is not that great but on the other hand
we are able to do this in basically
capturing the panoramas and then five
seconds per panorama on the whole device
which is mostly matching in some of the
reconstruction here's a bit of a bigger
scene and all of this you get from
basically here free to up to seven
panoramas so this is kind of trying to
limit the work and give you immediate
feedback for that and having then this
really structure in the background is
basically having the panorama plus more
depth information there which you can
use to put our annotations actually in
3d to locate it with distance and normal
information to orient them on the
surface to use that in rendering effects
and and similar aspect here as well so
the last way of using this kind of
method to to add more content is trying
to reuse other content and here we
looked at videos specifically so in
YouTube you will have lots of people
shooting videos for example of their
here skateboard tricks or other things
that are happening in location and we
thought well can we take that kind of
input and that kind of Content that
somebody provided and allow them to well
register it but rather just directly
view it in that scene in that
environment without them having to
anything else really and so the ideas
you go to the more or less same location
there as well and you want to see that
action in that video again because you
want to see your friend how they did it
and you don't want to kind of reference
it back and forth between the video
screen you want to kind of see it over
the obstacle life and all of these
things well the principle is again
probably not not not novel in that sense
we need to identify somehow the the
object of interest here which is kind of
indicated at the beginning and then with
grab card we extract it and transfer it
from frame to frame to get like a
segmentation in the videos and by doing
that we get on the one hand side the
segmentation of of the object of
interest here but we also get a mask for
the background and using that mask again
on the video we can establish now a
background panorama just as before as a
kind of reference here because we have a
clean background now where this object
has vanished and listen it's not present
anymore using that that background
panorama we basically at now the
information of this is the background
and this is how the object moved through
the whole panorama it's not like we're
it isn't every frame because we know
where your frame was in that panorama we
can actually like have this object move
through that again and finally we have
to in the end match this offline
panorama to our current panorama and
that's it we have this registration once
and after that we actually are able to
transfer basically the skater or this
object in the view as it is here to the
current view that you are having and
this is kind of independent of where
you're looking at the moment it's all
and correctly updated accordingly and
this is still working process so this is
just a bit of a dark video here but this
is the the figure as before and it's
registered here and looping again and
again so you basically get it overlaid
over the live view in this life direct
manner okay yeah with that I'm Greek I'm
really coming to to the conclusion for
us this kind of scene capture gave us a
lot of robustness and and amortisation
so that we could actually bring this
functionality on to
rather limited device such as mobiles
here what we are really interested in in
the future and this is maybe a lot of
where I'm actually very happy to see all
the talks here and see this input here
is we want to further understand the
scene a bit more to actually use that to
constrain our interaction so we want to
kind of have somebody just click at a
single thing and the system should
select what that is or at least give you
hypotheses about what it is that you
might have meant at that point and then
attach it directly it could measure for
example a window and then align any data
or graphics that you want to register it
with that geometric object with that
window we are not necessarily implying
that this should be fully automatic so
that I get the full scene understanding
back we are rather interested in having
the user input something but because the
user interface itself is very limited on
these devices we want to have the user
the minimal input necessary to do to
make something happening such as
basically pressing a single pixel on the
screen or something like this and then
after that the system should provide you
with like what you actually want to do
that what you could do there in the end
and how you how you have to do this in
places again okay that thank you and
these are the collaborators Peters
marketing is really are the head of the
group and Danielle bogna is here
claimants to be as Alessandro and key
worked on various parts and we have a
big project that deals with particular
these handheld augmented reality parts
that is also funded by Qualcomm so we
have a lot of collaboration with them</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>