<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session 6 | Coder Coacher - Coaching Coders</title><meta content="Oral Session 6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session 6</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ImalPzUhuWo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay and here we have our first speaker
we are going to learn about how to
detect three variable interactions the
authors have already worked on two
variable interactions now three maybe
this talk I mean there's a very big
difference from two to three and
hopefully will also get promise of how
we can do for and even more and the talk
will be given by D no see sadena bitch
Arthur Graydon and the ratio of asthma
are co-authors so it's from gas be in
the UK and London School of Economics
thank you so I will be talking about the
nonparametric hypothesis test for three
variable interruption okay as which is
based on embedding so probability
measures into reproducing kernel bed
spaces so let us start with a simple
motivating problem say that we're
interested in detecting V structures in
a director graphical model where care
wise associations between individual and
variables are very weak and very
difficult to detect in other words
dependence of Zed on x and y jointly is
much stronger than on external
individual for an illustrative example
of that you can consider the case where
X is whether you put sugar in your
coffee why is whether you've steered it
up and that is the sweetness of the
coffee so individually weak effect
jointly strong effect and you can even
argue that Y is independent exactly this
case it is also straightforward to
construct an extreme example where there
are there are all three variables are
pairwise independent but there is joint
dependence and you can argue there is a
restructure like this it suffices to
consider x and y being standard normal
and the sign of that depending on the
sign of the product of x and y so this
is the continuous analog on of the x or
in the binary case if you work so if you
look at the scatter plots it's clear
that text is independent of Y wisent
enters addicts
but there is joint dependence so how do
we go about detecting these structures
like this where we don't have pairwise
dependence that's the question we're
trying to answer but before we get in
there we will first review how to do a
standard pairwise independence test so
this has been known for a while for a
couple of years now based on embeddings
of probability measures into your
producer clearing a space so let's say
that we have a data that looks something
like this so we're interested in a fully
nonparametric framework so our beta may
lie in a non-euclidean structure domain
so say that X is paragraphs of English
text wise our paragraphs of french text
and we are interested in detecting
dependence between them perhaps we want
to test the hypothesis that one is
translation of the other we're not
interested in the translation problem
that's too difficult we just want to
detect whether in the pairing there is
some information here okay so what is an
intuitive way to approach this problem
so we could pick a similarity measure on
paragraphs of English text sake a form a
corresponding similarity gram matrix do
the same for French and then we can
consider what similarity between these
two matrices means it means that when we
have an entry in the matrix similarity
between two parallels of English text
and it's this is large then the
corresponding similarity between
paragraphs of French texts also tends to
be large so this is a good evidence
against the null hypothesis that these
are two independent data sets so it's
quite intuitive to consider just an
inner product between these two matrices
and that is up an intuitive dependence
measure so these penal gram matrices are
just centered by a standard matrix H
this means that we'll just removing the
mean from each of the columns in a
product is just the entry wise product
of these two central matrices and then
we just compute the sum of all elements
so this is this notation possibles so
that's an intuitive dependence measure
let us see where it arises power
so this is just a standard kernel trick
feature map formulas so we will replace
our explicit feature map by one up two
fires of an individual data points set
with an abstract feature map that lies
in a Hilbert space k-dog that we don't
really care about the explicit
coordinates of the skater that indeed in
many cases this is an infinitely
dimensional object only thing we care
about is that we can easily compute
inner product between individual
features that's just the kernel function
so embedding of a probability measure is
just one step further rather than
considering a feature of an individual
data point we consider feature of a
distribution so embedding a high
probability measure p denote the new KP
is just this expectation of the
canonical feature where expectation is
taking over a Hilbert space so again we
just have some representation of a
probability measure which replaces a
representation in terms of moments it
can be infinitely dimensional as well we
don't care about the explicit
coordinates of the representation only
thing we care about is that we can
compare we can easily estimate inner
products between individual embeddings
this boils down to just an expectation
of the kernel function with respect to
these two probability measures B&amp;amp;Q okay
so now that we have an embedding of a
probability measure its natural to
consider a notion of distance between
probability measures that's based on
these embeddings so we say that distance
between P and Q is just hilbert space
distance of their embeddings this is
called maximum in discrepancy literature
for a broad family of kernels k this
embedding is injective meaning that MMD
is a metric on probability measures and
for a strike slightly stronger condition
that is called integral in strict
positive definiteness we have this
embedding injective on all sign measures
most of the kernels that we know and
love are integrally strictly positive
definite okay so now we have a notion of
distance between probability measures so
what does that have to do with the
pendants well then it's very easy to
construct a dependence measure by just
looking at the Hilbert space distance
between the embeddings of the Joint
Distribution and the embedding of the
product of the modules so this is known
under the name of hilbert smith
independence criteria so here we're
working with product space so we need a
kernel on the product space this is
denoted cup we can use any colonel on
the product space that we want but one
choice is just to take the product of
kernels on the individual spaces KL and
when we do that and compute the
empirical version of this quantity we
get exactly what we started with so this
is precisely just the inner product of
centered colonel matrices let's
normalize in a particular way now we can
proceed with creating tests using this
statistic and this results in very
powerful a framework of independence
testing that generalizes other
approaches at the pyrrans literature
including distance killarian singh by
sekalian quarters in the statistics
literature okay so that is our
dependence measure Huber's with
independence criteria so let's go back
to our original problem of restructure
discovery we know now how to test for
pairwise independence we've known for a
couple of years so let's assume that
we've established for this problem
already the texas independent of Y then
we can proceed with the detecting this B
structure by posing a conditional
independence test whether X is
independent while conditionally on that
so this has been done by zanga quarters
in 2011 by extending the notion of HC or
we can proceed in a different way and
consider a factorization test where the
null hypothesis that joint distribution
of these three objects factorize us in
any way so this is now a multiple
hypothesis testing framework each of
these individual tests is just a
standard kernel based test because we
can consider x and y just to be a single
object so in a multiple hypothesis
testing framework we have to apply home
bonferroni correction on the p
values of individual tests and so let's
just do that on our standard problem
where we have pairwise independence
enjoy independence to make the problem
more interesting we're going to append
bunch of dimensions of noise here so we
just have a P minus 1 independent
dimensions there is dependence that is
only joined but not pairwise in the
single dimension so when we apply those
two approaches we get something like
this four dimensions one and two
building all right but the type two
error increases quite rapidly as the
dimension increases and that's somehow
to be expected because we're dealing
with quite a difficult problem this is
with 500 samples and weak with media and
bandwidth Gaussian kernels so can we do
better that's the question and the
answer is that we can buy the tackling
this factorization hypothesis directly
by looking at the quantities that
directly capture whether Joan
distribution factors so this quantity is
known under the name of Lancaster
interaction it's been studied in the 60s
by bahadori Lancaster so interaction
measure of a joint vector X 1 up to X T
is a sign measure delta p that vanishes
whenever i can factorize joint
distribution in any way as a product of
margins so x1 XD here need not be scalar
variables they could be random vectors
themselves the difference is just that
we consider the most d different objects
so what are these interaction measures
for the equals to its straightforward to
see that this is just the difference
between the joint and the product of the
modulus there is just one way in which
joint can factorize but for the equals
three we get something like this so this
captures all the possible different
factorizations of the joint of three
random articles so is this pictorial
representation
and to convince you that this is a
correct interaction measure I eat that
it vanishes whenever joint distribution
factor Isis say that we have the
factorization in red then p XYZ breaks
down as px py said but but egg p XZ also
vectorize SN p XY also factor eyes so
blue terms also cancel out so we get
that long stint Direction vanishes okay
so this is just some sign measure on the
product space we know how to embed
probability measures and sign measures
introduced into another space so we just
do that so we construct the test which
estimates the hilbert space norm of the
embedding of the lancaster interaction
measure colonel kappa on the product
space now just the product of three
kernels on the individual domains so we
have just this Hilbert space norm
squared this breaks down as some of
Barrios inner products inner product
between embeddings we know how to
estimate so we do and we get something
like this so each of these these
statistic estimators of inner products
just depend on the colonel matrices
individual three kilometers so then you
see that there is some interesting
structure in there there are entry wise
products and their matrix products as
well and we get some of just the sum of
all elements of a particular matrix at
the end now when we take these terms all
together and consider what the longest
interaction statistic is we get
something that looks like this very
simple so the whole thing collapses just
to a simple expression which is just a
three way and three wise product between
three Center Colonel matrices so an
incredibly easy quantity to compute
and this is not a coincidence because
what we can we can view the embedding of
the Lancaster interaction measure as an
empirical joint central moment in the
future space and this precisely captures
this kind of quantity okay so we just
use now this as a statistic rather than
doing three individual tests and we get
a much improved type 2 error in a case
where we have pairwise independence but
joy dependence so even for the lancaster
interaction test we have to apply a home
bonferroni correction because we are
still testing three hypotheses we just
happen to be using the same statistic so
that was the example where we had
pairwise independence but Joe
independence we can also consider a an
example that looks something like this
so here we have that depends on X and
depends on Y as well but the joint
dependence on the pair is stronger than
on x and y individually ok so we look
also at the comparison between the
conditional independence test standard
two variable tests and the tests based
on what is the interaction in this case
as well and we will again get
significantly improved type 2 error
especially in high dimensions so you
will not notice that the gap between the
standard two variable tests and the
longest interaction test closes in this
case and this is to be expected because
there is dependence of that one x and y
individually as well so to start to
variable standard based tests will will
also have improved power in this case
ok so now natural question arises is
what happens when we go beyond three the
answer isn't as satisfying as what we
have in the case the equals three
actually the reason is that the
interaction measure that was constructed
by lancaster does not actually capture
all the possible factorizations in the
case as d greater than or equal to 4 the
this was noticed and corrected by
stribild in 1990 this is the correct
stride Berg interaction measure and you
see what we have here is a summation
over all partitions over set of the
elements j pi p is here just a
factorized measure that corresponds to a
particular partition the number of
partitions increases quite rapidly so
here's 52 4 equals 5 and here is how
they grow up they go out quite quickly
so we can hope to be able to pose a test
only if this expression collapses just
in the case of 150 interaction measure
but it doesn't because the expression
collapses only when we consider join
central moments this is captured by
lancaster interaction construction
whereas this corrected version of
interaction measure actually corresponds
to joint cumulants and joint cumulants
and joint central moments only coincide
when the equals 2 ND equals 3 4 be
greater than equal to 4 joint humans are
just some other polynomials and we don't
really have a club that collapsed
expression so while in principle we can
go ahead have that huge table and
compute all the B statistic estimators
of inner products it becomes
computationally intensive quite rapidly
okay so just to summarize the talk so we
introduced a nonparametric test for
three variable interruption and for
total independence total dependence is
discussed in the paper I didn't really
have time to discuss it here this is
just another straightforward
modification of the test
as a methodology we used embedding
subside measures into producing cleaner
for businesses all the test statistics
that we derive our simple and easy to
compute corresponding permutation tests
are straightforward to construct a
significant outperform standard to
variable based tests on B structures
with weak pairwise interactions for a
very broad family of kernels integral is
strictly positive definite we are
guaranteed to the pack with sufficient
number of samples all forms of lancaster
three variable interaction because
embedding of silent measures in is
injective and thank you very much for
your attention of our posture is as six
thank you so much I'm i see this room
but also for talks for the next coming
years when we have to go to foreign and
above that so there's time for a
question at you so is there some notion
of a strongest test so is the Lancaster
test known to be best in any
quantifiable way but that's nothing for
an interesting question but we haven't
done in any comparison that's in that
sense any other questions
okay then let's say thank the speaker
again if this so by the way no now we
you're packing up this was all done in
synthetic data do you also have
experiments for a real data so we have
done some initial tests on on some
genomics data where you have that gene
expression of two genes actually has a
switching property when you consider
conditioning detergent and while
definitely it's interesting to also be
able to detect three variable
interaction in this case this is a data
set where it is also easy to detect two
variable interactions and I think where
our tests can be really useful in in the
cases where pairwise interactions are
very difficult to detect and this is
something we haven't really explored yet
but it's definitely part of the ongoing
work how to find these kind of cases for
in your data's okay good luck
and the next chalk we have a speaker
okay so in the next talk we are going to
move into into a machine learning
framework for synchronous parameter
sharing where you may you paralyze your
computation and the single worker may
get a stale version of the parameters
but as far as I can see there's a
guarantee that it's not more than a
certain amount still when you get it and
we are going to get a rigorous treatment
after problem once the the whole talk
comes up it is it's the talk will be
given by chi gong Oh Jer on hold your
own home sorry um and it is work from
Carnegie Mellon University all right so
good morning everyone and thank you for
having me up here talk my name is Jerome
and I'm going to talk to you about more
effective distribute ml via still
synchronous parallel parameter server
the motivation here is that we're trying
to take a single machine iterative
parallel ml algorithm from one machine
to Mellie that's the distributed setting
and normally in your algorithm you have
these critical update steps that are
executed on one machine in parallel over
many threads the worker threads share
these global model parameters theta V
Aram now we want to scale this up to a
large distributed setting so we now have
to share these parameters over the
network that's synchronization at first
glance it seems like a simple task
they're ready distributed tools
available so we can just pick one and go
right well it turns out it's not quite
that easy so it has two main distributor
challenges the first one being that
networks are slow far slower than the
CPU to ramp interface the second issue
is that just because you spec out your
machines identically it doesn't mean
they'll perform equally in practice in
fact that's really the case
so let me explain what I mean by these
networks are slow in the sense that day
of low bandwidth low bandwidth CPU to
rams about 20 gigabytes per second over
the network is between point one to one
gigabytes per second that means you can
transmit fewer para meters per second
over the network furthermore there's
high latency your messaging time
somewhere in the order of 100 to 1,000
times longer waiting for stuff to go
over network and that means you wait
much longer to receive parameters the
thing is that parallel ml requires
frequent synchronization we tend to have
to exchange between 10,000 to 1 million
scalars per second per thread in the
civil machine setting if the parameters
are not shared quickly enough you get a
communications bottleneck and this is a
significant bottleneck over a network to
illustrate this point if you try to run
a latent dirichlet allocation topic
modeling gibbs sampler on 32 machines
using the bulk synchronous parallel
synchronization model you'll find that
you spend six times as much time waiting
on a network as you do actually
performing useful computational work in
other words you're wasting most of your
processor cycles and that's for a plea
in setting with full control the
machines and full network capacity in a
real production cluster or research
cluster there are many other users and
you have even worse never compute ratio
because you have to share it with them
the other reason is that machines don't
perform equally even when configured
identically there's a variety of reasons
why breitung hard drives can slow down
this performance background programs are
usually running in a large cluster other
users might be using it or you might be
part of a vm or cloud service like
Amazon ec2 the consequence is that you
get these occasional random slow down to
different machines and I'll talk about
how these harm distributed performance
later as a consequence scaling up ml is
a hard problem going from 1 to n
machines Rella yields and n fold speed
up we do it naively that's because you
get slower convergence due to machine
slowdowns and network bottlenecks and if
you're not careful you can do even worse
than a single machine you can diverge
due to all these accumulated errors with
the slowdowns
so there's a lot of literature on
general purpose distributed scalable ml
roughly divided into more Theory
oriented work and more systems are going
to work on the theory side we focus more
on algorithm correctness and conversions
on the system side it's more about
maintaining that hydration throughput by
taking care of systems issues on the
theory side there is sometimes a
tendency to oversimplify systems issues
we might assume that the machines or
perform consistently and equally or we
might need a lot of synchronization in
some cases they are methods that just
assumed be done let's try not to
communicate at all but there really is a
middle ground that we want to exploit
then on the system side they may
oversimplify the ML issues such as
assuming algorithms just work in the
distributed setting without proof are
and in a lot of cases you have to
convert the program into a new
programming model which may be a
non-trivial effort can we take both
sides into account so we advocate a
middle-of-the-road approach we want em
algorithms to converge quickly enter
imperfect systems conditions the slow
network performance and the random
machine slowdowns that I've been talking
about the point being their parameters
are not going to be communicated
consistently under the setting existing
distributed em I work mostly uses one of
two communication models the bulk
synchronous parallel model and the AC
coolest model before i talk about still
synchronous parallel I'd like you all to
understand the pros and cons of bsp and
a sink in the bulk synchronous parallel
model every worker thread on the cluster
or synchronize or wait for each other at
the end of every iteration that's
denoted by the red synchronization
barrier in this diagram threads are
always on the same iteration number and
parameters are red and updated only at
the synchronization barrier this is the
ideal picture this is what happens in
practice because machines perform
unequally they take different about
times to complete their iterations the
fast ones have the short arrows slow
ones have long arrows and that's also
because algorithmic work will abide
sometimes be imbalance which further
complicates the problem so threads wait
for each other they spent a lot of time
waiting for each other in fact
furthermore the end of iteration
synchronization gets longer with larger
clusters due to the slow network you're
trying to communicate all those
parameters right Andy
and so you've got a traffic jam all that
white space and all that weight space
red space is wasted computing time in
the asynchronous setting we're going to
say threads can proceed to the next
iteration without waiting so in general
they're not going to be on the same
iteration number the parameters can be
read updated anytime so that sort of
removes the synchronization barrier
bottleneck what happens when a machine
suddenly slows down one thread will take
longer to complete iteration while the
others all speeding ahead that causes an
iteration difference between threads
that leads to error in the parameter
synchronization in the worst case on a
large cluster you can have an
arbitrarily large slowdown machines may
become inaccessible for an extended
period this is a common observation on a
cloud network like Amazon ec2 where your
machine might go off the network for a
few minutes at a time in that case the
error becomes unbounded because the
iteration difference between that one
thread and everybody else is growing
rapidly what we really want is a form of
partial synchronicity we want to spread
the network columns evenly and we don't
sink unless needed threads usually
shouldn't have to wait but we can't
allow them to drift too far apart the
other thing we want is a form of what I
call straggler tolerance the slow
threads must somehow catch up to stop
wasting everyone else's time the
question is is there a middle ground
between the BSP and asynchronous models
of communication
that middle Brown ideally should look
something like this when a thread is
taking too long to complete the
iteration we're going to stop and say
all of you have to sync up and after
that you need to make that thread catch
up somehow it needs to process its
iterations faster how do we realize this
we introduced a system called sale
synchronous parallel a model
synchronization in this diagram note
that the x-axis is now iteration count
and not time the idea is that we allow
the threads do usually run at your own
pace but we restrict that the fastest
into those threads cannot drift modern s
iterations apart the threads cash local
or still virgins of the parameters and
they read them whenever the model allows
it this is to reduce network sinking a
threaded iterations t under this model
will see all parameter updates made
before iteration t minus s the protocol
is simple you check your cash first if
it's fresh enough you use that value if
it's too old you fetch the latest
version from the network and incur some
network costs as a consequence of this
protocol fast threads have to read every
network iteration because the slowest
thread is going to force them to read
every iteration however the slow threads
only have to check once every s
iterations they make fewer never
obsesses and so they catch up naturally
thus SSP provides a best of both worlds
situation it combines the best
properties of bsp and a sink it has BSP
like convergence guarantees because the
threads cannot drift modern s iterations
apart and every thread sees all updates
before iteration t minus s there's
bounded error is asynchronous like speed
because most of the time so that's
usually don't wait unless there's too
much drift and solar threads can read
from the network less often so they
catch up naturally to the faster threads
more importantly SSP is a spectrum of
choices you can be fully synchronous if
you set s equals 0 you recovered about
synchronous parallel model or if you
push it to infinity you get very very
asynchronous behavior we advocate taking
the middle ground to enjoy the benefits
of both
now what does SSP converge instead of
seeing the true parameter X sspc some X
still which is the true parameter for
some error incurred by all the still
reads in SS pediatr they are opposed by
this stillness is bounded and over many
iterations the average error will go to
zero intuitively you can think of SSPs
approximating some sort of sequential
execution see the red line there that's
when I've ordered the updates in a
certain order we're going to compare
this at this order the sequential ideal
sequential order to the actual update
order which is actually given by the
white bars because of the staleness
window when you get to the point where
you need to update the dotted black bar
dr. black box you may lose some
iterations because some threads have not
given you the information you need to
proceed with that update in the correct
sense in this case that's the blue box
you may also gain extra updates because
some people are head of you so they
provide you these extra updates that one
part of the original sequential order
those are the orange boxes so the total
error in the system is the blue boxers
plus the orange boxes but that's upper
bounded by the way for the two green
windows so that's the intuition in which
SSP bounced numeric error and des bounce
the error in the parameters it's a
partial but bounded loss of civilized
ability so more formally if we assume
that we are trying to optimize a convex
function which is l that shits its
gradient is bounded and is a problem
diameter is bounded as well under
stillness set in s and with P threads
across on distributed machines as a
speed n converges at a particular rate
where t is the number of iterations but
when take away from this result is that
the right hand side bound contains both
the arguments of the function L&amp;amp;F as
well as arguments of the system SMP
there's an interaction between this
theory and systems parameters
so SSP solves the distributed ml
challenge by providing a synchronization
model for fast and correct distributed
ml it reduces network traffic by caching
local parameters whenever possible and
address the slow Network and occasional
machine slowdown problems so how do we
actually use this we implement SSPs
something called a parameter server we
call SSP table which provides all
machines with convenient access to
global model parameters it can be run on
multiple machines to reduce the load per
machine and it allows easy conversion of
single machine parallel algorithms into
distribute at once using a particular
style programming called distributed
shared memory if you look at example on
the bottom right if that's your critical
step you replace all the array accesses
y with calls to the parameter server the
idea is to keep the program looking very
much like a single machine program but
you get basically parallelization with
minimal effort distributed paralyzation
that is you don't need to do complicated
message passing and in terms of
interface it just has three commands you
don't have message passing you don't
have various you don't have lots you
just read you increment and you advance
to the next iteration
more concretely if we're trying to make
an application distributed if it's a
topic modeling mcmc algorithm we will
put the topic work table in the
parameter server SSP table if its matrix
factorization say using stochastic
gradient descent we put the factor
matrices in if you're doing lasso
regression which is important it's an
hour boredom we'd put the coefficients
beta inside more generally SSP supports
generic classes of algorithms with these
models as examples returning to this
slide I want to show you that SSP users
network sufficiently on the Left we have
BSP and as we increase the stillness the
network waiting time drops relative to
the compute time at some point we sort
of hit a balance point where it's about
roughly equal and then we get
diminishing returns as we increase
stillness but this is basically how SSP
realizer speedups we decrease that
bottle network bottleneck in terms of
convergence results this is an the LDA
example on the New York Times dataset on
32 machines with 256 scores now bolts
infernus parallel has strong convergence
guarantees but is slow in practice
because of all the stragglers and
slowdowns asynchronous is fast but as
weak convergence guarantees if you let
that curveball all the way to the end it
actually starts to oscillate SSP is both
fast and has convergence guarantees
there is a quality versus quantity
trade-off when you use a system like SSP
I've shown that the network waiting time
decreases as you increase stillness put
it another way you actually get more
iterations per unit time that's the left
graph here however the amount of
progress towards convergence you make
per iteration decreases due to the error
you introduce and that's the right graph
here the thing is what we really want is
progress per unit time which is
iterations per second times progress per
iteration so it's the product of the two
graphs and we want to find a sweet spot
stillness greater than zero that
maximizes progress per second that's
actually the graph looks something like
this you push up stillness you get more
iterations per second but you'll get
less progress per iteration but product
the two graphs has a peak somewhere and
that's basically the reason rationale
for using SSP there is a certain amount
of error tolerance you need to give for
the system to perform at maximum speed
we also have results for matrix
factorization on Netflix and lasso
regression on synthetic data and in
terms of scaling if we fix the data set
how does SSP performers we throw more
machines edit so for the New York Times
data set we can show about roughly a
seventy-eight percent speed up for every
time we double the machines and that
means you converge in about point five
six the amount of time every time you
double so it's more or less linear drove
it's not slope one but at least it's
scaling well up to at least those two
machines which is tested we also have
some recent results that didn't make it
into the paper here we're using eight
machines x 16 cause equals 120 threads
with 128 gigs of ram per machine now on
the New York Times data said if we
increase the topics all the way to
10,000 we can do about a hundred
thousand tokens per second on this
cluster GraphLab does about 80k per
second on the PubMed data set which is
significantly larger at about 7.5
billion tokens say if we do 100 topics
we get throughput of about 3.3 million
tokens per second and GraphLab gets
about 1.8 million per second we've also
been recently using this SSP system
table system to do network lighting
space role modeling large network
modeling in the sort of topic model
sense and on a network graph that has
about 35 39 million nodes 180 billion
edges we can do about 50 network roles
in about 14 hours
first there's about five days on one
machine in terms of future work we
actually want to explore in three
different directions there on the
theoretical side we need to show
theoretical results of SSP on MC MC and
we need to do automatic stay on a
student because no one really wants to
have to tune these extra tuning
parameters and we'd also like to have
some average case analysis to tighten
our balance a bit more on the system
side we'd like to investigate load
balancing to further leave rates
traveler problems fault-tolerance
prefetching to improve performance and
other kinds of consistency schemes that
are not limited to say SSP but what we
really want to do is to enable new
applications we want to enable
distributed ml techniques to be applied
to hard to paralyze ml models such as
the neural networks regularize Bayesian
methods and you know various kinds of
network analysis models or models with a
lot of the interdependencies between
variables and parameters I'd like to
thank my co-authors James sepa hangang
3gen koo kim soon haffley Philip Gibbons
from Intel Gulf Gibson Greg Ganga who is
not the kid in the picture and ever
exciting right and we I'd like to take
club here we have a workshop demo coming
up at the big letting workshop on Monday
where we're going to introduce a system
called pattern which is includes the SSP
parameter server as well as a dynamic
variable scheduler that's currently
under submission and here we're going to
show these sort of results that I've
been previewing on the previous slides
topic modeling on 10,000 topics lesser
aggression on the hundred billion
dimensions and network modeling on large
graphs with that I will conclude my
summary distribute to MLS non-trivial
there's slow Network unequal machine
performance SSP addresses these problems
by efficiently using network resources
to reduce waiting time it allows slow
machines to catch up and it's fast like
a sink but converges like BSP and the
point really is that parameter server
log SSB table provides an easy interface
to quickly convert your single machine
parallel ml algorithm into a distributed
setting slide available at that link and
with that that's the end of my top thank
you
and code is also freely available
somewhere it will be available very soon
okay we have time for one question I
think before mean time where we take
questions can the spotlight presenters
please line up over here if they are not
already ready okay so a sinker Kristen
somebody must be what is he here over
here so how do you choose as the number
of iterations to know where you put the
put a barrier did you know what the
optimal s is correct and so I mentioned
that part of future work we want to do
this automatically but practically what
you can do is run for a few iterations
say 10 iterations and observe the
convergence progress of each s you want
to take the one that basically gives you
the maximum progress but time so we're
trying to make that automatic in the
future version but for now the practical
thing to do is to just take what's a one
to two minutes worth of results and pick
the S that does it and do the rest of
your one hour experiment using of this
well good one also imagine that is
dynamic that you start out indeed indeed
in the beginning you can tolerate much
more that is correct staleness but then
you have to that's also part of our
investigation we do want to have a
system where at the start you can
tolerate more at the end you have to use
this in fact you might even have to
become sequential at some point and just
tell the threads to line up to actually
converge okay look at you win more
question effort
there's actually very close to my
question I'm curious how you handle
parameters which require like radically
different amounts of updates between the
steps mm radically different so you mean
like say you've partition your data
different threads are going to put
different updates well in that setting
the update is basically zero so yes you
are committing update is just 0 let's
thank this PK game</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>