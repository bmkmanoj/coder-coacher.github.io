<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Divide &amp; Conquer Methods for Big Data Analytics | Coder Coacher - Coaching Coders</title><meta content="Divide &amp; Conquer Methods for Big Data Analytics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Divide &amp; Conquer Methods for Big Data Analytics</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZKIfrqawvcA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
let me start off by thanking the
organizers this is the first time I'm
giving a talk but my second time at this
my sole part because last time I was on
the other side and I was an organizer so
I appreciate how much work goes into
organizing an event like this so thank
you to the organizers and thank you for
inviting me so today I will talk about a
new class of methods that we are trying
to develop for handling very very large
datasets and doing large-scale data
analysis and this is joint work with
some of my students Joe dongseok and CC
and a faculty member Pradeep Ravi Kumar
so you know you hear this all this talk
about big data
well what's new what is new is that
instead of the situation 20 years ago
where we were getting Lord of structured
data now there's a lot of unstructured
data there is growing immensely so you
have Network data for example at
LinkedIn in biology and the kinds of
things that you want to do is you want
to try and predict maybe that's
so is you know you may want to try and
predict people you may know this is the
problem of link prediction you have fMRI
data there are looking at trying to
establish relationships between
different regions of the brain you may
look at the voxel data that comes up
from fMRI and trying to get techniques
that automatically try and estimate
relationships between different parts
you have lot of images video email data
and one of the things that you might
want to do is identify you know the
objects within images or you want to
identify for example whether an email is
spam or not spam so that's the classical
problem in machine learning of
classification and of course the data is
growing a lot so it staggers you
sometimes to hear about some of these
numbers like 250 million photographs
uploaded per day on Facebook so the data
is clearly growing very fast and the
question is we want to try and analyze
it so one approach is well we take our
traditional machine learning algorithms
and we'll try and develop algorithms
that use more and more hardware you use
and use parallel computing paradigms and
do so what I'm going to talk about is
trying to also develop new kinds of
algorithms that scale to these very very
large data sets so divide and conquer is
a very common and successful paradigm in
computer science and in Applied
Mathematics so one of the organizers
just walked in so glad he could make it
and this is sort of a cartoon of sorting
in numbers using an algorithm called
merge sort there the idea is did you
take a very large problem suppose you
have many many numbers you divide the
problem into subproblems you keep on
dividing the problem and then you come
to a small enough problem that you then
will compute the solution so in terms of
merge so it'll be trying to sort this
sub list of the total list of the
numbers and then once you
the computer this subproblem then you
will somehow merge these solutions and
get a solution to the bigger subproblem
and then you go up the tree and finally
you get a solution for the whole problem
and this paradigm is very common in
computer science I've said sorting for
example in Applied Mathematics there is
a problem called the n-body problem you
can do that those calculations but in
sort of n log in time because of a
divide and conquer strategy when I an
impression this made of on me was when I
looked at there's something called the
symmetric eigenvalue problem and people
wanted to design new algorithms for
paralyzing and then they came up with a
divide-and-conquer algorithm and it was
immensely successful but what happened
in that case was not only was the
algorithm successful in parallel but it
actually ended up being the best
algorithm in serial Lots so that's what
you know a lot of people use throughout
the word Big Data some people are
skeptical about it whether it's just a
height I truly believe that something
quite good will come out of it because I
think it'll spur people on to look at
new algorithms and we will really
benefit from the algorithms that come
out so the question is can we used this
kind of paradigm in machine learning
problems
oh you mean where that come through more
I mean you know I gave examples over
here of you know if you are at many
companies not just you know these
massive companies but even medium-sized
companies you're going to generate data
now when you do the analysis whether you
sort of subsample or you work on the
whole data that's you might say well
subsampling will do but maybe you work
on the entire data because there's going
to be a very long tail so in that sense
you may want to analyze the whole data
in once and that will vary from
application to application
well this this is big 1 billion users so
I see it depends also on I mean you can
always sort of define big as something
that you cannot do 1 or 0 machine right
so you need more so for example if
someone gives me a data set with 40,000
variables if I want to try and estimate
a sparse inverse covariance matrix I
cannot do it on a single machine right
so I will need more scalable solutions
but and you know a lot of people will
talk about pre-processing before doing
data analysis but wouldn't life be
wonderful if you can just take your big
CD or so and out pop the answers so
though there's there's a lot of value to
avoiding a lot of human intervention in
the loop so here is sort of an example
ok so here you might this you can think
of as a representation of a network and
what you might want to do is you might
want to try and identify these
communities that say d1 d2 d3 you take
your big data you divide it into these
parts you may want to do this division
either in a randomized manner but you
may want to be a little bit clever to
try and identify these communities but
it has to be fast to be able to do your
subsequent analysis and then you can see
that maybe there are sub communities
within communities
you keep on dividing then you get
something over here small enough
problems that you can then not only do
on a serial machine do easily but also
there is going to be immense amount of
parallelism to be exploited over here
and then once you get the solutions of
these problems then you combine them in
some way to get a model ok and maybe
there's a possibility that this model
actually might be better because you may
be able to look at data at different
scales so what I'm going to do in this
talk is give you three concrete examples
where we have used divide and conquer
methods these are somewhat classical
problems in machine learning so the
first problem is going to be doing
nonlinear support vector machines and
I'll talk about a divide-and-conquer
method then dimensionality reduction for
very large networks and then trying to
apply it to the link prediction problem
in social network analysis and then this
problem that was motivated by looking at
fMRI data trying to establish
relationships between different voxels
in that case so I'm going to now go into
kernel SVM there are many experts here
in the audience so I'm actually going to
try and spend I think
more time on this problem than on the
other two so this is the classical
picture that you see in any textbook on
support vector machines it's widely used
classifier ok the setting is that you're
given training data these are points
which are d dimensional and let's just
consider the special case where a simple
case where there are two classes so the
red class or the green class the red
class is minus 1 the green class is plus
1 and the goal in support vector
machines is to find a hyperplane that
separates these two classes of depth and
the margin is given by this if W is the
normal to the hyperplane then the margin
is given by 2 divided by the norm of W
and so what you want to try and do is
maximize the margin while being able to
separate the data using the hyperplane
and then of course you can allow
violations if the data is not linearly
separable by giving the linear penalty
as you go away from the margin so once
you do that the problem becomes you're
trying to maximize the margin remember
the margin was 2 divided by the norm of
W the two norm and so I can try and
minimize the two norm of W subject to
these constraints and then if I allow
violations then I can have these slack
variables and these slack variables need
to be non-negative so the whole problem
can be formulated as a quadratic program
so and this parameter C trades off sort
of this regularization versus this
summation of the size which can be
looked upon as hinge loss so this is
what's called the linear support vector
machine and then again what what happens
if the data is not linearly separable
then what you can do for example in this
case is to map the data to if it's 2
dimensional data you map it to a higher
dimensional data by combining features
non linearly and the goal is after the
data will become linearly separable in
the right
but you still don't want to compute in
this very high dimensional space so
there's something called the kernel
trick where you can just look at the
inner products or the gram matrix
between the data in this high
dimensional space and then you can look
at the dual problem for the SVM support
vector machine so this is what we will
be concentrating on so it becomes this
quadratic objective subject to these
bounds the alphas are called the dual
variables and that's what we need to try
and find out each out they're going to
be out the number of alphas is going to
be the number of training points so if
you give me a data with 1 million
training points they're going to be 1
million values of alpha that we need to
know and then add the optimal you can
express this normal to the hyperplane as
a linear combination of a subset or of
the training data so some of them might
be alphas many of them might be zeros in
which case the W will be expressible
only over a small set of these training
days and note that you don't actually
you know you don't need to compute W
explicitly because what is finally in
here it is classification and the
classification can be done by taking
this inner product with the training
point so then you can we can express
this the classification or the
prediction by just using these kernel
functions or the kernel kernel values so
you don't never need to compute
explicitly these high dimensional
feature vectors as long as you have this
kernel function so what kind of kernels
do people use the Gaussian kernel is
probably the most widely used you of
course have the linear SVM or the linear
kernel then you can have polynomial
kernels now let's think about what
happens to big day so if I have lots and
lots of training later will I be able to
run a nonlinear SVM that solves this
quadratic program efficiently with
enough memory
so what is the memory requirement well
the memory requirement is going to be
order n square if you pre compute the
kernel matrix that you can get a get
around by computing the kernel matrix on
demand what about the time generally
it's going to take cubic in the number
of samples and that might be too
expensive so for example if we take a
data set this is a handwriting digits
data sets MLS data set 8 million samples
if you run the you know one of the
state-of-the-art methods for solving the
non linear SVM then it takes more than
three days to train on the state set
yeah and that's a lot so actually monic
was sitting with me for dinner and he
actually gave me a nice motivating story
so he apparently in his class decided to
give to these IIT Delhi students
problems in kaggle Carol is a website
where you can have upload datasets I
guess and you can submit methods and you
can see which method does well on which
data set so he submitted a data the
cover type data that I'm going to talk
about where the linear SVM does very
poorly the non linear SVM does much
better it's a data set with half a
million about half a million samples and
he said none of the students had enough
patients to run the non linear SVM
because they tried for six hours the
method did not finish so they tried
other methods so what my talk is that in
the next instance of this course that
monic can actually have these students
work on this data set and will not take
six hours it'll take much smaller and
how do we get there it'll be by using
this divide and conquer strategy so so
far what I have said is classical now
let's get into the server the new
algorithm that we've developed so here
is a motivation
okay so we'll have all these data points
we're going to try and divide them in
two different sets all clusters so we
it's equivalent to partitioning the
Alpha into these K subsets we want
through VK and then
we will do is we will solve each of
these problems independently and then we
can just think about gluing the
solutions together
really get a feasible solution obviously
because all these values of less than
equal to C and the hope is that this
will be a good approximate solution now
we'll get to why this might be a good
approximate solution and which case
it'll be a good approximate solution
well if you just carry out this thought
process then well you can see that if
each partition is of size n by K suppose
all the partitions are equal in size
then you'll reduce the competition to
compute this value this vector alpha bar
from n squared to n square divided by K
square then I will be solving care of
these problems each problem is our size
n by K if the computation is cubic then
again I will be able to reduce by a
factor of K square the computation
that's involved so that's the first
thought process that comes to your mind
if you're thinking of a divide and
conquer yeah you're you're being too
quick I have a whole talk to go through
oh yeah so it will you know I teach in
at the University so I'll get to all
these so do you want to ask me a
question or you want to hold it off you
can ask me but I don't want to
discourage you
we will solve this support vector
machine exactly we will solve that
quadratic program exactly so there is
going to be no approximation you're
moving too fast so let me once let me go
through my next few slides okay and then
you ask me these persons so this is just
producing this alpha bar and then what
we want to do is we want to solve for
the SVM so if you want to do that you
can take a software like Lib SVM you can
feed in the alpha bar as an initial
point and then you can start the base
via so then you will get the global
optimal for this quadratic program so
you will get the nonlinear support
vector machine the question is has it
bought you anything is it any use so
when would it be of use if the alpha bar
was close to the alpha star which
denotes the optimum the global optimal
of the QP right then it will be good
vise chances alpha bar was equal to
alpha star will be ecstatic but that's
not going to happen and then at every
step you're going to the alphas that are
nonzero will be the support vector
machine support vectors so if for some
some way we actually get good support
vectors identification over here that
means the alpha bars that are nonzero
over here or the handsome alpha bar
there are zero over here
also turn out to be zeros over there
that will be very good
so alpha bar will be a good solution in
this case and let's say the block
coordinate descent method or block
coordinate the same method of Liberty M
will actually converge very quickly now
the question is when can this happen
so rather than partitioning the problem
randomly it turns out that if we do some
sort of clustering to get to a partition
then it performs very so let's now go
through the process of showing that
suppose I have this alpha bar as the
solution of the kernel SVM then I can
think about using an approximate kernel
this just says that I'm going to ignore
the off diagonals
blocks so I can look at the
corresponding kernel matrix and instead
of having G which is the actual kernel
matrix I have G tell them where I have
zero doubt the off diagonal blocks so
then you can say where where is the
error coming from so I can quantify this
by a quantity called d pi which is the
sum of the off diagonal elements over
here and then what I will show us in the
next few slides that if I can drive D
bar D PI down then my alpha bar will be
close to alpha star so that's the first
theorem if I start with alpha bar this F
refers to the quadratic objective
function value right so this is you know
half of alpha transpose Q alpha minus C
transpose self so clearly F alpha star
is less than equal to F alpha bar
because alpha star is the global optimum
and then what you can show is that the
difference between these values is
bounded by 1/2 C square we're seeing is
this trade-off parameter times D PI
which is the sum of the off diagonal
blocks and furthermore because this is a
quadratic then there's sort of an
inverse problem given that the alphas
are close how close are the alpha pops
and they'll be closed modulo the
smallest singular value or the eigen
value of the current so what we want is
a partition which minimizes D party what
is the best partition that minimizes D
PI well just take one partition then
there is going to be no off diagonal
element and clearly alpha bar will be
identical to alpha star but that's not
helping me right because I want to try
and reduce the computation so I have two
goals one is I want to try and minimize
the PI and the other is I want to try
and have balanced cluster sizes for
efficient drink so what I will then do
is I can try and use kernel k-means
where because you are giving me the
kernel I will use that as a basis for
trying to partition the data and then of
course you might say well kernel k-means
itself
is going to cost quite a lot but
remember this is going to be a means
towards an end I'm not trying to get
clustering I'm not going to try and
output the clustering so typically what
we will do is we will take a subset of
the samples of size m which is much much
smaller than the number of points we
will do the clustering and then run the
clustering algorithm and then we'll just
fold all the data into these clusters so
just to give you an idea about how
effective this strategy can be here is
the plot here is this cover type data
set that Manny gave to his students
I've taken a subset 10,000 samples
because to compute this bound is not
trivial because it involves some work to
compute this power so on the x-axis re
on the Left plot over here I've given
the value the absolute value of the
difference in the objectives so this is
plotting F alpha bar minus F alpha star
this is on an absolute scale this is on
a logarithmic scale so here you see that
if you do if you look at the bound it's
very small
if you run the algorithm using kernel
k-means it's indistinguishable in the
absolute value select word if you use a
random partition it's much worse so if I
just decided that I'm going to go into
partition the data randomly and then
compute alpha bar and see how alpha bar
differs from alpha star then this
difference is huge and this is amplified
over here so when this is on a
logarithmic scale and by the way this is
eight clusters sixteen clusters 32 64
128 clusters so you can see that the
bound is actually reasonably close to
what happens the actual value that we
observe
so by doing this strategy we are able to
get alpha bars there are reasonably good
approximations to alpha stuff and then
we can actually also prove a theorem
that says that the support vectors also
get identified fairly quickly in this
process you know I you know you can't
always do it but
if one of my alpha bars is zero and if I
look at the gradient and the component
of that gradient is big enough then you
can show that when you try and compute
alpha star that's going to remain
anchored at zero the slight issue is
that these bounds are not necessarily
computable we haven't addressed that
problem too much but we can try and
approximate these values so this is an
example of the support vectors so
suppose we have this scheme now where
I'm going to partition the data going to
keep on partitioning the data and then
I'm going to compute alpha bars at the
bottom and then compute the alphas for
the parents and keep on going up the
tree so up the tree when I look at the
whole problem I'm solving the actual
kernel SVM problem
there's no approximation right so here
is you know as you go up the tree even
with 256 partitions at the leaf level
85% of the support vectors are correctly
identified so as you go for example with
four clusters then almost 100% of the
support vectors are identified and
that's why the method ends up being
faster and this is in comparison to
another method which is cascade SVM
which does partition the data but
randomly and then it has a different
strategy so it actually does not compute
the actual support vector solution so
what we want to do is you know we have a
trade-off we have this parameter K which
is the number of partitions if I choose
a small K then I'm going to have a
smaller approximation error but it will
take more time to solve the subproblems
if I have a larger value of K then I'll
have a worse solution it's going to be
quick so there's a trade-off so what we
do is instead of just doing one
partition we do a hierarchical partition
so what we will do is we will do a
clustering over here then we look at
this cluster and sub cluster over here
we'll do this data division and then we
will solve the leaf problems the alpha
bars
these will be the leaf problems we'll be
able to solve for the alpha bar for the
the Alpha values for the parent and then
go up the tree and finally we'll solve
the original problem so how well does
this compare with other methods so we'll
get to that the interesting thing is we
can actually try and do early prediction
so what do I mean by already prediction
maybe stop over here so that's one
thought process is you have these alpha
bars over here alpha bars here alpha
bars here alpha bars here I can go ahead
and solve the whole problem but I could
think about using those alpha bars
themselves so that's what we will call
early termination so let's that's not
computing the exact astrium solution but
let's see is obviously going to be a
little bit faster quite a bit faster it
turns out and then let's see what the
accuracies are going to be in the end
task of classification so remember the
goal is not it's really to solve this
quadratic program exactly the goal is to
get a good solution to the
classification rule okay so how would
you then do the prediction you have if
you do early tradition prediction or
early termination you will have faster
training time the prediction accuracy is
going to be it turns out is actually
close to or sometimes even better than
the global SVM solution how will you do
the prediction it turns out that the
best thing to do is to use the block
diagonal corner you can use the overall
kernel it turns out that this gives
worst accuracy is then over here the
nice thing is that if I do my early
termination my potential number of
support vectors is going to be reduced
by a factor of K as a result the test
time will also be faster than having to
use all the support potentially more
support vectors so it turns out that
this tool is very useful in practice so
we have sort of extensive comparison so
this paper is on archive and it just got
accepted to ICML in Beijing so you can
actually look at the results in archive
if you wanted to so I'll call our
the divide-and-conquer support vector
machine method SVM DC SVM and then I
will denote my early termination DC SVM
early so DC SVM the solution is going to
be exactly the same as the Lib SVM
solution modulo some epsilon right and
then there are a bunch of other methods
so let's look at the results so this is
web three data sets 280,000 about half a
million 8 billion this is the MLS data
set right so if you look at over here
you know the DC SVM the divided Congress
um takes 71 thousand seconds as opposed
to three hundred thousand seconds so
it's faster by a factor of what four
four and a half and the accuracy is very
much comparable to Libous via these are
numbers on a sequential machine we
actually have not programmed it on a
parallel guchi so so one could get
further speed ups because there is more
opportunity for parallelism over here
but these are sequential levels and the
time but then look at DC SVM early so if
I do early termination you look at the
accuracies very close to the SVM very
close to the overall SVM solution very
close to the overall SVM solution and
the amount of time is one seventh of
this time so it's about 30 times faster
than the Lib SVM solution so if it took
three days but for the best VM how much
time will it take one thirtieth of three
days so just over an hour right so you
know you can instead of having to sleep
overnight three days and come back and
get the results you can go I have lunch
and you can get the results when you
come back then in terms of what you can
also do is you can look at the objective
function value as you go
the tree so this is DC SVM and you can
see that the this is you know different
levels of the tree so this is the
finally globally optimal solution and it
takes much time it converges much faster
than having to do Lib SVM because you
end up getting a much much better
starting point and that's when you start
working on the overall problem you work
on these smaller problems you do
optimization there you end up getting
better solutions for the whole problem
as a result the global that block
coordinate solves that one has to do on
the whole problem the time is it
converges much faster and then over here
over here is actually quite interesting
over here look at this plot so as you
this is M nest data as you increase sort
of the number of levels the you know the
objective function value decreases only
at the last step but if you look at the
prediction accuracy it goes from being
maybe not so good at the leaf to quite
good as you get closer to the root and
this is a phenomenon that we have
observed over and over again and this I
say liz is actually a very interesting
table right so this you know in for
example if you use a Gaussian kernel SVM
you have two parameters you have the
gamma which is the width of the kernel
and yet we have this parameter C which
is the trade-off right so the question
is for different values of C and gamma
how do the methods behave so that's what
we've shown here for one data set in the
paper all the many different data sets
are shown so we very see in a grid we
vary gamma in a grid and we have now
results tabulated results for DC SVM
early the divide-and-conquer SVM and
compared it to Liberty because this
computes exactly the same solution as
libous via the accuracy numbers are
pretty much identical okay the time is
faster than Libous via the time here for
DC SVM with the bold are the best
numbers it's always faster but look at
their accuracy numbers so here the best
performance here is about 19
percent okay DC SVM is also pretty close
what's the worst performance over here
oh it goes to 61% when C and gamma are
not good so you might say well you'll
try different fees and gammas what
happens over here
well the performance is actually not
that bad so that's an including
intriguing part of this because what's
happening is that we are building these
different models at different scales of
the data and in some sense we might be
smoothing out some noise which is
present in the dataset or in the model
so this is actually an interesting so
you know you might ask you the question
like why ever use DCs fear why not
always use DC SVM only and that's a
candidate I mean it's it's a plausible
thing to do except that you know I
cannot give you a guarantee that DC SVM
early computes the maximum margin hyper
it's probably within my circle I mean
there is an epsilon right there's a
termination criterion and we may have
used slightly different termination
criteria in the different methods some
cases so for example here 86 percent
here and 61 percent that's a big
difference so that's you know not for
the best values of C and gamma but it is
the difference is big in terms of the
accuracy so I don't have a you know
bullet proof answer as like I can't
prove a theorem for you but what my
feeling is that you know there may be
some these values of C and gamma are
obviously not good that that's actually
also in the but I don't know if over
here he may have turned off the bias
term also I'm not sure but you know it's
your student who you know it's actually
his his master student was now my PhD
student so I don't know what he did
because he modified the Libya SPM code
to turn it into DC yes ma'am
so I I'll have a check with him he's
leaving for Taiwan soon so you might be
able to check with some quicker than me
yeah I I haven't thought too much about
it but maybe we can think on the fly
right so gamma is small over here so it
looks like it's happening for a small
value of gamma right when gamma is very
small then it tends yeah
gamma no a drama is in the numerator e
to the power minus gamma
oh these are accuracy numbers on the
test set yeah I mean that standard
machine learning that we trained on the
training set and then we are doing
testing on the test set and I don't know
this might be you know he might have I
forget whether this is done with
crossfire not cross-validation but on
you know different splits of the
training and test data yeah question
memory use so so the memory used was you
know if you wanted to store the entire
kernel matrix compute the kernel matrix
and use it
what Lib SVM does and what we do is
essentially compute it on demand right
so memory is not that much of an issue
although you know for example when we
have a small problem
we can actually do the computation once
and for all and then reuse it again I
don't know the specifics of the code by
the way there there is actually a
webpage for this paper and the codes
also available so you guys are actually
welcome to try out the code this is yeah
because there is we don't compute the
entire kernel matrix and try to store it
the data set is in memory oh I don't
think I forgetting all the
dimensionality in just these cases I
don't think the dimensionality is that
big then you cannot hold the data
centers what's that yeah yeah so the
data is but that's what would happen
right so for example if you cannot hold
the data in memory then you are you know
you have no option but then to try and
distributed memory solution but what I'm
saying is that this actually gives you
will give you a better algorithm which
it'll perform better on distributed
memory was there another question there
yeah
this the world right now I'm just
talking about this svm problem so
whatever we've shown is for the SVM
problem which is even more which is not
just convex but it's a specific bound
constraint quadratic optimization
problem your I can force you where
you're going where you can try doing
this on some different objective
function right and there I'll come to
that because the sparse inverse
covariance selection problem has a very
very different objective function again
that's convex and over there we end up
also using a divide and conquer
algorithm except the division strategy
is also then tailored to that problem so
if you see over here I did the division
was using the actual kernel so it was
really tied to the support vector
machine in the kernel Astrium problem so
that's what I will conclude by that
suppose you want to try and you know
generalize this class of methods to
different problems then you know there
should be something that you can do we
don't drop in it the the that gradient
condition is not easy to check so this
condition we actually never compute d pi
so that would mean that you have to
compute the entire kernel matrix I guess
we could compute it for small problems
right we could compute it towards the
bottom of the tree and then we could
check it and then we could discard some
values that's actually a good point your
way he's had his head in place for one
so from this plot you know it's 4 and
then 16 64 256 so four partitions in
each case I I don't know this one I
don't remember I think we used three or
three levels and I think it's K equal to
4 but we can go back to the paper
for yeah and these are all questions you
know in some sense why for at every
stage can you try and do model selection
where you figure out the number of
clusters there are all these parameters
that could be tuned in the code yeah
that's that's what I believe this
cascade SVM tries to do without doing
this SVM partitioning and that without
doing the kernel clustering at the top
so it does random partitioning my
chunking
junkets I know exactly yeah
random selection and I believe they
don't actually also solve the globalist
view here you know you run the DC SVM
you get the actual SVM solution but you
have the you have the option and like
you know you can start the program and
you can actually terminate early you'll
actually get some results early on in
the process I don't think there is all
the whole thing we can look at the paper
again but we can look at that so we do I
I will say that we actually do refine
the clustering so I didn't talk about a
little section but we do refine the
clustering as you go up the tree and we
use the support vectors over there so it
turns out that for the support leg you
can actually figure out the support
vectors and then confine the clustering
to some of the support vectors
you mean because they only are
propagating the support vectors yeah
that's something we have not checked I
mean for us it's going hand-in-hand now
you're sort of saying is how can we
maybe even make this algorithm more
efficient was there another question a
couple of questions there yeah so we
basically have alpha var the optimal
solutions for the subproblems we then
use that we merge basically take the use
the optimal solution for both of them
and use that as an initial point for
lip-lip SVM like so so it will basically
do coordinate ascent using but starting
from that initial point so we are
solving the optimization problem was
there a question there or okay yeah
No
so we have fast food they just they just
give wave I mean if you look at the
quality of the solutions I mean I know
that they have you know there is theory
and so on the approximation actually is
quite poor so you know when you try and
plot things like where was my plot so
this plot is very convincing right so I
mean you may argue that this bound might
be bad the one nice thing is that you
have to realize that this is an
exponentially declining curve this is a
Gaussian curve right and so it turns out
that the bound is actually pretty good
and you can look at the F alpha values
right I mean this is 10 to the power
minus 10 difference between alpha bar
and alpha star it's pretty good they're
constants are much bigger
yeah but we want something fast sure
yeah we can do that I mean lots of so
you know you're getting ideas for you
know future papers but I'm not gonna do
everything the care you mean doing some
sort of model selection and finding K or
yeah you can try it I mean I just think
that that I mean I put a lot and
clustering myself I just think that's a
tough problem you can have a lot of
theory behind it but the results that
you get in practice or not always very
good yeah I mean I did not want to enter
because it's supposed to be interactive
and you know the fact that you have so
many questions is good that means okay
okay that's fine
so these are there are more results okay
so this actually shows that in some
sense by doing this early termination we
get actually something which is more
robust than just a global SVM solution
right so I have ten minutes okay and I
kind of anticipated this that's why I
put the more recent work I mean I gave a
version of this talk in Microsoft
Research in May and at that time we had
not done that much work on the SVM
problem okay so today I concentrated
mainly on the SVM problem now what I
wanted to just tell conveyed to you is
well there was this divide and conquer
approach for this particular kernel
Astrium problems I will tell you the
divide-and-conquer basically works in
two other settings where two other
concrete settings one is social network
analysis so I'll go over this pretty
quickly this is you know nodes represent
individual people edges represent
relationships of friendships
this is the classic karate club network
that's studied by sociologists what do
you want to do in network analysis well
you want to try and get proxy name
proximity measures like like if you take
two people in this audience you know
they may not be friends for
on Facebook but they may be linked with
other people who are friends they may be
a part of them too in the network in
that sense they're closer than if I take
a random person off the street and
myself right so you want to try and get
proximity measures and the applications
are you might want to try and do link
prediction which is given snapshots of
the network so this is a cartoon where
you know you have five people in the
network Alison Eve first become friends
then Bob and Dave then Alison Bob and
the question is if you know the state of
the network till this point what can you
predict might be the state of the
network in the next time instantly
so one way to do it is look at what's
called friends of friends that means if
the two of if there are two people they
have a lot of friends in common then
they are more likely to become friends
with each other
and that corresponds to a path of length
two in the network and then you can say
well what about parts of length three
what's the paths of length four and it
turns out that there is there was a
psychologist called Katz who looked at
this and proposed this measure called
the cat's measure if the cat's measure
essentially computes what's called the
resolvement of a matrix so if I have an
adjacency matrix the cats measure
computes the inverse of I minus beta a
beta is a particular number which is
small enough if it is small enough then
this is true that that you look at paths
of length two you look at paths of
length three you exponentially damp them
by this factor beta and then you can try
and do friendship prediction you look at
F of a this matrix valued function you
look at the large values in this matrix
valued function and those are the people
who are more likely to become friends
but if my matrix is very large let's say
million then there is no way that I can
actually compute this so then the
question is what can you do you can try
and do spectral approximations of the
adjacency matrix and that works because
once you compute let's say the
eigenvectors then this function let's
say over here the power or here the
in verse just needs to be applied to an
inner much smaller matrix even this
becomes quite expensive if the
dimensionality is very big so what we
did was we said well there is a
community structure in the network right
and within each community there's sub
communities so what we are going to do
is we are again going to find
communities so this is the the analog of
the kernel k-means we were doing in the
kernel SVM problem we'll find sub
communities within communities and then
we'll get a similar tree like structure
then we will compute some approximations
at the bottom and similar to the SVM
problem we'll use that as initialization
to finding the matrix approximation at
the top level at the parent level and
keep on going
as a result let me just show this then
you have so this might be the spectral
approximation at the top level but then
you get matrix approximations at the
second level and the third level each of
this actually gives you a prediction so
what we end up doing in this case is not
just using this prediction but we can
actually combine all these predictions
that's like looking at data in the
multiple scales we've tried using this
in the kernel SVM we haven't seen that
much advantage there we just used
whatever top level we get but here we
saw an advantage in doing this so just
in summary so this is an example we've
done sort of multi scale link prediction
using the cats measure here it's just
some results so this is a Flickr data
set with about two million users 40
million link so plebe it again just
running on one machine and by using this
multi scale predictor we are actually
able to almost double the precision for
example here is top K so hundred so it's
thirteen point seven or so thirteen
point three about 13 out of hundred
predictions are correct they do become
friends in the future and if you
compared to the you know norm you know
the without this combining these divide
conquer method and combining these pair
ters it's seven so we're actually able
to get a much again a more a better
predictor by combining these predictors
at different scales so this is an even
more complicated problem but I have even
less time to talk about it so I have
about five minutes right so I'll try and
go through this quickly so this is an
example where now you have a very
different objective function than in the
SVM problem so finding the inverse oh so
you've probably heard about graphical
models so if my data comes from the
multivariate Gaussian distribution so I
have I see samples y1 through yn from a
normal distribution with mean mu and
covariance matrix Sigma the goal is to
try you only see the observations but
what you need to estimate is either the
covariance matrix or the inverse
covariance matrix and what you can show
is that the inverse covariance matrix
captures the graph structure so it's
easy to show that it for example there
is no link between there is no edge but
in theta inverse if you view theta
inverse sorry theta as a graph then if
there is no edge between North I and nor
J then do these two variables are
conditionally independent given the
other variables so given limited number
of samples but in very high dimensions
so for example in fMRI data you can
actually have about 300,000 voxels and
the number of fMRI is might not be that
many okay so the number of samples is
not so what people have proposed is to
use an estimator which has this form
this is the maximum likelihood estimator
regularized by this l1 regularize so
this is a case where you have a convex
objective which has a smooth part and a
non smooth part the smooth part looks
very intimidating it's log determinant
of some matrix with a linear term and
then this part is non differentiable
so what we did I've been working on this
problem for about three years
what we did was developed a quadratic
approximation method the that does a
Newton type of algorithm on this
objective function and then we were
wanted to scale it to bigger data sets
so the question is how do you scale it
to bigger data sets and then we said
well how about trying to use divide and
conquer methodology or and there was a
slight precedent because there were
these statisticians from Stanford who
had made the following observation so
I'm going pretty quickly over here okay
so this matrix S is the sample
covariance matrix that's easily
computable right but in high dimensions
when you don't have that many samples
the sample covariance matrix will be
singular so you can't just invert it and
get a good estimate for theta that's why
you use this kind of objective function
and what people have shown is that this
estimator has good statistical
properties so I'm not talking about the
statistical properties in this talk but
I'm just talking about solving for a
positive definite matrix X such that
this is minimized what the Stanford
students to the station's observed is
that I can look at what's called this
thresholded covariance matrix remember
lambda is the regularization parameter
sorry the larger lambda is the sparser
my answer so if I make lambda large and
larger and larger it will be sparse so
what they observed is that you can take
the thresholded covariance matrix so
lambda is a positive number
you take the thresholded covariance
matrix so the larger lambda is the more
zeros there are going to be over here so
what they said is that if the
thresholded covariance matrix has
multiple connected components that means
the problems are independent of each
other and the final answer you can get
by solving these independent subproblems
so
that's a very simple algorithm you
basically take your thresholded
covariance matrix think of it as a graph
do connect find the number of connected
components solve the problem
independently on each connected report
if lambda is large enough this can lead
to a substantial reduction in time but
in practice what happens is that this
this sample covariance matrix and even
the answer is block diagonal and then
there will be occasionally some
relationships over here and I was
actually collaborating with somebody who
was working on climate modeling and what
he was saying is that these in that
application the far away connections are
the more interesting connections so the
one connections over here would be the
more interesting connections and if
that's the case so even if there are you
know one element each in each of the off
diagonal elements over here off diagonal
blocks then this will just have one
connected component
and there will be no savings so that's
manifest over here so if you take this
gee lasso code coming from the
Stanford's to discussions you decrease
lambda you know it's quite fast when
there are quite a few connected
components but as you decrease lambda
you get only one connected component and
then the algorithm has to work on the
whole problem so what we did was we
realized that well you can actually do a
divide and conquer approach over here
okay ghin we do it trusting of the nodes
solve the problem the subproblems we use
the subproblem solutions to again seed
the solution for the parent and we go up
the tree and as a result this algorithm
is called DC quick okay quick was
quadratic approximation of inverse
covariance matrix matrices that was the
method that we developed in this nips
paper two years ago then so and then you
can see that this divide and conquer
method is much faster is faster and it
doesn't suffer even when you in the
connected components in the
covariance matrix becomes just one
algorithm Egret gracefully it does not
have this big spike how do we do the
clustering again the clustering is in
some sense customized we look at the
objective function and we then sort of
give a reason what the what kind of
clustering we must have and then we do a
clustering in this case it was the
thresholded covariance matrix and then
we ended up using some sort of graph
partitioning software so here is the
sort of answers that we get so suppose
this is the final answer right this is
what we want to get to one option as I
solve this problem in you know the whole
problem right from the beginning what we
end up doing is we partition the problem
into subproblems we will solve this this
these this independently of it then we
will combine the solutions and you can
see that we've gotten some more non
zeros over here and then when we combine
these two we'll get this and if we have
a good trust ring ideally you want a
clustering of theta star we don't have
that okay so we use the thresholded
covariance matrix as a proxy but you
will see that you know in the end you
actually get much sparser values okay or
the the number of edges is much much
less in these off diagonal blocks then
over here okay so I think I am running
out of time I'll just sort of this
method is faster than other methods so
this is the convergence plot this is
leukaemia data set a climate data set
and this is the divide-and-conquer
method in fact what we've done more
recently is so this paper appeared in a
year ago at nips and just in the lips
that was held what a couple of a week or
so ago we have a method called big and
quick which actually builds on this
divide and conquer approach and can
scale up to a million variables provided
that the answer is false so in
conclusion you know what I've tried to
do is
three concrete cases where just
divide-and-conquer algorithm calls that
should naturally arouse curiosity what
other problems in large-scale data
analysis can benefit from a divided
approach and there probably many such
applications regression if I want to do
matrix factorization which is what we
she will talk about for collaborative
filtering or recommendation multi-label
learning that's an interesting problem
in industry then what are the
corresponding algorithms we have to do
the division but in each case I tailored
the division according to the problem I
was looking at do we need to do it on a
case-by-case basis or can we developed
some sort of generalized kind of method
to do the division and do the concrete
the third part is actually also very
interesting right in the case of social
network analysis we got a combination of
local and global models and there we
were able to combine them and the result
was quite good what about other methods
because we are getting these local for
example in kernel SVM we are getting
these local classifiers also these local
kernel support vector machines can we
combine them so as we get better
accuracy can we build some sort of
statistical theory that says well that
those kinds of methods will be more
robust and I know that this is a
workshop in distributed corporate of
computation so you know this is an
algorithm that's really ideally suited
for distributed computation so if you
want to do parallelization well we need
why by doing this division we will
probably need some dynamic load
balancing because the work will be
different so in order to not be able to
be stuck in this sort of bulk
synchronous model can we do some
asynchronous and that's that sort of
segues nicely into wishys next talk
where he will talk about a method to do
asynchronous parallelism on very big so
that's all that I have to say each of
these has a
there's a paper associated with each of
these three methods that I talked about
and that's all thank you yeah question
you mean accuracy oh yeah yeah if you
have a time on your hand if you would
rather wait for three days for a result
then yes you should stick with liberty
right I mean you're getting the same
solution but faster right usually faster
is better exactly yeah so that I think
there could be variations right so
that's why I think that when one does a
parallel implementation of this it's not
going to be straightforward but there is
going to be some sort of dynamic
load-balancing to be done and if you
really notice if I go back to a lot of
time is still spent in the top level so
if I was not to do early termination if
you look at this there's a lot of time
spent in the top level okay so so you
can't just say that I've divided the
problem and there's no time that's
really needed in modeling distributions
so over there one would have to use
techniques like the one we've developed
with Bishop that Vichy is going to talk
about so so they're really both kinds of
parallelism one is sort of independent
jobs and then at the same time you're
working on the same job and you still
need to develop some good methods to
paralyze that point again it will depend
on your particular problem right I'm in
some cases maybe for kube might be good
in other cases you may need to go deeper
into the hierarchy</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>