<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Parallel Thinking | Coder Coacher - Coaching Coders</title><meta content="Parallel Thinking - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Parallel Thinking</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZDIHcrpTC74" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
so our next session will focus mostly on
the sort of educational aspects of the
center and putting education into
computational thinking some of which I
described earlier and so we'll start
with guide Wallach okay so i'll talk
about parallel thinking and this is
obviously motivated by the notion of
computational thinking from the name and
this first arose in fact this was came
out of the early days of the
computational thinking center was one of
the original projects so we all know
that parallelism is basically you can't
buy a processor nowadays which is in
parallel it doesn't have multiple cores
and of course there's lots of
parallelism even within the course you
know you can go to amazon you can buy a
16-core chip you can buy you know eight
and 12 cool ones you can also go to the
dell site and they'll put forth these
chips together and in a little slide in
rack mounted server like this so you get
I guess that's sick four times 16 is 64
cause you can buy a you know GPU all
sorts of flavors of GPUs and these have
you know up to a thousand in fact the
new ones even have more than a thousand
cause on them interestingly of course
your cell phones nowadays new cell phone
samsung is advertising you know a fork
or one and this wasn't something over
about a year ago and immediately Intel
put out a a news announcement that says
forget fork or 48 core smart phones in
fact it looks from the date it looks
like it was a day before but that was
just close it so anyway so we're going
to get 48 core cell phones or if you're
hobbyist you can buy one of these this
is sort of the follow up on the
Raspberry Pi this is one of these things
you buy four hundred dollars and you can
play around with and they have four
cores so pretty much everything has
multiple core
so we want to make use of this and the
idea of a parallel thinking is we want
to do this in a way where we're sort of
raising the level of abstraction and
this comes from a realization that if
you look at most the way that parallel
computing is being taught out the air
it's very low level it's very much
focused on a particular programming
language particular machine or something
and so the idea is in the same way that
Jeanette wanted to say what are the core
ideas of computation you know we wanted
to look at what are the core ideas of
parallelism and the premise here is that
first of all if you start raising the
level of abstraction a lot of things
apparel anyway so parallelism is pretty
easy it's not a complicated topic
furthermore we've sort of been
brainwashed into thinking sequentially
so we really have to step back in order
to figure out what this parallelism is
and and glassy here they aren't that
many ideas in parallel that we can sort
of extract a set of core ideas and that
you know so what are these core ideas
and ultimately the goal of course is to
put this into education into a
curriculum just a side note here there's
lots of different ideas of what
parallelism means I'm going to be
focusing on a somewhat limited form here
is when we have things running
concurrently it could either be because
your environment is naturally concurrent
in which case this has been true in
operating systems for many years we've
had servers running and I know a web
server could be actually run on a single
processor and you had to have all sorts
of incoming demands and so there's
concurrency coming from the environment
so I call that sort of a concurrency
versus having just one problem you're
trying to solve I know you're trying to
render a some image and that's just a
single problem with China sold so that's
along this axis between sequential and
concurrent and the other axis is if
you're running it on one core or
multiple cause one process of multiple
processes so what I'm going to be
talking more about today is
multiple processes but running what sort
of a deterministic parallel things so
not I'm not really talking about you
know in interactive environments I'm
talking about you've got one job to do
and you want to do it as fast as
possible and so that's what i mean by
parallelism and at the end i'll talk in
terms of that in future sort of work and
what we could do in this space here okay
so what is parallel thinking again it's
the idea of what are the abstract ideas
and parallelism so in particular it's
not particular programming languages so
it's not about teaching CUDA openmp you
know Hadoop etc okay it's it's not about
particular machines so it's not about a
particular particular GPU processor it's
not about a particular I don't know
multi cool or something like that okay
it's not about distinctions such as data
parallel versus task parallelism it's
not about em dolls law and I'll say
something about this at those Lewis to
fuzzy of an idea it's it's not about
libraries versus languages versus or
automatic parallelization I claim that
once you've figured out that your
algorithm or your computation is
parallel this is really just an
implementation idea whether you use a
library or language or a you know you
rely on some automatic parallelization
it's not about the main specific
languages and it's not about taking a
sequential code and trying to squeeze it
into some sort of parallel toolbox okay
what it is about is understanding
dependencies and i'm going to give be
more concrete this as being a little
vague here is understanding the
dependencies and your computation and
sort of breaking away from this idea of
doing one thing at a time and really
understanding what depends on what in a
comfy computation and that's where the
parallelism can come it's talking about
potential parallelism as opposed to
actually how it runs on real processes
so you can expose the
parallelism and then the scheduler
presumably would map out on two
processes it's understanding
computations in terms of the total work
they do which is sort of what the
sequential time would be and the span
which is once you think about in terms
of dependencies this is the critical
path of dependencies other key ideas of
work efficiency it's understanding the
sort of key algorithmic techniques to
get parallelism and it's also an if this
is all in the context of dynamic
parallelism we're generating parallels
we go so my favorite example and if
you've seen me give a talk before you've
probably seen this example before but I
I think it's very telling in terms of
abstraction because here's a piece of
code from a book that was published in
1974 ok it's quicksort they had no
motivation to talk about parallelism
this book is not about parallels it's
about the design analysis of computer
algorithm sequentially but if they had
the force for it to write it in a very
high level so they write this code at
high level and it's written at a high
enough level that it really is a piece
of parallel code ok oh it's a parallel
algorithm and that you know so there's
some notion that there's some pureness
about the parallelism here because it's
not you know we're trying to map this on
to some machine or something and so why
is this parallel well it's parallel
because obviously we can make the two
calls to quicksort and parallel and it
doesn't even say in the code that we do
want and then we do not so they do the
other so they even in some sense a
fourth thought of saying sort of it just
saying that this is how you put things
together rather than the execution order
so you put the results of the quick sort
of the smaller and the great elements
with a equal elements in the middle but
the other thing here is you can also
you've got this pivot and you have to
compare it to all the rest of the keys
you can also do this in parallel and
there's nothing in here that says we've
got some loop that goes over the keys
and compares them to the pivot and so
this itself can be done in parallel we
can do this giant comparison of all the
keys to the pivot and then subselect the
ones that which is smaller larger of
course this doesn't tell us much about
this is vaguely parallel but there is a
very nice way to sort of then make
concrete exactly how parallel this
algorithm is and what you can do is that
go back to the dependencies so if I this
is sort of a dependence graph along here
so we can do these two things in
parallel so they depend on first
splitting based on the first pivot and
then you know we have to split based on
the pivot and here to generate the next
two recursive calls and then these
blocks are supposed to show that we can
actually compare the pivot to the keys
in parallel and then somehow do the
splitting and turns out you can do that
in sort of a log independence and I'm
not going to go into detail on that but
at the end of the day you can show with
it an expectation or even with high
probability if you pick a random pivot
that the overall path longest path the
dependencies is something like logs with
it ok so you get a nice theoretical
bound on sort of the critical path of
this algorithm you can show the work is
n log N in fact the work is identical to
the sequential algorithm in a sense it
is the same as the sequential algorithm
we've just raised the abstraction to
ignore the false dependencies and then
you we can define at this notion of
parallelism which is basically the work
of the span which gives you a sense of
how many processes about you can make
use of ok so if this is the sort of the
level abstraction that we're talking
about when we're saying parallel
thinking it's not tied to a particular
programming language it's you know it's
written in suit in fact i'm using here a
piece of pseudo code that was written
not even in the context of parallel
programming and it's you know using high
level notions of dependencies etc ok so
there's we sort of spent some time
thinking about well what are these
high-level concepts in parallel
programming there's a list here there's
you know dealing with collections is an
important idea where you instead of
thinking about you know mapping
loop's it go over every element we think
of operations across a whole collection
so mapping a function across a
collection you know reducing means they
like for example summing the elements of
a collection there's a scan operation
its various operations that you do in
collections there's sequences dealing
with sequences ordered sets and tables a
lot more so it sort of gives you much
more of a Python or MATLAB flavor of
programming right where you're dealing
with whole sets or collections as
opposed to a job or sea flavor
programming where you're thinking of
writing loops over things and so it this
peril thinking almost goes in the same
direction as other so the changing is
we're making which is going more moving
towards languages such as Python etc
there's this notion of nessa parallelism
work span there's a lot of sort of
algorithmic techniques which turned out
to be very useful in parallelism which
also useful in sequential computing some
of them so divide and conquer of course
we know this the thing is that it's even
more useful in parallel computation than
in sequential because it has the
additional advantage of being able to
use of divided calls in parallel this
idea of contraction which is something
that doesn't show up so much in
sequential computing which is you take
some collection of values you can track
to do something small and then contract
it it's like divide and conquer but you
only making one recursive call there's a
lot of use of trees dynamic programming
anyway we've gone through this and sort
of figured out what the core idea is
impanel computing and we put this
together into a course which we now
teach which is this 15 to 10 okay we
teach some of these concepts actually in
the earlier course in 15 150 so if you
remember Randy's picture there was this
15 120 to 150 that led up to a data
structures course which was this 15 to
10 okay and we've so we've done that in
terms of teaching and we've actually got
White's a complete set of notes and
we're starting to actually write a
textbook and this there's also a
research component of the parallel
thinking which I'm really not going to
talk much about today except on this
last topic hit okay so 15 a 210 so this
is we've now been teaching it for three
years so this is to give you a sense of
how it's going it's their tour to second
semester freshman and first semester
sophomores we now have 4 15 to 10 we
have over 250 students a year in fact i
think if we take this pass a mess in the
next semester it's gonna be over 300
students we only have 140 majors okay so
this is a course originally designed for
majors and more than only half and we
get more than twice as many students as
majors so students seem to like the
course or they're taking it it's
reasonably popular I think this year
they it's it's going quite well that's
maybe desire much teaching it instead of
me and we found that the students really
don't have a difficulty with the
comparable concepts okay they what they
have difficulty with is what they've
always had difficulty it's the data
structures and algorithms course so we
have we teach recurrences they have to
solve recurrences we teach algorithmic
ideas and they have the same difficulty
with quicksort they've always had
they've you know in merge sort but the
parallelism really doesn't seem to add
much additional overhead in terms of the
complexity of the course we teach this
in a functional style of programming and
the motivation here is we don't want to
have to deal with a whole issue of race
conditions so if you call two things in
parallel and one thing does a side
effect and the other thing reads it you
have non determinism it's very hard to
deal with so we teach it in a way where
you're not not allowed to do side
effects and again this actually turned
out to work we were quite worried about
this because it's a non-standard way to
teach the data structures and now
scores but we found out it's not such an
issue most the same concepts we've teach
toward in an imperative style work quite
well in a functional style there is
significant programming assignments in
fact I'd say eighty percent of their
time is on programming assignments as
opposed to theoretical assignments so
it's not just a theory course and we've
made extensive course notes I think now
there's something like 250 pages of
course notes is there's obviously not a
textbook on these top on this topic so
what they get are these course notes
they get the lectures and and that's
what they they learn from and here's a
you know a detailed list of what's
covered I'm not going to go through it
but the key thing here is that many of
the ideas of the same sorts of things
you would learn in a sequential
algorithms course you know we do BFS we
do Dijkstra's we do min spanning tree so
you still learn everything they would
have learned in the old data structures
and algorithms but they learn it all
from the point of view of parallelism so
when we do breadth-first search we
immediately say well you can do each
level the breadth first search in
parallel right we it's not restricted to
you know doing it on a queue of one at a
time and this is just the idea of
raising the level of abstraction right
if you really think about what's the
abstract notion of retro searched it's
sort of doing one level of the graph
from the source as a time it's not
putting into a queue and pulling it out
of the queue that's a much lower level
abstraction some of these algorithms are
purely sequential so Dykstra's is just
one where the work equals the span so
the critical path is as much as a work
and so there's no parallelism
bellman-ford has lots of parallelism in
there's a when we get to min spanning
tree we teach a not the standard well it
turns out the peruca done in the right
way is a parallel algorithm so we have
to teach the parallel version of that so
basically wherever something's parallel
we teach the parallel version and for
most of these things they are highly
parallel for hat hashing is highly
parallel if you if you do it right you
can
certainly look up the hash and you can
do the insertion if you're a little
careful so anyway so that's these are
the topics and we just do them with
dealing with parallelism from the
beginning and again we're not focusing
on how to implement this in CUDA or in
OpenMP but rather at a high level we
have a operation on a sequence it says
insert a bunch of things into the
sequence at that given index and it's
defined such that the the first one wins
so for a bunch of things hatch the same
location one of them will win and you'll
then the one that didn't win might have
to try again to find if you for example
with linear probing to find the next
that's right we have atomic abstractions
to deal with that that's a good question
okay so the last thing I wanted to talk
about is so this is all nice in theory i
should point out that the actual code
the students run run sequentially so
they write the programs and they in a
parallel style with parallel constructs
but we don't have a good we do this in
the language ml we don't have a parallel
compiler but what we have done is we've
gone and we said well let's say we did
these ideas and wrote them in and c++
now we don't want to have the students
do them in c++ because there's all sorts
of issues of race conditions like i said
but it's just to get a sense of well
what sort of performance we could get
getting so we did this experiment where
we basically generated a bunch of what
we thought were quite wide variety of
benchmarks of the sort that you were
teaching algorithms course and also ones
which go beyond simple algorithms so we
have a computational geometry we have
Ray triangle intersections you know
we've got the standard graph type
algorithms operations and sequences
things for machine learning text
processing
ambadi calculations etc so what we did
it we took these and we coded them all
up in C++ but using a style of
programming that we teaching the class
you know the sort of nested parallels at
a high level the particular language we
use is actually a silk which is a
extension to C++ that's now part of the
standard good new release G+ bus release
so you've got these things and what
these show is on a 32 cool machine is
one of these bars is the relative speed
up to the set through the parallel
algorithm one process of verses 32
processes so that's the high of blue bar
here okay the lower blue bar is the
relative speed up of the parallel on 32
cause versus the best sequential code we
could get so this is the best minimum
spanning forest code we could find the
best min spanning tree the backs of
maximum dependent set and this sort of
gives you the sense that you get
reasonable you don't get perfect speed
up except on sorting but you get you
know an average of something like you
know 12 to 16 volt speed up okay so you
really can get good performance using
the style of programming and no need
that but you know you get almost in fact
everyone at case is that one that you
get at least a 10 fold this is like 9.5
or something to sort of argues that well
you really if you've got 32 cores and
you're running on one core you're really
wasting that machine you can get a full
order of magnitude by you know using a
parallel algorithm on everyone at these
and in some cases you get you know one
and a half what is the magnitude anyway
so that's we just wanted to sort of
convince ourselves that were teaching
them something useful and I wanted to
talk a little bit so as part of this
this was I won't in some sense thank
Microsoft because
it was because of the computational
scent of thinking that we even went down
this line of thinking about parallel a
parallel thinking which led to the
course so I don't think this would have
even happened without the original
computational thinking center and so I
think in that sense it was very useful
there's also lots of specific
interactions that we've had here with
Microsoft of the years in terms of
parallel thinking I recently there was a
nice big data analytics conference at
MSR Cambridge I gave a talk on there
there's been a nice there was a nice
workshop that duty Judith Bishop
organized with Dawn's I'm at pop wall
I've been good some faculty summits gave
a talk at Redmond we've been doing a lot
of talking about possibly using F sharp
in this course with a bishop and I think
what we'd like to do is have a version
like I say the course is supposed to be
language agnostics so in fact our course
notes don't use a particular language it
uses a pseudo code notation although the
pseudocode resembles ml which also you
know it's F sharp as a variant of ml so
but their homework assignments do use a
language and so right now they're in a
standard ml but we've been looking at
possibly doing a version of them in F
sharp as well as a version of them in
scouts kala in terms of future I think
like I said at the beginning this course
focuses on parallelism and not
concurrency so we ignore any issues of
race condition we're not we don't
discuss the notion of linearize ability
serializability of mutual exclusion
these are all key ideas and currently if
you look at our curriculum and most
curriculums these ideas are scattered a
bunch of a bunch of different courses
they're a little bit in our curriculum
their little bit in our 213 which is an
introduction to hardware systems some of
them in operating systems course summer
in our databases call
us and so we don't have a concurrent
wetplace that these are done and so it's
a little bit ad hoc and I think these
this is very much the same as the idea
of parallel thinking there's really are
set of core ideas here that could be
abstracted away from particular
languages and machines that all of our
students should know and in a nice
coherent way and so what would be nice
to do sort of a similar sort of thing
here where we take those idea and wrap
them up into a course so that would be
something to do in the future I wanted
to finish by it turns out that if you
look up parallel thinking on Wikipedia I
didn't do this at until after I started
using the term parallel thinking how
many of you have heard this version
apparel it turns out that there's a
Edward de Bono is a very quite
well-known person in these you know
circles of you know manage management
training so he's if you've probably
heard the term lateral thinking so he
coined that term and then after that he
coined the term parallel thinking and
it's a particular approach to basically
for large group discussion that's
particularly to come to decisions
quickly the ideas I know it sounds a
little hokey to me but six different
hats on and you put your logical hat on
then you put your emotional hat on and
when you have your logical a hat oh the
whole room talks about logical things
and you put the emotional hat everyone
they talk about anyway it's popular but
what I found very interesting is that
you know wikipedia says is you know what
it's really about is looking for what
can be rather than what is and I thought
well this is appropriate because you
know we're talking about parallel
thinking here is it's not taking this
really a focus and in some cases of
taking a sequential code and you know
sort of twisting it into being a
parallel code so it's you know what is
there and let's work with what we have
as opposed to thinking about let's raise
the level of abstraction and you know
think about what could be on this code
and I think that actually sort of as an
approach
you taking code and convert it to
parallelism is a very important sort of
thing to do is instead of taking a
sequential code and converting it
laterally to something that's equally
low-level as parallel code that's a
mistake what you should really be doing
is taking a sequential code raising the
level of abstraction like in the case of
quicksort so where it's a more high
level where the parallelism sort of
comes out more naturally and that's
going to have more than Jeff longevity
in the sense of parallel machines of
changing very rapidly so any code you
write in CUDA is probably going to be
worthless in five years and he code you
write in a lot of these languages are
going to be you know possibly not useful
so if you think about raising level of
abstraction and having maybe the
compiler map it down the useful thing
okay that's all i wanted to say they
worry about basic conditions they worry
about differential equations like
navier-stokes equations you don't teach
any of that I mean where you have this
sort of domain decomposition not OpenMP
but at mpi message passing and stuff
like this so yeah probably what I should
have said I think all that stuff is very
important and so in particular when I
said you know parallel thinking it you
know is not these things those things
are still important but the point is
that they should be taught you know up
feeling is after the fact right that you
can learn a lot of those concepts one
thing that I quickly went by here is one
of the things we've been working on in
the terms of the research side is how to
come up with high-level models for
counting for communication costs this is
sort of capturing some of these ideas of
how to deal with locality etc which I do
think is very important concept so it's
really just a question of teaching this
first and then teaching a lot of these
concepts and some of these sorts of
constants you're talking about uh
probably not useful to all of our
students but rather to the particular
subset of
but it didn't but the sort of
parallelism that they use through these
differential equations doesn't seem to
be captured by your list of topics I
would argue different that but we could
talk about we go okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>