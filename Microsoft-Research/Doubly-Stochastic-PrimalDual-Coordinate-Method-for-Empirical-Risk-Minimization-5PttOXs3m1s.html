<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Doubly Stochastic Primal-Dual Coordinate Method for Empirical Risk Minimization | Coder Coacher - Coaching Coders</title><meta content="Doubly Stochastic Primal-Dual Coordinate Method for Empirical Risk Minimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Doubly Stochastic Primal-Dual Coordinate Method for Empirical Risk Minimization</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5PttOXs3m1s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
today we are very happy to have Adams
way from CMU Adam is a beauty students
at SMU and he did a summer internship
last year today he will talk about
something different from his intern work
okay okay thanks for the introduction so
Adams from CMU today I will introduce a
very efficient doubly stochastic
primal-dual algorithm for empirical risk
minimization problem and this is the
genre with Chung Ling and timber yang
they are both from University of Iowa
okay so a lot of machine learning
problem can be abstracted to the
following empirical mini empirical risk
minimization problem it is essentially
minimizing and convex objective which
contains two parts the first part is
called the impure cross which it
computed from the given data and the
second part is called the comrades
organization function for example it can
be the l1 or l2 norm which can be
incorporated our prior knowledge on the
decision variable and we further assume
that the loss can be decomposed as the
summation of of the different loss on
different data okay so if the last is
the square loss then it would be a
linear regression problem and if the
last is is a sigmoid function then it is
a logistic regression if the last is a
smooth hinge loss then would be a smooth
as VM okay so indeed capture a lot of
problems in the machine learning
community and we have the falling
assumption on the problem the first is
that the requisition G
can have a decomposed structure which is
the summation of the different
coordinate and we also assume that each
GJ is lambda strongly convex and we also
assume that the loss functions Phii is 1
over gamma smooth ok this is all our
assumption and if we introduce the
ventral conjugate of the loss function
which is the Phi I star then we can
reformulate the original empirical risk
minimization problem as the bilinear set
upon problem which we will introduce a
new term which is the bi bi linear set
upon and this a is the data matrix with
each row is a data and it is in each
data is of dimension p ok so in the
following we we work on this formulation
rather than the original formulation
actually there is a lot of various work
were on this empirical risk minimization
problem there are two lines basically
the first line is called a gradient
stochastic gradient method actually in
each iteration the such kind of methods
use a stochastic gradient which has
which in its vacation is the same as the
full gradient but they have they will
have a variance the algorithm the
algorithm include the included proposed
by the pouring authors and another line
algorithm is called stochastic variance
reduced gradient method this this matter
this mess introduced a new term and and
substrate another term such that the
gradient the stochastic gradient is
still have the same expectation as the
food grade
but it has a lower variance so this
called the stockades a variance
reduction okay another line of algorithm
is called coordinate methods which in
each iteration they just sample it as
randomized select a few a coordinate and
do the coding descent this line of
algorithm is not a serrated but these
lines are accelerated which means that
they have a square root the dependence
of the condition number kappa recently
there is some algorithm proposed on a
primal dual algorithm which they were on
the set upon formulation okay so all of
this algorithm assume that the number of
data and is huge but the dimension p is
the relative small so all of them just
focus on sample one or a batch of data
point in each iteration okay this'll
slow cancer rice
yeah John has a paper in nips 2013 it
was different from yeah most of the SG
he returns yeah but basically the
general idea would be the same thing to
say but yeah oh wait is girly oh yeah is
it true feel qu chi Chi Yong Chi dunno
the most i'll go to pinterest parents
yeah she doesn't work with secondary she
is a assistant professor in the
university of hong kong ok so yeah the
way if when p is relatively relatively
larger than n which is which will happen
in the high dimensional space case in
the case we can just simply sample 11 or
a batch of feature in each iteration and
apply the diversion of the approaches
mentioned above but what if both both n
and p would be large a natural solution
is just to sample 11 date upon and one
feature in each iteration there few
previous work on this this line and this
talk we also fall in this category ok in
this talk we will care about and answer
the following questions the first is
that what is the iteration complexity of
the coordinator for problem that we can
stumble both from the primal and dual
spaces per iteration it is essentially
to answer what is the convergence a pre
bang of the algorithm the second
question is when with this type of
algorithm will be better than the purely
primal appeared do coordinate matters
and this will be concerned about the
overall complexity of the algorithm the
third problem the third question is was
the
mental image of this type of algorithm
and this is excessive essentially the
convergence lower bang of this time of
algorithm okay so we will first answer
the first question by proposing new
algorithm okay so let's quickly review
the problem we would like to solve this
empirical risk minimization problem and
we formulate it as a center point
problem and a sub is d is the eighth row
of data and AJ is the JS column of the
data okay and now let let me review one
very famous algorithm which is proposed
by each and Jung and lin xiao into a 10
15 i think so i think is famous because
the crappy a lot of citations and ok and
ok the idea is as follows in each
iteration the evidence sample a subset
of data points which that the number of
data phone would be m and first they do
and use this end data point to do a deal
update it this why is the deal variable
okay so if it is sample if the data
example then update the corresponding
deal variable and if not then and remain
unchanged and then we do a momentum at
they of this deal variable okay after
that we use this updated deal variable
to update the primal variable but in
this step they do a full primal update
which means that the update a full
vector of X ok and then again they use
the momentum update ok this is the
basically the SPDC
and in their paper they show that if
they choose a proper step size then that
convergence rate would look like this
which has the dependence on the log 1
over epsilon which means that the
convergence rate is linear and their
convergence on their dependence on the
condition number cover is scheduled
square root which means that they are
serrated algorithm and this condition
number is this it is defined as the
maximum out to numb of the data over the
strongly convex and a smooth very smooth
parameter okay it illustrate how you
post knees of the problem normally
lambda and gamma would be small then the
copper would be very huge so so they
have a square root of this cover which
means that they are low that the
complexity is relatively low okay so
this is basically the theoretical result
of SPDC and the overall complexity of
SPDC is just nothing but the number of
completion times the per iteration cost
since their prey iteration cause is mpm
is the number of some number of a sample
from the data and p is the dimension of
the data it is due to the inner product
so so the over complexity overall
complexity will look like this and but
what if we sample both from the data and
features this is what we propose the ee
stock has a primal do call it a matter
which is shocked uh it's the short term
is vsp DC the only difference is
that in the primal update we select one
coordinator update rather than to do the
full update and also the step size and
and in this moment momentum step size
will be rather different this is our
result after choosing this step size
tile and stigma to ensure this gap
should be smaller than Epsom our DSP DC
needs such kind of such iteration number
okay again it is the dependence on epson
is one log 10 web stone which means that
it is a linear convergence algorithm but
we have some other additional term here
now in the following I will discuss the
difference between these two algorithm
okay first of all we need to introduce
the capital lambda q m which is
essentially the largest l2 norm of the
block of data suppose we divided the
data into in the M times Q block and
this lambda is just the the largest
outer knock of the block okay so for now
we will connect this term to the
previous condition number cover if the
data is dense and evenly distribute and
it has the evenly distributed features
then this term is approximately
approximated like this okay and in that
case the DSP DC will recreate would
require this number of iterations this
our result is interesting if the Q
equals P and M equals to 1 which means
that we we do not do the primal
sample which means that we do the full
primer update then it will recover this
optimal rate of SPDC and and the
accelerated SD CA and if Q equals to 1
and M equals M which means that we do
not do that we don't do the do something
and do the deal for update then it will
recover the optimal rate by chi on
lynnes paper okay so that means our
result captures the previous result this
one yeah yeah I think it's the one with
you which draws only yeah okay I think
so ok so now for the following
discussion we we assumed at the M or the
the ratio of the prime prime or
something is larger than the do the do
something oops sorry the deal summary is
larger than the ratio of the prime of
something without loss of generality
otherwise we can just apply to do
version of of the DSP DC anything is a
natural sample yeah he's really bad yes
yes p is a number of additional 50 acres
more prickly yeah be sure the queue is
like a star problem feature you use yeah
so it's like a poster is a full batch
person versus a manufacturer of vision
yes so what is it I'm on the q hard work
yeah so another Q can both be one yes
you should just pick one example of one
determining that you will be an MP left
but you can use mini bachelor Brock
Coronus then that become
it is important part yeah yeah so yeah
exactly so if P equals Q which means
that we do the full primal update then
the rate will reduce to this one which
is nearly optimal for the deal
coordinate method the optimality is is /
is proved by Allah Allah and the about
to last year okay Lando is speaker yeah
yeah I Pearson yeah probably a better
proof and the correct roof yeah better /
yes yeah and if N equals there which
means means that we do not do the
sampling from from the deal then it
becomes this and which is very likely to
be optimal but none no one has proved
this at this moment so you have to swing
from pure for results likely means that
we do not see how can we accelerate this
that's why we call it's likely to be
optimal ok so the asymmetric parties you
do not have the square root right you
can compare this one to the first one
you have square root cop-out have square
root and / because we assume the this
one is larger than this okay that means
this far only have at the end and over
em and if it is eco then it would be one
right actually n equals to M &amp;amp; A and
this ratio louder than this implies P
equals 0 cube but but it's not symmetric
still no sir
you're if it p equal to Q then actually
this should be one right no I no not a
few just just assume the bridge were the
same okay temperature examples and then
you ever have equals to pd I like you
yes I'm could be ten Porsha okay the
hard way okay and then your complexity
in the world would be oh no this is a
magical it's also symmetric they have
this for it will have a spare look
forward oversee good PQ because i know
you assume that p over n over m equal to
P over Q right so then this would be
also equal to an end of I am yeah right
look at that you're the first level is
what Bob you have a square root oh yeah
which one that's what it'll one not
allow this once per second at this end
this way yeah okay yes square just look
at the coefficient of the Kappa square
will cover okay your square Moodle and
it p equal to Q yes then this weekend oh
god I just just just look at that okay
you're gonna square root gamma squared
will happen yes and the next one
universal spirit kappa p or q if for
symmetric about the environment of
square root of P and Q taps
yes okay you had this using a puzzle
okay easier oh we have additional as
that assumption I mean actually they
would be symmetric if you yes okay so
this is a comparison between the spdc
and the SPDC fold this part for the
second part they they are same except
that the spdc here has additional factor
P over Q and if and this part would have
a additional p over q If P equals to Q
then exactly recover the SPD sees
convergence rate okay but on the other
hand it shows that the spdc has a slower
convergence rate because P is always
larger than or equal to Q so this this
term is always large in this term and
this term is always bad in this term so
but it that makes sense because in these
in SPDC they do the full coordinate are
they but we just sample that means we
basically we need more iterations to
convert it to the same epsilon okay so
so that means tsv in terms of
convergence I bsp DC is lower than SPDC
and for the per iteration cost SPDC has
m times p but this busy also has n times
P that means the over can overall
complexity of bsp DC is this it is still
slower than spdc just hire ya slower
slower slower yes yeah because we this
we have additional P over Q term here so
P is away yeah if you're not yeah that
means in general this really is slower
than SPDC okay so that may not be a good
news so that we that we may ask to ask
when will the DSP dizzy as some
advantage over the spdc right so in the
pouring we will consider two cases in
which the in this case is the SPDC is
better than SPDC the first scenario is
when the data is factorized which means
that the data a can be represented as a
multiplication of two matrix one is you
and the others v you is the end times
the M by D measures envy is that the P
by P matrix and B is much much smaller
than the minimum of a no and then p and
this time of vectorize data can be
always obtained for example by the
singular value decomposition or
non-negative matrix factorization or
randomized matrix approximation and some
randomized data feature production and
it is often used as a pre-processing
step before training a statistical model
too
reduce the noise and also to speed up
the training okay so so now the a is is
represented by the x the multiplication
of but very tall matrix and a very fat
matrix okay so so now to be this is in
this case we want to build a prediction
model in the reduced low dimension space
using this you and the consider that the
original model is the is a I times X and
the reduced model is just the UI times X
hat ok but then reduce model can be
learned faster but it's a little bit
harder to explain because we already
project data to a lower dimension space
and it is also hard to use for
subsequent analysis for such as the
pretty new instance and features vaccine
so this is trade-off this is a
computation knowing explanation trade
are okay the factorized data model can
be also formulated as follows this is
the original model and after that after
the reacted do their data with the
dimension reduction then it will become
this
and now the factorize data is the U
times V just we mentioned before
okay so when we work with such vectorize
data we can see some positive and
negative for some for example we check
the quality of the data for vectorized
structure and the structure should
accelerate all the optimization
algorithm for the erm however for
different algorithm the benefit from the
factor structure is is to different
extents which we will explain in the
following table now we will maybe skip
the detail of how we implement this that
the message is that for the factorize
data the DSP is the per iteration cost
of DSP DC as TM x DQ which is much
slower than MP because d is much smaller
than the maximum and MP okay
and this is the this is the
implementation detail and we just need
to maintain a u bar and the rebar and
additionally to to make it feasible now
this is the comparison between the spdc
and the other algorithm on the veterans
data the this column is the number of
iterations we can see that in this case
we mean we assume that M equals to Q
equals to 1 which means that we in
iteration vng sample one data and one
feature so the number of iteration the
dis video is board is larger than the
SPDC and also louder than the asd CA but
the cost per iteration DSP DC is the
slope is the smallest so the overall
complexity the SBDC is is the smallest
by a factor of P okay
that's only when the first are dominant
yes so especially when when n is large
then the first term will be dominating
ok
so yes
so we can see that for the factorize
data a scenario DSP pdz in D is faster
than the competing competitor now we
will talk about the second scenario
which is called a matrix risk
minimization in this case the decision
variable X is a matrix and this adds
matrix is decomposed to 2p block each
block is the D time d by d matrix and
now we the formulation is almost the
same as the vector version except that
the inner product is replaced by the
chase nom and the outer norm is replaced
by the verb is no okay so there is a lot
of applications of this formulation such
as the matrix chase regression and
distance metric learning but for this
scenario we need for most of the
scenario we need to impose additional
constraint such that each block of this
decision variable is semi deputy
positive okay let me post is the champ
attentional challenge that is for each
iteration we need project the X the
variable X to the PSD cone so we need to
do an icon value decomposition per
iteration which is which has complexity
in is cubed of d ok now we will skip
most of the detail and just show the
theoretical result for this Mitch risk
minimization the number of iteration is
the same but we only need to do one
eigenvalue decomposition per iteration
but all the other competitors need to
pee eigenvalue decomposition
so the overall complexity DSP dizzy
again wing ok so again the advantage
consists in the first term compared to
the spdc and actually in the in
practical scenario the SPDC is much
faster and the reason is although the
number of iteration looks like larger
than spdc however both of all of this
upper bound which means that that may be
loose but the cost per iteration is
concrete which means them for each
iteration BSP dizzy in these safe p
times computation so the practical the
practical time-saving by SDS pdz is more
significant and we will see in the
experiment now we will show some
empirical study the first is on the
factorized data in this case we choose
the last to be the smooth hinge loss and
and we some regenerated data as the
following which I will skip so we
compare our algorithm with SD CA and SP
DC and in terms in terms of the the
number of arithmetic operations we can
see that DSP DZ is faster than the to
stay of the our competitors ok
what the Y X which is the this one this
is the objective value yeah the primal
objective okay the this is a primal
objective gap I think this cars you are
using the person now careful
implementation spdc why because with
Alba or two there's a sweater operation
with SPD yes but you can make it the
same as spc sec yes per iteration will
be same cause SPCA may be times to
basically just no photograph it at all
it's all sparse up there yeah we also
use that that we also use this update I
mean we may we use the because it works
on the factorize data okay yeah it's
factorize they're not not the original
formulation because i think the
publishers did you see father yeah yeah
it in the original I think spdc will be
the pan frances and okay
and we also use the lambda equal to 10
to the minus 2 which means that the
camera is not that large okay but even
for the for the small lambda which means
that the cover is large DSP DD is still
comparable with SBC and actually it's a
little bit faster yeah and yeah and as
these days the slowest okay as for the
real data we compare we run out three
algorithms on three real datasets the
code type there has to be one and Rio
seam and we we manually project the
original data to a low dimension space
which only contains 2010 two features
and we choose the spots past love the
one would be 10 to the minus 4 when the
out to lambda 2 would be 10 to the minus
2 okay again we can see that the SPDC
alpert from the other two competitors
okay the second experiment is on the
matrix risk minimization and we we we
generate the data as following started
also compressor macarthur this is a good
question so actually if compare is time
that them SVC might be a little bit
slower it depends on the implementation
because as I mentioned the SPDC use more
iterations and if you implement it on
matlab for example you need much more
for loop and then it would be much more
slower yeah
so we show the original arithmetic
operation just to for the sake of fear
in that case it would be much more fair
independent of how you implement this
it's no favors because in the sense that
it depend how much your numerical
computing can be in Paris yeah burgers
are so if you that's the direct
translation in our case we just enforced
they used a single thread computation
okay in in this matrix risk minimization
each each variable at the ground choose
variable is to choose as the D by D
identity matrix and there there are P
such kind of identity matrix so in this
case we again choose the smooth hinge
loss we assume that the DS is 200 and
the P which means the number of the
blood of this D by D matrix is 100 and
we used a wee example 100 data point and
the lambda is set to deal from 0.01 so
as we mentioned before these PDC in this
scenario is even faster and most
significantly faster than the added two
competitors under different sample
scheme no matter M is larger than Q and
equal to Q or M is more than Q okay
that's basically because in each
iteration we only need one eigenvalue
decomposition but the other two compared
to need p p value decomposition and that
the time that's saving of time is very
significant ok so this answers the
second question that is when the
ESP DC would be would be much more
efficient we give two scenarios now the
third question is what's the fundamental
limitation of this type of algorithm to
answer these questions first we need to
formally define what's the what this
kind of algorithm it is basically lie in
each iteration we restrict we we can
only sample a one coordinate m1 m1 data
and then we can we can add the points to
the current solution which is just the
linear span of e to this point or its
gradient or it or the inner product
between the data block and and this
point and we do it again for the four we
do it post for primal and do and this is
the this is the abstract description of
this time of EE randomized coordinate
algorithm now in the following we will
give a bad case to show that this
algorithm would need haha I mean at
least how many iteration this algorithm
would need to solve that problem the bad
case is just is a quadratic function you
both for the primal and for the do and
the data the data we generated we use is
very sparse it is just look like a
diagonal matrix ok in this case if we
also assume that the number of dimension
is the same as the number of data ok
which means the N equals Q
now we can easily compute in the
parameter that is necessary for example
the lambda the gamma and QM and by the
theorem of at the spdc it would need
this time this number of iteration to
make it to make the make it converge to
the optimal solution and on the other
hand because this formulation is
quadratic we have a closed form solution
it's as simple as this and just a few
feeling of of linear algebra now we will
discuss about why this case is bad for
this type of algorithm the first
observation is that the closed form I
mean the optimal solution has none none
zero inches both for the primal and for
the do okay but fraud the the algorithm
we consider only when there is one
primal deal interlace in the sampling
sequence could add one nonzero entry to
both primal and dual solution but this
is a very rare event that means that
only after a lot of iteration can be
increased one nonzero entry to the
solution now this is the example of the
sample sequence and I will explain why's
the interlace so each bracket is primal
and do indices that is sample and this
for the iteration want is for the
iteration 2 and so on so forth and only
when the only when we first sample one
index in the primal and later on we've
we sample another another index in the
do
they coincide with this one can we
increase one nonzero of of the solution
and we prove that that that's
non-trivial we show the the probability
that how can we get this and it would be
a very that's a very rare event and we
will skip the technical detail here and
we will show the result as follows so
suppose if we apply any any algorithm
from this of this type to the prop the
set up on problem one and the output is
XT and YT then to make sure that the
primordial gap is more than epsilon we
need we need such time such number of
iteration to make you converge okay so
compared with our lobe upper bang
unfortunately we show that our brand is
not typed by a number by an N factor if
n is more than it would be almost the
same but if n is large the gap is the
instill large so there is an open
question is can we close the gap our
conjecture is that the a program cannot
be tighter but maybe we can give the
worst case to make this lower bound
tighter ok so this answers our the third
questions was the fundamental limitation
of this time of algorithm
I just conjecture because I mean because
when we construct this case we we feel
that we can still find space to make it
worse so actually on okay in this case
on average we need to sample n times to
increase one non-zero nonzero component
of the solution actually we think that
if weekends only after the example n
square time n squared times we can
increase the number of nonzero then
thieves will be tight so and we think
that's possible so yeah
okay so that that's a remaining open
question so I remember that in our
garage paper that their their lower
bound is the bang of spdc and lob I
still have some gap right the square
root n also if you invest close okay
Isis is itself okay
okay the take a message of this tau is
that we propose a doubly stochastic
primordial coordinate method for the
empirical risk minimization problem and
this algorithm has a linear convergence
rate and it has a low lowest overall
complexity when we applied to the
factors data scenario or to the matrix
risk minimization scenario so we also
give a lower bound on the iteration
complexity for the class of primal dual
stochastic on their methods and there is
a open problem remain that is can we
close the gap between the lower bound
and the proposed upper bound and thanks
thank you so much high schools like so
hug reference ah oh that's all about the
show so so you you know your thoughts of
a body set up over to the future I'm
sorry different characters it even about
or it's not opening what about is are
not open one or a dog has some problem
this is fear actress yeah actually
actually we ok we have for this
theoretical result I mean for the upper
bound we have proved this almost one
year ago and to think about the lower
boundary or we also pay for a few months
and our intuition is just as why i
mentioned i think that the case could be
worse why did gap I mean the low rank
can not be I mean improve because I for
the SPD's the SPDC is opting all right
yeah so as we so I'll conjecture this
busy will be also optimal there is I
mean we have no space to improve write
this out our intuition so I'm sorry too
soon right yeah yeah i think i think the
your
zagreb on the looks more safe for me but
whatever it very easy to be tight one
normal again that's what I the same same
thing yeah yeah actually our original we
want to sample one okay when we sample
that there is one interlace that will be
increased by 1 10 and 0 actually I think
it would be 1 over N squared but our
comm potential that is just 1 over N
that's why I mean yeah so we have
finished this for a while but we did not
submit it because we want to close the
gap but then you didn't manage and do
that at this moment Shannon told me that
facing hog or Spanish not good but that
I could vary the freaking support okay
let's rest of it yeah so we reach amy
has a body away with that with which
pump it does still love them but the
land has already finished it yeah but
somehow I see transposes where's your
biggest nut lands from actually lends
more year jess is a little bit I mean
easier than then I gamez agoura I mean
take the lower boundary covers is on a
more general class of algorithm land
only focus on us more class of Evan
that's why it's easier to achieve it
yeah so you will also try to
over them more explosive us yeah we
actually try to we actually be
translated the matlab code to sleep
class files but in terms of time it
might not be faster but i mean for the
matrix case it would be faster because
we save a lot of time doing the
eigenvalue decomposition no meta meta
level c trans fats so are you
Byron critic reviews are simpler process
in berlin in the photo current issue as
you can grab in a new incomparable the
jack pan of your problems yes yes yeah
anticipation suggestion
clear</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>