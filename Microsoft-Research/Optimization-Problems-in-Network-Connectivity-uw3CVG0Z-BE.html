<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Optimization Problems in Network Connectivity | Coder Coacher - Coaching Coders</title><meta content="Optimization Problems in Network Connectivity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Optimization Problems in Network Connectivity</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uw3CVG0Z-BE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so we're delighted to have damania
long term visitor an intern here tell us
about optimization problems in network
connectivity thank you well is the mic
switched on yeah thanks so let's start
with tracing the history of
telecommunication and perhaps the desire
for a large-scale long-distance network
started with the invention of the
Telegraph in the middle of the 19th
century so much so that by the early
20th century there was already an
extensive network of submarine cables
connecting the world's continents of
course over the next hundred years as
new technologies developed new networks
came up as well for example the
telephone network in the u.s. was fairly
complicated by the middle of the 20th
century and we have the modern-day
internet but what connects all of these
networks is the desire to connect
individuals which means the natural set
of questions for any such network is how
good are these connections for example
how many cable failures can they us to
your ropes connection survive in a
telegraphic network or how do we
increase the capacity of a connection
say how do we increase the capacity
between Boston and Seattle or in the
internet how many link failures can
cause it to get disconnected into
multiple pieces so these are questions
which are limited not just to
communication networks but also to many
other kinds of networks for example road
networks or electrical circuits or even
social networks and process workflows
which are just virtual networks so in
all of these networks we have seen that
the key desire is to connect entities
and therefore the key questions are
related to the connectivity of these
networks so instead of asking these
questions specific to these applications
can we abstractly define some of these
questions so that we can propose
unifying solutions for all of these
applications and if you want to
abstractly classify connectivity
questions then there are two broad
classes of questions we come up with the
first are called network analysis
questions which is the finding the
connectivity properties of existing
networks so I am giving a network what
is the minimum cut in this network or
what is the spar syst cut what kind of
flows can be sustained between two
points in the network
the second class of broad class of
questions are what I call network design
questions and here we do not have an
existing network rather we are given
price network elements nodes and word
unknowns and edges and we want to put
them together to achieve some desired
connectivity properties so we want to
achieve some desired rate of flow
between two points or achieve some
desired robustness in the network so
these are the two classes of questions I
will focus on today in particular I will
talk about two but two problems or two
groups of problems for network analysis
questions I will talk about minimum cut
problems and there will be several
problems i will talk about in this
domain I'll also talk about network
design and the problem I'll specifically
concentrate on our the Steiner tree
problems and these are these are two
fundamental problems in these classes
and towards the end of the talk i'll
also bring up a third class of problems
called cut specification which is very
closely related to network analysis /
does not quite fall in either of these
two categories in so that's the general
plan of the top we'll start with minimum
cuts then we'll go to steiner trees and
will end with specification so let's
start with minimum cuts there are two
kinds of min cut problems that have
received a lot of attention one are
called the local connectivity problems
the other class is called global
connectivity in local connectivity we
are given a graph and we are given to
particular vertices to selected vertices
in the graph called terminals so here
these terminals are denoted by SN T and
the goal is to find the smallest cuts
that separate us from T in this case
these cuts of three edges each in global
connectivity problems or global min cuts
the goal is simply to find the smallest
cuts in the network we are not given any
terminals it's just overall what are the
smallest cuts in the network right so of
course both of these represent the
fragility of the network two failures
input and since these are foundational
questions in connectivity there has been
a lot of interest over the years and
many algorithms have been proposed to
find the local connectivity of a single
vertex pair this is connected to the max
flow question for example by duality and
similarly if there have been many
algorithms proposed to find a single
global mein Gott in the graph but what
we will focus on in this stock are not
these algorithms but how do I
find the local connectivity of all pairs
of vertices so if you think of any
application it is not it is not feasible
to say that I will find the local
connectivity of Boston in Seattle today
throughout my computation completely
find the local connectivity of two other
locations tomorrow instead it would be
much better if you could find the local
connectivities of all pairs of vertices
in a graph and encode them in a data
structure from which we can easily query
any particular connectivity so this is
very similar to pairwise distances for
example in a graph we often try to find
pairwise distances for all pairs of
vertices then encode them in a data
structure and query the date structure
efficiently right so this is the problem
we will focus on for local
connectivities for global connectivity
is imagine that you want to increase the
robustness of a network now if you want
to do that it is not sufficient to just
find one min cut right I if I if I
increase the number of edges in one wind
cut there could be other wind cuts
lurking around it still reduce the
effectiveness of my increase in
robustness right so what we would be
interested in is can defied all the
global minutes in a network how fast can
we find these two quantities okay so let
us start with local connectivity as I
said the goal here is to find the local
connectivity of all vertex players and
this question was asked as early as 1961
when Gamorrean who came up with an ex
with a very very elegant data structure
that showed that all these n square
different min cuts their n square vertex
pairs so there are n square local
connectivities all these n square
different connectivities can actually be
represented in linear space injusto of
n-space so the focus shifts to whether
we can construct this data structure
whether we can construct this linear
size representation of all these n
square min cuts in a graph yes yes only
talking about undirected graphs here
that's right okay so that is that is the
focus of the first part of the talk and
there have been many algorithms proposed
to construct a memorial tree or other
data structures that find all the local
connectivity's in a graph now the best
running time at the moment is M times n
to the three halves and I'll show you
why where this runtime comes from but
if you look at all these sequence of
algorithms over the last 40 to 50 years
we see something very surprising all
these algorithms are based on exactly
the same idea so take these set of n
square min cuts identify some linear
number of min cuts that are critical
from which I can construct my entire
Gomorrah who tree and then once I have
identified these min cuts use your
favorite local min cut algorithm to find
these wind cuts and put them together in
a Gomory hoochie all the all the
previous algorithms exactly follow the
same recipe and this is the recipe they
follow where is the difference well
there are differences in how you
identify these men cuts and also
differences in how you find them in
because once you have identified it them
right so the basic algorithm for finding
all local connectivities in a graph has
remained unchanged for more than 40
years which obviously makes the question
is there something we are missing out by
focusing only on this one recipe of
finding comedy who trees so to answer
this question let's step back and try to
see what a memory who tree is so here is
a definition it is a weighted tree on
the same set of vertices as the input
graph so let's say here's an input graph
on the Left I construct a weighted tree
on the same set of vertices and in it
must have the following property if I
look at any pair of vertices let us say
s and T then if I go to the tree and
look at the min cut separating us from t
this min cut on the tree is is very
simple it's simply the lightest edge on
the path connecting s to T so here is a
min cut that separates us from T in the
tree I take that min cut bring it back
to my graph the graph and the tree are
on the same set of vertices the cuts are
correspond to each other then this make
this cut should satisfy two properties
one it should be a min cut of which
separating as from T in the graph as
well and to the number of edges in this
cut should be exactly equal to the
weight on that edge in the tree ok which
means that the tree really represents
all pairwise connectivity values right
for any pair of vertices from the tree
we can easily find the local
connectivity of that pair so this is a
Gomory who tree how do we construct it
using local min cuts and it's actually a
very simple algorithm so initially what
we do is
we select any arbitrary pair of vertices
say s and T we find a local min cut for
that pair so here's a min cut that
identifies an edge in the gamma you tree
for us it's corresponding to that min
cut we get an edge with whose weight is
exactly equal to the number of edges in
that cut once we have gotten that edge
we know that each side of that cut will
be separated by the edge in the tree as
well so once we have identified that
edge we can recurse in the following
manner what we do is we create two
instances of the problem where in each
instance we retain one side of the cut
and contract the other side so here for
example we create two instances so if
you go back you will see that in one
instance have contracted the bottom half
of the cut in the other instance I have
contracted the top half of the cut and
now I recurse on these individual
instances by again picking two vertices
on the side that I did not contract and
repeating this until I get all the N
minus 1 edges under govoryu tree which
means that really what I am doing is n
minus 1 local min cut computations every
min cut computation reveals an edge in
the tree there are n minus 1 edges the
best algorithm to find a local min cut
takes time m times square root of n and
that's where the M times n to the three
halves running time comes from ok so
this is what the state of the art is but
again is there something missing here
and one thing that this discount this
algorithm is ignoring or this yeah
you know the problem is that this entire
recipe only works if it is an exact
algorithm for a wind cut if it is an
approximate algorithm then once you
contract the mean cuts change if it is
an exact minute algorithm you can do
this contraction otherwise you can't and
moderate Al's algorithm is only for only
getting you an approximate min cut it
does not get you an exact one so the
approximations basically build up on the
recursion tree so if we look at if you
look at the comparison a comparison
between local min cut and global wind
cut algorithms then till the early 90s
when these algorithms were being
developed it was thought that local min
cuts are easier in fact all the
algorithms for local men cats were
fasted global wind cuts were typically
found by repeated local min cut
computations but things have changed
over the last 20 years in fact now we
can find a global min cut in
approximately linear time whereas local
wind cuts are still quite far from
linear time so while idea is perhaps in
this recipe we can change the local min
cut computations to global mingott
computations can I change the local min
cuts to global min cuts what happens
well here is a graph I find a global min
cut in the graph it does identify an
edge in the Gomory who tree for me so I
can use the same idea this identifies an
edge I contract the two sides to create
two instances and wreckers the problem
however is in the recursion what happens
in the next stage well we are finding
global min cuts so we keep finding the
same when cut and we do not make any
progress whatsoever now the problem is
that we are by insisting on finding
global man cuts in the recursive stages
we are not being able to find cuts that
that split the part of the graph that I
did not contract it because I have to
find small cuts but cuts that split the
part of the graph I did not contract so
to do this we introduced a new problem
called the steiner min cut problem which
finds the smallest cut that separates a
set of terminal so I give you a graph I
also allow you to give me
a set of terminals in the graph which is
a set of vertices and I want the
smallest cut which splits these
terminals into two pieces so if you
replace global wind cuts by Steiner main
khud computations sorry well multi-way
you give many pairs and you want each
pair to get separated Here I am not put
enforcing any conditions on how this set
get separated I only want this set to be
separated you only want to make
protection into two halves done Sally
adore all this just want one at least
one I want at least one terminal in each
of the two sides yes ok so if you
replace global min cuts by stein and
wind cuts what happens well at the top
level at the first level of recursion I
will define my terminals as the entire
set of vertices so it's just a global
min cut at that point at the next level
what i do is i define my set of
terminals as the size i did not contract
and then i make problems right because
the new cuts i get actually split this
sides so this gives me a correct
algorithm after n minus 1 iteration so i
will get a Gomory who tree what's the
run time but one problem is that if you
notice carefully this notion of a
Steiner mincha generalizes both local
and global min cuts it if I define my
terminals as the entire set of vertices
that is a global min cut if I define my
term my set of terminals are just two
vertices at the local min cut so clearly
I can't hope to be bettering the runtime
of a local wind cut computation here in
fact the best algorithm we could come up
with for finding a steiner min cut takes
more time than a local min cut in fact
it takes time M times n so have you made
any progress we have a new algorithm but
really we are losing in the run time now
comes the key aspect of Steiner wind
cuts and this is that it gives an
advantage not just that it gives an
alternative approach but it gives a very
key structural at advantage over local
min cuts so if you think of a local min
cut computation what does it do it
essentially computes a
though between S&amp;amp;T so a flow is just a
collection of paths now once I have
computed a set of paths between S&amp;amp;T in
the next step when SNT get separated
these paths are completely useless so
have to start from scratch again what a
Steiner min-cut does is it computes a
set of trees it packs trees rooted at a
vertex s now s goes to one of my two
subproblems all these trees go with this
and can be used without any any change
whatsoever so there is even though we
are doing n n minus 1 Steiner min-cut
computations there is significant
overlap of the work I am doing at these
various computation nodes in fact if we
do an amortization argument which can
sure that we get an algorithm that runs
in time M times n which is the same as
just a single steiner mingott
computation it the whole entire n minus
1 Steiner min-cut computations amortizes
to just one single computation and this
improves the run time for finding all
the local min cuts in fact it turns out
that this is optimal provided certain
cut conditions Holden this was proved by
a button cell so quick recap there are
two things here one that local wind cuts
are not necessary for computing memory
who trees and can be replaced by Steiner
wind cuts too even though a single
Steiner wind cut computation is slow
slower than a local min cut computation
a sequence of Steiner minute
computations is much faster than a
single look and then a sequence of local
min cut computations and that is where
we get a new algorithm so how they
looked at a local connectivity here
sorry so either this algorithm cannot be
improved or a certain dynamic condition
is false which is hypothesized to be
true
and with that we sort of Ravi fat yes so
if you were to improve something
something else ah nah yeah you prove you
disprove some other hypothesis that is
generally believed to be true
also so you use crucially that the
sequence of Steiner what he sees is a
chain no I didn't get you so this
sequence of so you have a set of
terminals in the next set you get that
set of terminal splits into two sets of
terminals and the fact that you chose
one vertex to root your to you sat at
their parent is useful in the one where
the parent goes alright so having looked
at local Minkus let's move on to global
connectivity as I promised you the
problem we would be looking at is
finding all global wind cuts how many
global min cuts are there in a graph for
all we know there are exponentially many
min cuts they are exponentially many
cuts in a graph so really the first
question here is how do we even count
the number of cuts and if you can count
them how do we represent them it turns
out that one can show that there are
only quadratic number of global wind
cuts and not only that that all of these
cuts can be represented as in the local
connectivity case in just linear space
and this is called a cactus and this
this is the data structure that has been
around for 30 years now so really the
question is how do we compute it how do
we construct a cactus and there have
been many algorithms again over the last
30 years the best run time at the moment
is quadratic in the number of vertices
again exactly as in the local
connectivity case if you look at all
these algorithms they have the same
recipe i give you the input graph you
somehow list all the min cuts in the
graph and then put them back into a
cactus now that but this listing could
be very succinct there can be very
succinct listings each min cut might be
found by a very efficient algorithm but
there is this intermediate step of
listing all the main cuts in all of
these algorithms now is there something
very specific about the quadratic
runtime in the sense that if you want to
improve the running time of an algorithm
for finding all global wind cuts can we
still follow the same recipe and make
changes to how we list them in cuts or
how we find wind cuts or is the entire
recipe going to get stuck at quadratic
in fact we it turns out that there is a
significant fundamental barrier
act quadratic run times and it comes
from the following single simple
algorithm so sorry simple example so
here is a cycle a cycle has just
energy's so it's linear number of edges
but how many min cuts does the cycle
have well I remove any two edges that
some in cut for me so it has n edges but
n square min cuts now if it has n square
min cuts then no matter how I list this
mill cuts how I find them I will incur
at least n square time in the
intermediate phase as long as I am
listing wind cuts I am in trouble so
really if you want to improve the
runtime of finding all global man cuts
in a graph then we have to somehow get
rid of this intermediate step and the
key structural property we prove is that
it is not all these n square min cars
that are equally important there exists
a subset of linear number of minutes so
of em min cuts that encodes the
structure of all the mingotts in a graph
this is the key structural property that
helps us in reducing the runtime from n
square in fact once we once we have this
property the next question obviously is
if we can identify these men cuts how do
we find them and it turns out that again
the same technique as for Steiner
connectivity works and this is this is
using tree packings now there are many
algorithms for tree packing spawn
algorithm that really works out well is
something that I worked on in a separate
project it does an arbor essence packing
in graphs but that aside when we put
these two together to get the first
linear time algorithm for cactus
construction or the first linear time
algorithm to find all the wind guts in a
graph and of course this cannot be
improved except for logarithmic factors
we have to look at every edge at least
once Fineman cuts so this brings us to
the end of the first part of my talk on
network analysis and I will pause for
questions
alright so let's move on let us move on
to network design now and here i will
focus on the Steiner tree problem so
what is the Steiner tree problem it is
the most basic problem in any network
design context so I am given a set of
locations let us say some microsoft
offices and I want to connect them in a
network what's the cheapest way to
connect them so more formally we will
look at online Steiner tree problem so
all the terminals are not given to me in
advance but they come online and how do
we augment our Steiner tree to maintain
optimal cost so here is a formal
definition of the problem we are given
undirected graph offline and this graph
has edged in node cost so every node has
a cost every Edge has a cost online we
get a sequence of terminals so we get a
vertex then another terminal and so on
these are vertices in the graph now when
terminal t sub I arrives we have to
connect it to the previous terminals by
augmenting the graph we have already
built so here's where the algorithm is
online we can throw away something we
already bought and of course the
objective is to minimize the cost so
they understand the problem and to and
to see a very simple first attempt here
is a greedy algorithm for oh by the way
I should mention that edge costs can be
generalized by note load costs if an
edge has cost see I can always replace
it by two edges with a node having cost
see connecting them and this is without
loss of generality so I will only talk
about node cost from now on bye well it
makes it n plus M but I mean here we
will get algorithms which are polynomial
best serve a minute yeah I mean these
these would be the main focus will be on
the competitive ratio not on the right
thing all right so here is a very simple
algorithm for the problem it is a greedy
algorithm I get a terminal at this point
I do not need to do anything there is no
constraint I get a second terminal i buy
the cheapest path connecting my two
terminals in this case it is a path
containing the orange vertex and has
caused one I get a third terminal I
again by the cheapest path connecting it
to previous terminals again I incur a
cost of one
if I keep doing this for n terminals
then overall I have a solution which
buys all these orange vertices and has a
cost of n on the other hand if you if
you see the optimal solution that simply
buys the blue vertex and has a cost of 2
so clearly this greedy algorithm it does
not work well but a surprising fact is
this is the best algorithm that was
known for the problem there was no South
polynomial algorithm putative algorithm
known for the online Steiner tree
problem so let us look closely at this
algorithm what's going on well for every
choice that the greedy algorithm makes
which is a greedy path connecting the
current terminal to a previous terminal
the optimal algorithm also makes an
alternative choice right it also
connects the current algorithmic are in
terminal to a previous terminal and the
way we can analyze the cost of the
greedy algorithm is by simply summing up
the costs of the optimal algorithm of
the choices that the optimal algorithm
is making now if the sum of these costs
is at most Rho times the optimal cost
then we can claim that the cost the
greedy algorithm is Rho competitive it
because on every row the greedy
algorithm p is less than the optimal
algorithm now the key thing here is that
when we sum the cost of the optimal
choices then even if a vertex appears on
10 of these paths 10 of these optimal
parts we have to count the cost of the
vertex 10 times because on the
corresponding greedy choices we have no
guarantees of overlap so this is when we
are saying we some of the optimal costs
this is just a naive some even if the
same thing appears multiple times we sum
it up now now if we use this recipe for
the greedy algorithm that we showed what
happens so if you look at the optimal
paths these are the red parts these are
the choices the optimal algorithm makes
now the same blue vertex appears on all
of these paths so while the optimal cost
is just too when we sum up all these
optimal paths the cost becomes 2 times n
right because as I said even if it is
the same vertex since it appears on all
of these paths we have to sum it up 10 n
times and that's where the competitive
ratio becomes Omega
fell but now if someone gave me the
liberty of taking out a vertex from the
optimal paths so I have to some of the
optimal parts but I am allowed to do it
after removing exactly one vertex from
the each path if someone give me this
Liberty then in this particular example
I am in good shape it because on all of
these red parts I will pluck out the
blue vertex vacuous leaders all the
paths now some up to some small value in
fact zero in this case and therefore the
cost of the greedy algorithm is small
provided we are allowed this extra
liberty in fact very sorry so I am
saying that instead of the greedy
property if we make it a slightly weaker
property we're on all these greedy paths
we don't need to sum up the cost of the
parts but we we can we can identify your
vertex on the path say that I am NOT
going to some of the cost of this vertex
for everything else I will have to sum
up the costs it is exactly one vertex
that I am allowed to remove it's not
that you take it out you're not counting
that I'm not counting the cost in the
index in dilemma so in this particular
example it works out now very
surprisingly this works out in general
and here's the dilemma for any sequence
of terminals there always exists a set
of paths and they think of these as the
optimal paths such that path P sub I
connects terminal t sub I to a previous
terminal and has the following property
if there always exists one vertex on
each of these paths such that if we
remove that vertex from the path then
the cost of the remaining parts some sub
film so in fact the sum of cost of the
remaining parts is at most log and times
out remember that if we were not allowed
to take visa by out of the path then
this sum would have been n times up but
if just taking out one vertex from every
path we bring it down to login times out
and of course these are vertices on an
optimal on an optimal in an optimal
solution and therefore the cost of these
vertices overall is at most stopped when
we are not double counting if the same
vertex is visa by four
10 different paths we take it just once
okay so this is we call this the almost
greedy property because it is just one
vertex we have to remove now what is
what is this property gaining us in
terms of an algorithm well think of in
general when I get a new terminal I have
exponentially many choices of how I
connect it to a previous terminal if I
had the greedy property then when I get
a new terminal I only have one choice
it's the cheapest path to connect to a
previous terminal here I am sitting
somewhere in between if I can identify
visa by then really I have just one
choice i will reduce the cost of visa by
20 and take the cheapest path but how do
i identify visa by i do not know we
serve I in advance which means that i
have n parts now wrath which sits
somewhere in between the exponential
number if you had no properties and the
1 a 1 exclusive path if you have the
full greedy property right so for every
terminal we have now have n choices
rather than one choice and this lets us
reduce the online Steiner tree problem
to the online non-metric Facility
Location problem and if you if you know
this problem you would you would you can
guess what's going on the facility is
this one choice that I make there are n
facilities I make a choice of a facility
once I have made that choice the
connection cost is fixed for me it's not
not exponentially many choices but just
polynomially many choices and there were
already algorithms for this problem the
online non-metric Facility Location
problem which lets us guess the first
poly log competitive algorithm for the
online Steiner tree problem and in fact
this algorithm is optimal up to a log K
factor where k is the number of
terminals so moving on Oh before before
I move on I want to give you a very
quick proof of how how this property of
the property that i showed the almost
greedy property and here is a proof so i
define a spider as a tree we're at most
one vertex has degree greater than 2 ok
we call this vertex the head of the
spider all the leaves are called the
feet of the spider and a path connecting
ahead to the foot is called a leg of the
spider so here
I will prove the entire property in just
one slide and here is the proof we look
at the optimal tree this is the optimal
Steiner tree on this we first identify
the vertices which are non leaves butter
terminals now these can be replaced by
two vertices one non-leaf non terminal
vertex having the same cost as the
original vertex and then a dummy
terminal vertex which has cost of zero
right and this replacement is without
loss of generality now once i have made
this replacement i do a recursive
decomposition of the tree as follows i
find the spiders at the lowest level of
the tree so these are spiders at the
lowest level I look at a subsequence of
terminals on any one such any one such
spider ok so T t1 followed by t2
followed by t7 it is a subsequence on
the first fighter and then I define my
paths from each of these terminals
except the first one in the subsequence
to the immediate predecessor in the
subsequence so T sub 7 goes up to the
root and then goes down to T sub to the
vertex that I will pluck out of the path
i define that as the root of these
spiders once I have done this I have
gotten paths for T sub 2 7 and 4 I
remove these from my tree and then I
simply occurs I go through two more
recursive levels and now have got paths
for all terminals except the first one
and on all of these paths I have
identified these vertices to pluck out
so at this point I have identified the
paths and the vertices now do these
paths sum up well if the vertices were
removed well yes they do because there
are at most log n levels and on each
level every vertex except the route that
I plucked out appears on at most two
parts one going down and one going up
which means to the total cost over all
of these paths is at most log in times
of a toy slug and time zone so that is
the foolproof now let us move on to a
slight generalization of Steiner trees
the first rendered both I will talk
about two generalizations both
generalizations have the same basic
structure so instead of considering a
monochromatic set of terminals we split
the terminals into
multiple groups here there are two
groups of terminals now the constraint
based on this grouping of terminals is
different in the two problems in the
first problem called the group Steiner
tree problem the goal is to connect a
representative terminal from each group
from each proof I am allowed to select
any terminal and then these
representatives need to be connected in
the cheapest possible manner in the
Steiner forest problem we have to
connect terminals internal to a group
but do not need to connect terminals
across groups so for example whereas
this one orange vertex is a solution is
a valid solution for group Steiner tree
it connects one red terminal 21 purple
terminal it is not valid for Steiner
forest because it is not connecting
anything internal to terminal internal
two groups so for Steiner performance
these two orange vertices are a valid
solution so these are two very standard
generalizations of the Steiner tree
problem and we also give online poly log
competitive online algorithms for these
problems except that the only catch
being that these algorithms are quasi
polynomial time whereas the previous
algorithm the algorithm I showed you in
detail the online Steiner tree algorithm
is actually polynomial time so now let's
let me let me move on to a slightly
different conceptual generalization of
Steiner trees and these are called
network activation problems which are
introduced recently now if you look at a
Steiner tree problem then one way of
looking at it is the following at every
vertex we have two choices either we pay
a cost of zero or we pay the cost of the
vertex now if you look at any particular
edge I get the edge only if I choose to
pay the cost of the work both the
vertices as it two ends right so this is
this is just a different view of the
problems we have been talking about
there are two choices at a vertex and
edge gets activated provided I pay the
cost at the two ends now in many
situations things are slightly more
general for example in choosing powers
of power levels of vertices and things
like these instead of having two choices
there are multiple choices at a vertex
and when is a edge activated well it is
dictated by a general activation
function which maps the choices I make
at the two ends together the edge is
active or not and the only constraint I
have is that it should be monotonic if I
decide to pay more there shouldn't
finish so unless disk under this much
more general model can be solved Steiner
tree problems or other problems other
network design problems in fact we show
that some very carefully chosen greedy
algorithms can achieve logarithmic
factors for a wide variety of network
design problems including Steiner trees
but also higher connectivity such as Y
connectivity problems and so on and so
forth so is this natural question is is
this model redundant is it exactly the
same in computational power as the
standard models that we have but that is
refuted by showing that for the minimum
spanning tree problem in fact in this
model this problem is login hard it's NP
hard in fact log and hard whereas in all
standard models that we know of this
problem is very easily solvable in
polynomial time so there is a separation
and the separation is represented by
this one problem of course these are
just the tip of the iceberg there are
many other problems 11 can explore in
this framework and there has been some
follow-up work looking at some other
problems but a lot still needs to be
done so that is all I have to say about
network design problems and again I'll
pause for a short break if there are any
questions all right so let's move on to
the last section of the talk and this is
about cuts pacification so what is the
cut specification problem well the idea
is if I am given a very dense graph can
i specify it so can I make it if for
example if the graph initially has
roughly n square edges can I reduce the
number of edges to roughly million in
the number of vertices and then for
every edge that I retain in the sparse
afire I will make it a thicker edge so I
will put a weight on it in order to
ensure that for every cut the weight of
the cut is approximately preserved in
the specifier so I reduce the number of
edges make the edges
such that the weights of all cuts are
preserved so of course there is some
combinatorial interest in this problem
because it is not clear a priority that
such party fire is even exists but on
top of that it also has a significant
algorithmic implication so in almost all
cattle goes so for example minimum cuts
farces cut max cut etcetera the running
time tends to depend on the number of
edges this gives us a handle that can
reduce the running time from depending
on the number of edges to the number of
vertices by specifying and then running
the algorithm except that in some cases
we have to settle for an approximation
whereas the problems were potentially
exactly solvable in polynomial time ok
so this gives us a handle to trade of
accuracy of the algorithm with the
running time so how would we specify a
graph well the most obvious technique
would be to simply uniformly sample all
edges we sample every age at a
probability which is dictated by how
much we want to reduce the size of the
graph and the selected if and I just
selected we simply give it a weight of
one over the sampling probability it so
in expectation every cut is what it was
earlier does this work will not quite
because of what I known as dummbell
graphs so imagine that you have a graph
theory have to come to complete portions
but that are connected by a single thin
edge if you sample all the edges at
create one over n the graph for all you
know gets disconnected it almost always
gets disconnected because the single
edge is being sampled at a very small
probability so the lateral fix is that
this edge the single edge that connects
the two heavy portions must be sampled
at a high probability and the two
portions and the size should be sampled
at low probabilities to put this in a
formal language that probability of
sampling an edge should be dictated by
the size of the smallest cut containing
the edge or in other words the smallest
cut that separates the endpoints of th
which is the local connectivity of the
edge in particular we should
non-uniformly sample edges but the
probability of a sampling an edge is
inversely proportional to its local
connectivity if an edge has small local
connectivity for example the connecting
edge here its probability is high if it
has low local connectivity
as the edges in the two complete
portions then its probability of
sampling is low and of course we want to
make this unbiased so instead of giving
weights one the same way to every
selected edge we will give it a weight
according to this probability of
sampling so that the expected weight of
D H remains one so this is a scheme for
specification does this work well there
are two things that we need to check
first does it even produce a sparse
graph for all you know all the edges are
retained it turns out that this is easy
to show you can show that the sum of
reciprocals of local connectivity's of
all edges in a graph is at most n or n
minus 1 which means to the graph we are
getting after the sampling process is in
fact sparse but the trickier question is
is the weight of every cut approximately
preserved it so is there are so as we
saw it in expectation every cut is
preserved but how about concentration
bounds and this is a question that was
asked in the original seminal paper of
benzoin kharghar when they introduced
pacification and remained open for 15
years of course there was there was work
on specification in these 15 years when
people show that instead of using local
connectivity if you use slightly more
artificial parameters such as edge
strengths or effective conductances you
would in fact get sparsa fires but the
original question stayed open until we
show recently that in fact this is true
so the conjecture was true we sample
every edge by inverse of local
connectivity is that works and it's not
just for intellectual curiosity but this
actually proves the previous theorems as
well I should mention here that
effective conductance sampling also gets
stronger properties which we do not get
but for cut sparse efficient the theorem
that we show actually implies both the
previous theorems and brings them under
the same umbrella the two previous
results were incomparable also this
leads to better specification algorithms
in particular we get the first linear
time algorithm for cut specification
recall that one of the users of
specification was to use it as a
preprocessor and then run other cut
algorithms now of course if the
algorithm itself is not efficient then
you can't have this recipe of using it
as a preprocessor
that will become the bottleneck so it is
important to get in specification
algorithms that are efficient and we get
one that runs in exactly linear time it
was linear and there were many logs
after that several logs you know so if
you want to explicitly compute the
lambda then we have to construct the
gamma D hood tree which would take M
times n time so instead of that we use
some structural insight from the proof
to implicitly construct a different set
of probabilities that that also give us
Farsi fire so yeah are you something
with lambda he or the algorithm
sampling with different set of prod
really what we want is we need to sample
with probability says that the
probability is sum up to something small
and we get this concentration bound so
really it is a one-sided bound that we
want we want the probability to be at
least 1 over lambda e but as long as the
sum is small we are happy to have higher
probabilities so we exploit that the
probabilities are at least 1 over lambda
e but but but our individual
probabilities might be higher than 1
over lambda lambda all right so I want
to end with a general overview of my
research as I showed you our work turns
several problems in graph connectivity
I'm also interested in online and
stochastic problems problems where the
input is uncertain and then I worked on
some modeling issues in online problems
I've also worked in specific
optimization problems in the online
framework for example load balancing and
matching these are resource allocation
problems and also on more applied
problems such as newsfeed selection in
social networks and so on this also
overlaps with web-based applications for
which I have also worked in some search
algorithms and also in some networking
algorithms for example for long-distance
Wi-Fi networks adaptive channel networks
monitoring and p2p networks so this is
sort of the general structure of what I
work on some of the some of the graph
connectivity and online stochastic
problems are more theoretical some of
the parts other parts are more applied
but these are all algorithmic
applications of specific specific
systems all right so I want to end with
a couple of favorite problems so here is
one and these are very concrete problems
so I have been telling you from the
beginning that global min cuts are easy
to compute in particular that we have
linear time algorithms but I have been
cheating a little this is true if you
are happy with a Monte Carlo algorithm
if you do not want a certificate of the
dalga miss correct but if you want a
certificate the run time becomes much
worse so we really don't know how to
efficiently 35 min cuts in a graph and
this is one algorithm I would like to
another important question that we do
not know anything about is capacitated
network design in reality Network links
here but yeah well not explicitly
perhaps but by certificate I mean it
certifies correctness somehow
yes
well but still there has to be a proof
the fact that remains is itself
certificate that way guys
bamboo tube itself yourself
volume is not as you can
ruffle
he works a zero error I mean this Monte
Carlo algorithms would have a slight
error slight probability of error and I
mean one way of one way of certifying it
is to say that if we have a small error
will run something else but still I mean
be assertive certifying it somehow
okay the other problem I am interested
in is capacitated network design so in
reality Network links and nodes not only
have caused but they also have
capacities but in all classical network
design questions such as tiny trees
capacities are completely ignored even
for this very simple apparently very
simple question I give you a graph I
give you a pair of terminals what is the
cheapest sub graph that can support a
particular flow between these terminals
and we don't know the answer we do not
know any sub polynomial approximation
for even this very simple-looking
problem more broadly I am interested in
exploring combinatorial structure of
graphs to develop better algorithms for
fundamental problems and one thing I
want to emphasize here is I think
simplicity of an algorithm is also a
virtue that should be possible to trade
off with things like more quantitative
virtues like running time and with
quality of approximation competitive
ratio and so on and so forth so
everything I have showed for example are
very simple algorithms this there is
nothing complicated going on at all I'm
also interested in tech transfer between
algorithmic theory and applications can
be used this entire toolkit we have
built over 30 or 40 years to solve
various application oriented problems
and that's</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>