<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prior Robust Optimization | Coder Coacher - Coaching Coders</title><meta content="Prior Robust Optimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prior Robust Optimization</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XKY0WXAhfaQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so it's my pleasure to introduce a ball
subramanian see one acha Balu for short
value is a PhD student in university of
wisconsin-madison advice by shoot each
other and below has been an intern here
and during his internship he worked on
the so called the water level algorithm
which has had a big impact so over to
ball thanks nikhil thanks for the
invitation very very happy to be back so
i'll be talking about optimization with
uncertain inputs in the stock and in
particular i will be focusing on two
kinds of uncertain inputs one is as in
online inputs where the input comes
piece by piece and your algorithm has to
make a decision as soon as one piece
arrives and can't wait for the dead
input so optimization subject to
uncertain future is the challenge here
the second is as in mechanism design
where your input is distributed across
several selfish participants and each of
them may have their own well defined
goals which often conflict with the
optimization goal of the algorithm
designer and the challenge is to do
optimization respecting the incentives
of these selfish agents ok so I'll begin
by asking how do we formally model and
analyze these problems in theory there
are several approaches to do this but
there are two main approaches which have
gained currency and the cs literature
one is to do competitive analysis where
the algorithm you design phases the
input uncertainty but the benchmark
against which you compare yourself is
omniscient it knows the entire input to
begin with and the performance metric is
measured through what's called the
competitive ratio which is the worst
case over all inputs of the performance
of your algorithm the performance of the
benchmark so as you can see there the
opt has a subscript I which means it's
an instance wise optimal optimal but the
algorithm is the same for every instance
so it's an explicitly more powerful
benchmark than your algorithm and for
this reason no this being a robust
benchmark any positive result is great
in a competitive analysis and a good
example of a positive result is a
celebrated vcg mechanism to maximize
social welfare
and for the same reason that it is you
know a robust benchmark like this it
often leads to pessimistic results and
we will see this in the talk the two
examples I am going to talk about now a
frequent alternative in particular to
step around those pessimistic results in
competitive analysis is to perform
stochastic analysis where the idea is
that the input is drawn from a known
distribution that will be the nose the
distribution it tries to optimize with
respect to that distribution the
benchmark against which you compare is
the expected optimal for the same
distribution and the performance metric
is basically the ratio of expected
performance of the algorithm to the
benchmark as you can see they're both of
them have a subscript F which means I
know the art is not explicitly more
powerful and you can shoot for one
approximation and because of this there
are several success stories and
stochastic analysis I just give one
example myosins revenue opportunity can
ISM design is a great example of
stochastic analysis we know the
distribution but the biggest criticism
for stochastic analysis is that you need
to know the exact distribution in order
to perform this authorization and often
you have noisy data or distribution
center that could render your algorithm
really suboptimal if there is nice so
given these two extremes a possible
middle ground would be to say that the
input may be drawn from some
distribution but I as the algorithm
designer do not know the exact
distribution I only know the was the
huge universe of distribution from which
the input arrives and what I'm asking
for is a single algorithm which for
every distribution in this universe
performs approximately as well as the
optimal algorithm tailored specifically
for the distribution okay so basically
we are optimizing for the worst
distribution in the whole universe so
even if you know don't know the exact
distribution you can use the same
algorithm for all the distributions it
is in this sense that the algorithms are
prior robust because you are blind to
the exact prior distribution on inputs
as you can see there the op has a
subscript F which is which means its
distribution wise optimal tailor to the
distribution the algorithm is the same
for all the distributions
you can call it once and I mean it
depends over on line inputs it's
basically one by one in mechanism design
you have to elicit the inputs they're
typically you ask from all the people
that actually for both the examples I am
going to give in the stock we use an
even stronger metric the art is not just
distribution ways optimal it is instance
ways optimal okay so it is even closer
to competitive analysis that we are just
the presence of the distribution allows
us to step around the negative results
in competitive analysis now this can be
pushed to both extremes if you shrink
the universe to be a size one then there
is exactly stochastic analysis and if
you allow the universe to include
arbitrary distribution every
distribution possible then that is
competitive analysis and the goal in
Prior robust optimization is to develop
algorithms which can handle as rich
universe as possible typically these
distributions the universe has some
structure it is not an arbitrary bag of
distributions for example all possible
product distributions over inputs and
things like that okay the question is do
we have any non-trivial algorithms with
this model there are some nice examples
for example the classic mechanism
designed result you to build on
Klemperer and its generalization by dang
what no tariff garden in yon and in
purely algorithm fitting cities are due
to dinner and hayes i will talk more
about this later but these are examples
of pride robust optimization I'm not in
this area so for a basic version I
thought about the errors of consumers
ups or at a lot of the fonts or there
I'm going with the performance of the
algorithm there is an objective function
which you have and I'm asking how close
you can get to the objective function as
compared to the optimal value of the
objective function I mean harder so
performs the computing speed I know yeah
I mean it's it's not a question of
runtime he wanted to be polynomial but
apart from that the real performance
with respect to the objective function
optimally ok so that is a prior reverse
model so the plan for this talk is to
present prior robust algorithms for know
two fundamental seus problems and the
takeaway message is that several
interesting problems lend themselves to
this kind of prior robust algorithms and
you must look for them and develop them
so in part one of the talk i will
present a prior robust algorithm for a
resource allocation problem and in part
two I will do mechanism design I will
present a prior robust truthful machine
scheduling mechanism and I will conclude
with some future research directions
yeah so part one is basically I mean
it's based on a couple of joint works
wonders with nikhil dave normal jen and
chris wilkins others with nikhil day
Lauren yo CEA's are so this resource
allocation framework the formal
framework I will present in a bit but
this framework actually is very gender
and captures a lot of problems motivated
by internet but icing so i will quickly
go through a couple of examples 1 is the
display ads example if you visit any
website popular website you will see at
least one advertisement like that and at
a very high level you can think of this
display ads as proceeding in four phases
in phase one the advertiser which in
this case universe to phoenix signs a
contract with the publisher like msn
saying that i want so many impressions
in this period two people between the
age so and so on etc now msn science
contracts like this with several
advertisers and then contracts are
signed a user web page arrives and
Emerson has to decide basically in the
vacant spot which advertisement to put
yeah then you deliver these ads plus the
content so this third phase is basically
a resource allocation face you have to
decide which advertisement to put their
vacant spot and this next example is
basically you know something
evolves in a ton of times if you search
for a query apart from the organic
results that you get you also see some
paid search results and this can also be
thought of at a very high level
proceeding in four phases except that
there is no contracts here advertisers
submit bids and budgets and when I use
the query arrives the search engine has
to decide which ad is to be matched
basically the phase three is the
resource allocation face so now go into
the formal model of this phase three
okay so the model I'm going to talk
about is due to Methos a very was
Iranian vazirani it's basically an
online model of repeated auctions there
is no incentives here it's purely
algorithmic there is a bunch of
advertisers each I've specified some
budgets these budgets are the maximum
amount you can extract from them over a
day or some time period and they also
specified bids on various queries and as
soon as a query arise the search engine
has to decide whom to allocated to once
you allocate you subtract the budget
from that I could I say and you keep on
proceeding like this ok so the formal
model is that you have n advertisers
advertiser I has budget bi and queries
arrive as follows so there is this huge
graph right hand side of the graph is
all possible queries and at every step
you are drawing a query of independently
and identically according to an unknown
distribution ok so the universe of all
distributions is basically universe of
all possible you know Heidi
distributions over the queries I do not
know the distribution as the algorithm
designer I am going to call PJ as the
probability with which query J is going
to be drawn I do not know PJ and bij is
the bit of advertiser I for query J ok
so your goal is basically to get an
algorithm which maximizes the revenue
which is the sum total of all the
allocated bits respecting the budget
constraints so that is the model is the
model clear or the greys are drawn ok
so it turns out that all the results for
this problem or parameter is where the
significant parameter called the maximum
bid to budget ratio it is not difficult
to see why this is the case and I
illustrate this through a simple example
you considered a regular matching
problem regular matching means all the
bids are one all the budgets are one so
you have just to advertisers and three
queries let us say that two queries are
going to be drawn online let's say this
is the probability distribution force
query one arise first query to arise
first then irrespective of what you do
there is a constant probability that the
second query cannot be alerted to
anybody right you can this guy's budget
is over and you can basically get a gap
between optimal online and offline
revenue the point is that if instead of
the bids being one suppose they were won
by thousand then each mistake you make
is not going to be as costly as the bits
being one the bid's being one is a big
mistake if it's one by thousand is
lesser mistake so obviously if you have
smaller and smaller bit to budget ratio
you can get better and better
approximations so all the approximation
ratio you have to be parametrized
through this bitter budget ratio and I
am going to call this gamma laughter
okay so i will present what is no no so
ms VV who coined this problem study this
problem in a competitive sitting and
what they show is that you can get a 1-1
by approximation in the case when the
bit to budget ratio is 0 right so this
is the best case you can ask for in the
bids are infinitesimally small compared
to budgets and even then you can get
only 1 minus 1 by E no randomized
algorithm can go beyond 1 minus 1 by E
okay so that is for the competitive
stating and in a recent result
concurrently with our work Seidel a
vomitar JIAJIA guy and hot yogurt show
that in a stochastic sitting which means
you know the distribution you can get a
1 minus square root gamma approximation
where gamma is the bit to budget ratio
okay what this means is if gamma goes to
0 then you can get arbitrarily close to
one circumventing the bound here that
you cannot go beyond 1 minus 1 by E okay
the question we ask in this work is you
know can you get the same 1 minus square
root gram approximation
through a prior robust algorithm which
means you use the same algorithm
irrespective of what the distribution is
ok so here is our results the same model
iid model we do not ask for the full
distribution but we ask for n parameters
from the distribution ok i will explain
what these n parameters are later but
the whole distribution itself could have
an infinite support but we only ask for
a few parameters of the distribution and
given that we get the same 1 minus
square root graham approximation we also
show that you cannot go beyond 1 minus
square root gamma even if you knew the
distributions and does not appear in the
approximations at all and for the angle
you've ended up advertiser's yes sir
yeah n is the number of advertisers n is
the number of queries but the
approximation ratio is independent of n
yeah
so that is the result so this same
problem has been studied in a prior
robust model with a slightly different
universe than the iad universe of
distributions and in order to put our
results in context I will briefly go
through that model also before giving
the proof of this result okay so the
model is the same as this model except
that instead of queries being drawn iid
an adversary initially picks the set of
em queries to arrive after that these
queries are sent according to uniformly
random permutation okay so an iid if you
condition on the set of queries to
arrive they arrived in uniformly random
order correct so you can think of the
iid model as a distribution or random
permutation models because the ID nobody
is conditioning on which queries to
arrive so I ed is a distribution of
random permutations and for that reason
any approximation ratio that holds for
this random permutation model also holds
for the iad model but the reverse is not
known and there is no separation results
known for this but the distributions are
unknown so basically the unknown ID
model is really close to the random
permutation model to one of them is
stronger than other yes the queries must
be distinctive breaking with splitting
operations
it could be possible it's repetitions so
here is what i already showed so the for
the random permutation is actually the
first result which showed that you could
get arbitrarily close to one was the
result by dev nur and hayes and this is
the dependence on n and m show and
thereafter this whistle there were
several other results it's different
trade-offs basically the point is that
all these depend on n and other
parameters put for iad model we
completely get rid of n and get this one
minus square graham approximation and it
is an open question actually whether or
algorithms actually extend to the random
permutations also the first question is
whether random permutations actually
permit this kind of approximation
without any dependence on n but then you
could ask whether this algorithm itself
actually extends to random permutations
in your mobile bounds for the random
there is no known lower bound except for
this itself HS for Heidi ok so that is
that is the comparison for between I ID
and random permutations so I will now
prove this result this one minus square
root gamma here are some simplifying
assumptions just for the purposes of the
stark first I resume with the bids are
binary 0 or 1 the second is that the
distribution is the uniform distribution
or it is that every query arise with the
same probability of 1 by m m is the
total number of queries for the third
assumption I'll need a quick definition
I'm going to define what is an expected
instance for every distribution this is
a single offline instance where
everything happens as per expectation ok
so you have query J which arise with
probability P J in the online model
which means the expected number of times
it should arrive is m times PJ right so
in the offline in instances expected
instance you have exactly mpj units of
query j so for every query you have an
expected number of units of that query
in this offline instance in particular
for this example m times PJ is exactly
one so you have one unit of every query
the support of the distribution that is
the single offline instance and you
cannot compute it if you do not know the
distribution right you know the
distribution this is often instance and
the assumption is that you have a
perfect matching in the expected
instance the optimal solution to the
expected instances perfect be matching
which means that every advertiser is
budget is fully consumed I will examine
all these assumptions later but for now
say that every advertises full budget is
consumed okay so given these assumptions
what can you do if you knew the
distribution then you can run this
algorithm that is why I am calling this
hypothetical algorithm because you do
not know the distribution so here is
what the algorithm does it firstly
computes the expected instance for which
you need the distribution rate you need
to know the number of theories which
arrived and it finds the optimal
matching for the expected instance so
that is you find the matching and
tabulated as a table which Perry goes to
beach advertiser it stores the stable
and it uses the solution for the online
problem okay so what does it mean that
says
Qin of queries arise like this when
three arise it goes to the table and
sees it should be given to advertiser
seven you give it there and one arrives
you do the same thing go to the table
and see when three arise again it says
advertising sudden it will give it
advertiser seven even if by this time
advertiser 7s budget was exhausted could
be that 76 mph is exhausted but that is
why it is an oblivious algorithm it does
not depend on what has happened earlier
it is a waste of money to give it a
seven but I will still give it to seven
it's a simple non adaptive algorithm it
won't be he doesn't mean there is
nothing like feasible I mean you can
keep on giving it to the advertiser but
he won't pay beyond bi you won't get the
money
it's a waste of a PA that's all so hyper
typical yeah
and gloominess to the back
the claim is this simple non adaptive
algorithm actually gets 1 minus square
root gum approximation and for the
assumptions I made gamma is just you
know gamma has bid by budget of the bids
are all one so it is one by the minimum
budget right so I have shooting for a 1
by square root on by budget
approximation 1 minus square root
remember shit so here is the proof for
this let's analyze this step by step in
any given step let's see what happens
what is the probability that advertiser
IE gets a query well I said in the
expected instance you have full budget
consumption which means advertiser I had
bi query is going to him okay so if any
of those bi queries arrived then
advertise that I would have been given
that query it's a total of em queries so
the probability that advertiser I gets a
query is bi bem even step okay so now
this algorithm is basically like a balls
and bins process at every step it is
throwing a ball to advertiser I with
probability B I am and what is the
revenue at the end of all the steps
because I have assumed binary queries
that revenue is basically the number of
queries which is matched but truncated
at bi like you asked there is no use of
giving beyond bi and this this is
basically then a truncated binomial some
we know that a truncated binomial some
has a square root loss and you basically
get the result you sum over all these
people and touch the total revenue so if
you know the distribution you can run
this simple algorithm there are you want
to point out two things about this
algorithm one is the computational
burden of this algorithm increases with
the size of the support which means to
compute the expected instance in each of
the full support and compute the
expected distance so the support grows
larger and larger the computational
burden of this grows larger and if it is
infinitely the rendered infeasible
secondly this is oh no also this could
be a randomized algorithm which curve
because the table you get for the
example I had it is a proper integral
table but it could be fractional
so this make sure your computing is the
ratio
the algorithm achieves to watch exactly
what the algorithm achieves to what you
can achieve through in an offline
fractional solution optimal offline
fractional solution says you say and
thus essentially the same as this thing
that you computed in a glass yes exactly
more than the expectation of you so it
is basically the opt for the expected
instance which is larger than the
expected expectation of opt because the
expectation of off will be a feasible
solution here so the art of the expected
instance will be only be larger okay so
that is why i said in the original ides
a stronger metric which is an instance
where is optimal let's say we can get
offline optimal solutions okay so this
is what you do if you know the
distribution but you do not know the
distribution what do you do so consider
running the following hybrid algorithm
which is a hybrid of the following two
algorithms one is the hypothetical
algorithm they just presented other is
an algorithm a which is going to be
inductively defined now and heck is a
hybrid of PNA which runs p for the first
five steps and a for the remaining i
sorry a for the first I steps and P for
the remaining I steps it's a followed by
T so let us assume that a has been
defined for the first I steps and I am
going to define what it does add I plus
1 at step once the query arrives it
picks the following advertiser the
advertiser which maximizes the current
steps revenue plus the expected residual
revenue which you achieve if you run the
hypothetical algorithm for the remaining
M minus I minus 1 steps so you look
ahead and decide the best thing to do
now now by definition the hybrid H i
plus 1 will get a higher revenue than
the hybrid hech I because H I plus 1 and
H I differ only add I plus 1 that step
and the heck i plus 1 basically runs
this algorithm a which looks ahead and
makes the decision whereas I hi does
this pure the hypothetical algorithm it
actually looks at the table and does
something and clearly this OP doing
looking ahead and doing something is
better than looking at the table and
doing something
no this is used to assume that you know
the distribution or you know open I mean
I want to show that you can do this
without knowing the distribution but for
now suppose you could do this suppose
you could look ahead and make this
decision then I am saying H i plus 1 is
better than H I just the i plus 1 at
step is different but by definition it
is better now you can slowly change this
step by step just like you proved
indistinguishable rating cryptography
and then basically the point is if you
run the algorithm a all the way it is
better than running algorithm p all the
way definition and i already showed that
p gets this square root loss which is
what we wanted me see also gets it the
question is can you run this algorithm a
because i said you have to look ahead
into the future and decide this expected
computers expected revenue for the key
idea is that implementing the
hypothetical algorithm actually requires
knowledge of the distribution but just
estimating its revenue does not require
knowledge of the distribution okay and
all we need is to estimate the revenue
why is that because i said this
hypothetical algorithm is a simple balls
and bins process which throws a queried
advertiser is probability be i am i know
the budgets bi I know the number of
queries em so at any given point in time
given the number of remaining queries I
can estimate the residual revenue with
the number of queries remaining and this
these numbers is the sum of binomial
expressions one for each I so I can
estimate this exactly and the upshot is
that you can run the hybrid algorithm
without knowledge of the distribution
okay all I ask for is these numbers one
point is that it is not enough to
recognize that this P can be interpreted
as a balls and bins process at
probability behave ahem even after
knowing that you cannot implement p
itself because you are not given up a
bunch of balls and asked to play a balls
and bins process here as a queer you
arrive you have to do that whom to
assign it to so you can't run this
algorithm p but because you can estimate
the revenue of p can run some other
algorithm which does as well as p
so there is the point so I will wrap up
this analysis by examining some
assumptions I made one is I said it is
uniform distribution what if it is not
then this is turns out to be not a major
assumption the expected instance will
now have a fractional optimal solution
instead of this integral optimal
solution that is fine it only changes
the E or the hypothetical algorithm does
it will be a randomized algorithm but it
doesn't change anything about how the
algorithm a itself is defined
inductively it's let Syria all these
algorithms doesn't depend on the artists
of the distribution right if the
distribution was very easy say had very
low entropy presumably think that you
can just the first verse to learn it and
then you
yes so I add these algorithm i present
it doesn't depend on the complexity
doesn't depend on distributions you can
do better means no no even if you know
the distribution you can't tell even if
you know the distribution you can't cook
you on the square root class because I'm
comparing it with the offline optimal
solution it's not the online optimal go
to the afferent solution receive a move
that often solution can be better than
the store for some distribution
what does it mean to say offline we'll
do better than square root you have
suppose you know the distribution yes
yes so is the square root square root 1
minus P oh yeah it doesn't it doesn't
hold for every distribution yes yes you
could yeah it's only that there are
distributions for which you cannot go
beyond on a square okay and the second
assumption I made is that the bids are
binary and this is a more serious
assumption now what happens if they are
not binary well there could be anywhere
between zero and one now you can't
interpret this hypothetical algorithm as
a balls and bins process anymore because
you're throwing balls of different sizes
which is like a fractional balls and
bins process and this complicates the
hybrid argument and you basically have
to introduce a third algorithm F and do
a two-level hybrid argument to give this
proof I'm not going to go through that
but you know you can get the same
approximations for arbitrary bits it
lacks ations the third assumption is
that I said the budgets are fully
consumed in the expected instance what
if you do not get full budget
consumption you consume something which
is lesser than bi the only difference is
then the balls and bins probability now
is see i buy em instead of bi bi okay
you do not know the CIS and this is what
our algorithms arts for these are the
end parameters if you give me the
consumption optimal consumption in the
expected instance for every advertiser
if you give me the sea ice then I can
use them to estimate the residual
revenue and run the hybrid algorithm
okay so those are the end parameters we
need if you give me them by L give what
I promised here is an interesting open
question which is immediate from this
what if you simply assume you make a
wrong assumption just say that CI is
equal to bi rite and then run our
algorithm as it perform well we are able
to prove that for some special cases
this already does well and if you are
able to prove that for all cases does
well then you basically completely
eliminated all dependence on the
distribution you're doing as well as
known dissipation case I think they're
big great to prove that if you
I mean in fact this you wouldn't expect
to actually know these CI is right but
maybe you could estimate sure you can
only ask you made a stand-in for the sea
ice yeah i mean i don't think that if
you if you know the sea is within
epsilon yes yes if you if you can know
the CN certain epsilon these results
also will be within epsilon yes okay so
that is the immediate open question from
this now I'm going to generalize this to
a much more general resource allocation
framework which is a real present later
and this basically captures all the
special cases I said and then online
combinatorial auctions online network
routing and so on so the model is that
you have em requests like them queries
the event resources advertisers the
society has budget bi the major
difference is that now every request
apartment instead of just consuming
resource from one resource can
simultaneously consume resources from
all the resources available ok so I'm
going to introduce a third number which
is an option to serve a given request
and if you serve request a with option
care you get a profit of wjk and it
consumes some amount of resource from
every advertiser consumes bij cave from
advertiser I or the resource I to have a
complete the concrete example in mind
think of the request says think of you
having a graph and the request says
request to route from some source and
sink in the graph the resources are the
edge capacities in the graph and the
option is basically which path you have
to choose from the source to the sink so
you have an exponential number of paths
to choose the number of options is
exponential in n and obviously different
paths consumed from different edges so
that is the consumption information so
in general you have bi JK amount of
consumption from resource I okay so this
is the general resource allocation
framework and here is our results again
the inputs are drawn iid we do not know
the distribution in this case it is
completely unknown distributions we do
not even ask for any n parameters in
other words you can use this algorithm
for the previous results I said the
previous problem i said only thing is
the results are slightly words here you
have to depend on n here unlike the one
minus square root gamma i presented
earlier
the optimal depends on n and you also
show a lower bound matching which says
that you cannot get beyond 1 minus
square root gamma log n for this general
framework this is a complete unknown
distribution and also these results hold
with high probability not just an
expectation they hold with the
probability of 1 minus square root gamma
log n okay the high-level idea is to use
hybrid argument again but the algorithm
is fundamentally different we use hybrid
argument not on the expected revenue
they presented but hybrid argument on
the exponential moment generating
functions you stand Chernoff bounds to
compare it with what's known of the
previous best was 1 minus square root
gamma n log KN and i said k is typically
exponential in n like in the paths
example that expansion umber of paths so
if you have to then here if you bring an
end here initially really square root of
n square or an N sitting here in front
of the oh and the N is gone in a result
basically so n is typically large in
these examples but this is for random
permutations which as I explained is
slightly stronger than unknown ID model
so any immediate question here is does
our algorithm next into the random
allocation model with the same
approximation factor so that's the
summary of part one for the special case
of AdWords we get matching up on a lower
bounds but we have you asked for n
parameters of the distribution and for
the more general resource allocation
framework we again get matching up front
lower bounds but this is with completely
unknown distributions so this work was
done when I was an intern in 2010 with
nickel and thanks to nikhil you're able
to speak to the product groups and they
basically use this algorithm now for a
message display ad serving engine ok but
pretty much the exact problem i said
this players problem and it has been
globally operational since the summer of
two thousand eleven so these are the
open questions which are already
presented but I will repeat them one is
to see whether the iid and random
permutation model have any separation no
separation is known
you know my own guess is that you give
in our algorithms actually work for
interpretations but we don't know either
is to remove the dependence of the end
parameters the sea ice ok so that's it
for part 1 if you have questions you can
ask questions about part 1 no I will now
be moving to mechanism design
ok so now talk about a prior robust
bootiful machine scheduling mechanism
this is joint work with shuchi chava
largest hotline and David Malik so the
problem is the very well-studied may
expand minimization problem in computer
science you have n jobs &amp;amp; M machines and
jobs are different times on different
machines and your goal is to find a
schedule of jobs on machines to come
minimize the completion time of the last
job so if you have this jobs in machines
machine to is basically the makespan
defining machine for this schedule ok
and this is called an unrelated instance
because for any given job the runtimes
on different machines are completely
unrelated so this is the problem we
study with the twist that you're going
to study this in a strategic setting
this problem was introduced in the
seminal paper for algorithm a perkin
isn't designed by Nissan and ronan where
they say that these machines are
operated by selfish workers and the
runtimes of jobs on machines are
privately held by machines and you do
not know these runtimes to begin with
and what we ask for is a mechanism which
is not just a schedule of jobs on
machines along with that some payments
which you transfer to these machines to
incentivize them to truthfully report
their runtimes okay this despair of
scheduling plus payments is the
mechanism and the machines have their
own selfish objective which is to
maximize the payment they received from
you minus the work they are forced to do
which is the sum total of the runtimes
of all the jobs they are asked to run
okay the solution concept I ask for is
dominant strategy truthfulness which
means a mechanism which irrespective of
how other machines behave will make each
machine each machine is incentivized to
report its to run times ok this is the
strongest notion of truthfulness could
ask for in mechanisms ok so what's the
motivation to study makespan in a
strategic sitting there are several one
is that it is a central computer since
problem specifically in the context of
resource allocation even from an
economic point of view makes plan is
you could think of me expand as
enforcing some kind of fairness after
all makes mine is load balancing between
machines so it's like a min Max fairness
what makes it really interesting to me
is that it is a nonlinear objective
unlike other traditionally studied
objectives in mechanism design like
revenue and welfare which are all linear
and for this reason what is possible for
makespan is very different from what is
possible what is possible for makes my
very different rewards for welfare or
revenue and here are some differences
for example the Social Welfare objective
which is the most well-studied objective
in mechanism design you know you can get
tooth fulness plus optimality through
the celebrated vcg mechanism okay if you
didn't care about computational
considerations for a minute then you can
get a truthful mechanism which optimizes
Social Welfare all the time but
truthfulness plus optimality itself is
impossible in makespan as Nissan and
Rowan insured in the original paper this
means that even if i give you unbounded
computational time you cannot give me a
truthful scheduling mechanism which will
optimize the mixed plan okay there are
some hardness results there and I will
trace through the entire hardness
results in the next slide another
striking difference is that the kind of
impressive reductions that was possible
in Social Welfare where you take an
arbitrary approximation algorithm and
you inject truthfulness into it we can
morph an algorithm into a truthful
mechanism and it will preserve the
approximation guarantee of the original
approximation algorithm such kind of
impressive general reductions are not
possible even the simplest of settings
in makes them this again is a big
difference and this problem was
basically introduced as a challenge
problem for mechanism design by Nissan
and Ronan in the seminal paper and since
then this has become a hot area to work
on and let us see what is known so
nissan and Ronan give a simple em
approximation in their original paper
but much activity has been on lower
bounds problem so the author in their
original paper show that no
deterministic mechanism can get anything
beyond a to approximation even if you
give unbounded computational time there
is no computational restrictions here
and this was later improved two
randomized I can
also by volume in Shapiro and then this
pound was improved several times later
by christodoulou copious and Vitaly
later become CBS and Vitaly to recently
where a schlocky dobbs in skin LA we
showed that the upper and lower bounds
match basically is a very big hardness
listed here although the hardness result
holds for only a restricted class of
mechanisms called anonymous mechanisms
anonymous mechanisms means that you
should not use the name of the machine
to make your decision which means if two
machines swap their run times then how
you pre those two machines also should
be swapped okay so given this backdrop
of hardness results we ask in this work
if you make stochastic assumptions can
you get a prior robust truthful
mechanism which gets good approximation
for mix then given all these others
results in the competitive setting
that's the question in this work so i
will now formally present what is the
distributional model for scheduling okay
so you have n jobs in a machines and the
runtime of jobs a on machine I is x GI
and these x-ray eyes are independent
random variables and here is what I as
you about the distribution the runtime
distributions are identical across
species for a given job the runtimes of
the job on the different machines or iid
random variables but the jobs themselves
could be non-identical you see there so
basically the machines are a priori
identical but if you draw the runtimes
for these distributions any specific
instantiation of these runtimes will
lead you to an unrelated instance okay
so in so I'm going to use a short form
for machines being iid and jobs being
non iid but this is what I mean for some
results i also need that the jobs are
also eid but i will present them when i
press the results the goal is to
minimize the expected makespan like
before okay
and no I want to get a prior robust
mechanism which means for all possible
distributions we just satisfy these
conditions independence and ID so this
is what i showed you already that the
upper and lower bounds match in the
competitive sitting for a restricted
class for the lower bounds oh and the
stochastic setting is completely open
which means if i give you the
distribution can you do something better
that's not known here is our results we
give a prior robust truthful mechanism
which gets an order of n by M
approximation for up half n is the
number of jobs and m is the number of
machines m for machines in particular
when n is equal to M this is like a
constant factor approximation and this
lower bound there are also holds for n
is equal to n so basically you can get a
constant factor for those the benchmark
for us this acht half which is basically
the non truthful and it's allowed opt is
allowed to be non truthful there is a
non proof will expected optimal but it
is all out use at most half the machines
so it's like a resource augmentation
result okay so i will explain this
benchmark opt half in a bit but this is
our first result and later we show that
you can improve this to sub logarithmic
factors if you assume that the jobs are
also a 80 and make further distribution
assumptions so those are our results
I'll now go through this app have
benchmark and explain it a bit as i said
opt half means that our biz allowed to
use at most half the number of machines
which you are allowed to use so you know
if you get good approximation to up half
it means that you can do well with
resource augmentation if you double the
number of machines you can perform
approximately as well as optic in
general this octave could be much larger
than opt but we show that for a large
family of distributions up half an opt
or within constant factors which means
that it's really not a razor
augmentation for these distributions
because it's only a constant factor away
for Mona Don hazard rate distributions
intuitively these are distributions
which have tails no heavier than
exponential distributions which includes
uniform normal and exponential this is
basically true and even for a larger
class of distributions much
the class of half not paid within
constant factors I want to talk about
the class now that is about the
benchmark of half so i will now sketch
the proof of how we get this order of n
by M approximation particular i am going
to assume that n is equal to m and i am
going to shoot for a constant factor
approximation in the proof sketch the
proof to get a flavor of what is
truthful what is possible in this multi
parameter setting let's begin with a
very simple mechanism which is truthful
and this was introduced by a Nissan and
Ronan this mechanism is called the min
work reckon ism it just minimizes the
total work done by all the machines
which means it tries to minimize the sum
total of the runtimes of all the
machines this is nothing to do with
makespan so whenever you know job it
takes a job and basically schedules the
job on the machine which has reported
the smallest runtime for that job what
it means to minimize the sum total of
all the runtimes and it pays the machine
in the jobs runtime on the second
quickest machine so this will result in
a truthful mechanism you should just
take my word for it I won't prove it yes
independently exactly oh yes and that
also minimizes the total global runtime
also right just you know it is
independent for each job but that's
exactly what you will do if you want to
minimize the total you need not do
anything global actually you can do
locally to minimize the tour
the Sun this is exactly BCG basically it
is BCG forum you know we are doing a
reverse auction it's a procurement
auction here so instead of maximizing
welfare you are minimizing the burden
basically so you're amazing the total
runtime relation principal doesn't apply
to same objective focus that's why you
can fight
no the regulation principle applies but
that just says that you need whatever
you can do with a non truthful mechanism
we can also do with the truthful
mechanism but I mean here this is this
objective is a different objective right
you are minimizing the total work my
goal is to minimize makespan I am simply
giving a mechanism which is just
truthful and it has some non-trivial
approximation for makes man that is all
you can use this mechanism for makespan
but it will be bad that's what I am
going to talk about so this you know
simple mechanism already gives the M
approximation where m is the number of
machines this simple thing because the
makespan is just a max max is always
lesser than the sum sum of the quickest
run time is always less than some of the
runtimes of all jobs in art which is at
most M times the make span of ok this
was initially can get an approximation
has also truthful so what else is
truthful as to get a sense of what is
truthful it turns out that if you
minimize the total work over a
constituent restricted domain of
schedules that is also truthful okay you
put a pre-specified restriction on what
kind of schedules are possible before
you ask for the runtimes then try to
minimize the total work over this
restricted domain of schedules and that
is also true so the natural question is
why can't you try this simple mechanism
first maybe does better in a stochastic
sitting because that was for a
worst-case setting that I gave an Emma
proc you know I showed an approximation
of reason and Ronan let us consider a
simple example you have em jobs em
machines everything is iid all jobs and
machines run times are uniformly drawn
between 1 and 1 plus epsilon now the
quickest machine for a job is a
uniformly random machine right which
means that the min work mechanism when
it assigns a job to kick ass machine
it's basically assigning a job to a
random machine so you're basically
throwing balls randomly into bins and a
fundamental fact when the balls and
Vince analysis is that the heaviest
loaded bin has log my log log and balls
the implications in a logarithmic
approximation is trivial but we are
looking for much better approximations
like constant factor or at least sub log
to make approximations so what is the
problem with this min work mechanism it
is basically overcrowding machines like
what is happening there it gives too
much importance to putting a job on the
best machine quickest machine possible
so a natural next step would be to
explicitly prevent this overcrowding so
what if you say that you should not
schedule more than K jobs in any machine
okay so that is a restricted domain of
schedules you run the same min work
mechanism you try to minimize the total
work over this restricted domain of
schedules of at most K job spur machine
that's what i call min work a mechanism
ok minimize the total work with at most
k Jasper machine this is truthful
because the restricted domain of
schedules and it's also polynomial time
because it's a minimum size matching
problem machine has a degree of K the
claim is that this min work a mechanism
gives a constant factor approximation
for our behalf
the proof idea is basically roughly
these two steps first is that men work a
mechanism results in a roughly balanced
schedule unlike what could be for the
lopsided schedules that occurrent the
minimal mechanism secondly in spite of
the fact that I put a restriction and
the number of jobs if you go to a
machine it is not necessary that here
after a job goes to its quickest machine
still it basically a job roughly goes to
its quickest machine k is the number i
am going to put the restriction i am
going to put that at most k jobs can go
to any particular machine you cannot
crowd beyond that they are optimizing
I'll tues kk i will make a ten basically
yeah i mean k is a constant and i can
choose k to get a constant factor
approximation our results use k equals
10 yes okay so m is equal to n but if
you have one of more jobs than machines
then 10 times n by M number of chops
that is the restriction i put this is
the average basically 10 n by M so here
it is 10 and that is why the order of n
by M factor comes out
okay so the second point is that though
job necessarily not another lady goes to
its weakest machine roughly goes to its
quickest machine so here is the proof
sketch i will just present the two key
lemmas the first lemma that the
probability that a job goes to its I at
favorite machine if it ranks the
machines according to run times is
basically exponentially decaying in the
rank of the machine so it does not go to
below in its preference order the second
point second number basically nicely
complements the first lemma that placing
a job on this is quickest machine is no
words then basically fight to the I
basically placing fighters I independent
copies of a job on their quickest
machines so are basically the only thing
you can assume about art is that it
always puts the jobs on the quickest
machine so that is the best thing that
is possible for art but it is the
quickest among em by two machines ok so
this higher order statistics phrased
purely as a probabilistic result this
says that the i8 order statistic is
almost stochastically dominated by an
exponential number of five to the I
independent copies of first order
statistics the i8 order statistic is
among em independent draws with the
first order statistic is I know and he
cared with M by two independent Ross
there is the purely probabilistic
question of what we are showing and
these two basically you can see that
these two coupled together to give a
constant approximation because the I is
X name the rank to which the job goes is
exponentially decaying in a and this is
we have an expression number here so I
will just summarize part two then the
machines are iid when the jobs are non
iid we give a prior robust mechanism
which you know being blind to
distributions it can give a constant
factor approximation n is equal to M or
order of n by M proxima shin and as I
said opt half an dr. comparable to a
large class of distributions it's not
resource augmentation for those
distributions at least and when the jobs
are also ad you can further improve this
to sub logarithmic approximations again
with the prior robust mechanism so I'll
wrap up with open questions one obvious
open question is how broad a class of
distributions can you pack
with prior robust mechanisms okay in
particular can you relax this jobs being
iid here and can you can you have non
iid and still give the sub learn
approximations other question is I mean
this is this is by much beyond the scope
of this work at least you know if you
what if you relax the ad assumption on
machines everything asset breaks down
there and you need completely fresh
ideas the second improvement second go
open question is you know can you
improve what is possible for the
competitive sitting where as I said
upper on the lower bounds match at least
for this anonymous mechanisms can you
slightly relax the model and get
positive results okay here is one
relaxation possible computer jobs you
know computer programs need not
necessarily run on one machine you can
run the same program with multiple
machines and you can take the completion
time to be when the first copy of every
job completes its not a physical
constraint this will help in incentive
compatibility you can ask what's the use
of scheduling job on multiple machines
but this will help intensity
compatibility for example you could
schedule all jobs and all machines that
is a possible schedule that is truthful
only thing is it does too much work if
you also put a restriction on the
maximum amount of work that can be done
and allow a job to be is scheduled
multiple machines can you do something
better yes but it could be possible that
you know you want to put a restriction
the total work to be done if you simply
schedule all jobs and all machines then
that is possible to get the best mix
than possible by different you know just
adjusting the order that is fruitful no
incentives for any machine because you
do not use the runtime of machines at
all but it does every machine does too
much work we put a restriction the
maximum work that can be done and that
is one way to circumvent the lower
bounds competitive setting so I will
just briefly mention a selection of
other results that have done before
measuring future research directions one
problem that have recently worked on is
the design of optimal crowdsourcing
contests in a crowdsourcing contest
basically a principal or a firm has a
task to be completed and it advertises
this task to a crowd and it puts a
reward
and these users or the crowd basically
have submissions for this task and you
evaluate the task according to some pre
specified criterion and split the
rewards among the winner or some winners
so the obvious question which arises is
now what format of a contest to do use
to incentivize users to give good
submissions in particular the question
we ask you this work is now how do you
optimize the quality of the best
submission you receive a good example to
have in mind is the contest which many
of you may know that Netflix ran
recently Netflix ran a contest to
improve the prediction accuracy of their
algorithm they wanted to basically
predict how much a given user will like
a particular movie based on how much he
liked a given set of previous movies and
they promised that any user who improved
the prediction accuracy of their
algorithm first user to improve the
prediction errors of their algorithm by
ten percent will get a million dollars
this contest is over now so that is one
model so what is the best contest format
so for the model we use we say that you
know a result is basically that there is
a very simple contest format which
optimizes the quality of the best
submission which is basically you get
all the submissions and segregate these
submissions into different buckets to go
back to the netflix example you know you
segregate all improvements between ten
and twelve percent in one bucket twelve
and fourteen percent in one bucket and
all those users who fall in the biggest
bucket will basically share the reward
equally and that is the optimal for
madness what we proved given the whole
amount
ok so there ok so there are two things
one is what is the best thing to do if
you're in a static contest static
contest means you should not you know
you should not decide how much amount
you are giving to which person after you
have received the submissions this is
what TopCoder does for example the
trends these architecture competitions
where it says two-thirds of the award
will go to the best submission one third
of the order will go to the second base
submission among all those static
contest we proved that it is best to
give everything to the winner winner
take all is the best but obviously a
dynamic contest where you are allowed to
see the submissions and then decide what
to do later will do better right for
example when I said ten and twelve
percent 1 and 14 are different buckets
the number of users who fall within 12
and 14 will basically very base to the
submissions after that if I do something
the dynamic is always better than
statistic you're saying this is better
than giving everything to the highest
yes so why is that well I mean this the
buckets we decide are basically based on
the distribution and so for some
distributions it will turn out of the
buckets are all independent there is no
buckets at all which means that
everything will go to the winner that is
also possible for many do for so-called
regular distributions it might turn out
that there is no bucketing but if the
distribution doesn't there is a mano
tanha City involved in the allocation
function and we do what is called
ironing of the allocation function that
results in pocketing but suffice it to
say that it could happen often that you
will give everything to the highest
bidder and highest corner wow so you
seem to be claimed there are some
situations where you don't want to do
that
more interested in and he will give in
to the definition so the distribution
different distribution basically why
that arises is a distribution so the
agents have skills basically so you can
think of skill as the rate at which an
agent can be useful work the quality of
the submission is the product of the
skill and the effort of the agent is a
linear model so skill v times e effort
is p which is the quality of the
submission and you have distribution
over these skits so you can map it to
auction theory we basically extend
auction theory to model this now the
only thing is an auction theory for
revenue you study the sum of the
pavements you study the maximum payment
so you could implement the same thing
instead of you know dividing all this
revenue in the highest bucket equally
you can also break ties but you can we
should break ties basically in a
consistent manner instead of dividing
things equal you can always say that if
all these people follow this bucket I
will always give everything to
advertiser one if he falls in this
bucket then to advertiser too so there
is a way to run these things without
splitting rewards equally also which
works usually sort of continues unique
optimal solution because of that is not
this totally
Peterson who submitted the best solution
it's also not necessary no no even for
continuous distributions it could turn
out that you know you you should have to
display it rewards equally between
multiple players who fall in the highest
band nordy incentivize them what is he
who is a example for you can think of a
quick example for why this lil rick
might be better to do that well then
just give it to the best
so depending on distribution will so if
you be that some people they don't even
want to participate they don't have a
very good chance at winning by doing
this you are giving more
you have
yes so I have a mathematical answer put
it I don't know if that gives the
intuition so let's say I mean I will
give from auction theory what is going
on basically so if you truthfulness in
single parameter settings okay the
single parameter settings maybe jason
has only one private parameter is
basically you can give it directly in
terms of the allocation function so this
is V and this is X of V then the
allocation function has to be basically
all at all okay if it results in a
monotone allocation function then it is
truthful but if if you basically have a
non-monotonic ation function like this
okay but you cannot do an on water
allocation function the goal is to do
optimization with respect to the
monotonicity condition then you
basically iron this allocation function
okay which means you basically flatten
it out of this region which means all
values which fall in a particular region
are treated equally whenever you I on an
allocation function there is going to be
discontinuity in how you allocate okay
the reason is so far you are competing
only with these set of people people
with values here if your value slightly
increases then you're on par with all
these people okay so clearly the
probability with which you are going to
get served will increase similarly here
know if you value slightly increases
then you suddenly you know bypass all
these people because you only have a
rester competition here you are not
treated at par with these people so
there is going to be a discontinuity in
allocation function whenever you try to
flatten the allocation rule this
discontinuity in allocation function
will also result in a discontinuity in
payments the question is how do you
ensure that there is discontinuity in
payments the answer is you basically
explicitly disincentivize people to
build in certain regions the
discontinuity in payments means that
there are some forbidden zones of
possible payments so if I say that all
improvements between ten percent and 12
person will be treated equally for
netflix nobody will you know try to give
at eleven percent accuracy everybody
will go to ten which means
the region between 10 and 12 is a
forbidden region of payment nobody will
participate give that number here so to
get what is available here to mimic that
you know in this contest setting you
basically put this restriction so it
flows out of this theory but I am NOT
able to come up with an example where
you know this it is more intuitive to do
this this is the theory though the
discontinuity has to be modeled and I
hope you can see that this basically
ensures this continuity nobody will give
anywhere between 10 and 12 people will
always stick to the lower end of an
intern it is the waste of effort the
other work is what it is again in 2010
when I was an intern and the motivating
question for this work is that computing
truthful payments is often more
difficult than computing the original
allocation function itself now
oftentimes truthful payments is only a
secondary consideration the original
problem is a problem to be considered
but the secondary thing is basically
more difficult than the primary problem
itself and this is a slightly striking
observation which Nissan and drone and
make in their seminal paper where you
know for many problems it seems like
computing to through payments is much
harder than computing the original
allocation can you compute routes with
payments as fast as a virgin allocation
that's the question we asked answer is
yes they can be if you are willing to go
ahead with the more relaxed notion of
fruitfulness called truthfulness and
expectation and you can do this both for
single parameter domains which is
answered by way of framework and slip
gains and we also do for multi parameter
settings and this is application to pay
per click add auctions where you do not
know the parameters nice and I will just
mention one other result this is
basically has to do with revenue or
maximization in multi parameter settings
this has been an open question for long
in mechanism designed if you just have
one parameter then you know how to
optimize revenue but if you have
multiple agents are interested in
multiple items and they have different
values for different items this question
has been open for long and we basically
showed that you can get very simple
mechanisms which approximate the optimal
revenue we're just posting prices on
items and allowing agents to take their
favorite items
and because it has been open for long
there has been several follow-ups after
work improving our work on several
corrections for example we make some
interesting connections to profit
inequalities from statistics there is an
open question or a paper which depended
on improving profit inequalities and
this basically created renewed interest
in profit inequalities and the the state
of the art is improved for profit
inequalities in the open questions
answered ok so I will conclude with
future research directions no one one
thing which is obvious to people if you
see what has been going on with resource
allocation settings is that all these
problems have incentives or game
theoretic assumptions in them but the
game theory has been completely ignored
in these problems for the obvious reason
that the non game theoretic problem was
already non-trivial but the picture is
fairly complete now for the non game
critic sitting you know so the goal is
to bring back game theory to online
algorithms online resource allocation
problems I leave with one simple problem
you know where you could try bringing by
game theory and you have the same
display at setting and my emre question
and advertisers advertiser wants are I
want SAT mostly I impressions the only
difference is that these bids wi j's are
privately held and let us assume that
the queries are drawn iid from an
unknown distribution if you just look at
a purely online setting without any game
theory the algorithms i present it
basically solves this problem with
arbitrarily close to one approximations
if you look at a purely offline setting
without any game theory in it then the
VCG mechanism gets a one approximation
but if you mix these two if you mix
online and game theoretic constraints we
do not know how this works ok if can you
get arbitrarily close to one or maybe
not then you have a hardness of being
truthful and this is just like many
problems where there are two constraints
so one constraint alone can be treated
either of them but if you mix them you
cannot and this has been the prime
agenda and mechanism design where
computational and incentive constraints
can be treated individually well but if
they are combined you do not know how to
solve them there is a tension similarly
here instead of computational incentive
we have online and instead of
constraints is there a separation here
is there a hardness of being truthful
what can you do even if you know the
distribution that is
not known and there is unknown
distributions in separation between i ad
and I know permutations on this is a
whole list of questions here that's it
yeah expert isn't he
so like for this time without the IND
model but you can do this game so do you
think but in those kids yes it which has
a number less about he or something so
does that guy even know even the worst
case is not carry I where you can get
that in with game theory because all the
emissary has it is purely algorithmic
main questions up in the you try to
optimize in the sort of stochastic
setting expected expected value of the
algorithm over the expected sort of
optimal value over the distribution so
what if you need to take expected value
of the race or it's a different ratio of
expectations just to them it's in a way
for example the optimum values of your
instances can can differ a lot right so
you're just averaging over all of them
and then take expectation but what if
you hear that's true there's a stronger
thing yeah that's a wonderful results
for example me is a high probability
result which is better than the ratio of
expectation we said this whole with a
high probability that you can get this
one minus square root gamma log n that
is our stronger benchmark and yes the
one minus square root gamma presented
depends on be using the ratio of
expectations so i'm not sure years but i
mean for at least the kind of instances
he said where da ops 2 highly variable
different things it could be that that
is stronger
demolitions
this thing</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>