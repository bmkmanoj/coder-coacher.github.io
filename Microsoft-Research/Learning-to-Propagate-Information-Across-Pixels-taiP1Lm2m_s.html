<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning to Propagate Information Across Pixels | Coder Coacher - Coaching Coders</title><meta content="Learning to Propagate Information Across Pixels - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning to Propagate Information Across Pixels</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/taiP1Lm2m_s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
Smuggler's they told us wrong he is
finishing up his Beach d-max lucky suit
in Germany he did his undergrad and
masters and IIT in India working on
vision used to work in inference
algorithm oh poor vision problems now he
is working on how to embed prior
knowledge is into seeing in for vision
applications applications here so wrong
yeah thanks thank you very much for the
invitation and gloom and for this great
opportunity to talk in front of you and
I am were in Jim Perry and I am a PhD
student at Max Planck Institute for
intelligent systems in cubing and
Germany and I am mainly interested in
advancing techniques for tackling vision
problems for they are broadly two main
approaches for tackling vision problems
one is generative approaches where we
can introduce prior knowledge and we can
get quantified uncertainties and the
second one is discriminative approaches
which are fast under directly learned
from the data and both have their
complementary advantages on on one hand
the generative models need less data and
but the inference is slow on the other
hand discriminative approaches they are
fast but they require lot of training
data so the main research question that
I have been asking during my PhD is like
how can we come by in both these
generative models and discriminative
models for faster and better inference
in vision so towards this aim I have
been working on few few projects like
one is informed sampler where we learn
to do sampling second one is consensus
message passing where we learn message
passing and the third one is learning
bilateral filters and in this talk i
will i will give very very brief summary
of first two works before going into
detail on the learning bilateral filter
stuff so this is summary of informed
sampler this is joint work with
Sebastian knows in Matthew Loker and
Peter galley here the research question
we are trying to answer is how to invert
existing gray
fixed engine for example an example is
shown here for example a graphic system
can take 3d meshes input and produce
depth image as output and the inverse
problem would be like going from depth
image to 3d mesh so how can we invert
such graphic systems so since graphic
systems are complex it is difficult to
do influence invert them so generally
MCMC sampling techniques are applied
they are generic but they are generally
too slow and our our solution to speed
up is to be we propose a mixture
sampling technique we where we do
sampling with a mixture of mixture
proposals with a mixture of local and
global proposals and the global
proposals are learnt using some
discriminative approaches like CNN on
random forests so our experiment showed
that the global proposals help in quick
mixing in the exploration of the space
while the local proposals helps in
higher acceptance rate of the sampler so
is a very brief summary of this informed
sampler and the second work this is done
at Microsoft Research cambric with Elias
la me Daniel Carlo push myth Cola and
John win this is on consensus message
passing here the task is to do inference
in hierarchical graphical models
envision for example many many many
graphical models envision are
hierarchical in nature and example of
phase generative model is shown here
where we do inner product with normals
and light to get shading and when you
multiply with the reflectance you get
observation here the task is given the
observation in for these other variables
so these inference in such models is
generally performed using message
passing techniques so we implemented
this info dotnet probabilistic
programming language and we and we found
that the standard message passing like
variational message passing or
expectation propagation fails this case
so our solution is that we learn certain
key certain key messages in this message
passing which we call consensus messages
using discriminative approaches and we
observe that it helps in converge
message passing to better solution and
also faster convergence and it also
shows and is one of the first examples
where we show that infant dotnet can be
used to tackle vision problems
so the overall aim for these two works
is like how can we leverage
discriminative techniques for inference
in generative models like the four
informed sample we learn to sample in
consisting we learn to pass messages so
we we observe in both of the techniques
we observed significant speed up over
baseline inference techniques baseline
mcmc or baseline message passing
techniques but still not fast enough
version for practical purposes these are
one of the main issues with generative
models inference in generators is
somewhat slow and it is not practical so
next we ask this question can we can we
go in the different directions can we
propagate information inside
discriminative models like cnn's for
example the cnn's have been shown to be
quite useful in solving many vision
problems in tackling many vision
problems so can we can we add can we add
prior knowledge into these cnn's so
these kind of different direction from
going from general using prior knowledge
to to inference in discriminative models
so with this question so I will start
with the main topic of the presentation
which is learning to propagate
information across pixels this is joint
work with Martin even rugged appear
there and Peter girl is so just as a
motivation natural images exhibit high
level of pixel internet dependency for
example all the all the sky pixels are
blue in color in this in this figure but
the pixels are generally represented
with independent values so many of the
vision techniques try to capture this
pixel interdependency by propagating or
pulling information across pixels so one
of the simplest and fastest and most
used way of propagating informations
across pixels is spatial convolution so
this is these forms the basic building
blocks of most CNN's like Alex net CNN
or BG genetics so let us look deeply
into spatial convolution with a gaussian
filter example so these are spatial
convolution works we have some filter
you you you convolve at every pixel in
the image and you get some output so
here is an example with a Gaussian
filter you
you get some smooth image as output so
here filter is defined with respect to
position differences with respect to
center pixel in general the spatial
convolution can be defined like this
where V is the input vtech is the output
for every I output pixel it is a linear
combination over the neighborhood n for
all J pixels it is a weighted linear
combination weighted with respect to
filter weights and the filter weights
are defined with respect to position
differences p i minus pj with respect to
the center pixel so this is how
mathematically we can define spatial
convolution so next bilateral filter
this is a generalization of spatial
convolution to arbitrary feature spaces
so here here the filter is defined not
only with respect to position
differences but also within with respect
to feature differences this need not be
only position so the most used features
are for example the most news pages are
position and color XY RGB and the most
used karralys Gaussian kernel for
bilateral traitor so is this definition
player thing and so what does it mean
that the filter depends on the push
filter depends on the feature
differences not just the position offset
a surfer question how many different
features they are like engineered
features in the defined in the em both
out with a vision of the image yeah at
present the most almost all the use of
bilateral filters are generally the
engineered features like it is mostly XY
RGB XY RGB is kind of synonymous to
bilateral filter but it can be general
features yeah I will I will show in one
of our later works that we learn these
features also we can rinse and I will
see me the signature of your scale
because it has to wait at based yeah
yeah yeahs Sigma Sigma can go inside as
a scale for features yeah you can either
scale the features or scale your filter
colonel so generally it is implemented
by scaling the features it is earned and
used fixed fixed colonel width nothing I
usually don't like for example when you
do the normal conventional battle filter
where you have the range under space
people don't normally use the same six
yeah for both yeah for its kind of a
different Sigma 4 yeah maybe it's yeah
it's kind of yeah it's kind of Miss Lee
yep okay so what does it mean to have
the the filter depends on both position
and color color differences this means
that bilateral filter depends on the
image content as you can see in this
example the filter colonel differs where
where where we are computing that filter
for example here filter looks like this
and fearful filter looks like this so
after the here we are we can also see
the image after the filtering as you can
see it is smoothing the image but it is
also preserving the edges these one of
the most important properties of
bilateral filter so one of the main
computational hurdles for it is likely
it is computationally expensive to to
define to compute filter at H at each
location in the image kind of
computationally expensive so but the
good news is that there are several fast
approximations exist so we will use one
such approximation which is high
dimensional linear approximation so i
will i will quickly explain this using a
1d example what is this what is it
approximation so let us say we are given
this 1d signal input signal with just
some intensity values and let us say we
want to do filtering using position and
intensity features bilateral filtering
so this technique involves first
splatting to this feature space to the
future space position and intensity
splatting these values
and then do convolution Gaussian
convolution in this 2d space this is not
exact caution convolution there is also
some kind of normalization because to
account for these missing values these
are some kind of normalization and then
you go back to the original space and
you can read back in the original
locations to get to the filtered thing
so each of these operations can be can
be given as a matrix multiplication a
splat W&amp;amp;S slice so that is why it is a
linear operation linear higher
dimensional operation so here the one of
the main cause for the approximation is
that we are splatting when we are
splatting where we have to extrapolate
the input signal into the into the
corner points in the high dimensional
space high dimensional feature space for
example here cohesion and intensity this
so is the main thing so is this is clear
or any questions here yeah as I was just
saying it it would be equal to actually
sort of write down the concrete
computational complexity of the original
operational operation and this operation
right yeah the region's you like
depending on how many features you have
so yeah that's sure yeah there's a paper
from Adams at all and yeah he gave nice
plots for like what yeah which kind of
legacy is useful for which kind of
dimensions right i mean there are i mean
the the downside for this is that your
recent issues a lot fatter it was you're
basically lifting too hard original
space and our antenna signal is very
spot right in and an ego to decide light
the size of the splat and so if there's
a lot of magic numbers
whew yeah sure yeah yeah there's
something so so it is that only one like
Sigma parameter in the Gaussian
convolution which is determining what is
remaining is black in the right side
figure if the splatting is basically
filling up and then some areas are
remaining black yeah it is mainly
probably do too because there is no
there is no data here means that there's
some implicit Sigma of the Gaussian
convolution yeah there is some here I
use some fixed Sigma here but usually
it's like a fixed Sigma yeah yeah
generally people use fixed Sigma but
they change the they change the grid
with some scale the grid okay so this
was also same things will soon yeah the
scale liquid idea so one of the problem
with these higher dimensional greens is
that the corner points increase
exponentially with the dimensions with
to poverty for example if 442 d the
simplex as for four corner points for 3d
cube for four teats as 16 corner points
so atoms are all proposed another grid
lattice which is which is more suitable
for bilateral filtering here the corner
points increase linearly with dimensions
here for 42 2d the simplex is triangle
for pre dietz tetrahedron so the number
of corner points and so here the
simplicity will be less discretization
artifacts when we are splatting to this
high dimensional space so we also follow
this firm to hetero lattice one and it
is shown to be more suitable for five
dimensions six dimensions kind of thing
if we go further higher dimensions
there's some other data structure is
called KD trees which are more suitable
for for most of the vision problems five
dimensions are reasonably good
what yeah x-ray RGB features so many
many many of the vision problems x-ray
RGB is used so that's why we choose this
lattice so this inverse so the using the
pump to hydro lat is to do the bilateral
filtration involves first splatting the
input image to this high dimensional
feature space defined by the firm tuyter
lattice and they're doing convolution in
this space I've shown some illustrated
the Gaussian filter here and then
slicing back to the original image space
so all the existing works use
hand-designed filter weights nothing
like Gaussian or laplacian thing it is
mostly Gaussian so just to just to give
you an idea of how this lattice looks
like since its high dimensional I mean
for example for this sample image these
all the lattice looks like if we are
using position features so pixels
falling in the same simplex are shown
with the same color so here all of these
pixels fall in the same simplex in this
position that is and here is these are
the lattice looks like when we are using
color features for example all these sky
pixels fall into same simplex these are
the lattice looks like when you're using
position and color features projection
of the like this yeah yeah yeah this is
a projection of the lattice into the two
days just some visualization just to
give an idea of further so what it is
doing when we're splatting it is kind of
a grouping the pixels together maybe a
new question how do you specify the the
XY size that would be map the into one
yeah that is a very that's a very good
question here here and here we are using
particular X faces I mean depending on
particular scale for this we get smaller
triangles and bigger triangles so here
yeah these are hand you'd parameters and
this is just for a visualization yeah
these are the hand to impair emissions
because this is more like an image
depends on VI is more like a vegetable
yeah this kind of hand you'd better
meters for this which I which I did not
show here
n equations okay so one of the main
advantages of this using high
dimensional filtering is the high
dimension approximation is that we know
we we we do not need to stick for
spatial grid organization I can take any
number of input points in arbitrary
locations and splat them of thing and it
can be leached off any unordered points
example 3d points and input and another
advantage is that the input and output
points can be different I can I can
splash certain points and I can read out
in some other points so these are many
side advantages for this technique so i
will i will show how how we use this in
our experiments later so one of the core
technical contributions of this work is
learning bilateral filters all the
existing works use fixed filter colonel
fixed phantoon filter colonel so so what
we our our key insight is that instead
of using gaussian filter we can actually
parameterize these filter values using
some free parameters and we can learn
these values but the question is was the
intent to
what was the intent was very intense
what's the goal what is that what you
want I mean your visit for the noising
is it for segmentation yeah for there
are many many applications I will I will
I will come to the time and this is this
is more general technique of how can we
generalize bilateral filters and it has
many applications I will come to that
later so here we can we can get the
derivatives using perm title
approximation because splatting
convolution and slicing or linear
operations we are matrix multiplications
we can actually get the derivatives for
these using stranding standard matrix
calculus derivatives with respect to
input we and also derivatives with
respect to filter weights but you are
going to sort of think about size of the
bangles right yeah there is still some
kind of cross correlation going on
validation going on but but still the
actual ballot and filter also has these
yeah has this problem no but from a task
perspective right we are trying to solve
a particular task right and you try to
minimize this right and you are learning
the parameters here and and that risk is
linear in in these parameters but there
are other parameters that you have to
salute you wearing shriya yeah that is
kind of the feature work we are actually
planning actually we want to also learn
those also how will you go about
learning it is not very clear at this
point we're still working on it like
because in another approach actually
we're actually we can actually
explicitly do this bilateral filtering
actually we don't need to use this
approximation rules suppose we have
hundred super pixels in an in an image
we can actually compute explicitly
bilateral fit it easy so suppose we have
thousand thousand superficial it is a
thousand by thousand matrix it is not a
huge matrix so we can explicitly compute
so with this we can easily get
derivatives with respect to these filter
scales feature transformation another
thing yeah I'm not starting Olympic it's
a bidding operation so I don't I'm not
oh it's a major actually I was also
about to say that splatting in the
graphic sense in star taking a paintball
it's splatting it so he said to me what
you see Saria's could involve exactly
part of sweating yeah there is also some
kind of Gaussian convolution involved in
splatting it's like because what you're
doing what you call sliding is
essentially what Jacob just said right
essentially just spinning but there's
when you do a billing an illegal
convolution right to make the spatial
extent bigger just that's the
operational spider yeah so we can we can
we can view we can do splatting as a
different discretization of the input
points what is that are you implement it
is to better the matrix multiplication
so here here this is a big matrix let us
say for these 2d case there is lattice
points let us say there are thousand
lattice points and this let's say there
are thousand input let's say they took
2,000 input points and let's say there
is a thousand lattice points so this
this this matrix defines the barycentric
coordinates how how do how do you go
from point there is a quarter's that
there's a description say she said that
happened before that's implicit with you
don't share that on the me to design to
the heart yeah that is that is exactly
the splattering splattering involves
extrapolating these signals to these
corner points using barycentric
coordinates okay i'm using the wrong
term but if i look at if you look at the
heck but original paper on e wa ed sake
that was the original use of the word
start okay I don't know so we got these
terms from andrew adams paper so they a
lot of the new papers use this s slice s
plat I've seen that like also
fairbairn is also working on his pen and
basically they're just positive
coefficients right fractional values on
every row it says take each value of the
van each pixel value and spread it to
peace by this point we like a naive
nonzero coefficients in each row right I
think it's like a diagonal matrix we can
we can actually think of this as a
distance some other discretization of
continuous signal right i mean for
example image is one kind of
discretization of continuous signal and
this is some other discretization of
continuous signal nothing realisations
so the inputs of business is a sparse me
it's not it's not the actual picture
yeah it is kind of you can you can
conserve e as a set of points yeah I
don't know exactly how if the if the
terminology sky but is kind of standard
terminology used in bilateral filter
community I don't know in the more
recent community I think the original
sorry I don't know branch but you know
this is that people say resolution when
you fry is upset okay so is everything
clear so we can we can actually get
these derivatives with respect to filter
weights and also with respect to input
we can learn using back propagation
techniques likes stochastic gradient
descent okay so what is the use of
learning bilateral filters so it has
many interesting consequences we we
actually studied these three one is we
can actually learn problem specific
bilateral filters so we can actually
also generalize spatial cnn's to use
bilateral filters which we call
bilateral neural networks and we can
also generalize the den CRFs I will I
will explain each of these in it in in
detail next so first the first one
learning problem specific bilateral
filters but I rivalrous has wide range
of applications in vision graphics and
image processing and there are probably
several hundreds of papers on just using
better traverses surprisingly all these
papers use just Gaussian filter and XP
RGB features think I mean most of this
so we want to study whether with you
whether by learning bilateral filters we
can get any improvements or this so we
actually studied three different
problems one is joined by later love
sampling next is image denoising that is
3d mesh denoising so because of time
constraints I will mainly going to joint
bilateral of sampling in detail so what
is joined by lateral of sampling so this
is a task of upsampling a low low
resolution input with bilateral filter
constructed using guidance image so i
will explain is using a toy example like
let's say we have this low resolution
input input collisions and we want to up
sample it using a full resolution
grayscale image we can actually
construct the filter using these
features from the guidance image for
example here the here the features can
be XY and intensity so actually
construct the filter using this and we
comes from the low resolution input
image and when we when we do this
bilateral filtering you can get a higher
e high resolution color image so in this
case you limit your pictures no to
include color scan looks like anything
that's in the guidance yeah original
scans here is it kind of hurting you
because you might also need to use some
features that are only clean up from the
guidance yes normally you kind of yeah
maybe a guy it is actually a run so this
is something in the set of the problem
yeah in the setup of the problem for
example is this kind of ass toys example
it for example the original original
problem will be lets say the image
colorization how does it work people
people do some scribbles on the
grayscale image saying that this is red
this is a low kind of thing I see so we
can actually those are that is the input
to algorithm again this can look like
include added more information to the
guidance image no that is just the input
and the guidance is only the grayscale
image okay I see so that is there is a
more natural setting but here for the
ease of studying we just take this
artificial setting
so yeah here the features come from the
guidance image this is called joined by
a club sampling it is so we worked on
the cob sampling on Pascal vivo see data
set for example for this 8x of sampling
these are the results this is a low
resolution color image this is gray
scale image and let us say is a ground
truth for the high resolution color
image and this is a bilinear
interpolation by doing better
interpretation of that this is a result
with the gauze gauze bilateral of
sampling when you are using by a top
somebody with gaussian filter and these
a result within learnt bilateral of
sampling as you can see be the width the
length valid love sampling the low low
level details are better preserved kind
of thing and we also need some
quantitative analysis with different of
sampling factors and we found that the
lead by example we can get better PSN
our values compared to casa yeah so in
this problem set up you learned one
filter from yeah from a training or try
and secular it do you hear just as you
know graduating yeah yeah so this is
kind of single filter application
application yeah filter is not getting
adapted to the actual data your actual
image you're running it all know this is
kind of we have some training data and
we train the filter on that and we apply
the same filter for all the test images
actually I'm not sure because different
images have different levels of noise so
should be a noise adaptive rather than
just looking at the data itself so in
other words to be trained on images that
has very little noise and I give you an
image that is relatively noisier then it
will not work yeah sure sure yeah yeah
but this is general setup in denoising
right i mean we assume certain level of
noise model available contacts in a
contrast problem sure sure sure yeah i
did not show the results here but
actually we also studied training for
one of sampling factor and then testing
in different of sampling factor we
actually observe this kind of quite
robust to that with robust match
how many this is just about learning
traffic yeah I'm betraying you or try
examples here views from the Pascal we
will see data set either around 1500
images and so what is the observing how
much the lower resolution was yeah these
are different of sampling factor we
experimented with like a takes eight
times subsampling 2x does that mean does
that mean how well the resolution be the
low-resolution images yeah for example
this this means that the we need to for
every eight pixel we need to interpolate
eight times a times you did you wear all
of the low-resolution images exactly
safe resolution the question which i'm
wearing is it is it is it robust to the
low resolution have to be the exact same
resolution in order to generalize it all
yeah because yeah we we actually trained
for particular particular of sampling
particular lower resolution for example
2 times of sampling four times of
sampling but we actually found out that
even if we even if we train on four
times of sampling and if we test on
eight times it is still generalizing the
question is the original resolution is a
save something right if he is gay right
now did you train with k by 2 or and NK
and kept it constant and then basically
or what was what was constant was cake
on was this resolution the target
resolution constant or whether you yeah
okay bye to dismiss to is constant okay
so k is the target thing and then I
cavity by a by a 10k by 16 year
Boston okay target okay so what's the
order maybe to have learnt parameters of
this ah I not remember exactly well it's
rough is it is it the order of a
thousand or million or ten I don't see
terms of points tens kind of thing okay
see those people a single a dollar a
little so we also apply this on depth of
sampling for example scene and depth
estimation results are often low
resolution for example we takes the CNN
from i can at all and so this is a
sample imagine is the output of the CNN
which is by linearly interpolated and we
can we can actually use the original
image as the as the guidance and we can
get construct by adding a filter in
using the original image and and filter
these to get thing so with this small
with this small of a number of
parameters that you think something
would maybe like a grid like a like it's
pretty typical not differentiable search
like like this vision of termination yes
yes you should I've been here the whole
point of this experiments is that we are
not trying to solve this problem we are
we are actually showing that they are
these hundreds of this paper which use
single bilateral filter for many
applications so we we want to show that
we can actually improve over them by
bylo the nice thing is it with this with
a small number of their only learning on
the order of 10 parameters oh ok number
of parameters here and then you don't
really need the fridge okay so so so so
so you're you're you're asking number of
parameters right so here it's kind of
like here here it is a fire dimensional
space so we took one neighborhood so
it's around 500 parameters something
there's something around 600 something
yeah so again I think in your letters
the balance between can can be thought
of as a as a layer right yeah sure i
will i will come today come to the
latest exactly yeah well i will exactly
come to that leaf so is this is the
result of gaussian bilateral of sampling
is a result of learned by later
upsampling we get slight improvements
using the learning for this and we
tested this on niyu definite asset and
we get slight improvements in terms of
rmse but visually you can see better
improvements mostly that based on on the
texture so I'm not quite sure I mean
apart from making sure that the agents
lot crisper anything else it's you can't
really trust them yeah surely actual
reality is mainly in case why you see
you see the touch of the boats well yeah
that is kind enough right yeah it is not
its are correct I mean but it's kind of
helps little bitch in kind of let me
start right there Jess yeah yeah it is
as you can see it is not big
improvements and actually baron also had
one paper that deputy focus rather than
showing this would be nice to show arrow
map and that you need this institution
of the correctness and Cena only certain
parts of it will be better yeah yeah
that's true yeah yeah actually these
kind of a pretty many studies to show
that this could be used here I mean yeah
we are also not very convinced that this
is a good way to go for up sample depth
at convince there are other techniques
that should how do you come up with the
ground truth in these sit ups because
there is a 0 there is a sensor yeah I
did mr. Schmidt college paper from eccie
we yes collected seven Great Awakening
that I I to be in so that there is a
ground rules and culture but the sensor
is a local resolution right
no resolution so you have advanced ample
even more yeah so one of the main
advantages of this is a of this
bilateral filtering is that it can be
easily extendable to 3d data I mean the
input can be any unordered list of
points right so it just needs some
features for each of these points so for
example this can be applied to 3d 3d
image denoising i am not going to
details here but I mean this is easily
applied to 3d points and an ordered set
of points exam hero I show you now is
image this is a synthetic noise and we
can actually denoising it using a data
and here here we use the normals as
features so one of the questions which I
think singing was also asking in the
context of the previous application was
that the bilateral filter was it what
what is it learning right so is it
learning something very local about the
surface or is it learning something more
more global and actually some something
about the structure or geometry of
scenes right yeah sure show you and this
is a disappearing now in this context
again in this particular application is
the pallet resulting filter that you
learn learning something about the
quality or the or the or the shape of
parts of the human body or is it
learning something something very local
about the properties of the surface yeah
that's a that's a very very interesting
question actually actually we we try to
study a little bit like how how the how
the filters are changing compared to
Gaussian but is kind of difficult study
because there is a high dimensional
thing and we actually projected it and
visually we did not see so much
difference so this is the question is we
go back to the previous application what
would it take right if suppose you so I
I see that what you're what you're
trying to do is basically say say here
see here is a biotic
filtering approach and we can now make
this approach better by learning
parameters right that's obvious thing
right and that's obvious contribution
that now we have a we have we have shown
how to do learning of the actual product
right that that's obvious right that's
the technical contribution now from at a
specific point of view right in somebody
who is if your objective was not to
propose a new algorithm your objective
was to solve a particular problem
problem there how would you go about it
right so if you look at that result the
learned bilateral of some things are and
say okay now actually I want to solve
this problem I want to get to the ground
to death there's a problem it's already
interesting I really want to do this
right yeah so how would you go about
doing it right is there an extension of
hero approach you would you basically do
this at them at at multiple scales is
there an extension that you think can
capture the entry exactly we can know
these multiple scales and multiple
feature levels yeah and also designing
the features is the key right features
for example here we use XY RGB yeah
which might not be very good for depth
maybe we need to use different features
for for this problem and different kind
of thing yeah actually actually we have
more better applications I will yeah and
we will discuss this more yes so it
could be like that can be solved more
directly are there are sitting there
stay there which just uses noble CNN's
the way the people don't believe it was
better than yeah this is CNN but and
actually they actually improve the CNN
in in more recent works this is this is
one of the set of the arts when will be
overcome evil even just the baseline
girls if they can you be the based on
gas right now the point is the image on
the result on the extreme left is the
output of a theory as I'm saying it
looks worse than than the gossip I
lateral offset language yeah yeah exit
exactly so so actually what they what
they say is in their in their paper is
that we did not use they they did not
use any up sampling methods because they
did not get good improve
meant with in terms of number I feel
they did not get any good implementation
samus also they say it is ok whatever
upsampling we use we are getting almost
same number so but actually visually it
makes more difference rms is kind of per
pixel thing right here it's so one
question was if you have a linear did
you try to do black back propagation
end-to-end because if they you take the
CNN depth out and you do post-processing
ghost idea and you essentially learning
to prosthesis yeah right in this context
now what you can do is instead of
learning to post-process you can just
say okay let me just take the whole
whole thing and actually backdrop area
yeah actually actually we could also do
that but we did not try oh you here so
that would be a good thing you actually
look at the whole thing back yeah
because we are actually focusing on many
different applications and we did not go
into depth in so another important
consequence and and this is what we are
most excited about is that this Lenny
biofilters is that we can actually
generalize the neural networks to use
bilateral filters which you call
bilateral neural networks so actually we
don't need to stick to single filter we
can actually initialize we can use
multiple filters and initialize them
randomly and we can learn in to end so
this was not possible with with the
Gaussian filters because if used
Gaussian filters all filters would be
saying so we can actually randomly
initialize and learn them together and
we we call this bilateral convolution
layer such layer and we call such
network as bilateral neural networks so
we actually did some preliminary studies
on where this could be used and we are
still trying to find out possible
applications Ferguson or bg2 like number
of in book filter
right how can you lose instruct your
features in the middle layers of the
senior yep it's right away so look at
that we can actually use but but but
here we are still using x-ray RGB this
technique is still not extended to learn
the features I see the features are
still manually specified here as so that
we are trying for future work so what
are the advantages some I mean these are
kind of debatable if they are really
advantages are not what are the main
differences like we can get imager up to
filtering in CNN's we can also filter an
ordered set of points examples parts 3d
points which is not very feasible now
with CNN's and we can also get input and
output points can be different for
example we can go from pixels to super
pixels and super pictures to pixels and
we can store information at important
locations in an image understand the
details of the BN n so can you go back
to the videos like I think we talked
what is a laundry so what happened was
in a CNN there are a bank of filters
yeah I replaced each one with a
bilateral refer this diagram of this
lattice is like means that you are going
to represent each of them with that high
in this I dimensional space say is so
useful at the input suppose you have you
you get some input point you splatter
this input points to this I dimensional
feature space and we can use multiple
filters here but now so so although mate
there are three matrices for each filter
very coarse black matrice yeah Val
matrice and the slice matrix we actually
fix splat and slices we only let the
only limit is Cheney is w but early the
graded actually fixed slice and splash
are not seen yeah actually we could also
do we connect we can also define
multiple splatting right but defined so
what is it is it a convolutional area
because isn't it just a bunch of anyway
splat it yeah this is a convolution
layer in high-dimensional space not in
the original space so these higher
dimension sexing supposes you can we can
think of this as a high dimensional
actually implement the right it's
basically just what you do this you do
something together into some space yeah
from that perspective it's just a bunch
of linear combination yeah the the main
difference is that this is the we have
to deal with the sparse points here in
this high dimensional convolution the
points can be dense here we have a deal
with sparse point we actually in terms
of implementation we handle this using
hash table and we only convolve in this
sparse space otherwise it will be memory
inefficient otherwise we can't fit into
memory or the computation it will be too
too much for example the fire
dimensional space so so we only convolve
those lattice points which are populated
so that is the main difference yeah in
kind of in the retrospect it's kind of
looks obvious right but it's kind of no
one did this before I don't think it
looks obvious but I'm just trying to
think about what it's actually like
because once you learn work is what you
get it into that space there's no real
reason trick like you can start adding
all the normal things like rectified
linear units and stuff like that yeah
sure yeah sure yeah yeah yeah we could
browser to the pooling in that space
yeah we would also do that but we did
not try a lot which yeah those are those
are many pot possible feature works it
actually opens up many things in this
bilateral space so just to illustrate
where this could be used we we we cooked
up some example which where these is the
given a time with with random foreground
and random background color we need two
segments that image so think about we
actually cook this example because this
will be easier to solve in
high-dimensional color space if we
actually project these points to this
high dimensional XY RGB space all these
foreground and background points are
already separated in this space so so
the task becomes easier for bilateral
filter to solve rather than doing
convolution in the
original space how many layers did the
network yeah so actually we worked with
three three three different layers three
three layers convolutions followed with
Ray Lou and first layer has 32 filters
16 filters and again to contain the
background color distributions were all
random yeah exactly the same so so this
kind of very very very difficult problem
for normal convolutions it's really a
nice ones and that there's a small shape
and I need to say yeah Sigma at the
shape so the receptive field for the CNN
should be high to to actually solve was
there clusion or there is a small box
work on always there he has one ox is
always still no lengua office fully
completed yeah fully completed kind of
signing skills but but surprisingly the
CNN's one for small spatial filters it
is not able to solve this problem with
with small spatial filters because it
because it can't see the square there is
no pooling it's got a segmentation is
holding here so we actually replace the
convolution layers with the bcl layers
metal convolution layers with one
neighborhood filter in this XY RGB space
so there are some results and we
evaluated with a different CNN filter
sizes we use member and filters 13 by 13
17 x 17 21 by 21 criticizes and these
are the number of parameters 51,000 in
the in the entire in the entire scene no
sex with Vienna these are the number of
parameters as you can see with this
small CNN it is not converging at all we
need big large number of parameters and
bigger receptive files how much it also
remember it where is the room
good question and in terms of memory
requirement there are some extra
implement implementation things because
we need to store some hash table those
flights Platt and slice matrices as
other sparse matrices other than that
the memory requirement is same because
we are also convolving same number of
points as the input yes but it would
actually the memory requirements might
go down in some cases if we have big big
lattice cells so we are actually pulling
number of points into small number of
lattices so it's actually the same once
you once you do this part of the kitchen
and get out your thing is it actually
the same convolutions normal user or are
you just saying that it's a
mathematically approximation but it is
that it is the same convolution but it
is done in the sparse sparse space so
actually we have to do some kind of
normalization for example normal special
convolution if we can actually do the
special convolution in this Phi D space
I can actually define 9 x 9 x 9 x 9 x 9
spatial convolution and actually
convolve in this space but this is
memory inefficient we can't fit this
into memory so here actually we are
taking care of this using some hash
table and only only only only convolving
those points which are populated in this
high dimensional space it is it is a
sparse space so the so the main thing
here is dealing with this par City yes
much more like what is the publisher
that actually heard the like if it's
super spars but you're still using you
know whatever 9 x 9 like how many
neighbors are actually yeah here here
here actually we are using one
neighborhood will train this fire
dimensional space but still it has their
own 100 parameters something like that
yeah sure yeah if we if we actually be
also experimented with two neighborhood
but we observe that one neighborhood
works metal is go to the neighborhood
neighborhood in this high dimensional
space for example here in this to the
neighbor on the great yeah neighbors are
listed for example this is this is this
is one neighborhood oh this is too
neighborhood so the part I like a bitch
other many as its parts enough of these
medications were even with one
neighborhood it's still yeah generally
it is it is it is not the case because
the image there are many similar points
in the image so many many many points
fall into nearby regions kind of thing
thinking again why why this is working
is actually easier you are defining your
you're going from like no dimension to
higher dimension by just like XY RPG but
using any learn it features to define
this up some yeah for now in this work
we are using XY RGB I see in ladies kind
of knowledge is kind of the analogy is
helping this is kind of some kind of
prior knowledge here here the prior
knowledge is very strong it is saying
that all these pixels which are which
are closed under clothes in RGB should
get same label this can also be extended
to naturally made us actually natural
images also i will show later that this
also be a very natural images so the
only piece of information that you added
in the system is that is just like rbg
information yeah RGB information of
these and similar also yeah it's an IP
Samson okay it's very simple yeah oh
there was an XY yes I should be fun yes
yeah
so all these models converge to perfect
solution after a long time or prime and
convergence of BN is faster with much
smaller number of parameters and
actually parents already see the see
that see the color difference i mean the
task is just to need to assign the
labels this is just to illustrate why
this bnn might make sense is kind of a
boyish example so the next task we
applied is a sparse character
recognition like handwriting data is
spatially various parts over over ninety
percent of the pixels are background
background and these are some is
character data set and it comes with 183
classes and 45 writers nothing so we do
experiments with this so so these are
the standard seen in architectures that
we people are using for one is in 87 and
there is deep sea net for this character
recognition kind of it is a bunch of
convolution layers and at the end of you
have these inner poet layers standard
classification networks so how to handle
sparsity so these are image than the
this actually covers only these things
so CNN is exhaust agnostic to the image
content so CNN if even though all these
data is empty he does the same
convolution everywhere that is kind of
some kind of stupid to do and the best
we can do is like maybe crop this image
and they do do the CNN only on this
cropped version this image but still a
signal is sparse and we can and I will
show how how we can easy to easily
handle this part city using BN ends so
how can we use bellator layer to handle
sparsity we can actually only only pass
foreground information with bcl-2 other
layers for example either I can splat
only the forward points once by using
these features like for example the the
filter is XY why I position if bi is
foreground and it is some kind of minus
minus 100 or minus 1 if it is a
background so all these four grown
points are splattered to the one place
and background points of character
we'll point and then do the convolution
in this space so we experimented this by
replacing the first convolution layer
using this dcl layer in both linear and
deep cnet networks so there are some
empirical results in terms of training
performance we get better convergence
ennn compared to limit and also in
compared to crop version we also train
another Linnet using only the crop
images sophisticated attention mechanism
network which have a proper attention
mechanism because this yeah yeah it is
kind of a very very interesting question
actually the present attention
mechanisms actually they work mainly
spatial at attention mechanism actually
this actually allows to have attention
in high dimensional space like I mean
the attention need not be spatially
closed right I mean it can be a
dimension that so that's a very
interesting question that is very
interesting thing to work on but we did
not work on so in both Illinois and deep
sea need to actually we get improvements
in terms of numbers and also in terms of
convergence so the other two main things
we worked with the bilateral neural
networks and we are all planning to work
on things like so third consequence is
this learning battle filters is that
generalizing dense CRFs so dense here f
is for people here familiar with dense
CRF densely connected random fields
these are Bri some people would be
familiar that you should go ahead like
explain explain also context context
here okay so that ends here if is a
conditional random field where every
pixel in the image is connected to every
other pixel for example this is how the
energy looks like like it is a sum of
all unity potentials independence are
defined at every pixel and the sum of
all pairwise potentials where every
pixel is connected every item X is
greater to every jet pixel so it has
found many applications envision like
segmentation optical flow intrinsic
images etc so Sokka crandall eight all
proposed this meal PhillyD friends
technique using bilateral filtering
example mean field updates can be
computed using bilateral filtering like
this right
this involves let us say we have some
input you get some you Nerys after some
processing let us say CNN and each of
the mean-field step involves first
running bilateral filter using pairwise
potentials as your filter kernel and
barrel filtering the beliefs using the
biofilter and then adding the unit is
back and then doing again softmax and
this is one main field step so these out
NCR of inference is done in 10 CRF so
all the existing works use Gaussian
pervaiz potentials this scipy is defined
as Gaussian Perez potentials epower fi
minus F J square so this this
corresponds to Gaussian bilateral filter
one of the main reasons to propose
Gaussian Paris potentials is that we
people mainly have Gaussian bilateral
filtering available so with our
technique we do not need to confine two
gaussian gaussian potentials anymore we
can actually back propagate through mean
field inference that propagate through
all of these steps and then we can
actually learn these peremptory oil
filters this actually corresponds to
learning pairwise potentials in den CRF
and moreover we can actually learn
different pyramids potentials in
different steps of mail field we don't
need to use same pairwise potentials
anymore this this week all lose mean
field and we applied this for task of
semantic segmentation which is a main
example for den CRF things here is the
task is assigning semantic meaning to
every pixel like chair plant etc so here
this white beaches are unlabeled data
this was my publish this is a sure I
mean like in the previous for the
British line so this is you so yeah so
this is the den CRF is already published
and this dome casework they showed that
we can back propagate through main field
but all using without lines bilateral
finger so all the existing works use
Gaussian filtering Prezi and Gaussian
various potentials so would you calling
dense here it is actually fully
corrected see ya fully concern is can be
also like not fully yeah very good
equity are ya doing inference no
learning right Brandon yeah yeah it was
just showing that how this approach here
for example there is there is another
recent work from Philip tours group that
where they can back where they show that
it is a back propagate through this
dense erf and we can learn with it with
week series also so they call it yes
erfs are any papers are but still they
are using Gaussian filters they are
using Gaussian filters but they show
that we can actually learn end-to-end
nothing's so with these one more step
like
so what is so good about density RF
actually we can observe big improvements
over CNN with dense CRF on top here
these are the results on two different
data sets one is Pascal we will see with
with the syren we get 68.9 I owe you by
adding 10 CRF we get around four percent
improvement and the next one with the
material segmentation on bank data set
we get with scene and 55.3 class
accuracy with with renze era we get big
improvements and this is one visible
result is input ground truth and is CNN
result it looks blobby because it is
trained on pixels yes and with adding
dense here if we get very crisp result
we are just Pakistan was a left is
ripping up the sky when the sky is
actually quite why is your drink oh why
is the season is the ceiling hey we're
not how many days is it i think there's
no uncertainty in all these labels have
equal cost or something so thank you I
think we just shows you that neural
network even to be revision not where
yeah not up to they don't have much it
yeah I mean we still have not much
understanding but but but it's quite
amazing actually how this is picking up
this but I think like what's important
religion now you have a denser is now we
have a mogul that potentially would
learn this yeah I global structure as
well so why what do you think that's not
learning it this your then then theorem
is only you as a post processing step
course on duty he doesn't do bad sort of
overall backdrop oh I thought that you
do know so he it does not look this is
that what i was talking initially that
do you do the full back drop through the
cnl is the CNN feature the theater
output is fixed and then the dis n CRF
is just doing the inference so it is
taking the CNN output as unique
potentials fixed unity potentials and
saying now basically i will learn to
to inference in a graphical model which
has unique potential given by the CNN
and the fairway potential that I defined
will be done by me but he is back home
to learn the bilateral yeah I am like
only you only the perfect yes what's the
idea actually the one reason we did not
do it is actually we don't believe me
den CRF a lot i will show you why later
we don't believe in den CRF a lot so i
will show you why lighter winds
experiments so you have to in veins yeah
sure yeah so we we experiment with two
different data set one is somatic
segmentation on Pascal vivo see and
there is material segmentation on
roaming data set these are some numbers
maybe i will quickly go through the
numbers it's not very important because
what we observe is that with with
learned CRF we get improvements and this
has to step mean field and is kind of
lose to step we learn different filters
in different main field steps this is
one visual is it as you can see some
some regions get better with learn the
CRF comparatives here here only learning
dense here not learning CNN so dense
areas are good but they are time taken
compared to when is many many CNN
approaches so what we are so we don't
believe it instead of that much can so
what what den CRF is doing it is mainly
doing propagating information between
the pixels which has similar RGB values
kind of thing so can we can we actually
do the same propagation inside seen in
itself it could be even better right i
mean rockets in and so we don't need to
apply the post-processing thank you yeah
exactly exactly so so we we actually
have to improve the bnn to some things
to to me to to make it feasible for real
world things so i will show you so these
are the general architecture for
segmentation CNN so there are a bunch of
convolution layers followed with FC
layers FC FC is kind of one by one
convolutions people who are fully
convolution layers and then we get some
low resolution image because of Max
pooling and striding and we upsampling
using interpolation see our effort in
the convolution techniques so generally
required using post-processing
techniques so can we can be actually
propagate inside CNN itself so so so we
propose something called bilateral
inception module where we can insert
between any two layers in the existing
CNN and this can actually propagate
information between the pixels yeah now
now now we learn all the things you know
things so we call this bilateral
inception modules this is slightly
different from the previous work because
these are the military I will be flexing
what is bilateral inception modeling so
this is a previous layer output and we
actually do the bilateral filtering
Gaussian bilateral filtering in
different feature spaces which is places
in different future spaces and we can
actually linearly combine them to get
here actually a feature space are
learned because we use super pixels so
byxis so here we actually explicitly we
do not use permit idle approximation
here we explicitly do the entire filter
thing and we actually learn the future
transformations from the things I've
heard of the inception yeah because we
call this inception is because we got
this name from the Google inception
module so this kind of they use
inception because it can be easily
plugged into existing networks defined
what they mean why because they got a
layer looks like the inception what you
actually they got the name from the
inception movie so there is no formal
definition of what is in session
exception movie and it's kind of like
they say it does filtering in different
levels and then combine them together
for example 1 x 13 x 3 5 by 5
convolution you know no it is kind of
like convolution with different filter
sizes instead of one filter size in
normal CNN convolution with different
filter size and then combined together
here here we is kind of looking similar
light it's were convolution with
different bilateral filters and then
going with so we actually use super epic
instead of pixels because it is faster
and it's we can deal with unordered
points so why is this technique
interesting and useful because this this
enables doing long-range information
propagation between CNN units before
actually condensed to label size all the
density RF another post processing
techniques we actually first condensed
to label size maybe let's say 21 labels
and then doing the information
propagation by doing information
propagation inside CNN itself where
actually we can actually potentially get
better results and it could also help
grouping similar pixels would also help
in later processing also for example FC
layers already know all these pictures
are grouped now the task becomes easier
also so there I will show one results on
Pascal we will see with deep lab network
and they're also more results in the
paper so we took the deep lab
segmentation network and we added
inception modules inside it so be a six
off to represent bilateral inception
after FC six layer and with the two
kernels is the number of kernels so we
just train the module with by fixing the
other network so this gives some
improvement compared to deep lamp raised
during runtime you may like inference
that yeah entire inference time this is
1 35 milliseconds we had 20 20
milliseconds on top so by with with the
joint training of entire CNN and also
this we get some other improvements
sense and by adding more kernels six
Colonels instead of two we get better
provements and by adding every anywhere
we get some improvements after fz6r
efficient RFC eight we get improvements
yeah because this is only be aight aight
this is ba seven ba six so yeah we we
actually observe that B is earlier
stages work slightly better than the
last stage and adding adding multiple
modules further helps for bsx be a
seven-hour bs7 bah of the so this has
the result with the CRF this is the use
it den CRF denzel facilities worse than
this using bilateral inception and it
takes much longer compared to using
belittle inception things and and there
is also multi scale version of deep lab
where the use multi multiple scales of
the input and if you get the similar
results but here again it it actually
uses CRF on top the same results and
they are actually more other recent
works which is called engine eight that
also the dense pixel predictions things
so we get we get favorably better result
compared to their number and their
runtime which two techniques yeah sure
sure yeah yeah yeah this is this is this
is a snake this is the comparison with
civility try the bow
yeah we tried but it not give much
improvements because we are actually
propagating the same similar information
inside seen in itself this is this is
dense here yes den Seraphia these also
been seen and look learnt NLCS or yeah
actually in with Gaussian Perez
potential is here with Gaussian portion
is about the point of view in your
comparison you basically it will but
after learning and yeah we could also
learn ya Yin good and yeah we also
experimented with some other model at CR
FS r NN model also we also get
improvements compared to den CRF there
in engineer in in a cafe yeah we will
release code in couple of weeks this is
just accepted to cvpr so on a previous
work papers I was familiar with one was
FC and the paper from talk because again
the work from in the group pic tures but
oh yeah actually we just see I said
after what what accuracies where they
put it they have likely remember yeah at
present the state of the art is around
77 there yeah but this is new or worse
right that's the old early yeah but but
but here here the main point is to how
can we improve existing network it is
not like we are not just a split up I'm
just trying to gauge like how much it
things have improved like one year ago
like what these numbers like 50-60 like
or yeah before before before cnn's the
numbers are around 50 after that long
paper and other cnn's now numbers went
up to 77 it was a quite a big
improvement there is some other work on
d parsing networks and deep parsing
networks are now the philip towards
group also has some other system which
works quite well so here actually there
are some other we we also introduced
inception modules in some other networks
and also
showed improvements there so some
visible region super pixels ground truth
deep lab results which is which looks
blobby because it has low resolution and
these are the den CRF result and is just
kind of using bilateral inception model
we actually rectify some of the puts the
den CRF is mainly doing the crisping the
input image to actually rectifying some
of things using bilateral inception so
since you are using super epic said we
want to study whether we can generalize
to other super pixel layouts for example
we train on thousand super pixels can
this also work on six hundred and two
hundred super pixels so too so to study
this we actually hierarchically cluster
these super pixels into different number
of super fixes like 100 200 300 like
that and we actually observe that the
accuracy is dropping only marginally as
an amarok super pictures are going down
so this shows that with this also system
also generalizes to other super pixel
layouts not only on the trained ones so
in summary bilateral filters provide
simple at-rich framework for information
propagation and learning as i have shown
in the presentation learning by ATRA
filter has many useful consequences and
problem specific filters can be learned
and aj we're serious we can have
Bellator neural networks and generalized
NCR f2 non-gaussian potentials there are
three main consequences we studied and
as you know as they all showed that
information provocation can be done
inside CNN itself we don't need to do
later pre pro post-processing so why
another question is why why always grid
layouts for cnn's the greediest
particular layout with this approach we
can use any other layouts and we can
actually store information in
interesting locations so this is
somebody for this work and overall
conclusion overall work is like there
are many two main approaches for vision
generator and explanatory approaches and
previous works on informed sample and
concise message passing we bring the
growth worlds closer together a little
bit and in this newer work we show that
learning bilateral filter cells in
propagation inside CNN itself so we can
get the speed of CNN's
and also adding some prior knowledge
into it and the future outlook so how to
bring more prior knowledge interference
here we bring only very minor prior
knowledge how to bring more primal in
the cns while maintaining faster and
times and in general my main interest is
like how can we bridge gap between these
two models to further the to advance the
techniques we use for vision problems
yep that's it thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>