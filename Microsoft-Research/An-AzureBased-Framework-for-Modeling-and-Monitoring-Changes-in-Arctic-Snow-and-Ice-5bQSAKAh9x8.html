<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Azure-Based Framework for Modeling and Monitoring Changes in Arctic Snow and Ice | Coder Coacher - Coaching Coders</title><meta content="An Azure-Based Framework for Modeling and Monitoring Changes in Arctic Snow and Ice - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Azure-Based Framework for Modeling and Monitoring Changes in Arctic Snow and Ice</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5bQSAKAh9x8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so good afternoon our speaker today is
dr. Anthony aren't from the University
of Alaska Fairbanks geophysical
Institute and he is a career
glaciologist who will be addressing us
today on his work here at Microsoft
Research as a visiting researcher
working with Azure and with his modeling
framework so with that thanks very much
Rob and it's a really a pleasure to be
here today to talk to you about the work
that we're doing I want to just quickly
acknowledge the invitation from Rob to
come here and work as a visiting
researcher this has been a really
incredible opportunity for me to work
with Microsoft resources and really
apply it to the the signs that we're
developing at the University of Alaska
Fairbanks and also really grateful for
Microsoft Azure for research grant which
is supporting the azure cloud resources
that that will be the basis of what i'll
be showing you today and a lot of this
research is supported by in a wide range
of funding agencies most of which are
shown here a lot of collaborators
students and technicians too many to
list but I want to just recognize that
this research is very collaborative
effort and it comes from a lot of
different different folks at multiple
agencies so today I want to cover a few
different topics that are summarized
here the first is just to give you a
brief overview of the scientific
problems that we're addressing in our
research and summary of the different
kinds of data sets and I want you to get
a sense of the sort of complexity of
these different datasets and space and
time and how this calls for a kind of
different computational infrastructure
than we've had in the past I want to
talk about the sort of methods that
we've been developing in Fairbanks so
the over the past couple of years sort
of our academic approach to these
problems in the infrastructure that
we've built around that and then really
some of these incredible tools that I've
learned just in the past couple of
months here at MSR that are really
broadening those tools and allowing us
to accomplish really integrative and
collaborative science which is
the goal of all of our efforts here so
let's just jump into a bit of the
science just to give you a bit of a
flavor of the kinds of things we're
looking at we're studying the Gulf of
Alaska system this is a large band of
glaciers and ice caps that surround the
Gulf of Alaska and this is some of the
largest ice areas outside of the
Greenland and Antarctic ice sheets and
the reason we get so much ice in this
area is that there are some very large
mountain ranges there seven different
mountain ranges in Alaska and they act
as barriers to these massive storms that
coming off the Gulf and deposit large
amounts of precipitation in the form of
snow and so you get upwards of six or
seven meters some years and some of
these high mountain areas that create
the formation of glaciers of them flow
down to the ocean and discharge that ice
into the ocean so when you have all of
this ice and snow close to a coastal
marine environment there are a lot of
complex interactions that we have to
think about and one of the biggest
things that we're concerned with is the
fact that there's a vast amount of fresh
water stored in these glaciers that as
the climate is changing this is being
released into the oceans and this is a
major source of global sea level rise
Alaska contributes somewhere around
seven percent to the global signal of
sea level rise which is very high
considering how much Isis their relative
to the ice sheets furthermore we have to
think about all this extra fresh water
getting into streams and lakes and out
into the coastal fjord areas as that
fresh water which has a different
chemical and temperature signature then
the surrounding waters enters these
areas we it will have impacts on things
like the ability of salmon to spawn the
kinds of terrestrial succession that
occurs and then the overall ecology of
these fewer areas will really change
rapidly as the climate varies and we get
more and more of this water getting into
these fewer environments and we get
different salinity and temperature of
these coastal areas that causes changes
in the coastal current as you get
further out into the ocean so what we
have here is you know
the interdisciplinary and complex data
sets that for which we need a
computational infrastructure that allows
us to do collaborative and
cross-disciplinary research a little bit
more about the region that we're
studying this is in black the gulf of
alaska watershed it has about 27,000
glaciers somewhere around eighty seven
thousand square kilometers it's about
four percent of the area of the state of
alaska the annual discharge from this
area somewhere in the order of the
discharge of the mississippi river so
very high fluxes of water passing
through this area every year and we want
to determine how this is changing over
time so i'm going to take you through
just a few of the data sets that we work
with and again i want you to think about
the complexity of these data sets and
how they vary in terms of whether
they're vector data raster data and how
we can put together an infrastructure to
handle all this information the first
year is just basic inventory data which
is where the glaciers located so these
are really high density vector polygons
that tell us the location of ice in this
particular slide at two different times
will the yellow being areas where the
ice is retreated since the 1950s and red
being areas where there's been a bit of
advance so we need this basic
information before we can run any of our
models and do any simulations we have
point and line data that's for example
collected when we just put a
ground-penetrating radar underneath a
helicopter and we fly along and we use
that to determine the the travel time
and the distance then to the interface
between the snow and the ice that tells
us the depth of the snow pack for that
particular year and there's an example
of us flying on a glacier near the
cordova region taking these measurements
so high density of point observations
and it gives us you know traces like
this from which we can determine the
layering of the snowpack and pick out
that surface where we think the summer
surfaces from the previous hear and
determine how much snow is there and
then that is used to determine the
freshwater available for that particular
season go ahead
and when you fly over how do you get
anything underneath the picture of
anything underneath the surface but the
it's a 500 megahertz radar and it's
basically it penetrates through the snow
and then wherever you have a
discontinuity where there's ice it'll
show up like that this is actually up in
the higher accumulation area where we
get multiple layers it's called the
ferns own so we have to carefully trace
that line upwards there's tracing
algorithms that help us do that as well
and then a time series information from
things like these high-altitude weather
stations Alaska's very data poor at in
the high mountain regions as most
weather stations that we use to run our
models are located down your towns where
people live so we install these these
stations and we get off in real-time
satellite telemetry feeds of temperature
precipitation and so on and that is
another form of data that we're using in
our analysis and then finally a lot of
remote sensing information in this
particular case we're looking at data
from what's called the gravity recovery
and climate experiment and this is a
really fascinating mission it's two
satellites and we're measuring the inter
satellite range which tells us the
orbital influence of gravity on the
orbits of these satellites and from that
we can determine temporal variations in
the Earth's gravity here's a map of the
overall net loss of mass from different
regions around the globe and you can see
that really maps into where we are
seeing massive amounts of of glacier ice
loss and this is the time series for
Alaska so you see the overall trend of
mass change and then the seasonal cycle
as well from which we can determine that
that runoff component so again Alaska's
somewhere around seven percent of the of
the sea level signal ready okay that's
the net change in ice over a year so 65
gigatonnes of of waters leaving that
storage the ice and going to the ocean
change using by the year why is it it's
going oh there's less of water that's
being released in 2011 10 2003 um oh
this is the cumulative mass so that we
just start at an arbitrary zero we're
actually measuring the the actual mass
of the glacier we're saying relative to
where it was 2003 this is how much mass
it has a cumulative mass change
basically so summertime losing mass
wintertime it gains some ass and then
because that is trending downwards the
net change of storage is is around this
value here that gets down and flattens
out that means the ice is all gone and
now you know how much i stir was yep
that's true so what is the most you know
which faces the Antarctic they put it at
the greenback the Greenland right now is
the highest and what's happening here is
not only just the melting of the ice but
a big component of the loss there's the
fact that this calving of breaking off
of ice into the ocean we call it dynamic
mass losses and that's linked to the
fact that the oceans are warming up and
kind of eroding away at the aight where
the ice is meeting ocean and then you
get potentially very catastrophic losses
of mass in those areas i think it's
around thirty twenty five thirty percent
or so i'd have to look that up though
yeah okay so our goal is to kind of
model this process as much as we can to
reconstruct past conditions learn about
the system and also we're going to be
able to run future forecast models so
help you know where is the ice going to
disappear in a hundred years these kinds
of things so there's a lot of ways to do
this yeah go ahead sure
so so this is a friendly matter certain
latitudes in which you know pretty much
most of the main thing is happening in
stuff and why is it that the other
latitudes are not affected that much in
terms of the ice melting like winter
because pants you know close around so
it's the Arctic they make dashi at
Siberians oh so I've cut this map off a
little bit but you're saying why is the
loss focused on me right okay there it's
mainly this okay I understand yeah so
the biggest players are Patagonia Alaska
Canadian Arctic as far as the non Ice
Sheet areas also the Himalayas but this
it's off the map i just cut it off but
it's not as big a signal there's less
it's not as much quite not quite as much
ice there to begin with and also these
areas are more susceptible because like
i was saying earlier they're close to
the ocean they're more sensitive to
changes in climate because of higher
precipitation rates and they have ice
that actually goes into the ocean and
they're sensitive to these dynamic
losses as well yeah and then really
there's just not as much ice at
mid-latitudes you mentioned Siberia for
example save areas as fast planes but
it's not like big huge reservoirs
glaciers as much as it is just frozen
ground so the main thing is it just
frozen ground is amazing meeting the
water is that thing it's I think that
what Anthony saying is that we have
accumulated ice and glaciers big huge
massive ice that are facing into the
ocean and they're warm that's what's
happening in Alaska so you have this
mechanism in place for transport of
water and taxation whereas in Siberia
understanding is you don't have these
masses of ice so there's other things
going on there that are related to
climate change but not so much water
transported to the ocean
this is the net loss over this is the
trend in in mass right so that's this
sort of red line mapped onto here so
Siberia may still have the seasonality
but it's not going to be trending so
heavily because it just doesn't have all
that ice from which it can be taken out
of storage so I'll tell you a little bit
about our modeling efforts this is an
energy balance model approach and it
formally calculates the fluxes of
sensible latent heat solar radiation
solar incoming radiation is the main
factor that drives the melting of snow
and ice let a glacier surface a lot of
models to do this we're using one called
snow model made by Glenn Liston and
others and it has these subroutines that
calculate everything from the surface
energy bounce shown in this picture here
and then the redistribution of snow pack
due to wind the interpolation of climate
data so we're using these fairly coarse
gridded climate products and we have to
downscale them to an individual
watershed to get better estimates at an
individual location and then the routing
of the water when we generate some melt
at a given location how does that get
distributed across the landscape and
that uses a simple linear reservoir
approach so here's a picture of how that
sort of works the input to the model
will be things like a land cover mask
and that's where we go back to knowing
that where the glaciers are located with
those polygons these are examples of the
gridded to excuse me climate products
we're using reanalysis data which is
kind of a familiar with reanalysis it's
a it's like a weather forecast model but
applied to pass conditions so it
calculates all the physics of the
atmosphere but it also assimilates in
observations at the stations to give you
a best estimate of the past 30 years of
climate or so and that gets fed into the
model and then we get things like a
estimate of the water balance at our
location of interest so this is an
animation of the kind of output we get
we're starting in the spring of 2004 and
blue is where there's a net
the cumulative change in Mass into the
water blue is where we're getting
accumulation these high mountain areas
and you'll start to see some red
emerging that's where there's a net loss
of mass and that'll be the low elevation
parts of the glaciers so we're getting
into July and then it's going to stop
here soon in August and you'll see this
is sort of a net mass balance of this
entire region losses at low elevations
accumulation up high and we can use that
to inform a lot of our other estimates
and comparisons to the the grace data
okay so let's step back that we have
I've shown you there's point line
polygon gridded data multiple bands when
we look at satellite imagery we have
vastly varying spatial and temporal
resolution so we need an infrastructure
to handle that yeah go ahead interesting
about this group in that points with
substantial right I don't like the doc
left something yeah up in here yeah
that's seem like mainland appointed a
substantial amount of grain most of the
other is it was into the water yeah this
is the Alaska Range where you're right
let me this this I picked this year this
is a very it's a record warm year so
that was just a year where the melt
extended all the way up to the top there
was very little accumulation those
glaciers were really hit hard that year
so are you asking that with respect to
what I was saying about glaciers near
the ocean yeah I mean that wasn't a hard
and fast rule it's these interior
glaciers also lyrics they're especially
susceptible to the temperature of the
summer temperature whereas here
precipitation variability can play a
major role as well so things can really
fluctuate dramatically in here but you
get a really warm summer and those can
be hit quite hard as well but in terms
of sea level we don't we're not as
concerned about smaller glaciers there's
a few up in the brooks range as well
it's these are the main players because
they're so large and they potential to
add a lot to the ocean yeah that's a
good observation
okay so getting into sort of the more
computational part of the talk what do
we need to handle all this information
we need everything I've shown you has a
geospatial encoding to it so we need to
be able to do spatial querying in order
to accomplish that we need to be able to
route our data iteratively some of our
models require that we send the model
output back Ian and do ator model runs
direct access to our collaborators not
just within our Institute large storage
capacities and multiple processors to
run these large models so I'll go
through this quickly i wanted to outline
my different sort of views of
computational infrastructure that
scientists tend to use this is my
version of what i think typically
happens in at least in my field on the
academic side is a researcher will work
on an individual desktop and then
typically we're using you know things
like python and matlab fortran code
usually for the larger model simulations
generate a lot of flat files our raw
data will be an in tabular or flat file
format and then if we're lucky that will
get uploaded to some kind of national
data center and that's sort of required
by a lot of our funding agencies now and
then sent out from a web server there to
multiple web clients so let me just
quickly go through what I think is good
and bad about this this is a model that
works okay for the individual scientists
who knows where all his files are but
there are disadvantages because these
flat files require us to write a lot of
elaborate scripts to do any kind of
spatial relationships and it's hard for
us to collaborate with each other into
that model
store this in database so does it store
non-relational it started a text file
literally so it's really not a great way
to do things at all maybe you know some
standardized oh well it gets set you
know that's flat files up to a server
somewhere yeah then it's just not much
excited no it's not not at all I mean
speaking in general terms here one thing
that's good about this i think and is
that the data tend to be in these
national servers and labs you know more
stable longer term this is a question
that came up when a talk I gave at
university of washington recently is
y'all be telling you about the azure
sort of solutions that we're developing
but what happens to that in the long
term is it as stable as something like
some of these snow nice data centers
where we send our data and they tend to
have fairly good version control
metadata requirements but the main point
here is that the sort of a one-way flow
of information out to the client and if
i change my if i do some reanalysis on
this end or change some of the products
that i'm working with I have to go and
submit it again it might not get out to
the end users as easily as I like so
this is kind of a the model that in my
lab I've been trying to develop which i
think is a bit of a step forward over
the last couple of years and that is the
core of it is to build a relational
database for all of our work we still
write scripts to call into that and do
our analysis if we're doing any modeling
we run that off you know an institute
computational a super computing cluster
and then I have a local machine right in
my office that is a web server in which
this database lives and then that gets
served out to my Institute web server
and out to the community that way so I
really think relational database is
fantastic for our work and if more of us
put things into this format we would be
able to advance quickly i think is
just a example of some of the tables
here's you know those detailed polygons
here's some climate data time series of
weather stations stream discharge that
kind of thing we can join and relate all
this information together using
standardized relational queries you
probably know this really well i'm just
showing an example here of a sequel
query on a i'll run through it very
quickly i just wanted to show quickly
what you can achieve with this this is
an example of simply taking a yeah a
data of i want to know what's the
average temperature in denali national
park that's the polygon outline for that
and very simple sequel query like that
and i can get with just a couple of
lines the time series of the monthly
monthly data there so this is really i
mean that example is two kilometer grids
over this the state of Alaska so it's
not huge but it's yes gigabyte size
scouts yeah so we're using post GIS this
is really where the you know the
functionality comes in being able to do
queries whether a geometry is contained
in another geometry and I guess do you
work with sequel server a lot that's
something I see clue okay und no sequel
they have some of the crazy languages
okay so I'm going to talk a bit later
I've tried to migrate over the sequel
server and investigating whether the
spatial querying abilities are the same
as what I have with post GIS and that
they largely the same but there's some
differences that are making that process
of look okay okay good so this is an
improvement we can do these relational
table searches we can generate map based
products so we're using ESRI ArcGIS
server to serve that out to the
community right now and one of the
disadvantages of that is that we you
know requiring these proprietary
licenses with working with those
particular
it's still kind of a one-way flow of
information one of the problems is that
I'd become a database manager I have to
make sure i run the backups and keep the
database from from going down I can't
really scale it up or down based on the
needs of the community or how many
people are accessing it and then if I
want my colleagues to work on this it
hasn't been very successful yet I have
to teach them how to do sequel queries
make a connection to a database it's not
that difficult but you know it's just
haven't gotten over that barrier yet so
here at MSR this is a summary of the
kind of work that I've been doing is one
is to build an API for direct access to
our data and then investigate running
our hydrological models on on azure
virtual machines and then work on
visualization tools and see what we can
do in terms of visualizing spatial
temporal variability of what we're
working on to do tunisia oh the raw data
to the well the telemetry climate data
is just using you know we just send it a
static IP address to our collaborators
where the data is being generated and it
gets it's just a scheduled script that
runs and that directly I have a Python
script that then imports that into the
database directly uh it's iridium yeah
because it's out you know there's no if
we had 3g we would certainly try to use
that but most of Alaska right yeah I
just want me know what sorry what is it
rate the rate um so for that particular
data it's you know every every two hours
we get a dump of data that has 15 minute
Lee observations of temperature wind
speed and so on so it's if that that's
not huge datasets there but you know any
other data like the radar we would then
manually ingest that into the database
when we get back to the lab it's not in
real time
this is the work for the API I'm gonna
talk about next really is built on the
previous it's leveraging what Parker
McCready did here building working on
live ocean and I just want to recognize
that Nelson and Rob really did a lot of
the foundation work here I just sort of
took the live ocean API and just built
that into what i'm calling ice to ocean
and this is my sort of scheme attic of
that so the core of the work is this API
and we've changed things on this side a
little bit we still have this database
that has all of our data and right now
it's living on an azure virtual machine
and we're running our snow model scripts
or executive bulls also on that virtual
machine the output is sent to blob as
your blob storage and then their calls
from the API to get you know whatever
result comes out of the model and it's
served out to multiple clients I'm
working on learning more about
visualizations in in worldwide telescope
at all anything to show you on that yet
but learning how to make use of those
tools and that looks that's that looks
very promising as well this is just the
splash page of the ice to ocean API it
uses a rest webservice to get in this
particular case here we're asking for a
run off grid which is output from the
snow model for a particular year month
and day and then the API handles things
like the plot generation file transport
reproject so that call would produce
just a in this case an example of a PNG
that would come back through the API and
then I don't have to worry so much about
this side of things we're working with
for example collaborators at NASA who
are hoping to then make calls to the API
and build them into whatever fancy web
interface that they're developing this
is being put together using the Python
tools for visual studio the advantage
here is that again a lot of the research
community uses things like python and
matlab and this has allowed me to very
quickly get into this and really just
take my scripts that existed and build
them into the eighth
with very little effort so that's been
great it's using a django web framework
and gets published you know directly out
of Visual Studio to an azure website
looking under the hood of the API a
little bit so a typical one of the calls
to the API is get vector and so this
would be the request coming in from the
API call and then I can just simply like
I said cut and paste the queries that
I've already written this is that same
query that I showed you earlier for the
Denali data and so my region were Denali
would you know that's the variable going
into this query and then I go to another
function that's going to be called fetch
polygon that'll show you in the next
slide so you actually execute the query
and then the return through the API is
the data set that comes out so here's
that other function called fetch polygon
and it's using you know a particular
library that allows for connections to
any standard database this would be the
connection string to that database which
as I said lives on the azure virtual
machine just develop a set of a cursor
to that connection and then here's where
you execute the query so it's pulling
the data out of the database and then
returns it and then you know back out
the API again OMG i wanna show how big
is the distorted service it's just a
single virtual machine um I don't I
think I put eight processors on it right
now yeah but I haven't really played
with any of that do there nobody's
really hitting this with any queries yet
it's just sort of in development but
good look at them you know serious sorts
of visualization technology from the
looks like this is what we'd like to
happen eventually yeah yeah that's the
goal here so I wasn't sure who would
what the audience would be but I have
some questions or things that I've
investigated that I wanted to just put
up here and one is I did investigate
migrating everything to azure sequel
server database and I we're currently
still running things on the Postgres
database which as I said earlier this
requires a lot more work on my side so
why not just put it right into an azure
database advantage here is a really nice
seamless integration with visual studio
the management of who can access the
database is much cleaner I think through
Azure the web manager here and then
things like you know it's redundant Lee
backed up for me already and there's geo
replication I don't have to worry about
things like that the challenges that I
faced in doing this we've had to buy a
third-party software in order to do the
migration which hasn't been terribly
smooth yet and I haven't really been
able to quickly spin up on that so that
that just needs more time but that's
that's been a bit of a barrier and that
might just be related to the postgres
not talking well to a sequel server but
the other thing that I don't know much
about yet is when it comes to to me
having to pay for these resources out of
my my research grants I have to then
think about the costs associated with
with having things on on Azure and I
think there's a pate you pay for how
many queries people are using are
hitting it with and so those are kind of
the cost benefits i'd have to to
consider sequin solutions that are so
that would be using the table the azure
table
something like you know I do for
something like that you don't really
have you probably don't need database
acid sometime cakes and stuff so you
know I'm giving you know just store the
data in text files as it is huh and you
could there are you know I do i do jobs
that you can write including in Python
Java yeah that would basically read out
these what you just read text files put
the appropriate delimiters and calculate
this thick execute the same way that you
have the group by yep quick is it as
fast as something would be in the sequel
dozens of Hadoop framework yep they even
allow for a lot of cashing in memory
okay can actually be faster and what
about geospatial searching no find
things that are within a polygon is that
framework accommodate does dog oh but
but that but there are lots of like you
know optimizations that otherwise exist
okay now I've heard about that and I
just haven't gone down that path but
that's that's something I'll do for sure
I want to talk a little bit about our
modeling work here's the way things are
we're set up before I got started here
and that is we're working with
colleagues at Oregon State University
they run the fortran code this snow
model with all its subroutines on their
cluster and then manually upload it to
our Azure blob storage so one step for
we've may is start to run those models
on the windows server virtual machine
and then have a script that just
automatically sends it out to blob
storage and then from blob storage
through the API to the client so what
I'd ideally like to build into this in
the future and I welcome any ideas on
how to do this well is to actually allow
the clients to request for a model run
that is for specific
I'm an area so give me a bounding box
and I want to you know in a certain time
period so that we can then run the model
according to whatever the call that's
made from the client so that would look
like this which is focusing in on that
modeling side for a moment we would
actually go to the database to pull out
the grated climate data for that
bounding box in that time send that out
to multiple instances of the virtual
machine based on how many calls are made
and then that goes back out again an
application of this would be you know
somebody living in the the town of
Valdez is concerned about the amount of
water that's coming up this area
potential for flooding of this little
bridge that bridges this huge glacial
River so what's the discharge hydrograph
going to look like for a particular time
period and a particular area of interest
that's that's one sort of application of
that idea this is something I haven't
implemented but it's my understanding of
how I would go about doing this I
learned right when I got here about the
simulation runner and it was developed
here and in partnership with some other
folks and this execute parameter sweep
jobs which I think fits well to our
application where we're doing multiple
parallel executions of a program and
it's implemented as within the azure
cloud services so I understand that the
steps to do this are to create an
affinity group which is essentially a
virtual network it makes sure that all
the services are in the same location
for optimizing that process the storage
account stores the actual cloud service
and then it uses sequel databases to
store the information on different
executed jobs and then it's deployed as
an azure cloud service and you can
choose a cloud service package that
lists how many nodes or processors you
want to be associated with that so I
haven't done this yet but i understand
that would be a good way to approach
this problem now the final thing that
i'll show you is where we want to go a
step even further with this so i've
shown you snow model and how it can be
run
over different time periods and in
different spaces another step to add in
here remember we have all this great
information from our fieldwork that we
can use to better optimize the model
runs and so I'm representing that over
here and what's known as assimilation I
talked about it earlier and as you know
here's a model runs shown in this first
line that's uncorrect it and then well
we know from our observations out in the
field that the snow pack should have
been here and should have gone away by
this time so we do an additional
iterative model run that fits it back to
the observations and through that
iteration we actually get a better fit
through a process of assimilation so I
think how that would look is again you
have a call from the client for a time
span and a bounding box pulls out the
data that you need from the gridded
climate products and then it also says
we'll grab all of the other observations
from fieldwork and other sources that
also are within that bounding box and in
that time period and send that to the
snow assimilation subroutine and then do
an iterative run that allows us to
really improve that and again send that
out to the blob storage location so
that's the template I think for the
assimilation kind of approach so
correcting twenty percent scale errors
or higher percent scale errors I think
typically it's on the order of twenty
thirty percent but I don't know if
there's any limit on that I think it if
you have errors that large then it
becomes a little more nonlinear and the
actual iterations might be more unstable
i think so i think you want to be making
relatively small adjustments if you're
off by a hundred percent to begin with
and you should go back to tobias correct
your initial input grids so this is more
kind of tweaking things to a particular
basin or time period so this we're
almost done here I I wanted to just say
okay here's here's a future scenario a
couple of years from now here's a couple
of my colleagues out in the field
they've just taken
measurements of snow depth and some
ground penetrating radar from snow
machine maybe they've uploaded that to
to our sequel database and then they
make a call through the API for a
simulation of what's the snowpack and
look like next week and you can actually
do that using weather forecast data so
instead of using historical climate
products you can generate you know just
like any weather forecast that would run
through the snow model they come back an
hour later maybe have an app on their
phone say okay the snowpack is going to
look like this they can communicate with
the heli skiers and they can think about
whether that's going to cause a
likelihood of avalanche in this area
that kind of thing or talk with the city
of valdez okay there's a likelihood of
flooding this year and that happened a
couple of years ago the whole Road got
washed out and you know things were shut
down so that's to me kind of the nice
future model of how things could look I
think we have the capability to do
something like that with some resources
dedicated to that so I'll just wrap up
with a few summary points I think these
cloud resources are opening a lot of
opportunities for collaborative research
real-time delivery of data to our
stakeholders you know two main ways that
we can improve things really quickly is
more scientists using relational
databases and if we start putting our
computations into virtual machines I
think that can help us get a lot of our
work accomplished and then I really like
this trend of Microsoft integrating with
you know python and other scripting
languages this has allowed me to very
quickly get up to speed and that's what
a lot of academics are using so I think
this will help greater adoption of the
MSR tools thank you thanks a lot for
listening I appreciate it
and can you comment on scientists using
relational databases have you seen
anybody go through that conversion
process and that's my first two part
question and the second would be
supposing that everybody was you know
the community just kind of went off and
decided they were all going to learn
relational databases how would that
transform science at least in my field a
lot of my colleagues don't really even
speak that language I think it's a
different way of thinking it really
varies by discipline a lot i think so i
think people in geophysics just tend to
come more from just I'm writing a
Fortran program and I it's not
understanding of how data can be linked
together through a database I think they
don't realize they're using a relational
database when they use a GIS I mean
that's basically what that is is the
front end on a relational database so at
least in my career even you know I don't
see a lot of even just basic
understanding of what could be done when
I show what we're doing people get
really excited but they don't want to or
have the time to make that extra step
because it is a big task getting
everything into a standardized structure
like Abba perhaps you know some of these
other resources you've talked about
could help minimize that make that an
easier step sure you mean with the data
that I did showing here yeah absolutely
I mean I you know we have in I was
telling you about the thing that you're
working on right now on the students to
get data analytics we have a system that
is going you know well get some BB no
sequel
and you know it's something there's some
sample data and political ways that we
can try to like you know run it to see
how I think that'd be great and maybe
compare you know how quickly the queries
run and yard I'd be happy yeah yeah
let's do that that'd be great I have
plenty of different data sets we could
use no problem yeah as far as
transforming I mean I I did get a couple
of my colleagues recently to really are
in particular one of the postdocs who
were working with really adopted the the
Postgres database framework and I mean
the other co-authors on this paper were
just kind of blown away with how quickly
we could just investigate things you
know we really don't have to sit and
write a huge script to do things it's
just okay so it really enables a quick
look at our data I think and I think it
would vastly improve our ability to
collaborate and and relay data together
so yeah really I mean it's not the only
answer to this problem no it's like the
most evolved in a sense it's been around
for the longest time we see efforts at
University of Washington with e science
to feel how yeah working with your jog
refers to say will the solar and sequel
right and what's equal share and if
variance of that theme but you know it's
like you don't want to say everybody has
to learn you know left and right joins
but you do want everybody to sort of
have an awareness of of space yep you do
to have that know how I get it just puts
momentum behind me yeah I agree
there's a kind of intermediate stage in
between using sequel directly but sex
serve learners query and old school not
you when you're talking about which is
through the type of API are talking
about with live ocean and iced ocean
they'll have the ability to sort of
format stuff in terms of supported API
queries right right right she's thinking
you might be able to do and then as we
go through and do that we try and build
examples of an external habit that
generates those queries and then there's
sort of even closer to sequel step where
you say well I'm going to actually in my
in my interface my API your face on your
support of grammar which is even more
difficult to program but he's
potentially closer to the flexibility so
there's like this whole die out line
between old-school flat file at your
script and you know select star from
anyway the API would accept sequel
grammar is that what you're saying well
the example that I was talking to my
other collaborators with yesterday was
with this data system that we're
building for the for these old organic
matter which is actually related to this
the discussion about queries got so
complicated they're like they finally
sort of throw up their hands and they
said you know we really need to create a
logical grammar from which you can
construct where I think and so it's got
credit gets and it's got things getting
better more sequel looking feminine just
API calls with a main parameters myself
qualifiers angry it's a longer
discussion but it's great to have you
happy mr. yeah yeah yeah she'll have a
meeting at Jesse can I get your contact
information that's great you're just I
have
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>