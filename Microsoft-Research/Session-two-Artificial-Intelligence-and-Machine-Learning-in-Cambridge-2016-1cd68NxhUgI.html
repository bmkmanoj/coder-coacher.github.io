<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Session two - Artificial Intelligence and Machine Learning in Cambridge 2016 | Coder Coacher - Coaching Coders</title><meta content="Session two - Artificial Intelligence and Machine Learning in Cambridge 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Session two - Artificial Intelligence and Machine Learning in Cambridge 2016</b></h2><h5 class="post__date">2016-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1cd68NxhUgI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">back to the second session I hope you
had a nice coffee break
so the second session will will be for
talks from Microsoft Research and the
first talk is Antonio cremini Zee
talking about how machine learning can
solve medical imaging
thank you very much Sebastian for
inviting me to talk today and thank you
everybody for coming to this talk so I'm
glad there was a coffee break between
the previous session in this because I
think this session is gonna look and
feel quite different so I'll start with
some gentle introduction into medical
imaging and my work here is more to do
with the application of efficient types
of machine learning for the automatic
analysis of medical images especially to
do with automatic segmentation of
healthy anatomy but also are normally
such as tumors or aneurysms and these
sort of things which is of course of you
know great use in clinical practice and
so my research is always driven by this
basic question is what I'm doing useful
in practice or is it not and the theory
and the algorithms they come afterwards
so medical images are used you know in
Adam Brooks and in many other hospitals
in the Western world of course they get
acquired with you know very expensive
you know hardware they get used and they
use you know is also very expensive
because they are highly paid you know
individuals looking at these images and
then they get archived and they're
carving especially in the UK lasts
forever you know you have to archive
those images you know indefinitely and
of course it's also very expensive the
clinical use is interesting because
these images can be used mostly for two
in two areas one is diagnosis and one is
treatment and in diagnosis most of the
work is to do with Java of eyeballing
which is surprisingly incredibly
effective you know accurate and
efficient so trained individuals that
just look at an image like the one at
the top there
and say this is a glioma or glioblastoma
brain tumor and most of the time they're
right of course there are some edge
cases like diagnosing things like
malaria or other diseases that might be
very rare could be a lot more difficult
and that's where perhaps it would be
useful to have some sort of more
quantitative measuring tools but I
believe that you know automatic machine
learning based measuring quantitative
tools can be especially effective in the
area of treatment when you have things
like too was called longitudinal studies
you know two different time point images
images of the same patient taking are
two different stages you can see that
there is a tiny tumor here which was
grown in into a much bigger tumor in the
liver here then being able to say
exactly how much the tumor has grown
compared to the previous visit exactly
tracking the progression of the disease
especially as a function of you know
perhaps two different types of
treatments is you know invaluable right
now doctors do not have any good tools
for being able to do this content to
evaluation and so even in the treatment
area they just look at things eyeball
and say or
estimate the tumor is grown by it 100
percent of the sort of very not rigorous
type of evaluation so I've got a small
team here in Cambridge that now we're
looking at not only doing more research
in this area but also turning this into
a product that can be sold to clinical
experts in the field and can have a real
impact on how healthcare is delivered at
least in this you know particular sector
of radiology I'm gonna go through all
these points you know relatively quickly
because you know we don't have you know
the whole day of course I'm gonna touch
base on a little bit of you know the
science in particular trying to compare
one of our favorite tools which is you
know random forests with another one or
a feral tools more recent one which is
convolutional neural networks and look
at you know similarities and differences
I'm also gonna
a couple of demos so the you know can
convince you that these things working
practice on complex and dimensional
medical images and then I'm gonna you
know conclude right so of course you
know we're all aware of you know deep
convolutional neural networks and how
they have you know revolutionized the
field of computer vision and are
revolutionizing also the field of
medical imaging unsurprisingly but also
in you know this lab in particular we've
done a lot of work on random forests and
they also found very good applications
such as Kinect and medical images you
know what we've done ourselves and there
are some you know conceptions and
misconceptions perhaps you know some of
them are found that some of them might
not be so founded one of them that these
two worlds are extremely different from
one another other things like
convolutional neural networks are
incredibly accurate and indeed they are
and decision forests are also a cure but
maybe not quite as accurate as
completion on neuro networks you know
decision forest might be easier to train
and they don't require cheap use while
convolutional neural networks they
require big farms of you know GPUs and
and all these sort of things but are the
two models really so different from one
another this is what myself and my team
have been spending a lot of time looking
into and you know we reinvented
something there then we discovered was
known since the nineteen ninety which is
the fact that actually you can map any
decision tree into a two layer
perceptual and so this is really
interesting and you know the
construction of your how you can do the
mapping is you know quite
straightforward
let's think of a decision tree as a
function a nonlinear function which Maps
a multi-dimensional
input V into a unique identifier which
is an identifier of the leaf that is
reached by the tree by applying the tree
so what we can do is we can first all
take all the split nodes the internal
nodes and create a even
they are in the middle here at the input
we have just the different features of
the input vector you know V and here we
have all the nodes you know the are the
corresponding to the split nodes in the
tree and this is a fully connected you
know projection matrix and there are
some non linearities here and these you
know both base HS they're just you know
hyper planes which effectively
represents columns or roles of this your
projection matrix depending on how you
which were you represented and of course
there are nonlinearities as well we can
use the usual you know sigmoid or
rectified linear units there and then of
course you know the particular structure
of you know the tree can be you know
different you can you can have more
branches here or here is not necessarily
balanced and that is represented by a
sparse projection matrix on the second
layer where we have here we are the
final nodes here they represent you know
one to one the leaves of the tree and
this sparse connection here represents
the connections you know you have here
effectively is a way of representing and
operations and to represent the fact
that you have different branches have
got different depths you know you have
different biases attached to this your
final units and this is you know
well-known the world it is known perhaps
not very commonly known and you know we
we built on this and you know here is a
little simulation this shows how when
you input random numbers here
you always get to a leaf node and
actually in this case we have non binary
values because you know you can very
easily you know change the slant of the
sigmoid and so effectively that
represents your soft trees almost you
are for free practically the fact that
we have functional equivalence between
these two doesn't necessarily mean that
we have computational equivalence and in
fact this can be seen by the fact that
you know when a data point traverses a
tree it only follows one path if it is a
hard tree
therefore only computation attached to
the you know split notes to five and ten
needs to be computed everything else
runs no computation at all well instead
in this different equivalent
representation all these dot sorry all
these dot products in the first layer
need to be computed even if you have to
compute you know to multiply times zero
and then this and operation is applied
in the second part and so you can see
the in terms of number operations this
tends to behavior than this and so then
the question is you know what happens
when we go really deep and we have you
know deep convolutional neural networks
with multiple layers clearly they
perform a lot of operations and can we
you know use this intuition to try to
minimize you know the number of
computation and therefore increase the
efficiency of this new version of tree
if ID convolutional neural network if
you want and here's another way of
looking at this problem so imagine that
we have a conclusion on your neural
network and we represented this way
imagine that there are lots of say
layers before here and this is the final
layers and imagine that we have trained
this conclusion neural network on the
standard image net image classification
problem then what we can do is we can
look at the co activations of you know
neurons in this layer in neurons on this
layer so this and this you know purple
is high and white is low so we see that
there is a little bit of your structure
going on there and if we reorder the
rows and columns I will order the
neurons on the layer one and layer two
you know we clearly see this you know
block diagonal asymmetrical block
diagonal structure and so now we can
actually zero out you know the
off-diagonal elements completely which
corresponds to eliminating some of the
edges you know going from layer we want
to layer v2 and if we gave if we
continue doing this for successive
layers then you know we can see that we
start to get roots or branches and so
this is another way in which we can
understand you know the sort of
relationship between convolutional
neural networks and decision tree this
now becomes you know a branch structure
ie a tree so based on all these sort of
intuitions then we decided ok you know
let's really try to push these ideas and
you know first of all you know we need
to come up with a graphical language
that can help us you know deal with you
know combination of CN n type structures
and decision tree type structures and we
definitely cannot use this sort of
graphical notation you know which is you
know taken from the Chris s key paper
for instance because it's a very complex
and perhaps too detailed and so we
represent our transformations from one
layer to the next using this simpler
notation and this is just a graphical
language nothing particularly
interesting about this and so we have
the projection matrix just a matrix your
linear transformation followed by the
squiggle the your it just represents a
generic non-linearity could be a real
ooh could be sigmoid orange or whatever
you prefer so now we can make you know
we can create really complex and modal
models so we call this thing a
conditional Network just because we
needed to invent a new name but the
interesting part about this is that you
can really combine together some of the
properties of decision trees with some
of the properties of convolutional
neural networks so for instance you know
this part this will be behaves very much
like a tree you can see that the date
are going from one layer to the next is
not transformed just like in a decision
tree this is a identity and there is
this red thing which is nothing but a
very small single layer perception the
acts as your split function your
decision function and open source of
these gates to send the data either here
or here here this could be a hard gating
function or a soft gating function and
so on then we can have other
transformations so in this layer here
perhaps we have a gate or split function
but also the data it is
transformed through a nonlinear
transformation which is more typical or
convolutional networks and then we can
have merging operations which make this
whole thing behave more like a dag
rather than a tree you can select data
you know some features from this vector
into a smaller vector here so you can
mix and match all these different
operations into a more general type of
structure so if you look you know from
beginning to end your D sort of
yellowish path is very much a
convolution on your network a structure
like this is a more traditional decision
tree in this case we have a ternary and
then binary splits and the cool thing is
that if you choose the right
non-linearity is a sigmoid or rectified
linear units something that is
differentiable then the whole structure
continues to be differentiable and
therefore you can trained in using
gradient descent back propagation of
course this is a lot of maths I'm not
expecting you to read it all but just to
show that it can be done so what we've
done is we put this idea to the test
and using the imagenet testbed we looked
at one of the existing convolutional
neural network architectures this is the
one from the Oxford group you know
called vgg 11 because uses 11 layers and
because we believe the imposing on this
8 realized structure can improve the
efficiency rather than the accuracy then
we decided to start from the structure
looking at the layers which were most
computationally expensive so please
these are clearly in the convolutional
part and split those into branches
different routes to subdivide the
computation in different areas and so we
came up with this you know alternative
structure where you can see this sort of
you're branching architecture and indeed
when you we put it to the test we see
that in this 3d plot where we have error
cost ten time costs test time cost in
terms of number of per Asians and model
size
so for your good architecture you want
to be as close to the origin as possible
so Google Knight is an incredibly good
architecture is not only very accurate
but also very efficient it was designed
by hand and by search in many ways and
our conditional network is a derivation
of this network in yellow here the bgg
11 with exactly the same accuracy but
much greater efficiency both in terms of
number operations and also in terms of
reduced number of parameters only have
five minutes and so rather than talking
about more architecture than CNN and
stuff like that also wanted to give you
a flavor of the sort of applications
that were building and we're thinking of
where we use this sort of efficient
machine learning in the analysis of
medical images so this is a small demo
driven by our collaborators at the
University of Washington Medical Center
the radiologists we're working with they
work in ER and they're interested in
trauma cases you know people who are
admitted into the ER because they have
been in a car accident or bicycle
accident and something is broken
maybe bones are broken with them is not
clear first thing that happens is the
the patient goes into a CT scanner and a
sort of your large view of their body is
captured and what they do is they
typically use the patient's spine as a
patient-centric
coordinate system to figure out what is
broken and how to refer it back to the
individual vertebra each of them have
got a name and so what we do in this
application is I just load this image
may see if I can do it yeah and as the
image gets loaded the computation
happens in the background and done so
now we have the knowledge of where the
spine is and each individual vertebra
what it is called we can run a very
simple PCA type refinement which you
know you will see the
image wiggle about a little bit and now
that the system knows where the
individual vertebrae are and what
they're called we can just very simply
navigate to the individual vertebra to
the best view by clicking on the names
and this is actually a very practical
application that saves radiologists in
the ER Department a lot of time a
similar application again using random
forests and these sorts of technology
that I explained to you is this one this
is a magnetic resonance image of a
patient's brain this is from the
Cambridgeshire area this is in
collaboration with Adam Brooks and what
we do is we trained one of those you
know models to recognize for each
individual voxel in this picture which
class it belongs to so it's a little bit
like the Kinect problem but looking at
3d images and there are multi modality
images actually we have several of this
deal with different MRI meters so we can
very simply click on run segmentation
the forest model is loaded it is applied
you know live on on the image and this
classification happens the three
different colors indicate the actively
proliferating parts of the tumor in red
and then the necrotic or the dead part
of the brain in green and then the
inflammation area which is mostly
healthy but not completely healthy in
yellow and this is another one of these
tools which saves a lot of time and a
lot of cost in clinical workflows such
housing or radiation therapy planning so
I'm gonna stop here because I was
instructed to do so by a German guy not
by myself but it leaves time for
questions let's first think Antonio
again and then thank you
yes
usually when you start composing
structures on something which which does
not vary automatically it starts getting
the results down so you force your way
that the Newman efforts and the 3/4
meters there is a lot of previous work
where first they train a CNN structure
and then they specify and you can do in
many different ways we don't do that you
know we use the intuition to actually
construct a new structure which is not
trained at all is randomly initialized
and we train it from scratch so the fact
that we're turning from scratch is makes
a big difference to our results and also
compared to previous work yes in layers
you know you always have a
representation of the output of the
previous layer as a number of features
right so there are two ways in which you
can split the data to left and right
branch one is using an explicit explicit
branching operator a split function
which takes entire vector analyzes and
says this goes to the left discuss the
right in the number of dimensions is
exactly the same and another one is what
was used by the original Alex Chris s
key paper were instead without having
any explicit split function they just
say half of the features go to this GPU
and half of the features go to the other
GPU we explored both techniques here and
so we talked about both explicit
splitting and implicit splitting is not
a fantastic terminology but that's all
we could come up with
come up with okay if there one last
question could you give us an insight
into what would guide you in that versus
the intuition you know we looked at the
cost of the operations in each layer and
so you can you know compute the number
of operations where they need to be
performed in each layer and inevitably
you find the convolutional layers are
the most expensive ones and so that's
why we tackled those layers first and
you know split up those layers and you
can you can use a similar argument for
the number of parameters because of
course you know one of the things we are
aiming for is not just computational
efficiency but also achieving the
highest accuracy as possible with the
minimum number of parameters as possible
to avoid probe or minimize problems like
overfitting okay let's say thank you
very much and sorry for the delay so I
would like to talk about neural networks
today and one of the most interesting
properties of neural network is that it
is over primates right meaning that if
you think about the left hand side the
state of functions you can represent
with your neuro network and on the right
hand side you have all the parameters
the space of parameters the mapping
between the two sets are not one-to-one
so for example if the activation is
rectified linear then what you can do is
given one node you can scale down the
weights coming in to this unit and scale
up the weights going out of this unit
and that doesn't change what the
function computes it's exactly the same
function but therefore a different
weight configuration so I'm wondering or
I'm thinking in this line so because of
these all these ambiguities
so does neural network general is better
all because of these ambiguous
or over parameterization or other
question if if we are aware of what kind
of ambiguities we have can we use that
to build better optimization algorithms
and in order to answer these questions
there is a more basic question that we
would like to think about what sort of
ambiguity if they are so I talked about
the node node wise rescaling ambiguity
is that all the ambiguities we have or
do we have more than that so to start
with let's think about this rescaling
ambiguity in more detail so pick one
node in the first hidden layer and
assume that every normally narratives
are rectified linear so I can do the
following transform that I can divide
this incoming waste by the norm so now
the weight coming into this node have
unit norm and I multiply the constant to
the the weights going up so this doesn't
change the function at all and I can
repeat this process for every node in
the first layer so all the nodes in the
first layer have unit norm now and I can
repeat this process for all the nodes in
the second layer and so on so everything
is normalized up to here except for the
last layer the last layer you cannot do
this because there's nothing going up
there so the scale is meaningful but you
see that it can normalize all the nodes
up to the last layer without changing
the function at all so it means that the
scale of the weights carry no meaning
except for the last layer so it doesn't
change the function and in particular
you can write the following thing so
this is a set of functions you can
represent with your original parameter
ization all the functions with your
original prioritization
you can restrict the norm of the waste
coming into each node except for the
last layer to be one and this doesn't
change the state of function we can
represent right and it can even do
something even more crazier instead of
primary
rising by the original parameter you can
say that I'm going to parameterize with
W tilde and divide by the norm of W
tilde so now there is no example that
explicit constraint but this set of
functions is exactly the same as this
set of functions so all three
parameterizations are different but the
set of functions you can represent are
the same so this might be a little bit
worrisome so actually you can test that
easily on amnesty I said so this is a
four layer network with 1024 units in
each layer and I plotted the norm of
each layer along the epochs right so you
can see that there is one layer going
that the norm is going up and this is
you can see that it's the last layer so
that the normative last layer carries on
me but it except for the last year all
the layers up to the third layer the
norm is constant so obviously because
changing the norm that's not changed the
function at all the optimization
algorithm doesn't care about the norm
because it's it doesn't make you
classify better or worse so you can see
that only the norm if the last layer is
going up and all the other layers are
staying where you initialized the
network it's an interesting observation
also you might think that this is
special because if the role unit works I
also did a simulation with sigmoid so
now the theory doesn't hold exactly but
it seemed that the again only the last
layer the norm of the last layer is
going up and the norm of the first three
layers are staying almost constant it is
going slightly more it's changing a bit
more than the regular network so the
theory does not hold exactly but a
similar trend is here so in fact you can
show that if there is the direction that
you change the change the parameter but
the function does not change
significantly then you can say that the
actually show that inner product of the
gradient and the direction is zero so
this is a simple basically chain rule in
saying that this is a Taylor expansion
and there's no first-order term so the
dot product of the gradient and or the
this ya Cobian and the Delta must be
zero so that's the reasoning
so basically gradient if you do a
gradient descent you'll not go in the
direction that does not change your
function so that's very reasonable
right but is that is that good enough so
now think about the to parameterize
ations I I mentioned in the beginning so
in one parameter ization you update your
parameter in the direction of your
gradient fine and there's another
parameterization basically you divide
your nor your weight with the norm of
the weight and if you only do that for
layers up to except for the last layer
the set of functions in percent are
exactly the same so don't you don't have
to worry about it
but now the optimization step looks
different this is update step in this
new parameter ization you can always
come back from this privatization to the
original privatization and you can ask
the question if I move one step in this
furniture ization and do the same
mapping going back to the original
privatization we can do the same
calculation in Taylor expansion and you
see that this is not the same as if you
moved in the original privatization
moving in the original part miss
relation is not the same as moving and
other privatization is coming back to
the original poetry so gradient descent
is inherently privatization sensitive so
in this respect I think there are two
philosophies so one is that because all
these things or most of the things we do
are parameter ization sensitive you can
say okay let's speak a parameterization
that works the best so this relates to
what I'm going to talk about called
batch normalization that was proposed by
some people from Google last year and
also if you analyze a page in your own
net
there's a very nice book from Samia
Watanabe and this is a kind of
philosophy behind it so it's it so it
tried to justify one unit or network is
good in some sense the other part other
philosophy is that oh no
we we don't want to be sensitive to this
kind of arbitrariness in the way we
define the model we we should only care
about the set of functions we can
represent with the model right then we
want to be invariant to the parameter
ization so there are different methods
for example natural gradient or Bayesian
neural networks if you use Geoffrey's
bio
and there is a method called passive GIM
I would like to comment in this talk
that is built in this philosophy so
let's look at the two things so how much
time okay good good so let's look at
this batch normalization first which is
built on the let's find a good
parameterization in philosophy okay so
the idea of batch normalization is that
you want to normalize the pre activation
before applying a non-linearity I mean
so that it has mean 0 and unit variance
the way you realize this is that you
have to have a separate unlabeled
examples which you use to calculate the
mean and standard deviation or variance
and then you use that to subtract the
mean and divide by standard deviation so
if the statistics of the training
mini-batch and the normalization
mini-batch are the same then this
transformation written here will
guarantee that it will have mean 0 and
variance 1 and this seems to work very
well in practice and I'm going to look
at this as a repro miniaturization so
you can easily do this transformation
that basically what you're doing is
subtracting a mean and after that
multiplying this weight which is defined
as original weight divided by
something that looks like the standard
deviation so if you compare this to what
I talked earlier you can see now that
this is something that depends on data
because you have this covariance matrix
that is calculated from the the separate
unlabeled mini-batch so now this is a
repro motorisation of the weights that
you know you're not restricting the set
of functions you can prove them because
of all the the scaling ambiguities you
have in the network but this
privatization is a data dependent
reprime miniaturization so because this
is a reprioritization so you can call
this a new parameter and this is not
parameterization environment and what
you're doing is a gradient descent on
this new new parameter ization
but it works well very well in practice
in terms of both generalization and also
optimization so this is interesting
so there's another method that I would
like to comment about this is based
built on the other philosophy that if
there is such ambiguity or
in variances I want to be invariant to
such thing that the optimization
algorithm should behave the same way in
one parent relation and the other
prioritization so this is an algorithm
that is derived as steepest descent with
some specifically defined norm on the
network but if you look at it it is just
a element-wise rescaling so this is a
gradient original gradient and you scale
this with this constant Kappa E and what
it does is that assume that you want to
update this weight coming from this node
to this node so this is defined as the
sum over many passes that goes through
this edge so this is one pass this is
another pass this is another pass and
you can enumerate all the passes and
what you're calculating here is the
product of the square of the weights
along this path except for the weight
that you want to update
so this
seems like a very strange thing and also
it looks like you have to enumerate all
pattern that's going to be
computationally hard but actually this
thing can be computed very efficiently
with efficiently by just one forward and
one backward propagation so
computationally it's very cheap and it's
just a element-wise rescaling of the
update and what it does is that actually
it it is invariant to this kind of node
why three skating I talked about so
assume that the incoming weights are
scaled down with alpha and the outgoing
weights are scaled up with alpha then
what happens is that this constant is
going to be smaller for the outgoing
weights and this constant is going to be
alpha square times bigger for the
incoming weights and this cancels with
the skating of the degraded itself and
it turns out that update of the outgoing
weights are proportional to the scaling
factor alpha and the update of the
incoming weights are inversely
proportional to the skin parameter so
this will behave just the same way if
you randomly rescale all the nodes in
the network it'll just behave the same
way as you would in the original
prioritization so so finally I you think
I have three minutes or so okay so
finally the question is okay so this
algorithm is invariant you know twice
rescaling and it works well so is that
good enough because there might be other
kind of ambiguities or in variances in
the network that we have to be aware of
so let's do a small simulation so it's a
three layer network with 64 inputs and
thirty-two thirty-two and ten outfits
just like a small version of amnesty so
how many ambiguities do we have right
the question is how many ambiguities do
we have so there's a serum there
informally stated
the number of parameters in this network
is the sum of the degrees of freedom so
this is the active part degrees of
freedom is the part that if you change
the parameter it will respond in a sense
that it'll change the output of the
function and the number of ambiguities
is a part the subspace we can imagine
that you change the parameter in this
direction but doesn't affect the
function at all so there's a kind of
rank nullity theorem here that basically
you can decompose a subspace into this
the the space that your span with your
parameter change in your parameter and
the direction that doesn't change the
permit the the function you represent at
all so my guess was that if all we have
to worry about is no y3 scaling because
we we have 64 internal nodes so for each
of them I can do this trick right I can
for each internal node I can scale down
and scale up and your now let's go up so
I have at least 64 and the you atif here
for each internal node so if so the
total number prior is 304 3466 and if
node y3 scanning was the only kind of
ambiguities we have here then the
degrees of freedom should be 34 there
were two so right so is that true or not
so I did a simulation last night and it
turns out that it's not quite there so
what I plotted here was I calculate the
Jacobian of the network and I plotted
that the rank of the Jacobian along the
number of samples so obviously the more
samples you have the span will increase
so the rank is a monotone increasing
function and I tested up to four around
4000 samples and it seemed that the rank
kind of asymptotes around here
it is slightly increasing so you cannot
say anything concrete from this but
there seems to be some strange gap
between what I imagined from just count
the number of internal nodes to the rank
I calculated from this example so so
this is a last slide I just want to
discuss the limitation of the approach I
just did so the rank of the Yacoubian
depends on two things so it depends on
the parameter you pick and also it
depends on the input distribution so the
primary dependency is critical because
you can definitely choose the generate
parameter configuration you can set some
of the weights to zero and be pretend
that there is no you know units there
right but the question is is there a
typical behavior right if you are in the
typical operating regime with the
network is there a number that you can
say roughly okay this structure so the
degrees of freedom should be that and
this kind of typical behavior there is a
type of here which may arise if you
consider a large network that was like
64 and 32 32 so that's not a large
network at all but if we consider large
network maybe this kind of typical
behavior may arise the other dependency
is the input distribution so it
definitely depends how rich your input
distribution is and of course you can
always replace that with the random
numbers and say that my network
effective random numbers but the
question is how far that is from the
reality when we train something on M
Nisour when we train something on
imagenet that is different from a random
example Gaussian distribution so there
there is an interesting question here
that how can we separate the property of
the network that is the degree of
freedom I just calculated from the
property of input distribution thank you
very much
two questions so one of them is some of
the symmetries in the network come from
permutations which is hard to
incorporate into this framework because
there's a discrete so Jeff some some
might come from rotations in the weight
space right so you could do rescaling
but you could also I mean values that
are not rotation invariant but you know
the linear part certainly is and some
some of this has to do with the fact
that for Lu is just zero when it's zero
actually you don't care at all about the
parameters right right so in that regime
yes so so regarding there's definitely a
interesting aspect for the input layer
because rotation and empat layer is free
right again like rotate or transform
your input data in many different ways
and typically you want to be embarrassed
to that kind of transformation so I
would say you want to be invariant to
inserting a linear layer under your
bottom layer that should be a natural
requirement for a network and that is
definitely not accounted for this in
this rescanning invariants two ways of
getting around it one by basically doing
something like passed path sgt one by
doing batch normalization so what what
would you suggest would you prefer
so you know more principled view of this
would be to be embarrassed right that
that seems to be you know more
principled you know if you think of the
set of functions that that's all you
care about you don't want to worry about
the parameterization because there are
so many tricks you can play if you make
yourself parameterize ation dependence
but obviously people have come up with
all these tricks and they work well in
practice so it's hard to ignore those
progress and it's not only about
optimization but it's
also about generalization that makes it
even more curious based on online social
network profiles right so I'm I have the
privilege of being last thing standing
between you and lunch so I'll try and be
quick but you know interrupt with
questions anytime so yeah the topic is
hiring in social networks so you know
clearly social networks are very very
influential but when we think about this
we tend to focus on friendships and our
relations with friends but they carry a
lot of weight than in the real world
there are some mistakes that you can
make in social networks that will cost
you dearly like this example here right
so obviously they carry traction in the
real world and they've been a revolution
not just in how we communicate with our
friends but in other circles as well so
Java had very very interesting surveys
they do this every year and that's about
recruiters in social media so basically
what they did is they asked lots of
recruiters lots of questions about how
they used social media in hiring
decisions so first it was they asked
which social networks are be used for
recruiting and you know big surprise
LinkedIn is is one of the top choices
but then there are some other choices
that might surprise some of you like
Facebook is very very commonly used and
maybe Twitter and and Google+ and some
of the other ones and you might ask
yourself well why why are they looking
in these networks right LinkedIn was
designed for hiring why do I need these
other ones and then you know the from
the survey they've learned that 93% of
the recruiters will review
these social networks before reaching a
decision so it's not just a fringe
phenomena it's not just you know that
very very few recruiters use this
awesome use this and then so they broke
down the answers what what is it you
look for in a candidate based on the
social network so clearly you know they
used LinkedIn for professional
experience but they also look for
evidence for that you know some of them
in facebook and then they look at stuff
like how long you've been with your
current employer they look at your
industry related post they look at
things like mutual connections do we
know someone that we can ask about you
so they're quite sneaky they look for
specific hard skills again less in
Facebook and more in LinkedIn but then
you know comes up this nasty thing of
cultural fit how well will you fit
within the organization so they're you
know Facebook soundly comes in to say
much more and then sometimes they even
look for examples of some of your work
in some fields so clearly very
influential so you know some of you
might remember what I call the old world
like back then in the day if you wanted
to apply for a position you send your CV
and an application letter and then the
hiring manager or someone in the hiring
firm would look at this and decide
whether they want to bring you for a
face-to-face job interview and then if
they like using you know they would
think about this a little bit and make
you an offer or not in the new world we
have a slightly different structure so
you know they use both your CV and maybe
your application letter but they also
investigate you an online in online
social networks for example like
LinkedIn accounts and maybe headhunters
are involved in the process and the
screening is a bit more tiered right so
suddenly you know there are multiple
steps you don't just send your CV you
send your CV they take a look if they
like it then they move on and they
investigate you online if they like what
they see then they bring you over and
then maybe there you look at your online
websites a bit more in in-depth and one
nasty trick that sort of snuck in into
this world is asking applicants to hand
over their passwords so sure you know
some of these you might be aware that
some of these networks have some privacy
settings so not everything you put
online is available for all the world
to see you can restrict the axes so
assure you can restrict the access but
then the hiring managers or someone in
the HR department might come to you and
say look you know we understand you've
got a lot of nice content there we want
to see it before we decide whether it's
hire you or not you know send us your
password and you know okay some of you
are a shock but you know if you're
desperate for a job you might give them
the password
there are also lighter tricks for doing
this so they might come to you and say
look you know come on over sits with our
HR professional typing your password we
don't need to know your password we just
want to look at the content well you
know what we'll scrum through your
profile you can be there to make sure we
don't say the emails in your behalf or
anything like that we're not going to be
me so you know this is a rather new
phenomena some of you might be surprised
but this has happened if you look for
general media reports about this this is
becoming more and more frequent and in
fact some universities have also done
things like this and that reach the
general media when they look for
potential people to come for the
university so it's not just you know
hiring person fine okay what what are
they looking for right
so what job IQ is is is self reports
they ask recruiters well what do you
perceive as important so you know a lot
of the recruiters 55% have reconsidered
a candidate based on their social
profile most of these were negative
reconsideration so it you know the
social network did not help the
candidate in most of these cases what
what could you do wrong in in social
media you might ask yourself so here is
what the recruiter said you know you can
curse you can have lousy spelling and
grammar you can reference you legal
drugs you can have sexual posts
sometimes you can have something that
relates to volunteering in donations
that's actually on the positive side do
you see this yes right here you know
political affiliation does come into
play most of them said I'm neutral about
this I'm not sure but maybe alcohol is
probably on the bad side guns and so on
fine so that this is what they perceive
as things not to do in social circles in
social media but then there's the
question okay what actually influences
their opinions in practice so
you know that's the reason for studying
this through through an experiment which
I call the Facebook job interview
experiment so it's rather simple I
sourced a bunch of Mechanical Turk as
people on Amazon's Mechanical Turk
roughly 500 raters to examine a bunch of
volunteered Facebook profiles roughly
250 profiles in total so each of the
Raiders would examine one or more of
these volunteered profiles and they
would write a report about these
profiles I'll tell you what's in the
report in a minute but basically the
task is I want to hire this guy for a
job please tell me what you think about
this guy and they produce the report so
there's a nice little dataset was 850
such reports simple enough so I chose to
focus on some specific components and
some specific criteria that relate to
hiring so in terms of profile components
so this was carried through Facebook so
the profile picture then all the other
photos that people upload the items that
they like or the groups are members of
their friends and the textual posts
themselves so you know these are profile
components there are also some specific
criteria that you might consider with
regard to hiring like in that job white
survey so skills experience and
intelligence that's not a big surprise
attitude and personality you can
consider physical appearance some of you
might say well how is that relevant to
jobs but you know that's one possible
criteria there is offensive contents and
content relating devices so you know all
the stuff like offensive language drug
and alcohol and and so on then your
interests activities and hobbies and
then a lot of demographic traits like
you know your age family status whether
they're just or political beliefs or
other demographic traits some of these
might not be considered illegal to ask
during a job interview right so I wanted
to make the separation between perceived
importance and actual importance so all
the things in the data set contained is
basically here is the radar here is
their target the person that they've
looked in in their profile and then
perceptions regarding these various
issues about the quality of the
candidate you can ask people to give you
perceived importance like in the job
light
which basically says how important do
you think this factor would be in your
hire decisions before you've even looked
at a single profile and then the way I
did it is using a certain amount of
points that they had to allocate across
the profile components or across the
criteria so for example here are our
hundred points tell me how important
each of these factors are like your
experience and your intelligence or like
your physical attractiveness and so on
but alkie 100 points across these they
have to sum up to 100 and then to
measure actual importance there were
lots of questions like how well does
this specific candidate do in your eyes
regarding a specific criterion so you
know focus on a single thing regarding
this counting only judge their
experience and intelligence ignoring all
the other factors as best you can and
tell me how well this candidate does in
that specific criterion and then you can
apply all kinds of factor importance
measure which try and measure how
critical this factor is in in the
overall perception so some techniques
borrowed from econometrics so let's go
into this in a bit more depth so the
only tool used in this analysis is
linear regression so you know the
simplest form you can think of so we
have these focused ratings focused
ratings relate to a specific criterion
or a profile component and there are the
score of a candidate regarding only this
trait so the people that gave these
focused rating try to ignore all the
other factors so I ask you a single job
credibility criterion or I asked them to
give a rating regarding a single part of
the profile component tell me what you
think of this candidate based only on
the profile picture if you can and then
I applied some linear regression models
that go from either the focused ratings
of job stability criteria to the overall
score that they gave to the candidate or
going from the focused rating of profile
parts into the overall score so I have
both the total score that they gave the
candidate and these broken-down scores
to focus on specific things and I apply
a linear regression model so you know
the goal of this trick is not to figure
out
the best candidates are a rather it's to
figure out which are the important
factors when people actually make these
hiring decisions so this requires a tool
for computing factoring importance
measure in regression models and if you
look in the literature there are plenty
of of these floating about I chose three
of them so one is partial correlation so
these are correlations between a factor
and the target trying to control for all
the other factors you want to ignore
there are normalized core normalized
regression coefficients so basically you
first apply a transformation to z-scores
on every single input in the in the
system you try to normalize them fit a
normal distribution to them and then you
can just use the weights that you get in
the end of the regression as an
importance measure or you can look at
the change in the coefficient of
multiple determination so basically you
look at your accuracy measure does
r-squared when you take all the factors
together including a specific factor
whose importance you're trying to
measure and when you apply their same
regression when you exclude that
specific factor so the difference in
accuracy between the two allows you to
figure out how important that factor is
so if you start looking at the the
profile component so what I have here in
blue is the perceived importance of
various profile components and in orange
the actual important so perceived
importance is you know how important it
is using this hundred points allocated
how importance they think a factor is
and the actual important is how much
this influenced them and practice based
on the data available so as you can see
all these factors come into play you
know at varying degrees but even the
smallest one which is the friends this
is about 12% 13% of the overall opinion
regarding candy so we know all of these
come into play but there seems to be a
decent correlation between the perceived
importance and actual importance so you
know what whatever people thought would
influence their opinions actually bid
influenced our opinions in practice one
interesting bit is that the profile
picture and it actually has a much
higher importance than its perceived
importance so when in fact the profile
picture was incredibly influential on
people's opinions
when it came to hiring yes well so yes
they view the the profiles using the you
know the standard Facebook in so
basically I provided a link and they
follow the link except whoever
volunteers their profile had to remove
all the privacy settings so this is a
simulation of what happens if you were
to remove all your privacy protection
order to hand over your password and yes
you know though the picture does appear
they're visually it's very very
important but again you know the task
was trying to ignore the the profile
picture when you focus on the other
stuff yes yes yes so first impressions
apparently do count but then you know so
you know unsurprisingly the post that
people have are also very very important
the photos are still important maybe an
actual lesson perceived importance but
still carry a lot of weight and even
your friends carry some weight yes
Facebook right so I tell them you know
going and investigate they do whatever
they want right if they're very diligent
if ur overly diligent they start looking
at your friends and evaluating your
friends I did not ask them what exactly
they did but it does carry some weight
if really good that the specific
criteria that I talked about earlier
then I think that the picture becomes
even more involved so again you know one
thing that stands out is all of these
actually come into play all the possible
factors I could imagine actually carry
some weights regarding job job hiring
through social networks including you
know the demographic traits of an
individual unsurprisingly maybe you know
your skills and intelligence is the
thing that counts most
but even that factor is roughly a
quarter of the total importance and then
people maybe overestimated the
importance of you know offensive content
or all those mistakes that were on the
job white survey but it you know it
still carries about 15% weight and then
your attitude
personality are actually quite important
in this method for hiring physical
appearance does carry some weight and
obviously your interest activities and
the demographic traits so you know all
all T's it's a very very big mix and you
know if you were asking people what they
thought would be important based on your
CV I would expect you know skills and
intelligence to carry a much much larger
proportion so this there is a difference
between this new world and the old world
so you know I want to make some some
conclusions and then leave some time for
questions so obviously you know social
network based screening covers lots and
lots of factors and more factors in
Seavey's so in fact I actually did the
same sort of experiment with Seavey's
rather than Facebook so people
volunteered their CDs and those TVs were
evaluated by other people and
unsurprisingly it's pretty hard to judge
someone's physical appearance from a CV
I mean you can try but most people don't
put pictures in their CVS they certainly
wouldn't provide a lot of information
about you know there are demographics
they wouldn't tell you their political
opinion in the CV so obviously all these
factors only carry far less weight there
is still a leakage at least of
personalities through CV so people do
try and figure out something about
cultural fit just based on CVS but again
to a far lower extent and through social
networks so you know this is certainly a
new form of hiring or a new data source
because it's it looks at other factors
this is certainly shallower than a face
to face job interview right if someone
forms an opinion about in a face-to-face
job interview you have the opportunity
of defending yourself or you know
actually conversing with the other side
profiles tend not to talk back so you
know this is a bit shallower but you
know it includes additional factors so
we you know it's clear why employers are
using this it's a very very cheap and
somewhat effective way of getting
additional information and additional
insights about people than they were
just using their CDs obviously you know
the this there are lots of legal and
ethical considerations that we can talk
about here you know
some people went when I talk about this
people ask me is this even legal well
you know legal depends on unaware so for
instance in a couple of states in the US
it's not allowed to ask someone to hand
over their password for a social network
for hiring so I'm but this this is the
level of restriction here sure you're
allowed to use social networks but you
can't ask candidates to hand over
passwords and even that is only in a few
states in the US in other states it's
completely allowed sure you know if you
ask users to do this that this is likely
to be perceived as invading the users
privacy but it doesn't mean you will
never get these passwords or you will
never get this information
absolutely you know LinkedIn is is
fairly easy it's it's Fairplay people
put their LinkedIn profile out there you
know a lot for hiring possibilities so I
think that's more of a fair play but I'm
talking about the other networks there
are issues regarding protected groups so
you know you're not allowed to
discriminate against people for going to
a a alcoholic anonymous in the in the US
so you're not even allowed to ask the
question do you go to a a and then not
hire a person you could be sued but
obviously this this kind of information
might float about if you know there's a
picture of you drinking something people
might assume things there is also the
issue of you know does the internet
forget so you know whatever you put out
there online even when you're very very
young that's saying you know 18 years
old stay there for a long period of time
or maybe even forever depending on who
you ask and and that means this can have
repercussions much much later in life so
obviously people would become more
sensitive to this because this carries a
lot of weight for a long period of time
I wanted to point out some very very
important limitations of this word so
first of all as you probably know this
this is an observational study rather
than a controlled experiment right it's
not that I took people's Facebook pages
and change just one thing and then that
other people evaluate that like one
possibility just to take the same
profile change the profile picture from
a very attractive person to a less
attractive person and then see what
happens to all the scores that's not
what I did this is an observational
study and that comes
was a lot of limitations you know it's
hard to infer causal relations from this
sort of study and another important
consideration is you know I've used
Amazon's Mechanical Turk so these people
and Amazon's Mechanical Turk are not Big
Shot hiring managers most of them are
not Big Shot hiring managers in firms or
you know HR professionals with lots of
years of experience and knowledge of the
law so you know this represents what
some people might do I did ask a lot of
asked all of them have you ever been
involved in hiring decisions as a hiring
manager twenty percent said yes but you
know in their internet any-any anyone
can say whatever they want and including
dogs could say there are previously
hiring managers so you know take that
with a grain of salt
that that's all I had to say thank you
we have time for questions yeah so I did
not provide an any information about
what the person was interviewed for and
actually I got very few but you know out
of the 500 so radar is three rows back
look buddy you better tell me what
you're hiring this person for because
otherwise it's gonna be hard for me to
evaluate but only three of them did
dress just did the task and then and
they said you know if you're going to
ask whether this guy is good for laying
bricks that literally that's what they
said if you want to hire this guy to lay
bricks maybe not the best choice but
other than that I I don't see why you
should care about their physical
appearance or you know physical skills
three out of five hundred yeah is there
or I think or last time I checked
allowed in the other state it means it's
great territory right it's oh I suspect
I don't know the straight answer is I
don't know
I suspect that you
is harsher on this sort of stuff I would
hope but I don't know that's a very good
question so they you know given this
information there now lots of strategies
for handling this you know some some
people might say okay fine I'll just
erase my Facebook account I don't need
that anymore I don't want that with all
these implications but you know what
does that mean what signal does it send
to the employer this guy doesn't have a
Facebook account I mean is this guy even
alive in this century so you know I
don't know I think this is this deserves
an experiment on its own I don't have a
good answer for you there are lots of
other things that you can do right you
can be very cautious about what you post
online but that makes the platform a bit
less enjoyable for you
you can deliberately put content out
there for people to see like you know a
lot of people do in LinkedIn it's not so
clear so I didn't talk about that
strategy from HR professional yes that
that's another option or they can ask
some of your friends you know they're
looking for these mutual connections so
they can ask about you there are a lots
of way of circumventing the these
privacy settings I know which ones are
more used in practice and which run are
more successful
I suspect the job by serving would have
something to say about this need to
check most teens I don't know but that's
certainly yesterday so first of all
teens under certain agents shouldn't
have facebook profiles but we know some
of them some of them might might have
yes you know you can have two Facebook
profiles one for work and one for play
again you know they might still find the
other one not not so clear yes actually
if you throw them higher yeah are you
desperate right it's like asking are you
desperate for this job I have to say you
know it's done that all employers or
even most of the employers that look at
social or profiles ask for passwords I
think this is this is still a friend I
hope this is still a fringe phenomena
again you know I'm not sure how many of
them even bother to look beyond just
LinkedIn but it seems like from the job
white survey quite a few of them at
least mentioned looking in facebook and
again you know Facebook if you don't use
privacy settings they see a lot if
everybody uses privacy settings then
they have no business looking in
Facebook because there's nothing in
there anyway
but they bother to look in Facebook so
that means there is some information
there in most of the cases tell us
anything about the correlation between
the appearance that have had a vice
versus actual performance because
there's some privacy people say we
shouldn't worry about some useful and
discretion when you read to you they
caught on to Facebook because the
rational company would just ignore that
because you know we're all high
performers in this room and I bet high
proportion of us did something you
regret when you're 18 and yes I mean I
had no information about the the people
volunteering the profiles I did not ask
them you know are you well appreciated
in your jobs if you do anything crazy
when you were young I don't have that
information I think you can certainly if
least rely on self reports to try and
get that information so you can say you
know in your opinion are you successful
in your career or you can ask them how
much money they earn that that you know
trying getting all kinds of proxies but
not not in this study know it's maybe
one final question
so I am trying to be good HR I then I
should just perform I perceived
importance on the very disagree so I
think it should be a dominant one but
you know I think cultural fit does come
into play right I think personality and
now that you'd still are still important
for example I I think you shouldn't
place lots of points on physical
appearance and they didn't place a lot
of points on physical appearance for
example for most jobs not modeling for
example well it's not to say that none
of the other factors are important
actually I think you know honestly maybe
some demographic factors are important
for some very very specific jobs but you
know first of all these are not the kind
of questions you would ask you know face
to face job interview unless you really
want to scare a candidates off but you
can certainly get that information from
social networks so there I think it's an
interesting interplay but yeah a good
choice would be ninety points on
experience and intelligence and and that
sort of thing and few on on the rest so
let's thank your um again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>