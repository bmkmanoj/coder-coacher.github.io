<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Communication-Avoiding Algorithms and Fast Matrix Multiplication | Coder Coacher - Coaching Coders</title><meta content="Communication-Avoiding Algorithms and Fast Matrix Multiplication - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Communication-Avoiding Algorithms and Fast Matrix Multiplication</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nwdkeAxqZDc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone thanks for coming I'm modern
Maserati from the research and software
engineering group and it's my great
pleasure to invite grey here today grey
is at Truman fellow at the Sandia
National Labs and he's been interested
bullet in the theory and practice of
linear algebra algorithms
high-performance computing and he's
going to you know be talking about some
office work on you know how do we you
know do on fast matrix multiplication
how we can avoid communication it's an
interesting new direction for you know
optimizing very important algorithms his
work has he's received lot of awards for
his work and you know some of which are
I'm just going to look at them like he's
received a you know cm linear algebra
price and two conference best paper
award and his thesis actually received a
horrible mentioned at the ACM doctoral
dissertation award and he also received
the CV ramamurthy distinguished
researcher award at UC Berkeley so great
thank you very much thanks for the
introduction the invitation I'm happy to
be here so so yeah the title of my talk
communication avoiding algorithms and
fast matrix multiplication I'm going to
sort of address those two things in
order and i'll start with a little bit
of an overview of what i mean by
communication communication avoiding
talk about sort of the theory in
practice for for the state of the art in
linear algebra and then talk about how
straw sins matrix multiplication and
thinking about that from the perspective
of communication sort of led me into
studying fast matrix multiply sort of
and trying to do better than strawson
both in theory and practice okay so i'll
start with with talking about
communication costs so the first idea is
that if we care about performance of our
code we we can't just think about how
many computational operations we're
doing we got to worry about
communication so what I mean by
communication is on a sequential
computer it's moving data up and down
memory hierarchy
think about a dram in cash and on a
parallel computer I'm talking more about
messages so if you're on a distributed
memory architecture you're using
something like MPI you're you're passing
physical messages and and the point is
if you if you want to do analysis of
your algorithm to figure out how much
time it's going to take keeping up with
the computation isn't enough okay so
here are the pictures of the memory
models we need to start keeping up with
communication and and so they're these
these simple block models that are
simple enough that we can do sort of
high-level analysis of our algorithms
but they match the architecture as well
enough that they they tell us something
useful in practice too okay so to look
at our runtime model which which
includes both computation and
communication we keep up with
computation in the usual way flops or
arithmetic operations in general and
then for communication will keep track
of how many words decent so where it
might be an inter or a floating point
number or whatever the relevant unit is
for the algorithm so to get a high-level
sort of first order runtime picture we
can just add the two terms so how much
time do you how many computational
operations do you do times the time /
flop how many communication operations
you do times the time forward and in
when we when we get high performance
implementations we really want to
overlap these two things try to overlap
computation and communication but to
first order we're just going to add
these two terms and we care about
asymptotics we we haven't made any
mistakes the the big takeaway is that
the cost this this beta term is a lot
bigger than the gamma term so we're
getting into a regime where flops are
almost free and really what we should be
focusing on is just this second term
okay the the beta term being bigger or
beta being bigger than gamma is most
people sort of understand that in terms
of time I think that's pretty accepted
but it's also true in terms of energy so
let me just put some numbers to this so
the different the rates we can compare
so on the left that is the improvement
per year in the flop rate and more
recently this has come because of
multi-core not not Moore's law but
is in these next two columns are
measures of bandwidth both on a
sequential on a sequential machine with
vertical data movement and on the right
this is an on a distributed memory
machine with horizontal data movement so
everything is getting better over time
but the rate at which flops are getting
better is a lot faster than the rate at
which communication costs are sure you
say everything was intact sensitive
please so I've sort of left linked up
many of them are I just sort of to
simplify only worried about bandwidth
costs but yeah so to get a better
picture we keep up with latency as well
for for matrix multiplication that I'll
sort of focus on this talk it's usually
not an issue it's usually just bandwidth
and computation okay so these are the
improvements in time so if you if you
care only about time you need to worry
about communication but its energy costs
are becoming more and more important we
can look at how these things compare so
this is this is some data from sort of
the exascale thrust and so the blue
curve is how things are now in the red
curve is sort of extrapolated to
exascale but the big takeaway so this is
the the y-axis is a measure of energy
and these are different operations you
might do and and the big takeaway is
that these operations so this is a
double precision flop and these guys are
moving data on the chip and these guys
are moving data across the network and
there's orders of magnitudes this is on
log scale of the amount of energy you
use to move data off the chip so if you
want to reduce energy costs then do as
much as you can with the data you have
on the node okay so not just time but
also energy the cost of communication
are outweighing the cost of computation
okay so this is this is why we avoid it
and so so with this in mind you sort of
you know look at the computations you
care about and you say okay can we can
we do anything better this is our
bottleneck can we do anything better and
standard ways to say okay well how well
can we do and and how well we're doing
right now and and can we improve things
can we come up with better algorithms or
not and and so as I mentioned before so
we can't just worry about computational
complexity we've got to worry about the
communication complexity and sort of for
looking at at both better algorithms and
tighter lower bounds
a lot has been has been done for regular
computation so things like matrix
multiply which I'll talk a lot about
today but FFT sorting and even some
sparse matrix vector products stencil
operations there's a lot known there's a
lot of theory around how good we can do
and how good we're currently doing and
we get to regular computation things
that are more sort of data dependent it
gets harder but but there's there's
ongoing work on that okay all right so
this is the basic approach we want to
take with with our computations in the
high level analysis and so so now I want
to get into some of the linear algebra
algorithms so we'll start with with
matrix multiply because a lot has been
known for a while about about this okay
so assume we're doing the classical
algorithm so we're going to do order in
cube flops for in for dimension in
matrices and we're on this sequential
machine then in 1981 it was proved this
is this is the number of words that you
have to send to your matrix multiply so
again in n cubed is the number of flops
we're doing m is the size of this small
memory where we do the operations okay
so so that's the relationship between
computational operations flops and and
communication operations or moving words
and then later a little bit more
recently we have another lower bound for
the parallel case so it looks similar p
here's the number of processors you have
now in is the size of the local memory
but it's sort of like if that one
processor does in cube dover p flops
then the ratio of flops to two words
moved is the same as the square root of
the memory size okay so that's the
important ratio here so these are lower
bounds and in fact there are optimal
algorithms that retain them ok so for
classical matrix multiply at least in
theory the communication is sort of a
closed closed book so we have we have
tight lower bounds and algorithms okay
so the the result we had a couple years
ago that we were very excited about is
that this result is it's not just for
matrix multiply that this ratio holds so
if you're doing anything that I'll say
smells like three nested loops and and
and I'm thinking of you can write matrix
multiply with three nested loops of code
and you can do a lot of direct linear
algebra the same way so if it looks
enough like matrix multiply i'll say
very
concretely then then we get we get this
relationship so the number of words you
sinned is related to the number of flops
you do and the ratio depends on the size
of the memory is the square root of
memory size now it doesn't matter if
your matrices are denser sports it
matters how many flops you do and this
bound holds for both sequential and
parallel no no the operations that
you're doing the tendon you have some
properties like associative team I
mutated I mean are you relying on that
you're doing like you know
multiplication in addition it's
upsetting um no not really actually no
so this is not taking into account
dependencies you need or anything like
that I mean there's so there are limits
to how you can restructure the code for
example if you use straw sins ideas in
this breaks down but I mean you if I
mean some so everything we've looked at
was in your algebra you have this
associativity in community and so this
sort of says however you use those
properties and you can't beat this bound
so you know in whatever order you do the
dot product is free but this bound will
hold its number of flops though the
numerator you have to isn't there some
characterization of of the distinct
number of memory locations you're
touching there is because I can just
touch the same memory location over and
over again I can have as many flops as I
want so I don't quite understand how the
numerator is relating to know sure so
right so so if you yeah if you if you
redo the same flop over and over again
this this one well we have to assume
those things away
in previous slide matric my notification
use the three nested loops but you had
it and I'm trying to see what the lower
part of that side relates to theorem
because the number of process of pious
part right because it's just next
theorem is is per processor there exists
some processor and who has to direct I'm
sorry every processor has to however
many flops that processor does then the
number of words that it must read or
write from other processors is given by
this so if we look to help communicate
ok each bus as opposed to the the own
right so that's an important distinction
so so this is this is more related to
the runtime and so if we don't care so
much about the total communication done
by all processors we care about the max
the slowest processor so some processor
must do that must do this much and so
that processor means some processors
must do n cubed over P flops that's how
the argument goes that Mesa just the
memory of the local militia or one work
on one note oh yeah
I mean so I'm sorry I haven't been
specific enough in this theorem I mean
the important relationship is that you
have your flops are sort of embedded in
a three-dimensional object and the
arrays they're accessing our our
two-dimensional objects that's sort of
the the key here if you change that then
you get some different functions but all
of the new house were sort of has this
relationship where the the iteration
space lives in three dimensions and the
data that you're accessing lives in two
dimensions okay some other link
clustering algorithms like kings they
also have two bstress but i'm not sure
if so have you looked at jamies now I'm
not sure um this I mean so there's been
other folks at UC Berkeley who have
worked on where the number of loops
changes and and so there's there there
are really nice generalizations of this
so for example if your data lives in one
dimension and your iteration space in
two dimensions you get a different
answer and so they have very generalized
form of this theorem I'll just focus on
this controlling algebra but i'm not
sure they've looked at k means either
okay so in the context of linear algebra
this is like if you're doing direct
linear algebra you're not doing it ur
div linear algebra then this this
theorem will apply so so like the blahs
the basically new house or subroutines
these are ways of solving linear systems
for different matrices solving least
squares problems and eigenvalue
computations and in some of it if the
graph algorithms work enough like linear
algebra it applies to those too ok so so
the sort of the reason this is exciting
is because you know all these
computations you know they're in
libraries they're very important lots of
people use them and now we have this
lower bound and says this is the best
you can do so the natural question is
okay well you know are the standard
algorithms that everyone's using are
they reaching this lower bound and sure
yeah so that lives in if you do try
yourself with multiple right hand sides
that's a blast 3 operation yeah I mean
it's so this actually applies to trying
to solve with one right hand side but
you'll never attain it it's not a tight
bound the neck
but but for blast 3 then it's a tight
man ok so so we can start to compare and
see you know when its height and when
it's not and so i'll just give you a
table of some of the optimal algorithms
and just by sort of looking at the dates
and the references you can see that some
of these algorithms are quite old so we
just look at them do the analysis and
say ok it's reaching the lower bound
this is the best we can do in theory you
know we need to go tune these algorithms
now but some of them the references that
have more recent years they've been
recent improvements so we looked at the
standard algorithm one that exists in
the library that everyone uses and say
ok it's not optimal can we improve it in
some cases it was straightforward some
cases it was quite complicated but we
can have this nice we can look at these
important problems and see is there a
gap can we close the gap so this is this
this is the story for sequential
machines and we have sort of a similar a
little bit more complicated story for
that for a pub the parallel case so just
approve of you know Obama is it hinge at
a better algorithm Oh making each of
these improvements to do was it like a
template that you applied to come up
with a specific file or my dad check or
is it complete so no in this case the
lower bounds move didn't help very much
so in in particular the lower bound
ignores dependencies so in matrix
multiply your very few dependencies you
can do the the scalar multiple any order
you want and for this stuff the lots of
dependencies and so I mean lot I mean
some of these the the numerical
properties even change so you got to
make sure that the changes you're doing
are going to create stability issues so
for example for where Lu decomposition
so the standard way is doing partial
pivoting and and with partial pivoting
it doesn't seem likely that we can reach
the lower bound in terms of latency in
fact so this this paper here from 2011
they generated a new technique for
pivoting so it's different than partial
pivoting it's it's as stable and in a
certain sense but it's different so the
numerical properties change so the you
know it's it's much more sort of
particular to the different operations
the
giving them a better algorithm I think
maybe so I'll get to a straw soms matrix
multiply soon I think they're the lower
bound helps a little bit more okay so
let me so let me give you just a couple
examples so that was this this slider
sort of all theory of getting a recent
lower bound in theory sort of ignoring
all constants and so just just to give
you a few examples I want to convince
you that this does translate to better
implementations in practice so so just
to flash some speed ups for different
computations so things like if you're
solving the symmetric eigenproblem for
band matrices the dependencies are
really hairy and messy but if you're
careful about the communication costs
you can get get good speed ups computing
QR decomposition isn't maybe been the
most useful new algorithm because it
comes up in so many contexts and so
we've gotten speed up some different
different architectures there so it's
not just theory this stuff does
translate into into practically good
implementations okay so so from there I
want to sort of transition to talking
about straw sins matrix multiply in
particular and sort of how that leads
into and some of my more recent work
okay so so here's here's straw sins
algorithm in case you don't have it
memorized so the the clever idea is that
if you're doing two by two matrix
multiply you need only seven scale and
multiplies and not eight okay so you
take these combinations of elements in
the left input matrix and combinations
of elements on the right and you do the
seven multiplies and you take these
weird combinations and you get out your
output elements ok so just we can write
the classical algorithm in a very
similar way and here you see this the
eight multiplies a lot simpler
expressions here so eight multiplies in
four editions seven multiplies and turns
out to be 18 additions here okay so so
this is what you do if you have to
multiply 2 by 2 matrices and why is this
nice we can use the idea recursively
okay so instead of 2 by 2 matrices we
have in by in we just split our matrices
into quadrants and then run this
algorithm and call straÃe and
recursively okay so how many flops do we
do we write down the recurrence base
case let's say we go all
one we get a number of flops which is
not in cube butt into the log base 2 of
7 okay so now we're at 2.8 14 are ex- oh
this is a big improvement is improvement
in the exponent that's why it's an
exciting idea okay but I said that we
can't just do you know computational
analysis to figure out this is a good
algorithm we got to look at the
communication cause so what does the
communication cost if you if you run
this algorithm on a sequential machine
so i'll just change the recurrence down
here this line stay the same we change
the base case this is when things fit in
the cache and now we get a different
expression down here for and this is the
word count on a sequential machine so
I've written it in sort of an
unsympathetic scom placated but there's
a reason for that so I want to compare
it to the classical algorithm okay so
here sure sure can you explain the
Philosopher's crash this line yeah so
this is the same actually as the flopper
guns so here's the flop group we're
going to do seven subproblems and half
the size and all these guys are matrix
addition subtraction so that's order N
squared flops and we go to words it's
actually the same thing because when we
add two matrices we've got to just
stream through them and so this is
ordering squared words as well and today
we are assuming that least there's no
sharing between the seven some
congregations um it doesn't mean that
mean it doesn't matter if this might
change the constant there it's in this
seven is the number of multipliers we do
and so that mean that's important but
yeah I mean for the additions some cut
some constantly maybe they're sharing
maybe there's not oh okay but but
changing the base case and now if we
don't go all the way down to one we go
to when n is smaller than square root M
that means our problem fits in cache
then we just have to read and write the
problem okay so we get this weird
expression and so let me compare it to
the classical algorithm okay so this
slide will get a little complicated so
let me explain how the table works so
this column is all classical and this
column is all Strawson the first row is
on a sequential machine and this the
next two rows will correspond to the
parallel
everything all the units here are words
so in parallel I'm talking about the max
words overall processor and in
sequential it's just words moving a site
in and out of cash I'll use asymptotic
notation here so so big o is the upper
bound that means if this is true if we
have a bigger that means an algorithm
exists the lower bound is the big Omega
that means we have a lower bound and
then I'll write theta if we have
matching upper and lower bounds okay and
then in matrix dimension M is the size
of the fast memory of the local memory
depending on sequential parallel and
then peace the prosperous ok so the
analysis we have in the previous slide
gave us an algorithm so we got a big o
with this expression and I've written in
this way so we can compare to the
classical algorithm so you see that log
base 2 of 7 replaces 3 over there so
this guy is smaller than that guy ok so
so we saw that in the computational cost
we reduce the competition in the
exponent this is a big improvement we've
also reduced the exponent in the
communication costs ok so that's that
was the first exciting thing I mean
usually you have to do a trade off of
computation communication and here it so
it's a win-win so so that got us excited
but I mean this is just one way to write
down straw sins algorithm can we can we
do anything to reduce the communication
of straw sins algorithm and and so after
a lot of work and a lower bound that
took on a very different form at least
the proof did we were able to prove that
this big o is actually theta so we've
got we proved a lower bound and say that
this algorithm is tight so writing it
down in the usual recursive way that's
the best thing I do ok so that was nice
so now we have no gap and the other
thing is nice is the lower bound didn't
care if you were on a sequential machine
or a parallel machine so we got a very
similar bound in the parallel case
basically you divide that guy by P ok so
we've got a lower bound and then let's
compare it to the classical so here we
have matching lower and upper bound and
again the differences in the exponent ok
so a similar store for comparing
classical and straw sins and sequential
and in parallel ok so we had this lower
bound but we weren't sure if an upper
bound exists so we we looked in
literature because people had tried to
paralyze frosting before and turns out
they they weren't optimal so they had
some ideas for paralyzing it
and they weren't reaching this lower
bound so we said okay well let's see if
we can come up with a better algorithm
and we could and it was a lot of work
but I'll just signify by changing the
Omega to a theta and we have a tight
down there okay so so that was great we
we got the we match the lower bound we
had an optimal algorithm but there was
actually an extra term that showed up in
our analysis and so it's kind of hard to
compare these two terms this guy can
actually dominate in some cases so we
looked at this term we said okay so now
maybe our upper bound isn't always tight
let's compare it to the classical case
and it turns out the classical case has
a similar term that shows up and there's
a lower bound and upper bound for this
guy so okay this this term is necessary
in the classical case it's probably
necessary in the strawson case so we
work to get a lower bound and we got it
and so we have a fit okay picking belly
is that for small measures thank you um
I'm not sure if I have good intuition
for why happening so this term only
matters when you scale out to lots and
lots of processors and then you're never
really constrained by memory so if you
have a fixed problem sighs and you scale
out number processors you sort of get
more memory as you scale out those
processors and so at some point your
memory is never an issue and and so you
think about taking so if you simplify
this and ends up in the denominator and
take your memory 2 to infinity and then
this guy just dies away so if you had
infinite memory this is the only thing
because the only thing that balance your
communication um yeah I'm not sure yeah
that gives good intuition but but you
know we saw it we saw the term first as
an upper bound worked hard to get the
lower bound to make it tank okay so then
this was we felt good about this this is
you know matching upper and lower bounds
for both classical and stressed about
them ok so again just to show you this
is not only theory here's some some
performance results when we implement
this and put it on a big machine so here
is a strong scaling plot so we're
increasing processors along x axis and
this is a measure of performance on the
y-axis and it's it's effective gigaflops
so we're comparing this sort of assuming
that everyone does order in cube flops
which is which is not fair but you can
think of this as
first I'm so this the speed up will make
sense ok so this black curve is
basically the library coach the
classified algorithm I told you there
was an optimal algorithm the classical
case if you implement that you get the
green line these blue lines were the old
ways that we found in literature for
paralyzing strawson didn't meet the
communication lower bound and then the
red curve is our algorithm so this is
its outperforming and it's like a you
know almost a factor of two at the best
the best spots and and speeding this
this line signifies machine peak for any
classical algorithms so if communication
we're free or for a classical algorithm
it still can't beat that dotted line
there okay this is a really big problem
94,000 by 94,000 matrices and most of
the time except that the very writing in
the plot is computation so the big win
the reason the red curve beats the green
curve here is more computation and
communication but we we have a win in
both areas and so we wanted to test sort
of when things are communication bound
how do things compare so we looked at a
smaller problem so this is less than
5000 x 5000 and we scale it out to a
bunch of processors and now i flipped it
so now this is execution time so now
down is good and note it's a it's a log
scale okay so here's the library code
and basically when the curve starts
going up that means you start to use
more time with more processors so you're
not scaling at all you're not even close
to strong feeling you're not scaling and
if you use the optimal classical
algorithm you can scale farther but
strawson lets you scale even farther and
get better performance so here i mean
the the local computation is very little
this is a communication bound problem
and and we see the win here because of
this nice property of straw sins
algorithm of requiring less
communication okay so so that's just to
say you know not just theory this is
this is also a good idea in practice
okay so let's let's go back to the
theory slide so so this that you know
the story is almost done for for straw
sins algorithm I mean at least in theory
we have everything tight implementation
and it can be much improved but the
exciting thing is our our lower bound
also our ideas for the algorithm are in
particular strawsome so if you take
basically any fast matrix multiply
algorithm
and and say the exponent you get where
this fast algorithm is omega naught then
the communication costs just change log
base 2 of 7 Omega naught so if you get a
better algorithm in terms of flops then
Strawson you get a better algorithm in
terms of communication as well and we
have a way to parallelize it and every
and we have tight lower and upper bounds
so this is quite exciting to us because
people have beaten strawson before and
so let's just plug in their algorithm
and see the speed ups so so people have
worked on this since Strawson came up
with this idea in 69 and again when I
say fast algorithms I mean Omega naught
is smaller than 3 and we hope smaller
than 2.8 one which is straw sins
exponent and there's lots of
improvements over sure awesome so this
is a plot these are years so from
nineteen sixty-nine when Strawson came
up with his idea to the present of how
the the bound on the exponent of matrix
multiply has improved so here's Ross in
1969 there was a little improvement here
and then a big jump and these are sort
of I'm just sort of cherry picked some
good references there a bunch of dots in
between and now the world record is a
2.37 three yes not special cases now yes
and so there's some caveats which aisle
which I'll get to okay just right across
and go lots of people be stressed and
even strawson be strong yeah the
seamstress thanks Rob oh yeah yeah I
mean yeah and so I sort of picked these
names mean so there was a his idea this
is there a beanie this this was one of
the reasons I mean so the you know these
guys built on him strawsome built on
sean has idea and covers with me
winograd so it'll put those things
together and this is Virginia Williams
just you know a year or two ago she came
up with the improvement of a copy from
Luna grad and she used all the same
ideas just sort of took their ideas a
little further this looks pretty flat
because it is pretty flat this is 373
here this is 376 so a tiny proved but
people are very excited about it excited
very excited about it I mean yeah lots
lots of people have
worked hard on this problem okay so why
don't we just plug in this algorithm and
see how we do on supercomputer okay we
can't do that for a number of reasons so
pretty much all of this is just theory
and a couple big reasons are one is that
an important idea the one that they're
being came up with is that you don't
have to compute matrix multiply exactly
you can use an idea of an approximation
so you compute a times B but your output
is the right answer c plus a little air
and as and your algorithm has this
parameter lambda in it that as you send
it to zero then you get an exact
algorithm okay so it turns out that if
you have a scheme for doing this then
you can get an exact algorithm out with
only a log factor overhead and two
theoreticians that's fine you just make
it a tilde 0 instead of a know when
you're done the other problem is that a
lot of these algorithms aren't explicit
algorithms no one's written them down
because they don't they don't care too
but they prove that that they're you
know there exists an algorithm that
yields this bound on the exponent okay
so particular I mean so Virginia
Williams sort of thinks that her her
approach is constructive and it might be
but I think it'd be a lot of work and
it'd be a big algorithm to write down
and it's sure hello BTN below they use
this yes but I mean has left out there
are some points sort of in this area
that don't use that and I should say
that beanies algorithm is really simple
to write down so he this is an explicit
algorithm these guys are not explicit ok
the other thing that most people think
is is the main problem and it is a
problem is that no one cares about
Constance here and and no one's figured
out what the constant is but it for at
least down here it's got to be large
okay so that's going to get in the way
all right so so we can't just put in
these these other algorithms okay so I
don't understand this you're saying that
people care about 3,000 the difference
of 3000 s in this exponent but there's
no algorithm that actually gives you
that 3000 the hoop so why didn't take
care is
just this sort of like the pureness of
math that we can make this I think so
okay that's amazing and I think more
importantly people have not been able to
come up with it no more power press or
Santino so every every time you make it
official people I think that though the
best no but we have is quadratic
understand we know that we need a
gigantic bread as well but so in the
sense like that's still a big gap
between two at 2.37 three so they're
very important to close the gap okay and
it mean it's such a fundamental problem
and easy to stay it's very attractive to
work on I mean at you know I think this
is the result yes yes okay so so so the
goal is now become you know how do we
how do we implement how do we find
something that it's you know on this
chart but but is practical okay so you
know the techniques that these folks
used I've been really theoretical but
can we can we use some practical
techniques to find algorithms they may
be maybe some of these folks have
overlooked because and they didn't give
a great exponent sure I mean you show
that you show the path to the lowest
exponent and you know theoretical so is
there isn't there an equivalent one of
the implementable algorithms you think
that people would be pushing down on
that as well right well that goes
physical in here really this stops so no
one's been able to do better is that I
mean even trying cuz you'd assume that
there'd be a practical you know
motivation to do it right absolutely so
so i'll show you that we've done a
little better well congratulations I
think you're being modest your rates and
the same nobody even came down to
Strassen before you resign so people
have implemented strawson before I
assume they're attractive yeah you know
I'm arguing that we got better
performance for straÃe and distributed
memory and so we did better than people
had done before but people had the idea
that straÃe someone was practical a
while ago and they worked with it and
it's just I mean so things got
complicated here sort of the theory to
make these jumps I think it's hard to
understand for people who are interested
in coding these things it's just a big
divided ok all right so so so here's
here's the the effort to bridge the
divide a little bit ok against Ross ins
Algrim is practical most people think so
I mean it's it's not used enough in
practice for various reasons but you
know you can get good performance with
it so we saw that many are better in
theory or any better in practicing can
we find any ones that maybe people have
overlooked so maybe the exponents not
that much better but it could be
practically better and then you know if
we have an idea that the practicum we
implement and benchmark and show that we
get a speed up ok so that's what we'll
we'll go for now so so all these fast
algorithms are based on on recursion so
Strawson is is is based on solving the
base case of two by two and we use
recursion breaking the matrices into
quadrants so why don't we just solve
this two-by-two problem and instead of
doing seven multiplies do six multiplies
so if we could solve this tiny problem
with six scalar multiplies when we get
an exponent this 2.58 and if we could do
that it would be practical the problem
is so we can't do that right so so so
people have proof theoretically that you
cannot do 66 multiplies even if you sort
of relax yourself to approximate
algorithms ok so but we're not stuck
with this breaking things into quadrants
we can use different base cases so we
could break things in by factors of 3
and so now we've got a problem where
there's a little more open ground so if
you do the classical algorithm you do 27
multiplies for this one we know how to
do using 23 multiplies and we get a
better exponent it's not quite as good
as straw sins we don't know if there
exists a 21 which would give you a
better exponent straw sins we actually
don't know if there exists a rank 19 and
algorithm here so that's open so this is
kind of what attracted me it feels like
this is a practical algorithm we could
get you know any of these they've made
me better than straw sins so I'll tell
you now that I haven't found I don't
know I only know of this one but we
don't have to have to only look at three
by three by three base cases and we can
look more general okay so this problem
boils down you can think about in many
ways the way I've been thinking about as
a
is as a tensor decomposition problem
okay so we've got this tensor which is
which has 3 modes of three dimensions
and we want a low rank decomposition so
these guys are like outer products of
vectors and now we've got three outer
products so this little chicken foot
thing is a rank 1 tensor and if we sum
up these Rank 1 tensors to get that we
have a a rank or decomposition of that
tensor okay so this is basically the
problem we want to solve there's
different ways to go about it let me
just note that the size of this tensor
and say the rank that we're looking for
so that grows as the base case we're
looking for gross all right so let me
tell you a little bit more about where
these sort of structures come from so
this tensor is is the matrix multiplied
tensor so let's think about matrix
multiply as a tensor operation so I've
got my input matrices a and B and let me
just stack them as vectors like this and
I want a tensor I'm using some sort of
fun eat instrumentation notation here so
might insert e this is my matrix
multiply tensor I want such that this
operation gives me exactly the output I
want for it really any given input okay
so so thinking about this sort of
geometrically if i take my a matrix and
i do dot products with every stand up
pencil in this tensor that will give me
a matrix out and then i do dot products
in this direction with every row in that
matrix i'll get something that has still
one dimension left so if i plug in a and
B here this tensor I want and the tensor
operation to put out see here okay so so
basically the tensor is defined by the
base case and it turns out to be this
really sparse tensor so there's four
slices if for two by two measurement
there's four slices to this tensor and
this is what these four slices look like
okay so this is we can express any
bilinear operation this way as this
tensor operation and and so i'll just
point out that the the entries in tinton
so first of all tintri is the tensor is
sparse and second all it's nonzero
entries or ones so it's kind of a simple
guy okay so that's the tensor and that
and so what is the the sort of low rank
representation of the scent of this
tensor mean so the left hand side I've
sort of giving you an idea where it
comes from so what is the right hand
side so that's more funny notation but
for every or that's like one of the
chicken feet guys and one the rank 1
tensor so straÃe sins algorithm is
encoded by these three matrices so if i
take for a given RSA ours three that
corresponds to three vectors that I'll
take this outer product of so that you 3
i'm just going to put in the matrix here
you for i'm going to put and just stack
them all together so i'll use these
three matrices to represent the right
hand side of the equation okay so
strawsome is algorithm has pluses and
minuses in it and and you sort of see
things pop up there that's kind of where
the right-hand side comes from so let me
give you a little bit more of the
connection from the UVW representation
the algorithm to the algorithm that sort
of makes sense to us so here the factor
matrices i'm going to write them in sort
of a table form and I'm going to label
the rows and columns okay so so here's
how it works so there's seven multiplies
and I have a column for every multiply
so look at say m4 over there which is a
22 times BTW 1 minus b 11 so in four in
this column of you which corresponds to
my a I've got a 1 and a 2 2 and that
means i just take a 2 2 l and then if i
look in the fourth column of V which
corresponds to be I get a minus 1 B 1 1
we see that shows up and then e 2 1
shows up okay so then if I keep looking
down the column info r is going to be
used to compute c11 and it's going to be
using compute C 2 1 that's it showing up
there and there ok so this is this is
how the the UVW in trees encode the
algan so if you have this representation
doesn't seem hard to write the code for
it ok great so so now let me just write
down the problem sort of in general
because it's not particular to two by
two by two case but we can do any any
dimensions so if we have n by P times a
P by n bass case then we get this
problem which i can write down in really
short form so I've got my tensor is
specified and I'm looking for UV and W
matrices and I just need to satisfy
these constraints it's just try the trip
a set of trial linear polynomials ok so
these constraints this is just another
way to write this equation ok so that's
easy enough to write down in fact if I
find a rank or solution to this problem
then my
mega not pops out with this nice formula
and I can compare it to see how good the
algorithm is okay so write it down
easily it's not easy to solve obviously
now the problems at the top again ok so
it's an MP complete which you might
expect for a general T and this is just
this is just 1 tensor it's kind of
simple so I shouldn't slow us down too
much there's many ways to formulate the
problem using combinatorial solvers and
a comet Oriole search tools but the the
most effective way that we found is is
to take a numerical approach so there's
lots of numerical ways of compute lower
ranked approximations we're looking for
a decomposition an exact decomposition
but we can get approximations pretty
efficiently and the typical approach is
called alternating least squares but
there are some pitfalls and so we're
definitely not guarantee success and
even if we have success it doesn't give
us an algorithm immediately the
important point is we need we want exact
algorithms we really want the entries to
be from a discrete set so like plus and
minus one would be really nice and we
would like our answers to be as far as
possible okay so let me just give you a
little more details and maybe I'll go
fast over this slide but but if you're
interested I'll talk to you more about
the details but this is how the most
effective approach to finding the
algorithms works alternating least
squares says you solve for one of the
factor matrices given the other two and
you're just alternative so there's sort
of three steps to the inner iteration
solve for you given W and V solve for V
given you and W and so on okay so
there's there's funny notation again but
what I'll point out is this is a linear
least squares problem all these guys are
matrices and this is just a
regularization term okay so this is my
regulation parameter if I covered up
utility this is like using ridge
regression basically so every inner
iteration is really easy to solve we
usually pack to solve it we can even use
a communication of 1q our algorithms we
want these are really small but it takes
many iterations to get anywhere and I
sort of mentioned the other pitfalls and
and important point is there's a lot of
art and picking lambda and picking these
kind of funny matrices that we put in
the regularization term so there's
there's some art there which I won't go
into
okay but we found a bunch of algorithms
okay so straw sins just how do you get
rid of that proxima shins and you hope
that though the solution that you can
merge has zero errors okay yes you hope
so it's it's it you know it's in this so
you get something maybe something it's
small and so you try to convince it to
go to zero and if it does then maybe it
convinces something else to go to zero
and then maybe you can get an integer
out or a nice number instead of a weird
floating point number it's a lot of work
you have to okay yeah okay so and so
this technique comes from a smirnoff
this is a recent very recent paper and
actually we were working on a very
similar thing when it came out with this
paper and he was doing a lot more
successfully than we were especially in
picking these these sort of special
matrices okay but we found a lot of
algorithms okay and these are you know
they could be practical they're a small
base case and and so so this notation
here so Strawson is solving a two by two
by two problem and here's the 3 x 3 x 3
problem i told you about but we don't
need to square subproblems we can cut
things rectangle early and so you can
pick any three numbers and that gives
you a base case and look for the best
solution you can find so you can look at
the rank sure I didn't catch sorry what
did you say ok so this notation over
here there are three parameters that
tell you the size of the base case so
first rawson he cut things down by two
by two matrix times a 2 by 2 matrix and
we also talked about 3 x 3 x 3 x 3 but
you could you could use recursion say
two by two by four so cut the last
dimension in into fourths and cut the
first two dimensions in half and then
you'll you'll get another problem and if
you use the class I maybe there's an
easier well if I just use it i use the
classical algorithm here and i do it
recursively then I ended up doing 16
multiplies but if I find a fast
algorithm that's of rank 14 then my
recursive algorithm will have 14 sub
calls
so maybe he's our exact these are these
are exact yeah so maybe let me skip I'll
come back to the previous slide so
here's an example with 424 so i take my
matrices no matter what their dimensions
are and i split according to those
numbers so i get a four by two these are
you know not quadrants now but this is
you know in / 4 by n / 2 if i start with
a square matrix for 4 x 2 x 2 x 4 yes
exactly in all cases yes ok nice ok so
so so back to this table so we can
compare the number of classical
multiplies for that particular base case
to the ones we found and sure so this
when you're searching for this minimum
rank you have two things you either you
don't know if your approximate solution
iteratively score the alternating
squares found the solution at that rank
or whether that rank just doesn't have
the solution right so you are you
getting more rank as well yes but I mean
we have some idea of what rank we hope
to get there's been a lot of time there
so for many of these they actually are
known lower bounds so I told you that
three by three there's a gap but for
most of these other ones at least like
here and above and not three by three
there's definitely a lower bound the
lower bounds are kind of spotty that so
some of them have to take a certain
shape you know so but they're definitely
open questions here but yeah so you can
spend a lot of time looking and you'll
never find it you don't know um yeah ok
so all right so so you can you can sort
of compute quickly what your possible
speed up could be if you implement this
and sort of additions are free and just
looking at multiplies you can compute
the exponent very quickly and so all
these are in the ballpark of straw sins
but I will point out that this this row
here is a new algorithm found by
Smirnoff with a technique I showed and
to the base case is 3 by 3 by 6 it's an
exact algorithm that gives you two point
seven seven which is an improvement and
this could be practical these last two
are shorter for comparison the stars
mean that their approximate algorithms
so this first
b is beanie he was the one that came up
with that nice idea very small base case
this is sean haga who was also on the
plot but this is a different paper where
he came up with an approximate algorithm
for 333 so these guys have better
exponent in strawsome but again there
are approximate algorithms but we can
implement them and so we did and to look
at you know how fast they are even if
they're not that useful okay so so so
here's the example of four to four and
the main idea here is that this is the
structure of all these algorithms you do
some linear combinations according to
your u matrix so I'm according to your V
matrix you make recursive calls and then
you do some Leah Kamen and that's it so
and we should be able to implement all
these algorithms in the same way okay so
so let me get to the practical
performance okay so they have the same
structure which I just showed you so we
can do it we can write a code generator
to take in given a UVW matrix translated
into code that we can run okay so i had
a student working with me this summer
and he got that part done you can write
correct code pretty quickly but to get
good performance you know it takes the
rest of the summer so what do we have to
worry about for performance so so
basically we're just doing multiplies
and adds I mean these albums just
generate some sub problems which are
which our multiplies will use the
library implantation for that and to get
to those subproblems we need to do some
ads so it really matters the vendor
library performance for the classical
algorithm that's our base case that will
use it will matter the efficiency of our
addition so if we do that poorly then we
can just lose everything and then we
need to figure out when to switch from
our fast to the classical algorithm and
then maybe I won't get to the
parallelization that we use so that
brings in a new level of complexity sure
there any difference in numerical
accuracy of these algorithms yes yes so
I'm not going to talk about it but yes
definitely so and actually there's
theory so given a UVW matrix you can
actually write down some theoretical
bounds on in America ocular accuracy and
so and and and the numerical stability
you get is not as good as the classical
algorithm
but it's it's not that bad i mean these
things aren't totally unstable just that
stability guarantees aren't aren't as
tight as the class gago but we didn't
look at sort of empirically what things
look like yet and we want to okay so
they're so so i'm going to go into some
some few details about these how to get
good performance here so the first is
what's the classical library performance
what does it look like so here's matric
this is square matrix multiply to
mention here and performance of there
and this curve is very familiar for
people who've dealt with D Jim this is
we're using Intel's in kale and
basically there's a quick ramp up and
then you have basically flat performance
which is something like 90 or 95 percent
of peak for classical multiply multiply
so this is important the shape of the
curve is important because what we're
doing is we're going to sort of trade
we're going to say say we're doing
strawsome we're going to do seven
multiplies but it's half the size so if
we're sitting at N equals three thousand
then and we want to use a step of
Strawson then we're going to have seven
sub problems of size fifteen hundred and
if this performance is as good as this
performance then we should get the speed
up that we expect as long as we've done
the additions correctly and you know for
big matrices they shouldn't hurt too
much but if we want it you know and go
1,500 to 750 well then it's not clear if
we can maintain n phones okay so that
gives you a little bit of a rule of
thumb of how many steps you can take
before before you have to switch to
classical and you know that if someone
were to write better classical code and
and get a curve that sort of stays level
longer and we could run more steps of a
fast algorithm so if they can improve
classical and it actually helped me fast
as well but they worked very hard to get
this curve way it is so in this quick
case it's very good actually in a
parallel case it's it's much slower to
get up there machine we're using at
twenty four cores and it's hard to sort
of paralyzed small matrix multiplied
over twenty four cores but but I guess
the takeaway is that the shape of the
performance curve matters a lot for our
performance okay so what about additions
so maybe I won't spend too much time
with this but we basically looked at
three different ways to do the additions
I'll tell you about the right ones which
we saw was the had the best performance
you can do things like common
sub-expression elimination so the UVW
matrices come back at you and they might
have you do some additions multiple
times and so you can sort of account for
that create a temporary to do him only
once that's if you've heard of winter
grads variant of straw sins algun that's
basically what he was doing goodbye
dirichlet a minute things we use not me
we did a simple automatic thing um which
was just ask lydia transform library
daddy don't let him bite of that okay so
I mean we thought about doing someone
smarter it's just that it didn't help so
we got best performances right once and
it can actually hurt the right ones
performance it could help one of these
other two so it might be something to
pursue but we sort of started down that
path and then it wasn't helping so we
stopped but yeah we did a simple
automatic thing that wasn't as
sophisticated okay so this this the
right once is basically so s s are all
the multiple cans that come from a and
for each one you make sure that you only
write those ones so for every entry of
s1 if it depends on a 1-1 a 12 and a 22
then you sort of read in three streams
and write out one okay so maybe I won't
go into any more details there but this
is the approach that we found gave the
best performance so here's the
performance curve for a bunch of fast
algorithms and so I you know we could
implement a lot of them and the ones I
showed you you can do rotations and so
we got a bunch of algorithm so this is
this is obviously a bit a bit of a mess
so let me clear out most of them so we
can look at only a few so importance are
this black one is the classical that's
mko by itself the black triangle one is
strawsome so we can compare against it
and then the red one is getting the best
performance as beanies algorithm which
is approximate so it's it's not quite
practical but it's something that we can
look at the performance up and you see
some of these others of the different
shapes are kind of competitive with
strawsome okay so so nothing is really
being Strawson so this one actually
gives a better exponent it starts to be
kind of competitive here but it has a
slow ramp-up so nothing nothing
too exciting strawson seems hard to beat
in this case this is square matrix 2
square matrices in the sequential case
okay so where we found something a
little more interesting is if we have
rectangular matrices so suppose your
matrix multiply looks more like this
this kind of looks like an outer product
shape then it turns out strawson isn't
the best thing to do so here's an
experiment where we take the long
dimension and outer dimension and make
it big and keep this inner dimension
fixed and and so that's the x axis here
and then performance the black triangles
is against Ross and then we see that
we're being straÃe now the red curve is
beating it that's approximate but the
light blue and the green are also
getting strawsome and if you look at
those four to four and three two three
they kind of fit the shape of this outer
product so we're going to divide the big
dimension by four in a small dimension
by 2 and the big emission my four so
they're matching the shape and so a
similar story for this other kind of
shape where you have two small
dimensions in one long dimension and
there the dark blue curve 433 sort of
matches the shape so there we can start
to beat strawsome okay and it's not
there aren't huge wins so strawson is
beating classical by ten to fifteen
percent and we get maybe another ten
percent on top of that okay but but this
is this is practically beating strawson
and you know these the exponents of
square matrix multiply aren't as good as
strawsome but because they match the
shape better we get better performance
okay so let me maybe skip over the the
parallelization techniques basically we
had three ways to paralyze the algorithm
it's all about traversing the recursion
tree in parallel okay so so if you're
you know if you've written parallel code
for recursive algorithms before you know
there's kind of a BFS breadth-first
search approach and a DFS approach and
so we we did both those and we used a
hybrid approach which and we assumed
would-- would do better and sort of over
over a greater ranges and let me just
show some sort of performance here okay
so this is still a square matrix
multiply we're running on a machine that
has 24 cores and in two different new
merchants and there's two groups of
curve
so these bottom curves are with 24 cores
the performances per core and the top
curves are over six threads and so a
couple take away so so one is that the
six core performance so the the black is
classical and the others of the
different parallelization texting
techniques we had the performance looks
pretty much like the sequential case so
if you just take the best of the color
compared to the black then you know
we're getting something like twenty
percent win and like we saw in this
country case but for the 24 course we
don't see that and and there's a reason
for that so we're doing multiplies and
we're doing additions and with the
multiplies we sort of need the
multiplications to scale with a number
of threads and we kind of get that if we
paralyze correctly but with the
additions where it's all it's all
bandwidth bound memory bandwidth bound
and and when you run with one thread on
your node or run with 24 threads this
the memory bandwidth difference is only
like a factor of five or six so your
memory bandwidth doesn't scale with a
number of threads and so that means your
additions are going to be bound by a
scaling factor of less than six whereas
your computation your multiply scaling
happens that may be closer to 24 okay so
additions just become more of a
bottleneck in that case and there's not
much we can do to get around it and
share memory so with with six cores the
multiplies and adds both scale about the
same so we see the same sort of relative
performances in sequential case okay so
i won't go into the rectangular cases
sort of a similar story six threads was
like sequential 24 threads we never
really see a speed-up over classical
okay so let me let me wrap up here okay
so so in theory we sort of saw that
asymptotically fast algorithms reduce
both computation and communication so
that's kind of exciting and in in
practice strassen algorithm does well it
can beat the the classical algorithm the
d gem but if we look at these other fast
algunas that we can find with the small
base cases straw sins algorithm is hard
to beat for square matrices so we never
really saw anything and do better do
better consistently but for rectangular
matrices Strawson isn't the best it's
it's sort of tailored for square and if
you have algorithms that match the shape
then you can get
performance then we saw the shared
memory parallel ization has this
bandwidth bottleneck but the additions
start to cost too much but we don't
expect to see this in the distributed
memory and we haven't implemented all
these algorithms there and so there's
there's more hope for these these fast
algorithms with big base cases okay so
so suddenly open questions one is the
question about numerical stability is
still open so theoretically there are
bounds but empirically we we haven't
looked yet looking at distributed memory
is next on the list and then I mean is
it worth looking for bigger base cases
I'm not sure so we got we we implemented
this algorithm that had the better
exponent in Strawson and never really
outperformed it and and so you have this
overhead as the base case grows the
number of recursive steps you can take
is limited and number of additions seems
to be higher relative cost so I'm not
sure if it's if it's worth looking for a
lot of bigger base cases or not I've
gone back and forth of my pessimism
optimism but the last one is can we take
this and put it into other application
so other linear algebra algorithms can
use matrix multiply are there some
applications that could really benefit
from this and can we actually put it in
and see speedups there okay it's all in
there thank you very much for for coming
but the human context to extending to
other algorithms a lot of graph problems
fit into this framework but they they
have this additional constraint of
sparsity or or some sort of structure on
the graph which induces the structure in
the matrix have you thought about
extending these ideas yes yeah so I'm
sort of pessimistic about dealing with
sparse matrices I mean so so there's
been some theoretical work sort of using
some of the fast algo especial
especially the rectangular shaped I
don't know if they'll be practical
though and you know there's lots of
papers that sort of reduce their problem
2 matrix multiply and inherit the
exponent but I'm not sure how practical
they are either but yeah it would be
nice this if if some of the graph
problems with where things get dense
enough that this could help maybe
you could help out
cool so it's time to study</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>