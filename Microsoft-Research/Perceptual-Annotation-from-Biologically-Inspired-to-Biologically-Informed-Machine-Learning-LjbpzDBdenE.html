<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Perceptual Annotation: from Biologically Inspired, to Biologically Informed Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Perceptual Annotation: from Biologically Inspired, to Biologically Informed Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Perceptual Annotation: from Biologically Inspired, to Biologically Informed Machine Learning</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LjbpzDBdenE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
and just does sort of excuse myself a
little bit I'm a card-carrying
neuroscientist first and foremost but I
put me daba and your vision of machine
learning now they litigate that work but
the gist my lab terminal bit of hearing
services that these are treats
neurobiology as a reverse engineering
topics on one hand we natural systems
the other hand we built artificially to
try thank work the same height so that's
everything on the natural side-cave your
functional magnetic resonance imaging I
went 45 hard long years working with
monkeys and then we also worked the
grass and then meanwhile the on the
other side we we work with artificial
systems and we work particularly
computer vision and a face recognition
in contrast to a lot of computational
neuroscience groups that focus on
modeling the activity of neurons we
actually try and build systems that
really work and try and engage with the
computer vision and machine learning
field so a most basically all of what I
do falls into the category of
biologically inspired machine learning
and there was a time when this would
require justification that we'd have to
have to explain why it was interesting
to to look at biology and but but
nowadays it's it's not so mysterious so
you know we have neurons on one hand
which have many connections and that
inspired back in the 80s and and
actually with work the extended way
before that to build up these these
these neural network models and then you
know inspired by the hierarchy of the
visual system where one area projects to
the next area to the next area we've got
these through hierarchical models
including things like Lynette and also
this whole big world of deep learning
which has basically taken over
everything so so I don't really need to
to justify anymore which is which is
something of a relief to me that it's a
good idea to look at biology and take
something from it and build something
but what I'm going to tell you about
today is not that work so I'm say that
for another day another talk what I want
to talk about a very very new line of
work in my lab where instead of just
taking inspiration from biology but we
want to ask is can we actually take the
data the actual
data from biology and bake it into some
kind of machine learning system and
that's what I'm calling biologically
informed computing and the reason I call
it that is because once you give
something a name then it has a life of
its own and and that's generally a good
thing for grants and papers and whatnot
so you heard it here first biologically
informed computing so in truth all
machine learning or most machine
learning is biologically inform them by
that what I mean is we take an image if
we're doing computer vision say we want
to make a camel detector I have my camel
here I give it to a brain it happens to
be inside a head the head can talk we
have somebody maybe a grad student may
be a subject will ground truth that
image and they'll their job is to say
camel and you're all very good at this
and then that becomes your ground truth
annotation you have another image that's
not a camel in this case it's it's a
sort of castle looking thing and you
give it to a human human says not a
camel so this isn't sort of
controversial any way shape or form this
is just what we've been doing for a lot
of time we generally don't think about
it and then you know traditional to
provide learning again this is this is
nothing new to this audience basically
we have all the camels we have all the
not camels and we draw a line we have a
function that tells you which is a camel
which is not a camel so no big deal so
the way we do this is with loss
functions and this is you know basically
you're just optimizing to create this
function and you know for an SVM you'd
have hinge loss for the last punch as
well known loss function and the key
thing is that that this is where the
human is hiding the human provided
ground truth labels and that's being
used to train your system and then sort
of its indelibly sort of now marked into
your system that there's a sort of
biological information that's in there
but what we would ask is can we squeeze
more information out of our subjects out
of the brain than just these labels
because if you think about it a label is
sort of the shallowest thing we could
possibly get just a binary label is this
a camel or just not camel or is this a
face or is this a does this person have
Alzheimer's or not or whatever you know
it just it just it's just a label so
just like what polly was doing getting
getting sort of more information from
the actual biology where
asking can we do that in a much broader
way for all kinds of different kinds of
tasks and people have done various
versions of this we're certainly not the
first people to think that you could put
humans sort of more in the loop so
there's a whole field of active learning
where you take a human who's again
assumed to be sort of an Oracle of
ground truth and add labels you train an
algorithm and then you query to find
examples that you did poorly on then you
ask humans to label more examples to try
and help out so this kind of in the
human in the loop approach works very
well and it's very effective at the same
time you can also ask experts to try and
codify what they know take some domain
knowledge and and various people have
worked on having these sort of expert
svms where you try and take some sort of
idea that in a rule of thumb that human
gives you whom an expert gives you and
then try and use that to bias the the
evolution of the early the training of
the SVM the problem here is that at some
level humans are shockingly bad at
knowing how they do things and if
depending on the domain it turns out
actually a lot of domain experts don't
have good rules of thumb the rules of
thumb actually don't usefully inform
though there has been a little bit
progress in this area but we wanted to
do something that was much much more
general so the intuition for the idea is
actually very very simple and it's this
that not all examples are created equal
so if we're looking at face detection is
a widely studied problem it's it's a
largely unsolved problem what we can see
is that there are there are some cases
that all face detectors are you know
Picasso iTunes viola Jones whatever are
all going to get very easily this is a
frontal face that you would expect this
to be an easy example but the key thing
is that not all examples are like that
so if a human were ground truthing this
he would tell us or she would tell us
that you know as one here one here one
here one here one here one here but this
one's considerably harder than that one
the persons in profile they're slightly
blurry you have this other one which is
which is very blurry you have this other
face where there's clearly a person here
and but they're they're heavily occluded
you know a human would know that
somebody is there but there might even
be ambiguity depending on your
instructions whether you label that as a
person or not so the fun
just idea that we wanted to explore was
could we take advantage of this fact and
basically just say getting easy ones
wrong should carry a high penalty and we
shouldn't radically alter our solution
to the SVM if we're trying to catch a
hard one so if there's an example that
even humans missed maybe we shouldn't
break our next to get that one so we can
just operationalize all this and
basically what this boils down to is an
effort to ask can we model human
performance with the goal of making our
classifiers more human-like another way
to say this is we'd like our classifiers
to make mistakes like humans make
mistakes and it's a little bit
counterintuitive that that would help
but but fun fun really what we're doing
is we're piggybacking on a more powerful
machine learning system so even if we
haven't seen all the data that the other
system is seen in this case the system
we're talking about is a human we can
still benefit from all their lifetime of
experience we can benefit from the fact
that they have a working solution to the
problem and i should say at this point
this doesn't have to be a human that we
piggyback on if we wanted to piggyback
on Google or Microsoft or Apple we could
do that using all the same techniques
anything that's better than us we can
figure out how to make our solution more
like bears even though we didn't have
access to all of their training data
which is kind of a sneaky trick so so in
this case carrying on with faces you
know we have a situation like this so we
had faces on this side and not they
still on that side we look at some of
these examples maybe we have a Miss
classification that's a relatively
frontal view this certainly happens and
maybe we have a false positive sorry we
have one that we got but it was kind of
on the edge what our technique or the
intuition would say is well maybe we
should should bias this a little bit so
we carry a high penalty for getting this
easy one wrong and we'd carry a low
penalty for forgetting this one wrong so
maybe we want to trade it maybe we
wanted to sort of tip the hyper planes
that we catch more of the easy ones and
don't worry too much about the hard ones
and this if you've ever done any face
detection work even today in 2014 this
is not a typical of what you find in
face detection algorithms so here we
have you know a standard phase detector
it's missed this person probably because
she's wearing this headscarf and then it
also has these sort of hilarious like it
got this guy's arm kind of false
positive so this is this is still
happening even
today actually if you go into Google
Street View also it's actually really
easy to find images of faces even though
they're required by law to blur out the
faces and the place you usually see them
is occluded by the sort of side pillar
of the car so anytime is an occlusion
you get this beautiful unblurred face
which they get sued for in Europe if
they don't catch them so there's some
interest in getting this right so the
problem is that humans are really good
humans are actually excellent at doing
just about any vision task that we care
about and so fundamentally there's this
idea that will actually this is latent
landscape of difficulty that we're not
tapping into and humans are just above
it and that's why we can trust them to
be ground truth but thankfully we have
sort of in neuroscience and psychology
for for for hundreds of years even
people have been have been developing
techniques to bring humans off of
ceiling performance and to really
interrogate exactly how they're solving
a problem in exactly what they know and
this is called psychophysics or cycra
mykko metrics and the distinction is a
little bit subtle but but people
generally use those words to mean the
same thing and the key idea is that we
can bring humans into a regime where we
can now interrogate this this difficulty
landscape then we can take that
information and put it into our kernel
machine and get better solutions so this
is what we would call a psychometric
function so this is one of the classic
ways to get people off of ceiling so
here we have a face and basically all
we're doing is you're just scrambling
the phase we do a Fourier transform and
add a noise into the phase and we get
this back and then we can sort of have a
nice continuous gradation from something
that's very clear or relatively clear to
something that's that's completely
devoid of signal and when we whenever we
have this sort of continuous you know
continuum of of difficulty and when you
measure human performance and then we
just have percent correct on the the
axis here you can get these very fine
sort of judgments of exactly where the
threshold is where they start performing
well and there's a whole field of
fitting different functions to this we
get images is more difficult you know
could shift left or right so if you
shift more left I mean you're getting
more you're able to detect the signal
even when there's less signal shift it
the other way so on and so forth you get
steeper transitions you can get more
more more shallow distinctions and all
these things give you information about
what the humans know and what they don't
know and how they can do these tasks if
you look at phase coherence for humans
and for artificial systems you can see
that basically there's no contest so if
we take standard face detectors a viola
Jones detector this is basically hacked
how to grad student hack picasa so when
you get access to google's detector and
then face calm back when they were still
in existence we got numbers from them
they got bought by Facebook so so this
is sort of like the antecedent to
Facebook's face detection technology and
what we see here is as the image gets
less and less coherent all the
artificial systems crash to chance and
whereas the human sort of hangs on way
until there's this very very little
signal so even around here at this level
of coherence humans can do this but but
basically the the artificial face
detectors we're losing it way up there
so humans are awesome at this task for
occluded faces it's even more
interesting so if we take a face and we
just artificially just cover it up with
with bars and it doesn't really matter
how you do this and then we just plopped
the percentage of the area visible you
can see Google hangs on until about
seventy percent of the face is visible
and then you start taking away more than
seventy percent of the face and starts
falling off a cliff face calm you know
it didn't at the time at least tolerate
occlusion very much but if you look at
humans they're able to work with
incredibly tiny little slivers of the
face so this is why Gulf and the
question is can we use this gulf to
figure out how to make our systems
better the other thing that happens when
humans ground-truth something is if
you're measuring carefully you they
might also tell you how hard or easy or
how representative or a non
representative sample was by how long it
took them to make the judgment so this
is sort of in this sort of mode of you
know Native Americans using the whole
animal if we're going to bother to have
a human subject sit in front of a
computer or a mechanical turk or sit in
front of a computer make these judgments
we might as well measure how long it
takes them to make these judgments and
you know there's a whole fields in
psychology of taking these reaction time
measurements and their distributions and
figuring out what that means about the
internal processing going on inside
their brain and then finally the other
thing we can do something called an item
response curve so it turns out not all
humans are equally good at all things
so if we rank order or humans by their
ability so in the case of faces there
actually are people who are called super
recognizers this is actually a real
thing apparently and my collaborator
cannot count actually studies these
people so they're really good at faces
and then the other end you have
something called prosopagnosia where the
people who are face blind so there's
this famous book called the man who
mistook his wife for a hat those people
have some sort of congenital or acquired
defect that makes it impossible for them
to or makes it very difficult for them
to recognize faces and then on a per
example basis we can collect these
curves and understand different things
about them so you might be able to tell
that a hard image only your your super
recognizers are able to get it an easy
image everybody is able to get and then
you also have these sort of problematic
images where there's disagreement for
some reason we have these shallow
profile we're not everyone's performing
the same and that's also information so
the whole gist of all this is can we
take this wealth of stuff that
psychologists have developed over
hundreds of years and figure out a way
to put it into the into the into machine
learning so that's what we've called
perceptual annotation again you have to
give things names if you want people
that you want them to have a real life
and this is work that was primarily done
by my postdoc Walter shower in
collaboration with Ken nakayama and
psychology and his grad student Sam
Anthony so basically what we do as many
ways you could imagine doing this there
are complicated ways in their simple
ways as always it seems like the simple
way usually wins so basically we do is
we just hack the loss function of the
SVM and we just pack in sorry we just
pack in an extra term that penalizes
based on consistency by some function
with human judgments so if basically the
simplest version can do is just take the
accuracy and use that in a lookup table
you just blown up convexity but turns
that doesn't matter and then you can get
you just basically bake that in we
hacked this on top of lib SVM so it runs
very fast converges very effectively and
then this is the basic flow so we take
all these tasks off of this website
called test my brain org which is run by
my collaborators it's basically like a
fancier version of Mechanical Turk where
the people actually want to be there and
actually want to be doing these
judgements we can model the pattern of
human errors on a per example basis and
can use that to an informant constrain a
variety of different different tasks and
part of the reason I'm showing you face
detection is not just because it's an
interesting and sore well study problem
but it's also happen to be what what the
guys in psychology were actually
studying at the time so they were
looking at face detection so they
already had a bunch of data so they gave
it to us so we could play with it and
this is a test my brain I encouraged to
go visit you can find out if you're a
super recognizer or not or if you if you
have you know borderline person tech
nausea and what people do is they
basically just come here because they
want to learn about themselves and find
out about their brain so they'll perform
hours and hours of psychophysical
testing for free and they'll come back
for more so you can too so because it's
in the world of psychology we have these
sort of funny stimulate so the way this
works is as two things going on here
once we have these faces that are that
are put somewhere on the image randomly
there either is or isn't a face though
these occluding bars that are randomly
put over there and then the backgrounds
kinds of funny looking thing it's this
texture synthesis technique from aero
Simoncelli slab and nyu basically what
is trying to do is is match the rough
statistics of the face but have it not
be quite a face so these are very
challenging images and you need to make
them challenging like this to get the
humans well off ceiling and then that
their task is simply to look through
these images they appear one by one and
you ask you know is there a face present
or not and then what we do is we have
the subject come in we present as many
of these as well as they'll stomach the
subjects are responding in their home
computers we're recording accuracy
response time presentation time
everything about that and then we can
generate all of these psychophysical
psychometric measures that I just told
you about and then and then basically
what we do is we take an existing face
detection data set so normal faces so
the vast majority of our training set is
actually normal faces and then we can
pepper in these kind of funny
psychophysical images where these can
you know contain little slivers
presumably they're close to the boundary
close the margins but you just sort of
pepper in a variable number of those
we've sort of tried all different kinds
of variances it is we don't really have
a rule of thumb but if you just pepper
in something like a quarter of the
images coming from the sort of special
set and they have whatever the regular
training set well
for your particular benchmark that seems
to work pretty well and then the goal is
then to have these guys influence
exactly how you how you solved for svm
so then so we collect the data we
generate these psychophysical measures
and then we come up with this human
weighted loss function which basically
again where we hacked in that extra
thing the penalty for margins not
consistent with the human data and then
basically we can have solutions then
based on that and just give you an idea
what the actual data looks like so if we
have those those face those sort of
challenging included images this is what
they look like in terms of so on the
x-axis here i have the percentage of the
face that's visible ranging from about
thirty percent down to one percent if we
just average all of the examples so
there's many examples of at each one of
these points we can see there's a sort
of a fall off and accuracy which is what
you'd expect as less of the face is
available then the accuracy goes down
but if you actually look out on an
example by example basis it's actually
huge variation around these means so
it's not simply a matter of how much of
the faces is available it's also a
matter of which parts of the face are
available and what that gets captured in
this sort of you know that's this plot
which has lots of data and then what we
can do is what we're imagining is
happening then is that these examples
where you know a lot of the face was
available but subjects still did poorly
or very little of the face was available
but the subject still did well are very
informative when we put into this
formulation of the perceptual annotated
SVM so and then of course we need to
actually test this on some sort of
accepted data set so we use this face
detection data set called FTD be it
suffice to say it's just a bunch of
images taken off the internet a lot of
people report numbers on this data set
and they're importantly they're not all
frontal faces there are lots of
different views and different amounts of
blurriness and your goal is to try and
try and catch all the faces and then
what we do is we basically just take an
existing face detection pipeline we do
it at many scales loosen it up so we get
lots of false positives then we put a
perceptually annotated SVM on top the
filter things out when we have some
scoring strategy to avoid having
duplicates and then we get a final
result so there's a couple variants of
this so everything from those those
synthetic images like I showed you but
then we also took out of another data
set of
face detection called the AFL w the
annotated facial landmarks in the wild
said this one turns out to have enough
difficult examples because they're real
flickr photos that we can take these
images and do perceptual annotation with
these as well we can pepper each of
those in we can also try different
different feature vectors so we're gonna
take the images we're going to put them
through and two different feature
representations so we could use just a
simple hog histogram oriented gradients
a feature representation but we can also
use this was a bio-inspired feature set
that we had around the lab that was
previously the best one on a particular
face recognition a side called lfw so
this is another one we used and finally
we can try different you know try to
provide I'm accuracy but we can also try
reaction time and see which one's work
better and worse so the bottom line and
so here I'm just showing you performance
across a bunch of different folds you
can see this is the annotated faces and
the low nfh annotated facial landmarks
in the wild a flw data set this is these
are natural these are synthetic images
and you can see the performance is at
some level and then if we this is with
hinge loss but if we add in this human
weighted loss we get a substantial boost
in performance and we're getting pretty
close to the ceiling so any kind of
jumps this is like a you know on the
order of four or five percent boost for
using this human weighted loss so a
substantial game and then if this works
irrespective of which ever you know
feature representation to use so for hog
you get a nice boost for our
bio-inspired features which worked well
on places before and not surprisingly
work well in this context as well we
also still get a nice boost we can vary
the features and then we can also vary
which psychometric variable we use so we
can use accuracy to get this boost and
turns out if we use reaction time we get
exactly the same boost so we just we're
just using how quickly the person
respond to each one of these images that
gives us a nice nice solid boost and
then if we just look at the ROC curves
you can see get this you know healthy
boost relative to a default when we
don't do this human weighted loss and
then at the time at least when we
publish this which was just a couple
months actually just yeah just a couple
months ago we also this also with the
best system available so if we look at
the standard benchmark here a variety of
different different techniques in the
field and you can see one of the biggest
strength of our approach is it goes down
for very very low false positive rates
we still get very very
accuracy and then it's also worth noting
that it's built on top of this veal a
Jones detector so we're getting a huge
boost / / doing just regular be love
Jones and it should also be noted that
you could do this on top of any other
sort of underlying face detector so we
think we could boost of other ones
there's another metric that counts not
just how whether you caught the face or
not but how good the match was how much
you overlapped with it and there at
least at the time we were sort of even
more dominant so it's not only getting
getting detection stuff getting very
good detection and this just gives you
some you know sort of a picture of what
looks like so on the Left we have the
detector it's sort of standard detector
that's built on you can see it's missing
a lot of the faces but because we're
using something that it's sort of you
know relying on the fact that using
humans to look at to learn more about
some occluded faces and things like that
you can see it's actually catching quite
a few more and then not exactly for fun
but but for four out of interest with
there was this recent Boston Marathon
bombing and one of the big stories that
came out of that was the patient facial
recognition software just didn't help it
wasn't useful so we ended up looking at
those images in some detail that were
available and one of the things that's
sort of bad about existing face
detection and recognition pipelines is
oftentimes you lose the face you're not
even detecting the face so if that's the
first step of your pipeline you've lost
the whole thing and this was all crowd
imagery that they were using to try and
solve the crime and it wasn't helpful
they were they weren't catching a lot of
these faces but when you run our stuff
on it actually we have a significant
number of detections that the other ones
missed so here's some some other
detection that were missed and then if
we do perceptual annotation here we can
actually catch him and profile as he's
running away so we're missing a lot of
faces we have a lot of false positives
but but overall having sort of increase
in accuracy is is a big deal if that's
the first up in your pipeline one of the
things it's also true in this sort of
regime is that the a lot of the data is
very poor you have video low quality
video you have ghosting and things like
that and standard detectors don't do
very well but for whatever reason our
perception annotated detectives actually
can catch a lot of these examples you in
spite of very very low quality so
oh so that's sort of the overall
overview of what we've done so far as a
proof of concept that you can take a lot
more information from humans and bake it
into a machine learning system going
forward we're taking this indeed even
more sort of sci-fi direction so we've
been collaborating with Jack gallant at
Berkeley who does functional magnetic
rim functional magnetic resonance
imaging of human brains while humans are
looking at visual stimulus so for every
image the subject sees they image the
brain and we have a vector basically of
the activity across their brain and what
we're working on now is trying to figure
out can we use that the embedding of
that space of this high dimensional
space of you know sort of the brain
activity can we use that in a similar
way that we used it the human
performance data to to bias and
constrain regularize our solutions in
the SVM or in another kind of classifier
we also are looking at cellular
resolution data so this is actually
instrument that we have in my own lab
and the other side of my lab so this is
a laser scanning two-photon microscope
basically shoot a laser at into brains
and then get a virally infected
genetically encoded fluorophores to
light up in response to activity sounds
kind of sci-fi it kind of is and then we
can get cell by cell activity and again
we can look at the sort of vector space
embedding of each stimulus in some brain
space and then we can basically try and
build solutions that work the same way
and since I'm done time is going to skip
ahead a little bit and just thank
everyone in my lab particularly Walter
Shire whose work this was and then of
course everyone who's supported our work
and I'd like to thank you guys all for
your kind attention
so I'm sure I understand where the gain
is coming from so it seems like when you
strip everything away rather than having
the training set being a bunch of ones
and zeros you know faces there wasn't
there you're having a call most like a
voting scheme based on the humanik
annotation I'm is that basically it that
you're giving it sort of fuzzier okay
not not exactly so what we're doing is
for a subset of so we're peppering in as
many perception annotated examples as we
can manage that's smaller than the full
training set so not every example in the
training set has this extra information
one way to think about it is as a
regularization term so we're penalizing
solutions that are less human like just
the same way as you might penalize
solutions that are more complex one of
the interesting outcomes of this which
which what we didn't expect is that we
have many fewer support vectors for the
human weighted loss than we do for a
hinge loss so the solutions are sparser
they're simpler we don't know why that's
true something about if you rely on the
humans to sort of give you some idea
which ones were raising hard you get
sort of less crazy solutions to the
problem and we think that that that also
helps but we're just putting bias terms
in there to say you know for a subset of
the examples how painful should it be to
get this one right or get this one wrong
that makes sense when you actually do
the evaluation that doesn't come either
it's right or it's wrong there isn't any
when you're when you're doing this
evaluation so it was a little worried if
that doesn't that doesn't come to play
so I don't know I don't don't totally
get how it's helping you when you do it
yeah it looks like you just adjusted the
margin for certain examples based on how
hard it was for human stand it so I was
wondering why you chose to do that
instead of just importance waiting
examples you're just another way to do
it yeah i buy some weight yeah I mean
that's you could certainly there's lots
of different ways you could implement
this we need to have I mean it is it is
it is analogous in many ways to
important sampling we're waiting
examples
based on any number of different
different measures the one thing is that
the you know you'd also very what the
sort of mapping function is to to
individual waiting so different measures
you know reaction time we're gonna have
to it has to sort of normalize waiting
for that versus accuracy versus item
response versus slope looks like a
metric curve and all these other things
but the key thing is that this is a
mechanism for for taking prefer deciding
what's important and what's why users at
the additive yeah I mean they both work
and it doesn't really matter um so you
mentioned that about a quarter of the
training set was these texts included
sorts of synthetic images so um I almost
wonder why not a hundred percent you
know how valuable are those images and
why what happens if there's some areas
how come no true was it oh right sorry
so asking why do we only is about a
quarter of the examples why didn't we
just have all the examples be
perceptually annotated there's two
reasons for that one is there's a
there's a resource limitation so you can
only perceptually annotate they're
expensive perception annotate we have
tens of thousands of subjects looking at
these images so you can't do all of them
though you could in theory in which case
then we'd expect it would it would work
better but it's sort of question of how
you allocate that in the case of the
simoncelli textures those synthetic ones
if you just train on those you get less
good performance on something normal
because the classifiers never seen what
normal looks like so so you're sort of
in this weird regime where you're you're
off in this corner which is a valid
corner of space but you don't want to
spend all of you know only show you only
have training data from this weird
corner of space but if you have some you
know the vast majority of it tells you
sort of what the basically the land is
then using this this funny little corner
to help tweak things that's that's
important so for the flw data where it's
they're actually just examples from the
real world those are sort of safer to
have larger fractions
yep up any insight went when you engine
working longer recognizes what's going
on and how you model that yeah so the
people in psychology have come up with
all kinds of theories and and that
that's sort of like the whole game in in
reaction-time studies so sometimes
people think about you know multiple
processes happening sometimes people
think about you know you have to have
some sort of inference process that's
sort of percolating to a solution the
short answer is nobody really knows yeah
yeah it's just giving us a readout which
which actually turns out to be
equivalent to the readout of the
accuracy across a large population so it
we were hoping that putting the two
together would give us an extra boost
but it turns out not to be true there we
actually times giving you the same
information as a population accuracy is
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>