<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lossy Trapdoor Functions | Coder Coacher - Coaching Coders</title><meta content="Lossy Trapdoor Functions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lossy Trapdoor Functions</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LftOMLOI6bM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so we are very happy to have with
us today I read Hemingway bread is a
postdoctoral assistant professor at
University of Michigan he got his PhD in
the mathematics department at UCLA in
2010 he's done wonderful work on coding
theory public key encryption different
kinds of public key cryptosystems with
strong security properties and today
he'll be telling us about how to
construct lossy trapdoor functions
thanks and thanks for having me out here
so yes so today I'm going to be talking
about constructing lossy trapdoor
functions so sort of to gauge speed how
many of you guys are familiar with lossy
trapdoor functions from pykrete and
waters from the stock 2008 so um not
okay so anyway I will go over them and
talk about by just want to see sort of
it what speed I should hit these things
um so let me talk first about sort of
what this law see is so there are two
things i'll be talking about in this
talk sort of lossy encryption and lossy
trapdoor functions and they both have
this same type of flavor that they have
two types of keys so if you're talking
about lossy encryption you have two
types of public keys you have one type
which is injective and these keys you
can encrypt messages under these keys
and then decrypt them property properly
and then you have lossy keys and if you
encrypt a message under a lossy key the
message is lost it's statistically
hidden and you can say the same thing
for these lossy trapdoor functions if
you apply a lossy trapdoor function in
injective mode the it's injective and
the message can be recovered the input
can be recovered by the trapdoor and if
you apply it in lossy mode the input is
statistically lost and the the
cryptographic requirement here is that
the two types of modes or the two types
of keys are computationally
indistinguishable so that's the only a
cryptographic requirement
these things otherwise it's a
statistical requirement so let me talk
explicitly first about lossy encryption
so here's the definition of lossy
encryption and it's gone by a bunch of
different names growth ostrosky and
Sahai called it parameter switching and
pie curtain waters to find this lossy
trapdoor functions and then pie curd by
Clinton authen and waters talked about
dual mode encryption and then colon
naseem talked about parameter switching
i guess and Volare ha finds inuit called
it a lossy encryption but i'll just a
lossy encryption because it sort of
parallels lossy trapdoor functions so a
lossy encryption scheme right is three
algorithms it's a key generation
algorithm G and encryption algorithm Ian
a decryption algorithm d and it should
satisfy correctness so that if you
encrypt with a injective key then you
can decrypt correctly it should satisfy
this loss enos property that if you
encrypt with a lossy key this PK sub L
which is lossy that the distributions
you get are statistically the same for
any two messages you encrypt so the
message is statistically lost and the
indistinguishability property says that
these injective keys are computationally
indistinguishable from the lost ones
yeah
so it means that if you generate you use
the key generation algorithm and you
generate a an injective key or you
generate a lossy key that the two
distributions coming out of this key
generation algorithm are computationally
indistinguishable so there's no
polynomial time adversary that can
distinguish them is that just from the
public keys yeah yeah
is that clear so that the public keys
are basically they're given sort of as
inputs to the encryption right you need
a public key to input I mean to be able
to encrypt something you're encrypting
it relative to some public key so you
could view an encryption algorithm right
as either right i mean you can view it
as a function that takes a public key
and a message and some randomness and it
gives you a ciphertext right is that no
there's no cipher texts or anything it's
just about the two public keys are
indistinguishable and that then
automatically it's a stronger statement
than saying something about the
encryption algorithms because now if the
two keys are indistinguishable then you
know any output of the encryption
algorithm under each type of ski will
also be indistinguishable so it's a very
strong indistinguishability statement
yeah that's that's a good example of any
place of yeah yeah I think I have that
as a bullet here or something right this
immediately implies semantic security so
you don't have to assume that separately
because in lossy mode there's full
security because the messages are lost
and the lossy mode is indistinguishable
from the injective mode so let me now
talk about lossy trapdoor functions for
a second so that was the definition of
lossy encryption lossy trapdoor
functions are the same thing except now
we're just talking about a one-way
function instead of a cryptosystem so
you have two types of functions
basically injective and lossy functions
and the injective functions are
injective and the lossy functions
basically have a small domain right so
they both have that they work on the
same domain sorry and lossy functions
have a small co domain them sorry small
to me okay the image is very small and
the two types of functions again are
indistinguishable that's the the
security requirement we have so again we
have this some key generation algorithm
that generates a seed in a trapdoor an
injective mode or just a seed and bought
in lost mode and write the trapdoor says
it should if you apply the function and
then you apply the inverse you get back
to where you started and the lossy
requirement now is just that the image
is very small right in lossy mode right
it's many to one instead of one to one
that that's the are will be some
parameter they call it the residual
leakage basically so the smaller our is
sort of the more lossy it is right then
yeah and so sort of this you get
stronger requirements the smaller the
loss enos or the smaller that residual
leakage so yeah it's called the residual
leakage because it tells you
it gives you some and sort of how much
information about the input is being
leaked by the function and right we
require that these two types of
functions again are indistinguishable so
these two notions are very similar right
once the notion for injective or for
trip for one-way functions and one is a
notion for crypto systems and and so
i'll be talking about sort of mostly how
to build lossy trapdoor functions and
sort of how they these lossy trapdoor
functions and lost the encryption
interact with each other yeah you can so
the issue is that our has to be big
enough so that if you just try random
things you'll still never get a
collision right so you know if you think
of something about a security parameter
right so our will still have to be sort
of as large as your security parameter
but the domain will be much larger than
that yeah so it has to be two times the
security parameter so for the birthday
bound but two times the security
parameter is still basically the
security perimeter enters yeah so yeah
so are you can't have the domain so
small that you actually will find
collisions randomly and I mean have the
qatari the code go to mango and the
codomain it just has to be a little bit
smaller than the domain that's all we
need and any amount smaller and sort of
the the more the smaller it is sort of
the more powerful this is but if even if
it's just a tiny bit smaller we get a
good result even a constant smaller even
if it's basically one bit smaller you
get you still get in CCA you get yeah
you get almost everything so so that's
kind of nice and actually this this
point about are having to be at least a
certain size actually is implied by this
indistinguishability right because if
our was small you could distinguish just
by testing
so lawsuit raptor functions were
introduced by pi curtain waters and
stock 2008 and they gave constructions
based on decisional diffie-hellman and
learning with errors then under pie airs
a decisional composite residue ah setia
rosen and say gabba gabba construction
and there was also a the same
construction in a separate crypto paper
when they were talking about
deterministic encryption Freeman and
I'll give a construction based on the D
linear assumption and also on quadratic
residue ossett e the construction based
on Phi hiding was published in terms of
as a way to remove the random Oracle's
from RSA oep and that's essentially the
construction josh was pointing out so
the Phi hiding assumption that you may
not be familiar with it basically says
if you have an RSA modulus and you have
a small prime number say three you can't
tell if that number divides the event
and and basically when that number
divides the event and the RSA function
with that modulus will be many the one
and then with rafi I'd we worked on
generalizing this the PI curtain waters
construction to sort of this extended
decisional diffie-hellman which gives
you a very general construction of these
lossy trapdoor functions and I'll talk
about that more as we go on but now
basically we have constructions of lossy
chapter functions under a lot of nice
assumptions and well we can build lossy
trapdoor functions what do we use them
for right the original the original
application was to create in CCA so
chosen ciphertext secure encryption and
they also gave a lot of their nice
constructions collision resistant hash
functions most of these can be
constructed right in a black box way
from NCCA but they have really nice
simple constructions from lossy trapdoor
functions then it was later shown the
lossy trapdoor functions are
deterministic encryption which is
a really nice thing rosen and Segev and
Mullen EULEX correlated product security
so if you have a bunch of functions and
you're applying them all on the same
input do they remain one way if you have
a bunch of one-way functions and you
apply them on the same input do they
remain one way and with lossy trapdoor
functions they do you have yes so you
have so you have the functions f1 say
through FN and user one way and now the
question is if I apply them so so if
they're one way that says if you're
given f1 of X 1 write an FN of x n right
you can't find x1 through xn but what
about if you apply them all on the same
input f1 of X up through FN of x yeah
right can you find X right and this
doesn't follow from the one-wayness of
the functions alone so I mean basically
in there's usually not much study about
how the actual functions are related
they just come from some same family
they're all just the functions
themselves or independent samples from
some families if I just generate you
know n random lossy trapdoor functions
that that's how you can do the photo for
for any so the number of times you can
do this depends on the loss enos that
basically the proof that lossy trapdoor
functions have this property is that if
you switch them all the lossy mode that
if the loss eNOS is so big that even n
instantiations doesn't reveal all of X
statistically then you can't invert but
these things these correlated products
functions correlated product secure
functions imply in CCA
implies CCA secure so it's kind of nice
and these don't have the strong
statistical requirements that lossy
trapdoor functions have so the thought
is that there may be these may be easier
to construct and there are actually
black box separations that say that you
can't construct lossy trapdoor functions
in a black box way from this and again
if you want a nice sort of example of
this right functions that are not are
not correlated product secure right the
RSA function if I have you know f1 of X
equals x to the e1 + f2 of X equals x to
the e2 if I apply them both on the same
input right then i can get X to the GCD
of you want any two and I can you know
assuming these guys are relatively prime
i can just recover x even though
individually they're one way on and so
that's sort of one nice property yeah
yeah it's exactly it just makes the
proof so easy yeah that's that's
basically the first thing you do is you
switch to lossy mode and then all of
your proofs can now be basically
completely statistical arguments after
that you just say you do some sort of
counting our demon or some kind of
entropy argument or something and you no
longer have to deal with this you know
probabilistic polynomial time something
and it's so it really seems to be
something about making the proofs just
very simple and natural and so it is
kind of interesting though that they can
you can replace the random Oracle in RSA
oep with lossy trapdoor function in
still works to get CPI security and then
you can use them to construct these what
are called leaky pseudo entropy
functions and I won't talk too much
about that but the nice basically the
leaky part is that lossy trapdoor
functions are automatically somewhat
leakage resilient if i give you if i
give you f of X and I give you now some
bits of X if the amount of the number of
bits of X that I give you is basically
less than its if it's still doesn't
determine X right so so if the residual
leakage so suppose X is n bits right and
the image of F in lossy mode is less
than 2 to the are right if I give you if
I give less than then you know n minus R
bits of X you still can't you still
can't invert F and so this is sort of
makes these functions really nice in
this leakage models right if you're
worried about some kind of leakage they
automatically have some sort of leakage
properties and that's what they do this
leaky pseudo entropy functions
so let me go through quickly the
original pykrete and waters construction
of glossy trapdoor functions was based
on DD h and essentially just used the
homomorphic property of DD h a lot so
there's everybody's familiar DD h this
is ok so right you basically the
description of a function will be a
vector of group elements g to the a1
through g to the a n and the and a
matrix of group elements so we can view
sort of a1 through a n or like
randomness and an El Gamal encryption
and the k1 through KN are basically keys
in an L go mal crypto system and so
we're thinking of this is basically this
is basically a bunch of El Gamal cipher
texts where everything in the top row I
have n el-gamal cipher texts that are
all encrypted under the same key right
they all have k1 through KN and they're
yeah they're basically encrypting be won
through bien and you know all of the
rows have the same key with in a row and
then the columns reuse the same
randomness right all of these guys have
G to the a1 and so that's the
observation basically is that it's not
too hard to prove that right ddh remains
secure if you reuse the same randomness
to encrypt under different keys and so
i'll call these things this vector we
call our it's like the randomness vector
and this matrix we call a yeah exactly
yeah decisional diffie-hellman and
actually i think i have a slide which
says it formally in a second but um so
here's this this description of a
function and we have this parameter be
basically we have this matrix this is
essentially like a diff Eamonn or an El
Gamal encryption of the matrix of bees
except we've reused a lot of randomness
so how do we how do we evaluate this
function right
the description of a function is this
vector R in the matrix a and now we
essentially just do a matrix product in
the exponent and so we we take we have
some vector X and 0 1 to the N and we
take the components of our raised to the
X I power so it's just we you know we
take those randomness and now we also do
that to the rows of X so this is really
you just want to think of a matrix
product except now and we replace
addition with exponential plication and
multiplication with exponentiation but
it's exactly a matrix product and so
what do we end up with we end up with an
encryption of the matrix B times the
vector X and again it's nice it's all
done with relative to the same
randomness and now it's an it's an El
Gamal encryption of B times X with this
randomness some of the AI times X I but
this randomness appears in all of the
terms it's the same randomness every
time so again because this is an El
Gamal encryption of the matrix B times
the vector X we can recover exactly when
the matrix B is invertible and now the
point is if you switch the matrix B to
be something if the matrix B is the
identity then it's trivially invertible
and we can always recover and if the
matrix B is the zero matrix then what
happens is that all of these terms go
away and all we're left with is G to the
sum of the AIX I to some power and so
the the output of the function when B is
the 0 matrix the output of the function
only depends on this some mod p and that
has at most you know p different values
and so as long as n is large enough so
that 2 to the n is bigger than p we get
some kind of loss eNOS and we can get as
much loss enos as we want because we can
always specify m to be bigger and bigger
yeah so this is the little awesomeness
claim so here's a description of the
output again it's just the same line
from before and it's exactly the saying
that if B is the 0 matrix then write
this term goes away and this term goes
away right and we're just left with G to
the sum so there was a question sort of
how can we generalize this this
construction worked for DD h and
actually essentially exactly the same
construction worked for learning with
air the lattice assumption and so with
working with rafi he wanted to ask this
question of basically this construction
uses the homomorphic property of El
Gamal so can we just replace el-gamal
with any homomorphic crypto system and
the problem is right you need this you
need to be able to reuse randomness to
get the lossy property you need to be
able to encrypt under lots of different
keys with the same randomness and this
is not generically secure so the
question is how do you generalize this
so we thought about lots of ways of sort
of generalizing this but then sort of
nice way to think about this I think is
to use to use originally we came at it
from looking at the universal hash
proofs of kramer and shoot but i'll talk
about that a little bit later i'll give
sort of a cleaner description of it now
so the the original pykrete and waters
construction is based on DD h so here's
the DD h assumption basically one way to
say it is that right this topple GG to
the AG to the B&amp;amp;G the a B is
indistinguishable from GG to the AG to
be in a random g to the sea you could
rewrite this if you wanted instead of
saying calling this g to the sea you
could call this G to the a B times some
randomness if you wanted right it's the
same thing so what if you just switch
this to say instead of choosing the
randomness from the full group what if
you just choose it from a subgroup till
they said ok call this the extended
decisional diffie-hellman and it's
exactly the same thing
except now this H comes from some
subgroup instead of the full group okay
so it's the only change and it looks the
same semantically it behaves almost in
the same way and the nice thing about
changing this slightly is that now you
can instantiate this under not only
under ddh but under other assumptions
that we like so essentially what if you
tweak the Kramer and Shupe constructions
from their universal hash proofs paper
slightly you can show that pyres
decisional composite residue OCD problem
immediately implies this extended
decisional diffie-hellman and so this
gives an alternative way of framing this
and so so here's the construction here
right the the decisional composite
residue assa t problem says that
basically the subset of n powers mod N
squared is indistinguishable from all
elements mod N squared and so now you
can you can create this you can the the
DCR assumption implies the extended
decisional diffie-hellman assumption and
also the quadratic residue hacia
assumption also implies a decisional and
extended decisional diffie-hellman
assumption and here right quadratic
residue assa t is just the assumption
that the set of squares can't be
distinguished from the set of elements
with Jacobi symbol one and so now the
nice thing is that these these three
assumptions which are sort of three of
the most standard a number theoretic
hardness assumptions DD HD CR and QR
right you can cast them all in exactly
the same sort of semantic framework and
and if you if you view it this way then
a lot of these generalizations of ddh
constructions actually become really
pretty obvious so like now we're in
Pincus heads OT based on on DD h &amp;amp; Y
alkali and
then later Halevi incolai extended this
to Kramer and shoots universal hash
proofs so you could get a similar
construction based on DCR and QR that
you had under ddh but if you view this
if you use the extended decisional
diffie-hellman assumption you don't need
the second paper right it's just that
the same proof goes through in all three
cases the same thing happened with
circular security the first circular
secure cryptosystem was ehh oh is under
ddh and then it was later extended to
get security under quadratic residue
assa t but again right the same exact
proof goes through from the BHA joe
paper if you just switch ddh with this
extended decisional diffie-hellman and
then you immediately get crypto systems
based under DC r and q are essentially
for free and so we also then take this
and apply it to the PI Kurt and waters
construction and you get the first fully
lossy trapdoor functions under quadratic
residue off today but I think this is a
nice way to think about this we
originally started thinking about this
as in the context of kramer and Shoop's
hash proof systems and that you could
say well hash proof systems allow you to
generalize ddh but i think talking about
it in this sort of extended ddh
framework makes it much clearer what's
going on it makes the proofs much
simpler and it makes it sort of more
intuitive why why you can extend things
so there there's a way you can sort of
extend the construction of pi curtain
waters to get more general law see
trapdoor functions from essentially the
same construction that you can use their
construction to actually get lossy
trapdoor functions from quadratic
residue assa t as well now i want to
change directions a little bit and say
here's a completely different approach
to constructing lossy trapdoor functions
we can start with lossy encryption and
build lossy Trump door functions from
that and now this is we know that lossy
trapdoor functions can imply loss the
encryption just the same way you know
one way
trapdoor functions imply CPA secure
encryption but going the other way
taking a cryptosystem in getting a
deterministic primitive is actually has
proven difficult in the past and so here
we give a construction that says you can
actually take loss the encryption with
some properties and get lost trapdoor
functions out of it so I want to go
through that so it in a standard
cryptosystem right you have a message
space in the ciphertext space and if you
take some message there's a whole set of
encryptions it can encrypt too and these
little circles right the fact that
they're disjoint means that you can
decrypt properly right they should
always be disjoint now on suppose you
had a perfectly lossy encryption scheme
right this would mean that in lossy mode
all of the messages the distribution of
messages is identical no matter which
message you have so all of the messages
go to one space so now what happens
there can you get a lossy trapdoor
function from this well you can you get
it actually almost immediately for free
right if the number of messages is
larger than the number of crip shins we
actually don't need to deal with
encryption randomness at all you can
just say my I can define a lossy
trapdoor function as an encryption of
going to grab an eraser oh if this if
the domain of ciphertext is small when
you're in lossy mode I actually get lost
the encryption for free I can just
define you know F sub PK of X to be the
encryption of X and I can just even take
zero as my randomness you just fix the
rant on this and if if E is a perfectly
lossy cryptosystem this is
f is a lawsuit trapdoor function right
because as long as the message space is
bigger than the ciphertext space as long
as it's bigger than the random to space
the number of encryptions of 0 right as
long as you know message space is larger
than the number of encryptions 00 but
this is not may be that useful because
most things aren't perfectly lossy but
on the nice thing is that this intuition
kind of carries through in a fairly
similar way as long as we have
statistical loss enos so oh I actually
I'd forgotten I put this slide in but
basically this is exactly the
description that I have here right that
you just define this your flossy
trapdoor function is just an encryption
of X and you can fix the randomness to
be anything you want it doesn't matter
at all and you get a lossy trapped or
function exactly when the message space
is larger than the randomness space and
it doesn't matter how much larger can be
one bit larger that's fine so so what
about trying to extend this to some sort
of statistical lossy encryption so
here's again this picture for standard
encryption now it's the perfectly lost
encryption all of the ciphertext go into
one space all of them encryptions land
in one space and statistical loss eNOS
basically says the encryptions of one
you know overlap a lot with the
encryptions of zero and they overlap a
lot with encryptions of two so we end up
with this kind of picture and now the
question is right what can you say about
the union of all these spaces right so
the union of all these spaces will
actually be fairly large it even though
it doesn't look like it in this picture
will have to be basically larger than
the dome
right because the number of these
circles I have is the same as the size
of the domain and so as long as they
don't perfectly overlap they grow every
time so the union will be too big but we
can we can fix that a little bit so all
we have to do basically is we can define
this lawsuit trapdoor function in the
following way instead of just encrypting
with zero we just take a pairwise
independent hash function and apply that
to the message I mean to the to the
message i guess and use that as our
randomness ok so yeah actually it is
similar to this i think i have this
maybe on the next slide now two slides
later yeah so this approach of basically
taking so it's sort of making an
encryption scheme deterministic has been
applied before there is first
construction that just looked at
constructing injective trapdoor
functions from CPA secure encryption and
they showed that if H is a random Oracle
then that works and then there was the
first construction of deterministic
encryption also did essentially the same
thing but both of those constructions
use random Oracle's and so what's kind
of interesting here is that by just
having this loss enos property you
actually can remove the random Oracle so
that's I thought that was kind of a cool
thing so so the proof sort of works by
you have to bound the size of the dome
of the co domain of this function and
you can say the expected size is not too
big and if these pair what these
functions are pairwise independence
right the variance of the size will also
not be too big and um and you can sort
of go through and calculate exactly how
much dependence you're going again and
you can show that as long as the message
space is one bit longer
the randomness space you actually get
lost pneus and again once you have any
amount of loss eNOS at all you
immediately get correlated product
security you get CCA encryption you get
a lot of the nice things that you want
so yeah this is what I was mentioning
before um that basically there are two
previous constructions at least two I
think there may be more that said that
you know if you do this type of thing
with a random Oracle you get what you
want and that's maybe not super
surprising because right the random
Oracle breaks the dependence and here
it's it's in general it's hard to argue
about implying a cryptosystem where the
randomness depends on the message right
but in the lossy case you actually get
some tools to work with so it's sort of
nice one thing I want to say is that
lossy encryption while it's sort of a
relatively new primitive it's actually
exactly the same as statistically sender
private 1 out of 20 t so every time i
say lossy encryption you could be
thinking sender private 1 out of 20 t if
you want and there's reductions in both
directions and they're they're fairly
straightforward I like using the term
lossy encryption especially when you're
talking about lossy trapdoor functions
because sort of the ideas are very
parallel but um right this one out of 20
t is very well known primitive so it may
be nicer to think about that in some
contexts now the big problem with this
construction right is that we have this
requirement on the plain text length the
messages have to be at least one bit
longer than the randomness and this is
this is a strict requirement I mean this
is not an easy thing to get always and
now you can sort of always get it from
these homomorphic schemes that are
randomness reusable so anything you can
build under like extended ddh you can
get this property but that's not that
useful because you already have lawsuit
raptor functions under that so the big
question here is right can we can we
remove this requirement
that would be really interesting and
maybe you could relax the requirement
that you don't necessarily need a lossy
trapdoor function you just want an
injective trapdoor function that would
still be a big result to say you could
construct just injective trapdoor
functions from any lossy encryption yep
yeah yes now the nice thing is if you do
their construction with like composite
residue assa t then you can you can get
a constant ciphertext expansion because
instead of encrypting and bits at a time
you you do you know things mod N and so
you can encrypt things mod capital n so
there's two parameters now little n is
the dimensions of the matrix and capital
and is your modulus and they're right in
under DCR the encryptions are things of
size capital n and your group elements
mod N squared so you get basically a two
fold expansion instead of a a huge
expansion link like you get in the piker
atwater scheme now you can also just
yeah you so you can do if you don't
follow the piker Atwater's construction
you can just take the the damn guard
Derrick extension of pi air and that
immediately becomes a lawsuit raptor
function right away basically under this
a lossy key will be an encryption of
zero and an injective key will be an
encryption of one and this is additively
homomorphic and so f of X will just be
like PK to the X power and this will
either be an encryption of X or an
encryption of zero and under the damn
guard Derrick extension this is mod you
know end to the s plus 1 and so X can
live in n to the s and the randomness
space is only of size and/or size fee
event really and so this reveals this
sub this is
this is generates a subgroup of size and
basically where as this guy generates a
subgroup of size n to the s plus 1 and
so this is sort of the most efficient
into traffic or function that you just
get from from this power so you do even
better than just mimicking the pykrete
waters construction here yeah so I'm
doing in terms of time um I want to talk
a little bit now about pash proof
systems this is another way a lot of
this work was sort of motivated by
trying to think about how do you get CCA
encryption from homomorphic encryption
so here is another approach that Rafi
and i took that uses a lot of the same
sort of similar same ideas so here's the
description of kramer and troops hash
proof systems and so I was claiming
before that these are slightly less
intuitive maybe then the extended ddh
assumption but you should think about
them as the so far the only real uses
they've had have been in sort of
generalizing ddh constructions but
essentially the idea is you have you
have a subgroup you have some sort of
hard subgroup problem you have some some
big group X and some small group L
writing these things are
indistinguishable and you can sample
from l along with witnesses that
something is in L and then you have
these hash proofs which you have these
hashes which have two algorithms is the
public evaluation algorithm right which
which takes a public key some element X
and a witness that it's actually in L
and you have a private evaluation
algorithm which takes the secret key and
some X but you don't need a witness
anymore right and that these will give
you the same value so you have two
different ways of getting to the same
place basically one uses the public key
and a witness to the fact that you're in
a small space and one just uses the
secret key
and the correctness requirement is that
these things give you the same value and
the privacy requirement says that in
this case right in this case I can
actually apply this function to things
that aren't in l but are in the bigger
group and that if I do that then this
has high entropy even conditioned on X
and pub and the public key so the system
they're basically there are many secret
keys for every public key and when I
apply the hash to something in the small
space write it the function that the
action of the hash is completely
determined by the public key and when I
apply it to something that's out here
the action of the hash is not completely
determined by the public key right it
depends on the secret key as well have
people are people familiar with this
hash proof systems if so I mentioned
before sort of they've been used in a
number of contexts to basically
generalize these ddh based constructions
and that was kramer and Shoop's original
thing right they had their rid the first
CCA Sakura crypto system was based on DD
h and then they generalized it to DCR
and QR as well and they did it through
these hash fruit systems so these are
really nice to work with in a lot of
ways and they imply CCA security so
here's a list most of these things I
actually already head up on the board
but basically Kramer and Shupe
generalized their original ddh system
under hash Bruce kalai and halevi and
Clyde generalized their OT pro vot
protocols of narrow and Pincus here's an
example of not a generalization but you
get really nicely nice leakage resilient
encryption right away from this and
essentially for exactly this reason that
these systems they have many secret keys
for every public key and so if you leak
a few bits of the secret key
you still haven't nailed it down all
right and so it doesn't help you in
terms of breaking the security so that
these give a very natural construction
of leakage resilient encryption now you
can generalize the DD HBase circular
secure encryption and you can generalize
the you know pie cart waters
construction as well so how can we
construct these there are the original
constructions based on DD HQ r and d CR
they were in Kramer troops original
paper and then there's one other
construction which is on bone a Gentry
in Hamburg from there Fox 2007 paper
where they wanted to create ibe with
short cipher texts without pairings so
they wanted to build identity based
encryption based on QR and they created
a different QR based universal hash
proof in this context and it's it's very
different from the QR based one that
Kramer and Shupe use and has very
different properties but it's still
universe lash proof and still can be
used in a lot of the same ways and then
there have been modified hash proof
systems that are under computational
assumption so if you want to talk about
these are DD HQ r and d CR right these
are decisional assumptions right you
have to decide if something is in some
subgroup or not but there's also right
cdh the computational diffie-hellman
there's some various you know RSA type
systems where you have a computational
assumption as opposed to a distinguish
ability assumption and you can sort of
swap these out if you want to build CC a
secure crypto systems based on these
computational assumptions you can use
some of these hash proof techniques but
it's um it's slightly different
technique there so I won't talk too much
about these but they also go under the
name of hash proofs and essentially
they're a designated verifier music for
your crypto system that's how they're
sort of similar to these hash
so here's essentially the construction
of kramer and Shoop's original
construction of universal hash proofs
based on ddh you can phrase it in terms
of extended decisional diffie-hellman
and then write all of the things are
unified but so this is the basically the
kramer soup construction so we're going
to have a subgroup l and a big group X
and the subgroup is essentially the
diagonal group so I have I'm going to
unless my parameter T right I have a
bunch of generators g1 all right they
have G yeah G 1 through GT right in the
subgroup L will be the set of things
that have the the same exponent right so
this is the diagonal group basically in
this T fold Cartesian product and DD h
or e DD h basically immediately says
this diagonal group is hard to
distinguish from the full group and the
secret key will be a collection of
integers right just exponents here and
the public key will be the product of g1
to the a1 times G 2 to the a2 so the
public key will be a single group
element right so there's a lot of secret
keys for every public key right and the
relationship is essentially determined
by looking at the discrete logs of the
g's relative to each other so if you
know one secret key you can't find
another secret key right because that
would still be solving a discrete log
problem which you can't do even if you
have the secret key and so now write a
witness for X being an L is some power W
that says X is the right this is the
exponent that puts it in the diagonal
group and how does the public evaluation
algorithm work it just takes this string
of PK group elements and raises them all
to the w
and then multiplies them all together so
you get one group element out and how
does the private evaluation algorithm
work it takes the secret key which are
these a's and some value X which may or
may not be in the diagonal group and
raises each one to the a and multiplies
them all together and when X is when X
is of this form basically because
exponentiation is commutative you get
that these are equal so this is their
observation so this is how you get these
hash proof systems and so how is this
related to lossy trapdoor functions they
both have sort of these nice loss enos
properties in some way so the loss enos
property of hash proof systems that
basically says that if you use the
private evaluation algorithm and you
encrypt with something if you apply this
to to this X then you're going to get
some kind of loss enos right because
this value is not determined by the
public key and by X and so that gives
you basically receiver deniability and a
cryptosystem that you if you create a
ciphertext and you use an encryption you
take some X which is not in this hard
subgroup then for any m you can always
find a secret key that matches your
public key that opens the ciphertext to
that m so you can open it in any way you
want whereas lossy encryption goes the
other way its sender deniable and says
that if you encrypt under a lossy public
key then for any m you can open your
cipher text you can find randomness to
open your cipher text so in one case
you're trying to find randomness to open
your cipher text to nem and the other
you're trying to find a secret key to
open and so on yeah an interesting
question right is can you get basically
fully deniable encryption basically
these things are so closely related
right you really want some kind of full
deniability based on this what
yeah okay um um yeah so yeah okay well
anyway here's sort of I just want to put
this up as a relationship between the
two things that they both have sort of a
lossing this property and they behave in
sort of similar ways and you can
construct the lossy trapdoor functions
from any sort of homomorphic hash proof
system and so they are closely related
but they they sort of go back and forth
in terms of what the what what they're
giving you so one thing that we had to
like basically just a little observation
here is that if you take any homomorphic
cryptosystem if the if the plain text
group i mean if the ciphertext group is
cyclic or essentially almost cyclic as
it is in almost all the examples that we
know you give me immediately get hash
proof systems and essentially your hard
subgroup is the instead of encryptions
of 0 and this is the set of all
encryptions right and the encryptions of
0 are clearly a hard subgroup you can't
distinguish them from the set of all
encryptions and right you just apply the
hash proof system just by exponentiating
and sort of the natural way so you
immediately get if you have a crypto
system that has basically nice cyclic
properties you can get hash brew systems
you can get in CC a secure encryption
you can get lawsuit wrapped or functions
you get everything you can relax that a
little bit i mean it's easier to analyze
if you have perfect correctness if you
have as long as you get good correctness
that you all the things just go through
it I mean you have to have pretty good
correctness in any way yeah
yeah I haven't thought about it if you
only had constant correctness if you had
like three corridors correctness I don't
know but it certainly if you have you
know like one over one minus one over
poly correctness I'll go through yeah
anyway I better still best arriving
thought about that um but anyway so this
is sort of a little question right i
mean a bigger question right as much
more as can you get CCA security from
any homomorphic encryption right you
don't want to have to deal with cyclic
properties and you can extend this
pretty easily if you know how many
generators your group has you can you
can sort of tweak this construction it
just goes through but you'd like to have
something much more general that's the
sort of big question I think so yes so
you get you know hash proof systems lots
of chapter functions CCA encryption
deterministic encryption right these are
powerful primitives they give you almost
everything that you want so yeah so I've
sort of said this a bunch but um you
know what what do we have here that's
kind of nice well one thing is right you
can get extended decisional
diffie-hellman provides sort of a much
more intuitive framework for for
thinking about how to unify these
constructions of DC arc you are indeed
eh um and it gives you you know lossy
trapdoor functions I showed this other
construction lossy encryption with long
plain texts also implies lossy trapdoor
functions and then just this thing I
mentioned at the end then you know
homomorphic encryption if you have the
cyclic ciphertext group it implies these
hash proof systems which also imply
lossy trapdoor functions and you can you
can play some games too if you say you
know if you have the cyclic plain text
group you can you can do some games and
if you have you know cyclic randomness
group you can basically cyclic groups
are very easy to deal with and all of
these exponentiation tricks just work in
the same way there
so the big sort of the the flaws of
these constructions basically are right
when you're trying to construct loss
encryption or lost you trapped where
functions from Los the encryption right
we had this requirement that the
plaintext length had to be longer so you
could get some kind of loss eNOS and it
would be really nice to remove that and
I think it's if you remove that you're
never going to get a lossy trapdoor
function but you might be able to get an
injective trapdoor function it seems
pretty likely that you would in fact but
I haven't been able to sort of make the
arguments go through and then the other
sort of general question that sort of
motivated all of this work i think was
basically can you take any homomorphic
encryption without saying something
about the structure and get CCA
encryption and that would be the nice
question but it becomes sort of very
hard and it's even a little bit hard to
sort of model generically the
homomorphic encryption when you say
homomorphic encryption what exactly do
you mean how many you know which which
spaces are sample well do you know the
size of the group it's homomorphic over
you know what is and it becomes a little
messy but this would be a nice nice
clean because there are some yeah yeah
pretty much everything there I I don't
know I haven't found any constructions
that don't go through I mean ddh has
this one property that the the last
element you have a full group instead of
a subgroup but pretty much every if you
pick up a paper under ddh they're so
varied eh so if you just write this as
GG to a G the bg2 the a B right the only
difference here between these two things
is yes the last element right and does
this come and so under ddh this comes
from the whole group from group and
under ed DHS comes from a subgroup and
you could imagine some situation where
it's important that you have the full
group there in practice it doesn't seem
to man and pretty much every proof if
you just go through i mean it it it just
goes through exactly but I I suspect
that if you wanted to construct
something that only worked with bdh um
you probably could but but in terms of
sort of the constructions in the
literature you can just go through and
just swap out ddh for DC r QR with these
this framework of it and it gives you
kind of a nice way to do that and the
nice thing about swapping out for a DCR
and QR is that the the systems you get
don't require the factorization when you
write they're basically a discrete log
thing so now there's this extra level of
trapdoor you get from the factorization
and so sometimes you can actually take a
DD h construction and do more with it by
swapping it to DC r QR because then you
have this factorization you can give it
to the simulator and you can get some
kind of some kind of stronger form of
security
right you get some kind of global
trapdoor basically and that's
essentially exactly what happens with
with the bgh construction of identity
based encryption right you need this
global trapdoor for the master secret
key to be able to give out secret keys
for each identity right and so you can't
you can't do that with ddh you need the
hash proofs that are based on QR where
you have a factorization</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>