<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Everything I Know About Fast Databases I Learned at the Dog Track | Coder Coacher - Coaching Coders</title><meta content="Everything I Know About Fast Databases I Learned at the Dog Track - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Everything I Know About Fast Databases I Learned at the Dog Track</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/23IUpEG5buY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so it's my pleasure to introduce Andy
Pablo who's a PhD student about to
graduate from Brown University and so
today he's going to talk about his
adventures at the dog track hey thanks
it back alright thanks everybody come
today he's right I am going to talk
about my two main passions in life and
that's the science of database systems
and gambling on greyhounds at the dog
track now I realize I here's some
Snickers are for a lot of you these two
things seem like they have nothing to do
with each other whatsoever but what I'm
going to show today is that there's been
specific research challenges or problems
that we faced when trying to scale up
database systems to support modern
transaction processing workloads or
modern workloads where the answers to
those problems have come directly from
things that either seen or learned or
saw while at the dog track so before I
get into this I want to a quick overview
of what sort of the state of the
database world is right now for people
that want to run for fun and
applications and I'll loosely categorize
the types of systems that are out there
today into three groups so the first is
what I'll call our traditional database
system so these are things like db2
sequel server oracle my sequel postgres
and the key thing about the systems is
that they make the same architectural
design assumptions and hardware
assumptions that were made in the 1970s
when the original database system system
our ingress were invented right but
obviously a lot of things have changed
since then the second group is is come
around about the last decade or so these
are colloquially referred to as no
sequel systems so these are things like
MongoDB Cassandra react and these
systems are really focusing on be able
to support a large number of concurrent
users at the same time because they want
to be able to support web based and in
turn based in internet based
applications and so for them these
traditional database systems weren't
able to scale up to support their needs
and so my work my research is really
focused on this emerging class of
systems that has come about called new
sequel and we're trying to have the best
of both worlds or trying to maintain all
the transactional guarantees that you
would get in a traditional database
system but while still being able to
scale up and support a large mocha
current users in the same way that the
no sequel guys
can and so for this talk I'm not really
going to talk about the the notes equal
guys other than to say that their their
work is complementary to ours right so
there's certain applications where you'd
want to use a no sequel system and not a
new sickle system and vice versa so now
let's look at one of these modern
transactional workloads look like so
here we have a workload that's derived
from the real software system that
powers the Japanese version of American
Idol now realize is not call an American
Idol in Japan but just just go with me
for this and so in this application
which you have our people either calling
in or using their laptops to go online
and vote for contestants that they like
on the show and so when one of these
request comes in one of these calls
comes in the application starts a
transaction that will check in the
database to see whether this person has
called as called before voted before and
if they haven't it'll go ahead and
create a new boat entry for them and
update the number of votes that a
contestant has gotten all right and so
this seems like a pretty simple
application it seems like a pretty
simple workload that you know really any
modern database system should be able to
support without any problems so to test
this hypothesis we took to open-source
traditional database systems my sequel
and postgres and we tune them for these
type of sort of front-end transactional
workloads and we want to measure how
well they can scale up and get better
performance as we give them more
resources so in one machine we're going
to run the database system and we're to
scale up the number of CPU cores that
we're going to allocate to the system to
process these transactions and then on
another node we're going to simulate
people calling in and voting for
contestants and what we found is in both
cases both of these systems they can't
break past ten thousand transactions a
second right in the case the postgres
actually the performance gets worse as
you give it more CPU cores and so I'm
not trying to pick on my sequin postgres
here but I'll say that these results are
emblematic to other performance results
to be seen in other traditional database
systems so we've done other experiments
when we take you know Oracle and how to
pay for an a very expensive DBA to come
out and tuna for us and we see the same
kind of results as you add more CPU
cores you don't get better performance
so now the question is what's going on
what is it about these traditional
database systems that it's causing them
to not be able to scale up for the you
know what seems like a simple were club
so in another project in our research
group they took another
source traditional database system and
the instrument of the code to allow them
to measure how much time is spent or how
many CPU cycles are spent in different
components of the system when they run
one of these transactional processing
workloads they found about thirty
percent of the time is spent in the
buffer pool so this is managing an
in-memory cache of records that I've
been full from disk and running some
addiction policy to be able to decide
when we need more room what to evict
what to write back out to disk another
thirty percent of the time is spent in
the locking mechanisms of the system so
because there's a disk transactions can
be run at the same time and one of them
could try to touch data that's not in
the buffer pool so if it has to get
stalled while the record that it needs
gets fetched in to main memory so this
all accounts for about thirty percent
overhead you know if you have to do this
another twenty eighty percent of the
time is spent in the recovery mechanisms
of the system I said this is because we
have an in memory buffer pool there
could be dirty records that have not
been safely written a disk yet so they
have to use things like a redo log or
right ahead log and other mechanisms to
make sure that any transaction that gets
committed that all the changes are
durable and persistent if there's a
crash you don't lose anything that's
already been committed so that leads us
a poultry twelve percent of time left
over to actually do useful work for the
transactions so this is why these
traditional systems are not scaling up
right because we just simply have all
this other overhead for all the other
components alright so where does that
leave an application developer yes that
scalability not simply a characteristic
of even a uniprocessor sleep see
question is the multi-core I go on this
so this is this is like in a multi core
system right it's um I see what you're
saying the question is this is more
emblematic to traditional systems rather
than being a multi-core system and I'll
say so like the idea is that well in
these type of workloads are usually
cpu-bound right from these experiments
here it's a main memory database system
so there is no disk right to these
that's why we're showing the the CPU
latencies so this is just showing that
if you want to be a little scale scary
system to support on these modern
hardware with with a lot of cores as you
start to scale scale them up you think
you could get better
performance but you don't because you're
paying so all this extra overhead to do
all this locking stuff you're shaking
you head no sorry yes what says that the
overhead doesn't can scale for safe
question is yes correct oh no it's I
mean well I'll come to this but yes
there are other things but the basic
idea is that there is all this
architectural baggage from the 1970s
that all these these these three things
are representative of and so if you have
if you take a you know a new look at
what these workloads looked like and
what the hardware can do what you can do
with that hardware then maybe you don't
need these three things right parts is
having lots of synchronization
primitives in them and the green part is
being like absolutely yeah so in case
they especially the lock manager right
right see if you have concurrent
transactions locks latches and you text
and other things the other problem is
about the overhead is the
synchronization those
but these types of operations like
maintaining a buffer pool recovery they
may be in inherently synchronization but
you had to get the pin pages yeah yeah
yeahs order to prevent scalability you
would have to argue that if the
percentage of time spent in each of
these increases as you increase the
number of course it's not just a matter
of having this overhead if this overhead
remained constant while you increase the
number of cores you get scalability
right so let me go to go further and
then if you have more questions when I
say to what our system is doing we can
we talk about that okay so where does
that leave application developers well
up to like I said a few years ago
there's really only two choices you
could go with a traditional database
system a lot of people do this because
they provide the strong transactional
guarantees and it's actually easier to
write programs when you have
transactional semantics but these
systems are notoriously hard and
notoriously expensive to scale up so
anecdotally I'll say I have a colleague
that works at one of the big three
database vendors which is in Microsoft
and he tells me that they're one of the
largest customers is a major bank in the
US that pays about a half a billion
dollars a year you know per year just to
run their transaction processing system
is for most companies most organizations
that's simply infeasible so a lot for a
lot of people in the best that's decade
or so we saw the rise of these no sequel
systems right because they'll be able to
get the better performance that you want
to support internet and web based
applications like we have a large number
of current users but these systems
achieve this performance over the
traditional database systems by forgoing
all the transactional guarantees that
the traditional systems provide so in
the application you have to write code
to be able to reason with your
inconsistent or eventually consistent
views of the database so again our focus
is going to be on the class and new
sequel class of systems or again we're
trying to have the best booth which
probably have a scale up and get better
performance and while still maintaining
support for transactions so a real the
research problem trying to solve here is
how can we actually do this and what
I'll say is we're not going to do this
by being a general-purpose system right
we're really going to focus on a certain
class applications that have
key properties that we can exploit in
our system and we're not we're not going
to try to claim to be a
one-size-fits-all database system for
everyone and so the first question is
well what are these properties in these
type of applications that we're going to
focus on that we want to have our system
be optimized for that you know that
we're really gonna take in consideration
and so the answer to this first problem
is can be found actually at the dog
track so specifically there's three
important characteristics of greyhounds
in dog and dog racing and the type of
transactions in the applications we want
to support that are actually directly
analogous to each other so the first is
that both of these things are very fast
so in the case of these trent in case of
greyhounds they're the fastest type in
one of the fastest animals you could
have on the planet they go around ohms
about 40 miles per hour and similarly
our transactions are the fastest type of
transactions you can have a database
system so we're talking about
transactions that can finish on the
order of milliseconds rather than
minutes or even seconds we're not
talking at long-running transactions the
second thing is that both these things
are very repetitive so in greyhound
racing the dog just runs around and the
circle on the track and that's it right
there's nothing else that it actually
can do right similarly in our
applications we're going to focus on the
database systems can be doing the same
set of operations repeatedly over and
over and over again I see if you
remember back from our American Idol
example when someone calls in there's
only one transaction that's ever going
to be invoked and that one transaction
only has three steps so we're not going
to focus on optimizing the system that
allow people to write arbitrary
transactions or open up a terminal and
write a random query in the last thing
is that both of these items are very
small or have a small footprint so what
I mean by that is greyhounds actually
have a small footprint or paw print for
a dog with their size or stature and
similarly our transactions are going to
have a small footprint in the overall
database so the data set itself could be
quite large but each individual
transaction is only got such a small
number of records at a time so we're not
talking about long running queries that
are doing full table scans doing complex
joins to compute aggregates and things
like that we're really talking about
transactions that come in use an index
to find the you know do you point
queries to find individual records that
it wants to read and only processing
them
so now based on these three properties
we've designed a system called H door
that's optimized from the ground up to
be specifically work for you know
operate efficiently for transactions in
these types of applications so this is
work that I've done as part of my
dissertation along with colleagues at
Brown MIT Yale and what was at the time
vertica systems and so an H store and
this maybe get to address some of your
questions in H door there's three key
design decisions that we're going to
make that are direct reaction to the
bottlenecks that we saw in the
traditional systems so in the
traditional systems they're inherently
disk oriented right so all that
machinery that I talked about before in
that pie chart a lot of that is based
you know you have to have because
everything you know a transaction could
try to touch data that's not in the
buffer pool but that's own disk but for
these modern workloads that we're
looking at in many cases the database
can fit entirely in main memory you can
buy a few number machines that have
enough RAM that is able to store the
entire database entirely in main memory
and so in a store we're going to be
we're going to have a main memory
storage engine we're going to assume
that the database is small enough to be
able to fit in RAM so we're talking
about databases that are usually for
these type of applications or but a
couple hundreds of gigabytes and the
largest one that I know of is Zynga
which is roughly about 10 terabytes so
again it's perfectly perfectly uh you
know feasible to buy enough machines
that have enough memory to do this the
second thing is because in the
traditional systems because there's a
disk they have to allow transactions
among concurrently because at any time
one could stall because they tried to
such something that's not in the buffer
pool and so to begin to do this you have
to have a quick vertical streaming
concurrency control scheme that's using
locks and latches and you texts and
other synchronization methods to make
sure that one running transaction does
not violate the consistent view of
another transaction that's running at
the same time but now if everything's in
main memory you're never going to have
those kind of disk dolls right so maybe
does not make sense to actually have a
lock managed and a half concurrent
transactions anymore so in a store we're
going to have serial execution of
transactions meaning we're gonna execute
transaction is one at a time add a
single core and this sort of makes sense
because if the cost of going acquiring a
lock in main memory is the same is
actually just accessing the data in main
memory you models go access the data
then lastly in a traditional system
they have to use a more heavyweight
recovery mechanism to make sure that all
changes are persistent and durable after
a crash so they have to use something
where they record the individual changes
were made on each record that was read
or written to by a transaction so in a
store we're going to use a more compact
logging scheme that's more lightweight
more efficient what we only need to
store water transaction was rather than
what it actually did and I'll explain a
little more than me that in a second so
now the basic architecture of a store is
that the database is going to be split
up into disjoint subsets called
partitions that are in stored entirely
in main memory so this example here I
have saved a single node I'm going to
have two partitions and so my databases
will be split into disjoint subsets
where have one half the database between
one partition and the other half of
being the other partition now for each
of these partitions it's going to be
assigned a single threaded execution
engine that has exclusive access to all
the data at that partition and what that
means if any transaction needs a touch
date of that partition it has to first
get queued up and then wait to be
executed by that partitions engine and
because what have these granular locks
at the partition level when in
transactions running since the engine is
single threaded it knows that no other
transaction is running at the same time
so we don't have to set any fine grain
locks or latches down within the
underlying data structures within the
partition so when who's run beginning
the end without ever stolen so now to
execute a transaction the application
comes along and it's going to pass in
the name of the storage of store
procedure that it wants to invoke and
then the input parameters for that
transaction so in a store the primary
execution API is going to be through
store procedures and stored procedures
are essentially in our world java class
file where you have a bunch of
predefined queries that each have a
unique name and then a run method it
takes into the input parameter sent in
by the application and invokes program
logic that will make invocations of the
predefined queries and so we have a very
important constraint in our store
procedures in a store and that is they
have to be deterministic and what I mean
by that is they're not allowed to make
you know use a random number generator
inside the run method or go grab the
current time or make a invocation using
RPC to some outside system and all the
information that it needs in to process
that transaction has to be you know
contained within a passed in from the
client and this would be important later
on for recovery mechanisms yes
you can't depend upon this prior state
of the database you can have conditional
logic which yeah that's okay like yeah
so the determinism really has to be if
if we re execute this transaction at a
later date in the same order that we
process it originally we need to end up
with the same you know and ending state
this is perfectly fine to do a query
rebeck the state and then if branch do
something different that's fine okay so
now the transaction request will be
acute up at the partition that has the
data that it needs and once it reaches
the front of that engines q it'll have
the global lock for that partition and
it can be allowed to start running now
when it finishes we're going to go
commit its changes right away but before
we send the result back to the
application we have to write out the
same information in the application sent
us originally out to a command log on
disk yes the individual transaction
can't access multiple data for multiple
partitions no cannon will get to that
too that's that's later yes and so this
I'll taste through this command blog is
don't this writing this out is done as a
separate threads we're not blocking the
main engine yes this mean as you scale
up in course you have to do finer
grained position I
yeah so yet you could mean there's 14 1
cor there's one partition I mean finer
grain and sense of like oh absolutely
yes yes and so there is an upper limit
almost that sort of relate to his
question there's upper limit as you
partition more you could end up with
more multi partition or distribute
transactions yeah yes so we'll do we're
gonna bash these these these entries in
the command log together and do a group
commit where it's just one F sink to
write them out all at the same time as
we're advertising the cost of doing that
right across multiple transactions so
now once is notes safely written and
durable out to the command log it's a
safe for us to go ahead and write send
back the result to the application now
there's also a replication scheme that's
going on here we're doing active active
replication where we just can forward
these transaction request from the
application to our replica nodes in the
process them in parallel but I'm not
going to talk about that today or right
now because it sort of complicates
everything we'll talk about later on but
if you want to know more about it be
happy to talk about it afterwards so now
while this is all going on the database
system in the background is we taking
asynchronous snapshots of the partitions
in memory and then writing them out the
disc as well alright so we're use a copy
on write mechanism so we don't slow down
the main execution pipeline of the
execution engines and so now there's a
crash all we need to do is load in the
last checkpoint that we took and then we
can replay the command log to put us
back into the same database state again
so this is why that fit be deterministic
on recovery would make sure we end up
with the same result yeah correct yes
but you need make sure all the nodes are
doing the chap one at the same time but
then you can use a copy on write
mechanism make sure that you don't block
everybody else but yes okay so now if we
go back to our Japanese American Idol
workload we looked at before and this
time we're going to run the same
workload in the same hardware using H
door we can see when we give a short a
single CPU cord the process transactions
it can do over 20,000 transactions a
second but now the main differences as
we scale up the number of cores that we
give the system we can get better and
better performance up there about factor
25 x over what we can get in the
traditional systems on on a coors and so
now immediately every sing one of you
here when you see a performance gaining
like this should have a you know red
light or siren going off back in your
head telling you to be skeptical and
what I'll say is that this is not a
parlor trick right all three systems are
running the same workload the same
serializable isolation level and then
running with the same durability and
persistence guarantees so the database
crashes all three systems can recover
any any transactions that were committed
so we can actually look at some other
workloads and see the same kind of
performance roll results so TPC see as
everyone I'm sure it is aware of its the
canonical benchmark that everyone uses
to measure the performance of these
types of systems and we see the same
kind of performance result as we get
more cores to a store we can get better
performance whereas the traditional
database systems is simply flatlined
telecom one or TM one or is actually
referred to as TTP now it's a workload
from erickson where that simulates
someone driving down the highway with
their cell phone the cell phone has to
update the towers and say you know if
you need to call me here's where to find
me and we see the same kind of result as
well as we get more cores to aged or we
can do better whereas the traditional
systems the performance actually gets
worse so now you would look at these
results and say well this is great you
know a store does much better than a
traditional system you know why would I
ever want to use a traditional system
today when something like the
architecture of a main memory
architecture or like a store came much
can get much better performance and the
answer should be quite obvious everyone
here and that is its inherent problem to
main memory database system is that
you're limited to databases that can fit
in main memory but that's okay because
out-of-the-box a store supports multi no
deployments so here if we look at the
same three workloads that we started off
with before and this time we were to
scale up the number of nodes in our
cluster so we're gonna go from one to
two to four nodes with eight cores per
node and we see the same kind of thing
as as we as we add more hardware to the
system we're able to get better
performance and this checkered line here
is sort of marking where we want to be
in terms of achieving linear scalability
which is the goldmark standard what you
want to have in a distributed database
so as we double the number of nodes we
want to get double the performance in
the case of the voter benchmark were
pretty close to achieving that in case
of TPC in telecom one but a little bit
off because the nature of the work load
so again now you look at this and say
well this is great because in a store I
can add more machines I can support
databases that are larger than
memory on a single machine why would I
ever want to use a traditional database
system when I can just buy more machines
and scale out with H door and again sort
of related to the generals here's
question is in these three workloads all
the transactions were single partitioned
meaning course is number of issues I cpu
cores and / / aids and it's it's 12 to
24 yeah I should have marked that sorry
so for these workloads here all the all
the transactions only need to touch data
at a single partition so when they ran
they did not need to coordinate or
synchronize with any other node in the
cluster and that's why we able to get
this really good performance but now if
you have a transaction that has to touch
multiple partitions you end up with what
is known as a distributor ins action and
this is really the been the main
bottleneck of the main problem of why
alive a distributed databases from the
1980s did not really come popular
because these this is why these systems
aren't able to scale up and although a
store is a new sequel system its modern
code base and modern you know harbor
assumptions we're not immune to this
problem either so now if we go back to
the gpcc benchmark again and this time
you make ten percent of the transactions
be distributed meaning ten percent of
the transactions need to touch data at
two or more partitions now we see as we
scale up the number of nodes the
performance is terrible it's completely
flatlined and we're nowhere near we want
to be in terms of linear scalability so
now you look at this to say this is
god-awful why would ever want to use H
store because if I try to scale up and
support and have multiple nodes in my
cluster I'm paying for more hardware and
paying for more energy paying for more
maintenance for those machines and I'm
not getting better performance at all I
might as well go back to traditional
database system where at least if I pay
more money I try to scale up and get
better hardware that way and so you know
this is the this is this is a real
problem because not all transit and not
all workloads can be perfectly single
partitioned in the way that we assume
before so in the early days of this
project we actually visited paypal and
paypal had this legal requirement where
customers from different countries
couldn't be on the same partitions so if
you had an account in Italy if you
hadn't account in the u.s. you want to
send money between the two people they
had some legal requirements as that had
to be a distribution Zack shin so an
architecture like Asia are simply will
work for them and in many cases the
application schema itself is not easily
partition abul at all either so you end
up with this bottleneck so this is this
is the main thing we're trying to solve
here how can we try to achieve linear
scale building and when we have
distribution sections so to do this the
first thing we have to do is get to
figure out what's going on what is it
about these distributions actions in a
system like a store that's causing with
Bob immaculate so now we want to look at
example where we have a multi-node
cluster say we have an application comes
along and it submits a transactional
quest to the system and this time this
transaction needs to touch data at these
four partitions so the way eight strokes
concurrency control protocol works is
that we have to acquire the locks for
these partitions first before the
transactions allow to start running and
the reason why we have to do it first is
we don't want have to do deadlock
detection if we have sort of more
fine-grained locking and now because
that'd be expensive to do in its
distribute environment especially when M
transactions that are finishing on the
order of milliseconds but now the first
problem we're going to hit is we don't
actually know what partitions this
transaction actually needs before it
starts running so we have to lock the
entire cluster even though we're never
going to need most of those guys so now
once we do this in the transactions
allowed to start running it can issue
the query requests to these remote
partitions to either access or modify
data that's located at the other nodes
and we're to see a second problem and
that is if we actually knew the number
of queries this turns actually need to
execute at each node we would see that
it needs to execute more data at this
node at the bottom or touch more data at
the node at the bottom so what we really
wanted to do is when the request came in
we want to be able to automatically
redirected to this note it down here and
run the stored procedure there because
that will result in fewer number network
messages you know because most the data
was need to be local we don't have to go
to the network to send query request to
these remote nodes but again we don't
know this information because we're
dealing with arbitrary stored procedures
all right so this is a difficult problem
and you could try to apply things like
static code analysis and other things
but that would be too slow to do an
every single transaction that comes in
so luckily for us the solution to this
problem can actually found back at the
dog track so I wouldn't say that i was
going all the time i go every day but
you know you go a couple times a week
holidays fourth of July one more day
mother's day stuff like that um and it
was one of those things we start going
the same place over and over again you
start to notice the same people in the
same patterns right people doing the
same thing every single time and this
guy named I met this guy named fat-face
Rick and he first came to my attention
because he was winning every single bet
he was making with all the bookies at
the track but he wasn't betting a lot
amount each time but every single time
he made a bet he was right owns a hun
percent of the time and it took me a
while but I finally figured out what he
was doing every single morning for a
race he would go down to the parking lot
where all the trainers were bringing
their dogs and sort of check in for that
night's race and he would pretend to be
a vet from the state Gaming Commission
and he would tell the trainer's I need
to look at your dogs make sure that
they're up the regulation and they don't
have any health code you know problems
right but what he was really doing was
checking them out to figure out which
ones were you know in the best shape
which ones with the strongest and which
ones didn't have any injuries and those
the ones he would go make make his bets
on and that's why he was always winning
so this is the same thing we need to do
in our database system we need to know
what things are going to do when they
come in before they start running so to
do this we built a machine learning
framework called Houdini that we've
integrated into H door that's going to
allow us to predict behavior
transactions right when the request
comes in without having to run first and
so we have a very important constraint
in this work of how we make these
predictions and that is we can't spend a
lot of time figuring these things out
alright so we can't spin about you know
100 milliseconds figuring out what a
transaction is going to do if that
transaction is only going to run for 5
milliseconds so the underlying component
how Houdini works is that we're going to
create Markov models or probabilistic
models for all the store procedures that
the application could could execute and
so we're going to build these based on
from from training sets of previously
exceeded transactions so for each model
we're going to have the starting in
terminal states for the transaction so
that begin the commit in the aboard
States and then we have the various
execution states that the transaction
could be in at runtime so these
execution states are represented by the
name of the query being executed how
many times have exited the quarry in the
past and what partitions this query is
going to touch now each of these these
states are going to connect together by
edges that are weighted by the
probability that if I transactions that
one state it will transition to another
state so now
at runtime when a request comes in
Houdini will grab the right model for
that unique request unique transaction
invocation and I'll estimate some path
through this model and based on the
states that the pet that the transaction
will visit in when it traverses through
the model that'll tell us what are the
optimizations we can ply at runtime now
if we get our prediction is wrong it's
okay because at runtime we're actually
going to follow along with what state
transitions that transaction actually
does make and so we're maintaining
internal counters of how many times it
goes across some paths so we start
noting that our predictions are
deviating from what we see in reality or
in the actual one-time behavior
transactions we can just recompute these
edge weights really quickly online I you
know it's a cheap computation it give it
back now and sink where the application
what the application is actually doing
so how we're going to generate these
models is that working again we're gonna
have a training set of previously
executed transactions yes I can
understand why you protect them predict
commence with more frequent to the
boards but what you seem to me what you
do need to do is be able to predict
which partitions being referenced
alright so again that's as we didn't
select so the state has the name and
equerry being executed how many times X
to the queer in the past what partition
is this invocation that query will go to
and then because there's a Markov model
we have to encode all the history of at
each state we also have the history of
all the partitions we've touched in the
past so you your your waiting's their
secret suggests sort of an overwhelming
packed choose but you could easily end
up with a path which in which there are
three possible partitions and reach
around thirty three percent yes give me
like two minutes and I'll solve your
problem okay great great segue all right
so we have a training set of previously
exceeded transactions right and so these
are all the queries and input parameters
that were invoking each transaction and
we're going to feed that first into a
feature cluster that's going to split
them up based on characteristics or
attributes of the transactions input
parameters that create the most accurate
models so these can be things like the
length of an array parameter the hash
value of another parameter and now we
have with these bucketed training sets
we're going to first feed that into our
model generator that will create the
mark-up models for each bucket and then
we'll have a decision tree across prior
to create a decision tree that will
split them up based on
features that we that we originally
clustered them on so now you know one of
the features could be what what's the
hash value of some parameter and that'll
tell us what partition we're going to
exit this query on all right what
partition we actually this transaction
on and that'll tell us now that it only
becomes like a linear state machine
where we don't have that equal
probability of what position to take
because it's no longer trying to giant
monolithic strawberries model it's more
individualized yes yes so we chose the
decision tree because we want to be able
to quickly traverse it and say at
runtime yes correct so with this this
whole top part here is actually us gonna
say next the whole top part here we're
doing offline so if you take a while
that's fine but now at runtime we can
quickly traverse the decision tree and
then quickly estimate some path in the
model so we can do this bottom part here
in microseconds per transaction so the
parameter values in Basking yes from
that you can exactly know this event no
prediction because you know exactly know
which partition that transaction is
going to touch or you don't it tells us
it it suggests to us what partition to
run the store procedure at but now with
in that store procedure it could touch
any any number of partitions it just so
happens in the case you know help you
with the law so they're so this sec take
a concrete example yes so this is a
dedicated name equal to ? yes and let's
say you have being equal to angle yes
yeah which particles to go if um sorry
go ahead yes alright so when request
comes in we grab the right model do
little hand where you magic with machine
learning so we take the model that's
best representative based on the
decision tree and then we estimate what
the path is through it and so when we
estimate when we're trying to figure out
what the state transitions we're making
its since we know what the tables are
partitioned on because if we have to be
told that ahead of time now we have you
know we know what the input parameters
are two to that transaction and that can
tell us what you know what actually
partition the query the court is going
to get to have to touch
there could be some dependencies here
that are actually stored in the database
okay so if I look up my main customer
and I yes that the hash of my main
customer now determines deposition yes
what is your logic so so in many cases
this this should work fine notice if you
have to read the state of the database
and say based on that now you hash one
of that that the output of one queries
being used as input for another query
these models don't capture and capsule
that information right so we take care
of that in other ways so we take care of
that again for the partitioning work
through down the automatic database
design work we've done we cannot figure
out hey we see this pattern happen a lot
it's usually read only read mostly so
we'll have us will create secondary
indexes that are replicated at every
single node so that we can do that look
up and then that'll direct us to the
right to right location so there's other
things more than that higher but you
wouldn't know right so in these cases do
you have any mode of saying well for
these transactions we would get a very
accurate model for this case for these
cases we just don't know so will back
off from our prediction and use
something more conservative because even
though you're you're most likely half
may not be very likely yes so your
question is is a way for us to identify
you were the different types of
workloads that we see that have this
dependency where you're reading stuff
from the database system and then maybe
back and then maybe not apply these
optimizations for them just more general
do you know when your model works well
and when it doesn't um so you I we have
not done anything formal about that but
i can tell you sort of off the cuff
things again doing that six that look up
and then using Apple of queries and put
another query that won't work well with
this but again we take care of that in
other cases if you have sort of large
range queries that the touch more
partitions and it's arbitrary what
petitions you have to touch that won't
work well this but I'll say for those
types of workloads for that second type
of query we don't see often in for this
type of applications work focusing on
that's more getting to like the real
time analytical stuff which we haven't
focused on yet I'll talk about that in
future work in our questions yes
so liquid into optimizations for docking
the partitions as well there is run your
predictions on which partitions the
transaction is going to access so so we
can do like in this case here we have
our path right so we know what
partitions we think is going to touch so
now we'll only lock the ones we need
during that I'm thing you decide now
yeah thnkx you know a lot of segues here
right so if we predict that a that with
if we fail to predict that we need a
partition in the beginning and we don't
lock it when the transaction tries to
actually access that partition we all
board it roll back any changes and
restart it and choir locks correct yeah
so yeah you can't you can't touch
anything without having a lock before
you so subasta police protection is just
the transaction applause sending starts
with the victim yeah there's there's
other things as well so like you could
try to lock something you don't actually
end up meeting and now the thing is just
sitting idle and can't do anything um I
don't have a good sense of what what's
worse um I know that you know I guess
I'll have to jump to it now um question
is yes how big is the difference so much
i remember correctly from our previous
conversation yes the difference between
a transaction that runs you know
correctly predicted correctly runs yes
either completely local or on Pete Rose
action versus a transaction that need to
run locking everything the gap between
this is huge it's mad yeah simply is the
idea of running a transaction in a
boarding halfway to then we run the very
slow one the over Adam of that the board
is that you're not the finder site crack
is not that relevant is that a fair
statement the overhead of aborting the
transaction and restarting it yeah so I
try to run the first version of it yeah
yeah I feel big yes I run this over yes
they try to run when you say slow
version it's more like you restart it
and you you lock you're still not
locking the full cluster if you have the
lock the full cluster so this is the
naive prediction scheme this is when you
assume that everything single partition
and then if you get it wrong you boarded
restarting quite a lot such a game right
and then the the top line is when we
actually could use Houdini a one more
accurate accurately predicting what you
know what we actually need it and so
it's about about 2x difference between
the two
right um I thought you'd ask I don't
know whether it's better to lock
something you don't end up meeting or
miss the lock my senses it's roughly the
same but if you have to lock the entire
cluster every some time to performance
is absolutely terrible right so right so
this is what we can do if we use our
model I sorry yes ah so this model like
how would I compared to say doing like
just a dirty run of the transaction do
you like simulate it yeah use that as a
model um I mean there's other techniques
that you could use you could simulate
the transaction and you could use static
code analysis you just ain't checking
for the simulating one I I think the
over I mean the overhead of doing it
might be problem with that is you miss
things like if I read back a value in
doing if branch statement here is the
fast version the fast version is much
faster than the slow version yes you're
doing even faster fast version which
didn't acquire locks just to see where
you think it might touch but you be
still up to acquire the locks bgg are
used to I are you are you suggesting you
just simulate the transaction not in the
engine in a separate thread yeah and
what data tries to touch doing that
Holly wreaths and that tells us how to
schedule wreck yeah um you almost art
scene call special enjoy I'm not
acquiring the locks is a big delay like
you have to do to you stick for your
case you would still have to acquire the
locks no like I'm saying like when you
do the dirty simulation you acquire no
laws so yeah use of my oh but so you're
sort of like doing optimistic
concurrency control kind of thing really
don't you like to run a transaction yes
on a separate thread yes actually real
data pretending that you know there's
nothing right only draws actually exists
and that and that'll tell you what what
partitions you need to lock but now when
you want it for real you have to do the
current circuit old scheme with quite a
bit lost that you're just wondering like
this dirty sim you
is also a model of what Craig yes to be
lost and so you have supported like a
fancy machine learn static analysis soon
as I'm just wondering you know what's
the straw man I guess is what I'm trying
to get at like how should I think of
this model is being better than or worse
than other attempts to guess it what's a
lot but I got that so i have i've not
done that simulation example but that's
something I have to do for my
dissertation work the there's other
things that we can do with these models
that I'm not really talking about today
so we can do things like we can identify
when we're done with a partition so so
we can go ahead and send like the early
two phase commit message we can also the
I don't want to I don't bring it up
because Phil's here but you can do
things like if you know there's never
going to be a user abort with absolute
certainty and maybe don't need your undo
locks I undo logging and you get about
ten percent speed up as well um but I
yeah I don't know good answer well how
much you can get just using a simulation
there's other things I'll talk about
later on how we can leverage models in
to do speculum execution and other
things like that where we we can talk
about afterwards whether we can still do
simulation for that but it's more we're
doing more than just figure out what
where should we send it and what should
we lock and that's my hunch of why just
doing a quick and dirty simulation might
be insufficient any more questions okay
so we so this is what we get back to X
improvement over that naive prediction
scheme but we can compare how well we
can do versus what I'll call the optimal
case so this is he had an Oracle that
knew exactly what every single
transaction was going to do this is the
best performance that you can get so
we're about ninety-eight ninety-nine
percent accurate as in some cases where
we lock something we didn't didn't using
or we lock something we don't lock
something we do do up needle aiding on
and so we're not that far off and where
we want to be in that case and actually
we run this even longer over time we can
learn more and our models get improved
and get closer and closer but so now two
EPs improvement is always a welcome
prediction yeah absolutely i have I
hard-coded something in the system that
said here's a request what is it gonna
do sort of fitting to see why how these
mortals can be so accurate so what's the
occasion so let me go back to the
example I had before let's say a lot of
transactions were named too tiny it has
something yes I should so you have a
history of that a parameter value
showing up that specific glad you right
it's more like um but but imagine for a
second yes then you see a parameter
value you've never seen do yes however
the modern do so we can we're doing what
TV what could I would imagine that we're
very long day though right so again
you've never seen before even though the
store you see there is exactly the same
yes right so we are this is like the up
me hold your question and then I'll have
a slide later on that i'll show you
because again we're doing hash
partitioning we don't yet have to encode
here's Andy here's of avec you know
individual records and how the map and
partitions learning the correct yes yes
but i'll show how we need missing our
selection later on okay so 2x
improvement is always pretty good it's
always a welcome improvement and when
you're doing Gaeta based research but
the problem is if we change the graph to
be reflected where we're going to be in
terms of linear scalability we're still
not in the right direction so the
absolute numbers have improved but the
trend is still not where we want to be
so this doesn't help us so again we're
adding more machines and we're not
getting the better performance that we
want so the question is what's going on
what's causing us not be 0 scale up so
it has to do with the inherent nature of
our current role model right and that is
because we have these granular locks add
a partition one in transaction starts
running when a transaction is running
and it holds the lock for at remote
nodes these remote nodes the engine for
them are idle doing nothing because they
have to wait before we have to wait
until the distributors action sends a
message over the network to have to
execute a query and send back the result
or start the two-phase commit process to
finish the transaction so they're
essentially doing nothing so now once
the once the disturb procedure does send
a request to these remote nodes they
have something to do they can process
the query but now we go back we sort of
flip
now the guy at the bottom he's idle
because he has to wait for the results
to come back before he can make forward
progress and once it does come back
again the remote guys go go back to
being idle right and this is because
we're optimizing our system for these
single partition transactions which I'm
the majority of the workloads that we're
looking at for these type of types of
applications but because we have these
such granular locks and we have these
long wait times this is why our system
is just being completely slowed down so
once again one last time the answer to
how to solve this problem can be found
back at the dog track so I first met
these guys they were ecstatic drivers
from Argentina and they were kind of
like a CD group they've sort of they
didn't really talk to anybody else they
were always you know talking amongst
themselves and I first noticed them
because they were always running around
looking very very busy so most people go
to the dog track to relax and at least
that's what I do and because there's
actually not a lot of things to do while
you're at the track because there's only
about 14 to 15 races per night and each
race is about 50 seconds it's over
pretty quickly so there's a lot of time
you're just sort of sitting eating food
waiting for the next thing to happen but
these guys weren't they weren't here to
relax they were at the track to make
money and so what they would do is they
would go down to the pay phones by the
bathrooms and whenever we were in
between a race at the track we were at
they would go call their bookies at
tracks in the next county over so they
can make more bets and so they can make
more money so again they're doing
something useful when when everyone else
is sitting around idle so there's a
guess it's the same thing we want to do
in our database system we want to be
able to do some kind of useful work
whenever we know that an engine is
blocked waiting on the network so to do
this we developed a new protocol that
allows us to speculate the execute
single partition transactions at an
execution engine for partition that is
blocked because of a distribution is
action and the key thing about this is
that we have to make sure that we
maintain the serializability of the
database system so my apologies feel you
have to sit through this so in a
serializable for sale as well database
what we wanted to do is we want to have
the n state of the data to be the same
as if we execute transactions
sequentially one after another and this
is essentially what we're doing now all
right so we have disturber transaction
and two single partition transactions
and each of those single partitions
actions do not start executing until
knows that its previous guy has
committed successfully but we see a 4 in
the case the distributors action we have
this huge block of time here wherever
idle whatever waiting for something to
come over the network that tell us to do
you know do work waiting for the result
to come back before we can continue
forward to a transaction so we're gonna
try to do here we can try to find a
schedule where we can interleave the
single partition transactions during
this time when we're blocked and then
we're going to hold on the results into
the end and we're going to do at the
verification process to check whether
everyone didn't create any conflicts and
that the end database is still the same
as it was if we execute in them
sequentially and so that we do know and
then that we have a consistent view at
all times so this sort of looks like
optimistic concurrency control right but
in that original paper from Professor
Kong in 1981 they assume that conflicts
are very rare so the number of
transactions you have to abort because
in the verification step because there's
a conflict is small but a lot of the
applications that we've looked at that
usually is a lot of skew either temporal
skew or popularity skew so conflicts are
not rare and you end up having a board a
lot of things over and over so what
we're going to do instead is we're going
to use pre computed rules that allow us
to identify whether two transactions
conflict and then if they don't we can
do we know that it's safe to enter leave
them and then now at the verification
step because we own because we scheduled
them based on the predictions will be
generate from our markup models if we
know we selected them that they wouldn't
conflict as long as they did what we
thought they were going to do we know
there aren't any conflicts and we can
commit everybody all of the end safely
so now let's look at an example here so
let's say we have attributions action
the nice touch data these two partitions
so the store procedures going to run at
the top and acquires the lock for the
partition at the bottom so when it
starts running at some point it's going
to issue a query request to this
partition at the bottom and then that
means that the disturber teacher will
get blocked in his idol because it has
away from result to come back so now
when this occurs hermes will kick in and
it's going to look at this engines
transaction queue to try to find single
partition transactions that it can
interleave as we have two important
requirements how we're going to do this
scheduling the first is that we have to
make sure that that a single partition
transactions will finish in time before
the distribution is actually needs to
resume so we've extended our models that
we used in
to now include the estimated run time in
between state transitions so because you
want to call it like cascading skulls
because you're holding these these locks
you want to go to finish up and have
everything as soon as possible so so net
weeks let's say we have attributions
action we have the prediction that we
generated from from the model in the
beginning and so it just exited this
query here and we anticipate it as going
to execute this next query here so now
our models include the estimated elapsed
time in between these transitions at
this partition so now when we when we go
look in our queue try to figure out what
guy we want to execute we will make sure
that they'll finish within this time but
then the second requirement is that we
need to make sure that we don't have any
read write or right right conflicts all
right so let's say for this speculation
candidate if we scheduled it we have
this problem where the distributors
action just read the value of this
record X from the from the database at
this partition and then it gets blocked
because that's a weight you know 2x do
some query edible note node but now if
we execute this candidate here it's
going to write some value that's going
to over it's going to change the value
of that record X the same one that the
distributors action read so normally
this would be okay except that when the
distributors action resumes its going to
try to read that value back again and
this is going to be a phantom read it's
going to get an inconsistent result so
this is this is a read this is a
conflict that we can't allow to occur so
we're not going to we're not going to
choose to spec we execute this guy and
we're going to skip it and go down to
the next one this next guy will finish
in time and it doesn't have any
conflicts so we know it's safe to
execute so we're going to pull out of
the queue and actually to directly on
top of the distributions action at that
partition as if it had gone through the
normal locking process this will never
ever going to have any conflict or
you're saying it's very unlikely and I
will abort if he if he lose weight so
I'm not claiming that we can do these
are all stored procedures for some
things it's too complex then to try to
figure it out but some some basic things
like I i read table X our table foo this
guy reads and writes tables bar I know
there's no conflict so there's okay to
do that
ok this is rules like that yeah
heuristics but don't know my question is
so imagine there are two there are two
versions of these you can imagine if one
is like it's absolutely guaranteed by
construction these two terms actually
will not interfere yes so I'm going to
do it then I potentially cannot even
check afterwards what happened because
we still can we still can well check at
the end at the logical level and weeks
to every there is a conflict occurs we
can't abort and roll back so we're not
we're not going to be unrecoverable I
might have a conflict but that there's a
zero zero one percent probability yes to
do it you can go ahead anyway yes and
aborted needed yes yeah and so um it's
actually sort of related to to to his
question is right so when it finishes
will commit it but won't well we won't
commit it right away will put its
results on a side buffer because we need
to wait to see learn where the
distributors action finishes yes think
I'm missing something but if you know
there's no conflicts then then you can
commit the single partition ones right
away from you how but you single
partition transaction could have read
something written by the distributions
action and therefore you have to hold it
kid you can't you know release him bag
uh it's a comp it's not a conflict in
the sense of like you know I read
something that I try to read twice and I
don't go back to a result it's more like
you don't wanna release the state of the
database of any changes for the chip
intersection until you know the
transaction has committed successfully
for some cases yes if you read something
that the distributors action hasn't
touched then it's okay to send back the
result yeah so is that if I read
something the distributed transaction
how far will you love it correct yes
indeed you know depends on whether
distributions action has it has read as
written anything before you know before
got stalled revy just read something
then it's okay I'm just sort of giving
an example of like when you would have
you want to read right a right right
complet so normally um and this is sort
of related to the vexed question in the
beginning normally this would be unsafe
thing
do because if you're using these
problems mr. models you know there's a
bit of randomness involved in picking
what path you're going to take and
trying to identify what data you're
going to you know the transactions going
to end up reading and writing but we can
actually exploit a property of our store
procedures to make these selections but
predictions more deterministic so let's
say we have our transaction comes new
transaction comes in the system and
starts off with begins day and weekend
we want to figure out what path we're
going to take through this model and
that's going to be able to tell us what
data we're going to read and write so
normally you'd say all right well here's
the two transitions I can make from
where I'm at now so you know I think you
know roll the dice and based on the edge
weights the distribution of the edge
weights you'll make you know go down one
path or another but in this case here we
see that there's only one query that
this was actually could ever execute
this get warehouse query and so in the
get what house query we see that there
is an input parameter that's being
passed in from the program logic of the
store procedure that's going to be used
on the warehouse ID which is the primary
key for this table and so since we
partitioned this table on this warehouse
ID that tells us we know the value of
this input parameter that tells us
exactly what record we're going to
access and so we can generate a
statistical mapping between the input
parameters that are passed in from the
transact from the application into the
transaction to the input parameters of
the queries so that means if we know
that since we know the value of the
transactions input parameters we know
the value for this input parameter for
the query that tells exactly what path
we're going to take what transition
we're gonna take for this case here and
we can keep cleaning and do this all
down the line so again this I'm not
claiming we can do this for all store
procedures but for a lot of them in the
applications we look at we see this
pattern and we can exploit this it's on
automatic yeah okay so it's a what is
the inducement for this
they cover given an arbitrary program
how do you go alone so now you had a
semantic knowledge that we're house ID
is yes you have a semantics of
associated with their homes ID but given
an arbitrary application how he is so we
have this no procedures receipt we have
these workload traces we say there's
much of your ways you can you could do
this we'd end up with the same result
the ecstatic a code analysis to tell you
do this cain't checking is another
example how to do this right we we do a
dynamic push we take these previously
exited workload traces and we say I will
this input parameter we see the value of
this query being executed within this
transaction how often does it correspond
to the input parameters for the for the
store procedure as we see if we see that
there's a direct mapping then we know
that we can identify this okay so now
again now this makes our predictions
more accurate more deterministic so now
at runtime when we want to commit a
bunch of specular transactions instead
of checking the read/write sets and in
dependency graphs in between them we
just need to verify that all our
transactions at the logical level made
this correct path transitions that we
predicted they were going to because we
have our pre computed rules to schedule
them to not conflict if I put in those
those rules are based on our initial
predictions as long as our predictions
are accurate we know that there's no
conflicts and go commit everybody all at
once and we're happy so there's the size
of the of these set of rules I mean for
every transaction typing qcc how many
different rules can can come out and you
know like how big is this pace of things
you have pre computed yeah I don't have
a good look I don't have a good and I'm
a number itself it's like fencing the
thousands in the billions in the
hundreds KCTV CC in the low hundreds
yeah if the other person who the hell is
PCC benchmark yes you know there are
some assumptions you know in this work
then may make sense in t PCC and I just
don't know if they make sense for the
types of applications that require you
know this sort of scaled out lightweight
transactions so I would say so t PCC is
actually very representative yes you
typically have those clothes of conflict
something yes in tvcc the workload in
the terms of the complexity and what the
transactions actually do is actually
very representative of what what I've
seen from in that in the industry I have
to confess up a little bit surprised
because it seems to me like if I'm
thinking about you know amazon shopping
baskets or something like that and it
doesn't seem to me like they're likely
to be a lot of conflicts because you'd
have to have you no conflicts within the
same shopping basket and that doesn't
seem likely to me um baskets that are
hot but necessarily transactions that
are going to conflict with each other
have invested a bad example because they
don't actually use transactions for that
um I like I can't give you a number to
say like eight percent the were close we
see have to have these properties but I
I'm saved and the things that I've that
I've looked at it's fairly common but I
don't have a way to quantify that just
yet well that's just right so again
we're not i'm not trying to go to
general purpose system i'm very careful
about saying that right so yes there
there could be some things that clearly
would not want to use a store for and
because they don't have these properties
but i would say that there's there are a
significant number of them so all right
we will do be if you get the impression
that they this research would apply to
their customers yes yeah look at that
okay so let's go back to the tpc
benchmark a third time again we're gonna
have the same number two super
transactions before i want to scale up
the number machines and this time when
we use a straw using her B's you can see
that we can get and get on this about
sixty thousand transactions a second
across four machines with a course a
piece right and so now we see in terms
of linear scalability we're in the right
direction so we're not actually going to
be we're never gonna be perfectly linear
scalable because we're bit conservative
in our estimates there could have been
cases where we could aspect executed
transaction but we didn't because we
doin calls these stalls yes before I
previous slide shows tvcc and you said
something like ten percent distributed
transactions so what's the fraction of
exhibited versus single partition here
and then if that if i change that what
happens to you yeah that's right so this
is this is they're all executing the
same number absolute number of
disturbance transactions percentage is
various because what we're doing is
we're executing more single partition
transactions you absolutely is a student
observation we're not actually doing
anything to make each individual
redistribution section run faster we're
to sort of be able to do more work on
the otherwise be idle its future work to
make a little speed up you know each
individual just reduction reduces
latency right so specifically a your
angle is speculating as
single partition transect yes so what
what fraction of the transactions here
are single partition I guess I'm just
wondering like because it seems like you
need to have basically a nice q of
single transaction or single partition
work that's ready to correct yeah to
exploit yes so it's that typical and
it's not a dial you can turn here to
show me what happens to the slope is if
you add more disturbances actions yes
you're gonna be down here again oh yeah
i'm not i'm not claiming was you have to
be right it's future work for us to
figure out you know how to slope keep
going up and ended up to xq more
distress transactions Oh specific odbc
see this a specific transaction makes
presented
these ten percent yes yes I did are you
anything now again it's it's the same
number of distributions actions for all
three points within a single column and
is that the fix ten percent or whatever
DPCC asked us to run right what's that
so the point is 60,000 f430 course is he
running the TPC see with the fixed ten
percent three transaction or or by
running attack with absolute numbers so
like you know chem person or whatever
this is say this is like 1500 it's
sticking on or whatever it is for all
because we're executing more single
partition transactions so you're not
running ten percent of these readers
actual anymore you know down to say the
absolute number is the same I suicide
yeah
that's a thousand sixteen thousand cuz
it over 60 okay can you can you always
tell whether you have a distributed
transaction versus a single partition so
we put in in case of these workloads
we're about uh you know I think we're
almost always accurate like ninety-eight
ninety-nine percent yeah but I mean
other workers we look at it we do the
same thing as well yes cam one's a
little different because you do this
well you can't always tell so if you
can't when you can't tell you how to
better I when we can't tell we ok when
we can't tell it's more like our the
same thing is getting a prediction wrong
we assumed it a single partition we
always exude a single partition and then
when it tries to touch something that we
didn't know about we just have to born
we started so we actually compare
ourselves what we're doing here versus
what I'll call the optimal case or the
unsafe case so this is where if you
assumed that there are no conflicts and
you blindly pull out whatever is in the
the first in the transaction queue when
you want to speculate execute something
and you don't care about sterilized
ability this is the best you can do so
we're not that far off from in terms of
the optimal case and we're maintaining
sterilized abilities that that's good
okay so yes when you say distribute
percentage distributed transaction so do
you have an idea whether any of the
let's say if we keep the percentage of
distributed transactions are same yes
but the transactions start touching four
partitions let's say I think in case of
gpcc typically to partition so yes
practice to be written as X let's say
for example if you start introducing for
partition transaction or a partition
transactions but do you expect as
similar enough or you expected to
flatten out I I suspect reforms will get
worse
yeah it's not only the percentage of
distributed transaction but how much is
it I mean if you are the Cinco de
speculative want to get in I mean
they're not changing how much much
itachi boy yeah if you touch more
partitions they died for the Hermes case
I don't think it'd be anything would be
the same trend because we're just asked
you to more single partition erza
transactions so it's ok so the hidden
assumption is that the workbook is sort
of there there are different guys some
we should try to do as the go as fast as
they can on single you know CSEC one
transaction is some other guys are
trying to go as fast as they can yes
distributed ones yes that is the case of
thing this holds otherwise um you know
you're not meant in the ratio yeah okay
I guess so in this particular mix you
have ten percent of the Cross partitions
yes in ninety percent within a partition
right ah yes so that means that if you
just sort of did a very simple
optimistic concurrency control thing
here you would commit ninety percent of
your transactions since you have
serialized access for those and they
remain on a partition now of course you
know you potentially could have a high
percentage of imports for for the things
that span multiple partitions do you
have a sense of of how how this compares
to that solution see your question
question your question is what did they
know you've delayed basement yet before
she was it I think yes and I even if
nagging is where you just use execute
the same partition guy without any
rewrites that says let it fly right and
then be able to transaction it'll try to
touch something it does have the lock
for you at the board and restart it or
need to remove it to another node
because you don't know when it comes in
what node you should I can run it on the
next time around you're taking
correct yes great so I'm saying suppose
you just never do that you just keep
trying to rerun it and maybe they even
start right you can we haven't said
anything about an SLA on on you know
being able to actually commit a
transaction that you try to do right
echoes cccc commit the ten percent of
the transactions and I think your
suggestion if you run optimistically you
can run very fast but yoga you know will
be so transaction single birkat risha
today welcomed me let me yeah and check
so how much do you have I'm in 10
minutes you have ten minutes yeah but
you know why this out words maybe let's
hold the boys to be right so if you
don't know this a story acts actually
been commercialized as both TV it's
actually another open source project
that's you know based on our code and
it's actually used in a couple hundred
installations throughout the world today
so the Japanese American Idol example I
talked about in the beginning that's
actually a real world deployment of the
system and actually found a few weeks
ago we apparently power the Canadian
version of american idol what you think
is just called Canadian Idol there's a
lot of people using the system to do
sort of network traffic monitoring and
particularly the government the CIA and
the NSA apparently love this system they
don't tell us what for obviously but
it's other than to say that its
installed in every single
telecommunication hub throughout the
country so that you make me feel warm
and fuzzy at night um lot of hot is why
PES guys yeah well you're you're Enzo is
it a run they apparently they run the
same amount of data through volte be as
they run through accumulo which is their
their version of big table of security
stuff we don't know what they're
actually doing with it a lot of high
frequency trading guys and high finance
guys are using the system because they
like the low latency properties in
high-throughput so we power the
Malaysian Malaysian derivatives market
runs entirely off of of the system and
then a lot of online game companies like
aol's gamecom use the system Anna's all
the runtime state of players and my
personal favorite is that there's a
shady offshore gambling website that I
can't mention that uses it all the
system for all their bets and wagers so
that's
so what I thought would say so I usually
started off by describing a system that
we've built that's really optimized from
the ground up to be support these types
of high-throughput transactional
workloads that we see a lot of but then
I showed how if you have just a small
fraction of the workload have to touch
data multiple partitions the performance
ability breaks down so nice showed how
to use problems probabilistic models we
want to predict behavior transactions
and you know do the correct
optimizations to lock the minimum
resources you need in the beginning and
then I showed how to extend these models
further to allow you to safely
interleave single partition transactions
whenever an engine is blocked because of
divisions action and so it's no longer a
question of can you scale up and get
better performance in a system well
maintained shins actions what I showed
today is exactly how to do that so where
do we go from here for future work I can
unleash Lee say that there is probably
five or six new projects we're working
on in the next couple years to extend
the performance of a show for a bunch of
different things so I'll loosely
categorize these into two two groups so
first we want to look at ways to improve
the scalability of the system so i'm
working with i think erin delmore who's
coming to do here a internship with you
guys this summer it's colleague at UCSB
we're looking at ways to have elastic
deployments of the system so right now
if you have a partition that's a hot
spot and all the transactions are
getting cute up there there's nothing we
can do at this point to be able to
mitigate that so we weren't looking at
ways to say to identify that we're
overloaded at a partition maybe it'll
split up into multiple pieces and
migrate the data or maybe coalesce
partitions that aren't being actively
used we're also looking at ways to
exploit new hardware that's coming out
and this is part of my involvement of
the big data ICC that's headquarters at
MIT that's sort of sponsored by the
Intel so in particular we're very
interested in looking at what kind of
database system you want we want to
build if you have some of the new
non-volatile memory devices that are
coming out so whether this is something
you something like H door or what you
want to try different currency role
models we won't explore that to figure
out what's the best kind of system to
have you have you know many storage
device that has the speed of region
rights of DRAM but the persistence even
SSD and then we're also looking at
exploiting sort of the new many court
architectures that are coming along and
then the thing that I'm probably the
most excited about that's coming out the
next year
which is sort of similar to your glacier
project here on the heck attend system
is a new system model that we've been
working on called auntie caching so in
aunty caching although I spent the last
hour talking about a data main memory
database system with auntie caching we
were to go ahead and add the disk back
in and so we're going to use the anti
cash as a place to store cold data right
potato we're not gonna need anymore so
we're going to monitor how much memory
is being used at a partition and then
when we reach above a certain threshold
or to go ahead and evict data that has
not been used recently or is unlikely to
be used in the future I want to store
that in a block storage hash table add
on disk and then we still have to
maintain index information on catalog
information in main memory for all the
data that we've evicted and so the main
difference between a traditional
database system they're not and the
inter cashman model is in traditional
system a single record kidada exists on
disk or in memory at the same time and
so in our model it's either one or the
other so now when a transaction request
comes along if it only needs to touch
data that's entirely in main memory then
excuse just like before without any
problems but as soon as it tries to
touch data that's been evicted or to go
ahead and put it into a Mont special
monitoring mode where we keep track of
all the records to try to touch that are
out in the anti cash and then when it
tries actually do something with that
data either modify it or send it back to
the application we're going to abort it
rollback any changes that have had in a
separate thread in a synchronously we're
gonna go fetch in the blocks that it
needs to get the data from merge that
into main memory all the while we're
still going to X cute other transactions
not block them and then once we know
that I date have been merged in and we
update our indexes and we can now
restart this transaction and it runs
just like before so again the key thing
about this is now we're able to support
databases that are larger than the
amount of memory on a single machine and
so we've done some additional
experiments where we taking teepsy see
this time with han % single partition
transactions for 10k gigabyte database
we want to scale how much memory were
going to give the system and we see when
we use my sequel the performance isn't
so great using my seagull with memcache
for certain read operations the
performance isn't that much better
because tpc really can't take advantage
of sort of a caching properties or the
read/write properties you get from
memcache but now when we use a store you
see that performance does not degrade
that much for databases that are eight
times the amount of memory owners that
available to the system at a single node
so this is pretty exciting work here the
other stuff we're looking to do is we
want to expand the type of workloads
beer on Iran on a straw system so we're
with a colleague at Brown we're looking
how to integrate stream processing
primitives or continuous queries as a
first-class entity directly in the
system so we have a mix of streaming
data and transactional data and sort of
intertwine them together and then sort
of related to what all the discussion
was produced ribbons actions we want to
look at how can we improve the
performance of system for workloads that
are not easily portable so if you add
more distributions actions how can we
make the system before better as it is
doing when due to things like batching
and other techniques to amortize the
cost of acquiring these locks at all
these different partitions I'm also
looking getting involved back into
getting back involved in working with
scientists do apply database techniques
for x tific workloads so this is
possibly building a whole separate
system that's really optimized for their
types of workloads because there's a lot
of them are sort of running these is for
transcripts or see scripts for mpi
programs that don't take advantage of
the kind of things that we know about in
the database world and then lastly the
thing that the the next major trend I
see which I'm sure you guys are aware of
here is adding support for these
front-end main memory systems to do
real-time analytical operations so now
we want to be able to do the longer
running queries that have to touch more
data without having to run that is just
as a distributor ins action that needs
to slow down the entire system so a lot
of the game companies care about this
kind of thing they have the front end
system is maintaining all the runtime
state of players and they want able to
say here's the top 10 player top 10
players were bunch of different metrics
so instead of shoving that off into your
back-end system her dupe or whatever
running the analytical query there and
then shoving it forward we want to be
able do that directly in the front end
and so to do this we want apply
techniques like using hybrid storage
models and hybrid nodes and applying
techniques like relax consistency
guarantees of these analytical queries
these aren't new techniques but we want
to sort of come up with clever ways to
combine them together and again other
leverage other machine learning
techniques to make the system do this
automatically because that's really the
main takeaway for what my research is
all about is really having the database
system know as much as possible about
what the application is trying to do
with the
work leaders trying to do and be able to
the system have this system go faster
and get better performance to cuspid so
I thank you everyone coming today I have
to ask you any more questions and
there's also a website set up for the HR
system where you know more information
about the documentation about all the
things that I talked about today so
thank you you measured the JPC yeah so
yes so TBC e won't work well on this
you're absolutely right um I will say
though from the vote to be guys what
they tell us is a lot of the workload is
very very few workloads that they see
our complex of TPC he t PCC is actually
very representative of what's out there
terms of the size and complexity but
you're right TBC II won't work well on
this system and that's one of things we
look at the future yes so TPC see was
designed as a benchmark it's got its got
you know uniformly distributed data it's
carefully designed yes which is not true
in most cases yes so petitioning that
will might result in pots possible yes
the other thing is that it's designed
for the Oracle in mind which is to say
it's also designs that it runs just fine
using snapshot isolation yes so T PCC is
you know interesting benchmark but it
certainly isn't conclusive in terms of
so we you know we done I didn't talk
about that we've done other experiments
I mean with Carlo and I here we have a
whole benchmark suite with a bunch of
different OTP workloads if we tried it
out and again key PCE excellent example
of something that won't work well in
this anything like it looks like a
social graph Twitter Facebook best off
that won't work well on this but again I
would say from what I've seen in
industry working with the ball TV guys
KPCC is actually very representative for
large number of applications that simply
it doesn't you know they're not getting
the points they want using x tan or
other things and they're switching to
something like bowl TV
alright thanks guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>