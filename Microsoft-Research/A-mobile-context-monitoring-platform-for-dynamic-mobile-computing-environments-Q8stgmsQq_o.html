<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A mobile context monitoring platform for dynamic mobile computing environments | Coder Coacher - Coaching Coders</title><meta content="A mobile context monitoring platform for dynamic mobile computing environments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A mobile context monitoring platform for dynamic mobile computing environments</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q8stgmsQq_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay we'll get started now it's my
pleasure to have professor John Mayer
song from heist crisis in de Jong in in
Korea and a professor song actually
spent a number of years before he was at
Christ at IBM TJ watson and he's going
to be telling us today about some
interesting applications that he's built
context applications using wearable
sensors and sensors in the environment
thanks so parts of the word is quite
getting old but I am packing up to
present past potentially everything of
what I have done I've been doing so far
so it started around near to 2006's six
or seven and is continuing until now so
so we have been developing a platform
for mobile platform to support
context-aware applications context of
where applications is it's good and I
believe it's a future direction of
mobile applications however the problem
is that it's difficult to develop okay
first of all is complex okay sensing
feature extraction recognition and all
those things the problem here is that
usually I believe average developers are
not good at those I mean it's to be
those who know some about machine
learning or pattern recognition things
so it's not common yet that's once again
nothing problem is it's computationally
heavy rain in the third it's if it is
mobile and then it also if we use a very
limited sense devices and resource and
energy limited so we have two problems
complexity in the logic and complexity
in the environment so we want you to
provide them developers with platform
that include effective runtime
environment as well and is to use api's
and I recently saw that a presentation
from ms Allah as well somewhat related
in in this year's census I know I was
also there to present different thing so
the key building block of context-aware
application is porous context monitoring
this is common to many different
applications so so it is good to know
that it is key key building block away
variants again complex multi-step
continuous processing and you should go
with multiple sensors multiple devices
to get intelligence so we use built-in
sensors and we use body sensors we also
use spade embedded sensors to get the
most of of what we what we critics and
extract so forget the problems the
challenge is here yes so Christian area
network so that's what I'm saying it
includes built-in fallen sensors and on
body sensors and personal area sensors
ok so we support dynamic system to to
get connection to the sensors I mean in
those three steps in so so here is the
phone and the sensors and applications
so this is a reasonably good system
however it is too small it will get
better and we have in the number of
sensors we can use a number of sensors
and and we can also we should also
support number of application here one
big difference is that again we should
not only support application by
application if it is platform and then
you should support multiple applications
at the same time if you live right so
that should be the platform so if we
think about multiple applications at the
same time than then here
in fact scalability problem occurs
because the system is rather small scale
that the hardware is rather small so so
even if is not that serious system as in
what we see in the internet still
considering the resources here is pipe
is getting quite complex further problem
is then sensing should be corrected or
Kirk continuously and this old
computation swallowing should occur
continuously I think it will complicate
the problem further so we have naturally
scalability problem considering the
resources and energy problem and also
once we say that multiple applications
run at the same time we should do
research coordination resource
management because so the current way of
developing applications using this kind
of sensors is it's rather odd hug it's
done from the application layer so we
are providing a pH to applications so so
that the application developers just you
use that high level API and don't care
about what's going on in underneath and
also the system provides abstraction to
to the hardware sensor devices and in
the middle it does you should do a bunch
of things so these two things are what
we what I will talk little bit about
this but rather and the second i planned
to talk each of those a bit by bit but
it don't think i will have enough time
i'll probably stop from there ok so C
Mon so again we provide very high level
I will go very fast about this but the
users are given like curly length type
of in
phase I mean the developers so that they
can use this did this this for to
specify their they're requesting in in a
decorative way and using high level of
presentation so context is for example
represented in this way activity is
running and and temperature is hard and
humidity sweat and it this one says
alarm from false to true so it means
that they want to know that the problem
wants to know if this this condition is
is getting satisfied so it wants to know
the change not the the context value
itself because it is important because
it's rather event based systems because
I mean so what what the application
usually do is not to know I mean it's
more to know the changes so that it can
trigger the services right so so we
included it as a part of the language
and you're raising speech file is the
time so given this high level
representation we do translation so this
is the deconstruction in the system
during the translation we have context
translation maps of all all these
provides the mappings of the result is
something like accelerometer one y value
and why energy value is something larger
than 52 and echo me the three XD sees
less than 500 cetera okay so the good
thing about this is once we have this
representation this one has lower level
medium level representation for example
it has met we mapped the high level
representation toward those including
the resource information here right so
we have sensory information here so from
this we can do system optimization to to
some extent so it's just like compiler
we use and so one example we do is that
from that medium level or presentation
we can
extract what that really the information
the sensors we should use and what
adults we don't have to use okay so here
we did things like short-circuit
evaluation so that if we are given to
given this kind of curious like is the
weather hot and humid then and let's say
we know that it's already hot or humid I
mean I'm sorry the other way if it is
cold then we don't have to know the
other one right so we in that way we can
identify what can't what are the sensors
we don't have to use a so so we do
evaluation on those so so what what we
do is that less given a false context
look a complex false context then we can
identify what makes them false right and
then we don't have to worry about
although hold the other things until
something comes to true right no no no
video I don't I to be honest I think
video is kind different wells a study
type of sensor and so we can do to some
extent if we really go dig into the
different parts of the video encoders
and decoders me but I'd rather say is
usually if you considered as candy
system for me so it's rather i mean i
would do exclude video but i do some of
the sound but in that sense in the sound
as well if we do the regular sound
recognition i would not be very much
interested because again it's it's very
very well understood process however if
we use part of it for example let's say
if we want to understand who is there
there man and or female or there
how many people are there though all
those things are variations of those and
then we can use part of religious I mean
this kind of idea so the reason that I'm
what that means I mean the reason I'm
saying that is that whatever I do it
there will be better ways of of doing it
which has been already developed if it
is the regular processing okay did you
guys reverse engineer
yeah yeah we did they did yeah but yes I
mean it's not easy it takes a lot of
time for students however it's
relatively easier than doing that with
video that's what anything so anyway I
will just go briefly here the high-level
idea is that once we know this is false
and and we don't have to count on all
the sensors we can extract the number of
sense the sensors which we don't have to
worry right so that's one idea so here
we could save about half of the sensors
and the reason for that is think about
the context the situation let's say
there are tens of sensors a question
soon from the applications then how many
of them will be true at one moment I
think it should be just small we can't
for example let's say there are tons of
questions about the locations for like
ten ten different location-based
applications and you can ask him if I'm
in this room if I'm in this building if
I'm in this area all the front different
kind of pressing send and also that
there could be many many ones but but I
not too many of them will be true
because I'm in a certain situation me so
many of those will be false so even if
there are a number of contexts
monitoring request is the number of
those who which is true at the moment
will be limited right so this heuristic
really worked out and we could save a
lot and the second idea the second idea
we developed what so here we are dealing
with multiple questions at the same time
right so we we could do optimization
considering all of them together so we
developed a shared processor as well as
incremental processor
from from some heuristics of the of the
of the requests so again this helped us
to improve the performance at the
minimum like three or four times in the
CPU time wise so those were the first
time the ideas we develop which has been
quite old now now once we had that the
second question is again if we run
multiple applications than at the same
time then then we should do further
let's look into a little bit more about
environment so here we have a number of
applications which share scarce
resources of the system and so for
example this mikaze mode which is quite
old now but anyway that can run even
less than one fft even if it is
lightweight right so so it's very
limited and we are considering running
multiple applications problematically
and also it's dynamic users are moving
around so we should identify sensors at
runtime and dynamically and connect to
it and applications will come and gone
right is the one is that resource
scarcity and energy scarcity and and
also we have dynamics of the system so
applications cannot do it see it's not
the metra of difficulty to run at the
same time together sharing the system
applications you to know what others are
doing so it should we should provide
system support me but the problem was
that in this kind of environment we
didn't have the kind of support so with
this design the system to help me so
briefly sing again so we so current
system everything was done in the
application
they specified low-level resource status
or resource requirement from the
application layer and the system just
receives in and do it if you can do
right so here the system specifies again
the high level web in high level I mean
the application specifies the request in
high-level any and the system receives
it and translates it investigated and
analyze it and identify what what are
the resource requirement and he also
understands what other applications are
doing and what the system has now and
and does the research binding in at
runtime dynamically so so the system
should provide holistic view of the
applications and resources so the key
idea here is that so we have we have the
we are we are providing high level
representation I mean api's to to the
users example if if somebody's running
and there are many different ways to to
identify that fact I mean there are two
levels one is the sensor level we can
use different set of sensors to identify
if the user is running and the second is
we can also use different logics 2448
way so we use that alternative resource
usage and logics to provide different
plans to the system right and so sitting
in between the applications and
understand the resources we the
orchestrator has the holistic view of
the system and understanding on the
system is as well as the application and
use that flexibility to orchestrate this
whole system sample again is that so we
have let's say have application a and b
c
application is is translated and the
system prepares students to different
plans application be is translated into
two different plans and see two three
among those seven so for each plans we
have so this plan be one uses
accelerometer and the wrist and it also
in in more detail it also has the
different processing methods so so in
this case it uses frequency domain
feature extractor and decision tree and
all these are done and the mobile side
in this plane and in this plan plan B to
it uses extra meter attached to the belt
in this case and uses statistical
feature and a decision tree again
however in this case the processing and
this part is done under sensor and the
mobile side is only the classification
is done okay so this way we can select
different note different processing
method also different use different part
do the computation on different parts of
the system so also we have research
demand map okay analyzing the plans we
prepare the resource plans and the
demands to CPU and bandwidth and energy
et cetera and the system in the
background prepares the availability map
I monitoring basically the system ganda
undergoes continuous system monitoring
to to abstract what's available now so
it has CPU memory bandwidth and energy
in this case case and comparing the
different plans against what is
available now we can select what a
possible for example we can have
this one is is available according to
the what we have and the resource ID but
probably this is not we further
undeveloped and a framework to to define
and enforce different policies for
example we can the one Palace can be
maximizing the number of concurrent
application okay in that case we been
translate that the policy in force it in
on top of this constraint matching okay
another can be optimized the research
other the energy usage okay so we can
say then that the amount of energy to
the minimum so we can do all different
kind of things to include are using the
yes we wanna be to those remove provided
by its picture or repaired by the system
the system from the query single query
being right it's right it right if you
remember the transit context translation
map that's prepared by the system so
that should be configured by external
expertise ready so eventually what we
should do is to have a kind of ontology
thing which probably exists in the
internet so that we can collect all the
knowledge is from different people so so
that it can be imported to the system
and use that as a as an extension of the
map so this shows you the overall
architecture of the orchestrator so we
have api is an application broker and we
have a planning process processing
planning part we have resource monitor
and which monitor system resources in
the background and we have polish
manager plan generator implant selector
this is the part for the processing
feature extraction
and recognizer and in the sense of part
those sensor broke apart is the part way
to communicate with the sensor also in
the census ID we have we use tiny wedge
in this version and on top of tiny waves
we have resource monitor so the sensor
itself monitors resources including CPU
and memory and also energy and
communicates through the mobile broker
with the mobile side orchestrator so
that the data okay straighter can can do
the orchestration II also the process
for a plan processor on the sensor side
makes it possible to participate in the
processing part if it is requested as
such for the communication protocols we
have developed a pseudo protocols
including sensor detection protocol
sends a reporting protocol and
controlling protocol and data reporting
protocols okay so so this provides
infrastructure to develop a kind of
micro distribute system around the
mobile device that dynamically so we
tested with with about less than around
10 sensors many of them on body some in
the environment I will just go very
briefly so to show the performance of
the system this was different from just
simply showing the throughput or
response time because so we should have
shot we should have shown it works under
different environment in the dynamic in
changing environment so what my student
did was to to stimulate the changing
environment what they did was that they
they divided a timeline to 24 different
faces and in the first phase they
controlled the number of available
sensors to throw 22 and in the second
phase they controlled it to two to four
and six and four to six and six and
eight and again every probably 2.5
minute they changed the number so every
25 minute they increased one or
decreased one but the total number was
controlled in in this range okay and
also they did simulating with number of
requests from the applications so they
controlled in different places between
the number of curious between 0 5 5 to
10 10 to 15 and 15 20 right and that
shows that workload so it shows the
result so this is a phase a a B and C
and D so in faith a and B so here in
this case is the number of Curie was
fixed to 20 and we try to see a number
of activated curious a number of
activated sensors right and in phase and
a and B okay Strader supported much more
number of requests we're a little bit
less number of sensors me and in in
phase and C and D again it could feel up
I mean the older requests but the other
case was less than that and still the
number of sensors was very controlled
very tightly okay so so an average
energy consumption was because it used
less sensor was controlled a little bit
lower but number of active Curie's that
was almost double
and similar thing was observed when we
controlled when you fixed the number of
sensors 26 and looked at the same thing
and again what's confusing here is that
in this case this color shows our system
performance okay so it's a again about
half of the sensors even if we have six
we control it less and show the
performance of the supported fury is
similar so that's mean in a very high
level but with what we have in the
background we have an infrastructure for
monitoring the system and both sensor
parking and Adam and mobile part and we
also have sensor generation part sensor
detection part and policy all those
things a regular system thing and
actually it's getting larger system now
and we are extending still toward you
know into a larger scale this yeah yes
that's
I wanted to show video but I was given a
second part
so we have mobile device okay and it
should be really moved by support mobile
mobile ding right it's not because for
example let's assume that you are
running rest assume that you are coming
to work and you get a phone call and if
you stop and push out your phone and at
the minimum you should touch 5 times you
should push this button and touch
probably five or six times think about
your jogging and you are listening to
music and you want to skip to the next
music you should first to stop and touch
at mode at around ten times you don't
want to do it so the mobile device
should really support mobile situation
but it's not because it's a due to the
limited interfaces right so I know
Microsoft is doing gesture or voice to
interact with Mobile's without attention
much attention so this work is about
developing mobile gestural interaction
platform okay again it's a perk platform
so we want to support application
developers they don't want to spend time
in developing the detailed look complex
logics and care about the systems right
so so provided by providing this
platform week developers can develop
their own application I mean
interactions rather easily the problem
in that case was again energy and in
this case one of the problem was the
gesture recognition accuracy okay the
energy is natural in this case we
because we don't want hold the phone we
put in it into the pocket and we use
wrist watch style sensor node okay so
that requires more energy and what's
interesting is that a lot of people
worked on just a recognition however in
mobile scituate they say mobile but in
reality they did in nomadic situations
so they all the experiment was that then
you move and stop
and the experiment to do the gesture and
experiment then you move again right so
we wanted to do real mobile experiment
right so these are the two things we
have done so what we did was that one
thing for the energy problem what we did
was that we used collaborative
architecture we developed an
architecture where to device the sensor
in mobile node collaborate to save
energy and the second is the sensor node
itself uses two different since the gyro
and accelerometer but in a clever way so
that so here that these two sensors have
very different characteristic gyro it's
very good for its motion adjuster
recognition however it requires a lot of
energy so we instead of using the gyro
of the time we we have accelerometer in
the front okay this is chip but the
accuracy is bad in mobile situation okay
is it it is not robust to the mobility
errors right so what we did was we put
this one in front of that and made a
feedback loop between the two so in this
part it does the segmentation but in an
adaptive way by having this this control
close control loop okay so in that way
we can achieve the same accuracy level
of the the gyro provide and the energy
is saved much using this this is
accelerometer so what it does is gyro
makes the extra matter adaptive to the
noise of the mobility noise situation
okay so by that we could save the energy
under sensor node about 2.4 times okay
and the energy on the mobile side was
reduced about this much forty-three
percent one interesting thing here is
that the energy saving here is
is due to the segmentation done on the
sensor node okay the reason for that is
say segmentation identifies the
potential segments of gestures so what
it does is that rather than sending
older sense data to the mobile node it
sends chunk of data and the interesting
thing is that human gesture interaction
is sporadic you don't do the gesture
input all the time you do you do it for
a while and don't and do it for a while
and don't so that data changing behavior
is is chunked right so for the other
times the mobile device can go to a
lower power sleep mode more easily right
and that's the big issue in energy in
this environment so this could achieve a
lot of savings so in that way I mean I
should stop here so in that way we could
solve that problem of energy and the
accuracy as well but there is some some
more optimization which they've gone
through two to deal with the mobile
different mobility situations so here in
this experiment we did mobility
experiment understanding position and
running walking position and running
positioning and in the put in the
situation in a car so we picked up those
four cases as representative mobility
situation and and we could achieve about
59mm sony 96 point of a percent of
accuracy which is good enough in in this
kind of weather course level gesture
interactions but we should learn more 22
cope with rather final level ding but so
far we could just do this kind of remote
control type of thing like you run in
you have a phone call then you if you
want to receive the phone while you are
running you you do this gesture and you
if you don't want to you
receive the phone then do this if if you
do this then it's volume up and if you
do this name volume last I find him down
and skip and go back so this kind of
gesture could be successfully
implemented so we implemented an
application for mp3 player control while
in running or running or walking
situation but we are doing further
extending the system toward different
application and different things so
those are a bit of our mobile platform
to support developers and users as well
in two levels one is the context
monitoring level and another was an
interaction level okay so I will stop
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>