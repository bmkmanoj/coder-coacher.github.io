<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ushering in the Era of Programmable Hardware | Coder Coacher - Coaching Coders</title><meta content="Ushering in the Era of Programmable Hardware - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ushering in the Era of Programmable Hardware</b></h2><h5 class="post__date">2014-06-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/76f1QU0MOO8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we're here with Doug Berger Doug
tell us a little bit about who you are
and what you do at Microsoft I'm a
researcher in Microsoft research
technologies division the new division
that was just formed so I'm a processor
architect by training I was at the
University of Texas on the faculty for
10 years before moving to Microsoft in
2008 where I built advanced processors
and new architectures and how did you
get into computing and programming what
was your first computer well when I was
seven and eight nine ish I my dad was a
professor at Smith College and he
brought home a CRT monitor and a modem
where you plug the handset into the the
foam pads and we could connect to the
the VAX at UMass University of
Massachusetts at Amherst and so we
hooked up and I started playing games
and you know a Colossal Cave adventure
and then started writing programs so
tell me more about your team at MSR
there's some work that you've done
around data centers and that's what we
want to focus on today tell us about
that work and and what what you've done
well the first thing I have to say is
that I'm almost embarrassed to be
interviewed about this work because so
much of the innovation and the ideas and
the hard sweat was done by my team the
team is it's one of the best teams in
the hardware business the the set of
people is absolutely amazing
they really are a ninja team and they
you know we all are on this shared
mission together so they work really
hard they're brilliant they're much
smarter than I am you know there's just
there's a phenomenal team and and I know
we're going to talk about a little bit
today but what they've pulled off is
amazing it's really disruptive and it's
a testament to the team that they were
able to succeed and how does that impact
felt around around the industry let's
let's talk a little bit about what what
you've worked on here what what is the
impact that it's made it's a platform
for accelerating data center services
and so up until now
data centers have been
typically built with servers with
processors CPUs and memory and network
and flash and disks there haven't been a
lot of specialized accelerators put in
because you really want you really want
all of the servers in your data center
to be as similar as possible yeah and
Microsoft has moved in that direction so
that your services can run across you
know many servers as opposed to being
stuck in one corner you know if you if
that service grows faster than you
anticipate and does the the server's
aren't the same you're you're kind of
stuck or you might buy too many servers
and so what we've done is created a
programmable hardware platform based on
FPGAs and put it into a large number of
servers as a pilot and showed that yes
you can actually do this you can put
these hardware accelerators in and and
accelerate many diverse services with
them so let's start there what is an
FPGA for someone who doesn't know well
so enough probably be a lot of people so
in FPGA it stands for field programmable
gate array and you can think of it as a
chip that can be where the the the
hardware can be changed on the fly in a
few seconds typically and the core idea
is rather than burning in the circuits
as transistors with polygons you know if
you want to make say a a chain of and
gates yeah on a chip you know typically
what you'll do is you'll create a mask
and you use photolithography light to
etch down you know some polygons on the
chip that form those and gates out of
transistors and it's a one time and
that's one time endeavour you you you
burn it and and it's there and it's
never going to change
what an FPGA does is they put a lot of
little little memory blocks on the chip
that are all networked together and
those memory ball those memory blocks
will hold a little bit of state that
allows you to put in some inputs and the
state says what's the logic function in
them and then the outputs represent that
logic function okay so I program the
ands and ORS in that little table by
setting the bits in it appropriately so
that I get the right output since it's
programmable logic and for these chips
do you program them for the data center
or they per application so they're per
application what what FPGAs have
traditionally been used for is
prototyping Asics or custom chips so you
have a design you do the design
you want to test it out an FPGA first to
make sure it's right before you go and
spend millions of dollars to have the
chip manufactured yeah and you know they
run much faster than software because
they really are you know they really are
running in hardware I mean the look-up
tables are slower than raw gates but
only about 10x you know software is one
hundred to a thousand X slower yeah to
simulate a chip and so you can run very
rapidly and then FPGAs as they've gotten
larger and larger they've been used in
more and more things they're used
heavily a networking gear telecom gear
network switches but until now they
haven't been widely used in the data
center and what is the problem that you
were trying to solve with the data
center is it a speed issue or are you
trying to speed things up in the data
center or what what solution does this
solve well its speed power and cost so
what we've seen over the past few years
is that the performance of individual
microprocessors especially server
processors but also in the clients as
well has started to cap off you know we
don't see bak rates growing very much
faster anymore the chips aren't getting
nearly as energy-efficient per
transistor as they used to this is all
the challenges we face with silicon
scaling yeah and you know it's my belief
and and some of my colleagues share my
view that the number of cores that we're
starting to see you know the growth in
the rate of core counts is starting to
slow down - or yeah imminently will be
slowing down so this is a curve of
Moore's law yes you're saying that
Moore's law may not be the constant that
it had been believed to be right and
that it may plateau in the near future
right so and what does that mean for
computing well Moore's law was defined
by Gordon Moore in a publication the
really short one you know three or four
pages in 1965 and he really talked about
the number of transistors per chip was
gonna double every 18 months to two
years yeah and that really held true for
50 years it's amazing a 50-year
exponential is a long time
yeah for doubling every you know within
within a couple of years now there was
something else called Dennard scaling
that Bob Denard who was the inventor of
DRAM defined in a paper in 1974 from IBM
and that he gave the rules for how you
shrink transistors on a regular cadence
yeah and Dennard scaling the the way he
specified it was such that each time you
shrink the transistors you shrink the
channel length the
gate oxide with the channel doping
concentration the doping atoms in there
the voltage all of it would be scaled
proportionally and when you did that
you'd get a reduction in power the
transistor would go much faster and it
would be much smaller so it was like
magic
yeah and that we ran for 40 years
Dennard scaling failed around the mid
2000s 2005 because the the insulator on
the transistor started to get so thin
that you couldn't shrink it anymore
without lots of electrons flowing across
it leaking yeah and so that that that's
really why we saw this this big pivot
from frequency scaling to multi-core and
so now we've we've had these stresses
and this power crisis grow and grow now
we're starting to see real challenges on
Moore's Law itself and you know the
transistors used to get cheaper because
they'd be doubling for the same every
generation and for effectively the same
price yeah and we're now starting to see
that flatten out as well yeah so
sometime between now and I'd say the
next five years mm-hmm we're gonna hit
an economic end to Moore's law and
transistors are going to stop getting
cheaper and at some point it won't be
economic to make smaller transistors and
then we'll see a hard stop and the
implications for the industry are really
really strong and talk about the
implications what does that mean that
Hardware just won't come down in price
like it have in the past
well it certainly won't come down as
fast but what it means is if you're if
you're a semiconductor manufacturer and
you've been offering a chip a product
that every generation you double the
number of transistors which allows you
to add a lot more functionality and run
the thing at the same power despite the
doubling of transistors and the extra
functionality and sell it for say $100
now if you want to double the
transistors if the transistor cost is
flattened you'll have to sell it for
$200 yeah and that really breaks the
economics of a lot of these businesses
and it will really change people's
business models and so we will see we're
already seeing that pressure you know
we're already seeing the the cost
improvements flattened out and so we're
starting to see pricing pressure but
that pressure hasn't gotten strong
enough to terminate the silicon scaling
yeah I mean once that terminates I think
if you're if you're a silicon
manufacturer you have to move to a
different business model for the most
part
so talk about the impact that the FPGA
will have on Moore's Law is it something
that will extend this a little bit it
gives us a little bit of breathing room
in the meantime I would say that that
you know an FPGA can accelerate software
when you move something in and if it's
an amenable workload by about a hundred
x yeah a six you know typically a
thousand x in terms of performance per
Joule at PJ's are about a hundred x and
so I mean what that means is that if we
are able to deploy this programmable
Hardware in our cloud we will be able to
start moving workloads over yeah now I
should emphasize here that this is still
a research project sure you know this is
I think our goal as researchers is to
try to shift the industry but but when
you have a lot of programmable logic in
your cloud it's a target where you can
start migrating services over each time
you do you might get 10x 20x to knit
some more get 100x and as you move more
and more services over you get more and
more efficiency so even if Moore's law
stops or even if even if Moore's law
keeps going but for cost reasons our
CPUs aren't getting that much faster
yeah ok this gives us a scaling path
yeah for some number of years we can
keep improving the performance of our
services we can keep improving the
quality of our services running at
larger scale we can improve the
efficiency of our services the cost of
our services well past the end of
Moore's law yeah so let's talk about
some of the collaboration you've had
with the Bing team so when we started
this project back in 2000 early 2011 Jim
lares and I and jimson's left Microsoft
to take a prestigious position elsewhere
and Andrew Putnam who's on my team now
and is our Hardware guru got together
and we said let's let's think about what
we could do to really change the you
know change the game for Bing
give them a big jump in efficiency for
for search and so we met with being
leadership in January of that year and
they said this looks really interesting
could be disruptive I mean it's a little
crazy but you know programmable hardware
in the cloud yeah but it seems like a an
interesting opportunity and if you can
get the efficiencies that are
possible it would be really great for
them yeah really great for us from and
so we started building a prototype and I
have to say Bing throughout this entire
process has been simply wonderful
I mean they've given us a lot of
consulting early on you know giving sort
of guiding us on what would be feasible
for them design requirements they put in
a lot of time later on they started
partnering with us you know being
software engineers spent a lot of time
working through the interfaces the api's
they hired an amazing guy named Eric Chu
who is the lead Hardware leads a
Hardware team in Bing to support this
effort and and so they've really put a
lot of skin in the game and been
partners and I think Saddam Lanka who
runs the indexer platform team has been
simply amazing I mean he's a brilliant
engineer he's open-minded he's willing
to take risks but he's conservative
because he has to deliver you know just
the sort of person you really want to
have working in Microsoft and so the
Bing team throughout this has been I
think conservative because they have to
deliver working solutions sure but
aggressive and that they're open-minded
and willing to try try really
interesting things and it's just it's
phenomenal to see to see how you can
balance that aggressiveness and
conservatism and really try to jump out
into a technology leadership position
while ensuring that your stuff will work
at scale and what does catapult afford
the big development team that they
couldn't do before well it really does
two things so in the ranking engine we
moved I would say a significant portion
of the compute heavy part of page
ranking which is for search engines is
you know when you have a document or a
query and you want to decide which
documents you return in what order
mm-hmm you'll cite you'll take some
subset of them that look like the
promising candidates you'll rank them
you'll sign the meet you score then you
sort the order and that's the sort that
lists and that's the order in which you
return them and that is an enormous ly
computationally heavy task I mean both
both our competitors and Bing do massive
amounts of
mutation on each page to figure out is
it a good response for this query and so
by moving most of that into FPGAs bing
has shown that they're able to run
larger models potentially within the
time they have to score the documents
and they can also run higher throughput
so they can do this with more servers
mm-hmm so it saves big money they can
get potentially better results it saves
energy you know better for the
environment yeah you know at at we think
affordable cost and what are the next
steps for catapult so Bing has worked
with us on this pilot you know we deploy
we brought up their ranking engine on
our Hardware on a bed of 1,600 servers
actually 1632 serves to be precise so we
had the hard Hardware manufactured we
deployed it in that bed of servers and
then we worked as all custom hardware
that you were working with well we don't
custom hardware put it in our servers so
it was designed to be compatible and fit
in there cleanly yeah we had a little
tight spot in the back to fit it mm-hmm
the servers are very dense and then we
worked closely with Bing to bring up
their their software stack on these
servers offloading the part of ranking
that we had ported on the program
hardware yeah and so the results we
report in the paper are that we just
about double the throughput by adding
this accelerator with the potential to
run better models so so right off the
bat you can run with half the number of
servers and we think there's a lot more
gains to be had so if someone wanted to
get into this area working with FPGAs
what what language do you typically use
and what are the the technologies behind
it that people should look into so right
now today much of the FPGA programming
is done with with things that are called
HDLs for hardware description languages
these are VHDL or Verilog and they're
they're basically concurrent programming
languages where every function is being
evaluated all the time with regular
synchronization intervals which are the
clocks on the gem determining ok now
it's time to do the next evaluation so
you can think of these as large numbers
of concurrent modules with well-defined
communication interfaces FIFO x' latches
between them yeah and so every every
cycle of the clock you evaluate all of
the law
so it's very it's very much a
fine-grained parallel programming
language and then increasingly because
these chips are getting more and more
pervasive in in many sectors people are
writing stacks now that will target
domain-specific languages or you know or
concurrent programming languages more at
the software level that can be
automatically translated down to the
hardware description languages that's
great well I look forward to what you
have coming up next wonderful thank you
so much thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>