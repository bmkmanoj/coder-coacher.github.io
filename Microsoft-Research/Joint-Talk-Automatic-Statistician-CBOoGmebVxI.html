<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Joint Talk: Automatic Statistician | Coder Coacher - Coaching Coders</title><meta content="Joint Talk: Automatic Statistician - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Joint Talk: Automatic Statistician</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CBOoGmebVxI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
there seems to be some silence so i
think i can start so i'm going to talk
about the automatic statistician and in
fact James and I are going to have have
merged our talks so we're going to
switch at one point and then I'm going
to come back at the end this is a
project I'm hugely excited about this is
joint work with James who you'll see in
a minute David do vinho Roger gross and
Joshua Tenenbaum Roger and Josh are at
MIT so let me first start by motivating
it we all know that there's a growing
need for data analysis so we live in an
era of abundant data data is everywhere
everybody's talking about it you know
the the buzz phrase big data is is sort
of kind of annoyingly in every paper
that you read but there's some truth
behind all this in that it's really
transforming the way that science works
is transforming the way the society
works and it's transforming a lot of
businesses so in a widely quoted
McKinsey Global is to claim the united
states alone faces a shortage of 140,000
290,000 james and i laugh about how they
come up with these numbers but you know
there is a massive shortage of people
with analytical skills that can analyze
this kind of data and a lot of fields
are relying on expert statisticians we
can call them statisticians or machine
learning researchers or data scientists
if you want but these are people who you
know have become essential to the
computational sciences businesses like
online advertising rely on statistical
modeling quantitative finance and list
just goes on and on so what do we do we
need a lot of data analysis we have a
lot of data we don't have enough experts
out there so let's build an automatic
that efficient all right so the basic
idea here is first of all let's think
about
perhaps a caricature of what a
statistician or data scientists might do
with a particular modeling problem so
first of all you need to have some some
data and this statistician would
presumably also have some space of
possible models that they're willing to
consider things that they've learned
about in graduate school or things that
they're familiar with things that they
have software to work with some
representation of possible models for
this data and we're actually going to be
talking about a language of models and
you'll understand a little better what
we mean by that but this is just a
representation of an open-ended space of
possible models that's generated through
a compositional grammar of models okay
so you have the data you have a space of
possible models and usually what you
want from a statistician is to be able
to find you a good model so there's a
process of searching over models and
during that search you're evaluating
different models using some principled
method and at the end of that search you
might output a model or perhaps a small
set of models but let's focus on a
single model and then what do you want
to do with that model well the
statistician might understand what that
model is but the the client the end user
probably doesn't understand what that
model is so the status statistician
needs some way of first of all being
able to make predictions that might be
useful for for the end user from that
model secondly the statistician might be
interested in checking whether that
model is sensible or not so there's an
aspect of model checking and finally
you'd like to be able to communicate
that model to the end user by
translating the model into some sort of
report in English so this is the big
picture view of what we'd like to
replicate in a fully automatic system
alright so so our goals are you could
think of as threefold so first of all we
want to provide some tools that are
useful for understanding data and the
require minimal user input secondly we'd
like to use this automatic statistician
problem to really drive a lot of our
research into challenging problems so
bye-bye even formulating this over the
last year and a half or so that we've
been working on this we've discovered
that there are a lot of elements in this
whole pipeline that require careful
thought and they've been driving our
research into things like automated
inference model construction and
comparison data visualization
interpretation and finally I think this
should be in general a good goal for the
field of machine learning to kind of
just automate all of this reasoning so
we think that this is a nice way of
driving machine learning forward rather
than working on small individual
problems that are very specialist and
are not really understandable by
non-experts okay so going back to this
flowchart here so what are the
ingredients that we need in this
automatic statistician I've kind of
hinted at these already so first of all
we want an open-ended language of models
we don't want to just have like you know
the five models that are implemented we
want to be able to have a language of
models this expressive enough to capture
complicated real-world phenomena and we
want that language to include a lot of
the standard things that statisticians
would would use but maybe go beyond that
into things that statisticians might not
think of composing together you know at
first thought but of course if we have
this open-ended language of models we
need to be able to search over that
language of models so there is an
important aspect of trying to do this
search efficiently and this is going to
be a huge space and we'll talk about
that in a while as well so search is
is an important component well what do
you do when you search well you need a
way of evaluating models and you need
kind of a principled and robust way of
evaluating models so a principled and
robust way that can trade off complexity
and fit to data because as you go
through this language of models you're
going to get more and more complex
models coming out so you need to trade
that off in a principled manner and then
finally to produce this end product here
we need a procedure to automatically
explain the models so verbalizing the
assumptions of the models explicitly in
a way that's intelligible by somebody
who's numerate but not a statistician or
machine learning researcher okay so
here's a preview of the kind of thing
that we've done this is basically taking
in some raw data which is a one
dimensional time series and coming up
with a model with predictions so this is
the full model posterior with both
interpolation and extrapolation this
data happens to be airline passengers
over a number of years and there's some
things in here that correspond to maybe
Christmas and other peak times of travel
and essentially what this automatic
statistician has done is produce an
interpretation in this text literally
but actually this is just the snippet of
a 15 page report that's produced the
beginning of the report says something
along the lines of you know I've
discovered that there for additive
components that have been identified in
the data a linearly increasing function
and approximately periodic function with
a period of 1 point 0 years and with
linearly increasing amplitude a smooth
function and uncorrelated noise linearly
increasing standard deviation because
that's the interpretation that's being
given okay so let me talk a little bit
about the language of models
that we're using so we're going to focus
on this so in particular although the
project is much broader and at the end
I'll talk about some of the other things
that we're trying to tackle right now
what we're focusing on and the work that
we're going to present today is on
learning functions okay so we want to
define a language for regression models
so regression consists of learning a
function f from some input to some
output Y from input-output pairs and our
language should include simple
parametric forms that are commonly used
things like Lynn ears polynomials
Exponential's etc but also properties of
functions like you know smoothness
periodicity and other things that you
can explain verbally but might not be
something that you want to write down
parametrically and we want inference to
be tractable for the models in our
language so the design choice that we
make is to use Gaussian process models
for regression so we can build these
regression models using Gaussian
processes which I think many of you
probably know here their distributions
over functions such that at any finite
number of points if you evaluate the
function values of that vector of finite
number of points the Joint Distribution
is Gaussian so that's just a definition
of a Gaussian process and like a
Gaussian distribution a Gaussian process
is completely parametrized by a mean
function which is just the expected
value of the function at any point and a
covariance or kernel function so this is
a function that takes two arguments
which is just the covariance between the
function value at X and the function
value at X at X Prime and you can think
of the mean function and the covariance
function as generalizations to function
space of the mean vector and covariance
matrix for a multivariate Gaussian so
then we just denote F being drawn from a
Gaussian process with me
function mu and covariance function K in
this way and this is just the depiction
of a Gaussian process defining prior or
functions and as you start observing
values of your function you get more
more sensible predictions of what's
going on with the error bars shrinking
around where you've observed our values
and being large where you have an
observed values ok so now gaussian
capacities are parametrized by a mean
function to covet and a covariance
function but it turns out that it's it's
very common practice to assume that the
mean is 0 and there's a good reason for
that and that is that if you have a
Gaussian process with a particular mean
function mu of X and a particular
covariance function k but you're somehow
uncertain about the amount of that mean
that you have if you marginalize out
over that uncertainty if there's a
Gaussian uncertainty about the amount of
the mean that you had then equivalently
you get a Gaussian process with mean 0
and a new covariance function so often
we can just marginalize out the unknown
mean function and get a new covariance
function and mean function of 0 so we
specifically are going to look at a
language for Gaussian process regression
models in terms of a language for
kernels or covariance functions so how
are we going to do that well the basic
idea in the language is going to be that
we have some atoms in some ways of
combining them so we're going to be able
to generate all sorts of complicated
kernels by combining a few base kernels
here there are five base kernels which
I'll explain in a little bit more detail
in in a minute these are the squared
exponential periodic linear constant and
white noise kernels and we're going to
be able to combine these into more
complicated kernels so what are these
things correspond to well the squared
exponential kernel corresponds to
smooth functions so these are two
samples drawn from gaussian processes
with that Colonel the periodic colonel
corresponds to periodic functions the
linear kernel corresponds to random
unknown linear functions constantly
kernel to constant functions of white
noise well white noise okay these are
the basic elements of our grammar and we
have two main composition rules addition
and multiplication so if you multiply a
linear and a linear kernel what you get
a quadratic kernel which gives you
random draws from quadratic functions so
our grammar includes all polynomials
therefore by composing these if you take
something like a squared exponential and
you multiply it by periodic what you get
is functions that are locally periodic
if you take a linear and you add it to a
periodic for example you get a periodic
plus some linear trend and squared
exponential plus periodic you get
periodic plus some smooth trend okay so
by combining these things we can get all
sorts of interesting things and one
important thing that we need in our
grammar is a way of encoding change
points especially for time series where
you might have is behavior function that
looks like something before some point
in time and then changes so for that we
can use the following simple property
which is that if F 1 is drawn from a
Gaussian process with Colonel k 1 and f
2 is drawn from gasping process with
Colonel k 2 then f of X which is just a
combination of f1 and f2 with 1 minus
the sigmoid and a sigmoid where sigmoid
is just on any function that goes
smoothly between 0 and 1 then this f is
also drawn from a Gaussian process with
a new kernel which is given by the
kernels for f1 and the kernels for f2
combined with these sigmoid functions
and this defines a new operator an
additional operator in our grammar which
is the change point operator so you can
say k is a change point of k1 and k2
with some parameters for when did the
change point occur so here we are
changing from a very slowly moving
function to a more rapidly moving
function here as well and you get you
know other kind of different things like
you're changing from a slowly moving
function to a periodic function here etc
so you can do all sorts of things like
that ok so combining these we end up
with a lot of the motifs that you would
find in standard statistics papers
things like gaussian process smoothing
linear regression things like multiple
colonel learning models like trans
cyclical irregular models Fourier
decomposition spar spectrum GPS changed
phones heteroscedasticity etc okay so
I'm going to pass it to James
right well thanks for much sleeping for
the intro to the project's ends and the
language of models so the second half of
this talk I'm going to be discussing how
we use this language of models to then
we then search through that language
trying to find an appropriate model for
a particular data set and then once
we've got that model going to show how
we sort of translate that to produce
predictions from that to beautiful
report sort of explains what and the
rest patterns have been discovered in a
data set so to recap we've defined a
language by the arbitrary composition of
five base Colonels and three operators
those being addition multiplication and
change points now this defines an
open-ended language and when these
expressions get quite large is this
language also has a very large branching
factor when you consider adding sort of
absorb multiplying very large
expressions with many sums and then
perhaps so that's going to require quite
a judicious search to sort of explore
this space and so in light of this we're
going to propose a greedy search firstly
because it's simple and also because it
has some some relations to human model
buildings such as the adding a kernel is
sort of similar to adding a component
for model like forward selection
strategies and multiplying by another
kernel is similar to modifying an
existing part of a model so i think
searches best explains in two pictures
so whether they search starts out is it
proposes all of the base kernels on
their own so apologies this is from an
earlier version of the language we have
some different base kernels but the same
principles apply so we propose all of
these the base kernels we then fit the
models corresponding to these kernels
and then evaluate them using a score
which I've discussed in a moment when we
do this we found in this particular
example that it's the RQ Colonel the
rational quadratic that scores best on
the data set shown on the left now this
is of course with one kernel we produce
a very simple model post data as we can
see it's interpolate in the data fairly
effectively but the extrapolation is
clearly showing that the model has not
captured any of the structure in that
data it doesn't pick up on the
periodicity it doesn't see the trends of
the data are in any sense but
fortunately the search continues and so
once and so in this search is greedy and
so once we've picked that rational
quadratic we then consider adding and
multiplying their rational quadratic by
any of the other based kernels so if we
try adding square exponential to it
periodic or we'll try multiplying it by
the periodic a colonel we then fits and
score all of those models and this
particular case it's the periodic plus
the rational quadratic that one and so
here we can see that it's an
extrapolation is picked up on the
periodicity of the data but it's still
is not really extrapolating that trend
very well but the search continues and
then this particular case discovers that
the best fitting model is that when we
multiply it by the whole expression by
the square exponential kernel and we can
now see that that's starting to get that
long term trends in the data and then
that search continues and so so the
difference there was quite small but
it's now sort of getting that trend even
sort barest of becoming but
extrapolations are looking more and more
plausible and that search then continues
until no improvements can be made or we
reach on some some others or stopping
conditions such as a maximum depth now
what I didn't tell you there is how
we're evaluating all those models so
whenever we propose a new kernel
expression there are some parameters
associated with that kernel which we
optimize using gradient methods and then
what we'd like to do is we'd we want to
compare all these models and we do that
using by computing the marginal
likelihood of the data given a
particular model which fortunately for
Gaussian processes for off any kernel
can be computed analytically however we
can't use the marginal likelihood
directly because we haven't done the
fully Bayesian thing because we've
optimized some of the parameters of our
model in particular we've optimized
those colonel parameters so to account
for this optimization we penalize the
marginal likelihood using the Bayesian
information criterion which is defined
here so we can see we've got the
marginal likelihood of the data given
the model but then we've penalized by a
factor that's proportional to the number
of optimized parameters okay
so that's how we do the search so that
search of runs we evaluate lots of
models for the greedy we get some model
outfits so now the fun part it's sort of
house we take that sort of abstract to
looking sort of Colonel expression which
I mean some of you may be familiar with
Gaussian process with single kernels but
what is it what's going on when we have
these big expressions so that brings us
on into how we translate these kernels
into natural language to give an
expression of for the description of
what's going on in the model so the
search can produce arbitrarily
complicated models from this open-ended
language but fortunately there are so
tues of key properties of Gaussian
process models that allow us to simplify
this task and so ultimately to automate
the description so the first one of
those is that these colonel expressions
can be decomposed into a sum of products
of kernels unfortunately a sum of
kernels thoroughly corresponds to some
of functions meaning that we can
describe each product or each function
in turn and then in each individual
product each kernel in that product has
a sort of consistent way in which it
modifies the resulting model so with one
kernel we know that that's often codes
for smooth functions periodic functions
linear functions but then when you
multiply it has a consistent effect on
the model which and since that effect is
consistent we can just roughly we can
translate each kernel in a product as a
sort of adjectives sort of modifying an
existing model but I'll show you a
worked example a second okay so first
how do we distribute how do we convert
these into a sum of products form so
let's suppose our search space out this
expression now what we first do is we
turn change points into summer products
notation now this so this bold signal is
just shorthand for the equations we
showed earlier with all those sigmoid so
those bold sigmas are in fact products
of sigmoid once we're in that form we
can just simply distribute
multiplication over addition in the
normal manner these are just functions
there's nothing sort of unusual going on
and finally there are a few
simplification rules that we can apply
to these expressions which makes things
easier for us later in particular the
square exponential multiplied by the
white noise kernel is in fact just
another white noise kernel with
different parameter
squared exponential x constants just
turns into it square exponential with
again different parameters so as you can
see we've got down to a sum of products
of kernels now as I mentioned before
some of kernels directly corresponds to
summer functions so so written
mathematically that's just if we have
some function f one with Colonel k 1
function f2 with Colonel k to
independently then their sum is again
the asking price is distributed with us
the sum of the kernels so this is going
to make up so as I said this going to
make us with task of describing the
models simpler and it also means that so
it has the other nice effects that we
can take us of complicated posterior
like this and decompose it into additive
components like this and now just from
these pictures you can see that this
sort of seemingly complicated function
has been to company composed into
visually simpler functions and that's
what's also going on at this with
translation level with decomposing into
simpler objects okay so now I need to
tell you how we describe a product of
kernels so when we have a kernel a zone
eg the periodic colonel we just
translate that into a soft standard
description of what that Colonel encodes
for so there's great exponential that's
smooth function for a periodic colonel
that's a periodic function and at the
bottom here I've shown some examples of
random samples from a GP with that
Colonel now when we multiply by a square
exponential kernel this has the effect
of removing long long-range correlations
from a model so something that was
periodic across all space is now
periodically in a sort of local in a
local region but far away function
values are no longer correlated and this
so in this particular case we sort of
choose to describe that as approximately
periodic and again we've got some
samples multiplying by the linear kernel
is easy to explain because that's
exactly equivalent to multiplying the
function by a linear function and that
has the effects that whilst the variance
of the model might change according to
the linear function the correlation
structure of the function doesn't change
in any way and so that means we can
ezell described it as perhaps like with
linearly growing amplitude in the case
of a periodic function again the samples
and finally the only other thing we have
to describe are these are bold Sigma's
relating to those change point operators
and they have the same form as linear
kernels allowing the same trigger sort
of interpretation as multiplication by a
function and so we can see here from the
samples that what's happening is it's
just switching off the function in
certain regions and so because we've
easily described that as of until a date
from a date or between two dates so now
i'm going to show you so now those
explains how we define our space of
models how we pick a model and how we to
describe it in natural language let's
see some of these reports generated on
some real data sets so here's the the
data showed in the preview this is
indeed international airline passenger
numbers in the 50s and the model has the
system has identified four components in
the model it's ultimately fit so let's
look at those in more detail so firstly
we've got a linear function which picks
up sort of the approximately linear
trend of this data now once it's modeled
that it then picks up on their sort of
on the periodicity of the data so
there's a colinear sure yes exactly yes
so I mean I hope you might agree that
maybe the first thing to do is to get
the trends out here so the way that
we're picking that once we get these
components the ordering is arbitrary but
in the expression we actually order them
by so computing a cross-validated mean
absolute error so we saw see how well
the components can predict the data we
use that as of a good good heuristic for
measuring such a firm so if what makes
what makes something more interesting
it's things that can predict so yep
following that heuristic for ordering
the pendants the second one that comes
out is the annual periodicity of the
data and it's able to remark on facts
such as it so if it picks up about the
period so it reports the periods of this
of the variations but it also points out
that piece of amplitude of this function
is increasing linearly so which indeed
we can see from the data that it is once
it's model those two things it finds a
very sort of small smooth function a
sort of a small amplitude small function
a smooth function describing the
deviations from that linear trend and
then finally it picks up on some the
heteroscedasticity in this model where
since all those are the values are
getting larger instead of using a sort
of just additive noise mop closet of a
home eschewed a stochastic noise model
it it picks up on the fact that the
variance of this noise is increasing
linearly okay and now a second data set
here we're showing soda a radiance data
derived from counts of sunspots since
about 1600 and so the first thing that
this so we ran them search on this and
the first thing it picks up on is that I
didn't normalize the data so it sounds
with the constant that goes through it
it then picks up using some change
points this unusual periods called the
man's a minimum between about 1645 and
1715 we're almost no sunspots were
recorded for over half a century once
it's done those things it can then model
the long-term trends outside of the
mounds of minimum and then finally the
last commit i'm showing here is it picks
up on the the solar cycles so they
roughly 11-year solar cycle and also the
service description has also picked up
on the fact that by looking at the
parameters of the colonel its learned it
can remark that the period the shape of
this function is approximately
sinusoidal of which i think of a good
description of the model its fits there
okay now just to show you these in
context of a show you these in the
context of a full report there we go
cool so all of those things would meet
yourself copying the lakers mostly
produced by the system that produces
these things so here we are applying it
to some data some cool sense of volume
data and I don't actually know thats of
the generative process that's going on
here but clearly something rather
interesting happens at this point no so
we run run system on this and it
identifies I think it's at seven there
seven or six I can't count from this
angle so
identifies these components and so first
thing it picks up on is fine sort of an
approximately linear trend going up
until this this point here then after
that it ends with models the function
separately after that point so it sort
of picked up on this change point in the
data where as we're going along with one
linear trend something happens but then
it carries on very smoothly thirdly then
swill picks up and soft smooth
deviations until this sir sort of smooth
deviations from the linear trends until
that point and sort of carries on and
one things actually quite like about
this particular this particular
decomposition is that it's chosen to so
something you can see that this is this
is a sharp change point but it's sort of
a little bit of turbulence on the way
down which it separates out is it sort
of his own compass special components
here so we could see that in the sort of
as his models being built up the
residuals here it sort of annoys roughly
similar but then there are sort of some
shocks here which it's separated out as
a specific region now this is what makes
these put reports 15 pages is that we
then sort of discuss also how that model
then extrapolate sort of discussing sort
of limitations of the model sort of how
trying to give you a sense of how for
how long you can actually believe these
extrapolations and something that's
working progress is that it didn't we
have this in our big diagram that didn't
explain it just yet is how we sort of
check whether or not these models are
any good and so here we're providing
some statistics derived from posterior
predictive checks whereas sort of some
fancy diagrams essentially saying sort
of the blue curves here are what we
expect to see from a model that's been
fitted than the green is n relating to
the data and we've tried to search for
deviations from these various statistics
looking at of auto auto correlations
periodic grams and QQ plots and that
sort of thing ultimately trying to sort
of check whether or not the data doesn't
seem to match our modeling assumptions
ok and now before I hand back to zoom in
to wrap things off so hopefully what
I've shown you is that these models
being produced by the system asked of
producing sort of faithful models to the
data that sort of captured the various
of patterns in there and then also
described those patterns so it's a
models are interpretable amount of are
also interpreting themselves but a good
check of course is just to see do these
models predict well can we use them for
extrapolation can we use them for
interpolation and so what we've got here
is box plot off standardized root mean
squared errors over 13 different time
series and similar to ones I've just
shown you the good news for us is that
sort of two variants of our method here
are falling pretty well I think what's
more interesting for me is that in
general as we go from right to left
these models of increasing in capacity
so for example the square exponential is
less complex than multiple colonel
learning which is less complex than
these models on the left here what's
interesting that is even on these very
simple data sets there's no real sign
that was of plateauing in performance
yet I think that's what kind of
surprising that there's suggesting that
there's actually a lot more structure
that we could be exploiting even in
these very simple time series data sets
so now I'm going to switch back to Z
good okay so so that's the overview of
one of the basic projects that we've
been doing in the automatic statistician
focusing on time series prediction and
it's already highlighted a few very
interesting challenges for a lot of the
other things that we're going to look at
so one of those challenges is this
tension between interpretability and
accuracy now if we want to be able to
produce a report about the data then we
need to focus on interpretability on the
other hand we might be able to squeeze
out more accuracy by adding in
components to the grammar so if we just
go back to this slide without going into
too many details
a good news for us is that even our
interpretable version of the model which
produced those reports actually does as
well or better than most of the
competing things out there including
some you know equation learning a sort
of standard commercial equation learning
package and various actually rather very
flexible models for a kernel learning um
but it does appear that we could squeeze
out more accuracy by adding more
elements to our kernel although then it
makes it harder to write a nice report
about that so that's the tension that I
think occurs in in real statisticians as
well then we're finding it in our
automatic statistician another challenge
is that we can have we can handle a lot
of interesting phenomena but clearly we
could actually increase our grammar our
language by adding new elements things
like monotonic functions positive
functions symmetries that it might
discover in functions etc and so that's
currently work in progress thinking
about how to add some of these things
and some of these things are not that
easy to do with gaussian processes so
that's why we didn't include them in the
first version here of course we're
searching over a very big space so there
is a you know an increasing
computational complexity when we look at
higher branch branching factors and so
on so one of the research challenges for
us is to think about more clever ways of
doing the search perhaps guided by some
of the model checking along the way to
decide where to go next and presumably
that's also what an actual statistician
might do they wouldn't just try out all
the models they would sort of do some
tests and look at some things and then
try certain models and finally in
particular for this project we actually
started out looking at multi-dimensional
regression problems uh and for the
interpretability purposes we focus on
series to be able to produce these
reports but now we kind of immediately
want to go back to multi-dimensional
regression problems and that presents
itself some of these challenges like the
search and descriptions to extend them
is going to be interesting I think is
doable but you know it's computationally
expensive and representing
multi-dimensional functions in a report
might be challenging if the function is
of you know dozens or hundreds of
variables then it's going to get very
challenging to write that report okay so
looking at some current and future
directions there there a few people kind
of on the team right now working on
several variations on this kind of
concept I mentioned already the
multivariate nonlinear regression idea
which i think you know we're making good
progress on as probably all of you know
a lot of applications of machine
learning are to classification problems
so we want to basically do the same
thing for classification that presents
itself with some interesting challenges
as well just because classification
problems don't actually tell you that
much information about your unknown
function but we're working on building
this sort of thing for classification
and we're also working on completing and
interpreting tables and databases which
i think is a very similar in spirit to
the idea behind tabular and inferno DB
but in particular with the idea that
we're going to be searching over a big
grammar of models and producing these
interpreter reports for people this line
of work actually ties in very closely to
another line of work that you've heard
about this morning and there's also
quite central to my current work and
interests which is a work on
probabilistic programming so clearly if
we have a general-purpose sort of Turing
complete programming language like
church or Anglican or one of the
variants then that's a very powerful way
of expressing a large space of models
and of course these things come with
universal inference engines but as John
Winn mentioned it is ha
they're not kind of at the moment up to
speed for doing these large model
searches but you know maybe maybe by the
focusing attention on particular classes
of models we can get them to be more
scalable and clearly leveraging
probabilistic programming is a very
sensible thing to do if we want to build
a big grammar / models so just to wrap
up so I presented we presented the
beginnings of an automatic statistician
focusing on regression our system
defines an open-ended language of models
agree de lis searches through this space
and that produces a detailed report
describing patterns in the data its
interpolation and extrapolation
performance looks very good at least
competitive with all the other things
that we compared it to and we think that
this has a lot of potential to basically
make accessible to non-experts our
wonderful sweet of techniques that
statisticians and machine learning
scientists have available to them okay
so thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>