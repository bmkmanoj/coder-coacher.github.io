<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Coming Genomics Software Revolution? | Coder Coacher - Coaching Coders</title><meta content="The Coming Genomics Software Revolution? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Coming Genomics Software Revolution?</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Y2V078kA6eY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so welcome to the genomic session I'm
bilbil osky from microsoft research and
I'm just going to talk for a minute the
format of this is that we have we have
four speakers David Haussler David
Paterson David hacker Minh and George
verghese and so there's the obvious
lesson that to be respected in this
field you need to be named David oh but
other than that one of the things that I
that I've learned from thing about
science in general is that science
follows tools which is to say because
science is experimental where you where
you find progress made is the places
where you can explore new things and so
if you think about progress in physics
the really cool stuff in physics happen
a hundred years ago and the reason for
that is that the while physics is
intellectually complicated it the
experiments that you need to do to
understand relativity or to understand
quantum mechanics or things that mostly
you can do in a high school physics lab
they require decent clocks and a lot of
brain power in a blackboard when you
think about biology the amount of
information in the sort of information
theory sense in a biological system is
tremendous the human genome is is three
billion bases the the microbiome is ten
times bigger than that or or more and in
order to be able to understand this you
need both instruments that can can read
the genome and then you also need
instruments can process that amount of
information which is to say computers so
only in the human genome project
finished 10 years ago high-volume
sequencing has only been around for a
couple of years and then honestly the
computational power to be able to handle
data at that volume has only been around
for a couple of years and the
consequence of that is that there's a
lot of reason to believe that tremendous
progress is going to be made of the next
few decades in biology and that in order
to do it it's going to need help from
from us computer scientists so this has
caused me and a number of other people
to start start looking at problems in
this area to see what we can do to help
so I personally started started this
after listening to a lecture a couple of
years ago that was given by our first
speaker David Haussler and he presented
some I'm not going to talk about what
I'm doing but he presented some problems
and I thought gee I could probably do
that better and I was insanely naive but
it got me working in the field and and
I've been very happy about it so the
format that we use here is we'll have
our for talks and then we'll have time
for questions at the end so if you think
of questions for any of the speakers you
can just do them at the end and with
that I will introduce David Haussler who
is a professor at UC Santa Cruz and
really the guy to talk to about
computational genomics and the CGHUB the
cancer cancer genome hub David thanks so
much bill and it's pleasure to be here
and let you know a little bit about
what's happening in cancer genomics
first of all genomes the ability to
sequence a whole human genomes is poised
to have a very profound effect on
medicine nowhere more than in cancer
though because cancer is a disease
that's caused by individual mutations in
the genome right now our cancer
therapies are one size fits all for the
most part you get a chemotherapy drug
that kills every dividing cell in your
body that's very indiscriminate approach
a targeted approach can give spectacular
results as you see here so this patient
with metastatic melanoma has a
particular mutation each one of these
lumps is a metastatic tumor and they all
were generated from an initial cell that
spread throughout the body created
tumors and each one of those has a
mutation of being a gene called b-raf
where it's a particular change amino
acid change that occurs at position 600
in the protein there's a drug that was
made by roche that specifically kels
every cell that is exhibiting that
mutant form of b-raf patient takes that
drug bear marathi nib 15 weeks later
that's what he looks like
working out at the gym swimming in the
pool unprecedented in this type of
metastatic melanoma so this shows you
that there is incredible potential with
precision medicines approach to cancer
rather than the one-size-fits-all if we
understand the molecular cause of the
disease what mutations are precisely
driving each tumor then we can come up
with a single drug or perhaps a cocktail
that addresses this why you might need a
cocktail is illustrated by looking at
the patient 23 weeks later and if you
actually look at these tumors that map
is reproduced a few weeks later by and
large not all of them but by and large
there was at least one cell that either
was inherently resistant or became
resistant to this one blocking drug very
reminiscent of hiv/aids each one of
these tumors has billions of cells and
it only takes one to evade it and regrow
the tumor so you want to hit it with a
cocktail or with an adaptive therapy
where the patient's own immune system is
adaptively changing to the cancers every
move these are the exciting new
frontiers that we have in cancer and
we'll get to them through cancer
genomics which is the subject of today's
talk so motivation obviously saving
lives is the number one motivation
cancer is in a very pervasive disease it
is is so pervasive that I'm sure
everybody has been touched with it
somewhere in their family or close
friends if we can gather together
information about the molecular nature
of cancer by sequencing cancer genomes
then we can start to understand what are
the basic insights into how we can
precision treat with precision every one
of the different kinds of mutations that
drive cancer
we have to link these mutations to the
clinical outcomes both in response to a
frontline normal standard of care and in
response to these new targeted therapies
we have to create an infrastructure that
will allow us to collect data on
specific pup subpopulations that we so
we can get them into clinical trials and
carefully test but what we really need
is more than traditional clinical trials
is a rapid learning cycle that's global
essentially so whether it every cancers
course is recorded and those case
histories are shared electronically and
become part of the great greater
knowledge of cancer and finally with all
of this molecular understanding we will
gain a mechanistic understanding of
cancer at the molecular level every one
of these things require statistical
power and that will be the main theme of
this we are not in a position to share
large numbers of cancer data today but
we must get there so we must get to a
point where a patient has molecular
information assayed for their tumor and
soon that will be whole genome
sequencing and it in cancer in our
research today you sequence the normal
tissue of the cancer maybe of the
patient now maybe from a bloodline or
other blood sample or other tissue
sample but you also sequence the tumor
and you look for those changes that are
mutations that are specific to the tumor
in that person and not in the rest of it
sells the normal cells in that person's
body and if you had a database of
millions of other cancer genomes and you
could find patterns in similar cases you
would be in a better position to treat
that patient with precision to the
actual molecular categories that his
mutations or her mutations fall into in
order to take a survey of the cancer
genome the different types of genomes
the National Center for cancer the
National Cancer Institute
instigated a Cancer Genome Atlas program
a few years ago with the target of
10,000 tumors 500 tumors from each of 20
adult cancers there's a parallel program
on childhood cancers called target these
are the Institute's that are involved in
that in terms of generating the data and
these are the Institute's that are
involved in analyzing the data and all
of those data come to a repository that
we built called the cancer genomics hub
we also were successful and we hold the
childhood cancer project in fact CGHUB
holds all of the large-scale cancer
genomics information created by the
National Cancer Institute at this point
these data are used not only by the
researchers that you see in this network
but we have hundreds of other users who
have applied to get access to these data
their sensitive personal genome
information so we don't post them on the
Internet freely but you can apply and
state your research goals and your
credentials and then gain access to
these data the total cost of this
repository for all of the current nci's
programs when they get to about 50
thousand genomes which we assume now
after these this 20 thousand genomes
from from from the Cancer Genome Atlas
and more from subsequent program soon
there will be at something like 50
thousand genomes would be about a
hundred dollars per genome per year and
this is costs that we these are real
numbers because we operate the thing so
that's that's how much it costs to pay
the personnel and I must say a lot of
that is compliance and paperwork and you
know other things that aren't directly
related to the actual storage
infrastructure but that gives you a data
point of how much it costs to house
already 50 thousand genomes that will
take about 5 petabytes where we're
planning to grow we're about 1 petabyte
now and planning to grow up to 5 at some
point
at this point we've transferred about
six terabytes of information to
researchers around the world and this
happened essentially in the last six
months people are downloading at about
three gigabits per second but if you
look to the future the need here is
international and so we are now working
on thinking about how we would set up a
global network of hubs and move this
into environment where we're not just
storing the data but we're actually
bringing compute to the data as in a
cloud type system Dave Patterson and I
have been involved extensively in this
and dave is going to tell you all about
the details so I'm not going to go into
the design but we put out a white paper
recently last summer and described how
we would do a million cancer genomes
that was part of the inspiration for the
formation of a new global alliance that
has now formed it was announced in june
i'm on the organizing committee for this
and we now have over 90 different
institutions worldwide that have signed
on we're modeling ourselves after the
World Wide Web Consortium as as a way
that people can come together over
standards and a neutral area that that
allows party parties to innovate and
share in an era of open open and level
playing field platforms and we're really
hoping to work with you and others to
develop some of these first prototypical
platforms and set the standards we
believe strongly in developing platforms
at the same time you are developing
standards so what does it look like to
look at cancer data I'm gonna now for
the next several slides of dive in and
give you a feel for these data so the
data come from raw reads and raw reads
are a few hundred bases of DNA that are
extracted from one of these
high-throughput sequencing machines each
of those reads is mapped to a location
in the reference human genome we're all
night
nine point nine percent identical and so
it's relatively easy to map most of
these reads to a unique portion of the
genome and that where they will only
differ in a few bases and but there are
highly repetitive parts of the genome
where it's very confusing and difficult
to map these reads each of these is one
of those reads that you see so this is a
read of DNA and this is a location in
the reference genome here on chromosome
2 at a certain position so it turns out
that this is a case in which we have an
unusual mapping if this is the
chromosome to coordinate system for the
standard genome and these are reads this
gray represents a bunch of reads that
are piled up like you see here a pile up
you see that the reads mapped normally
but then there's a segment where you're
getting extra reads and more importantly
there's a segment where part of the read
match is here and the rest of the read
matches back here so it's as if the
genome looped back and redid this
portion and in fact that's exactly what
happens in the genome a big hunk of DNA
about 500,000 basis is simply repeated
in tandem in this cancer genome
abnormally whereas the normal genome and
only have one copy you have two copies
in a row so we can deduce that by
combination of looking at the extra
reads that map to that region is if we
were somehow getting too many reads from
that and these funny loop backs that go
back and take a second shot at that
region this is the art actually
unfortunately not completely a science
but an art of interpreting these data we
are doing lots of comparisons for the
best groups that are available right now
that are working right now on the Cancer
Genome Atlas and the International
Cancer Genome consortium data and when
we look to see how we can interpret
those cancer genomes even if we look at
single base changes is an a change to a
tea at this position well there's a lot
of agreement there are actually a lot of
cases where we disagree about that
so it's not easy to read these genomes
with a hundred percent accuracy and that
comes from the fact that we have noisy
short small reads if you look about
these structural changes these are
structural changes where one part of the
genome is connected to another part of
the reference genome by a read then
there's a little bit less agreement
something like seventy to eighty percent
agreement between two of the top
programs our program at santa cruz and
another program at the Broad Institute
so we got a ways to go to really really
understand how we can interpret the data
that's coming off the sequencing machine
we get better if we go deeper read the
genome instead of an average of thirty
times an average of sixty or a hundred
times but that's very expensive so
there's a trade-off between cost and
accuracy of interpretation of these data
we study cancers that are related to
each other and one of the specialties
that Santa Cruz is brain cancer I
actually have a wet lab as well as a dry
lab and so we do studies of developing
brain cancer in our lab and other
aspects of the developing neocortex and
here we see one of the deadliest brain
cancers glioblastoma analyzed from the
point of large scale events which have
actually knocked out large parts of the
chromosome and what we see is repeatedly
in in in these gbms and in particular 11
out of 16 of the cases of glioblastoma
we see two separate events that knocked
out both copies of this gene cdkn2a be
this is what's called a tumor suppressor
gene you have to have this gene in order
to be resistant to the cancer once you
lose it then the can't then the cells
can grow in an uncontrolled fashion and
in order for that to happen you actually
have to lose both copies of the gene the
one you got from mom and the one you got
from dad those typically happen in two
separate events and and we can see that
the prevalence is enormously high in
these other gbms those genes are
probably inactivated by other mechanisms
and we're looking at other ways in which
small mutations are
something like this might have
inactivated them in one of these cases
the way this gene was inactivated was
rather spectacular it was with an event
called chroma therapist in this case the
net effect is that the chromosomes are
actually broken up into hundreds of
pieces in certain areas usually it's one
area of one chromosome or an area of one
or two chromosomes that's essentially
shattered and those pieces are put
together again in some random form to
repair when you glue the ends of pieces
together randomly you often get these
circular pieces which are called double
minded chromosomes and had been observed
under the microscope for decades
actually in cancers this is the first
time where we can take one of those
circular pieces of DNA and actually
deduce its structure completely from
genome sequencing and what we did was
essentially trace the patterns of reeds
and we see that this part of the reed
connects to this place this place
connects to this place and so forth and
if you trace these paths of connection
you eventually get back to where you
started from and you realize in this
case there are two small circular pieces
of DNA in this person's brain tumor not
only are their circular abnormal
circular pieces of DNA but they contain
very important genes they're highly
amplified so we have almost 80 copies of
this one and 40 copies of this in each
cell and on those are the opposite side
opposite kind of tumors tumor suppressor
genes like I was talking to you about
before are the breaks that you lose then
these are the accelerator pedals of
cancer EGFR and mdm2 our genes that when
they're overactive drive the cells to
evade program cell death and and
propagate in an uncontrolled way here's
a second one GBM that also has one of
these small double minded circular
chromosomes and it also has mdm2 in it
as a driving force and this is uh
laboratory work showing the the this
this pink background shows the enormous
numbers of copies of this spread
throughout the cell so it's lit lit it
up in pink and these are control the
greens or control things that you see
only have a few copies so this pink bad
circular piece of DNA is pervasive in
these cancer cells according to our wet
lab validations I'm not going to
summarize but in fact looking at all of
these data now not just about these
jeans like cdkn2a you here in egfr
mutations so if you actually look at the
mutations in all of these genes
different types of mutations that are
happening then you see that gbms
actually fall into a discrete set of
classes that have different
characteristics and should be treated
differently so when you have a molecular
understanding you realize that any
cancer you looked at clinically or
through the microscope is actually not
just one cancer it's not ten cancers
often it may be hundreds of different
sub sub types of cancer based on the
actual molecular pathways that are being
disrupted within their and when you know
that you know to treat them individually
now for you you are all computer
scientists one of the real challenges
here is that we think about this
simplistically we think about every cell
in your body has your regular genome
that you inherited from mom and dad it
just gets replicated and then these
cancer cells have one abnormal genome
well it turns out that cancer cells keep
mutating and so a tumor actually can
have cells with different genomes in it
these are called clones and essentially
what happens in a tumor is like a little
bit of Darwinian selection all by itself
the the book to the cells with a
nastiest combination of mutations that
cause them to grow fastest tend to take
over the tumor and shove the other cells
out and this is happening throughout the
time course of the evolution of a tumor
so at any time now you look at it you
see these dark gray dark orange to light
orange and so forth you see different
clonal expansions so there's actually
usually not just one but maybe two three
or four different genomes within the
cells of a tumor
we need to separate those out when we're
looking at all these data so it's it's
hard so a cancer is a meta-genome people
who are looking at bacterial samples are
used to this right they dig up the dirt
or some body cavity and you find all
different types of genomes from bacteria
mixed together now cancer is like that
too sometimes there's a framework which
I won't have time to go into that we've
developed which not only deals with the
fact that you glue back and repeat
things and you rearrange things but the
fact that you mix together different
versions of that whole rearrangement
pattern into one genome it's a flow
based theory we're very excited about it
and we'll we'll have a paper on that
shortly we apply that from an algebraic
computational point of view the little
linear programming to analyze these data
and in simple cases that give you what
you expect so this is the case that we
this is another case like the one we
looked at before these are the reeds
there's extra beads in here and there's
some reeds that loop back and so what
we'd like to do is go from this noisy
data to integral data it's essentially
saying well if this is happening in
sixty-seven percent of the cells then
we'll break it out and we'll assume that
every one of the will will say that
every one of those cells is digitally
duplicated from copy one to copy too at
exactly this point and connected in
exactly this way so we go from noisy
nasty analog data essentially to precise
digital genetic interpretations of the
multiple genomes that exist in the
cancer sample and this is quite
challenging I don't claim that we've
completely solved the problem but but
that's the essence here's the case of a
deletion and finally last content slide
here after you have actually determined
what clones are in the cancer and what
actual genetic mutations are in those
clones and how those might be driving
the cancer you have to think in terms of
the biology of cancer and the pathways
for example
cdkn2a and be act in concert with other
genes they repress these genes which
were press RB which were presses the
progression of cells cell cycle and in
if you see the same similar thing
happens over here where it's involved in
our friend mdm2 that you saw so when we
amplify mdm2 it has a similar effect to
losing cdkn2a which are the two things
that we saw because they cdkn2a inhibits
mdm2 and amplifying mdm2 screws you up
because it's supposed to be inhibiting
it's it inhibits p53 and you don't want
to inhibit p53 that is your protection
against cancer so the logic of these
pathways of interaction are fundamental
to deciding how you would treat a
patient with a certain set of molecular
lesions so the logic of going from what
you read in the genome accurately to an
accurate understanding of what's
actually going on in the genome and then
finally to an accurate understanding of
how the pathways are perturbed and then
finally to a combination of drugs you
want to prescribe are all very
complicated decision procedures that
must be automated as we were hearing in
the previous session it's beyond human
capacity to do this for the doc to just
the kindly old doc to actually do this
stuff in his or her head so it's a big
deal that we're collecting all of this
information at all of these different
levels and I think this is really the
age of opportunity for cancer research
and for other types of medical research
based on genomic information but the
number one infrastructure issue is we
will not be able to achieve the
statistical power we need to decompose
these pathways and understand all of
these genotype-phenotype relationships
unless we get into the millions and
we're looking at information silos where
every hospital is keeping their own data
secretly away and that's what the
Alliance is trying to break we want to
get people to share so that scientists
can aggregate data and understand it
together even with a million genomes
though that
interpretive challenge will be immense
we need to understand these pathways and
how they work and we need to be able to
read our molecular data better this is
the key and I thank you for listening
this is a work with many other people
Dave will get set up Dave's one of my
closest collaborators we have a number
of collaborators at other institutions
and there's a great group at Santa Cruz
thank you next slide any questions we're
good we'll do questions at the end okay
was it be in what you do that on I'll
introduce you okay okay so now I have
the great pleasure to introduce Dave
Patterson who almost needs no
introduction but I will anyway he's a
professor at eeks and Berkeley he should
be there somewhere can we help them
there uh yep okay Oh see um go down and
select a PowerPoint down here anyway oh
oh the one happen oh this one yeah
counselor no those aren't try to go
there you go there you go yeah it didn't
open it oh that looks like it open ok
yes how many distinguished computer
sciences taked open a powerpoint deck
anyway dave has done a number of things
that have changed the world already he
he ran the raid project he did risk he
did the network of workstations project
that the ideas of which are basically
all of data center cluster computing now
and he said something and I've been
going to his retreats for for 20 years
now and he said something when we're
starting this that really run true with
me which is that it's not the number of
projects you start that matter it's an
or that that you finish and so you
should do a small number of things do
them well and make them be something
that matters and so when the idea that
that you could take computer science
techniques and apply it to cancer
occurred to him it it motivated me to
work on this and I think he's really
right if if it's the case that we as
computer scientists can actually help
with curing what soon to be the number
one cause of death the United States
don't we have to try I hope I haven't
stolen your Thunder yep each stole my
thunder I think what I'll do a couple of
slides to set up but then I think I'll
wax philosophic here David Haussler is
we should have just given the whole
session to him he does about as much
work as at least five to 10 faculty he's
got 75 people working for him he even
does Kevin does wet lab work you don't
have to do that to work in this field so
I think talk about that I'll take a
little bit about the amp lab
opportunities for computer science were
all wax philosophic the amp lab is a big
data lab it has serious machine learning
people as well data businesses some
networking our goal is to really release
a new software stack the next Berkeley
UNIX or the next ingress or postgres we
hope we got funded by the NSF and by
DARPA and lots of companies including
Microsoft and we got mentioned by the
president when the government government
got interested in big data so building
on top of the Hadoop file system we have
a cluster operating system resource
manager an idea of keeping things in
memory yet be reliable spark which is a
cluster programming framework an
alternative to Hadoop or MapReduce we
have a streaming version of that well we
can put a sequel interface on top of it
we are going to do an approximate as
part of a big data project a rather than
a database give you the exact answer
give approximate answer much more
quickly and then you know the big
excitement and machine learning and big
data is how we're going to teach
millions of people not everybody can
work for Michael Jordan is a as a
graduate student so how are we going to
teach people do that so we're trying to
do a declarative machine learning thing
that anybody can use the right machine
learning algorithm
David's let me do this quickly it's you
know it this this disease grabs us cause
it's so pervasive third of all men half
of a third of all women half of all men
will face it it's that we now know
surprisingly recently it's a genetic
disease the sequencing prices falling
this is required every talk has to have
this slide it's approaching a thousand
dollars and what are the bad news well
the people building the software aren't
trained in how to build software the
prot ads sell the processing costs are
going to be bigger than the wet lab
costs and there's no place to store this
stuff so where can we help and I say
there's three things build stuff that
works and is reliable for these software
pipelines to do the analysis like David
was talking about put this all together
and make it easy to use dependable and
privacy preserving of this million
genome so that david says the minimum we
need to get and bring our benchmarking
culture to this field now I think I
wanted to wax philosophic so I don't
forget about this you know my story of
getting involved was that I had to give
a talk at Santa Barbara about the amp
lab and we really didn't have any data
that was big we had a big data lab with
no data so i found a person visiting us
and he convinced me that genomics was a
cancer genomics was a good opportunity
and so we invited david to our first
retreat and he said he gave the
inspiring talk and convince us to do it
that's where Bill got hooked hooked into
this so we were two ish year the retreat
was two years ago so I'm two years into
this my last biology course was in ninth
grade i'm pretty sure so I don't have
any expertise just faith that this is a
good thing to do so I you know kind of
the hypothesis is that those of us who
know how to use keyboards can make as
significant a contribution as the people
who know what to do with test tubes so
that's a pretty you know arrogant
statement to make but I'd say two years
into this I believe it you know I asked
David that question two years ago and he
said sure but he says sure to everybody
he's a really nice guy and does it but I
think it's true I think I think we're
going to be we have the opportunity may
gives big a contribution but I've
learned so far as biology is a very
different culture
computer science we're the hippie
friendly let's get along let's share our
data will even show you the code before
we finished it yet biology is not that
way so there's a culture clash there
they're not welcome us with open arms
the oh boy please publish in our
journals no that's not the way there
they and they're pretty nasty in their
comments they don't they I'm not willing
simply to reject the play / with a nice
try their go they'll go after your
scientific integrity is part of a review
it's so I'm is a kind of a senior person
I think there's this great opportunity
it's a little unclear to me how
publishing is going to work in this
right there's a danger in forming new
conferences or new journals you know but
there's also a danger in the biologists
locking at us out I what I've been
interested in and of course funding is
always a challenge for for everybody I
think what's been really interesting to
me is I find my soul mates in the
clinicians the people who have patients
that they're trying to cure are kind of
like you know having impact technology
transfer we want to build things lots of
people use them they have problems they
want to solve and patients won't use
them and they want our help they they
have all this data there's is that
there's talk before lunch there's been
more data than any human being can know
what to do with they would love our help
right so these people like us they think
we have value it's not clear about the
biologist but the biologist control kind
of the academic journals so I don't know
the the solution man so it's so I'm
enthusiastic I'm leaping forward I'm
going to spend the next five years of my
life working on this thing because so
compelling but there's a bunch of kind
of sociological issues we got to fix
here so stepping off my stepping back
from the sermon here from my so this is
the group that we I've somehow sucked
into this like Tom Sawyer painting the
painting defense got people from all
over working with this a bunch of great
students and postdocs at Berkeley in
both machine learning computational
biology system and then external
collaborators including bill right here
and David Haussler as well as some
people at a place that's called the
broad which
the leading eat maybe the leading East
Coast place in this area so lack of
software driven by scientists another
arrogant thing to say well this one's
documented this was a article in nature
a few years ago they surveyed them the
scientists and most of them have taught
themselves programming they only think
formal training only a third of them
thinks formal training is worth anything
and clearly less than half of them know
what testing is so what's the
consequences of that so at one of my
colleagues at UC San Diego in the
Scripps Institute he had to retract five
papers including a paper from science
which is the thing that gets these guys
all excited it's like a mini Nobel Prize
if you get a on the cover of science
magazine he had to draw five papers
because his result was just a bug and
software from another lab so five papers
including very prestigious ones so I
never want to withdraw has to draw up
papers my career and I never want to be
the guy that they blame for doing that
so so there's an opportunity you know
just building what they're already
building but building it well and plus
we know algorithms right and data
structures or else I'll give you some
examples about that so there's a huge
opportunity there so our first result is
snapped that got inspired by hustlers
talk the Pelosi said we should be able
to do it faster than that and that's
when to collaboration between Microsoft
a great close collaboration microsoft
and and berkeley traditional approaches
had used the burrows-wheeler lab them
from you know that's the burrows and
wheeler for the guys from good old deck
circ instead a bills idea was to use
hash tables we have bigger memories now
we can use bigger hash tables will do
overlapping seeds rather than depend on
one's the hatchet was little bigger but
so what and one of the berkeley students
came up with a better string matching
algorithm then is typically used and
this thing happens to get better as the
redlands get bigger the red lines are
the number of base pairs there that
these high-throughput sequencing
machines their high throughput because
they take break up the the recordings of
DNA and these little tiny pieces so you
do a lot of them didn't parallel but
over time there's a Moore's law for this
base pair thing and it's increasing
rapidly over time an algorithm works
even better than the competitors as is
the reed links get longer and which is
happening quickly
this is an old version of a slide this
is the standard way you're supposed to
do it the x-axis is the error rate so
this is a very very high error rate and
this is you know an iterative one in a
million or 1 in 10 million Snap is the
one ours right there and you can see
this and bwa is the standard that by far
most people use but we were excited
about the speed of it and I I don't know
where we are bill is that 2 30 anyways
every time bill sees the slides he tries
to get make snap do a little better and
so this is this is a couple weeks this
is a month old so I think it's probably
even higher now but it's it's about that
fast so it's much faster because we
believe that curve about all of the the
sequencing that's coming because of the
dropping cars we put it out as open
source before was even done and a person
picked it up and added an RNA and RNA
aligner we're a DNA in liner and he did
it all by himself and he so that was a
success of open source and being hippie
sherry kind of people and then they
wanted the output in the standard format
to be sorted so Ravi who is here and
Bill or I can't remember which one did
it so the standard pipeline is called
gatk from the broad and just the pieces
that we did went from about a hundred
hour yeah so these aren't when you see
times like this I'm used to thinking you
know seconds you know or minutes maybe I
thought maybe it was miss these are ours
right this is this is a hundred hours
when was the last time you saw something
that last 100 there's a there's a
24-hour sort you know what are they
doing so so bill and Ravi put this
together and just that piece is a factor
of ten faster for what's being done so I
think there's a really opportunity to
build these pipelines the david alluded
to what are we going to do with all this
data that we got back we have to sort we
can't throw it away and part of what he
convinced us to do was to to make a
right paper to talk about it i really
was influenced in getting into this by
this book in prayer formalities if
you're interested I'd encourage you to
read the book it's a history
of cancer by a really great writer who's
also a medical researcher when the
Pulitzer Prize and he said you know
towards the end of the book because it's
very depressing about the way it could
be in the future is a woman shows up
with breast cancer goes the oncologists
her DNA's already sequence she kind of
brings there with the equivalent of the
USB key the software identifies the
pathways associated with those
variations in the genome they target the
therapies those pathways after the
tumors removed and then over time like
David suggested the bees HIV like drug
cocktail and as cancer transforms itself
which it does nastily over time we
change the cocktail and maybe take
medicine wrestle like like HIV but it be
controlled much like other diseases are
today the bad news in this book is he
said 2050 so I was I was with them right
up to 2050 2050 is a tad late for me and
so I'm I believe them one that this is
something that affect all of us so okay
if not me what about my kids I don't
want to I want us to make progress so
it's not just our grandchildren who can
be cured from cancer but both more
closely in last November we worked on
this thing for six months a
collaboration between UC Santa Cruz and
UC Berkeley to try and say hey we could
do this now the main point was to say it
stopped that expensive we could save all
the data and put the clinical
information hibbett so we could do the
correlations and make progress on this
terrible disease that led to the well
out of that lead we got invited to we
got invited David got invited and I I'm
his I tagged along to this meeting so
that created this global alliance was
his attempt to break down all these
silos which is very unlike the culture
of biology to clinical data that
countries around the world would pledge
that we're going to figure out a way to
work together to share this data
including the information so you know
we've got as far as announcing it so
we'll see what happens but this could be
a big deal what would be in there the
very basic reads that come out of the
sequencing machines we can't throw that
away because later on well we may find
like David was saying about the hundreds
or thousands of different types of
cancers in a tumor we
may not our our equipment may be too
primitive our algorithms do primitives
to see what's going on so we want to
save it the variations of the
differences between our between us in
our in our genomes that's what's called
the variation database and then putting
in the patient data which isn't all that
much we said David's experience the cost
of one hundred dollars a year at the
million genome scale we said
conservatively you know fifty dollars a
year or $25 year just really to say if
it's a thousand dollars sequence the
genome it costs almost nothing
relatively to keep keep it around and
cancer is kind of the poster child for
all kinds of other genetic diseases a
big obstacles aren't technical in our
view with cloud computing there's lots
of things that are sensitive in much
bigger than the the gene on you data
that's out there we know how to build
this stuff but we have to agree as a
society how to have what's called a
portable consent that a patient will
come in and say yes that's fine you can
use it for medical research and we don't
have every hospital has their own
consent form which is today's case and
we need to standardize not on file
formats which is the field tends to use
but more on API is like we're used to so
we can allow innovation to have to
happen underneath it you know our field
has been transformed by benchmarking I'm
in computer architecture there was a
time when we didn't know how to we made
everybody built a computer and ran their
own program and said mine is better than
yours and you didn't run the same
program well we you know kind of 30
years ago computer architecture figured
out that's a bad idea we standardized
benchmarks up into the right computers
you know what people think of as Moore's
law was really just transistors and
architects turn that into performance
same thing happened in computer vision
about 10 years ago no benchmarks
everybody have their own own new
algorithm they ran their own image and
they said there's better years but
without running on the same data people
at Berkeley realized you need to have
benchmarks to compare things for the
last decade there you have all agreed on
these standard benchmarks and huge
progress in computer vision we need to
bring that culture to genomics so why is
it harder we don't know only God knows
what's what's the actual 3.2 billion Bay
spares so we don't know what the ground
truth is which is one reason it's there
we don't but we don't even agree on what
the right metrics are do we don't do
common data sets at people if you use
simulations they simulate their own data
and they say well this must be right
because we got the same answers these
other guys quoting a Lord Byron he's if
you can't measure it you can't improve
it and that's kind of I think that's one
of my life's lessons here so we got to
figure out what to do so what we really
like to have for this for the variant
calling is these three pieces what's
called the reference than you think if
we're assembling a jigsaw puzzle the
reference is kind of the cover of the
box the short reads themselves from
these high-throughput sequencing
machines the millions of those things
are billions of those things and how to
validate it since only God knows was
there we need to have air bars on this
you know there's no exact right answer
we have to say we think this is within
this is how well we know the data so but
we this benchmark proposal that we have
has three pieces are C and H that is
real not synthetic comprehensive it's
the whole genome not just samples and
human that's what we care about people
nothing has all that so what I'm going
to try and do is we're going to try and
approximate that with two of the three
pieces this is complete human and real
so this one is human and and complete
but it's synthetic this one is complete
and real but it's it's not human and
this one is real and human but it's only
a sample not the whole thing and so what
that's where our name comes from this in
synthetic and the mouse and thus pieces
of the human that so our acronym is
smash synthetic mouse and sampled human
how should we do benchmarking today we
want to here's the air bars we were
talking about in terms of accuracy and
here's two popular snip variant callers
from two leading organizations and we
thought the right way to do this take
advantage of cloud computing and we tap
into pick amazon that we could pick
anybody and conclude both the cost and
the hours to be able to do that so you'd
see the gatk from the broad is much more
accurate but much slower
and and therefore more expensive and for
some of these cases the M pileup was
virtually as fast so depending what
you're trying to do we do that so let me
wrap up here it's you know the
excitement is about the dropping cost of
genetic sequencing hopefully to get to
the mythical thousand-dollar genome
pretty soon cancer is this a terrible
disease that affects affects all of us
and I really believe there's this chance
for us to be able to help fight cancer
which is I never thought that I never
thought at any time and be able to do
that philosophically to a bill made
another interesting observation at that
retreat is that you know for the first
50 years of computer science we didn't
have to ask other people what was wrong
with computers right they crash suffered
in more card was expensive week we had
plenty of stuff to work on but now 50
years into this or so maybe we need to
be inspired by other problems and I and
obviously you know helping people with
cancer is a very inspiring problem so
building better software pipelines with
new algorithms saving the data away and
bringing benchmarking so we can really
accelerate this progress so we don't
have to wait till you know so that week
it's not you know our grandchildren or
benefit that it's our children or us who
benefit and then stealing my punchline
you know after I gave that talk at Santa
Barbara said talk went well and then I
woke up next morning said oh my god I
believe what I said we could really help
so if there's a chance we can help
aren't we as you know moral people
aren't we that we have to try all right
thank you very much
okay i think if i do this look at that
okay all right okay thank you and sorry
about the punchline oh I will I promise
not to steal yours uh-huh so next up we
have David heck erman who runs the e
science group in microsoft research from
Los Angeles although his people heard
scattered all over the place and he he
actually designs algorithms to look at
the output of some of the stuff that
you've heard about that you heard about
from dave patterson and thank you very
much bill its pleasure to be here at the
david session George and bill if you
change your name we can make it 545
alright so this is a very boring title
so what what I'm really going to talk
about today is let me get this clip done
wrong turn
how's that what I'm really going to talk
about is making something that was
essentially impossible from a
computational standpoint it's now
possible and it's in the area of
personalized medicine otherwise known as
precision medicine which is what we've
been talking about today by the way this
work is in joint collaboration with and
backspace that's not work there we go
yeah with Kristoff Jennifer bob carlisle
and huy fong all in the East Science
Group our group it basically takes
machine learning and applies it to
important associate society challenges
we basically engage with scientists we
listen to them find out what tools they
don't have and tools they really need
and then tried to create those tools for
them I'm not going to talk these are
some of the projects we're going we're
doing today I'm not going to talk about
two of the three of them we're working
an HIV vaccine design also vaccine
design for common cold and hepatitis C
we're working to sequence sugar cane
which is a very important biofuel but
today I want to talk about human
genomics obviously and personalized
medicine so this is the slide that Dave
showed I think all janome as's have this
slide and it should basically just be a
screensaver for the the session and then
we can dispense with it obviously
there's a genomics and revolution that's
being driven by the rapid rapid decrease
in cost of sequencing not only is the
cost coming down rapidly but the time it
takes to sequence the genome is is
dropping dropping rapidly and it's just
creating a massive influx of data and
interesting problems so again I want to
talk about personalized medicine or
precision medicine you've already heard
the example of that in cancer but you
can also use genetic markers to diagnose
a disease to infer the propensity that
you're going to get a disease not just
cancer predict a favorable reaction to a
drug predict a non favorable reaction to
drug and so forth and one of the main
components of personalized medicine or a
key component of personalized medicine
is what's known as genome-wide
Association studies which we all call G
wasps and this is what what you do is
you take the genome and measure you can
you can do a whole genome sequencing now
or exome sequencing but what has been
done traditionally up up through now in
the last half a decade or so is to just
measure areas in the genome that vary a
lot from one person to another these are
typically these single variations are
referred to as single nucleotide
polymorphisms or snips so most your DNA
is the same these are this DNA of
different people every once in a while
you have a variation and now what you
can do if you're interested in what
variations are associated with disease
you take a bunch of people that have the
disease you take a bunch of people who
don't have the disease you measure these
snips and then you look for differences
and this has been very successful you
can barely see the chromosomes here
these bars these gray bars are your
chromosomes and each colored dot here
represents some successful jiwa swear
they found some interesting correlation
between some snip or set of snips and
some trade of interest so with chivas
there's there's some tricky things that
go on most of genomics is not like
Mendel's peas it's not one variant
determines whether the flowers pink or
white for most traits of interest
certainly for common diseases it's it's
much more holographic the signal the
genomic markers that influence the trait
are spread out across the entire genome
and they're very weak and they all
integrate together in subtle ways to
produce the trade of interest and so to
pick up these signals you need lots of
data David Haussler mentioned this
already and this is this is certainly
happening for example decode 23 me
Kaiser these groups have already
collected these chivas datasets for over
a hundred thousand people each
and when you have these these large data
sets you inevitably get yourself into
trouble speaking statistically you get
what we call false positives namely you
have a situation where you think there's
an association it really looks like
there's an association between the snip
and the trait and it turns out it's not
and it happens because there's
confounding and confounding happens for
many reasons but basically when you
collect lots and lots of data you end up
getting individuals that are related to
each other rather closely related to
each other you get people from different
ethnicities and these are just some of
the sources of confounding that can mess
up these G wasps now here's a here's a
simple example so let's just just
consider one snip so we're looking
across the whole genome we're just
looking at one single nucleotide
polymorphism and it's either an a or T
as shown here here is a set of people
that have let's say they have the
disease or cold cases here's a set of
people that don't have the disease
they're called controls and if you just
stand back you see oh there's more blue
here and more orange here so there's a
difference but it what if it turns out
that there's two populations may be two
different ethnicities here's population
one as I've shown here and here's
population to shown down here and in
population one there's more blues and
oranges and in population to there's
more orange than blue and if it now just
so happens that there's more population
one in the cases than controls suddenly
you have this false result it looks like
there's a difference between the cases
and controls but there really isn't it's
just due to this this confounding of
multiple populations okay so this
problem just gets worse and worse as
your data sets data gets larger and
larger which is what you want to do to
find these weak signals so one solution
is throw out the data start tossing
people that are closely related but this
is actually a very bad idea
statistically think about if you have
two people that are very similar and
their phenotypes or their traits are
different
and if the if there is a genetic cause
of that trait then you only have to look
at the differences between those two
people and you know that the whatever is
causing those different traits are in
that difference so if you have people
that are very closely related you can
actually get more power out of your
analysis so it's a really bad idea
statistically to throw away related
people now fortunately the the plant
animal breeders figure this out long
time ago they've been as in this
genomics business business for a lot
longer than the human genome is's have
and they very early on that decades ago
realized that they could solve this
problem with a very fancy statistical
technique known as a linear mixed model
I'm not going to go into the details but
suffice it to say you take this mix
Molly apply it to the data and it gets
rid of that problem i just showed you
and a whole lot of other problems that
have to do with confounding but there's
a catch when you run a linear mixed
model on a data set representing n
individuals you're going to take an N
cubed computational hit and an N squared
memory hit and that's going to be okay
for sample sizes you know 5,000 10,000
but you get much beyond that and you're
stuck you can't do it so what we did
basically with a set of algebraic tricks
is we figured out how to make this
linear mixed model algorithm linear both
in runtime and memory and as a sort of a
freak bonus it turns out that what we
did to make this algorithm faster also
made it better it made it a buil signals
and so we published this in nature
methods about a year back so let me for
the machine learning people in the
audience this is a slide for you it I
mean it's very very very rare situation
where you make something faster and you
make it better how in the world is that
possible well here's the quick machine
learning story so what a linear mixed
model does to capture this confounding
is it basically compute similarities
between
a pair of individuals in your data set
and then it uses those similarities to
correct for the confounding so if you
take this view naively you would think
well if I have a million snips or 10
million snips I should use them all to
compute the similarity between two
individuals because the more snips I
have the better estimate of similarity
I'm going to get right well it turns out
that using a limb to look for these
associations is actually equivalent to
performing linear regression with these
snips that you've chosen to compute
similarity with those snips used as
covariance so now as a machine learning
person if i were to hand you a million
potential covariance you would laugh at
me or you certainly wouldn't use them
you'd say I'm just going to throw a
bunch of noise into my analysis and what
you're going to do is feature selection
or something like feature selection to
get rid of most of those coverts before
you use them well we just did the same
thing when you do this in the space of G
was-- instead of using millions of
markers for most g was-- you're ending
up using hundreds of snips to compute
these similarities between individuals
or alternatively to use them as
covariates and when you have this very
small number that's where you get one of
the o/n savings you go from 0 n 0 n
squared o n actually there's another
trick another algebraic trick that we
introduced to go from 0 n cubed 20 n
squared but that's the story in a
nutshell so now we have this algorithm
that's very fast and we said let's do
something that you know you just could
do before so we said well instead of
just looking at correlations between a
single snip and a trait one at a time
which is what everybody does we said
let's look at all correlations between
all pairs of snips and a big particular
trade and there can be interesting act
it's generally well believed that
there's going to be interesting
interactions between pairs of snips but
no one you just couldn't be done before
at least for large data sets and so we
said let's give it a try so we took a
standard data set the Wellcome Trust
data set in that data set there's about
seven
there's fina traits corresponding to
seven common diseases shown here and we
analyzed all of them there's about
350,000 snips and so if you do the math
that's about 60 billion pairs so we set
out the task of checking associations
between all 60 billion pairs of these
snips and these seven traits now if you
do that on a single machine even with
fast Lim all the speed ups of fast limit
would take a thousand computer years to
do that work if you didn't have fast
limb I mean forget about it's absolutely
infeasible but because we had fast limb
and because we had as your where we
could fire up many many thousands of
nodes at once to do this computation in
parallel we got the job done in 13 days
and sure enough we found some new
interesting results the most exciting
ones I think are in coronary artery
disease we found two genes that interact
with each other a whole lot and each of
those jeans separately interact with
lots of other genes as well so this is
now being followed up by the experts so
with that I just want to say if you're
interested if for the the genome assists
in the audience if you want to give it a
crack it's freely available just being
fast them thank you
okay and finally you have our token non
David George was a professor at UCSD and
now he's working for microsoft research
in the networking group which is
actually other great things about
Microsoft Research I'm in the
distributed systems group and spending a
bunch of time doing biology says a great
deal of flexibility between where you
report and what you do and he's going to
he's going to tell us about some work
that he's done about how to how to do
fast querying of genomic data well thank
you Bill and today I'd like to talk to
you about interactive genomics which is
a joint project between UCSD and MSR and
some of the people like vineet and Ravi
are here so the stage is being set and
i'm sure you'll believe that all these
things are all these good things are
happening sorry should we put it in
sorry good job all right so the state
has been set and i'm sure you'll believe
that hardware costs are falling there's
lots more genomic data being produced
there is electronic medical records that
are coming soon and cancer genomics is
very interesting but I and I hope you
will also believe that software is a
bottleneck from some of Dave's numbers
software today genomic software is very
batch oriented it takes days for
analysis it is hard to write even the
best frameworks takes you take you a
long time and their script oriented and
sharing Israel so the natural question
for a computer scientist is software is
bad what abstractions can be used to
help genomic software so here's what we
propose we start by proposing a browsing
abstraction to query these large genomic
data sets that the two days are trying
to assemble so that we can quickly sift
through genomic data and remove
fruitless hypothesis before we test them
in the web lab in order to do that we
need to propose some other abstractions
and the first one it's a very simple one
is that prop a lot of genomic processing
its problems taken
like to separate them into two what
software people would call layers with a
well-defined interface and the interface
is characterized by three specific
operators that we believe abstract what
we would call noise tolerant interval
computation we also propose that you
need fundamentally different
optimizations for this database that's
here and in particular will try to
describe an optimization idea called
lazy joins that seems kind of useful
I'll also briefly refer to a prototype
we built where we ran some of these
queries and the longest query on the
azure cloud and it took about 60 seconds
and it was about 20 times more concise
and eight times faster than gatk which
is a pretty competitive framework all
right let me start with this vision of
interactive genomics so imagine if you
will there is this large database of
genomes and the two davids are putting
together but you've annotated them as
well with electronic medical records
where each genome has a list of disease
codes like alpha leukemia and h4 heart
disease and in addition there is a guy
called Bob who's a drug designer and he
has a hunch that deletions in some gene
cause hypertension and therefore he can
sort of build a drug for this but he
doesn't know which gene so he goes ahead
and types in a query which says please
tell me all deletions in hypertension
patients the database selects out the
genomes that correspond to hypertension
it then goes ahead and looks at the red
regions which are these deletions and
then returns what is common to all of
them and let's say this takes a few
seconds so Bob could keep on trying
various hunches validating them with the
existing data just as we do today with
regular data and before he goes to the
wet lab and this seems to be compelling
and so we could this is a discovery
paradigm but you could also extend this
to medicine if you have in addition to
diseases you also have treatments like
the drugs use so a doctor could use this
to try to figure out which treatments
have worked so why not just have a
simple relational database in the sky
why have all this complexity everybody
has read
this but let me just repeat this basic
bit of background think of DNA has two
large linear strengths right which are
three million bases long a base is 4
characters a cgt but it can't be read
like a tape from left to right the
current technology can only grab little
snippets or fragments about a hundred
characters and then go ahead and read
them and then align them to a reference
the data is quite big the single DNA
from one person is about 100 gigabytes
because each base is amplified with a
quality score the thing I'd like you to
remember is in terms of software what's
going on is there are three levels of
software first we take those reads in
order to get some vantage point we align
them to this reference the human genome
and then we leverage the observation
David refer to that we're all 99 percent
similar and we say well we really don't
want to store all this it's hard to work
with all this data let's just get a
high-level diff in the computer science
terms and they do it in terms of what I
call variants once we've done those we
can get to the interesting work which is
actually correlating variance with
diseases like David does and David heck
of it and so think of these three steps
as in your model and the thing that is
that really complicates matters is that
everything is probabilistic right
because there are all kinds of problems
because the fragments are randomly
sample we can make mistakes in reading
the instrument could make mistakes we
could align it in the wrong place so all
our conclusions are tentative and have
to be labeled with probabilities okay so
that's all I text so I'll just give you
a picture quickly which would make sure
this model is firmly in your head those
are the two strings on top of the blue
strings one from your father one from
your mother the yellow yellow substrings
our little random samples that are
picked with uniform probability from one
of those two strings and there's lots of
them so that they kind of cover this
entire thing and then you align them at
the bottom which is simple string
searching with a few errors with the
reference all right well let's give you
a quick model now a variant calling and
that's an important fact is that we
don't assemble these things together
which would make things a lot easier
it's technologically hard and very
expensive all right so now the next step
in the software would be to actually go
ahead and call some
arian so to get a high-level diff and an
example of a high-level diff is a
deletion and and so that red portion has
been deleted in the reference compared
to the subject and the way people tell
this is a very nice piece of technology
which they leave owner uses where they
pair up to read too short ribs and they
should be close to each other because
they actually sample close by to each
other but if you map them back to the
reference because that red portion has
been deleted they're going to spread
apart and so the so-called discrepant or
wide apart three pairs that map wide
apart are a telltale sign of deletions
all right that's just a little tutorial
what are we doing now let's be computer
scientist and the abstract we're
abstraction merchants so now forget
those reeds and let's just see them as a
bunch of small little green intervals in
this vast line and these notice that
these green intervals are paired because
some of them come in pairs and most of
them should the pair should be close by
but in the middle we see a bunch of
pairs that are far away so we might want
to figure out the regions where we have
a sufficient number of wide apart pairs
and that's that blue region we ignore
the region on the right because this is
one of them and conflicts with some
other stuff now we might want to get
back the original reads that contributed
the original green intervals forget
reads that contributed to this blue guy
and now we want to get some metadata
about them like for example these
numbers like 9 and 1 indicate our
confidence that we got that read mapped
or aligned in the right place so in this
case we're not too happy there are two
of this there's only two of them one of
them is very confident the other one is
not so the evidence some of the
witnesses are not so reliable and so we
assign a probability score to them let's
say point 1 because we're not so sure so
there is a natural abstraction boundary
between the probabilistic part and the
interval munging and we're going to try
to exploit that ok so if you look at the
layering picture a very familiar picture
to any of you who work in systems we
tend to have let's say the ethernet then
we have IP and TCP and we have the
applications in the similar way we have
here an instrument layer which is
hardware with like alumina that produces
these reeds
these Reed's are mapped or aligned to a
reference and then those that that file
which is called BAM is then passed to a
variant color and finally the variant
Philo VCF file is read by some
applications like G was like david
hackman's or many other kinds of stuff
like cancer genomics so what we the
first and simplest proposal is let's
just split these two apart let's
separate the variant color into an
inference part which can concentrate on
the probabilities and an evidence part
that is concentrating on the on on the
actual deterministic a gathering of
reads so and the and more than just have
a file format we want an API we want an
interface and that interface would be
select the evidence you need by
specifying by by a bike whirring now in
order to do that you need to be more
precise we need to actually specify what
you mean by ferry and so in order to do
that we've specified a query language in
which intervals are first-class now
that's not surprising because lots of
things in biology are our interval the
inputs reads our our our four-hour
intervals the outputs like the variant
files are our intervals and even
intermediate concepts like jeans are
intervals so it's kind of natural to
think in terms of intervals so the data
model we started with toying with an
interval calculus and he decided let's
just make it sql-like because it's more
likely that it's when we use and so
think of SQL tables except that tables
now have two columns for a start of
intimal and an end of interval and we
have three operators the first one is
just vanilla SQL select select a bunch
of rows that have a property the second
one is mildly more interesting we can
join two tables and we join them not
just by equality of key which is equal
but where the intervals intersect we
join them so we want to join a bunch of
reads for example with a bunch of genes
the third operator is probably most
unusual because the first the second one
is actually in geographical databases
and spatial databases the third went
into is third operator is a way to merge
intervals and basically we try to make a
kind of noise
rent Union think of each interval as
voting for some activity and it's only
when care of them vote for it that we
actually believe that something is
actually happening so in this picture we
have a bunch of green intervals and if
you say we want three pieces of evidence
and we say only that blue portion has
three weeds 333 such intervals
overlapping and those two blue intervals
and that's the operator of a great use
so let's go back to our abstract picture
of deletion and so we start with the
reeds and now naturally what we want to
do is select out from those that table
of reads the the subset of reeds where
the pairs are pretty by the part not
only that we probably want to merge them
with k equal to 2 so that we get rid of
these cruddy little reeds and get these
large intervals logical intervals but
now we may want to go back to the
original data so that we can actually
compute interesting scores for inference
and so now we join back with the
original data okay and so you can think
of this inference layers making all
these calls across this abstraction
boundary which allows us to make
progress so what's the point of doing
this inference is pretty much in flux
there are different methods and
different people by having a very
determinist thing take a set of
operators it's something we know as
computer sciences how to do it allows an
industry to guard and build this while
people are figuring out which kind of
inferences and allow them to use
different kinds of influence just to
sort of make the point again we actually
embodied all these concepts in a
language that we built at UCSD and and
so you go ahead and the same query is
now in programmatic terms you do a
select you go ahead and merge them and
finally you join them back and then you
output the results so the equivalent in
gatk which a number of people mentioned
would take about 1 50 lines of Java code
and this is certainly more concise all
right so we ran this so what happened
well we ran this on something called na
1 850 6 which happens to be a genome and
thousand genomes project why it's a
genome from Africa so gql this is the
lag name for the language we use as
opposed to the concept gingy don't query
language it
only 113 deleted intervals and just grow
someone now the irony though is that
there was an earlier study by Conrad at
all that found just ate in the same
individual and so there was a natural
question how do these results compare
and I think David Hauser will tell you
at least that such conflicts in biology
are common results are often conflicting
but what do you do you just leave it
that way well that's the whole point of
gql it allows us to probe further so we
said all right let's just join let's not
do any scripting write any code we'll
just do a join find out the set the set
of deletions in conway that's not an
arse and now let's just look at crown
relations a little more clothes leaving
the microscope well let's select from
them reads which have high pair
separations and we found none that's the
Delta Y sign but there's the other piece
of evidence you might want to do well if
there is a deletion very few breeds
should have mapped to that region and
they should have been reduced coverage
in that region and we didn't find any
evidence there further it happens that
the one we used happens to how we happen
to have the parents genomes too so the
deletions we found that Conrad didn't we
decided to try them and see whether they
were in the parent and it looked very
likely during the parent remember we
didn't assign any probabilities so this
is all pretty loose but nevertheless it
gives you a sense that you could use
this kind of stuff to allow interactive
sifting of results and we have a
bioinformatics paper which is a lot more
details about how we go about this and
the results and maybe some results for
cancers to which with and again we
haven't done wet lab confirmations but
it suggests the percent the potential
forums now don't get focused in
deletions right this applies to many
other things like snaps copy numbers
inversions and phasing and inversions
are simply finding reverse substrings
copy numbers as the davids referred to
is finding replicated substrings and
phasing is ascribing substrings to
either your mom's copy or your dad's
copy all of them are useful and we use
deletion as an example our abstractions
apply to these as well we have a ca-siem
paper which describes the the mappings
all right so we try to build this and
our first version you know it seems so
simple right it's just a browser what's
that it was awfully slow and
we had to learn the hard way how to
optimize this and we had to do it from
scratch we didn't use a database we did
everything from scratch and so the first
thing we did was to notice that many
whole genome Curry's walking across
these three billion lines right most of
the time you only need the metadata
you'd really you just want to find the
reads that are far apart you just need
the mate IDs so to speak you don't need
the entire text of the genome of the
read so why not store a separate file
with just that port in database world
it's very common it's known as a
materialized view right so all we do is
use the small view for scanning so we
have very little dis bandwidth and then
when we are ready to dive in we go into
the actual text files right obvious idea
mr. kind of step computers this so much
fruit waiting over here for us to me the
more interesting optimization with
something called lazy joints so all if
you probably know what a joint in
databases what you do is you look for a
row in one in one table and ruin another
table that match in this case that the
intervals intersect and then you stick
up together you join them now normally
what you do is you actually join them
and you make a new table but here's the
problem some of our columns are way
humongous big right and if we allow
users to make joint after joint after
joint we're going to write a huge amount
of stuff to disk and in the end we might
actually select out a small portion so
why are we doing this so instead we use
trees and stuff like to find the indexes
say row 12 joins two row 15 row 62 row
18 fun and we just store a logical
structure and this is recursive so we
have to build a tree it's not easy right
and at the end we have to walk the tree
but it's still worthwhile because these
columns are so wide right and it's a
good thing to do and so we build the
stuff and we use the obvious parallelism
each chromosome in a separate azure
virtual memory and we use twenty four
virtual memories which cost 96 sense and
are and this deletion query which is
quite hard and the halloween at a number
of them the hardest one took 60 seconds
but on a single genome so we have a long
ways to go to get to the million but but
i think there are ways out and if you if
we believe in databases we believe in
indices i think there are ways to do
this so i think interactivity
plausible all right so let me summarize
so situation is simply that we would
like to give biologist and geneticist
the idea the ability to hypothesis
generation in seconds not in hours or
days what we'd call interactive genetics
so thus best three ideas that call out
our this evidence inference separation
by the way i sat on an obvious idea it's
a very controversial idea there's lots
of problems take information at the
reads that we are ignoring right so
people will see why are you doing this
how can you do this you know and but if
you look at the history of networking we
had very poor links and if he started
leaking stuff about links to file
transfer we would never have built the
internet they'll forget about Facebook
so we learned that sometimes you have to
give up information and when you do that
you hope that eventually the links get
better the instruments get better and so
that's the proposal we're not sure it's
going to fly but certainly we're going
to make it keep the database mixes
existing ideas and then there's the idea
of interval operators thinking of
intervals as first-class and lazy joins
the database mixes existing ideas but i
think it's crucial to get the whole
package right i really don't think it's
going to work if you take sequel server
and start trying to do you have to think
do this from scratch the applications
are exciting this cancer genomics is new
one genomics this personalized medicine
and there's a whole raft look all right
so I'd like to specially call it the
person who did all this work was not
here sadly right his Christo's cause a
nidus who is going to be a postdoc at
Berkeley with Dave Patterson and young
striker and the rest of the gang so he's
great guy he's built this and certainly
if you want details also there are two
people in this room beneath bafna who I
learned all my biology believe can you
put up your hand if you have any
questions you ask relief and and finally
Ravi who helped us with all the
experiments where is Ravi Ravi something
everybody knows Robby and so and and for
more details we have a systems paper
which has a lot more details and numbers
and charts and all the good stuff that
I've ignored so thank you very much
sure yeah we have people I got a minute
or two yeah we have maybe one minute so
so you guys come up think so Freddie any
of
structure as well as associate
yeah the socio-technical I think we need
to offer them something that induce them
for getting the data out of their title
that means that we actually do things
cheaper and better and they're doing it
themselves a lot of these major medical
centers are investing huge amounts of
money to keep up their silos and their
have your sticker shock on the cost of
genomics at this point for retention
early and say now we have a standardized
and by using large-scale cloud
infrastructure it will actually be
cheaper for you money money talk and I
think that will be our main lever the
other lover also is to be able to join
in our science project you mean I want
to share your daily sure someone will
get other people's data and so get them
to play on the media on the framework
they will be by Karen were to get the
answer and it's very compelling
talk about you don't get good results of
these do you watch until you're up under
10,000 hundred thousands and so for most
of the that community has already had to
get people to kicking and screaming did
not want to share their data but we're
getting any results of statistical
significance they actually were brought
to the table digging screaming in and
combine their data in order to get those
results so you might happen my sense is
really from looking for them to karate
ecosystem to start computing nice list
of major challenging problems
we get
that science is alliances is a like a
lube job right is the team that global
alliances in the newspapers it's like a
moon shot right it said trying to
organize people all around the world we
agreed to collaborate and share data and
so now we've got the the trees in
science yeah so now we'll see you know
what's in a Bali right there is money
that's been set aside by some
organizations that are people there Dean
identify divers communicating form but
you know it's like the early days of the
internet I guess in you know how's the
center now before this just say
depressingly it's not going to happen
right look at the history now there's
some hope maybe good people well being
people will happen no problem no within
two years yeah it doesn't happen in two
years they're going to go find by the
harbor side with men okay well we're now
officially over time so I like to thank
the speakers again and all in the
audience for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>