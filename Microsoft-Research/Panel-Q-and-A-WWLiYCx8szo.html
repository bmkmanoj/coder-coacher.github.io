<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Panel Q and A | Coder Coacher - Coaching Coders</title><meta content="Panel Q and A - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Panel Q and A</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WWLiYCx8szo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
all right welcome everyone welcome to
the last session of the day hopefully it
will be the most interesting all right
so so we thought that you know we'll
just sort of initiate the discussion by
asking some concrete questions to the
panelists and then you know we can just
have a discussion going and everybody is
you know welcome to chime it so the
first question that we were thinking was
like it's to everyone here that what do
you think are sort of the big challenges
in machine learning what do you think
like if you let's say if you think that
okay this is one area that I can solve
or this is one problem that I can solve
then this will really make a big change
to the world or to the signs so I will
start with children okay so so now we
have the big question is about and like
what are the most challenging things in
machine learning ah okay so in my
opinion I think machine learning so
right now machine learning is a hot
topic and the so hot tool appointed us
sometimes people think that maybe no
power is is good there is a bubble for
me
so so the problem is that machine
learning is often part of a whole
application so right now in many
applications that machine learning is
used but it's only one component of the
hosting and sometimes it's very
difficult to know if that like improving
the machine learning part actually give
you something for the whole application
so that's actually a big issue then
another thing is that in applying
machine learning in those application
areas then usually there are so many
things you need to decide in the machine
learning is not an automatic process yet
so we don't have anything called
automatic machine learning well even
though right now quite a few people
think as an important research area but
those are very challenging issues is
just not that easy to to make a machine
learning fully automatic therefore our
many our users they have big problems
to use machinery so roughly Rosa what I
think and right now the maybe most
challenging sinks in a coming future
yeah I think I I agree with what Ian
said it's of course many people out
there whenever you talk with them they
think it's magic word solves everything
but it's very far from being fully
automated so one of the things I think
is that several things related with
cheese and said that well I'm interested
in and I think these are also jelly
upcoming challenges and current
challenges one is how to represent data
and data representation is either a part
of machine learning but it's also
pre-processing because it is related to
realities of how to collect this data
the cost of collecting data Poisson data
there many things so getting data in a
shape that you can do learning with it
is for any realistic data science type
problem one of the hardest things and
that's where vast amount of user effort
still goes and how to reduce that how to
automate that as far as possible while
still being able to get out the
inference from it that we warned that
will remain kind of sketchy for a long
time which is then also really I think
in my opinion quite closely connected
with a different aspect is to the aspect
of can we somehow based on a more
high-level description of the task
automate the construction of a rich in a
family of mathematical models which can
help us take go from the data to the
prediction that we wish and fully
automating this is a it's currently a
near-term challenge but as our
understanding and as these things grow
in complexity it
for many years I think remain a
challenge so those are just echoing what
jjun said but slightly maybe different
perspective can you hear me with this
so I also agree with what the previous
two people said I think what I could
maybe add to the like the challenge of
the different types of machine learning
that are there that's maybe also
difficult for the user is that often
machine learning is applied an
application that people might not really
understand what are the assumptions
behind the model so I think
communicating what are the assumptions
what you can do with the model and what
not and how much you can trust the model
and how much you can maybe not trust the
model maybe where you can even let the
model tells you I am certain or I'm not
certain might be an important thing in
the future if you actually want to base
some decisions on what the machine
learning model does if you want to
actually use it
another thing that maybe tries to what
the other two said is that well one is
representing data one is sort of
representing models and basically
designing models that capture what the
application actually wants and not just
using something off the shelf so really
understanding what's the application
about and we are capturing this in an
appropriate way so basically what our
mathematical structures and what's an
appropriate representation that we can
but that's also computationally feasible
and from the analysis perspective it's
maybe also making assumptions that are
realistic in the analysis may be
realistic assumptions of the noise and
the actual data that we have
understanding that ever yeah other
things that people are working on is of
course things like scalability of the
algorithms that kind of thing
yeah so adding and building upon what
Stephanie is just introduced the whole
concept of structure right so the
machine learning I think is rather
unique in the sense that it is it is one
of these sort of fairly
interdisciplinary areas that has come
about as the confluence of several other
very classical areas right so you have
you have optimization you have as varied
fields as natural language processing
statistics bitter science and so on
right so it's something that cares both
about you know notions of information
and measurement as well as notions of
recovery or computation computational
capabilities so in that sense it's
really I think set out to try and solve
big problems involving lots of data
maybe in an imperfect largely imperfect
or you know corrupted fashion that one
works with so in that sense I think
conceptually what appears to be the case
is the research for identifying and
exploiting as much structure as possible
in different types of problems that
various disciplines might pose in the
sense of machine learning so I think one
big conceptual challenge is to try and
either in an automated way or in other
ways try and identify structure that
would help you to really push the limits
of machine learning procedures I mean
whether it be you know trying to show
develop some you know non convex methods
that previously were thought to be you
know impractical that work really well
in the presence of some specific
structure in a large-scale inference
problems whatever that or or even that
scale optimization problems with
specific structure that now become
tenable once you identify some specific
structure that you can exploit so I
think my perspective on what one of the
directions that
learning algorithms are going to it so I
have some questions about some of the
things that speaker says that but before
that do you know it does anyone in the
audience have any question or any
comment
really strong succession step brain
distribution is Gaussian
and then having those comments
sorry one minute just to interrupt you
can you just briefly introduce yourself
before here it will be good for everyone
to know the question he says it's a
recharge journey kind of more important
computational or
I'll make some comments on that so
machine learning to me is fundamentally
like any other realistic discipline a
discipline of trade-offs and you one
cannot give a generic answer of which
way you should be trading of things so
for example I talked a lot about convex
optimization but convexity is convenient
and not everything's maybe naturally
formulated as convex problem so now
should we chase the theorem and just
forcefully look at things through convex
lens or not and so it's what kinds of
trade-offs are we willing to make or so
the best example of this is one of the
reasons why this simpler optimization
techniques are so popular in machine
learning people say that well we have
large amounts of noisy data I'm okay
optimizing my models to an inferior
level of accuracy then a numerical
analysts who writes an eigen solver
would care about and we made that rid of
that we accept these low accuracy
solutions but what huge benefit we get
from that we can crunch through lots
more data because our ultimate aim is
not to really fit the tiny Epsilon's of
the model because the model is always
wrong but to try to be able to process
more data in the hope that we can do
better generalization at prediction so
that is that motivation is coming
statistically and it is matching quite
nicely with the mathematical
computational framework and machine
learning actually is unique in the sense
that it cares about both and in some
nice lucky circumstances you can even
mathematically characterize how those
things trade-off in some you cannot but
every time you design a model you should
always ask exactly the kind of question
you asked and they will always be a
trade-off and whether that's a good
trade-off to make or not you should
never really decide a priori just try it
out
see it on actual data on actual problems
so start with the data is really the
statement here and then use your
understanding and empirical extremely
important empirical experience to help
make a good trade-off and decision so
here I will actually more of a question
so like you know for the last question
you said that we want to automate
machine learning whereas here you are
posing an entirely different approach
we're saying that an expert should set
and he should look at the data and then
figure out what model to use and
everything so the seems to be a very
like and that sort of thing happens
quite often like we either say yeah we
don't we want everything to be automated
or we want that you know an expert
should set so do you have a sense of
like you know how one should try to
think about it like one should like
should we just try to aim very high
automate everything or when he is you
know let's say working on a problem
should he just look at the problem and
really deepen it I pick a application
let's say I pick a application envision
go very deep in it and then figure out
all my algorithm so from research point
of view like you know what what do you
think is the is the better option or
correct approach well I think from
pragmatic point at least for the next
many years the you cannot get rid of the
expert hand but if you can understand
the trade-offs that are at play and you
repeatedly see these trade-offs
happening across more than one
application then these things start
becoming slightly automatable but
without exploring that I think it's hard
to make a sense of what could be really
automated so so my question is even like
little bit more sort of low-level in the
sense that I am saying that let's say
many of us are here you know researchers
or PhD students and also whenever there
is a problem should we abstract the
problem out and just look at very
abstract problem or should
we dive into an application I pick up an
application and then try to solve that
application very well liked and well I
guess I let yeah so somebody else Pequod
yeah I'll just quickly answer but also
look at specific problem if you would
like to speak we're still model
dependent and model or fitness function
or something these have to be I think
selected properly and the model has to
respected properly only then the thing
can work so I don't think it can be in
answer to all the domains from an image
we are formation of going for do you
don't know what a creature something is
on or should be here I have one comment
which is that have you tried to run a
deep learning system so so the thing is
they are sure the system is very generic
but all the complexity has been
transferred to training of the system
and you need a lot of expertise there so
and the way you set your parameters may
might actually dependent on your
application or you have to mean so again
it's I don't think one can say that Oh
deep learning is completely independent
of domain and you can just take your
noodle networks apply it and you will
immediately get very good results like
if that was the case then I don't think
everybody would be caring so much for
Jeff intern and Yan lacunae and all
right so so still I mean expert advice
is very important there also despite the
promise that it will lead to automation
of each's I was just about to answer the
previous thing that boutique had
mentioned I think when I have a real
data problem to ask even the right
question what I should do with the data
I will rather first spend a lot of time
closer to the data before trying to
throw any models or anything else at it
because it could be that many of the
things I may try in the beginning they
may give me some insight I may try them
but they may be superfluous so it's like
the way physics connects to math you
have to pay the price of experimental
observation before you begin to abstract
out and create a grand theory that
starts explaining and then you verify
the theory but it's like you said it's
also chicken-and-egg but still i think
for any new data problem if you have if
you this is the first data problem
you're looking at then certainly should
look at the data as closely as possible
to be able to ask the correct questions
so uh this has happened to me in a few
applications just to be able to even
posed which question I should be
it's really important to go close to the
data and coming back what predicts that
I also agree neural networks or deep
learning business right now I may view
it as we are using a computer to aid to
extend the human intelligence because
the expert knowledge is going into
making the architectures controlling
other parameters etc before this thing
works and there are million other data
domains where you cannot even without
significant more application of the
human mind you cannot even start to
maybe do these things I not necessarily
for mathematical reasons but for other
reasons which we may forget but these
are practical reasons from real life
they may be legal reasons privacy
reasons that you just cannot access the
data in a way that your deep system
requires access those practical issues
will only be more serious so it is
important to all that's why
representation or just seeing how you
are consuming the data I think is very
important
all right so so that is let me say a bit
about the okay as in the original issues
to ask about if you have a problem
should you like getting to the details
of ratification or try to think like a
more general general abstract way of
thinking well as my I think in a in the
area of mission and usually hard happens
is that in the beginning for a certain
type of applications that people don't
know how type of methods are the most
suitable then then lays like like a
crowdsourcing kind of things that
different groups they try different
methods but since then will converge
that's usually what happen it is from my
observation so for example etiquette you
can see see why like a why deep learning
things to start from the immediate
thought man yeah so it's like a lot of
people raised somehow try this thing and
find out that it works then a lot of
people try to reproduce the result or so
say for example in computational
advertising in a beginning of you
you know how to train a a model to do
Creek prediction and now almost
everybody use like to run very
large-scale distributed or logistic
regression that's after some kind of
like sometimes experiments by different
people but what after that
but once after that in the next thing is
led now you need to figure out why these
kind of things work so for example for
deep learning now a big problem is
there's no deep learning right now like
a black box okay so now you just put
like multi layer something and suddenly
it works but can you provide a certain
theory to to explain why it's deep
learning systems are much better than
some other existing things those things
are very important and challenging
things so usually I think for hotel
theories that you need to try some kind
of more engineering work that makes
things work but after that you need to
study the theory I think that's not less
the way to do it yeah I would have also
said that to really get an application
work usually requires much more than
just taking a model off the shelf and
applying it but really doing engineering
and studying what works and so on and it
it's a lot more work to make something
really work in practice
on the other hand what you will also
find then is based on your empirical
observations that sometimes there are
interesting theoretical questions
popping out like for example your deep
learning why does it actually work and
you might also observe for other things
I've had this in my research too that
you apply a model even though in theory
in the worst case it shouldn't work but
you see it actually works well in
practice so then you can start asking so
why does it actually work
and does this generalize to other things
now you've really understood what are
the properties of this model and
approach does this generalize to other
settings does it generalize to other
applications or so so these things are
not really an X or it really goes
together in some way
when we talk about machine learning or
optimization then we are if you seen
some data sets like particular reducing
the number of vectors and so means we
are somewhere comprised compromising
with the accuracy if the we can analyze
class data we will get more accurate
results than compared to the smaller
data but there are some framework like
MapReduce based approach and platforms
like a dopant there they can use
clusters and can alight very very big
datasets in very few seconds and in the
seconds so why we use machine learning
for optimized mean why can't we use them
if we can get with us on a large data
set then they will be more accurate than
this one if there are many assumptions
so maybe there will be some coverage
with accuracy so I think even if you use
MapReduce or so you will still need an
algorithm that is actually running on
that system so you will need some
algorithm that actually does the data
analysis in the end MapReduce is just
the computational framework and that
algorithm will be a machine learning
algorithm and now you can of course
there's a different question about like
the more data I have maybe the fewer
assumptions I might need to make in some
cases more regular more versus less
regularization or so but you will always
need like an algorithmic framework like
a model that you're actually learning
it's the analysis requires some
I think that's a misconception so that's
even if you want to and do in eigen
vector computation on a MapReduce
framework it's very hard in fact it's
it's a big research problem how to speed
it up over a single box computation like
if you have a single box with eight
goals that turns out to be much much
faster then if you do this computation
over MapReduce with even 200 machines so
first of all it's not that you know you
can just throw a map like lots of
clusters at a problem and the problem is
not in fact a classical example of this
comes from the domain of deep learning
so like if you remember a few years back
there was this amazing work by Andrew
Hank and his team which used around 1600
clusters to train a deep network after
around I think seven days or 10 days
worth of you know using this cluster and
then later on using proper algorithms
and you know proper techniques
well not that they were not earlier and
I can doing was not using proper
technique but by using very specialized
techniques and specialized algorithms
Jeff intense group was able to achieve
similar numbers after just training for
seven days on a GPU which had I think
what eight cores or something so you can
see that you know huge difference like
you then run using sixteen hundred
machines with multiple cores here you is
using one GPU but using it in a proper
manner so that you can you know optimize
much faster so MapReduce number one is
not the answer to everything
another thing is that if you have a lot
of data that doesn't mean that the
problem is solved immediately sure as is
Stephanie said that your many of your
assumptions can be realized you might
not need to regularize a lot but still
you need to apply proper algorithms that
will use those data points properly and
again many times you have budget issues
like you cannot just say that oh I have
a lot of training data and I will just
let my algorithm train forever you might
need you
let us say algorithm let's say your
rancor to be trained within five hours
so in that case you might have to do a
sort of choice that do I get a very
accurate solution or I get a solution
within five hours so again there you
know many of the techniques that
sovereign cheesin talked about like you
know stochastic gradient descent or
doing some very cheap techniques which
optimization people will laugh at but
still they might give you reasonable
solutions so you might depending on your
system you might need to design your you
know algorithms properly that will
optimally use your system related
comment actually to that in fact because
iterative algorithms in general are not
friends of MapReduce that led many
people to invest a lot of research
effort in creating systems frameworks to
better support iterative algorithms so
by now each company reinvented its own
Facebook as its own Microsoft has its
own Google has its own other companies
have their own different research groups
have invested a lot of effort in
creating frameworks which are better
suited for iterative computation with
persistent state and whatnot so it
actually the demands of machine learning
applications triggered a huge amount of
research in systems networks
architecture OS etcetera so it's
actually yeah so they worked rather
together quite nicely and related to
what to take mentioned that some of the
algorithms may be simple but a lot of
the actual victory of these methods in
large-scale industrial applications
comes from significant amount of high
quality engineering effort being put
into making these things run with time
budget resource budget failing hardware
and all sorts of things so it's still a
very active area and ultimately if I am
running my computation on the cloud as a
user or even as a company I have a
limited budget not only of time each
extra hour on 1,600 or 2,000 cores costs
a lot of money so you want to that's why
have no your model betters to come up
with better algorithms to also reduce
the running time there and benefit not
only that you get higher quality
predictions but everything happens with
much smaller budget of both time and
money
hi everyone I am mass will recess
scholar from reply tea and even work for
impulses so Infosys labs we are doing
this machine learning application so
this is an interesting topic how much we
can automate in the machine learning
because I'm seeing in past a couple of
years how can we apply different models
it can be ice implies the clustering or
classification models but I am getting
models data from different domains I am
talking about the practical data so we
are well versed with the basic algorithm
so there is a classification or
clustering but it is taking a hell
amount of time to read the data and what
model will fit it for them so it may be
a simple example we where okay it may be
ticketing description for example where
I am using NLP techniques to convert it
into the model and give it to a
classification model I am trying to
simple predict the model break the model
in the sense of if the new ticket comes
so to what will be my solution that we I
want to automate that part so I have a
lot of historical data and we have lot
of models we are using even the dictate
analytics what are the tools we are
talking about on SPARC ml labor are you
don't have any limitation on that front
but it's each case is very unique there
in this name if I work for one set of
data I am NOT pretty sure if the new set
of data comes gonna use the same model
so how can we tackle these kind of
issues because once we tackle is there
then I think we will be approaching
towards automating that process we have
different models but I'm not sure what
kind of model will suit curve for each
data that's where our most of our report
is going so how can we improve that kind
of process or structuring the model and
then we are doing a lot of
pre-processing as you mentioned sort of
time spending with the data then I maybe
I may be trying with all possible top
five models which will pay it then I am
validating which will fit for this one I
am giving it if that is a just like a
customized solution for one but can we
improve that process I think that will
trigger to the basic theme what we are
talking on how much we can automate
so automatic machine learning has mega
many many things related busy I mean you
want to fool your make machine learning
automating that's very complicated so so
about how do you you are thinking is
like okay how to like easily try
different experiments and data and once
you have data how to like to manipulate
them or use them in the future well
there are certain directions going on
right now so for example now layer
research trying to connect machine
learning and the user interface so for
example traditionally you see like you
see how machine learning or data mining
software look like they they are not let
interactive or later user interface
phase is not designed in a way so that
you can easily like adjust the way to
you change your model okay so so some
HCI people are involved right now in
trying to improve this kind of things
then then there are also some efforts is
like now if you have several models and
you want to try all of lab so behind a
lot up so so one direction is if you
store those models models on the cloud
so and once you have a good mechanism
then you can easily do a kind of so to
sweep all all the models to tell you
which one it's better with rows models
are independent so it's possible if you
have enough computing power so those
things are possible and to further give
you like and another similar direction
is a lot of some companies they are
doing the so-called web-based machine
learning so those are like learn a bit
like the next generation of machine
learning a package this is traditionally
you download R or way car these kind of
packages but but in the future because
your data may be already on the cloud
also traditionally for waking those
things sometimes you still need a
command line mode but a lot of people
lay down they don't write programs so
how do you if you can use Mouse just to
click on say okay I want to do data
split I want to do a classification yeah
so some efforts are not exercising
Microsoft edge or Mao is actually one
example I think neck I think next week
in the workshop laser may even have a
trending session and less a bunch of
companies are doing such thing so those
are the small efforts to try to address
your issues I think that your question
I'm a PhD student I triple I do daily so
I have a question regarding let's say if
I want to enhance the performance of an
existing system so sometimes it's like
you add more data and build a model over
it or you build a better model over that
existing thing instead of adding more
data to it so what is more important is
it the data or the better modeling
sometimes even by adding a different
kind of data you can enhance the system
the performance of the existing system
or and sometimes it's mostly by addition
of more data you get better results
rather than by doing better modeling
over the existing data
yeah I think I mean in general
definitely like whenever you start a
problem you should try to get as much
data as possible like whatever side
information you can get you should
definitely get in the system and then
apply good algorithms which will
automatically let you know things which
will automatically figure things out so
in fact so as a lot of things that I
talked about about this partially linear
regression so one big motivation for
that is coming from that direction only
that is in companies what happens is you
know we have this you know amazing the
right software engineers who are very
enthusiastic who have their own features
or who have their own idea that if I add
this extra information in the system it
will limp it should improve the system
but many times the result don't match
and the reason is because learning
algorithm then started to overfit or was
not handling the data properly so you
want to design algorithms which can
handle a lot of data without overfitting
or without running into these type of
issues so the goal should be put as much
as data as possible in the system but
design algorithms we should be able to
handle that data which will which will
not offer it to the data or which will
not get confused by the data for a
particular application you have like
some set of features but now with the
addition of more data you have another
set of features so is it that you should
just concatenate the features so
sometimes with the existing features you
build up another model with that you can
enhance the performance of this and
there's another way by which addition of
new features you can enhance this
performance of the system yes of the
same thing that term and this with the
question correctly that you should add
more features to the system and the goal
should be that performance of your
algorithm should not decrease by adding
more features that that you have to
ensure that the algorithm does not
suffer by addition of more data or
more features but certain like if you
add data you should expect more and more
performance yes so that's why your
algorithm should make sure that it is
doing correct features and it could be
that sometimes you may need to slightly
grow the model with Richard glass of
added features but if you don't have
additional knowledge of how the new
features relate to the existing features
then it's slightly harder to fully
exploit it but otherwise like as Pethick
said if you are adding additional site
information which is correlated in the
same way with the predictive prediction
as the previous features then you should
be able to benefit from that I'm a
righty my student from a team of the
undergrads run from my team others my
name is particle yeah I wanted to ask
well in communication field the focus
from wired communication to wireless
communication shifted a lot majorly
because of the technological development
now does a similar analogy hold in
machine learning and all these features
as in what are the major frontrunners in
the technology aspect of the same then
what do you see which research is in
some some major frontrunners which are
focused on the research something of
them okay
and Vegas if you think of it this also
happened happens and will keep happening
in machine learning for example neural
networks have been around for more than
30 years now because we have more
computational power and we have more
data and we can crunch there and with
the other bells and whistles that are
there but it's actually thanks to this
gigantically increased computational
power that suddenly these things are
showing results that they couldn't back
in the day and so in machine learning
technology is not necessarily just the
hardware it is data is the major
currency of machine learning and if you
have more data and you can crunch
through it successfully
that is what transforms the capability
of many machine learning methods on
problems I am
Manohar by professor from MIT money pod
of course we're second adela is a
luminous as well as he did is the BTech
from there I have a question about data
model well every one of you have
highlighted the importance of data model
in solving big data analytics problems
in software development there is a data
I mean model checking and verification
is there any research going on in
verifying whether the model is correct
or not that is model correctness
checking and verification is there in
such research going on in this area I
just wanted to have comment
what I well I'm not sure if a lot many
research works out going out of our lot
but what I do know is well you know
recently that a lot of people worried
about let of in the future that the AI
is going to like maybe change a human
word or something but then the Triple A
Society president Lee they wrote her
later
okay to tell people no no we don't worry
about such things but in their later
lady the highlight once they think
instead of worrying about that one day
light it was robots can like conquer the
whole world no that's not something we
should worry but lay did highlight one
very important thing in the future AI
research is led how to ensure that our
machine learning models they are they
are they are correct they don't make
like a CD mistake so that suddenly read
a lot of things just don't work yeah
so right now let's consider something
very important but you know I don't know
if validating machine learning has been
a if we already have a good read results
a lot probably not yet so maybe still a
good research direction if assumption
goes wrong the model is no no board only
thing is there at least there is some
tool to verify that ok whatever the
model is developed is do that well but I
guess verification of software is easier
than machine learning with machine
learning models I'll appeal like a more
dynamic she's not like that easy but
certainly that's very important actually
this some work that is going on at
Microsoft Research India along that
direction so I'm not sure if anything
has been published suffer but Aditya
nori he is heading a project where he is
like using programming languages he is a
programming languages guy so he is using
some of those techniques to validate or
check a model whether it is correct or
not but I think at the basic or the
heart of it the approach is based on
simple
cross-validation that you have a holdout
set if you are algorithm or model works
well on the holdout set then it is good
I mean then you don't care much about it
but then he has something so maybe check
his web pay yeah I am not sure if
anything has been published so far
hi so my name is bond topo and I'm a PhD
scholar from IIT Kharagpur
I just want to clarify about you talk
something about contradictory features
could you elaborate more on that like I
haven't heard that idea before I think I
just said if you have more features that
are correlated with the ones that you
have previously which means that if the
previous features the features with
which you are predicting something about
your data some kinds of features are
predicting some kind of output that you
care about the new set of features if
they have similar information about what
you are trying to predict then they are
positively correlated with your existing
features but it could happen that the
previous features just covered a small
dimension of your space and couldn't do
prediction for the whole thing and this
the new set covers the remaining part so
together they may end up giving you a
better coverage of the data space and
one crude example of that is also when
people train complicated models they
often to prevent also some sense to
prevent overfitting they synthesize all
sorts of new data points which try to
cover the space of features so that the
model that you train can actually do
accurate prediction for the entire space
of potential features rather than just
for the small number of features that
you input the first time so I just use
correlation not in a very formal sense
but having predictive power which does
not contradict the what the previous
features were trying to predict because
you know if you have some features which
say
this is a cat but the new features they
somehow lead you to a different
conclusion then these are not really
suited for the same task yeah this is
the regarding deep learning methods
because a lot of young researchers are
interested in it yeah on second sorry
I'm setting from IIT Bombay I'm a PhD
scholar okay this is regarding deep
learning methods so it got mentioned
that deep learning methods are black
boxes and it doesn't seem to me like
that because the most simplest of neural
networks that get used the feed-forward
neural networks each of the processing
unit seems to be like a logistic
regression unit if the activation
function is a sigmoid and in the early
90s there was a concept of stock
generalization and machine learning
where there was an it was basically an
ensemble inc method where instead of
using some arbitrary method like a
majority vote to get the final result
out of multiple machine learning methods
we trained a different model at the
final level from the results of a pen
ultimate level of machine learning
methods so each of the new layers and
the neural networks can be seen as a
stack generalization basically from the
logistic regressions of the previous
layer so it doesn't seem dot and
principle to me under that that it might
be called a black box because the panel
didn't seem very happy with deep
learning networks right so so so you
think that because from one layer to the
next layer we're combining several
logistic regression well so that's a
kind of example of models but turns out
that I don't think we we already have
enough understanding about example of
models so like can we can we have do we
have good theoretical support saying
that why doing in symbol of different
models actually improved I may give you
better performance than another of a
single month I don't think we are very
clear about the Bartlett
so so it's about almost maybe eighty
years ago sings to the Netflix
competition they started using like 100
models or something but you know at a
time in the beginning I sort of will
find out using 100 models I mean that's
just totally ridiculous but insulter
that works
letting the subsequent competitions I
mean people always I mean example
becomes a necessary part in order to win
a competition
so now does doesn't another things like
it works but we don't know much about
the idea behind it that's what I think
all you have one question that we asked
for concluding comment regarding it any
more here am I suppose it's okay
I'm a master students from IIT couple my
questions specific to professor's so
rigid you talked about optimization and
deep learning in your constant context
so I kind of working scaling the
networks you know optimization setting
or providing theoretical guarantees to
the deep networks or something else so I
think maybe mathematically we may remain
too far from giving theoretical
guarantees on the rate or power of
learnability that this network may
achieve the easier task to that I I was
talking about is that people are
training the parameters of these models
the weights etc using some simple
methods can we and can we understand
these optimization methods better for
these difficult non convex problems one
so that we could maybe speed them up to
use lower converge faster second is if
we could while trying to understand some
theoretical properties of the more the
algorithms using which we are training a
deep model we may discover what
components within the network
matter in which particular way which may
even give us some insight into modeling
but it's I don't know if I can fully
clearly answer your question beyond that
can you shed some light on learning
representations other than deep learning
it was as in some other setting but is
there any work going on on learning
representations on the screen so the the
more general thing I am aware of is this
conference ICL r which is conference on
learning representations and people
however present different flavors of
neural network based learning or
previously people used to do single
level dictionary learning that is one
way of learning or recording the data
but I don't think there is a broad
general kind of impetus beyond the ICL
our conference what beyond what that
conference collects but I may be wrong
we should probably wrap up I think we
originally planned to have two sections
to this panel I think me take a second
section for the next Q&amp;amp;A slot otherwise
we'll probably overrun too so any
controlling concluding government
wishing be planning the best of luck in
the future
well I think my conclusion from this is
I guess pretty much what we started with
that data representation is important
but in data itself that's why is
important and finding good models
automatically will remain a challenge
and but we are interested in all of
those things and contrary to what
somebody may believe the panel is not
criticizing deep learning but as persons
with scientific interest whenever you
observe something that works
you should be curious enough to ask why
and if you manage to even get 10% of an
understanding why you may discover other
phenomena which may help you solve it
even better than what's out there so
that's actually the viewpoint I think
most of us probably here hold about deep
learning yeah pretty much
I think I agree with that but I think we
can maybe also take from this is that
it's important to really work with the
data and the other thing is it's
important to look out and look for
different maybe mathematical concepts
and different techniques that we could
apply and and bring into the field to
make things work better I have more
accurate analysis so I'll actually
conclude with so through the penetration
I felt that the panel raised like sort
of two big questions which I feel like
you know are worth you know whole thesis
itself so one thing was what Jason was
pointing out that we do not understand
and symbol methods at all but we know
that they work very well so for example
these techniques like render forests
work amazingly well but currently we
have very very little knowledge of why
they work so that will be like you know
one thesis question that can you take
very simple randomized tree models or
randomized forest models and show that
they will perform better than their
parts that is each like if you have each
of these trees which are not very
accurate but somehow when you combine
them they perform much better I think
that say
open question in the area and it's worth
thesis and the second thing is what sort
of sovath was pointing out that we have
this you know noodle networks and deep
networks and we as a sorry how could you
name but someone pointed out that the
model is reasonably clear but what is
not clear is how to train it and
currently the theoretical work on deep
learning has mostly focused on coming up
with different models and very different
type of algorithms to train those models
those algorithms are not very practical
or they are not used by you know
practitioners who are using deep
networks so one question would be if you
can take these practical algorithms like
the algorithm that you they work under
very specific models and under lot of
data assumptions that is fine but if you
can show that those exact algorithms
work and they achieve some sort of
convergence some sort of global Optima
some sort of rate of convergence some
like you know theoretical notions that
we understand well that also I think
will be amazing service to the field
both to like you know more whatever
theoretically oriented people and also
to deep learning in general or to a lot
of applications as well so that's
another you know big open problem I feel
which came through this panel discussion
so I guess we should conclude so thanks
a lot for asking amazing questions and
thanks to the panel each year Microsoft
Research helps hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>