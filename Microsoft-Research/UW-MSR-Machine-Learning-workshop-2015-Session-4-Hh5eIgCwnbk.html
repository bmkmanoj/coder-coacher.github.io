<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UW - MSR Machine Learning workshop 2015 - Session 4 | Coder Coacher - Coaching Coders</title><meta content="UW - MSR Machine Learning workshop 2015 - Session 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>UW - MSR Machine Learning workshop 2015 - Session 4</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hh5eIgCwnbk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
excellent I'm Larry so today what I want
to talk about is this problem of image
captioning so given an image can you
generate a caption describing that image
and this is one of those problems that
recently has really captured the
imagination both of the public and of
researchers there's a new york's time
times article a few months ago that
probably some of you might have read and
if you think about the problem I mean
why is it interesting I mean as far as
like you know for me as a researcher
what I think is cool about it is it it
is essentially a I complete in order to
come up with a good description for an
image you not only need to understand
what's in the image but you need to be
able to understand the world in which
the image exists to come up with a
description which is compelling to the
human and then from the public's
perspective what's nice about it is
something that's easily describable you
know I can go to my mom and I could say
she says you know Larry what are you
working on now and I can say describing
images and she's like got it so there's
something nice about that any graduate
students kind of gravitate towards that
thing too because they're like oh I
understand that that makes sense I know
why I'd want to work on that problem so
today I'm going to use as this problem
kind of motivate a couple different
things talking about common sense
talking about vision in language as well
so as I said there's been a huge amount
of work in this space recently just in
the last 3-4 months seven papers popped
up on archive around November time frame
with the icml deadline just a couple
weeks ago I think there's another two
three papers I popped up I don't have
them on the list here but it's been a
huge amount of work in this space and
just to give you an idea for you know
there's type of captions that these
algorithms can generate let me just give
you a couple examples so when I already
showed you but here's another one a man
with a colorful umbrella walking down
the street if you look at this caption
and if you're at all familiar with
computer vision this would just blow you
away I mean to you know three to four
years ago you would never be able to get
a realistic caption like this and you
look at this a man we recognize man
colorful umbrella it's awesome to the
street he's walking down I mean is it's
indistinguishable from what a human
would write here's another caption a
train traveling down train tracks next
to a train station
grammatically okay a little iffy but not
bad and it's traveling you might if you
look really close you can see the bumper
there it's probably not traveling but
you know it's pretty close you know is
again as computer science people we give
it a break we're like that's we were
impressed we have this one a herd of
drafts walked down the street in the
middle of some trees you know again for
my mom she'd be like you should be
laughing at me but again it's computer
scientist you're kind of like that's not
too bad you know I can see the two
giraffes I can see the trees you know
the walking there's a street you know
you kind of get it you know blur your
vision you know as Jeff it and would say
and of course you know it wouldn't be a
computer vision it wouldn't be you know
a I if we didn't have you know a dog
sitting in front of a window you know
the complete failure cases yeah so but
you know it kind of works sometimes and
that's that's what about it that's what
has this you know impressed so let me
just what do we do is go off on a little
bit of a brief tangent right now and
what I want to do is just described one
of the algorithms that recently came out
one of the ones that I'm involved in
that I find particularly interesting I
really like the model in this one so
this is work that I did wish inlay Chen
who is a student at CMU and he was an
intern with me last spring so let me
just read a sentence for you a girl and
I'm wearing this more to the sentence
but what's interesting is when you read
a sentence is that you have a tendency
to especially a descriptive sentence
visualize what the sentence is saying as
you're reading it so if you meet a girl
you may picture a girl and you say a
girl and boy you picture a girl on a boy
they knocked you know now you're kind of
picturing a boy and a girl knocking on
the door and then you can say down the
tower then you might picture you a boy
and girl you know knocking down the
tower and you know whenever we read a
book you know or just you know any of
these kind of descriptive sentences this
happens automatically you know part of
our understanding of the sentence is
this being able to take that sentence
and it's you know it form and
translating it into the visual
incarnation of what it's actually trying
to describe right now let's go and look
at you know kind of our simple recurrent
neural network and it did come basically
the most naive way that you would set up
a network to generate a sentence from a
bunch of visual features so here you
have an image let's say and you have
blue it's the visual features as input
you have some sort of hidden state i'm
showing the current neural network here
unrolled so you have that your hidden
state here is the s's and then you have
the words that you generate which are
the w's and what happens in a network
like this is the s's or the the s head
in state basically just encodes the last
few words that you stated it's not
encoding anything visual it's not
encoding you know a visual
representation of the scene and what
happens in a network like this is it
generally has kind of a short-term
memory has a tendency to repeat itself
it does kind of Babel in a way that's
vaguely reminiscent of the image but it
doesn't give you what you want so we
decided to do was to build a model that
looks like this very similar but we just
add one layer up on top so now we're
going to take the visual features we're
going to then try to produce a sentence
but from the sentence we're then going
to try to reconstruct the visual
features and this hidden layer you would
it essentially have to do is take the
words as they're coming in and try to
encode the visual scene as it goes from
one word to the next and without doing
anything special like the lstm the long
short term memory networks that are also
popular in this space we can actually
get long-term visual memory just using a
standard standard wicker network and
then if we want to do sentence oh you
use a long term memory so then if we
just want to generate a sentence we can
chop off the top and we have the visual
features come in and we have the w's and
notice that the long-term visual memory
is still used to help generate the
sentence because the you basically says
the visual part of the scene that I've
described so far you can look at V which
is a part of the visual scene in which I
haven't described yet you look at the
difference of the two and I can tell you
what to describe in the future and then
if all you want to do is create a visual
scene actually compute the visual
features given a sentence you just use
the top part of network and you don't
have to actually have the bottom and as
far as accuracy what was surprising is
that twenty percent of the captions that
this can generate where you judge to be
equal to or better than human captions
generated on the same images and twenty
percent is a pretty good number and if
you look at the other papers that I
showed you most of them are actually
hovering right around the twenty percent
to I think another
one from MSR got a bit better i think it
was like twenty three percent but
they're all kind of around that twenty
percent range and like I said that's you
know that kind of perked everybody act
like wow this is a task it might be kind
of doable but I don't want to talk about
the successes in this talk actually I
actually want to talk about limitations
in this talk because I think there's
been a lot of hype a lot of hoopla
around these algorithms and about how
great they are and I think there's this
perception out there a little bit that
this problem is I don't want to say
solved but you know we made huge
progress but I want to talk about today
is some fundamental limitations that the
current pipelines that we're using have
and actually dealing with this problem
so in order to talk about these
limitations let me just describe to you
the space of all images alone two
dimensions so one is boring the
interesting images which are boring
images which are interesting and we have
images which are common in images which
are unusual so you can take images which
are interesting and unusual like you
know cats karate chopping people flying
through the air you can find images
which are common and interesting so
these you know things that individual
find interesting like pictures of their
kids let's say we have images which are
boring and common like this one believe
it or not somebody still uploaded this
to Flickr you know I don't know why you
know I guess wanted I didn't I need to
know the background story to this one
because I like why would you bother all
right so the question might be okay we
have Willie this whole space where does
our data set come from where our
training data set where is that coming
from well let's look at the training
data set most people using right now so
for this task the data set that most
people using is the Microsoft Coco data
set and this is a collaboration between
people at Microsoft and people all
throughout academia and under industry
places like facebook and the main
purpose of this data set was to take
images like these which have a lot of
objects in contextual relationships
where these scenes are actually kind of
confusing and realistic and try to
identify the different objects in them
and second segment them out and the data
set has a t800 object categories tons of
images on tons of object instances every
instance is segmented the huge amount of
effort it took
it's essentially seven years or eight
years of somebody working full time
around the clock to generate this data
set the equivalent of that on Mechanical
Turk but one thing that we did is we
just happen to add five sentences per
image and it's at data that this
community is really taken off on so now
we have even majan at 300,000 images we
have one and a half million captions so
what are these images come from where
they came from Flickr where do Flickr
images exist Flickr images exist here
they're going to be relatively common
because you have to actually be around
to take the pictures and yet to be
interesting but you wouldn't want to
bother to upload to flickr in the first
place right so let's look at the
consequence of the images kind of being
in that space so let's look at another
great caption generated by an algorithm
amen riding a wave on a surfboard in the
water again you're like wow this is the
amazing caption we recognize the
surfboard the man there's a way if he's
riding it you got to think of all the
information you have to recognize in
order to get this caption right well
let's just look go back to the cocoa
data set and look for images which have
a surfboard in them what do we find this
image caption works pretty well for that
image this one yeah pretty well for that
one to this one not so much close you
know they get kayakers you might mention
no no this one just fine so what you
find you when you look at the data set
you begin to question your assumption
about how intelligence some of these
algorithms are are they actually
recognizing the surfboard and the wave
and everything or they just recognizing
that that is a scene which you can take
this entire caption and just kind of
splat it on and it'll work let's do
another example a draft standing in the
grass next retreat again this is an
amazing caption you get the draft you
have the fact that it's standing on the
crash you had to recognize of grass and
draft is standing on and it's next to a
tree at the understand next entry you
know like amount of information you'd
understand seems really amazing but
let's look up the word giraffe okay so
the caption won't go really well with
that one here that would work just fine
here they work okay no no yes yes yes
you see what I mean so here again you
might just recognize when there's a
draft the draft is probably standing
on the grass and it's probably next to a
tree because most drafts are standing on
the grass you know so it's not as
amazing as you might think all right
well I know I know I know one thing I
don't want to do is give me the
impression that everything is just kind
of gaming it in this way there's other
so you take this caption here now this
caption ones that automatically
generated this is one I just made up but
you look at this image a man walking his
dog in the park you might ask yourself
okay what if I look up dog and it seems
like a caption I'd be fairly common how
do we get you know what if we look at
random images with dog in them well here
no it's very cute but doesn't fit no no
no dad you probably would mention the
sandals definitely not so it's not all
categories that does this work with and
I don't think it's quite a coincidence
that if you look at the you know the
good results from these papers that
there's not many dog images in them you
know it is more the draft the surfing
that sort of thing all right so what we
notice is that when you have high image
density these algorithms of doing pretty
well and maybe it is happens to be that
twenty percent of the data is of this
type and that's why we're doing well in
that twenty percent and it's kind of in
this space art here let's talk about why
we're not doing good in the other parts
of this space what about here what about
boring and common images so why are they
hard well when we were trying to create
the cocoa data set one of the object
categories we wanted to have was a hair
dryer the problem is is we wanted 10,000
examples of these objects being used in
real life in like natural situations and
just so happens you can't get 10,000
images of people using hair dryers on
the web they don't exist it doesn't take
too much imagination to figure out why
you know I mean this one yeah it is not
your flattering same thing with like
pumping gas you cannot find a picture of
somebody pumping gas online it just
doesn't it doesn't exist I don't know
why I mean we pump gas every week right
we're if we're Americans you know proper
Americans we pump gas all the time but
we never take pictures or ourselves
actually pumping gas all right so
basically a standard doesn't exist it's
too boring nobody wants to take pictures
of these things so we just don't have
the data it's not uploaded to Flickr and
if it doesn't exist on flickr we don't
have it you could have google glass but
you know it doesn't seem to be turning
out as
people would hope so yeah all right what
about this other area unusual and
interesting images no this one crazy
super climbing a giraffe to get a better
view and you think okay so why doesn't
this image work well okay for your
computer vision person you're not gonna
be there exists literally two examples
of zebras climbing on the web what this
what this one here that you're currently
looking at another one of a zebra in a
tree climbing it's not enough training
examples to actually learn what a
climbing zebra actually looks like so we
can't get at that and the other thing is
the language model is going to be
screaming please do not say zebra
climbing climbing never ever ever ever
ever occurs up there zebra it just
doesn't happen and it's going to be
swimming just don't do it don't do it
don't do it it's going to force you to
say something else so our language
models aren't allowing us to do as a
computer vision algorithms can't
recognize it so we're toast let's look
at another example amen is rescued from
this truck that is hanging dangerously
from the bridge now if you look at this
image I love this image because if you
look at it it's it's a single snapshot
in time right yet this image is
describing a whole a sequence of
probably you know over a three hour time
sequence right and we I think everybody
in this audience right now could
describe what happened in those three
hours and our stories would be very
consistent if this image is just of a
single particular instance right that's
why this image is so awesome that's why
one all sorts of prizes when it was
photographed if you look at nord rescued
think about the amount of information
you need to understand that come up with
a word rescued even though i think
everyone again in this audience would
probably use a word rescued when
describing this image but coming up with
an algorithm a computer algorithm which
would actually know about best you know
that this truck is about default know
that the driver used to be in the truck
knowing that the truck fell the driver
would probably die and that the truck
now they drive the driver even though
he's hanging you know in the middle of
thin air if being rescued you know that
takes a lot of reasoning all right so
how can we go from here which is where
we're kind of doing good to this entire
space and what I just mentioned here is
it's well no problem i'm not sure has
anybody seen this video
it's a pretty if anybody interested in
computer vision to watch this video is
awesome it's a made-up word called Vemma
dahlin which has the made-up meaning of
the frustration of photographer de
graphing something amazing when
thousands of identical photos already
exist I think we've all had this feeling
you know you get a really awesome photo
you take a picture of it and you take it
back home and you're like huh but this
looks like every other rainbow picture
that I've ever seen in my life you know
it doesn't have this we all want to take
the same pictures yeah see I mean we've
taken that picture i know you have you
know even even painters you know 500
years ago whatever they're like the same
thing so I mean as far as computer
vision researchers or machine learning
researchers what matters is that this is
you think of it as the fear that our
data sets will never have a diversity
that we desire right we're never going
to sample the world and the way that we
want it sampled and if you can't sample
the role that in the way we want to say
about how we actually going to get
training data so this brings up the
question is photorealism necessary so do
we actually when we want to learn about
a world do we need two views real images
what if instead what if we used cartoon
images what if we tried to create a
world this guy here with let's say a
bunch of his friends and they had a
bunch of things that they can interact
with and all sorts of animals that can
do all sorts of crazy kind of wacky
things and let's say we gave it to
mechanical truckers all these pieces of
clipart they could just drag and drop
and create any type of scenes that they
wanted to just like a nice children's
interface tons of tons of fun to play
with what do they create they create an
amazing diversity of scenes they create
really boring scenes that create really
interesting scenes they carry it really
funny scenes it's just basically across
the board and this is just showing a
small subset from living room scenes
that we are in dining room scenes that
we are creating we also have outdoor
scenes we have street scenes we have all
sorts of different things and since it's
fun it's pretty easy to collect then we
can get sentences for these scenes so
here's this is an older clipart dataset
but if you look at the sentence I love
this my trips on his way to catch a
frisbee jenny is frustrated with his in
competence
you know this is incredibly simple scene
but he'd think about trying to come up
with a sentence you know try to come up
with an algorithm that realizes that
that Mike is pathetic at catching a
frisbee because he fell down and
therefore he's incompetent that's why
Jenny's frustrated like that's a hard
research problem another one a cat says
anxiously in the park instead that
unintended hot dog that someone left on
a yellow bench I mean what does it mean
to you know be unattended what does it
mean to be anxious why is it cat anxious
you know etc this is it's it's simple
visuals but complex meaning and allows
us to study this now one thing you might
be thinking is but Larry like how do we
how are we going to get enough training
zip that's how we're going to learn what
it means to be incompetent how are we
going to learn what unintended means or
you know anxiously well that's a cool
thing about clipart you can go from a
clip art you can go to a sentence from
that sentence you go back Mechanical
Turk and say generate a whole bunch of
scenes which have this meaning which
have this that this sentence depicts
like this so let me just give you a
quick example we give the mechanical
trickers to send this mike fights off a
bear by giving him a hot dog well Chani
runs away these are different scenes
that independent mechanical truckers
made and you can get we can understand
what it means to give your hot dog to
somebody you can understand what it
means to fight off a bear you know what
it means for Jenny to run away etc
here's another example I'm not showing
you the sentence and I don't want to run
over on time but i'll just give you the
sentence jenny and mike are both playing
dangerously in the park right and you
look at how the me the creativity that
mechanical turkish have and playing
dangerously is awesome you get throw a
baseball at a bear you can kick ass take
you can jump off a bench you can launch
rockets off etc and from this data set
you can learn all sorts of interesting
things you can learn what it means that
run away from that if you're running
away from something that thing you're
running away from its behind you and
then the thing that's running towards
you is facing you you can learn concepts
for things that you don't necessarily
think of as visual i mean you don't
think I've want is visual but if you
want something it means that you're
probably looking at it and we can
actually learn that when you want
something you look at it we can do other
interesting things we can take the week
and we can try to learn these concepts
of just dancing with and wrestling with
or running to and running from and it's
hard to collect images which convey
let's say running two verses running
from right but we can do is in good
mechanical turk and give them
essentially it's paper doll
and you there allow them kind of move
their joints around in any sort of way
and basically they create this large
variety of different poses we can learn
from this and try to apply it on real
images to learn what these different
concepts mean it's amazing the
creativity here is again never
underestimate a mechanical turk oh they
just do an amazing job the fighting
scenes are not priceless I mean I don't
want to show them there's some other
like graphic but they're awesome the
wrestling was bad enough and then you
can apply it to real images and then one
of the things that's important it like
rescued when you think a rescue rescue
has a very specific temporal meaning you
know recipe doesn't exist in a specific
time it actually involves overtime so
another thing you can do with clip art
is you can actually get Turkish to
generate sequences of scenes so you can
learn what happens through time so you
know Jenny Mike they have a hamburger
and then a bear shows up they both run
away the berries a hamburger there's a
lot of things you can learn from this so
for instance then if you have a scene
like this and you say which waste any
going to move she can you can say oh
Jenny is more likely move forward at
bearish shows up in the scene jenny is
more likely to move backward good stuff
so just in conclusion is i think you
know a lot of times in especially in
computer vision in machine learning in
general we have this tendency to kind of
think of these real-world datasets and
have this kind of bias towards a certain
kind of style of training and i think
it's important especially for a lot of
these things to kind of open up our
minds especially when we start going to
more realistic applications things where
you want to learn common sense knowledge
even as humans you know we don't learn
things firsthand I don't know that I'll
beaten by a bear because I was eaten by
a bear right I learned that through
secondhand knowledge and there's a lot
of ways to gain a lot of knowledge and
probably much more efficiently if we do
it through other means we do it through
playing we do it through you know
basically people showing us perhaps
through clipart through these other
modalities and then from a computer
vision perspective I think it's really
interesting because what's happening is
as we start thinking about semantics and
we start the thing about semantics in
conjunction with computer vision we
realize that we can't really think of
ourselves as computer vision researchers
anymore but really need to start taking
inverse
Moore's AI researchers I think it's a
really interesting time right now to be
in this space right Thanks questions
very fun talk by the way I'm curious to
know what did your algorithm label that
image of a zebra climbing is you have I
you know I I haven't gone back to
actually try but I'm actually kind of
curious as well be honest with you I bet
you were to say a draft standing on the
grass next to a tree if I had to make it
guess it's another question back there
oh regarding the cartoon images and
learning from them if we do a really
good job in identifying a bear and in a
cartoon and then identifying real bad
and if you are able to make that
Association and then we have a
linguistic model wherein we are able to
do an association to say bear is a
dangerous thing we would still perform
as as well as a as the current research
is going on right like why is it that
you know there is a special reason so as
to why we have to learn from you know
cartoons alone as opposed to problem is
just a you think of em as a pro manic
reason which is it's easier to sample
the world through cartoons and then
generate this data through cartoons then
it is to get the data itself from the
real world because we can base it like
ice mentioned we can actually go like
this say that's a part of the world we
don't know much about we don't know much
about pumping gas because nobody takes
pictures of immune gas or you know
videos of people pumping gas we can act
that out in cartoons and get that data
or things that are unusual we can act
that out so I think it gives us a way of
exploring the world in a way that we
want to explore it without having to go
and get the real data because getting
real data is well it's really tough it's
sort of like simulating the yeah totally
choked and also makes a lot of the
learning problems easier we do like this
way you don't have to focus on like the
computer
your problems you know you don't have to
sit there and think okay what you know I
you know classification algorithms I
want I know computer vision right now is
doing awesome relatively to what it was
doing you know three to four years ago
but it still has a long way to go you
know and this allows us to completely
bypass all those issues with computer
vision not working that low and just
really kind of study those higher-level
problems so okay one more question isn't
it true that you're still facing right
there still facing a sort of a problem
with sampling because there are so many
combinations of objects and how many how
much can you really label yes I the
interesting thing there is when you
start thinking this is something that
we're just beginning and just beginning
the work on right now but when you have
active learning you know is when we're
not just going out to you know the
mechanical Tarkas and say generate a
million scenes for us and we'll learn
from it but when you generate a small
number scenes we learn something from we
go back Google out and we ask more
questions right and you get this active
learning framework I think then that
allows you to kind of explore this space
in a cost-efficient manner you know and
they still need to kind of like you know
kind of have a couple random samples you
can kind of you know discover a new
space that you hadn't explored before
but it does allow you to kind of fully
fill out the space you know and in this
way so I think that's kind of where I'm
thinking that is is the most compelling
it in a direction to go in for that
because otherwise you have the same bias
as a photographer bias that I mentioned
before yeah and also the clip art that
we give people is randomized every time
and that helps a lot with creating a lot
of randomness great okay thank you very
much let's think
okay so the next speaker is a professor
bill noble bill is a professor in the
genome sciences department the
University of Washington and actually
this is given how long I've known bill
this is the first time I've actually
introduced him in a talk and I can say
firsthand that I think bill is one of
the most creative researchers I know
who's working on the intersection of
biology and machine learning so I'm
looking forward to hearing what he has
to say so please join me in welcoming
nope thanks Jeff so I want to do a
little proselytizing today obviously
given my affiliation in my title I work
on the biological end of machine
learning and i hope to tell you about
some of the problems that we're working
on and give you a flavor of some of the
exciting directions that are going on in
the field both of genomics and
proteomics so in order to do this I
think I have to give you a little bit of
background sort of a primer on the
basics of the of the biology we're going
to talk about so this is a picture that
I grabbed from the web but it's it's
portrays what's commonly known as the
central dogma of molecular biology which
is that biological function is encoded
in your DNA and carried out by proteins
so you can see DNA and proteins here
both of these are strings drawn from
discrete alphabets so DNA is a string of
length 3.1 billion bases in each of your
cells drawn from a four-letter alphabet
proteins are strings of amino acids at
alphabet of size 20 and they're usually
about 50 to a couple thousand letters
long and what one of the things that
we're interested in is characterizing
how these molecules encode information
and get jobs done in your cell
essentially allow you and all other
living things to survive so I'm going to
tell you a little bit about genomics
which is the DNA end of things a method
called semi-automated genome annotation
and three two three dimensional
structure and inference and then also a
little bit about proteomics as well so
my way background for the genomics end
of things
the if you look at DNA in a cell there
are a lot of interesting questions we
can ask about it you've all presumably
heard of the Human Genome Project which
was completed over a decade ago which
consisted of a saying or measuring all
of the DNA bases in the a single human
genome but there are a lot of other
kinds of measurements that we can do
about the DNA so for example regions of
the DNA that that encode for a
particular gene get transcribed when
they're going to produce a protein and
you can measure that transcription of
that gene you can also measure where
particular proteins come and bind to the
DNA in order to get various functional
process is done you can also measure
sort of how the DNA is packed locally
along the DNA sequence and all of these
measurements are done with a variety of
different experiments and they're
generated in very large scale so you
might have so each experiment in
principle generates 3.1 billion numbers
along the genome and you might have
fifty or a hundred or more experimental
measurements per position in the genome
so there's a large consortium called
encode which is nih-funded consortium
stands for the Encyclopedia of DNA
elements that's generated on the order
of five or six thousand of these kinds
of experimental data sets and one of the
things that we've done in the context of
that consortium is developed
semi-supervised door actually
unsupervised machine learning approach
to try to make sense out of this big
heterogeneous collection of data the
basic idea is to try to simultaneously
partition and label the the genome
itself on the basis of all of these
different types of data in such a way
that if two segments of the genome
received the same label then they have
similar characteristics in all the
associated data as you can imagine you
can solve this using essentially a
hidden Markov model or more generally a
dynamic bayesian network where each of
the observations is one of the
experimental measurements along the
genome and what is usually thought of as
the time axis in this context is
actually the genomic position along the
genome
in practice of course we don't use just
a simple hmm we have to handle things
like for instance missingness there are
a lot of positions in the genome that
are more difficult to measure and so you
have a lot of missing values and you
also have to handle the expected length
distributions of the various kinds of
segments but in the end you have this
unsupervised system which you then i
forgot to say you run this system and
eft afterwards it's a signed integer
labels and then a human comes along and
assigns a semantic interpretation to
those labels so that's the sense in
which it's a semi-automated procedure so
we can use this to do things like
rediscover the fact that the genome
contains genes so what you're looking at
here along the bottom are annotations of
known genes in the human genome and
along the top is a representation of our
labeling where each row corresponds to
one label we've signed assigned
something like 25 different labels to
different positions here and the key
phenomenon to observe is that you have a
subset of labels that are associated
with sort of the beginning of genes the
middle of jeans and the end of jeans and
those same patterns recur over and over
again so you can say okay these cluster
of labels are the things that tell us
where a gene is starting in the genome
obviously knowing about genes is not
something that came that those new from
this algorithm but you can also use it
for doing things in addition to
annotation it's helpful for
visualization as biologists try to
interpret their data with respect to all
the other data available and also
identifying novel instances of elements
and novel classes of functional elements
in the human genome now one of the
things that's that we've been thinking
about a lot recently is the fact that
the DNA sequence is itself not the full
story so it turns out that everyone in
your cells contains 3.1 billion bases
but if you take that DNA and stretch it
out it's two meters long and that is
folded up inside each cell in your body
and the way in which it's folded turns
out to be important for function so this
is a representation of a particular
genome or each color represents a
chromosome
and we know that the structure the shape
of the DNA in the nucleus effects for
example how genes get expressed how the
proteins get translated from the DNA and
also how the DNA replicates and we
believe it's probably important for
other biological processes as well so it
turns out that recently a variety of
groups including some at the University
of Washington have developed
experimental methods that allow us to
start measuring DNA structure in a
high-throughput fashion so the basic
idea is you run an experimental assay
called hi-c and that assay outputs a
matrix in which it's a in principle 3.1
billion bases by 3.1 billion although in
practice we don't work at that
resolution you might have maybe twenty
thousand or 40,000 basis for each entry
in the matrix and the values in the
matrix represent counts of how
frequently in a population of cells
those two DNA positions are close
together in Euclidean space okay so from
that matrix there is an inference
problem which is it's sort of a analog
of the protein folding problem given all
of these contacts can we infer the
three-dimensional structure of the
genome so this is the first such
structure that that we produce this is
the structure of the yeast genome so
there are 16 different chromosomes
colored here you can see things like for
instance this is the nucleolus which is
a particular structure on chromosome 12
the white circle here is the clustering
of a particular kind of element called a
centromere this was really the first
time that we were able to visual
visualize a full genome in this level of
detail and the this kind of approach can
also allow us to do things like this
this is the genome of plasmodium which
is the parasite responsible for the most
lethal form of malaria we've measured
its structure at three different time
points during the red blood cell cycle
of the parasite and then interpolated
between those three different
confirmations and what you can see is
the sort of an expansion of the nucleus
and contraction
as it gets ready for egress from the red
blood cell what we're hoping for and
what we've shown already to some extent
is that the the process is involved in
virulence of this particular parasite
are structurally occurring close
together in the in the three-dimensional
space so there's a problem here if you
go back and think about this task that
we set ourselves in the context of the
encode consortium we were doing this
semi automated genome annotation where
we have all of these tracks of data and
now someone comes along and says well I
don't have a single vector from this
experiment that's three billion bases
long I've got a matrix and how am I
going to put that into this semi
automated genome annotation procedure so
one idea is that we can use the matrix
as a way to sort of encourage pairs of
low sigh to receive the same label so
the idea is if you're in a part of the
genome where a lot of the genes are
turned on a part of the nucleus excuse
me where a lot of the genes are turned
on and you're close to some other gene
or some other place that's also it's in
that same area it's also going to happen
receive the same functional label so
we've developed a procedure this is in
collaboration with Jeff builds where we
essentially extend the dynamic bayesian
network to use a posterior base a
graph-based regularization so the idea
is that if you're doing standard am you
can formulate standard e.m as doing a
sort of minimizing a KL divergence
between the model and the posterior
'he's write the output posteriors and
then we add an additional term in which
we're minimizing the KL divergence
between the posterior zhis oh ciated
with two different positions in the
genome and the strength of that
connection is weighted by the graph edge
so if we if we think these are very
close there we really want their KL
divergence to be small and I'm not going
to go through the details here but you
can actually show using external data
that wasn't used in the learning
procedure that when you use just
our procedure which the software is
called Segway if you do it alone with
ignoring this hi-c data or if you give a
very strong weight to the hi-c data then
you do a fairly a rip sorry a relatively
poor job of characterizing this external
data there's replication timing data but
in the middle of the curve here things
get a lot better when you're using the
regularization in a smart way and then
finally one other thing that we've just
published recently is an analysis of the
human genome across a lot of different
types of tissues on the basis of many
different data sets from encode and sort
of characterized the large-scale
structure of the genome into different
classes of what are called domains
previously there were a variety of
different kinds of domains characterized
these quiescent domains where not much
is going on and then two different kinds
of domains that are sort of shut down in
a constitutive or facultative way and
those were already described in the
literature one of the things that we
were able to pull out using this kind of
analysis is a subdivision in the active
portions of the genome between what
we're calling specific and broad
activity so broad being these are
regions that in any kind of cell whether
it's a liver cell or a skin cell or a
brain cell those are active in the
genome versus specific which are really
specific activity for a particular
tissue or cell type so I also want to
tell you a little bit about proteomics
as i mentioned proteins are also have
interesting structure these are just
images showing protein structure but the
high-throughput technology that we focus
on a lot in in my lab anyway is called
mass spectrometry mass spectrometry is a
way to essentially characterize what
proteins are present in a complex sample
and it operates not on complete proteins
but on digested proteins essentially
substrings of the protein that are we
call peptides peptides go into a machine
like this this is a machine in Mike
Mikasa slab in our department and an
output is a bunch of spectra each
observation is a spectrum in which
one axis is the mass-to-charge ratio of
the fragments and the vertical axis is
the intensity of those fragments the
canonical problem that we want to solve
here you can frame it like this the mass
spec devices does a good job of getting
a homogeneous population of peptides
sending it into the device and producing
one spectrum and the job of the computer
is to try to figure out what was the
corresponding sequence that was
responsible for generating that
particular observation it's a somewhat
difficult problem because you
essentially the spectrum consists of two
different time series that are overlaid
on top of each other essentially
corresponding to suffixes and prefixes
of the of the sequence so in this
particular picture all these dark blue
lines are ones that correspond to
fragmentations that each fragment can
produce as two pieces and those each
make a different dark blue line here and
then of course as you can see there are
a lot of other excuse me a lot of other
gray lines there which are maybe other
peptides that are Co eluding that are
present at the same time or other kinds
of noise so what we've been the norm in
the field is to use relatively ad hoc
scoring functions to to do this kind of
assignment and what we've been
developing is a more principled dynamic
bayesian network framework I'm not going
to tell you the details of how the model
works but the key idea is that we're
trying to capture prior knowledge about
how the peptides actually fragment
inside the mass spectrometer we're in
this case the time domain of the dynamic
bayesian network is the mass to charge
ratio of the fragments and so that's
what this model shows essentially what
you end up doing is you say we have a
spectrum along the bottom an observed
spectrum you have a sort of theoretical
spectrum for a given peptide along the
top and then the the dynamic programming
of the DBN procedure essentially does a
matching between those two and tries to
minimize the number of insertions and
deletions and your task essentially is
to choose the right peptide to generate
an observed spectrum and we've shown as
we compare this
to a variety of existing methods in the
literature that at least on some data
sets this approach works very well one
of the challenges here is that the data
changes pretty dramatically as different
kinds of instruments are used and so one
of one of our hopes is that by using a
learning framework we could have this
our learning system automatically adjust
to say a new platform a new machine
platform or a new experimental approach
whereas some of the other methods may be
more brittle as we as the technology
advances the other thing that we're
working on right now is generalizing
this too instead of so i should say any
particular mass spectrometry experiment
is typically generating on the order of
10 or 20 spectra per second over the
course of maybe a half hour or an hour
so you get thousands and thousands of
spectra generated from a single sample
maybe from a single drop of blood and so
far this the normal way to analyze this
data is to treat each one of those
spectra completely independently and try
to figure out okay what generated this
one and what generated this one so we
would like to instead take into account
the the time domain associated with this
kind of experiment so now you can think
of all of the observed spectra has one
big matrix all of the theoretical
spectra as another matrix and what we're
trying to infer is the abundance of
those peptides over time so this kind of
joint inference you can frame as a sort
of basic least squares regression you
can add in a regularization term to
encourage sparsity because we know that
over the half hour period any given
peptide probably Lutz for maybe between
five and thirty seconds so most of the
time that peptide should not be there
and then we also know that actually when
the peptide goes through the mass spec
device it has a characteristic trend
right it should become more intense and
then fade off and so we can actually
encode that in a sort of by tonic
elution profile penalty sort of like an
isotonic regression but with two
directions on the
isotonic penalty and this is what we're
currently working on what we then want
to do after that is working is to
generalize this to the the complete case
where instead of just trying to say here
are the peptide sequences that were
responsible for generating these
peptides we can also take into account
the fact that we know that these protein
sequences were originally broken up into
peptides and so for instance if you see
a protein consists of five peptides and
we have strong evidence that these four
peptides are there we should be more
likely to believe that the fifth one is
also there so we have now this two
levels of many-to-many mappings from the
observed spectra to the peptides to the
proteins where you have many thousands
of examples at each level and you'd like
to be able to do a joint inference over
all of this data at once and of course
so for instance my one collaborator has
seven I think seven five or seven
different mass spec devices running
essentially 24 hours a day so they have
tons and tons of data and so we need to
be able to do this in an efficient
fashion as well because that's just data
from one machine from one half hour or
one hour experiment so we've got a lot
of work set out for us there are a lot
of people to acknowledge here I've
highlighted in red the names of the
people who are here in the audience that
can tell you more about it and I'm happy
to tell you about it as well and I'm
happy to take questions
time for questions
just what
so I have very limited knowledge in
biology but we were learning to about
the double helix structure and what we
showed here today are much more complex
structures of the DNA can you explain
how these two relate to each other yeah
it's a question of scale so the double
helix structure you know the each turn
of the helix is about 10.3 or 10.5 base
pairs and then that helix itself makes
another larger helix and and and
actually that helix then gets you know
bound up in many other structures so
when we look at it from hi-c data we're
looking at the order of you know forty
thousand base pairs per sort of bead in
the optimization so the the single helix
structure is much smaller than that
there is actually there are actually
lots of interesting problems about
inferring local structure of DNA as well
and that's a separate problem so I have
a question um I mean it's actually based
on what you were saying at the beginning
was that the DNA it's something that if
you were to fully unroll and it would be
a two meter long string which then is
bundled up together into this 3d
structure which is this tiny tiny 3d
region and I guess like just you know
from the perspective of real world
actual objects if I were to take the
string that's 3 3 2 meters long and to
bunch it up I mean very very quickly it
would get you know very complicated Lee
entangled into lots of knots and all
sorts of other things which would make
it very very difficult to sort of even
sequence through the string and do
anything with that string and I guess
the question is how is it possible that
especially in a DNA sequence where the
actual width is so small when you've
bunch this stuff together that it just
doesn't impenetrably become entangled
and inaccessible so it's a great
question and the short answer is we
don't really fully understand how but
one thing to keep in mind is that that
string was it's actually you know 2246
different strings but they all have to
be copied and so there's a lot of
proteins a lot of machinery in the cell
for
sort of keeping track of the DNA and
separating it out in such a way that you
can make a complete and faithful
replication of all that DNA so that
machinery is partly is responsible for
making sure that these things fold into
some pattern that is then then can be
unfolded and replicated again but
there's a it's a much more complicated
story than that and we don't know the
full story yeah when you think about the
mechanics it's it's it's much more
complicated than any deep model you
could imagine for example sorry Lee like
deep wherever you are are there other
questions there's one over there Jeff
behind you over there oh oh sorry you
need the microphone
yeah so when I was at Johns Hopkins
there was some work on synthetic
genomics so trying to create like a
minimal version of the East genome and I
don't know if any of this would be like
practical but would there be any way to
maybe like have like a hand designed
genome that gives you like more relevant
data that you can train on so something
like an active learning it's an
interesting so hand-designed gym to
train on you mean for the structural
inference or for which or any of those
problems is that what you're thinking
yeah just in general if you could get
like a particular genome that you might
think would provide better training data
for a model yeah right right I'll of
conscience influence right i mean it is
true that one of the things that limits
us is that we can only look at the
genomes that exist in nature and
obviously if you you know that might be
one quadrant of the you know the
presumably the interesting not the
boring part but maybe the common part
and so getting other ones would be would
probably be quite helpful the question I
guess is if they're not compatible with
life they may not be as interesting but
they could tell us what makes you know
what makes the difference between
functional and non-functional one thing
that's happening now so you know in the
state of the union address Obama talked
about this new precision medicine
initiative and then I had a meeting last
week where they're planning to do data
collection from a million individuals
where they'll be essentially doing this
kind of genomic sequencing so that would
give us information about what the DNA
sequence is anyway but certainly having
some principled way to get sort of non
you know to characterize the rest of the
space I think that would be very that
could be potentially quite valuable
because well what we're focusing on now
is characterizing the diversity within
the human population
but there there's plenty of outside of
that as well thanks okay so the next
spotlight presentation is by reza egg
body and it's going to be some nice
stuff I'm sure
hi I'm Reza the topic of my posture is
decomposable no minimization with
proximal gradient homotopy algorithm
this is joint work with my advisor
Malian puzzle so the problem that we
consider is this known regularized
linear least squares problem minimize
half X minus B squared plus now lambda
now norm of x where this the
optimization variables X and this norm
or the regular rising norm is some type
of known that promotes specific desired
structure in the solution so lambda is a
regularization parameter the norm the
assumption that you have on the Nome is
that it's a decomposable norm which if
you're interested to know what the
decomposable norm is you can learn that
in the poster session the matrix a is an
n-by-n measurement matrix and m is less
than n meaning that the number of
measurements is less than the dimension
of valuable x and finally the vector B
is equal to ax naught plus Z so it's a
noisy measurement from the vector X
naught as is the added noise so this
optimization this convex optimization
problem has a lot of applications if
that Norm is l1 norm you get you can use
that for the sparse vector recovery you
saw one example of that in the previous
talk if the norm is an l1 to norm that
problem is good for grouper sparse
vector recovery and if the Nome is a
nuclear known it can be useful for low
rank matrix recovery so a very suitable
algorithm first order algorithm for this
problem is proximal gradient algorithm
which is natural for this type of
composite objective function it gives
you a sub linear rate of convergence of
1 over K it has been observed in the
literature that if you do some type of
homotopy continuation on parameter
lambda you will start with a large
lambda and you reduce it up to lambda
target and apply proximal grading so if
you mix somewhat to be on lambda with
proximal gradient you can speed up the
algorithm a lot so two years ago joanne
jang they proved that so they analyzed
the algorithm and prove this speed up
really happens that when you have a 1
norm you get a linear rate of
convergence if you mix proximal gradient
with a homotopy continuation on
parameter lambda under some condition on
lambda target and the sampling matrix a
so we asked this question that so that
analysis was very specific to a 1 norm
we ask this question that what is the
biggest or largest class of norms that
we can generalize this linear rate of
convergence to when the answer was
decomposable norms it's a class of
knowing that container 10 nuclear norm
and l1 to norm and so in order to do so
we drive some interesting properties for
this class of norms and we were we were
able to show that the linear rate of
convergence it still holds for that
class of gnomes if you're interested I'm
happy to talk about the details and
during the poster session</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>