<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data-Constrained Environmental Modelling: FetchClimate, Filzbach, and Distribution Modeller I | Coder Coacher - Coaching Coders</title><meta content="Data-Constrained Environmental Modelling: FetchClimate, Filzbach, and Distribution Modeller I - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data-Constrained Environmental Modelling: FetchClimate, Filzbach, and Distribution Modeller I</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yHrCwItuKwY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay everyone welcome back to this
afternoon's tutorial for the lat Tammy
science workshop prepare to have your
socks knocked off because our next
speaker is Drew purvis who's head of our
computation ecology and environment I'm
sorry I just said that computational
environmental science and ecology group
in Microsoft Research Cambridge drew is
becoming quite a superstar in fact he
was recently on CNN in March right and
then in January he published his
comments on ways in which we could model
environmental science systems going
forward from here and it is a really
fantastic article so without further ado
I'd like to turn it over to Drew and I
promise I won't make you play the
accordion so hi everyone oh so scritta
said untrue let me just go and look
myself up on bing and ike remind myself
who I am and what I do we have is that
me there here we go look we can test the
network connection to on the head of
this group here in Cambridge and as you
can see on this web page if you take
time to read it sometime we're a group
of ecologists but we sit with a
Microsoft Research it's a bit weird and
I never thought as if someone with the
background in ecology that I would end
up working for microsoft research but
the group that we have is its that's why
it's called the computational ecology
group and we really do a very particular
kind of ecology and environmental
science that is aiming at trying to
produce quantitative predictive models
of various aspects of the natural
environment and that's something that by
and large ecology hasn't been able to do
and perhaps you could argue hasn't
really tried to do very much they've
always been people interested in that
but by and large we've tended to either
look at some pure theory or look at some
kind of relatively simple fieldwork or
maybe some controlled experiments and
those and there it has been fantastic
and ecology has a lovely rich history
we've learned a lot about the natural
world but it hasn't enabled us to make
the predictions that we need now to
enable us to make a joined-up decision
about how we manage our natural
environment and so that's that our group
is trying then to is one of several
groups around the world focusing on that
kind of quantitative predictive modeling
I think what makes our group a little
bit different now is we have an explicit
goal of also producing software which
will enable a much wider group of
scientists to become involved in that
kind of work so to be able to build
these sorts of predictive models to be
able to share those models with others
and modify them and extend them and
combine them in different ways that's
what we think of as open modeling you're
about open data but we're thinking
they're about open modeling and then
finally share those model predictions
with important decision makers like
governmental decision makers people to
work for NGOs businesses and citizens
and so on so the tutorial this afternoon
is these are all early prototypes these
are certainly not you know kind of
finished articles and we're very
interested in anyone that might want to
work with us to provide feedback about
these tools as we say in Britain do you
have the phrase having a thick skin I
don't know if you guys have that in
their Latin America but it means I won't
be offended if you were insult the tool
so I'd be very very interested in
feedback positive or negative on these
tools and if anyone wants to try using
them you could become a kind of beta
tester if you like for these tools just
just let us know we you know some of
them are already open and those that
aren't will be soon and we're just very
interested in in working with everyone
so that's what this is about so today
we're actually going to do some we're
going to use some of these tools
together going to be breaking into
groups and we're going to give that a go
and if everything's successful by the
end of the day together in this room we
will have created a set of I think ten
new models predicting fire natural
primary productivity that's the rate
that carbon goes into trees and the
yield of ten different crops and we're
going to we're going to created those
models together train them using
Bayesian inference to estimate the
parameters and then we're going to
upload those models into a web service
that means that anyone in the world with
an HTM
tml 5 compliant browsers that can
actually go and run those models on
demand any scale anywhere in the world
for the past or into the future okay but
cuz it might not work so we'll see but
that's the idea and so I know it's just
after lunch so I thought what I'll
quickly do is take you to one of these
tools it's the first one we're going to
use today fetch climate it's called and
here it is and if anyone wants to get
there you can try fetch it so did you
see that URL let me try that again
because it kind of gets longer when you
do it so it's fetch climate to cloudapp
net so anyone with a laptop and and I
hope you got html5 compliant browser
that means just a recent browser of any
kind you can go and try that one out and
have a look and then we're going to be
using a slightly different one in a
minute so the one we're going to be
using is is actually even shorter AKA
AKA ms / fc2 so if we go to
aka.ms/offweb here it is a little um
this is where you choose on the mat
we're going to do this don't worry in
the next hour if you're not keeping up
right now because as you'll see I've got
a really trivial reason for doing this
right now because this my understanding
is left in America roughly right right
here start starts with Mexico and goes
all the way down right and most of the
people here from Latin America is that
right must be right so what i thought i
would do is I'm going to hover the mouse
over the map and when your country when
the when the point is hanging a whole
hovering over your country want you to
shout out and go yay or okay all right
so we'll give that a go that way you
want then I'll give me an idea about the
geographical distribution of my audience
ok so here we go no one from Mexico just
make secure count as Latin America does
right there no one from Mexico is that
oh come on we need within your shout all
right cool all right it's that's right
let's go down now whoa here we go I
won't take too long over this but here
we go
oh wow we're definitely missing some
whole countries while looks of it no
Colombians either hey all right great
good good good Venezuelans Ecuadorians
excellent good those Hummers a colleague
of mine so there we go Peruvians well
done all right we go and there's this
really big country here all right good
excellent and here we go since you've
already been up this quite a long time
let's try the rest in really quickly
ready here we go okay right oh you'll
manage to get over there we go I was
hoping you might cheer a bit louder but
hopefully by the end of the set it is
just after lunch hopefully by the end of
the session you'll love you'll have your
cheering head on ok so when so this
thing here fetch climate to as it's
called because it's an upgrade of the
one that we launched a year ago is one
of our tools and I'm just going to zoom
back in a little bit that's a bit too
far out even for me and what you can see
here it enables someone to go and this
is what you can fetch ok and so for
example we could go and get bird species
richness or we can get things like
elevation above sea level or the
predicted mean carbon storage some of
these things the ones I just fetch
aren't but some of them are indexed by
time so this one isn't so we won't index
it by day or time you're going to pick a
region on the world somewhere so put
little thing here we go and hit fetch
and what's happening here is this html5
front end is communicating with the back
end that is running on Azure and it's
looking at our query it's choosing the
appropriate data set it's subsetting
regrading doing the calculations the
interpolation and so on and it's sending
the results back to the browser and
we're then viewing them in the browser
ok and so we can see here that south of
the Amazon River we tend to see a much
higher bird diversity about 600 species
we think could potentially live in that
region six hundred different bird
species that north of the Amazon River
there is much less about half as
and that's because the Amazon River is
such a big river that it's actually the
northern boundary of lots of different
bird species it's the northern northern
edge they don't extend beyond the Amazon
and we can look at predicted carbon here
so this is the predicted amount of
carbon that's held in the trees in those
different areas and we can see it's 17
kilograms per kilometer squared here
ignore those units it's definitely more
than 17 kilograms per kilometer squared
I'm gonna have to talk to my colleagues
about those units but that's fine i'm
sure in relative term it's fine and it's
much lower here okay so the green to
brown is the is the idea so that's this
is one of the tools and we're going to
spend the first bit of our session I'm
going to go through this again more
slowly I'm going to tell you about some
of the different fetches that you can do
and how to do them and then we're going
to break up into groups and we've got
some questions here on these bits of
paper that we print it off earlier and
you're going to use fetch climate to
have a stab at answering these questions
and then at the end of the tutorial
you're going to report back to the rest
of the room your answer okay that you
that you got using fetch climate that's
the idea so I'm going to have another
maybe 10 minutes 15 minutes maybe using
fetch climate in front of you so that's
the kind of lecture each style bit over
and then we're going to break up into
groups and so on so so here we go let's
go back to thee so again you know that
obviously it's a there are not so there
are several ways to use fetch climate
that this is the kind of the most visual
kind of friendly way this is the UI way
but there's also a text request kind of
form that you can fill in and that
enables you to do some more precise
kinds of fetches like if you had an
exact list of coordinates and times you
could do that using the text form so but
let's just stick with the and also we
have an excel add-in which I can show
anyone if they're interested and so you
can if you've got a list of lat long
positions in Excel you can also hook out
information using the Excel add-in and
then we're working on having fetch
climate from our so there is an API in a
rest interface so in principle you
should be able to get to this tool from
many different many different places and
you can get to it from inside any don't
that program already so here we are so
this bits the easy bit you go and choose
your variable and you can see the the
description of the variable here in bold
and underneath there's a list of data
sets that can potentially fulfill that
query for that variable this is
important with fetch climate that you
don't go in looking for a data set but
most of the Open Data initiatives that
we see you go in looking for a data set
here you go and you ask for the
information that you want and that
information can potentially be served by
many different data sets so one example
here is the air temperature near the
surface and you can see that there there
is no less than five different data sets
already in there that can potentially
fulfill the query about air temperature
that so for example the gfdl data set is
actually a set of predictions about the
future the whereas the NCN car
reanalysis is a data set that pertains
to the past and the sea are you data set
is what's called a climatologist set so
that gives you an average condition over
a grid so these were originally designed
to fulfill different purposes but what
fetch climate is going to do is given
the details of your query is going to
choose the the right data and if
necessary stitch them together so if you
do a query from the past into the future
it will actually stitch together data
sets as necessary to give you a single
time series of air temperature so let's
do air temperature why not so the
simplest thing we could do is say so we
have to pick so then we go to so that's
what now we go to when and we can pick a
range of years so to start very very
simple we could do twenty twelve to
twenty twelve and it says over that
range of years do we want a value for
each individual year now twenty twelve
to twenty twelve just one year but maybe
I'll tell you what let's illustrate that
by at least starting off with a
different one so here twenty twelve to
twenty twenty-two and what it's saying
is do we want to take that period an
average over all that whole period to
give us the average or do we want to
split it by individual years and now
we're going to get a time series
stepping forward in
at ten years at a time or if we want we
can actually averaged over chunks who
say two years each and that way we'll
get an average for the first two years
followed by an average for the second
two years that make sense pretty under
some computer scientist in the audience
is not difficult stuff right that's the
point it's supposed to be easy now
similarly we can index also by day of
the year and hour of the day so if we
wanted for example to do a typical
seasonality of temperature in a place
then we could use this one which is 12
monthly averages and now we'll get for
that range of for each of those pairs we
will get 12 values that make sense so
we'll get 12 values for the first pair
12 values for the second pair but if we
average over the whole period we're
going to get 12 values which represent
an average over that whole time and
there's similarly with them hours of the
day so if the average over the whole day
will get what I just described but if
for instance we do in we do individual
hours will now get for that period and
for each month we're going to get 24
values so we get 24 values for January
which represents the typical daily cycle
in January and we'll get 24 values for
februari representing a typical daily
pattern in February so and so I think of
time as an ecologist as a
three-dimensional object we're trying to
think of it as a one-dimensional line
that we split up but really because
nature the work you know that well at
least the earth has these three has
these two dominant cyclicity the diurnal
cycle which is obviously the earth
rotating and the annual cycle the earth
going around the Sun so it makes sense
to think of 3d time does that make sense
three-dimensional sign year's day of the
year and hours of the day we can average
over that in different directions as a
little straight in a second but let's
say but if we do an average over the
whole day and an average over the whole
year and an average over the whole p
we're just getting at one number okay
but then there's where to think about
and so here we can either do a point so
the very very simplest thing we can do
is a point and now we really are just
going to get one number for that point
however in addition or instead of we can
add an area and now the wind that we
just did will apply to each grid cell in
that box and we can cut up the box how
we want by default it's 30 by 30 but we
can do 100 by 100 or 200 x 200 or what
have you so for each of the each of the
cells in this grid we're going to get
one value because that's what we
specified up there let's finally do the
fetch them I know that was a bit slow
but I just wanted to step through it now
I'll step through some a little bit
faster so this thing's then going off
and it's actually running the query
against multiple data sets is how it
works and it calculates the uncertainty
so you get the uncertainty on the fetch
back as well when you use fetch climate
and by default it returns the answer
that has the lowest average uncertainty
but you can override that with the text
form as you'll see later too if you're
interested in particular data sets so
now we can hover over and we can see
that for the time period that we defined
in when and given that this is air
temperature near the surface we can
hover over here and have a look at the
answer and we can see it's 27.5 over
there and a little bit less over there
24.2 simple stuff right but if we going
so if we go and kill those regions now
let's say we had a point just here
instead now we can go and do something
more sophisticated in time and let's say
well let's look at individual years then
that for that period so again averaging
over the whole year and the whole of the
day but now indexing years a year and
now we're going to get air temperature
we're expecting to get air temperature
for each of these years and can you see
there's a control now it's appeared on
the UI there and anyone feel free to
keep up with this by the way as much as
you want some it is open so and now you
can see as we step through we can see
this time series up here for the time
that we asked that makes sense
and if we go back and we can we can have
more points if we like obviously doing
that by hand is not necessarily very
useful but as I mentioned in the text
form you can actually paste in a list of
lat lon values so if you did have for
example Flux tower sites or places where
you placed environmental sensors or
anything at all you'll be able to paste
that into the form and now you would
have that time serious reach of those
points and as you hover over here you'll
see the one that you're hovering over is
highlighted see that and step through so
that's that so that's an example for
each of those places are pulling out a
year by year time series but now we can
average over time in a different way by
saying well let's average over the
period let's say 1992 2000 so now we're
going to average over the years but
dividing according to the months for
that same list of points so it's the
same data but we're looking at it I
think in a different dimension and so
now the time index has changed into 12
months you can see that right and so for
each of these places we now get a time
series that represents not the change
year to year but the chain a typical
seasonal pattern Roy actually is not
typical seasonal pattern it's actually
an actual seasonal pattern for each of
those for each of those places can you
see that so it's saying over that period
of 10 years that was the typical annual
cycle of temperature at that point and
that was a typical annual cycle of
temperature at a different point so
let's go and clear our fetches now and
we can also do a grid by time so we can
sail it and these these take a bit
longer so some have to be a little bit
patient sometimes because the fetches
can take a bit of time is quite is
actually quite a lot going on in the
background so here we can say now let's
look at individual years again say from
2000 to 2030
for that grid and now what we're going
to see again now we've got a yearly
index up there and we're now we're going
to see how that map has changed in the
past from 2000 up to about 2010 after
which fetch climate is going to stitch
is going to use one of the historical
data sets to fulfill this part the query
and it's then going to switch into using
the gfdl predictions okay and that and
so we can see how not only how the map
of temperature across that region has
changed in the past but then we can see
according to this model prediction how
it might potentially change in the
future that makes sense and these
vectors can sum this is a variable one
so as in sometimes these fetches take a
shorter amount of time and sometimes
they take a bit longer this one usually
takes a bit less than a minute and put
some occasional it takes all three so is
is one of those tricky things because we
specifically built fetch climate because
in the work we were doing we needed this
kind of information all the time and it
took just ages and ages to get that
information whereas now it might take
one to three minutes but unfortunately
for a demo it's a really bad demo right
one two three minutes though so here we
go all right and so we you can see here
as we go from there's not much going on
in the map there now there is an issue
here I noticed is that something funny
happens here when it's stitching the
data sets together there's some funny
stitching so where and with their
looking into that so I told you this is
a prototype right you can see then we
can we can hover over and you know
according to according to the model the
region is projected to warm to a certain
extent see that here now for any of the
fetches we can download the data so we
can do that by clicking this button here
and we get a cloud of thumb points here
and we can just copy that and paste it
into Excel for example and then that's a
bit simplistic but it's just to keep it
as simple as possible
no it paste it in a picture of my
desktop instead okay i'm going to try
that again i did I didn't expect that ok
let's try that again there we are
amazing oh well oh yeah moving on ok I
tried it by hotel room and back in my
office I promise there's something funny
going on with the way that that's called
that's very funny all right so am i
guess that's going to limit us a little
bit at some point I mean most of time
you can look it you'll see later we can
use this text request form by the way to
work to do these so I'm just wondering
if there's anything else in terms of the
nature of the fetch that you really need
to understand but I don't think so you
can obviously practice in a bit when we
get to these questions so but the idea
is you can combine any combination of
the variables with the indexing by year
on average i go over years with the
months or particular page you can do
weeks if you want so notice here that
the 12-month average is only one option
you could do an average for part of the
year so you could say do 120 to 180 and
then this will give you an average only
for may and june so that way you could
do for instance and a year-to-year time
series of june temperature that makes
sense or you could even have a
particular day if you want so you could
look at how the temperature on your
birthday has varied for the last 30
years i might change into the future and
you can do the averaging over chunk so
you could say one to 365 in chunks of
seven and then that is going to give you
a weekly index okay so you can you can
combine so you can combine the temporal
fetching with as many different grids as
you want placed anywhere and any
combination of points it doesn't have to
be one or the other and then you just
and you can have you can fetch multiple
things at once as well so there's some
if you want to
notice if you do in this case we indexed
it by a year and we indexed it by day of
the year so now we'll get a double thing
here and you can actually navigate back
and forth and there's probably that so
the final little thing I probably say
about the UI is that you can all the
information about the fetch is in the
URL so if you copy that and put it in an
email to someone else then all they need
to do is click on that link and they'll
get back to exactly this the factual
happen again and so that means that you
know if you wanted to say as we'll see
you in one of the questions you're
advising your sister on where she should
get married or when she should get
married then you'll be able to just
paste it into an email and say trust me
you don't want to be getting married in
Colombia in the wet season or whatever
it might be so that's that's kind of the
UI bit any questions straight away
because I don't want to go racing head
ok yag please
well the way I'm are loaded that data
Series in the database really affect the
output and true put the trooper tyne so
for instance if I mahler it is a
sequential for me it the query takes
three minutes if I'm other than some
other form it takes eight hours so was
there some kind of special modeling in
order to extract leaders so fast from
your database it I mean the short answer
is yes so that there's we the
performance has been a real issue and
there's there's lots of clever stuff
going on in the background to make it
faster actually so we notice oh
that's a nice chance to be able to
acknowledge university group that we
work with on this and that's a group at
Moscow State University and very very
clever PhD students and a professor and
others there and who've actually built
all of this stuff and yeah there is
quite a lot going on in the back end so
one thing that's easy to describe is
which is really good for your sharing
the URL or for repeating bits of science
is that there's local that is client and
server side caching of all queries so
that if you run a query that's where
exactly that query has been wrong before
it be very very fast because it simply
gives you the same answer back which can
be really nice actually for sharing you
know for sharing these with other people
otherwise there's there's basically a
lot going on in terms of you know even
things like the data is compressed as
needed and so on and they don't
necessarily look at all the smaller
decimal points at certain parts of
calculation and lots of things let's say
imagine and the second question is one
of the operations I do frequently is a
moving average instead of say getting
the marriage of every week I want the
average of the last seven days yeah and
that's a very hard operation it is yeah
so you can't do that at the moment in
the U is that we have so you can't do it
in the excel add-in in the UI that I've
just shown you all
text form but you because there is a
rest interface you can do it yourself
because you can fire off lots of
individual queries and so you could just
say give me exactly this having said
that I think that moving windows are
something that we should perhaps think
about as a standard thing anyway because
it's um it's quite a reasonable thing to
want yeah it's quite interesting and
form for various ecological processes
you know you even a moving window you
might want like you said the 10 days
running up to a time so like if you're
thinking of fire the chance of fire on a
particular day is probably a function of
things like the rainfall over the last
two months up to that up to that day
right yeah so yeah they're both good
questions thanks thank you very much
you're very welcome so honestly I'm not
going to hang around too much longer
doing doing lectures I think you going
to get to use this but just just to let
you know that here's the text request
form here okay it's just down there and
if you click on that you'll get
something like this and so you can not
that you can choose which variables just
like before but you can actually choose
which data sets you want to potentially
fulfill the query and then here you can
choose all of the information that you
just saw in terms of years days within
the year and hours within the day but
then down here you can paste in a list
of lat long positions so if you had a
file somewhere else where you've got a
list of lat lon or you can divide define
a precise grid so you don't have to
click with your fingers and then you can
download the results and that that does
work I've tried that one and then while
it's a wire in the group's i will see if
i can work out that copy and pasting
because it should be very simple you saw
me copy that list of numbers and paste
them into excel so well maybe i'll try
it in the old version of excel maybe
that work so it so this is where the the
group bit begins right I wasn't joking
about splitting into groups so we've got
13 different questions and what I would
like is therefore for you to break into
13 different groups and each group has
to have a laptop that has wireless
access and then you've got to come up
and grab one question and then i'm going
to give you about 20 minutes to have a
go at answering the question and then
we're going to take it in turns
answering back and we decided to trust
in your ability to self-assemble so
rather than try it so I'm just going to
trust that you're able to break into 13
groups just like that so uh how long do
you want to do it come on I'll be get
and you need to I figured if you turn
round some chairs like this you can you
can make little groups of six people
looking each other or 10 people or
whatever it is say all right everyone I
think I think we'll reconvene and then
oh yeah sure just here now this tutorial
this tutorial was always going to be a
two-way learning experience so I've
definitely learned some things about the
fact that we need a bit more space on as
your if we're going to do these things
again and there and so on so one I think
there was a common problem across the
different groups which is that some of
the fetches are taking a very long time
is that right so a lot of people were
looking at at this so I'm sorry about
that
all right is everyone ready to settle
down and report back that kind of thing
alright everyone so it's three o'clock
now we're so as I mentioned as I
mentioned we are interested in feedback
about fetch climate so and if the
feedback is the fetches take a really
long time that's fine you see if I have
a collection of feedback like that then
I can go in a lobby someone for more
space on as your to make the fetches
faster so what I thought we'd do is
we'll take it in turns to report back so
we do you have someone with a handheld
mic we've got a handheld mic in the room
right ok cool so I've got the questions
up here and I realized that because some
of the fetches were taking a long time I
think most of them would have most of
what I saw it look like people have got
it right which is great news and
unfortunately some of the fetcher to
take a long time so some people weren't
able to get to the answer they should
have been able to get to for that reason
so I'm sorry about that so I realize
that some people may not feel they have
answers to these questions and that's
okay but hopefully we'll get more than
one or more groups that feel that they
did get some kind of answer to the
question and because I don't know where
in the ring the questions are so okay
great so I'm going to call out the
question and then the group that had
that if you guys shout and there and the
mic will come over to you and then just
give us a really really brief summary
and then I said it ideally if you do
feel you're able to get an answer do
email me the URL because what I'm going
to do is I'm going to collect all that
together and I'm going to email all of
you guys so it's not for me really it's
for all of you so then you can see how
the other groups the URLs they used to
look at the fetch okay and even if some
of those didn't work because they took
forever hopefully when we all go back
and we're not all using it at the same
time because it is just one service on
as your those those fetches will work
okay so here we go so who had how has
rainfall been changing in Paraguay come
on you guys depends on your shell so he
wants to report back
okay so the answer is you don't know
because it's too slow all right that's
okay that's not your fault so what about
describe and illustrate the pattern of
see depth in the Caribbean Sea who had
that one cool and did you email me the
URL yet Jamukha good okay so we were
able to compare the whole pattern but
not take the average in a single block
oh okay now you go ahead you talk it and
just we just selected to the two regions
with the selection mode and then we look
at for bathymetry and just virtue the
bathymetry we're using the old version
of fetch climate your market I think
you've everywhere I know I forgot to
tell you there is an old version that is
working is it's fine for that question
the old version works this molecule see
that but it is a Silverlight app so it's
gonna have to download okay you know
hope you realize I'd the podium these
things are kind of a high-risk activity
right they are safer there's always a
good chance if you give a tutorial
you'll fall flat on your face in bed in
one of several different ways and so
here we go so this is the fetch that
young marketed in the old veg climate
but you could do the same fetching the
new one and you get very similar answer
so here's the Caribbean and so Jamarca
was able to send me that URL I've
clicked on it and now we can hover over
here and have a look at how see depth
varies around the Caribbean right gian
marco and then we can go over here and
compare that with thee how can you take
the average of a single block is it
possible to do that and I've read my it
is in fetch climate too okay I'm not in
this verse so like in if if we did that
here CT
no it isn't I lie I don't think we have
made it possible to do an average over a
whole region looking like that and if
that's something interest yeah I mean it
so I guess you'd have to do it the hard
way right now you download that grid and
you would take an average pipeline but
that's a perfect we ought to do that's
perfectly reasonable thing to want to do
actually so yeah we only support points
or grid but we don't support regions so
that was just a choice you made on the
on the UI okay let's have a look at some
of the other questions we'll just sit
through them and as I said it's fine if
you didn't manage to get there because
it was too slow that's not your fault
you have which major city in South
America has the greatest seasonality and
temperature anyone all right we're going
to move on someone had it i thought okay
your sister's getting married on made
the 15th next year who had that one okay
you guys was just here and did you
manage to get through that yeah alright
cool let's let's hear what you would
advise your sister and did you manage to
email me the URL at all cool let's have
a look writes on yeah so we decided that
central brazil was a good place to have
the wedding we tried to drill down
further no my name's mark oh buh buh buh
oh boy hope you didn't get put into junk
that would be really mean oh you did
that's not me right okay not junk oh
look at that i'm gonna have to copy this
manually okay
and I said it does so we started with a
large area we try to drill down to a
small area but the queries took a very
long time cool try streaming so you
looked at air temperature precip and
relative humidity right makes sense and
and you can see roughly based on that
idea and achieved it you did you can see
the ocean there as well right you'd be
able to give your sister some advice
about given that she was so they would
understand their that this is although
it's another average it's an average for
just a particular time of year right I
think may the 15th or or whatever okay
so you can say what is it typical May
fifteenth look like over that map and
obviously your sister's wedding is a
kind of trivial example for
environmental science but that but it's
a similar kind of fetch that you would
do for other things thing alright well
done thanks we've got you are test
replacing a wind farm in Brazil you've
got the wind farm we're going to need
some hands up you guys it's sir who got
the wind farm okay no wind farm where
you guys come on hands up shouting
thanks tease thank you alright unit we
couldn't because the fetches were too
slow where they are no we found some
places outside of Brazil and couldn't
just narrow down to when there was
inside the country okay cool yeah the
square was we couldn't select only the
Brazil area alright fair enough and did
you say when we could maybe talk about
that after a little while but um
obviously the idea would be that for it
we talked about I mentioned in my peanut
about decision making and the idea is
that looking across a set of these
variables include so for wind speed
obviously the average wind speed and the
seasonality of wind speed then you be
able to make some kind of informed
decision about where to put a wind farm
that was the idea so thanks for trying
we could talk about it a little bit
later there was a similar one about
solar energy
did anyone have a ago at the solar solar
plant though no solar panels all right
what about the bird reserve anyone do
the bird reserve okay right fetch is
taking too long yeah okay all right fair
enough well I tell you what we'll do I
think did you guys get anywhere with the
MPP one alright cool thank you so let's
have a look at this and what's your name
sorry there we are great so these guys
were tasked with looking at how NPP
natural properties net is the net
primary productivity it's the rate that
carbon goes into vegetation and that was
actually something that wasn't in the
fetch climate originally so I put it in
there white why he's after a couple of
minutes and these guys were tasked with
looking at the ratio of the two right so
did you find anything out there we go oh
yes well I think we will move I don't
think we're going to carry this exercise
on too much longer but you get the idea
right does anyway does anyone else feel
that they had a question that they did
get an answer to and they'd like to
volunteer rather than me going through
them anyone fancy reporting back or do
you think we should move on well by
looking at the map itself we had the
frost question we decided that a frost
is the question on the Vizier okay yes
which air is in that American never
experienced frost we decided that there
were the green areas from this and the
white areas had have some frost and the
lower latitudes get more frost and if
you're near to see you also seem to have
some little bit more frost also okay all
right but you didn't actually managed to
get unfortunately again you found that
the fetcher is too slow yes but I sent
you an email with the the federal
alright guys so let's just change gears
slightly then and i'll say you know from
the heart thanks very
much for trying and them and it sounds
like ninety-nine percent of the problems
were on our end not on yours as I
expected so it's definitely been a
learning curve for us I hope that you'll
be enthusiastic about trying this
offline because I do think that the the
slow fetches you know even if it takes
10 or 15 minutes to effect so some of
the large fetches do because if you do
say grid by time for a lot of points
obviously that that will take quite a
while I think we experience a lot more
problems here because we're all hitting
the service at once and it does only run
on a certain fraction of as your so I
hope you'll be keen to at least try
using it in your own research at some
point and again very very keen on
hearing your feedback so anytime you
want to get in touch ok so i really
appreciate you trying and being up for
it so to speak and and then just as a
reminder there are other ways to use
fetch climate so you can use it directly
from a dotnet program some of you tried
using the text request form down here
you can and then also watch this space
because we will be releasing details
about how to use fetch climate from our
for example and possibly from things
like MATLAB as well and we may also be
open sourcing important aspects of fetch
climate so if you fancy actually getting
hands dirty and modifying it that would
be great too so that was kind of the
first phase of the three bits of the
tutorial this afternoon the fetch
climate bit we're at quarter past three
now we're going to finish at four
o'clock okay and the second part of the
at this part of the tutorial which
finishes at four is going to be about
we're going to shift gear a little bit
and it's going to be about mostly about
the concepts about how we go about
fitting complex models to data so it's a
very different sort of activity okay so
we're going to change gear a little bit
and them so don't remember when i was
talking at the start about this to make
it easier for people to build models and
then train those models and then share
those models with with others right so
we've got this at a three-way activity
and the fetch climate is about saying
you know the typical way you would do
that is you would have some private data
some data of your own about something
that matters and you can supplement that
with fetch climate so you would then
have the two things sitting next to each
other and then you can go and define
some model
relates the two relates the
environmental predictors to the thing
that you care about and you want to then
train that model against the data as we
say right that then means we can then
reapply that model to make predictions
somewhere new like a new part of the
world or predictions into the future and
so on now in the middle of that is this
idea though that of course you can
define the model that you want and fit
it to data and that simple idea is very
easy to say but traditionally it's been
very hard to do so I just want to go
through and this is kind of a more of a
kind of lecture style style bit here
mostly because there's been kind of a
revolution in the way we do that now in
environmental science over the last few
years I know many people in the room
would be well aware of this but which
has been enabled by this bait that the
use of Bayesian statistics and they're
typically very heavy on the
computational side but as computers have
got faster and then we've learned how to
use that fast computation we're now able
to train much more interesting process
rich models against the data in a way
that was kind of unthinkable before so
by way of a background we can think
about you know when I let's say was at
University doing ecology and which was
no by my standards wasn't all that long
ago like mid 90s the way with the way we
were sort of taught to think about
answering a question in ecology to the
extent and talk about a question is
informed by a model here was that you
would think about your your model so
here's a and a model here is like a
model always is a set of ideas about how
the world works in cap you know encoded
in some mathematics and equations so
let's say so we in eCollege you have
things like the lock of all terror model
bit and I know there are lots of people
here that do different subjects but they
would be this common idea of a model of
the processes involved but how and
because you could play with the model by
itself and that can be great fun that's
theoretical ecology right so you just
play with it and you say and you're able
to say well if this then that and that's
kind of interesting perhaps for for
helping you're thinking but if you
really want to tie it down to reality
there's going to be data involved so how
is the data used
in conjunction with the model
traditionally in traditional this is
traditional kind of hypothesis driven
science and so here's your data here
let's say so you've got your model in
your data well how was it done well what
you really do is you take your model and
you would boil it down to some
qualitative prediction often so it might
say something like well according to my
model of leaves there should be more
evergreen leaves where it's cold or
whatever it might be so here's your
quote be qualitative and you would take
your data and you put it through a
predefined statistical routine there's
our stats and those stats would give you
some kind of metric like an F value or
the ants the p-value from a t-test or
something like this and you would then
compare this metric with your
qualitative prediction and you'd
obviously use your brain right i mean
you know and it's not that we didn't
learn anything doing this we learned a
lot but so you'd have to choose the
right statistical test and look at the
right metric in the right way so that it
gave you a kind of yes/no answer
basically it's your qualitative
prediction that's the traditional way
that you would use sort of model and
theory in conjunction with data to try
and understand the world better whereas
and but what if you wanted to actually
make a quantitative prediction about
anything this really doesn't give you
that at all and that's and that's
because the model that's being used here
is not the model that you actually think
represents how the world works is a
different model this model was designed
by an ecologist this model was designed
by a statistician so obviously you could
you there's a quite different activity
which would use again traditionally a
different suite of predefined
statistical models to look for
relationships in data kind of regression
line modeling to make predictions and
you know that can give you predictions
in fact in the last section of the day
will actually be using models that are
really kind of in that direction they're
sort of not very rich in terms of the
processes about the reality that's going
on they're just something like a
regression they're just a set of
relationships so they still have their
role but once again though the model
that you're using to predict then in
that case is has not been designed by an
ecologist that knows anything about the
way the actual system works or a
physiologist or imap model or anything
it's been designed by a statistician and
the reason that statistician design
these models is because they needed to
be able to train the models so when you
when you run a statistical a traditional
statistical routine like a linear
regression or a t-test or an anavar
actually parameterizing training a model
against the data you don't usually
verbalize it that way because we as
scientists we don't want to confuse it
with this kind of model but that's
actually what's happening and those
models that are used in those predefined
statistical routines have been chosen
because they could be trained against
the data very quickly without lots of
computation there are analytical ways to
jump to the the correct answer trained
against the data again so they've been
chosen for kind of computational
convenience not because we actually
think they're a good representation of
the real world now when does that matter
and when does that not matter well that
approach can be okay for interpolation
so as an example let's imagine that just
imagine that I gave you a time series
like this and time and I gave you some
point
that's what you've got and so the
traditional approach can say well that's
okay I can fit a straight line to this
let's say approximately linear
regression and that way so what we know
what's good and bad about that approach
well that's a pretty good approach if
you want to interpolate if you want to
know where this what the data was in
between when it was observed you can see
that's pretty good who really cares what
the model is as long as it seems to go
through the points fairly well when
you're interpolating but if I told you
that if under prediction however you can
see that you just you go zooming out now
if you do it properly of course when you
do your linear regression you should see
something that's kind of like this you
do you so to be fair it should tell you
that there's a huge amount of
uncertainty because it should say as you
extrapolate from the data you know but
it's not very useful prediction but if i
told you that those points were the
position of a rubber ball measured
through time then of course instead of
using some predefined statistical
routine you'd prefer to use something
like Newton's equations of motion that
describe bodies moving in space and then
you might be able to then make a
prediction that looks something like
this the difference is then that you're
able to extrapolate with some confidence
because you replaced your statistical
model with a process based model that
actually represents how you think the
real system works and so and the reason
then you don't necessarily get this huge
fan out of uncertainty now actually you
kind of do later on you can talk about
that later but over this say range here
is because you've constrained the
possible futures by loading in the
knowledge that's in your brain about how
the real system works so you didn't have
to just rely on purely on the data
anymore because you actually got a
scientific understanding about how a
rubber ball behaves so the idea is what
we'd like to do then to make predictions
and almost all the interesting
environmental science is about extra
ocean it's about predicting what it's
going to happen in a world of higher co2
of unprecedented for habitat
fragmentation and invasive species and
all the rest right so the idea is what
we'd like to do is is define process
based models of how the real system
works and actually train those against
the data so how can we do that well the
in general I actually think it's pretty
simple to say and the difference is it's
now become possible to actually do in
reality as easily as you can say it
almost so let's go back to our example
before and say well you know what is a
model well it's a set of equations that
we think describes how the world works
so in the rubber ball it would be
Newton's equations and there'd be
parameter and so that would be the
equations and it comes to the set of
parameters maybe these constants
involved in there in Newton's equations
it might be the gravitational constant
the elasticity on the ball the initial
velocity of the ball this kind of thing
so you have your data it sorry you have
your model and you have parameters and
given those two things a model and a set
of parameters you can obviously make a
set of predictions
all kinds of things and it's anything
you choose to you that can logically
follow from that set of equations and
those parameters now if you have some
data and I'm deliberately something is
very simple I know if any more familiar
with this it will seem really naughty
but let's say they've too long you can
compare the predictions of that so that
you can think of this is forward
modeling you take model and parameters
and you make a prediction and then you
but if you compare those predictions
with data you can obviously immediately
say well is it any good or not do the
predictions of the model look like the
data and at the moment that kind of
eyeballing because we might call it in
eyeballing just you can obviously do
that informally and if they don't of
course the temptation is to change one
of the to the model and the parameters
right see and so immediately your kind
of feeding back now on the parameters or
on the model structure and you could
improve either or both in principle and
if and the more and more you did that
you go round lots and lots of times
that's what you can think of as
optimizing a model so you're actually
finding the best combination of model
and parameters that you can that
produces predictions that look like the
real data and obviously that's something
that you could automate you might turn
into a a bold arrow and so now I can
actually generate some routine then in
the computer let's say so it especially
if i fix my model structure and now i
can try different parameter values again
and again and again and find the best
set of parameters that gives predictions
that look just like the data and that's
what i would think are in very very
simple terms is optimizing a model but
that's not quite how the whole Bayesian
thing works not quite it sort of is in
the broader scheme but but there's
something very special about the
Bayesian thing and it's this is that how
do you how do you actually compare the
predictions and the data if you're going
to put this into some automated routine
there has to be a number here that
actually measures how good the fit
between the predictions and the data are
and how you're going to calculate that
well
the likelihood-based statistics a
maximum likelihood method and bayesian
methods are a special case of that based
on the maximum likelihood you have to
have a special kind of measure here you
can't just make something up so it's not
just necessarily an r-squared or a city
block distance or an area under the
curve whatever it might be it has to be
something called a likelihood the
likelihood so what's the likelihood well
we could talk about how we write it for
a start might help so we write it like
this which I think makes everything
clear don't even have to say anything
now right but it says this this is
saying the likelihood of the data given
my model structure s it's a set of
equations and a particular set of
parameters which are encapsulated in
this vector theta so vector might have
ABC and it'll gamma alpha or whatever or
whatever is relevant for the model but
the likelihood is something special the
likelihood is and sometimes you have to
be a bit careful with your language but
it's effectively a probability it's
actually saying how probable is it that
this exact set of data would arise from
this model structure and those
parameters so it uses a special idea of
probability and actually in practice we
often use it's a probability density
okay so that's what's a good bit tricky
high probability in probably density
this is the likelihood here now the
reason that's special I like to use this
analogy of shoes falling out with the
Attic so it's like we need you go up
into the attic and there's a bag of
shoes and you take one shoe out of the
bag and it opens the bag and all the
shoes come down falling on T right and
that's what's special about using this
probability is when you use this
probability as the measure between your
predictions and your data then a lot of
other things come for free a lot of
other shoes fall out of the Attic if
that makes sense so for example you can
immediately now when you optimize not
just get a best set of parameters but
get an error bar on the parameter so you
don't just get a best alpha but you get
a ninety-five percent interval an alpha
and that's both scientifically important
because you might care about alpha might
be something that's important to you but
also as we'll talk about later when we
make predictions we need to propagate
parameter uncertainty through the
predictions and so it's crucial that we
have a proper notion of the uncertainty
in the parameters and that effectively
comes for free it's very hard to do if
you're not in this world it's very easy
to do if you are we had the same notion
with the models so one of the very
common things I want to do is is choose
between models so we might want to
optimize one model and then have a
different model and optimize that model
and compare the two and if we just use
some arbitrary measure here it's like
well how much better does the second
model have to be for me to reject the
first one it was a city block distance
what was here because we're in
probability we can actually do things
like reject a model at five percent
probability and say actually this model
is ninety-five percent or more than
ninety five percent probable compared to
the other that kind of thing so we get
these two very special things that come
for free and the third one is that
because we're using this same measure of
probability we can use the same suite of
techniques to fit the model to the data
and this for me is really powerful
because I don't know how you felt
different people are different but
before I discovered these techniques I
always thought statistics was like a big
recipe book it's like this rule if this
then this you know if my if my X values
are measured without error and my Y
values have error and the variance
around the expectation looks like it's
independent of X and then then a bum bum
then oh okay that means i can use linear
regression ordinary least-squares linear
regression however if I'm depth then I
guess I and I just can't hold it all in
my head right it was too confusing and
furthermore every real case that I
looked at didn't conform to the recipe
book there was always something about it
that meant that it wasn't in any of the
special cases and then what you end up
doing is changing the model or changing
the data transforming the data to make
it fit into one of these predefined
boxes or actually altering the model
which is not really what you want to do
with these methods we're almost all the
time basically able to define the model
you want take the data as it is and use
the same technique every time to
parameterize it so all the stuff i
mentioned earlier like in our group you
know we've we've done more of this sort
of thing that I can remember now all
sorts of things from deforestation to
the global carbon cycle two species
distribution modeling to tree growth and
death to you know wherever it might be
and and all the time we're using this
these Bayesian methods and you wouldn't
you might have thought in our group that
we are always talking about Bayesian
statistics but we almost never do
because we just use the same techniques
so we talk about the model the ideas to
talk about the data and the questions
but when it comes to actually fitting
the models to data as it were doing this
stuff basically we use the same approach
every time and so for me it's been
incredibly an incredibly enabling thing
okay so what so just broadly then let's
just to reinforce that we're going to
assume that we're able to take ideas in
our head about how the world works and
create a Model S or or or several some
different ideas these model s and we
have some data pertaining to that model
x and it's up to us of course now too
and that this model s implies a set of
parameters so our parameters oh there we
go these three concepts again and it so
the first thing is we need to be able to
look at our data and say how likely
would it have been to have arisen from
our model and so we always and so that's
just a little skill you build up but
it's not nearly as hard as it save
you've never done it it sounds really
really tough and we can get pops go
through say let's go through a linear
regression in a second maybe that's not
very interesting model but you get this
kind of same you can see once you've got
a small set of tricks in your mind that
you're familiar with doing then it's
often not very hard to write out the
likelihood which is the key thing that
you need to be
too right so the likelihood of x given s
and theta so as I said we'll go through
an example in a second so if you can
well what kinds of things can you do so
let's let's imagine you had a really
simple model that just had one parameter
alpha and this is what this is often the
sort of example you see when you talk
about maximum likelihood then you're
able to create something like this that
has alpha on the x-axis and it has this
likelihood on the Y and actually in
practice computational reasons we use
something called the log-likelihood
which is just a lowercase l n equals no
prizes for guessing the log of the
likelihood says that's the natural log
of the life is that simple so we use
this log likelihood for various reasons
because otherwise this probability can
get so low that we can't hold it in
memory in the computer so we just use
the log don't worry about that so we use
the log likelihood and so we can draw
this this we can vary alpha and
calculate the log-likelihood so we've
got a set of data here and we can try
alpha and we can say how like what's the
log likelihood for the data given s and
this particular value of alpha might not
point 1 and we can do that for different
values and we can then end up drawing a
curb a bit like this it's called a
lightly hood profile and it's just the
funk is just the likelihood plotted
against the the value of alpha given
that the model s and the data X is fixed
so no surprise is that in saying well if
you if you're no prize is I should have
said for for saying well you know what's
your best guess as to what alpha might
be its kind of this one yeah that's
simply the value of alpha that gives me
the highest likelihood of the data X I
also talked about just a second actually
implicitly your employee employing
already employing some bayesian logic
there because remember this thing here
says how likely is the data given the
model and the parameters we've just now
made a judgment about how likely the /
amateur is given the model in the data
because that's what we've really got the
model s and the data X we've gone an
estimated the parameter alpha and that's
why this is sometimes called inverse
modeling because we kind of gone
backwards from the data through the
model to get the parameters whereas in
the forward model we had the model in
the parameters and we went forward to
get some predictions which a bit like
the data is that make sense that's why
it's good inverse so it's a bit like the
models a bit like a lens and you're
using it the other way around used to
use it that way around and now you're
looking at this way round and so you put
you put model output data in here and
you get the parameters coming out so
you've just done this switch in your
mind it's very natural to say well it
kind of makes sense right the value of
alpha that makes the data most likely is
my most likely value of alpha given the
data you switch the two dazzles as we
come across the second in the Bayesian
world it's not quite true because
there's a thing called the prior which
is about our previous belief that can
change that okay well the other thing we
can do is say well okay but what's this
uncertainty then around alpha we only
just want the single best value what's
the uncertainty around it and to cut a
long story short essentially you can cut
this thing at an appropriate place like
this and if you're if you if you'd like
to work with in the world of maximum
likelihood this thing for most problems
you just go down what a magic number one
point nine six units so don't worry too
much which gives you a ninety-five
percent interval on alpha so you come
down the curve in each direction you cut
it and that gives you your ninety-five
percent interval and alpha it's that
simple so now you've got an estimate of
alpha with uncertainty subject to your
model structure s and your data X that
makes sense pretty easy right I mean you
know when someone explained it to me
like wow that's really easy so for you
to do it obviously we almost never fit a
model with one parameter so so we can we
can up it and go let's say you had alpha
and beta so now you've got to prep
matters in your model we can kind of do
the same thing you can keep playing
around without from beta try all
different valued and you can plot as
surface here something like this where
each of these is it is a contour and
this is coming out of the board right
you you get that so again no prizes were
saying what in the maximum light you
well what's my best guess as well from
betta well it's here right it's the
value of alpha and beta that gives me
the highest the highest light unit okay
and if I want to get my uncertainty on
alpha and beta once again i climb down
this surface to some critical contour
and the critical contour might be here
and then this i say there's a
ninety-five percent probability that the
combination of alpha and beta is in that
region and if i want to talk about the
uncertainty and one or the other I just
drop down the lines basically from here
aerobar on our fair a bar on beta simple
as that and this logic extends into any
number of dimensions in theory so that
kind of makes sense so so before going
too much further let's let's maybe just
work through that linear regression
example I know that it's a lot of effort
to do a linear regression which you can
do with two clicks in Excel but again
it's really important to remember this
logic extends up to really complicated
models okay with hundreds of parameters
so in linear regression we say our model
structure is is actually like this is
where you can write it different ways
but technically it's like this
and what this says is that each piece of
data why is drawn from a normal
distribution with a mean of MX plus C
and a unexplained variation of Sigma did
I make a mistake that's good right pizza
so what do we mean by that well we plot
up a straight line here so it's got a
nonzero intercept size plot up a
straight line this is X this is y and
what we say is there's a bunch of points
here so the points don't lay on the line
so people you people think of linear
regression where the model is y equals
MX plus C that's not true because
otherwise the points it all be on the
line obviously not so what we actually
say is that there's a bell curve
centered on this expect expected line
here and this is the normal distribution
so the mean of that bell curve is MX
plus C that's the that's the line then
there's a spread of points around it
which which is defined by this standard
deviation and the larger that number the
more the spread so a linear regression
is a three-parameter problem through
perimeter mall so delta to do the
likelihood we just turn this thing
around I mean it's extremely simple for
a linear regression and we say for a
particular piece of data y so the
likelihood is is is we say that there's
a likelihood for a particular piece of
date of an observation why I given the
model structure s and given up
parameters equals and then there are
different symbols of this but we say
there's a normal density so different
people would use different notation for
this and this says what's the
probability that you would observe why I
from a normal distribution with a mean
of M X I plus C and a standard deviation
Sigma so your data is pairs of XY write
x1 y1 x2 y2 x3 y3 and then there's a
there's a formula for this and it's
don't worry about what it is for now
okay it's like um there's a simple
formula for this so it just says if it's
true that it's a normal distribution
centered on MX plus C then you can look
up what this value is now it's actually
a probability density not a probability
but it turns out that doesn't matter
with this stuff and then finally if you
want the log likelihood you take the log
of this but that's still for one value
and then finally you sum over everything
so the likelihood of Y given s and theta
equals the sum over all I of this that
makes sense it's that simple so now if I
give you a set of XY pairs and I give
you an M and a C and a sigma 3 numbers
you can return something called the log
likelihood that says this is how lightly
that combination of XY values would
arise from a model with those three
parameters m MC and Sigma and then once
we've got that we can do the same stuff
we just talked about so we can try lots
of combinations of em c and sigma and we
can find the triplet that maximizes the
fit the mle values and and those emily
values are exactly what you get out of
something like Excel when you do the
right click that's what's actually
happening behind the scenes is that that
thing is actually fitting the model and
it's finding the maximum likelihood fit
of that model to the data so it makes
sense when you hit when you do a linear
regression thing but again because now
we're in this this world here it's
important to realize that it is a three
parameter model so there's another
parameter which isn't usually reported
which is the Sigma and all three of them
have uncertainty and that's where you
get the spreading out and all that kind
of stuff right so that's linear
regression but most of our models are
not are not like that but even so a lot
of our problems actually boil down to
something that's quite analogous to
linear regression so if you think about
what happened what we said was he made a
predict another way of writing out would
be to say that there's an expectation on
why
that we usually do that right an
expectation on why I which is equal to M
which is a parameter times X I + C and
then separately to that okay we said
that the actual observation the observed
why so the observed why is a normal is
drawn from a normal distribution around
that expectation subject to a noise
Sigma so that's the other way you can
write it out you can break it apart it's
important to realize here then that if
you accepted this bit then you can have
any model up here really doesn't matter
how complicated it is as long as you can
create an expectation on your data given
whatever your predictors are so in this
case why being predicted from X but this
could just as easily be MPP predicted
from temperature and precipitation or it
could be you know the lifespan of an
animal predicted from its body mass or
anything at all and you split the
problem into the bit that you think you
can understand in terms of the processes
and then the noise around that around
that okay so but but but that's not the
only way to do it so some other problems
don't fit into that into that package
okay that's how you do that so let's
imagine that you actually did have
something like this but you had a much
more complicated model with 100
parameters let's say and let's assume
you could also have a more complex noise
model so for instance that the noise on
the model might depend on a whole bunch
of other things so you might have a
really hideous model with loads and
loads of parameters so it's very easy to
say that what you should do is try every
combination and get the best fit but you
but you're very quickly find if you do
some back-of-the-envelope calculations
that for interest in e size models it's
impossible to try a decent number of a
combination so you'll never find the
best fit because you don't know where it
is before you begin and it's just too
much computation to find it so what do
we do and that's the thing that's
actually really revolutionized the way
we're able to fit models to data and
that's what this tool fills back does by
the way which I probably not be able to
mo to you too much I'll show you
something in a second but so so the what
we do instead is and it is kind of a
weird thing to get your head around but
you can illustrate it again with
remember our two parameter problem alpha
beta now we imagine that there is a real
contour behind here like we saw before
there is some combination of alpha and
beta that gives us the best fit and
there's this contour but we don't know
where it is before we begin well we can
we have a technique and it's called
metropolis Hastings Markov chain Monte
Carlo there's a lot of letters mhm mcmc
is that but it's really simple it there
is it's a Markov chain is because it's
it's any one time at a particular
position in space so it's defined by a
combination of these so in this case
alpha and beta there's a two parameter
problem and then from there that's all
you need is that to determine where it
goes next so it makes it a Markov chain
and so it what it does it sits somewhere
and it looks at the likelihood here so
you pick somewhere randomly to begin and
it says what's the likelihood and then
you propose a new place say over here
and it looks at likelihood again and it
says what's the likelihood there now no
surprise if the likelihoods better it
accepts that move that's a really
obvious bit and so so it tends to climb
uphill and if you keep doing that enough
obviously it has a tendency to climb to
the top of the hill but if it picks if
it thinks are likelihood that's worse
and this is the weird thing it doesn't
reject it that it accepts it it might
accept or it might not it flips a coin
and it makes a probabilistic decision
about whether it accepts or rejects that
move and it does that based on how much
worse the likelihood is you played now
that has two important effects one of
them is that it means that it doesn't
get stuck so often in local Maxima as
there cause I want you to imagine that
there's a there's a place here that's
quite good but it's not as good as here
which is actually the best place to be
ok if you
you ever climbed up hill it might climb
up here and get stuck where is this
thing because it sometimes comes down
hill it sometimes comes down and makes
it onto the onto the higher mountain top
so that's that that's kind of the easy
there so that's so so you know and that
to cut a long story very short that the
point is now that you can this thing is
very blind it can be applying to any
model but it's very very good at finding
a good set of parameters no Matt and
this can work efficiently for a very
high dimensional space it's quite weird
because it is not too d it might be 15 D
and yet is able to navigate through that
15 dimensional space to find the best
set but actually that's not really why
this particular cheap algorithm is
designed that way it's designed actually
to do something incredibly magical so to
illustrate that if you can see this is
let's go back to imagine that we did
just have a one-parameter problem like a
parameter a so for a one-parameter
problem that the viet the algorithm is
just hopping back and forth right on the
line that's all it can do so it's only
got one dimension of the poor thing that
it can actually navigate in so you'd
start some way here and it would maybe
move this way maybe sit there for a bit
maybe move back maybe move forward and
it reaches and if you watched it off
long enough it would go pump a moment
like this and it would go back and forth
and back and forth and important it
never settles down it never freezes on
the best set because there's always some
probability it's going to come back
downhill right so it never settles down
he just goes back and forth so it's like
so what well if you actually break up
this axis now into chunks and you say
what fraction of the time did it spend
in each of those chunks you'll find you
know you plot that as a bar chart let's
say you'll find something like this and
you find something really amazing which
is that that bar chart exactly follows
the underlying likelihood surface and
took and it might take a while for that
to sink in why that's so magical but
let's let's look it back at
Judy case again so if it is in 2d now
from beta now this chain comes in from
the cold and then it starts moving
around like this and that's the bit I
can actually show you so get away from
me in the board so the thing is moving
around and
so we have a little so we're not going
to we're almost done with this session
so I'm not going to actually take you
through tutorials about how to use fills
back today but this is what fills back
does behind the scenes but you can use
fills back from C++ from our and you can
also use it inside the thing we're going
to use this afternoon called
distribution modeler and you can use it
from MATLAB as well so just get in touch
if you want to use those things but if
we do i'll search for me again so
remember them that this thing is
wandering around in this
multi-dimensional space and it never
settles down so let's have a look down
here things just in a spin open for a
second so it goes wandering around and
around it never settles down but it
distributes its time here in proportion
to this likelihood surface and so what
it means is that if we we allow what's
called a burn-in so there's a phase here
where it's coming in from the cold it's
finding the right place to be and so we
let that happen and we wait until it
seems to have sort of settled down not
frozen but sort of settled down where
it's distributing its time and once it
has settled down we say well if you want
the best guess basically at our
parameters here we can just look at the
average of where it spent its time to
average all the alpha positions and
that's called the posterior mean and for
a problem with non informative priors
and plenty of data that's going to be
our MLA value and similarly if we want
to do the the ninety-five percent
interval around that mean we find the
region within which the chain spend
ninety five percent of its time and if
you want to know what the correlation
between these parameters is could we
often have a correlated space here we
can just look at the correlation in
these samples which is treat them like
anything else and look at a correlation
so all that stuff just comes flowing out
for free and again I'd remind you it
works efficiently on really large
problems
now it does still require quite a bit of
computation and which is why it's only
reached prominence in environmental
science recently but let me just give
you a quick example we have for example
if we do the linear regression you can
read about the model there and you can
see the code that would run that in C++
here and then we can have a look at some
of the data here so obviously we've got
points that have come from that model so
we just generated those points
artificially but now we can start the
sampling and when we do this this we're
going to start somewhere random and then
the thing is going to move around right
and so we can see that here you know
what it's too fast I think we ought to
pick a slower one like this one that was
too fast to see here we go so here it is
moving around and it's this is the
burn-in it's coming in from the cold
okay and it's finding that it's better
off down here so this the likelihood is
so bad that it sort of propels the chain
this way but after a while it starts to
settle down into this sort of banana
reshape and you can see that although
the chain itself is moving around all
the time the the shape of those samples
is kind of stabilizing and that's that's
our sample them from this posterior so
if we want to know what the this is m0
here what what the bayesian mean on that
thing is the posterior mean we can take
the center of gravity of this cloud it's
about here and if you and similarly for
this parameter and if you want to know
the spread we can we can do that spread
in either direction okay that's the idea
so it's phil's back in a nutshell and in
so let me just I could do a couple more
concepts or I could do the stuff here I
think what I'll do is I'll do the
concepts because we're about to break
and when we come back we're actually
going to be fitting models using fills
back anyway in distribution modeler so
we may as well leave it until then some
of it some of the details about how you
can use fills back so I will finish on
time by the way at four
I would never take anyone away from
their tea or is it coffee here I guess
you guys will see okay so what's left to
say well look that what I like actually
is the fact that to be perfectly honest
I don't think there is that much more to
say about the overall idea about this
approach to 50 models to data and just
to backtrack and you saw what i mean by
the idea that we can imprints will fit
any model now to date it because as long
as we can write the likelihood long as
we can say you give me a set of data and
i can say given my model s and my and
for a particular set of parameters long
as i can generate that log likelihood
then that is enough to set that thing
running and then i'm going to get my
estimates of my pram no matter how
complicated the model is in principle
it's going to work fine and it and it
works for really horrible models now so
you have to worry so what's missing well
the main thing that is missing is that
so far we haven't really been in what
most people would think of is a bayesian
world and the reason is that really what
we should be using and what we have been
using is the idea of a posterior
probability density not a likelihood
surface and the difference is the
posterior this is the true the actual
probability we didn't use that p yet the
probability that the data would arise
from a particular model structure thing
here is proportional to the likelihood
but it's also needs to be multiplied by
this which is the probability of the
parameters in the first place now this
doesn't have doesn't on it this probably
could be conditional on s to be honest
but that's a bit pedantic but the idea
is that given our model s you could
actually have a guess ahead of time
about what the parameters are likely to
be and the more ideas reach your model
the more you're going to have an idea
about what the parameters are so if we
got something like the maximum growth
rate of a tree let's say in millimeters
per year I can tell you that something
like 10 is what you're looking at
and if you're talking about the maximum
height of an Amazon tree you know it
should be about 40 meters and so on
you've got an idea about what they are
and that's what this thing is so it this
is called a prior and it says if this is
my parameter alpha and this is my my
prior on alpha then I can actually have
some prior belief here in my problem now
if I say I don't have any prior belief
I'm ignorant then I represent that by
making this a straight line what it says
is i think i think for a height of an
amazon tree one centimeter is just as
likely as one kilometer okay obviously
it's not true right but I could do that
if I want to and if I do that notice
then that the likelihood only depicted
if i compare two different parameter
sets which is all remember that's all
that drives the chain the metropolis
thing all it does is compare this thing
won't be any different because it's flat
when it proposes a different alpha or
get this so i can ignore this and i can
just run it on here and that's why a
likelihood analysis we think of is
really working on a posterior with it
what's called a non informative priority
and then just in the last minute or two
I just say so what happens if you do put
an informative priori no it's very
simple is that you can now draw two
things i can draw my likelihood profile
surface which might be like this and
I've got my prior belief which might be
say like this so this is this is what i
believed beforehand this is what's in my
brain this is what the data says and
what's going to happen is the chain that
much that that chain that the metropolis
algorithm is going to compromise between
these two and it's going to give me some
hybrid which is going to have a mean in
between the two so something like this
so it's going to balance between what we
thought was true ahead of time and what
the data seems to be telling us now if
we have huge amounts of data this will
be much stronger than this and will
simply get what the data says so it
doesn't matter what we believe
if we've got huge amounts of data I
beliefs get swamped by the data if we
have tiny amounts of data then all we're
going to get back out is what we
expected in the first place the prior
but men a lot of the time we're
somewhere in between we do have some
previous expectations and some believe
on the one hand and we've got some data
but not enough on the other and the
Bayesian thing very very naturally have
reduced between that it's okay and that
honestly is basically the difference
between a bayesian approach and a
maximum likelihood approach and what's
funny is that they're often and I don't
know how many people are familiar with
these ideas at all but they're often
presented as if they're two very
different worlds like oh you know you're
a frequentist and you're a bait or
you're a bayesian or when use different
tools my MC MC is a bayesian tool right
and maximum likelihood you use simulated
annealing apparently with a likelihood
profile etc etc but really to be honest
I just I don't think it's a useful
distinction for a lot of people really
you should just think if i put a prior
in its bayesian but basically the
concepts are really really similar and
for complex models a lot of the
machinery that's traditionally used in
maximum likelihood is not efficient
enough to fit them anyway and so you end
up using the the mcmc machinery which is
more traditionally associated with a
Bayesian analysis okay now just in the
last minute there are a few other topics
that do come up in this kind of work so
I guess I'll just I'll just list them
really quickly we'll come back to them
after the break anyway so one is when we
go to make our predictions and this for
me this was the single piece of magic
that meant that I just dropped all other
forms of stats and so onwards what if
you want to make a prediction and put
the parameter uncertainty through the
prediction well you know we've got a set
of samples here right from from the
posterior distribution so if I can
afford to run my model a hundred times
all I need to do is pick a hundred of
those at random and run the model a
hundred times with those different
parameter sets and the output that I get
is going to is going to be it an
unbiased estimate of the uncertainty on
the model predictions it's incredibly
easy
to do so again this could be a hideously
difficult model but if I can run that
metropolis Hastings chain on the thing I
get a set of samples of the parameters I
choose so many of them run the model and
boom out again it can be anything so it
could be the potential response for in
see if you want to know what's the
probability that the Amazon rainforest
would collapse under climate change in
principle if we've got a decent model
and enough data we run this on it the
answer is well for some of those
parameter sets it collapses and for some
of them it doesn't and if we choose a
hundred at random and it collapses for
15 the answer is fifteen percent I know
so just a couple of other issues so one
is we often want to choose between
models as I mentioned long story short
there's there are ways to do that okay
they're irrational ways to say if I run
this entire analysis for several
different models then I can make an
appropriate choice between them and
there's a thing called hierarchical
Bayesian analysis that comes up okay and
again I go through that but it's not
very difficult in practice it's actually
really easy to and it's a very simple
extension of this basically this idea
that and and I think with that because
it is according to my watch one minute
past four we will stop so that was the
lecture style bit thanks for that and
again thanks very much for being up for
trying fetch climate I'm sorry we hit a
few brick walls there nonetheless I'm
going to throw myself open to falling up
my face again so where we come back
after the break we're going to try and
get hands-on with a tool that enables us
to specify models and run mcmc chains on
them and make predictions and we're even
going to try and push those predictions
in to fetch climate and then we'll we'll
find out how foolhardy that idea is
after the break so enjoy your break
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>