<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR Vision Faculty Summit - Visual learning @ CALVIN | Coder Coacher - Coaching Coders</title><meta content="MSR Vision Faculty Summit - Visual learning @ CALVIN - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR Vision Faculty Summit - Visual learning @ CALVIN</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jGRyTCpgboE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay welcome to this talk I'm vitara
Ferrari and I ran the Calvin research
group at the University of Edinburgh a
little bit of it is still left in Zurich
but we are migrating so today I want to
talk about visual recognition particular
object localization which is one of the
main tasks on my group so let's have a
first look at what can we do today or
over the years the things started to
work the 90s finally in recognizing
specific objects such as this beautiful
toys against no background and a lots of
training images so that's i would say
solve since 2002 there are a hundred
percent recognition rates there good job
we managed to do some visual recognition
to work then later on people said well
you know specific objects exactly the
same identical beer-can that's kind of
boring can we do object categories so
all instances that you would associate
with the same noun in language so I
start with faces which are obvious
application and they have limited
variability and people have been banging
up and probably arguably since viola and
Jones two thousand and one things I've
been working very well and i would say
since 2005 year we kind of reached a
platform it's the task is almost solved
this means more than ninety percent
detection rate when the number of false
positive per image you get is about one
in ten for me that's a good point about
one in ten is not very useful so faces
that's one class it's always frontal
moderately boring so the people moved on
to something harder and this data set
I've had the pleasure to see and work it
on a lot in the last five years digit a
ship classes you start to have more
category there are five they're all
defined by shape so you cannot rely on
texture patches too much gets harder
still this data set was fixed viewpoint
and there was never occlusion then i
would say that recently we finally come
to the point where he can be considered
solved above ninety percent detection
rate at one false positive rate n images
so that's kudos to that as well however
the
holy grail is fully and constrained
mixed viewpoints occlusions regression
and many categories so everybody's
working on the basket visual of your
class challenge and I have taken the
pleasure of going through the 2011
challenge results and the best method
which is unpublished very interesting I
hope the others will make it available
soon and gets below fifty percent
detection rate that one was positive
every ten images that that's not solved
and far from it so this fully
unconstrained case typically gets better
when you have more and more training
examples and another thing that we would
like to get is more and more classes
hopefully getting into the famous 30,000
that villain from his dust in the 80s so
in order to learn a good model that is
able to achieve this this performance
these days what you do is the following
you collect a thousand examples about
fare per category you have people
clicking bounding boxes on each example
because modern detectors they need the
van in box annotation to train and you
repeat this for many of you points than
for 10,000 categories and after 100
million clicks you're done so of course
somebody did that yet fortunately
otherwise I'll be out of business
mannion rotation is very time-consuming
and even if we could do it as some
people argue we could buy Mechanical
Turk maybe Microsoft gives us a nice
clicking grunt but even if we would do
that it would be incredibly boring and
we would not have learned anything like
maybe except us to distribute money to
the third world awesome we would like to
do something humans can do which is
learned without a clicking at least
after certaine child period where they
train themselves so in fact that's the
main goal of my research group to try to
reduce the amount of supervision that it
takes to learn object actors and today I
am going to try to give an overview of
three of our recent works they are all
future work so in this sense i'm looking
at the future three month ahead all
activity are and we try to exploit other
indirect sources of supervision rather
than humans clicking and the first work
the supervision will come from the
motion that's absurd
available in a video we're going to look
for YouTube videos which maybe would be
more suitable for Google than
Microsoft's and another source of
supervision is trying to exploit the
semantic relatedness in classes that are
organized in a hierarchy and a third
thing you can do is to try to learn
multiple classes jointly in images
annotated by which classism appear so
let's start from object detection video
so as I said is the current way of doing
things lots of training images with
bounding boxes you got an object actor
model what we try to do is obtain it
from video the big problem is a video
it's that it is easier to segment the
object from the background than in
static images because typically it moves
different than the background and that
replaces the bounding box and that's
what we worked on so far another big
promise would be that the video shows
the range of your points so it's easier
to be in the multi view model that from
separate images and of course there is
lots of evidence and studies in and off
of the brain itself but also in
development studies of children that
object motion helps a lot and learning
vision so there are many challenges well
where do you find the videos that's a
big one once you've found the videos how
do you detect the object of interest in
there without you man annotation how do
you put it in correspondence between
videos then how can you learn a class
model that is learned from video but
performs well on still images because
what we want in the end is betta basket
challenge right and that's still images
can you learn from video and the
technics two images that's a big task so
here is our view of the pipeline we got
our training videos and we want to get
an object actor so the the first thing
we are going to do is to try to exploit
motion segmentation to propose some
candidate spatio-temporal tubes that go
through the video some of them would be
on object some of them will be on the
background fashioning objects then we
are going to try to select one tube for
each video so that's hopefully joint the
selection will get those tubes that are
on the object then we are go to sample
some frames and train an object detector
so that would be a pipeline that purely
on video however what I am interested
especially in is mixing the training
data from the video we training data
from still images from Pascal in order
to get
better object actor from the two sources
and here is a some of the videos in our
database for ten classes from Pascal
their thang classes that move so potted
plants are so fast boring birds and cats
fun that's what we do a we collect about
five hundred shots for each class and
the Perficient video into shots so you
can see it's highly variable especially
cats they like to roll around a lot kiss
gasps that are filmed may be sitting
cats are not so Phil mobile so the first
thing we do is to do point tracking
motion segmentation with the method of
Sandra yam and blocks which is very
recent very nice basically its long-term
optical flow that goes beyond two frames
and for videos like this one where you
have a very clear very funny jumping cut
with a clearly different background the
objects very big it works very well
however you can also have videos like
this cows tend to be em a lot lazier
than cats at least one filmed and they
hang around very slowly and they don't
get segmented so we have to manage to
work with this kind of data once you
have this motion segments you can then
fit a spatial temporal bounding box
which we call a tube we can do some
rubble statistics to avoid points that
are moving substantially different than
the rest of of the segments to be a
little bit more more robust and you get
these candidate tubes we call them so
which one is the cut well from a single
video it's going to be very difficult
you know why not the little thing being
the cut but fortunately we have many
videos and that's where I did the smart
part of the word comes in so the task
will be to select one candidate tube for
each shot in the videos so that a
certain energy function is minimized and
this energy function can be visualized
in this graphical model form where every
node represents a shot the gray nodes
are observed and represent the image
features you observe in the tubes and
these bigger bubbles are latent
variables that represent which of the
shot in the tube in the video you pick
so these are integers that go from one
to the number of Canada tubes in the
shot
and if we look at the terms of this
function we made you have unary
potentials that basically operates
within a single tube and they try to
evaluate whether the tube is
self-consistent attracts a single object
so its appearance is homogeneous in time
it tries to also have it more in the
middle of the image elsewhere and it
also as a term that Steve's the
selection process to objects rather than
background this is a work that we did
CPR 2010 that i advertised it a lot of
times basically it's a general measure
that measured the appearance of a window
have to our likely it is to contain an
object with a closed boundary and
different opinions on the background in
general class generic this is other work
so the inner potential alone tries to
prefer certaine tubes over others on
their own but the very interesting
potential is the pairwise one which
compares two tubes in different shots
and looks a similar there this makes a
lot of sense because you are operating
within a single class so all airplanes
to look somewhat similar but as
hopefully their backgrounds look more
different so if you want to minimize
this combination of unity and Pervez
potentials hopefully you get one shot
per one tube there shot so that they
look somewhat similar the selected ones
and they are opinions consistent and
objects because we have a fully
connected graphical model we optimize it
approximately with related message
passing by Cole mcgaughan company so
this gives you a very nice approximate
solution so here are some results of
this selection process to the left you
see the motion segments pretty bird and
in the middle you see all the candidate
tubes and to the right the one that was
selected by our process remember it has
seen many birds in parallel it didn't
select it only on this one you can see
sometimes object move very little
sometimes like the motorcycle to move a
lot sotas even go beyond occlusions
these are all the cases where it works
very nicely because there is one tube
that's on the object if there is
normally picks it but there isn't always
actually our results indicate that there
is less than half the time of sometimes
there is dis over segmentation phenomena
where the wheel of the motorbike is
taken fig my laptop is struggling
flagging to follow these videos yes o
heroes yes pixel why and no no I was
trying to give an overview but sure I
can give him more detail sure so what we
do there we sample a fixed number of
frames on the tube let's say 20 per tube
inside there we compute hog bag of surf
other 50 feel like an den color
instagrams and then we compare every
pair of frames between two tubes take
the max of each and learn the waiting
between these different appearance cues
so glad I standard okay this doesn't
move very much which is about the point
I think my laptop got my punchline
better than me and if the desserts
doesn't move and so you don't get a
motion segment and then it fails then
you can sample once you select the tubes
can sample some frames out of them and
use them for training so here you have a
pipeline that makes some sense I hope
and you can try to see how well it does
you use the Pascal 2007 test set which
is the latest one for which you have
tough data available and you can play
around and you will use the standard
evaluation protocol so you only count a
detection as correct if you overlap
slightly with the object more than fifty
percent intersection over Union as we
said we focus on objects that move sofas
and potted plants are boring and also
very difficult and here you can see some
just focus on the last column you can
see what happens if you train Felton's
all my ramen on the for mobile part
model which is currently one of the best
working you train it from a standard
still image data it does about this
performance if you train it from our
automatic pipeline it does about half
but what is interesting is what would
happen if instead of using my automatic
pipeline I went and labeled every frame
of the video well 100 per hectare video
and manually clicked on one in boxes so
the best possible case for video well
performance is still floating over there
at the same spot so this is good news it
means automatic pipeline doors close to
the best you can video but the bad news
is well why is video so much worse than
still images for a comparable same
amount of training data in terms of
friend this shocked us for a while we're
thinking maybe you should stop and not
publish a cpr after
then we said now let's try to mix the
data but first we got to go to the
bottom of this issue otherwise doing so
bad and in fact we discovered by an
inspection that we face a domain
adaptation problem the quality of the
video frames is substantially lower than
that of the images still images are
better in terms that their gradients a
lot sharper you can see that well maybe
on this display you know sit so well
that the video will be a lot more blurry
on the edges and it has compression
artifacts that's a lot heavier at least
on YouTube videos and to prove this we
collected certain manually rotated
bounding boxes and computer the average
strength of the gradients in there and
then we rank them by strongest to
weakest gradient and you can see Pascal
images are a lot sharper a lot sharper
then we also found a fantastic paper i
recommend to everyone one of the best
papers and recognition there is no no
novelty there but it's fun so albinius
and vayas look at data set bias where
they said what are we doing let's try to
train and I plane detector on video and
test it no I'm wrong let's train a
generic detector to distinguish video
from still images can we do that class
independent just tell me if it's a video
frame or an image and you can withstand
the features you get to eighty-three
percent accuracy that's bad and that
really tells you that the two are
different domains so we need to do
something to try to breach between the
two domains and there are you know maybe
I don't go too much leaders but there
are many standard methods that you can
find a fantastic paper by Hal donated
shows us a very nice name this guy and
some of the standard methods are for
example the prior matt is the most
common in computer vision you train a
classifier on one domain the source and
then you use it as a prior to buy as the
perimeter of your target classifier that
steals the target to the source and
helps you to communicate between the
domains or lenient which is the one that
works best for us you can train a train
a classifier separately on target and
source domains and then linearly rate
their output on the test domain in order
to to to keep them together the world is
funded where does many HD videos which
we couldn't so easily download but maybe
one day things will get better
we didn't try that so this is an
argument on youtube videos so here are
some experiments with these two main
habitation where we tried to fuse the
training data the still image high
quality and the automatically discover
low quality video we try to fill them
together bridging with this domain
adaptation techniques and this is a lot
of numbers but what a message is is that
if you try the standard approaches so
all the approach all is mix all the
training data plan and priorities to
meditation techniques you always get
negative transfer basically a single
learning model gets confused by the
different distribution in video and
still images and you actually do worse
when adding video we were really
desperate at that point then finally we
found this leaning technique which
sidestep the issue because heat range
separate detectors and only mix the stem
at the end and that one gives an
improvement what do you see these are
all deltas with respect to the best
model of let's elope on the VLC training
data so above zero is good at zero means
video didn't help and we never have
negative transfer in every values below
zero and you get p % mep improvement
over after classes so here are some
examples of what happens when you train
a director on on you're still images on
your video and then the combination and
typically they help each other to refine
position and sometimes to get amazing
phenomena like they are both completely
wrong but when they work together they
get a cut so it is possible to improve
instead of the our detectors by having
videos just thrown in without any human
and notation so that for us was a good
news so now we'll talk about knowledge
transfer and I hierarchy which is a
second way to try to get rid of minor
supervision there is this fantastic data
set by five alien orders at Stanford
called imagenet did you all hear of it
maybe she should ask the reverse but so
basically here you have 16 million
images over 15,000 or 16,000 categories
organized by parent-child relations so
our crafts our parents of airplanes and
airliners some classes have a few images
annotated by bounding boxes about five
percent of all images have one in boxes
so la
of room for us to try to to do better if
we had money box on objects for all the
images we could train object actors and
your soul computer vision win Nobel
prizes stuff like that but we can't so
the purpose of our work is to try to
exploit those classes that do have one
in boxes in order to transfer knowledge
to the classes which do not to populate
these target classes with bounding boxes
our dream is that one day we will have
16 million accurate funding boxes and we
are not there yet don't hold your breath
and so these are some examples automatic
so there is not transferring knowledge
from the sources to the target classes
so they the overview we have is that we
are going to consider for one target
class say this one we are going to
consider its siblings so other hard
drives and ancestors which are all the
classes above it until the root of
imogen we are going to consider them as
the source as if they abandon boxes we
are going to transfer three types of
knowledge location opinions and context
I'm going to go over that and then
combine them together and the general
scheme is that you go on a pair of
knowledge source and type and you learn
a model for it and then you can use this
model to evaluate every window on your
target image which don't have a box and
you obtain a window distribution which
basically captures an uncertainty of
where the object could be in that image
then we are going to use this to to
drive the selection of other than money
box for that image so the first type of
knowledge we transfer its location we
are going to basically take that the
source class which has spawned in boxes
and learn a KD distribution over 40 and
then you can use this caddy to evaluate
the windows and you get something that's
not very informative but you know it's
kind of better than all of the window in
the image so this should be four
dimensional but i displace it in 2d here
then a very useful type of transfer is
to transfer the appearance of the
siblings so siblings are very related
obviously because they're siblings
especially imagenet they you know you
have a transporter van as opposed to a
police van it's quite similar you can
learn on a penis model for it's a a
chi-square SVM on own bag of words and
then you can use it to evaluate all your
windows and you get a type distribution
another thing you can do is transfer
appearance from ancestors instead of
from siblings and and for this basically
because we go back to ancestors to the
root of the imaginary we use our object
nice work which I desire design we
described before that tries to separate
arbitrary objects from their backgrounds
not class specific and this gives you a
more broad distribution but still on the
object you can also try to transfer
context I think is a particularly fun
idea we might believe that sibling
classes look similar but in fact the
background is stand against a it looks
more similar so my argument is always
cows and horses kind of look like but
the grass they stand against it's always
exactly grass and in fact if your
engineer your way through you can make
contacts model that captures only the
background and the way we do this is to
cluster background windows in there in
the source class into background
prototypes and then measure our clothes
positive windows are two day but Ron
prototypes to learn thresholds that say
beyond this value in opinion space you
you're pretty you below this value are
pretty sure to be a background patch
then you can use this again to evaluate
windows in the test image so now you've
got a lot of window distributions they
all kind of give you a probabilistic
opinion of where the object is we got
integrate them all the way we do this
well log linear model where we wait the
log probabilities by certain weights
alpha and you can the best alpha so you
can ever design to find are those alpha
so that the combined distribution has a
maximum as a mode which is most
accurately localized on the object but
of course you don't know where the
object is because that's it so you have
to train it on some other part of the
tree in imagenet but you do have
bounding boxes so we do all this
cross-validation all the time so this
function if you try to maximize it it's
tricky because you need to find a
parameter so that the maximum of the
distribution is good so that's a hard
worker but if you replace these arc max
with a softmax then you get you get a
differential objective which you can do
gradient descent on to and then you get
optimal waits for the task of
localization and then to conclude once
you select one window per image on these
target images for one class you can then
use this localized windows
train appearance model which is for the
target so for the police van you can use
it to add it to the combined
distribution and improve things
iteratively this is a good thing to do
it done in many works but in this scheme
is linear in the number of images of the
classes supposed to quadratic in our
previous work in sec v which allows you
to do it on a large scale so here are
some results we work on really we try to
be as large as possible we took 219
target classes they do have one in boxes
so till we pretend they don't we keep
the bunny boxes only a few they only
have maybe 100 out of a thousand but we
don't use them use them for evaluation
because we need to know how well we are
doing right so we took on purpose some
classes which have just a few bounding
boxes and put them in the freezer never
look at them just for evaluation the
siblings also have been in boxes and
those we do use for transfer in total we
have five hundred thousand images we
work on only twelve percent of them
abandon boxes which we do not look at
and then we evaluate pascal intersection
over Union this hard criteria of
localization averaged over everything
and these are a lot of numbers of course
but what the message here is that we
tried all combinations of a source
sibling ancestor and type of knowledge
and with this particular type of
training of the Alpha parameters every
every combination improves overall its
components which is quite pleasing and
what one sees is that object list alone
is ready doing quite well 30 around 50
but if you integrate all the cues you
get a lot higher and also if you exploit
an opinion smaller which is for the
target class learn on the fly you go
even better and not surprisingly bag of
soft cast square is the best at the desk
you um want to claw conclude by showing
you some actual results which are quite
informative I think on this scale we are
not aware of previous works that tried
object localization and a half a million
images so it's quite interesting to see
what happens if you ask the software
give me random results you can no longer
look at them yourself and these the
first row is a classic result it's a
classic situation imagenet the object is
quite centered occupy soft the image and
we get it right but our meta rosoff can
find very small objects like the candle
next
lady or a soccer ball next to the dog
and more interesting even is when you
fail what happens some some images are
so imposed like here there are many
balloons which one is with the ground
truth annotated it's completely
arbitrary or metal fondle balloon but
maybe it's not the right way also
sometimes it finds groups of animals
that you know correspond to the concert
but they are not one anymore my favorite
example is this image is labeled
basketball and you know this guy's
playing basketball is not so that that
is not on the ball and here are some
more classic results where it finds the
object half way but is not very accurate
it is a distribution over all the
sixty-two thousand boxes we could
evaluate of the iou result and fifty
five percent might seem small but in
fact the typical result is around
seventy percent which is very accurate
for this measure and many windows we
just thought many object we just don't
find them at all so the peak is the fact
good enough we believe to train object
detector soft words so I would have had
a third part but I'll skip it because
with 25 minutes already I'll I just stay
questions I think that's a better moving
the interest of time thanks for your
attention so Andrew had to go so I'll
take over beautiful isn't there a bit of
a self-fulfilling prophecy and that the
tubes or the the transfer is is using
measures of sort of measures of color
and appearance that are easy and so the
thing that is that you're able to find
and you're able to learn and able to do
then on the unlabeled data are the
things that are sort of easy to segment
already right so an object that changes
appearance drastically over the course
of a video won't have a satisfying tube
and therefore will be rejected and not
trained okay that's very interesting so
training appearance drastically can
happen only issues we turn off the
lights or the object changes viewpoint
and the other viewpoints that
extremely different opinions in fact I
would argue that that's only partially a
problem in fact I shown your sequences
with their pants turning around as long
as the other videos also have been
standing around there will be some
viewpoints that match and then the
opinions q which is simple like you
argue will be satisfied so if you are
comparing on a car going to the right
the cargo in front the two will not
match well and we probably not be
selected unless the United potential
helps you but because we have a variety
of videos it always happens that you
have you know some of the view points
correspond between two videos and there
they have been a similarity function
fires which is also a maximization
overruled the viewpoint so you know i
didn't give and anita's we keep all the
tubes we have like 20 per shot and many
of them are really absurdly small Thanks
ok so in your first word you said that
your performance which was like off the
detector which was trained on the video
and evaluated on the still images was
not perfect I think one of the good
reason for that could be that there is
much higher variance in VLC dataset
because if you train it for example from
1,000 samples but of the same object
then for example if we used color as a
discriminative feature you would train
for example something from a red car and
then you would try on some different car
and it would not work because you know
one of the reasons is not really that
exception to have same amount of frames
of the same object to training the
detector you just need different objects
which are of the same class because you
just don't capture that variant so I
thought that's a great question actually
because it allows me to clarify things
in fact what we trained this video
detector from was all of the shots of a
class that we downloaded from youtube
not one object at the time so the the
algorithm that does the selection
already goes over all shots about 500
and they typically contain maybe 50 to
100 different objects there are many
repetitions within a video but not
between videos and the model that
strained on top of them is not color but
it's the standard forces of tpn model so
in fact we have diversity is not one of
study time does being trained is joint
over the class but still your point
awesome you still have a point although
I corrected the technique because still
with the amount of video data we have if
we had with the same number of friends
they are distributed over about 10 times
less different object so there may be 15
success with many of your points instead
of 500 in caesars so there is an issue
there it's tough to collect so many
diverse videos that's an open issue so
we are running out of times there are a
few questions that we can probably take
them over lunch so let's thank Vito
again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>