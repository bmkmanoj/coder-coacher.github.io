<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generative probabilistic programming: applications and new ideas | Coder Coacher - Coaching Coders</title><meta content="Generative probabilistic programming: applications and new ideas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generative probabilistic programming: applications and new ideas</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M_vG_6pq0XM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so today we have a Europe air of
visiting us for the day euro is about to
finish his undergraduate degree at the
Siberian Federal University he spent one
year at MIT working with the cash
machine and josh 10 mm and since October
he's been working with Frank would at
Oxford and he'll be studying his PhD
they see our if I'm correct and he's
here to talk to us about and probably
stick programming languages good mornin
so I'm going to go through January
privacy programming applications and new
ideas the structure of my presentation
will be as follows first I will describe
briefly about Pro ballistic programming
will show you a few example of
generative probably graphics programming
then I will describe briefly some
advanced and techniques for better
inference for probabilistic program and
then I go touch briefly what I'm more
the most exciting point of my talk is
automatic programming using
probabilistic programming languages oh
and how I like to describe publicity
programming is first of all to compile
compositional representation of your
probabilistic models so this is picture
I think familiar for all of you it's
Latin additional allocation model and
for humans it's very easy to present the
model in this way so if you look to it
you understand what it is it is a
representation of this model and it's
handwritten sampler written in C++ it's
about three hundred lines of code and
you can actually make inference in LGA
using this representation of this model
and this is third representation of the
same model lda and it's about 10 lines
of code to describe the model and then
you have observations as many as you
have your chin your data and do you can
see that Pro ballistic programming gives
you opportunity to write in compact way
your model and
using different infants techniques I'm
sorry for example metabox has an
algorithm sequential monte carlo mcmc
variational inference and other
techniques we are general-purpose
techniques rah making friends in your
models and this is church like languages
mit georg baer venture paper hopefully
will come very soon sequential
multi-column p.m. she's been developed
at Oxford by Frank woods group and I
will briefly touch how inference happens
in charge like languages using multiple
sites and algorithm so imagine that you
have your model image that you have some
bison network model the easiest simply
can't describe in turn complete Church
language you have no TV s which is
sampled from prior from categorical or
point 10 point 9 and then you have not
vm CH which depends on f vs and if it's
if m vs is equal to 0 you central MV CH
from categorical and if m vs is equal to
1 your sample from another category
cookies and other parameters and then it
describes a whole version network in
this way and then to make inference in
this model imagine that you have some
last notes and your model fixed so you
have some observations from your data
how you can make inference described in
church paper you sample first of all
your model to make for evaluation you
something from prior then you remember
all random choices you have to your
random DB and then you select for
example uniformly randomly origin other
scheme some random choice you make local
perturbation on it some using some local
kernel and then you run the whole
program review all the random choices
whenever possible getting them from your
random DB and then either accept or
reject this small perturbation of this
node by calculating your Maj substance
ratio where you have the
ability of all state of all execution of
your program your new locks core of your
new execution of a program and
transition kernel which in this case
will be if we pick up notes uniform
randomly 1 divided by number of random
choices in your all random DB the local
proposal on your note and potentially
new randomness you create in your
program but because this is a fixed
structure you don't create a new
randomness but potential you can if
you're generative model has for example
at predicates flip Bernoulli statistic
be another random variable and you
either have one statistic procedures in
predict in consequent or other sahaja
procedures in alternative and you create
and remove urine your randomness does
this make sense and hopefully you can
judge from prior to posterior if you
don't have h cases if it's a gothic and
smoothed enough yes flipping a predicate
and new new random variables are
introduced how do you sample the values
of those new random variables there are
different approaches and the easiest one
who just simple it's from its prior it's
the easiest way you can do and it will
yoke it's not the best thing you can do
but it's it will look I'll agree that
it's not the best sink ok so this is
another example of model you could write
in Ranger I don't know how familiar you
have is I think first of all you're
familiar much publicity programming I
don't know how familiar with church like
languages adventure should I show this
example ok so here you see syrupy
mixture demo written in church like
language ok sorry one moment
sorry
ok one I'm sorry okay great so here you
don't have any points yet and we have
two dimensional syrupy mixture demo so
well first of all we describe our model
so we have some alpha hyper parameter
which is sample for example form uniform
continues then we have our cluster
syrupy process we create new Chinese
restaurant process with this parameter
alpha hyper parameter and then we have 3
lambda procedures first is you get
cluster min depending on the class to
Reggie and dimension then you have
cluster variance again depending on the
class to Reggie and dimension one or two
and then you get cluster for your point
edgy and because we don't have any
points we don't have any date and judge
described the generative model and if
you put point here you first of all
sample for this point number zero it's
cluster and then you sample a mean and
variance with this cluster but this
predicts adjust for you so if you run
this program and get what the world
class service point it relates to and
the main things that you have two
observations so you observe that your
classroom mean of the point number zero
with the class relate to and dimension
one and this with variance point number
zero with for cluster it relates to for
dimension one it's equal to minus five
point 25 so this is x coordinate of this
point and you define the y coordinate in
the same way and you add some noise
which is normal so it's classical
therapy mixer demo with some noise and
if you define a few other points you
have all your generations defined as
probabilistic programming code so you
define your model and then you add
observations and you see in real time
how infants converge to a clusters and
here i described not very interesting
model
not interesting data but
does this make sense on this one this is
K parameter I just omitted it when I
described the model but they e is when
you have your variants you have your
very own distributed by better and
multiply by some scale because you don't
know how logical I still be you have
additional our uncertainty on it and you
can see scale and scale as well helps
you to find to find your solution by
this I mean so sorry yeah this is from
posterior yes okay and now i'm going to
application so our this is one example
of generative probabilistic programming
in graphics it's nips paper which was at
the snips 2013 and i will show you how
you can easily write your generative
models and apply to in different
applications in this case and vision and
this is joint yoke by with lucas ransom
contagious Kulkarni who are mine authors
of this paper and with josh o Tannenbaum
and the idea is that computer vision is
a pretty developed field it's for many
years many much work done in this field
and there are efficient bottom-up
approaches for example feature
extraction segmentation but there are
limitations of bottom-up approaches
envision for example they require large
training call process as well let's load
label training data for many approaches
and bottom-up techniques for vision they
difficult to build including the
software engineering aspect and they
difficult to modify they require a lot
of feature engineering the clumsy in the
variability if you have let's say r1
capture it's easy to come up with some
bottom-up approach
but the more variability you have in
general it's harder to consider all
possible things you can of course you
some features but in general current
optical recognition systems which use
bottom-up approaches don't to solve
captures don't recognize captures and
this and the question is for humans it's
easy to understand what happens here if
he seems that there is some blue and
most probably all of you can reduce its
140 point maybe beware for freeware as
for a vehicle recognition system it's
also hard to do it and humans are able
to process almost all of this kind of
captures because we generalize your
grease which we have been captured and
does this make sense what I just
described and here we developed a
framework of general Vista graphics
programming so you do it top bottom you
have just a high success in generator
very you sample your sin object for
example you sample object 1 x coordinate
from some distribution and you also
simple control variables then you have
approximate render which you feed your
information about your sins and
parameters and control variables so you
generate approp you the high stickler
render something your image and then you
have the highest a comparison and you
have data from the real gold you have
your generated image from your
approximate render and then you
interested to consider this term in
order to find this distribution what are
control variables and what's in
parameters describing the best way that
this data you have from the real world
so one way to do this the hi-c
comparisons its simplest one we tried
for capture example
is for each pixel you put normal
distribution and you have are your pixel
from your generated image for example
coordinate 5x5 y7 you get red channel
you'll get noise and then you observe it
to the pixel you have from the real
world image this is the simplest way you
can do your statistic comparator and
combatants of january's of privacy
graphics program in our main a for its
probability programming itself it's
estimated conference which previously
programming framework provide you this
is a pageant after approximate version
computation and computer graphics
approaches like we don't propose because
we used renders we can get benefits of
bottom-up techniques as we did in our
second example I will show you so when
you generate your image at the end you
can use so different things for example
reduced number of colors you have in
your image and first examples capture
and this is a generative model for
capture and you can see that here we
define number of glyphs you have in your
cup letters you have in your capture for
each letter we define if its present or
not with the new distribution Oh point 5
we define for each letter position
exposition why it's sizes if it's rotate
it or not and how its rotated from
uniform continuous distribution what
later I did will be English alphabet +
10 numbers we define blue we define
noise then we do some software
engineering sings to initialize the
renderer then we render approximately
our letters we have from the model and
then will you do the stochastic and
person and we use ninety percent of our
inference time Metapod fast and other
ISM and ten times use
Gibbs inference for this our model let
us make sense and here you can see uh
yeah he this is your real girl data your
input capture and here you can see how
it's converged from prior to posterior
and this is speed up complex rendering
model how would you do an executive step
that seems to me if you have something
like a blur it affects many variables at
once yeah I have to discretize all
somehow approximately good stead and the
question is great they give step in this
case you are applied only to discrete
variables and it in this case is this
model it was applied to our the glyphs
what letters IDR you're right there is
ways to make for example discretization
but we just make gifts for because I
mean gives for exactly necessary for
glyphs and it's off our dependent
currently owns the it's about 30 minutes
30 minutes changement so this is speed
up March and though puristic programming
is convenient in the way to represent
your models now it's tremendously slow
for example there is a 10 digit
allocation model which right in church
like language with the new v's multiples
license and it's asymptotics is the same
if you write your hydrogen stepper but
because so you interpret your model etc
you had compiled your handwritten
sampler it's now about 1000 slower and
there is much you have to do for this
kind of the kneeling or parallel
tampering or some other kind of things
to make it mix no I though on what but
but I see your question and ok I will go
to the slide and have the slide special
for this so this a Baptist be soft and
you can look more careful to the paper
and this again inference so answering
your question we don't have any lien as
it is but what we have we have Biogen
relaxation so we have a few control
variables for our are in our model we
have blue blue on each letter we have
global blue and we have data blur on our
data from the real world as we will have
epsilon parameter our noise which comes
from also some stochastic distribution
it's not fixed and you can see that if
we have this it converts and if we don't
have it if you don't have this stock but
if you don't have distributions or our
control variables and if you don't make
inference over them it doesn't converge
reliably and you can see what happens
here you have your blue and you have
your iterations you can see that firstly
the blue increases for your letters and
then when it found its locations it
starts to decrease so what really
happens I mean as if you're a person if
you look to the program if you look from
global perspective and then when you
soft it's from global perspective you
make more precise look and solve details
and it's the same you see here is it
answer to your question
and another problem is so wrote example
where you use its standard key to data
set and the problem here to find the
road the left of roads and right off
road and lanes to locate them on the
each frame and here we use the same
framework so we have ours to high-stick
render we used a gain graphics engine is
latent variables i will show you the
model soon and the what the symbols
represent kit in the 3d environment so
we just have the highest kick
coordinates of this 2d plane in 3d box
and then we head camera and we render it
the image and very shift term the road
as 2d image and this is a model so we
have Road tweets road hide we have
position of lanes we again have epsilon
as noise so we'll hear it works it a bit
different way I will explain how we have
our software engineering sings to
connect our render then we have one
train example data from one training
example and we draw our road to the 2d
image two surfaces and then we observe
have the highest a comparison of our
drawed image drawimage fro with our
real-world image and this has samples
from geometrical prior does this make
sense so this is are you really just
prior or is there some important
sampling based on you see some
preliminary use of the data to make the
sampling from the prime
efficient oh this is just something from
geometrical prayer and if I understand
your question correctly I will i have
this slide so we have terrain image and
back from train image we locate where
our left or for Ortiz right off road
left the road itself and lanes and for
each region of four we create color
histogram of 20 colors colors we reduced
using k-means and this is sample from
the model itself based on one train
image does this make sense I'm not sure
okay so you do know this before use
before you sample all this not sure what
this oh yeah I always playing in steps
okay yes sir so we have our image first
of all very used our colors to 20 using
K means for train image we like it left
a fraud right a fraud the road itself
and three lanes so we select regions and
we receive color histograms of each of
four regions and this is in the model
this will this set us are let us make
sense yes yes geology prize is picked by
hand no I mean this a generative model
so when you sample film from your from
this model you first example your
generative are a geometrical prior and
you can see how generative model is
described first example your junior a
geometrical prior and then you that part
right you haven't read the part that
generated the the geometry the same I
mean in terms of image all happens here
it
did you write the first six lights of
the program manually or the queue some
outfit them from daga oh my no man
you'll hear sorry mother and first if
example first parts of the program
samples from geometrical prayer and then
it's samples from color histogram and
your shift image like this and we do the
same thing for our test images we
reduced number of colors and then we
compare our histogram series have for eg
from our train image and we from our
test image and we try to influence
process tries to match color histograms
of test image to be as much as possible
closer to color histogram soft terrain
image and by this it finds the location
of left off road right of roads and
other agent does this make sense so you
have important sampling there's no
important something of the geometry you
just so in other words you have to
generate a lot of samples of roads that
don't even merely fit the data before
happily you generate some samples which
are close reasonably close to the data
rather speaking yes where you are using
rejection something right
yes so I mean we use a b c and e any
sample from prior is appropriate so I
mean it's by this I mean it will not be
rejected you just will have some log
likelihood and the higher log likelihood
the more probability the next tale field
we accept it and you do this random walk
and I agree that there are other
different techniques which as well could
be implemented in general purpose
published a program in frame York people
ah this is the
it should relate to carry on okay and
these are results and you can see that
we can pair this with one of bottom-up
approach and at all 2008 and it requires
many parameters to be said before about
90 parameters and it's big amount of
code about 10,000 lines of code while
our approach is a few lines of publicity
programming code and some render sinks I
can be an aneurysm and other sinks which
at most one thousand lines of code and
you can see that you with one train in
which we receive accuracy close to this
approach and with 15 terrain images and
using maximum log likelihood approach we
receive better accuracy grams as part of
the policy program and just learn them
as well and what I'm sorry I don't sent
a question well you seem to be fitting
the color Instagram sort of as a
separate process not within the problem
sick program right yessir doing this
Maxim like that it separately yes I via
after we yes the reason why you couldn't
do it as part of the prophecy program I
think there is no reason and it's
possible to do I cannot come up very
quickly but I don't see any reason why I
can't hear we maybe we just try to make
the model as simply as possible to show
that you can write it is very simple way
but yeah it's possible I sent to make it
the ability to curve the road ah sorry
could you repeat a pattern so right on
your fitting a straight road all the
time it but a lot of the French actually
ever wrote that that curves to one side
would that be infeasible to have in your
would it be too many too many geometric
it's possible I mean maybe it will be
two times or one-half times learner but
it's of course possible so the main
advantage of this paper is I'm not first
author of it but i believe that you we
showed that you can write simple
publicity programs and it works not
yours and for simple problems then hard
long bottom-up approaches and their
implementations and you even with simple
models you have the same performance as
bottom-up approaches and the more
complex to make the model hopefully you
will receive better results and there is
much to explore does this make sense
because I cannot see you have a like you
how would you mean ABC computation so I
really happens in this log likelihood so
each time we have our image our new
sample we have where we make some small
perturbation up on our local variables
you either accept or reject your
proposal and happens analogously to ABC
to MC MC ABC statistics truck from
Istanbul yes but you in ABC the problem
is that you cannot integrate over all
possible our explanations of latent
variables of your provided real image
and the same thing we can do here yeah I
mean yeah of course the Mason that you
Peter's okay so there is think uh which
I also are interested in and I my
contribution to last year for church
Lakeland which is the problem was that
in church and mi mi t church and birds
the asymptotics of inference using
multiple sites and algorithm was not the
same as ah for hydrogen sampler so Ralph
speaking sorry when you increased your
data linearly the inference which was
necessary to converge to posterior our
number of inference steps grows
quadratically because if you remember
the first think I showed you about
publicity programming each time you make
small perturbation you really the whole
program and you rerun the whole program
you're using all random choices whenever
possible but actually if you have
analogous notion of markup blanket you
just should recalculate log likelihood
of direct children this know depends
this node provides its values for and
you can accept reject because all other
fellows and they look like it will not
change and this is what ventures and its
data structures follows and we have this
no analogous notion of markup blanket
and hopefully paper in nature will be
published soon so you will just keep
track of dependencies and just make the
small perturbation and not reveal the
whole program and this allowed us to
have the same asymptotics which you have
for
some real-world problems for example i
zink hmm or LG to have the same
asymptotics for generations in person
does this make sense this part though I
mean coefficients of constants I still
very slow and there is much work to do
for example it's 100 1000 times slower
for 900 and simpler and finally I am
going to describe the preliminary work I
have if you don't have any questions on
previous parts and finding the mouth of
banking and arrival in arbitrary program
seems like quite a difficult task please
say something about I agree because the
sink or you don't have fixed structure
and you should make VOC to find your you
should do it term online based on your
current structure you should find all
nodes which will which are structure
influencing notes for example predicates
until you can absorb your propagation
let's see if you have this nodes which
is you make local perturbation on and if
it's predicate node you can't stop here
you should for example delete SSH l and
create another node if you go from
consequent to alternate if you're
breaking a change its whether from true
to false and we developed data
structures which will be described in
this paper and out salaries yeah so I
mean it's the paper current is there is
a draft about 40 pages and I mean if
after the talk I am very glad to discuss
it in person but
means the intuition is to the right it
it's not Marco blanket as it is because
we have flexible data structure and we
just should find all nodes which
influence the structure up to notes you
can absorb it and then have this part as
we make local perturbation another
limitation the note that you stop
sampling up you have to build compute
the likelihood at that point yes so if
you have some black box like a renderer
you will have to sample through that
even in this scheme right I agree so
okay blanket I agree you're right i mean
if for example as this is not the high
step choice but deterministic for
example this is sinus function you
should propagate up to the node which
can absorb all for example this is H
case and it's log likelihood will be
zero if you feed a new boiler which has
been produced by local kernel on your
principal not use well shoot either do
rejection which will be very stupid
rejection because you will each time I
checked if it's an edge case or you can
propagate further and all this will be
described here and I'm glad to discuss
it after the top and by this i want to
show you that previously previous
implementations of church language your
head not the base station projects you
can have but now it's better can I move
for next section so I DS to use church
to have some method / ballistic program
which will pride use publicity programs
generative models so you will make
inference over probability program space
and because during a church student
completes language you can use eval too
evaluate your programs are generated so
rather speaking if you have your inputs
and outputs pairs this is one of
educational formulation of a problem you
want to come up with some programs the
resistance error this should be it
should be n which from this input
produces output and they are connected
by some state so in the simplest case
it's just the one program it's more
complex our situation its programs which
share the same parts and so let's make
sense and you can do it also in
unsupervised way where you have just
output and you need to find programs we
generate this output and this is one
very stupid way but just to see how it
can be also you just have some
productions rules here you define
somehow how you generate your program so
you have then you wrap it using compound
procedure so you receive your sample and
you evaluate this expression so you have
received your self loop and sample from
and then imagine that you sample from
this sampler 100 times and you provide
two arguments in the center 5.7 and 3.5
and imagine that mean of this 100 sample
65.7 and since the dark deviation is 3.5
and kurtosis is 0 point 0 and skewness
is 0 point 0 as any reason predict your
expression so you want to understand
what program it was we want to predict
to 100 samples from this sampler by
choosing other officials and the ID is
you sample from a degenerative program
model then you apply several generative
models to Train input-output pairs and
you make inference here using some
inference algorithm and then you
hopefully converge the reason of
generative model
which describes you into the output
pairs and the idea is to use currently
p.m. CMC and image to automatically
produced classical distribution samplers
like mrsa glare or book similar
algorithm because there is a much work
on generation domestic programs there is
field of functional adaptive programming
of inductive logic logic programming of
programming synthesis but probabilistic
programs are more general in the way
that they have started cct generally and
it's interesting to see how in this
framework you can generate and
synthesize probability programs which
describes distributions and not some
fixed algorithms and for example this is
productions rules written an anglican
adventure so for each combination type
of sample from some process and for
example for real combinations if your
combination should return real value you
sever from some categorical have some r
plus minus multiply your mouth if you
have booze and you have boolean
appearance this is one way to describe
this generation create against in some
way yes but he might you mean that we
should just have reals right just cool
okay but imagine if we have pride use a
model where we have if Samsung some
predicate which returns boolean and
consequent is some normal and alternate
is some other real so rather speaking
your combination first one which you
want to have output value will be real
but this real could be if construction
condition control
with predicate which would be boolean
and consequent alternative should be by
recursion reals in this case you need
boolean for example because IDs to have
in prior all possible are generative
models does this make sense do you see
okay oh and this is smart ugly written
in other kind of ensure and these are
samples of generative models so this our
programs which has been produced by
metaji initiative by metal stick program
and this I distributions so these are
samples from sample generative models
and I sorry these four pictures are they
supposed to look like each other oh no I
mean so rough it they should be in
generals it should be different because
in general you want your program you
generate from meta produced a program be
to define any distribution you find
normal distribution gamma distribution
poisson distribution some processes so
yes samples from prior of your priority
programs so firstly we sample programs
and then we produce samples from
programs to samples from metal program
and the idea is to combine functional
probabilistic programming with
exchangeable random procedures and with
different inference algorithms you get
benefits of using noise because it
should allow to
make smooth perturbations when you
propose different programs or change
parts of it of them paralyzation and
distributed inference techniques and
bison nonparametric and three models to
generate grammars how to have smart
priors and your programs ok so I
described you very briefly bistec
programming is as i see it i described
two examples i described one sink you
can advance and asymptotics in church
latin witches now is in correspondence
for many models to be handwritten
samplers and I also showed you ideas an
automatic programming we for this mystic
programming how would you know if your
prior over programs is a good prior or
what you even evaluate them mm-hmm look
I don't know yet I don't know i mean the
problem of prior is all the support
their question i don't know yet also one
thing that seems weird with the way you
defined it is that if i make a tiny
change to a program it could have a
gigantic effect on the distribution
produced by that program right clip it's
your passion place now make a very small
change the syntax of a program it can
have a very large impact on the
distribution of the samples generated by
that program I green so it seems like
one of the difficulty one of the issues
you'd run into is trying to smooth out
that space that you can do a good
reasonably good search to that space of
programs right because if it's if it's
extremely roughly and be very difficult
to find a good program to that space
right I agree and yeah I mean so what I
believe is you think is there a way you
can set up the problem so that so that
said when you change the program you
know that it will have a small effect on
me on the option program so you're not
also you're actually making arbitrarily
syntactic changes to the program as it
sounds like you're doing but some sort
of more control I mean the way how for
example if we follow the church our
inference algorithm the what will happen
to you have some operator and if you
have conditions so you should have if
sleep one part of a program and another
part of a program if you make some
perturbation on the construction of your
program in the consequent branch you
will just touch this part and let's see
if the predicate simply to be true 50
times you just affect half of your
sample services think so this is one man
agrees is more to explore and to try
ideas from problem synthesis and program
transformations when exactly what they
doing they're trying to transform
programs with small are changes to them
program is is more feasible program
transformation you start with a very
simple program then apply a sequence of
transformations remove my grandma
similar to the work of motivation and
also some we're going on here in
Cambridge and then in some sense perhaps
the the steps that you can take a more
carefully control I agree with you for
example this paper by prefer no good man
and rest
umer answered offer was published a
since technical report one year ago
where is the exactly trying to first
disease X Z have observations let's say
100 observations then they create
exactly the program which creates this
100 observations and the so just using
if constructions profit speaking and
then they make transformations on this
program as you describe truffle speaking
so and is a made it for one specific
class of problems and this automatic
programming it's just a proposal I'm
trying to explore just now all this work
is a paper this judge description of
acoustic programming this is our venture
just to paper in preparation and this is
just a proposal and I hope it was
helpful for you I mean this is I mean I
hope it was helpful for you so we learn
how much and how much the program
transformation would change the
particular outputs so because if you met
like you got a minus sign at the top
it's really going to do a lot but if you
put it somewhere down very deep into
yesterday that's not so there's a way
you could get some kind of feature but
would give you a way of learning bucks
to help you then smooth up your kind of
search space I agree I agree this kind
of you agree with you
so you describe the church venture
metropolis Hastings algorithm one
question I had was if each variable in
the program is a big data structure as
in your automatic programming thing how
does it know what to propose for that
variable like how does it propose
changes to that variable it means how in
das local perturbation currently samples
again from prior yeah so yeah so I mean
you can't solutely so sorry so it's not
a local inspiration I mean it's local
perturbation but it's sample from weeks
prior so rather speaking i will show you
imagine that you have this uniform
discreet and size x so and imagine that
this for size x for later number seven
it's size x was 17 you make small local
perturbation in the weight set for size
x you will just simply gained from
uniform discreet and for example you
produce 55 and you will propagate tip it
up to this render surface procedure and
then to Sahaj the comparison so in this
way its local perturbation and you have
some local kernel here which in this
case just sampling from its prayer so I
mean it could be another kernel for
example you can sample based on the
previous value you can have another
Colonel not specifying others ah it's
not to the best of our knowledge it's
not yet described so current venture
samples from prior and there is just to
yoke reaches in the process where you
will be able to specify but the own
venture you can download now it's not
possible to specify another a proposal
rather than just simple very simple from
the sprayer so um if no one else is the
question I want answered
Lucy could send a boolean sorry I
suppose you could send the boolean and
use it as a choice between the Colonel's
right yes I mean me neither of the prior
take it that way yeah yeah that was that
was actually my name's always good so
because the infant saga more in this
very particular way based on the
structure of the program it sounds like
by rewriting the program into an
equivalent one you can have a
significant effect on how efficiently
emphasis okay with examples of that or
other like student programming practices
that you should follow to make the
interest more accurate oh yes are there
is no described as some tutorial but now
anybody pro mystic programmer should is
it there's no tutorial but your
intuition is right currently when you
write any publicity program you
depending on what our inference
algorithm you lose you should sync how
infants will happen because I for
example I can provide you an example
imagine that you have this node if you
put more attention to this it's written
in the way that it will jeopardize
inference in church like language
because here we have categorical but
this categorical is not hidden if you
change your predicate you Jasper sample
another categorical and if you write
your whole model in the way like this
even you do your small local
perturbation if you even if trying to
keep dependences making perturbation of
this node will yield in repose in all
other nodes because your predicates will
change you will go from consequent to
alternate and you will go up to bottom
notes if you want to make it better you
should write categorical and for first
argument if echo mvs to 0 0 point to as
our ISO point 3
and so suck second argument of
categorical FM vesicle 20 should specify
0 point eight other why so point 7 in
this case it will absorb so I'm in there
eight other tricks for example if you
have if in your interpreter in your
engine you can specify ok if I am
switching from category two categorical
i still can try to absorb I mean there
is a tree craft speaking but I agree
with your intuition that when you write
a prolific program you should understand
what happens especially at the current
stage where it's rough what about this
suggestion that if I have a prior which
is very broad I write it as a sequence
of steps that add noise that had a
little bit of noise at each step does
that does that help the efficiency of
the of the interest for some also Julius
I mean the problem with publicity
programming now that there is just few
algorithm really implemented matter
post-high silk and smc and some other
algorithm for another permissive program
language she said not very familiar
voice for example the third would net
and when you write your program I should
really just have in mind how this
algorithm yolks and there is no tricks
for example like this in current eh oh
well let's thank you're a mover sign
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>