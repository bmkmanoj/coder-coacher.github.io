<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From Data Science to Data Intelligence | Coder Coacher - Coaching Coders</title><meta content="From Data Science to Data Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From Data Science to Data Intelligence</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0WHxRf569HM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged public showing internet or
public display is forbidden and may
violate copyright law
good afternoon everybody good afternoon
to the last session the afternoon
session of the faculty summit so this
session I'm going to be very brief to
leave plenty of time to our speakers but
this is a session around data and the
data centric fields like machine
learning we have three talks by three
different speakers who are all going to
provide some different aspects on the on
data right so the first one with Roger
Roger introduce him in in a minute so he
will be giving us like the business side
of how to unleash the power of data he
is at Microsoft on the second talk will
look more at the research aspect of what
doesn't me know when you do machine
learning and now you're faced with large
scale we're talking really really large
scale data how does it change the way
you need machine learning and last but
not least most of you here in academia
we will have a talk from somebody from
academia from Stanford Brazilian who
will be talking to us about an approach
about how to best collaborate and what
and what's the best way to do that and
he'll we talk about some of his opinions
and the platform which is done in
collaboration actually were with
Microsoft all right so now we're going
to go back to research aspects and
looking actually at what it means from
the machine learning point of view when
we start to look in at really really
large scale data set and for that we are
very happy to have Leon butu with us
Leon has extensive experience in the
topic actually and in actually industry
research lab he worked in 80 80 labs and
the subsequent labs John Microsoft about
three years ago
and the first induction in one of our
business groups also experienced
actually in working with large-scale
data like in our being organization and
with that okay is this one that's the
one yep so I would like to thank you
very much for giving me a chance to talk
here and I would like to say have too
many slides to let me rush into it so
okay so I'm going to start by describing
three inferential IDs in machine
learning and I say and free unsure
because I don't want to say defining but
they're probably the three ideas that
define machine learning in the last 20
years the first one is ID and it's not
ID as a concept is ID as a pragmatic
think at the end of the paper there is
always a situation where you have
training that are testing that are and
you train on the training data you test
on the testing data may be using
cross-validation that you always assume
they're follow the same distribution and
this has driven machine learning
progress for at least 20 years and
probably one of the main factors in
progress the second ide is model
selection trade-offs that when you have
an excess arrow you can split it as
approximation arrow which is how much we
lose because we're looking for a
function in the limited familiar
function and estimation error which is
how much we lose because we're using a
fight training set and when you increase
the size of the family of function
typically the approximation error
decreases the estimation will increase
in there is some optimum and you can
model selection that way and the idea
here is how complex the model can you
afford with your data and the last one
which is related is what I call the vet
Nick's result when solving your learning
problem of interests do not solve a more
complex problem as an intermediate step
and again the idea is our complex model
can you afford with your data again so
for instance if you want to learn a
classifier you use a model that outputs
the class and nothing else if you want
to do something more complex you have to
carefully define the complex and solve
the problem and nothing else because if
you solve something else you're going to
use data for that and you'd be less
effective in yet and these three ideas
you find them in the values
currents in machine learning you find
them two values degrees and dal couple
knows in this graph but what you have to
see that the Noah's a little footnote
that says that maybe it's no but maybe
it's yes to a little bit so my goal now
is to show that these three IDs for
completely flat when you have large
scale data and the first part is the one
that's probably the the best understood
is the large-scale learning trade-off
and I'm going to go fast that one to
spend more times than the other ones so
okay I guess you can read the slide so
so the idea is that a couple years ago
people were considering that the last
cal problem was mostly an engineering
problem you just have to use a bigger
machine better optimization algorithm
and this is were not connected use the
best statistics the best optimization
and that didn't work that way but in
fact it's going to be a bit different so
suppose you have a large problem
optimizing on the training set is costly
and optimizing on the training set is an
approximation of finding for finding the
best function in the familiar function
which is this a line approximation for
finding the best function so it's
already a lot of approximation so there
is no need to optimize that exactly so
suppose you optimize to a level where to
a certain accuracy row on the training
set so when you do this you add a third
term in the decomposition of the arrow
in addition to emerge you lose because
you use a financial a smaller fun in
your function and what you lose because
user fire training set how much you lose
because you don't optimize perfectly and
the problem is to choose the familiar
function number of example and accuracy
to make this Sun as small as possible
and you have constraints of course
constraints is the total number of
examples you can have and the computing
time so when the constraint that active
is the number of example you in the
small-scale setup so when you in the set
in this situation because you have ample
time to reduce the estimation arrow you
take n as large as possible asked me to
take all your examples to reduce the
optimization arrow you optimize
completely and you are left with
adjusting the size of the family
function which is the model selection
trade-off that I discuss at the
beginning but in the last scale case
this is more complicated because this
time the constraint is the computing
time you have so many examples that what
really miss you is a computing time and
the trade-offs are more complex because
they depend on everything they depend on
the algorithm after twenty cents if you
choose row smaller meaning that you
optimize more well you decrease the
optimization arrow but in order to fit
in a time budget you either have to
consider a family of function that's
simpler or less examples which has a
negative impact on estimation and
approximation error so the trade-offs
now depends on the optimization are
going which means that we can compare
optimization algorithms using a
different criterion that optimization
quality so computing time testing a row
and the base limit for your problem if
you change number of example typically
you get things like this so basically
less example a market you get the ID if
you change the model or you change the
optimizers you get a collection of
curves like this and of course if you
look at the bottom of the envelope
depending on how much time you have
you're going to choose a different
number of examples a different model a
different optimizer so if you analyze
this in a relatively simple gaze fixed
familiar function and you compare simple
algorithms like ridin descent second
order graydon instance which means baby
newton stochastic gradient descent and
the combination of the two and you run
the analysis you find out that the batch
algorithm that look at all the data have
a time per iteration that grows with the
number of examples why the stochastic
algorithm do not have that the batch
I'll go it will converge much faster to
a certain accuracy row and the newton is
much faster than a simple wedding decent
and the stochastic algorithm completely
hopeless but if you that the new line at
the bottom and this new line represents
how if i fix this quantity here how much
time do I need what you see is that now
the stochastic algorithm looked good and
if you run this in practice where it's
been verified many times once some
prefer text categorization you get
things like this where you go from
six hours for plane is vm to 1.5 seconds
and get the same kind of results and the
figure that summarized this is that here
you have the optimization accuracy from
not very accurate to very accurate here
the training time if you take the
sophisticated optimization algorithm is
going to give you very high accuracy in
very short time if you get the
stochastic algorithm but if you want
high accuracy it's hopeless but by the
time you cross and you look at the
testing costs the test across saturates
long before this to a little cross so
running the stochastic algorithm is the
better option and this has been verified
many times and the nowadays there are a
lot of more algorithm not better I'll go
in the stochastic gradient the plane
stochastic gradient but they all have
this component that you're not going to
look at all examples to do a step so in
conclusion from the point of view of
model selection there is a large
difference between small scale in our
scale and in particular you should look
at optimization in a rather different
way and you have to look at it in a
holistic way so that was the easy part
the second point I would like to make is
a comment about breath versus accuracy I
go back to this curve we have the
optimal by 04 problem regardless of the
feature gallus of anything and the
testing arrow and if I use 1 million
example i get eight point one percent
arrow good and i use tell me on example
i get a pond 0 1 100 million example 8
400 1 well at some point you know we
should choose another problem because
the accuracy improvement cannot justify
the computational cost forever so why
what do you use it very large training
set for this kind of things should we
use large training set for learning is
it really the problem so let's look at
it differently let's consider zip
distributed data we know that roughly
half the search query have this thing
sorry we are unique regardless of how
long the period you look at you look for
one month half of them a unique you look
for one week half of them a unique and
if you caught your queries in frequency
order so you have the very
click on one and not very frequent one
and you do something pair query there is
a point where you don't have enough data
to learn so in the green part you can
run correctly in the red part it's
pretty much random now suppose you
double the size of your set so when you
double the size of your set you double
what you see in the head pretty much so
you see a lot more of the common ones
and you see a lot more of the tail so
now because you have a lot more example
you'll be able to learn up to that point
somehow the change in average accuracy
is ridiculous it's very small is the
8.12 8401 but the change in number of
situation number of queries for which
you give the good answer is large so
what you see that if you look at the
average arrow you have diminishing
return seem so bless if you look at the
number of queries for which you can
learn the correct answer doubling the
data set give a very large increase so
if you want to do rascal you should of
course look at the scenario in which the
returns are not diminishing the more
data you have the more you can and then
we have a problem so accuracy
improvements are subject to diminish
returns I average accuracy breath
improvements are not subject to diminish
returns and it's not how accurately do
we recognize an object categories how
many categories do we recognize well
enough and then you have two different
point of view you have the scientist
says should we optimize the different
criterion seems very attractive to
optimize something that actually can
benefit from data and if the business
guys how does this help if average
accuracy is what we care about the
dollars did average no they're not the
breath while the business guy drama is
wrong first question is our example
identically distributed really in
traditional machine learning for real
problem if you explains that you know
that collecting the data set that's
representative is the hardest part it's
a lot of work to get a nice data set for
your problem the ID assumption is not
automatic it's hard work now if you're
in a big data set up
big data exists because the data
collection is automated there is no
manual curation you shouldn't expect ID
in fact is much worse in general you're
going to use the data to make decision
and your decision is going to shift the
data submission anyway so you can count
on I ID so look at the distribution X is
the input wider output let's suppose I
see classification why the classes X the
patterns you write it as P of Y given X
times P of X P of Y given X you hope is
relatively constant because this is what
you want to model if you want to model
something that's changing all the time
you're in trouble so P of X is going to
change meaning that if you look in terms
of queries the queries from last month
are not the same of the queries from
next month so if you minimize the
training set error well the
approximation errors they push towards
the patterns X with low probability in
the training set and what if this
pattern so come more frequently a
testing time because we are shift so
robust approach to this problem to try
to robust this kind of shift is to not
optimize the average arrow but try to
maximize the diversity of the patterns
that recognized well enough and our
values ways to see this problem and the
one of the interesting side effect of
this stuff is that if you decide that
you don't want to optimize for average
accuracy you want to optimize the domain
in which the size of the domain in which
you working well enough well look at
your data in the green you have enough
data to trend in the red you cannot
anyway but on the head all these data
that's most of the data you don't really
need it if you want to optimize average
accuracy we're going to work very hard
to model the head very well but the
basic of basis of this idea is to say as
soon as my system works well enough I
don't need more data so most of the data
is something you can discard so at any
time what you want to do is filter the
data to work to keep inner data for what
you already know imagine what you
already know but also
use data that's just at the border of
what you know in order to to increase
the size of the domain you are you want
to know you know you know sufficiently
well and this is very interesting
because it connects to the idea of
curriculum learning which proved very
effective in the number of our shared
problems it's also very interesting
because this offer scalability gains
across the board no filtering the data
that's easy you can do in parallel you
don't need complicated loops you noted
very high performance clusters so it's
not clear that if you want to train
something for that purpose you need huge
computing power in a very dense cluster
maybe you can do it in a way that's much
more smart now my last point is about
trying to undo the VAT Nick result about
deep learning and transfer learning and
I'm just going to go back in history
because deep learning is an old lady so
in the mid-90s we are trying to engineer
machine learning system to do for
instance check amount reading and there
was with younger guy you're sure i was
here and a couple other people the input
axis can check image and the computer
was small so that was largely data for
the time the output is a positive number
with two decimals just the amount now if
you just use pairs of XY you're in
trouble because maybe it's possible to
train a full system with just check
images and amount but at that time was
certainly not possible given our
computing budget so what we do is that
we split the problem we identify
subproblems like look at the amount
field on you check segment characters in
an on-field recognized isolated
characters translate character strings
into an amount which is not a bit
obvious because the we have crazy
amounts with stars and stuff like that
then you define a submodel fault problem
and the fairly complicated recognition
models they were commercial network
security highly engineered location and
segmentation models with only a few
adjustable thresholds you collect and
level data for each subproblem this lot
of manual work that you know manual work
is not that expensive
even at a time and then you have values
training strategies you can do
independent training you train each sub
model separately you put them together
it sort of works you can do sequential
training you labelled output of a sub
model and you train the next one which
somehow you learn how to correct for the
previous one you can do global training
where you pre trained in one of these
produce ways and then when you have a
model that already works quite well you
trained on big examples of check and
labelled with very weak labelling now
the other number of issues this is
complicated it's actually worked and it
is what well enough to process I think
about fifteen percent of the checks in
the US for on these 10 or 15 years so
this kind of things work now in the
recent years we had some ideas on deep
learning and at least the one excellent
idea is to preach when with sequential
and supervised learning so instead of
engineering the model and defining sub
problems that makes sense to us you just
create a sec sequence a sub models and
train them unsupervised with no
particular criterion and then you tune
with global training and it works very
well and that was a surprise for me the
surprise is that well engineering
learning system is much easier than we
thought we don't need to think so hard
about the sub problems or maybe we do
but not as hard we don't need to
labelled specific data by hand simple
task like unsupervised learning go a
long way to getting a problem to be
completely solved unsupervised learning
I'm always surprised about unsupervised
learning because intuitively I don't
trust it and here is why the prototype
of unsupervised in a supervised learning
is clustering so what is a cluster where
is an assumption between up the shape of
the density reverse the underlying
categories that because you have a whole
there you go into somehow of categories
that makes sense now the problem is that
if you change into space for instance if
I squeeze the points here and iOS
Dominican be connected
by changing the input space applying
your continuous function to my input
space like for instance when you have
images we have all these nice squares
reads of pixels but you in eyes they're
not square so the actual features there
is no reason why they should be as quite
weight and with this kind of linear
intensity and RGB so when you change and
you the input space you can perfectly
well make the cluster disappear even
though the classes still remain so
clustering we visited clustering is the
expression of the prior knowledge that
we encoding the choice of I input
representation so unsupervised a
learning is pretty much the same as
saying that we have really cheap labels
x1 and x2 are closed or not closed in
the geometry we have chosen for our
patterns nope so in fact what we have is
auxiliary tasked if you look at the
tasks that are possible some tasks are
qualified as interesting and interesting
me that the label data is caused because
the label that I was abundant that
wouldn't be a problem and the touch
wouldn't be that interesting and
observation that in the vicinity of an
interesting task with expensive levels
they often less interesting task with
chip labels that we can put to good use
and unsupervised learning is just one of
them with trivial labels that arise from
my choice of representation so when you
look at it that way deep learning
semi-supervised learning transfer
learning there's three facets of the
same thing examples face recognition
it's an interesting problem you have an
image you want to see who this is is not
so easy to get a thousand labeled image
for each person but there is a related
but less interesting problem is to take
two images of faces and see whether this
is the same person or not and for this
you have lots of data if you have a
picture and two phases that different
people except for twins and Nero's but
you can quantify that and if you have
frames faces in successive frames of a
movie they're likely to be the same
person not always but at least you can
quantify it you of noisy labels but
they're so you can build a system where
the faces are transforming some
creatures and then there is a comparison
and you train this and you have plenty
of data then you have a good feature
extractor you take this feature
extractor you plug a very simple
classifier you get you answer and you
need what one two examples to trend
that's enough another example is natural
language static tagging so that's the
work of one on Colbert just on Western
and other people we're looking at
standard natural language processing
tagging tasks like a path of speech in a
negative a publisher and the related but
less interesting problem is a problem
where you try to say whether a sentence
appears in Wikipedia or not at least a
segment of a sentence and what you do is
that you take a sentence and sometimes
you change the middleworld when you
change the mirror world you want the
score to be less than when you have the
original one when you train this you can
learn a word representation that proves
to be very useful for all these tasks so
if we do go back to that nyx reso deary
ID was too short directly the probe
interest and not to solve a more
difficult problem as an intermediate
step because well I complex the model
can we afford with our data in fact is
that we have plenty of data with
different value and sometimes solving a
more complex task for which we have lot
of data and transferring features or
transferring hidden state or
transferring layers or whatever allows
us to leverage more design of a
different nature and this has a lot of
implication not only on deep learning as
it's on today but our deep learning is
going to down in the future so my
conclusion is that large-scale changes
everything and that we shouldn't be we
shouldn't feel bound by the usual dogmas
of machine learning it's going to be
difficult because the idea that the
training set on the testing set and you
can make experiments and everything is a
very powerful ID but it's not a Dogma is
not something that we should consider as
completely frozen it's not anymore so
it's a very interesting time because
we're free to think in different terms
and also very dangerous time because
people are expecting a lot from us and
the landscape are shifted in such a way
that we have to invent a lot of new
things and that's my conclusion right
thank you questions for leo hold on a
second we need the mic so that people
can hear it when they see the video
later thank you so I'm I find very
interesting your idea of the debt the
breath is something you want to optimize
rather than the average error can you
have you thought about formalizing this
a bit more I know many ways to formalize
it i'm not sure which one is best so the
issue is that here i presented in terms
of number of distinct queries now you
can understand another domain where you
have a continuous features you cannot
count them so easily so somehow or there
may be many ways in which you can
categorize your data and count different
ways yes and so somehow the way you
count them is an anticipation of the
kind of avoid ships you expecting so the
goal here is to try to develop something
that robust to provide shift and the way
to we suggest to do it is by playing a
game on the distributions but I don't
want to say much more than this at this
point I just want to say that we have to
think about this very seriously any
other question
so in the second apart you mention the
different kinds of queries and some are
very frequent and you probably don't
need need really do that make learning
some are at the tail that's where we
need help and do you think you can
leverage the structure of the data to
actually help the tail query part
because you you didn't elaborate on your
point there it seems that there's not
that much we can do to that part but
that's where it matters right so there
are connections between queries they
share same words and there may be other
structures that you can leverage that
the structure part of the data let me
let me continue what you have in mind
you're thinking maybe we smart features
i can connect the tell queries to the
head queries i'm going to learn on the
head and exigent eyes to the tail but
regardless of what you do with features
when you look at the future space the
features you have chosen you always go
into a space where you have a lot of
data and specially while you don't have
enough data to learn so you just shifted
the problem by doing this of course you
can shift it in a smart way it can be
useful but it's in there yeah so the
opportunity is still there right the
problem will stay exists but it's
possible to have the features that that
are generalizable to the to the tail
part so for example if you just learn
how to combine different ways to score
on documents may be that same way it
doesn't necessary change it from one
query to another query because they need
to emphasize different strategies for
example navigational queries tend to
maybe gain more from link and Lance's
like page rank but informational queries
may need more weight on content matching
so that's characters in the chancel
because of the features divine at high
level I understand amount so that you
can do feature engineering it's going to
help but it's not always like this form
disappear yeah so my questions can you
actually learn the features for example
deeper learning or other strategies to
to learn features so the learning part
yea the deep learning party we should
learn editor and to learn the feature we
don't have only the task at hand but we
have plenty of other tasks for which
you might have cheap levels cheaper
levels so the word translation relations
can potentially be transferred from the
head queries to the tail here is for
example cars related to vehicle and that
knowledge can be potentially acquired
from the tail hair queries regardless of
the feature engineering you do whether
they learn or not long you're still
going to have the same problem because
once you've learned the feature you find
your nice features when you look at the
space you're still going to have domains
that are where you have a lot of data
and domains where you are very little
and sometimes the domain where you have
very little are going to be those
important in the future yes oh I agree
and the programs i have a question about
whether you can leverage the strike my
to decouple the two problems in the
presentation but you solving one doesn't
solve the other one you displace it you
don't solve it okay thanks all right
Thank You Leon okay so the last talk of
this session is given by Percy lien from
Stanford University lien Percy sorry
persistent as John Stanford actually in
September before that he did his
training PhD at MIT and berkeley today
he's going to give actually a talk which
is very different from his er research
the research is doing and it's going to
focus actually on the another facet of
data centric research and how much in
learning how it plays a role there in
the context of the community and with
that personally too okay thanks a blunt
for having me I'm thanks everyone for
coming like Evelyn said this is a
different talk than i usually give so
we'll see how it goes so this talk will
have a lot of similarities
infrastructure alee to what Roger talked
about earlier but I'm going to focus
more on the research side of things and
in particular my goal is to really build
tools that enable researchers to be more
productive okay so let's start by
thinking about you know the current
research process and I'm going to
highlight some of the issues I think we
can really try to improve on so let's
start with idea so researchers you know
I in some sense should be about this
right so come up coming up with good
ideas and you know kind of even test
them out but that's kind of only one
piece of it so anyone who has some
research knows that you know step two
involves finding data cleaning a
converting between formats and then you
have to compare with other people's code
so you go find their code doesn't
compile the email the authors and then
you know yeah you give up and we
implement the whole thing you run a code
and of course it gets so different
results often worse and you have to run
all these experiments and keep track of
it so we'd like to somehow focus
people's attention on step one it makes
step 2 as easy and clean as possible
second point is that the issue of non
exhaustive comparison so you read a
paper and usually it looks like
something like this previous method gets
eighty-eight percent our method gets 92%
and on some data sets and if it's a good
paper then I'll try another data set and
show consistent results but then you you
know you wonder what about data center
three and what about all these other
datasets which the paper obviously
didn't report results on right so
another point which is related as you
read a paper and you have this result
well if you think about you know science
and you know controlled experiment you
know you ask the question what is
responsible for this difference and you
read the kind of the paper and there's
actually many things I could probably be
going on the different types of
optimization algorithms different models
completely different kind of setup and
of course and you know you have
different bugs in your joint so I mean
maybe that's what's driving at all so
you really would like to be able to
tease this part because I mean as part
of research we have to be kind of
scientific in rigorous okay so yet
another one is on just the lack of a
broad overview right so if you ask me
what kind of algorithms work well on
what types of data
sets you know I've been in this field
for some amount of time but I can't
really give you a kind of a crisp answer
I can give you a kind of general you
know this you know linear classifiers
tend to work on sparse data and you know
various internet stem to work on these
other types of kind of image and speech
data but I can't really give you
anything concrete so um I think let me
draw an analogy so the analogy is you
know what the kind of world map looked
like in circa 1500 s right so people had
a rough idea of kind of things in Europe
but you know if you look over here we
think South America I mean Brazil isn't
and I line and so on so this is
something that I think is you know the
important to fix and related to this so
now that unfortunately all these slides
are cut off but let me see if I can I'll
just put it there oh look a little
uglier but at least you can see the
whole thing um so now that machine
learning has become so pervasive I mean
I'm kind of astounded of you know you
know even at this kind of conference how
many times the water machine learning
has been mentioned and I think it's a no
responsibility to try to make machine
learning kind of research more clear in
some sense so from an outsider's
perspective you know what are the
problems in the field and also what are
the solutions we have a lot of kind of
words but we need some systematic way of
cataloging and seeing how things relate
to each other and carefully okay so so
my objective really is to build we want
to build a collaborative ecosystem for
conducting computational research mostly
I'm focused on machine learning and AI
related problems but this is kind of
more general than that and for
conducting a research in an efficient
and reproducible manner and for the rest
of the talk i'm going to talk about two
projects on one and ml comp which i
worked on in grad school and coda lab
which I'm currently working on in
collaboration with Amazon okay and these
are going to get at
friend aspects of the whole host of
problems that i mentioned earlier so NL
comp is about stands for machinima in
comparison so you can kind of guess what
it's trying to do and you know in the
world there's a two types of people
there's people with programs who come in
and say I have this brand shiny new
hammer em you know how it is my work
compared to others because then I can
say my paper my algorithm works better
and then there's people with data sets
or problems and they want to know they
just want what is the best method for my
problem and mo cambia is kind of a way
to bring these people together so that
each person gets what they want ok so
what are the just diving a little bit
deeper what are the kind of components
there's an idea of programs for example
this SVM implementation there's data
sets and then a run which is basically
taking a program on a data set and
producing some metrics ok so it's
extremely simple at this I'm hiding some
other details but conceptually is
exactly this so how does the website
work users come along they upload some
programs users arcane other users can
upload data sets and on the system runs
the programs on a data set right so
everything is kind of happening it
distributed in a synchronous manner so
conceptually what you can think about
this is building out a huge matrix so
this matrix has programs and data sets
as axes and each entry corresponds to a
result maybe accuracy or error rate
right so as and the goal is to kind of
fill in this matrix so now when I have a
new data set what I'm doing is adding a
column and I can run you know whole host
of programs on this data set and see
which one works best and correspondingly
i can if i have a new program i can run
it on all the data sets that people
actually cared about because they
uploaded it and you know get a very
comprehensive benchmark so that's kind
of the idea and if you're wondering you
know of course you can't run any program
in a data set this is one slice of a
larger tensor so this is a force a
binary classification you have something
like this for other
um problems I sequence how you would
have different I matrices okay so let me
say one word about generalization so as
in machine learning we think a lot about
generalization from the training set to
the test set but this is kind of at a
meta level which I think gets just
omitted so you have a bunch of data sets
and programs which come along and
evaluation you take a program run on a
dataset you get some accuracy and are
you could I want to stress that this is
in some sense should be only meaningful
when the data said isn't dependent on
the program because of the program was
constructed based on this data set there
might be kind of subtle overfitting
issues but of course in papers is never
true because the author took the data
set and they ran at their program in
there you know saw what it was so what
kind of interesting in ml comp it allows
you to do this kind of blind experiment
by virtue of running things on cloud so
if people upload a program and then
separately people are going to upload
new data sets after and by virtue of
being afterwards these are you know
going to be independent unless it was
the same guy who upload these data sets
I mean in general it's going to be
independent of the program and that
provides these new datasets provide a
much more objective evaluation of that
program okay um just so some of this key
design decisions so because this is an
actual website not just an idea exists
you can go to it we want to make this as
low barrier to entry as possible because
everyone has their own favorite way of
kind of writing code you know but at the
same time people have to conform to
various standards and we have a fixed
set of kind of interfaces that people
co2 and currently all runs are executed
in the cloud and importantly any users
can download programs data sets and runs
and reproduce it on their local machine
as long as it's not marked as restricted
access okay so just a little brief
detour about what else is out there
right so there's obviously code and data
posite or ease which are kind of static
repositories with only coded data
there's a machine learning as a service
is becoming a very big thing these days
but they kind of focus on providing
machinery as a service you have a fixed
set of programs which are believed to be
really good and then people submit their
data and you know they get some
predictions and they walk away right so
this doesn't exactly help the algorithm
designers get better algorithms because
you know that everything's kind of
siloed off and on the other hand you
have a competitions like a goal which in
some sense is the reverse they provide a
set of data sets people come with
different programs and upload their
predictions of those programs but
generally you get these solutions which
are targeted towards data set and you
like you lose the generalization so why
I think really focusing on both of the
data set and code axes and getting that
matrix you really kind of force people
to think more generally okay so what's
the status so this is I guess now kind
of a order project started with some our
colleagues at Berkeley today there's
some number of two thousand users some
hundreds of data sets and programs the
website is up everything is open source
so you can go check it out okay so am i
doing on time feel like I'm going fast
but so next I'm going to talk about a
coder lab which is this new effort and
to address some of the things that ml
comp was deficient then and the idea is
no mo comp was kind of very simple you
take a programming take a dataset you
run it and you get some number right but
we all know that no things aren't as
simple as that so often in learning you
want to even at this basic level you
want to fuller analysis you want to dive
in and say how sensitive is programmed
to a bunch of the parameters like the
learning rate the the rigorous a shin
term the choice of you know Colonel or
whatever and health sense of visit ood
you want learning curves you want kind
of not just a single number but you want
kind of the whole curve of you know
confidences confusion matrices do you
want to look at the individual
predictions and furthermore you want to
visualize all of this in some nice way
right so ml com currently doesn't really
support this it gives you some set of
metrics and importantly this is not
something we can do code this all up and
put in Malcolm but it's the idea is that
these are things that are kind of more
open-ended and can't be just captured in
just a set of five things and the second
kind of Reese motivation for one is
something else other than I'm a copy is
you know I work in natural language
processing and which is I think of as a
quote AI problem and same way the kind
of vision and some other problem is that
these require you know complex these
private problems could require complex
workflows it's not just taking one data
set and just doing some prediction and
then we're done right you know give you
a kind of example of this so in text
understand you want to take you know
long passages and you know try to
understand the content and be able to
answer questions right so this is an
example from taken from a medical exam
of course who wouldn't I don't think any
system can do this right now but it's
you know it's a stretch goal so if you
wanted to do even something remotely
close to this you need to build some
sort of you know there are a lot of
components right so even starting with
basic issues with breaking out the
sentence into words so in for example in
Chinese you don't know what the word
boundaries are you need get to tag the
identify nouns verbs you know the who's
what are the named entities you know you
know find structures figure out where if
pronouns are referring to what other
entities in the sentence or maybe more
globally and figure out how everything
is kind of related which are the
subjects and objects and how everything
is related right and so this is just the
beginning to get to a basic level and
furthermore each of these components you
know involve
other source of information so part of
speech Hagen usually comes by you know
taking some raw text and clustering and
there's kind of a lot of different
sources that come in and of course
there's more that fits on the slides but
the point is that there you have this
graph okay so so how do we deal with
this typical use case and kind of new
system well there's kind of three
principles I want to stress the first is
modularity right so you know it's clear
that these kind of AI problems are going
to require you know the global efforts
of communion it's not going to be
someone in their basement just like
writing you know million lines of code
and it will be done so and I think what
happens we'll you know people will
specialize and contribute to this kind
of endeavor in a decentralized way so
kind of pictorially you can see a bunch
of these modules are people kind of
contributing different modules and you
know every things kind of happening in a
decentralized way so this is kind of a
cartoon of how we might imagine the
development process to go okay and
what's kind of interesting about this
nail this graph this graph is going to
kind of come up several times in the
remaining slides is that you know if I'm
working on this part of speech tagger
right there right so how do I you know
evaluate well i canna value on part of
speech accuracy but you know that's so
who cares about I mean why do I care
about part of speech accuracy so I can
also try to you know arguing a paper why
part speech tagging is important and so
on but we really want something a little
bit more systematic with what we like to
do I mean is to swap in this power
speech tiger with my new fancy thing and
see all the ramifications downstream
right so this is something that you know
would enable people with kind of very
little infrastructure so I'm it doesn't
take much work to work on like part
speech tagging to evaluate how their
system would impact for example with
giant machine translation systems or
speech recognition systems you might
take another module and try to see what
its downstream
actors so this provides a way of kind of
entering evaluation in a automatic way
without kind of the overhead of each
individual research group setting up
their own system so I me imagine you
have this kind of collective ecosystem
where these modules kind of interoperate
you know analogy is kind of like the
internet and on so for the the benefits
are kind of about two levels one is the
inn at the individual level you avoid
duplicate works right if you're just
plugging in here you don't need to
reproduce everything that someone else
has done and furthermore that gives you
reproducibility and furthermore on in
this kind of ecosystem you produce a new
result if it's doing well then it will
be easy for people to see aha this is a
great tool why do I use it and you'll
continue and it will give more kind of
attention to the better tools and at a
community level I think this actually
increases the pace of innovation because
you can either see really combine their
modules as I've talked about before or
as we've kind of learned throughout the
last few years is that the really good
way to get accurate predictions is to
take ensembles of in parallel of
different kind of modules and put them
together and this is all kind of
possible in this ecosystem right so the
principle of number two is kind of
immutability and this is kind of in
draws inspiration from version control
systems the idea is that well if
everyone is kind of collaborating and
adding to this graph I mean isn't going
to get completely out of control and the
idea is that well every program data set
and run is a new node so it's kind of
right once and this allows us to you
know not step on each other's toes but
alleys and but at the same time
contribute and furthermore it also
captures the research process in the
kind of reproducible way so we can see
that evolved evolution of kind of the
research community as it kind of ink
improves the tools over time and the
third point I want to make is on
literacy so before I talked about kind
of ml comp and even so far we've been
thinking about what is true you know
this program gets this accuracy on this
data set we have a bunch of different we
have a whole host of numbers right now
we're going to have this huge graph but
the question is you know what what does
it all mean right so and this is about
interpretation and so here I'm drawing
some inspiration from mathematical
annoyed python and idea is that we want
to be able to interleave this kind of
more formal specification with texas
descriptions right so and this is we
know we do in papers where we describe
what and motivate our findings so
imagine something like this where we say
now we're going to train a classifier
you you know put this module there and
then you write some more stuff and this
kind of continues so it's sign of a self
documenting paradigm and this could be
used for a number of different things
one other aspect i want to mention is
the opportunity to do medical research
right so now imagine we have this work
complicated work flow graph which is
constructed now kind of some way
automatically so now you can actually
take this graph and you know do further
analysis on it right so there's big
questions about you know what kind of
methods work for what time of data sets
which I think can only be answered at
looking at across a wide range of things
that currently it doesn't exist in one
place so there's a bunch of related
projects I think in the interest of time
I'll just you know skip over that so let
me talk about some of the two challenges
one is that you know is people there's a
lot of inertia so everyone has their
favorite way of setting up their
environment and using it so what's
important for I think these projects is
to make it as easy to contribute imagine
just dragging and dropping into a
program
whatever format and have be able to use
it for execution right and I think the
benefits of online sharing as we've seen
in kind of some other domains with like
photo sharing or Dropbox is I think
going to hopefully I'll wait the the
kind of initial barrier of entry and you
can think about this is like really drop
box plus execution and once all this
data kind of comes into one place we
need to way to you know search and
filter and there's we're kind of
building into the initial system
thinking really hard about how to make
the the user be able to find what you
wants quickly okay so what's the status
so this is like I said before a
collaboration with MSR and there's a
number of people which are I have to
think for kind of making this really
kind of happen but it's still a work in
progress maybe in a few weeks or months
we'll hear more about this so just final
remarks you know we're trying to build
as collaborative platform you to you
know provide hopefully will provide some
value to researchers and I think a lot
of these ideas you know are still kind
of being sorted out so I would kind of
encourage you guys to give feedback and
suggestions and if you're interested in
getting evolved up please come talk to
me thank you thank you
clusters mentions questions and also
solutions to some of the challenges
works yes so this is a question for both
of you Roger and Percy so it's about one
particular slice of my talk to which is
the engineering machine learning systems
you have three ways to combine machine
learning system lives separate training
where you turn each module separately
you have sequential training when you
train each module on the output of the
previous ones which might involve
reliable link and you have global
training which is to Train awaiting
together which assumes that you have a
way somehow to quad quad back propagate
derivatives in the system and I'm very
interested and okay the third one works
better than the second one which is
greater than the first one so I'm
curious to know if you have IDs to
support such scenarios yeah so
definitely joint training is very
important to get our good accuracy I
think one of the challenges kind of
working in this at the system's level
rather than their level of equations is
that you know I want to take your module
and jointly train it with you know my
module which I developed for a different
case and I think there we've thought
about ways of setting up the interface
so you can pass uncertainty into my
model and think about I mean other
conceptual level techniques like you
know in some sense you know duo
decomposition ideas kind of allow you to
D couple problems into two and have kind
of each one do inference separately and
then have ways of passing messages
together so yeah so that can if so if
you have a messages kind of you know
modules arranged in a series you know
you get some downstream signal these
messages can be back propagated as well
another question
solutions ok ok so here now just the mic
is the winner do you have any even
preliminary demos or screenshots of co2
lab so we can get a sort of a feel for
what it actually sort of what is the
feel of the user experience because a
lot of these are very high level ideas
that I think we can all agree on in
terms of good intentions but but in
these with tools like these the devil is
unfortunately in the details in terms of
how does it actually work and what the
experience looks like that's a good
question so for ML comp there it's
running live and you can see it for Kota
lab currently there's I can't give a
demo right now but I think very surely
there will be something and it's open
source so I think our goal is to make
the development process transparent so
you can go to github and check it out me
again I'm sorry sorry about iid so at
least in ml com it's very based on the
idea that you have a training set
tactics that you can trust the result in
the practical world it's not like that
and still now the training set that is
that might be the best thing that you
have but you don't want a single number
you want to characterize it by a lot of
other things about you don't want a
number you want to know how the number
was produced in order to evaluate how
robust disease and how is going to react
to possible changes in the case where
you have a feedback loop you have the
problem of evaluation in reinforcement
learning why you want randomized data in
a certain way and relatively
sophisticated evaluation methods and
again I'm curious to know if you have
time to comment about this particular
problems yeah so so so definitely one of
the changes going from my comp decoder
lab is supporting more deeper analytics
and not just kidding a number and be
able to draw learning curves and I think
all those can kind of be you know the
idea something I didn't talk about is
being able to set up these kind of
standard templates where you want
certain
you know ROC curves or whatever and make
them kind of macros which you can kind
of consistency apply regarding the
question about a value I mean
reinforcement learning I think is a
trickier one one way that so you know
part of ML comp and deepen if you delve
deep enough actually does to provide
support for that kind of training where
remember these are just in your programs
running on data I mean it's you know
it's so it's very generic so if you set
up if you want to serve a kind of
reinforcement learning environment where
you have kind of a environment talking
to a learner and sending signals you can
you know you can do that and in fact I'm
elk hunters are online learning have a
domain where the learner communicates to
this kind of host in and you know
they're get messages that way so one of
the most interesting things about gaggle
is the amount of hand-holding that goes
into setting up the competitions and the
contests right so where is the human in
the loop in these efforts I mean I think
we need an army of machine learning you
know enthusiasts who can handhold some
of these you know datasets setting up
the frameworks and so on so so I think
unless we are very careful about putting
the human in the loop and ensuring that
we have that you know each of these
frameworks has that same depth of human
touch in explaining the problem
understanding the problem envisioning
the solution to that problem I think we
are doomed for some very bad failures
that's just my comment but if you can
address some of that yeah so so I
definitely one of the things I was
trying to highlight with a documentation
and the lack of literate programming is
that you know when you go to one of
these sites you don't want to just see
you know some you know data and the some
numbers right and go if you go to
leaderboards I mean leaderboards are I
mean from a scientific point of view
they don't offer anything did you see
numbers and you know know kind of what
the methods were but the idea is to you
know how this system of modules and you
can go into each module
and see how things are connected and
people will have rid of how insights
into the different modules so I'm I
guess part of this was you know I'm not
advocating kind of a completely
automatic way to approach to machine
learning right where you just upload
your data set and bam you get back the
results but this is hopefully supposed
to automate things that you would have
done manually so if you actually were
going to run these five programs and you
know in manually one by one you know
this can help you do it in one click
right but it's not going to replace any
kind of brain cells okay last question
so I can see this can be a very useful
service I wonder if you could comment on
two things one is do you think that this
can potentially evolved into a more
sophisticated workbench it for doing
more research on machining I this is
actually tied to neons question earlier
I think about can you can you for
example feed some of the output as
features together with some other they
are just manipulate the data generator
derive the data set that can simulate
some of the strategies so that you can
you can turn this into a book page where
you can turn more analysis I think you'd
imagine analysis just yeah I think the
intention is to turn it into I mean a
workbench I mean even from the the
sketch oh is that you have these you
know if you have intermediate kind of
modules you want to be able to plug them
in and and all the little visualization
you know everything is just you know
code running on you know data right so
one of the visualizations would be a
module that allows you to like generates
graphs from the raw data and so on so
yes so the then the question here is how
to handle the interface is among all
these different modules and right so I
mean I think the right way to think
about this is you know this this isn't
some some providing operating system
right and some what I mean the operating
system is kind of general but at some
point you you need the programs that run
right so in some sense we decouple this
you know don't worry about kind of
versioning and systems issues just focus
on writing these modules and then
hopefully as a community if the number
of modules increases then you'll be able
to kind of do things with that so the
second thing is do you envision this
will be connected to literature search
and just connect the sum of the relevant
content in literature to the results
here yeah definitely so one thing I
actually didn't talk about and I'm kind
of surprised I didn't talk about is the
notion of oh I think it was on one of
the slides is this idea of an executable
paper right so when you read a paper I
guess in the first thing
age I hope that you know there will be
no tiny URL links in the figures that
click directly into the site and you can
see well these were the results that
back this figure and you can you know
delve into the data rather than just
like squinting at one of those tiny
figures that and a stage to i think is a
little bit more i'll kind of
forward-looking is you know there's no
reason why you know papers have to be
restricted to you know PDF file right so
you can imagine you're worth these
worksheets you know creating these
exigua papers where you have code and
data kind of in the same place right
with that we're going to close this
session and thanking our speakers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>