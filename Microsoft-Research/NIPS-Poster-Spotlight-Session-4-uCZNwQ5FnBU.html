<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 4 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 4</b></h2><h5 class="post__date">2016-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uCZNwQ5FnBU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
this is the last pot light session for
the evening three minutes for all the
spotlight speakers and please visit
their posters at the end hi my name is
Jamie Morgenstern I'll be talking about
the suta dimension of nearly optimal
options this work is joint with Tim Roth
garden from Stanford so the problem we
study in this paper is the problem of
designing an auction for selling a
single item in a setting where you're
trying to maximize the amount of money
from doing so this problem is well
understood in the setting where buyers
are assumed to be drawn from some
product distribution which is known to
the auction designer upfront and it's
even possible to get revenue guarantees
in a setting where you assume nothing of
the sort of that buyers may be coming
from some distribution or in an hour
instead worse case although the
guarantees are obviously much weaker in
such a setting more recently a number of
pieces of work have studied the problem
assuming buyers are drawn from some
product distribution which is not
explicitly known to the auction designer
and instead the auction designer has
some small number of samples from that
distribution and from those samples aims
to design an auction which works well on
a future draw from that distribution
much of this work focuses on choosing a
specific class of auctions and showing
that it is simple enough that a small
namely polynomial sample size is
sufficient to approximately optimize for
revenue our work on the other hand looks
for a specific definition of simplicity
and in this case we propose the use of
pseudo dimension which is just a real
valued analog of VC dimension of a class
of auctions and for free once you've
shown that a class is sufficiently
simple or has sufficiently small suited
to mention one gets sample complexity
guarantees for revenue maximization over
that class of auctions and this allows
one to balance the trade-off between
optimality in terms of revenue
guarantees and both the complexity and
sample complexity of a class of auctions
the main technical contribution of this
work is showing that there exists a
class of auctions with polynomial pseudo
dimension and that class actually has a
Shin in it which is approximately
optimal in terms of revenue this has two
interpretations one of which is that C
has small sample complexity right so
with a polynomial sample one actually
gets good generalization error small
generalization error and also that
there's good representation error that
the class actually contains a good
auction great thanks come see me at my
poster hello I'm Philippa dawat and this
is joint work with tongue boy and
Richard Turner we are presenting a
generative model for time series
consisting of convolution between a
white noise process and a
continuous-time in the air filter this
model is well known and it's been around
and because it arises in many branches
of science and engineering and the
reason is soft is this model is so
flexible and it's because once we
conditioned on the filter or once we fix
this filter arbitrarily the resulting
signal ft this dysfunction here becomes
a Gaussian process when an arbitrarily
defined covariance function which is
here so our contribution is to place a
Gaussian process prior on this filter
which in turn induces a nonparametric
distribution over the covariance
function of the resulting Gaussian
process and this sounds great but as we
all know great models come with a great
inference in fact abilities and in our
case there are three main challenges and
inference in our setting means finding
the the processes h &amp;amp; x and the first
program is that x is a white noise
process so it only behaves well well
inside the integral the second
intractability comes from the fact that
hate and eggs are multiplying one
another and the third one comes from the
nonparametric structure of these
processes and we're going to overcome
this these challenges by using an
approximate variational inference
setting where we're going to find the
approximate posterior of a finite set of
variables that encapsulate the
dynamic of this of these two processes
hate a neck and an X in an alternative
domain where they behave behave well and
conditional on this finite set of
samples we are able to approximate the
posterior over the kernel of the of the
signal and and the power spectral
density and we can see very briefly some
results and the that weekend we produce
with our with our model we called the
Gaussian process convolution model and
the first top different and the top two
plots and show a true spectral mixer
kernel and where our model was able to
recover the the the kernel with with
uncertainty bars and the bottom two
boxes show a spectral estimation for
real real world data where we used an
unevenly sample data the take-home
message is that the Gaussian process
convolution model is a prior over models
for time series not directly on time
series and we have a many more results
and things to discuss please visit us in
our poster later today or in the time
series workshop on saturday thank you
very much
hi my name is Jackson Gorham and this is
joint work with Lester Mackay at
Stanford University okay so markov chain
monte carlo methods are good because
they allow us to do inference on
problems that are very complicated and
don't have a closed-form way to we can
do inference so that's good but they
come with a cost whenever you're using
an mcmc method requires you gather you
iterate through the entire data set to
generate a new sample point so if you
have a huge data set this is just not
computationally feasible to address this
recently there's been work that has been
called are we put on the class of names
approximate MC methods with subset
posteriors and if you don't know if
that's our that's just a fancy way to
say rather than using a whole data set
in order to do the mcmc iteration you
just use a random subset of the data so
this is good because you sped up the
operation but it's bad because now these
procedures are biased and by bias what I
mean is that the invariant distribution
is no longer the target distribution
that you're trying to do inference on so
how do we assess these bias-variance
trade-offs what could we do so one maybe
idea that we could use is we could just
pull an off-the-shelf mcmc diagnostic
like ESS with that would that work for
us so here's two samples that we drew
from a bimodal distribution each came
from a different approximate mcmc
sampler so if we were just use ESS we
would choose the sample on the left
which you can see is like horribly over
dispersed for this distribution so in
our work we developed this new practical
tool called the stein discrepancy and in
this example it would choose the
procedure the sample on the right which
is a far better approximation to this
distribution so what is the sign
discrepancy this is like the infomercial
part of this so the stein discrepancy
it's based on Stein's method and it
selects high-quality sample samplers or
even tuning parameters for your samplers
and the previous example I just showed
you was for approximate mcmc samplers
but this would work for any sampler
whether it's exact or whether it's
deterministic or whatever else it's
useful for her assessing the convergence
rates of samplers so we've actually used
it to show that the convergence range of
something like hurting is faster than
the theoretical bounds that we've been
able to prove thus far and one of the
cool things about this also is that you
don't need to know the normal is a
constant for your target distribution so
we have some good theory too we can
actually qualify under what properties
of the target distribution you have the
stein discrepancy metro size notions of
convergence like convergence in
distribution of the vast erskine metric
so if any of this sounds appealing to
you guys come check it out at poster 62
Thanks hi my name is Lou young from IBM
research and i'll briefly introduce our
paper cause form estimators for
high-dimensional generalized linear
models this is a joint work with orale
Lozano at IBM and project lab akuma at
UT Austin so in this paper we revisit
the standard problem generalized linear
models that includes several different
models like linear regression logistic
regression or Poisson regression models
the only difference across different
models is on the log partition function
in the exponential family form and
therefore in the maximum likelihood
estimator so even for high-dimensional
setting the regularized emily is known
to be working very well with respect to
the statistical performance but if our
problem is really huge or the target low
dimensional structure is like very
complicated like lowering matrix
regression parameter then this
regularize mle becomes steady sorry
computationally very expensive so the
goal of our paper is to provide a
closed-form analytics solution for
high-dimensional glm that is
computationally extremely efficient as
well as statistically strong so we start
from this stationary condition of
unregular smle but it's obvious that we
cannot have a closed form expression for
the regular eyes our regression
parameter theta from this stationary
condition because the optimization
problem on the regular smle is not well
defined or not uniquely defined for the
high dimensional sampling regime
actually in the paper we found some
two main issues in the construction of
the closed form and proposed some
approximations for this but relax but of
course we have to be very careful for
the statistical guarantees what happened
here so yeah so we have a closed form
expression now from the slightly
modified or a slightly relaxed
stationary condition of unregular eyes
emily and finally to encourage the low
dimensional structure on our estimation
we perform very simple operations like
element wise software shoulding for the
sparse sparsity case for instance hope
so moreover we also show that this kind
of construction can can guarantee the
statistical strong statistical
consistency across in terms of different
error gnomes so yes that's it so if you
wanna know the further details on our
construction please come to our poster
that's the post ID 85 thanks
hi I'm Andrew Wilson a postdoc in the
machine learning department at CMU I'm
presenting the human kernel which is
joint work with Kristoff down
Christopher Lucas and Eric Singh i'll be
at poster 24 tonight truly intelligent
systems should be able to learn and make
decisions without human intervention yet
humans can easily solve many
generalization problems that are very
difficult even for the most advanced
machine learning approaches consider for
example the figure on the left in panel
a we humans can easily generalize this
sawtooth pattern which is illustrated in
black various human extrapolations are
illustrated here in purple however
gaussian processes even with quite
advanced kernel learning approaches
produce the rather unconvincing
extrapolations shown in blue in panel B
in our paper we learn human kernels
which encapsulate human function
learning biases we use these kernels to
produce human-like predictions on a
variety of problems and also to gain new
psychological insights about human
function learning compan LC we use the
human kernel to produce the
extrapolation shown in purple which are
essentially indistinguishable from the
genuine human extrapolations in panel a
on the left we were very interested in
our human participants we explored how
humans progressively learn about various
function classes we often found that
what they didn't learn was more
revealing about their prior biases than
how they did adapt their new
representations we also considered how
humans perform model selection over
infinitely many choices we compared to
gaussian process based model selection
which can surprisingly be biased towards
underfitting towards over simplicity our
interactive experiments are available at
WWF unction learning calm and everyone
here is welcome to participate and
provide even more human data you can
find me at posterior 24 tonight and I
would look forward to meeting you thank
you
hi everyone I'm you entering from
Columbia University and this is a joint
work with large square singh Krishna
shenoy and my PhD advisor John
Cunningham our paper proposes a linear
dynamical system model for
high-dimensional point processed data
that captures the under dispersion and
over dispersion of the data so the data
we analyze is neural spectrum data which
is represented as a high dimensional
time series and data with count
observation so we observed either
spiking activity exhibit strong under
dispersion namely the variance of the
spike counts is smaller than the Ming of
the spy counts this is inconsistent with
the common person assumption to proceed
when you introduce a general count
distribution family that generalizes the
Poisson distribution and captures as
special cases many other common count
data distributions our a welcome a while
maintaining a exponential family form so
our distribution has a parameter G that
controls the dispersion of the data
coupled with a classical linear
dynamical system model we are able to
perform dimension reduction while
accounting for the dispersion of the
data and we develop efficient and
variational Bayesian algorithm to fit
the model and we show that our model
gives significant performance
improvements on real data as compared to
the Poisson counterpart if you're
interested in a and flexible hunted data
model and comes the art poster number 26
thank you very much
I am Jonathan this is a joint walk with
on the way Zack Meisel a long period
from A&amp;amp;T in Marseille and my advisor
Gabrielle Perry from Beethoven the paper
introduces a biology keep me motivated
the generative model of dynamic textures
crafted for probing visual perception it
comes with a width of psychophysical
experiment as a proof of concept our
algorithm generates dynamic Goshen
texture parrot raised by distribution of
three classical parameter tested in
visual neuroscience and psychophysics
name is a special frequency orientation
and speed on the slide you can see for
is on pairs of texture with different
distribution of the parameters these
distributions are linked to the
observers motion with wrestle with
respect to with respect to its motion
movil will superpose the first real-time
texture synthesis based on stochastic
partial differential equations it allows
the experiment earth which generates
estimation on the fly without the need
of pre-computation or storage we use
this model in a two alternative faucet
choice experiment in which we test the
effect of special frequency on speed
perception second trial of the
experiments consists in the presentation
of two consecutive stimulii stimuli with
different spatial frequency and speed
the subject is asked whisk which of the
two soon I was pursued faster to analyze
obtained data we propose a statistical
model based on average Bayesian
inference using a bayesian observer
model is a powerful approach to to
explain basis in human decision-making
processes in this Bayesian model the
observer is assumed to estimate speed
using a maximum at mysterious
so with lake hugh based on simulation
and an internal prior such a medela
allow the computation of the
psychometric curve and the estimation of
the observers parameter based on
classical psychometric clue namely the
point of subjective equality and to slap
at that point we explained a simple
method to relate the regarded
psychometric curva with our asian model
and does provide a solution for the
embers bajan problem and the values
condition we observe a positive effect
of special frequency of a speed
increment such an effect could be
explained by a change in pious lab I
will be pleased to offer you more
explanation its post there are 14 thank
so good evening my name is pure Sri I'm
from IIT Kanpur and this is joint work
with Chiang Mei who regarded an hour and
Lawrence carrying from the Duke
University so this paper is about a new
bayesian approach for doing multi-label
learning so what multi-label learning is
so basically given an object with
features in the Golan multi-label
learning is to predict a label vector
which is binary each element is of
binary value and the each element
basically tells us about the presence or
absence of a label from a large label
vocabulary so in this paper work we are
focusing on this this problem of
so-called problem of extreme multi-label
learning where the number of labels
could be massive and consequently the
number the size of the label matrix in
the training data could be very massive
and at the same time the label matrix
can be highly sparse so in this work we
are proposing a fully Bayesian model for
doing extreme multi-label learning and
our approach is based on doing a load
and committing of the label matrix so in
this work we are treating the
multi-level problem learning as a topic
model learning problem so the way this
model works is that we assume that each
label can be treated as a word and each
and there is a each label vector which
is basically an L dimensional binary
vector so that can be assumed to be
generated as a combination of a set of K
latent topics each topic itself is a
distribution over the labels and the
combination weights of these topics will
depend on the features of the given
example our model is fully Bayesian and
at the same time its conjugate it has
full local conjugacy which allows us to
develop very efficient Gibbs sampling as
well as VB or iam inference algorithms
another key aspect of all of our model
is that learning the model scales in the
number of non zeros in the label matrix
which is very nice because many of the
extreme learning multi-label learning
problems have very massive but highly
sparse label matrices another key aspect
of our model is that at test time you
don't have to infer
the combination weights and the test
time prediction for the test example can
be done in a closed form so for more
details please visit our poster or
poster ID is 17 thank you so this is the
close of the afternoon sessions let's
thank all the speakers one more time
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>