<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards optimal algorithms for prediction with expert advice | Coder Coacher - Coaching Coders</title><meta content="Towards optimal algorithms for prediction with expert advice - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards optimal algorithms for prediction with expert advice</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/D7MskLrpJ6s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I good afternoon everyone so as the
title of the talk indicates this is very
much this topic is very much a work in
progress so while you I think you'll
hear some nice results there's this
interesting challenge to find the you to
understand the generalizations anyway
happy to have value talk about the
toward optimal algorithms for prediction
okay so thanks you all this is giant
work with Nick Gavin and you want Paris
and Leykis postdoc in Microsoft Research
at New England he visited us for a few
weeks this summer okay so this talk is
about prediction with expert advice and
since basically a classical situational
decision making process and you're going
to study this problem in a purely
adversarial setting is nothing
stochastic here except for the
strategies of the algorithm of the
player and the adversary ok so the
setting is very simple here is how the
game proceeds so there is a player and
at each time step till the game stops
some stopping time the player has to
make a decision and for making this
decision he has k experts it is disposal
each of the experts at the beginning of
the day I ask him to do something and he
has to follow one of these experts and
after he does that there is an adversary
who comes and sits a gain okay so if you
follow expert I then the expert i gets
gain GI t and the player if he uses
expert I he'll get the same gain as the
expert okay so the player chooses expert
Jay he gets gjt and after day t he gets
to see the gain of all the experts okay
so setting clear when daty begins the
player gets to see the entire history of
gains but not the gains on day t using
these things he has to pick which expert
to follow after this gains are revealed
okay so this is purely adversarial and I
am NOT going to assume anything about
how the Gita related except that there
all counted in zero to one the one the
player feet when he sets the game for
the expense well the adversity even know
the player strategy we are going to do
minimax but but it doesn't know what yes
but I mean if you knew what he picked in
no because the way rolling okay looks
like the bigger X and then gamers in
there okay yeah so I'll make it here we
go to talk about the minimax yes okay so
to illustrate variants one is that the
stopping time is finite equal to some
capital T the other is the stopping time
is geometric random variable with mean
one over Delta both of these are natural
and we will study both of them stock
okay so what is the strategy what are
the strategies of the adversary and the
player so at time step T if you have to
make a decision you have a disposal the
gains of all the previous days till tea
and going to call capital G use capital
g for the cumulative gain or on the past
days so what is adversary strategy the
adversary strategy is basically a
distribution which is a function of all
these gains in the past days using which
it will draw the gain vector 480 and
basically binary adversaries of most
powerful I won't prove this that you can
restrict the gains to be 0 or 1 and what
is the algorithm strategy it has to pick
a probability distribution day T over
the K experts which is again a function
of all the culet of the gains over the
past is I am going to use 80 for the
algorithms reward at time T and what is
the measure what you want to measure
here what does the algorithm want to
minimize wants to minimize the regret
which is the difference between how the
best expert did max overall I of the
cumulative gain of it but I minus what
the algorithm got which is the sum over
all the days of what you got ok ok so
what is the worst-case regret if you fix
the algorithm a what is the worst regret
possible you maximize overall the
distributions of the regret you get for
distribution d so what's the worst case
optimal algorithms regret see you choose
the best algorithm as you mean that the
adversary is going to be tailor to kill
this algorithm right this is the minimax
optimal figure 8 then we will use the
minimax theorem all over there you can
switch the order of the adversary and
the algorithm that is if the Al Gore the
player goes first and the adversary is
designed to kill the player the same as
adversary going first with algorithm
designed to kill the adversary so these
two result to the same regret triplet ok
ok so now I have to go through this
mandatory motivation slide this is
really the first frame or proposed for
online learning there's special
significance because of that there's
applications all over the place I feel
you know the standard application other
predictions and asset protection click
predictions ok so what is known for this
problem is the beautiful multiplicative
weights algorithm which works as follows
if you have cumulative gains at day t
minus 1 z 1 2 gk for the k experts then
you compute the Exponential's of all
these cumulative gains and decide to
follow expert eye with probability
proportional to this exponential of the
game okay so there is a parameter ETA
which can be tuned and for the optimal
value of the parameter this
multiplicative a top arisen gets regret
of square root teal or k by 2 t's again
the time horizon and k is the number of
experts beautiful thing about this
algorithm is it does not reference the
number of remaining time sets of the
total number of time steps t it also
does not reference the past predictions
it all does this use the cumulative
gains and this is asymptotically optimal
WS in particular optimal is both the
time horizon and the number of experts
goes to infinity okay okay so this is
there what do we want to do here so all
along the optimal algorithm in the lower
bones have been two separate streams of
work algorithm with lower bounds and
this has resulted in the gap between the
upper bound at the lower bounds what we
really want to do is simultaneously
develop the optimal algorithm the
adversary to completely bridge the gap
between
upper and lower bounds okay so basically
we want the optimal algorithm and the
adversary but not just that if the
algorithm is hideously complicated then
there is no point k is there an
algorithm which is as simple and
intuitive as the multiplication
algorithm but it is also optimal for a
small number of experts not just
asymptotically yeah so that's really the
question will be studying complex
algorithm already said was there in
complex algorithm know there is a
multiplicative its algorithm I mean if
you design some complex algorithm then
you have to justify is the small gains
vs. the complexity you get okay so here
are our results I will begin with two
experts ok so the optimal adversity is
the following it is a very simple
optimal adverse three so you set the
gain of the experts always disagree
basically you set expert one to the gain
of expert 1 to 1 and 2 to 0 with
probability half or expert to 21 and
expert one to 0 with probability half ok
so the pairing Lee disagree and this is
the precisely optimal adversary not just
asymptotically and the optimal regret
that you get in the finite horizon model
is exactly equal to the half how they
expected absolute distance travelled by
a simple random walk in T steps again
this is exact not asymptotic but as T
tends to infinity this quantity is
square root T over 2 pi okay now if you
compare this to what the multiplicative
wait algorithm gets the writable
algorithm the multiplicative regret is
47.5 percent larger than this and
similarly for the geometric horizon--
model this is the exact regret okay
where delta is the probability which the
game will stop today and therefore they
expected Thank force the game will last
is one over Delta so this is the optimal
regret and as Delta goes to 0 optimal
regret is this number given geometry
corazon model is that the swapping time
is not fixed the game end any day with
probability delta
okay so this is the optimal regret and
the optimal adversary and here is the
optimal algorithm okay for two experts
so what is the numbering convention and
going the numbering conditioner uses i
am going to number express the
descending order of cumulative gains
okay the experts one and two do not
refer to the identities of experts it
refers to the leading expert in terms of
gains and the lagging expert so the
algorithm is extremely simple okay as
simple perhaps even simple as a
multiplication algorithm so you
initially site we some function of Delta
this is basically roots of the
characteristic polynomial or recurrence
we will set up later but for now which
is some function of Delta okay as Delta
goes to zero it is basically 1 minus
square root 2 Delta okay you said this
and for each time step T till the game
stops you do the following you compute
the cumulative gains for the two experts
right g1 and g2 t minus 1 you compute
the difference the accumulated gains
okay you choose the lagging expert with
probability half of sight of the tea and
with the remaining probability mass you
choose the leading expert okay and this
is the precisely optimal algorithm for
every Delta not just asymptotically okay
ok so this algorithm is not
multiplicative its algorithm but is a
novel twist on multiplicative weights if
you think about it it's really
multiplicative weights on the lagging
expert right so you're doing
multiplicative weights on just a lagging
expert here the remaining probability
mass is being used for the leading
expert in comparison the multiplicative
actual multiplicative weights is this
okay and our algorithm optimal algorithm
cannot be expressed as somebody racial
rhythm or even as a convex combination
of multiplicative circle so it is
outside the MWA sampling
okay so that's it for two experts so we
got the optimal adversary optimal regret
and the optimal algorithm finest Rossum
the algorithm for finite horizon will
depend on the number of remaining time
steps T so it will get complicated
because there is going to be an
inevitable dependence the number of
therefore it's not succinct cannot be
sexually described like this because I'm
describing this for every Delta and if
you want it for every T surely for
instance you know if you know that
maximum distribution is going to be
beautiful yes the adversary strategy is
the same for both the finite horizon--
model and the geometric model it is
going to be half and a half yes for two
experts yes is there a simple way to see
that's the adversary ability I'm going
to do all those things yeah i'm going to
go direct optimal adversity and Albert
so this is just stating the results so
the more bounce of the Triffids fanta
rising is the same as the one for two
constant experts GD asymptotic minimax
value you know one when I have to
consult experts 0 0 0 0 or 1 mm-hm you
get the same asymptotic value okay and
that's exactly because the your maximum
distribution is oblivious okay public
via so because it's just hahahahaha it's
oblivious to what the to the strategy
you say generally the adversary will
depend on on pasteurization of gain
losses yes this is history independent
yeah in this case is not yes and so this
also applies to be the model in which we
should have a static expresses the
experts are just constant and making
this always the game simulation this
isn't all the result bike over there
working is this tactic kicks if the
setting is different I guess it's a
because you experts are constant right
and you know that
or not the adversary is not maximizing
over the position of the experts but to
just showing that this is not right
giving any extra power right you already
get this right great mmhmm yeah um
but for three to the competition I ok so
that's the story for two experts so here
is what we proved for three experts I am
saying two and three differently because
three significantly more complicated
than to the optimal regret the geometric
model is precisely this for every Delta
not just as Delta goes to zero but as
Delta goes to zero this number is two
thirds of square root on over to Delta
so in comparison to to it was the
constant was one over to hit half
becomes two or three and the
multiplicative wait algorithms regret is
39.3 percent larger and the optimal
adversary in the geometric horizon model
as Delta goes to zero so here I am going
to describe the asymptotic optimal
adversary but even the absolutely
optimal adversity for every Delta is
simple okay so note that a pre or it is
not even three of the adversary should
lend itself to a sucks in ten small
description it could be very complicated
but it turns out that optimal adversary
is simple ok so you advance experts 1
and 3 the leading in the lagging Express
together with probability half and the
middle expert with probability half
separately ok so this is the
asymptotically optimal adversity and if
you want the exact optimal advisor you
have to do something slightly different
at the boundary points at the origin you
do something different put give
advancing
okay yes so so you set gang of 101 that
probability half and 0 1 0 with
probability of synthetically optimal
adversary for right mm-hmm so this is
the leading expert the middle expert and
the lagging expert so this is basically
what I mean they're right is how you set
the games again this much simpler than
whatever expect okay so so that is the
optimal regret and the optimal adversary
and here is the optimal algorithm again
you initialize sigh to the same number
which you recall is 1 minus square root
2 Delta Delta goes to 0 square root of
the characteristic polynomial and the
optimal algorithms as follows so you
compute the cumulative gains for all the
three experts you compute the difference
d1 between the gains of the first or the
second you compute the difference d2
between the gains of the first to the
third right and you follow the lagging
expert with this probability which as
you can see again is multiplicative its
algorithm on the lagging expert and it's
a further generalization now so this
probability mass is now split between
these two guys to cancel after that it
is again multiplicative its algorithm on
the second like an expert and the
remaining masters put on the leading
expert okay and this again is the
precisely optimal algorithm for every
Delta okay okay
so that's the story for three experts
sighs the same sigh for that it is the
same characteristic polynomial it's
about 1 minus 10 it's yeah sigh
is
so things things get even more
complicated for four experts we don't
derive the optimal algorithm but we
derive lower bounds on the optimal
regret and the techniques are again
completely different from what we use
for two and three lower bounds are based
on connections to random walks and this
is the regret we get I specify the
adversary strategy in a minute and we
very strongly believe that this is the
synthetically optimal regret okay
there's strong experimental evidence for
this basement computer simulations and
if the conjecture is true then this is
all so basically the what the optimal
algorithm can get by minimax and
therefore the multiplicative eight
algorithm is 32 point eight percent
larger then what you get here and the
adversary which are cheeves this is
again extremely simple okay it's a
generalization of what i wrote for 3 so
1 0 1 0 probability half 0 1 0 1 with
probability half
on X
is the adversary we study we do not yet
prove it is optimal don't think it is
about so we call this the comp adversary
right so you could think of the way in
which kids will divide into teams while
they play a soccer match the leading
prayer will go to team won the second
player will go to another team with our
go to the stream so on ok so let's do
regret so yes
so that we love kiss the auto experts
will have the same game as they even
experts know because you know because
they'll swap right is changed I mean
they do the orderly the ordering here
it's not the identities yeah yeah hi
Elvis yeah yeah yeah you okay so I'll
spend the next it was a question well
what we do it in time I guess it doesn't
matter yeah
so I'll spend the next few minutes
talking about how we compute the optimal
adversary ok so the most useful thing
for computer up computing optimal
adversity is that it is a weekly
dominant strategy for the adversary to
always be balanced ok so what do you
mean to be balanced adversary the
expected gain for each expert can be the
same ok and this is irrespective of what
is the cumulative gains that you gain so
far ok so you might expect that the
optimal adversary is balanced overall
basically but not just that even
conditioned on what has happened so far
you still I can afford to be balanced
that is the probability mass you assign
to each expert could still be the same
ok the optimal is always balanced the
proof is very simple ok suppose the
optimal adversary were not balanced ok
then what is the best response algorithm
for this optical adversity it will
obviously go for the expert the largest
expected gain right which is larger than
something else because it's not balanced
now consider you know taking an
arbitrary step at which this happens
right the adversity is not balanced and
considered making it balanced ok raise
the expected gains of the other experts
who are not equal to any expert this
does not help the algorithm because it
already picked the this does not help
the player because it already picked the
expert but it could help the adversary
because the max experts game can only
increase we're doing this right so there
is no point not being balanced and the
biggest thing that we get out of this
balanced adversary is that the algorithm
is completely out of the picture no ok
well you analyze the optimal adversary
it does not matter what the algorithm
does write the algorithm could bind
blindly choose anything what would be
the algorithm you get the same regret
and in particular from now on we are
going to fix the algorithm to be
uniformly randomly choosing an expert
okay you choose expert I with
probability 1 over K for each I and
therefore the regret the just you know
expert minus algorithm is now max minus
average quantity that's it right you
think of frigid here f is max minus
after I am going to talk to you
max minus average often okay so this is
not you're not saying this is the
optimal yes this is not the optimal
algorithm if if you fix yeah
so maybe say again why you're allowed to
consider it okay so I mean if the
adversity is going to be uniform along
all the experts the algorithm does not
care really which expert is going to
pick so it could just throw stone
somewhere and order expert it hits you
choose that expert you get the same game
is an equalizer yeah
for this is not optimal virtus we spent
the sliced or CC Cardenas okay so we'll
go back oh you know other slides were
about tell you have these evidence that
do better than multiplicative weights
and the whole time I'm thinking how can
you possibly do better it is you know
what can you do against these
adversaries just doing something
balanced analyst and house so this so I
mean you're doing minimax here so if you
fix the worst adversary what we do as an
algorithm and if you fix the algorithm
what the worst adversity does for it if
you fix the multiplicative its algorithm
I can design an adversary which Lin
flicked some amount of regret on it okay
hi I can design a better algorithm where
the best adversary designed for it can
only inflict lesser regret that's
basically the point right you give me
the multiplicative ace algorithm I
design the adversary specifically for
that algorithm and in fixham regret and
then use this algorithm is this forty
seven percent extra regret isn't the one
in folder something up yes right because
minimax is in equilibrium and
equilibrium breaks if that algorithm is
the player is not up following the
optimal algorithm right actually when
you say that you are forty-seven percent
better than which because you ways do
you know that this is tight I mean you
ever know yes so I it is just what is
known for analysis we wanted to
construct a counter example we have
something but not quite pin down the
that's totally collagen safe
transportation together in the best
bound on the directories our flower
bounce for the guests ways there are
lower bounds for multiplicative weights
multiplicative aids but those lower
bounds are yeah because I mean the best
lower bound I know is square root
log to the base to ok this is in the gap
is larger so I mean I said 47.5 percent
which is 1.47 and this is more like 2.35
yeah at least 4 K equals 2 so that that
lower bound doesn't help so you would do
something else but we try to do it we
have something but not quite ok so so
for the optimal adversity which is a
balanced adversary while analyzing that
adversity you can forget the algorithm
ok that's the point I've made so far so
what does this balance this give us read
it immediately gives us what the optimal
adversity is for the case of k equal to
two experts why is that so so you look
at the regret expression which is the
max minus average just is proved now for
the case of k equal to 2 max minus
average is simply the half of half of
the absolute value absolute difference
and you just split the cumulative gains
ok and write this right capital G is
some of liturgies right now consider
this random variable XT which is G 1 t
minus G 2 T what can this be right I
said the gains are just 0 or 1 so g one
team and minus G 2 T could be either
minus 1 or 0 or 1 right if the adversary
is balanced what does that mean
expectation of XT has to be 0 right so
what is the only choice left for you the
only choice left for you is what is the
probability with which xt is equal to 0
right see you are the guy who is
designing the optimal adversary now and
these are the restrictions because of
balance to see of these restrictions the
only toys in your hand is what
probability can set for XT to be 0 but
the goal is to maximize the total
distance you have to have absolute
distance of origin and if you want to
maximize that there is no point being
lazy right there should not be any
element of laziness in the random walk
so you basically have to design a simple
random walk right you go right with
probability half and left with
probability half which is precisely
experts always disagreeing right gain of
10 with probability half at 0 1 with
probability
exactly so what do we do so far so the
adversaries available actions are one or
two or no advancing expert one alone or
two alone or both of them together or
none of them and we said that there is
no point being lazy and sitting at the
same position which means there is no
point in choose advancing both of them
or no point in advancing one of them the
remaining is just one and two so you
advanced expert one alone with
probability half to a lot of probability
half and this is the optimal adversity
ok so the simplification offered it for
the k equal to 2 k's does not carry over
to k equal to 3 or larger this max minus
average business boiling down to half of
the absolute difference does not go
through things become way more
complicated so what do we really want
rate so the adversaries available
actions are all subsets of 12 k just
like adversaries actions here was 12 12
0 2 to the K actions and you basically
have to construct a balance to
distribution at every single step right
to maximize this max minus average
quantity okay so you have constructing a
controlled random walk so to say so
basically at every step you have a
convex comp no convex polytope of
balance distributions and there are
exponentially many vertices for this
convex polytope okay it is not just that
the number of vertices the convex
polytope is exponentially where even if
you have just two choices is still
complicated because you have a number of
steps it is a controlled random walk
that you have to design
so this is exponential in tours we yes
eventually MK exponential k yeah okay so
i'll come back to deriving the really
optimal adversity for three and four
later for now I'll go to the optimal
algorithm so the optimal algorithm for
the finite horizon-- model can be
computed by backwards induction right it
is going to be inevitably dependent on T
the number of remaining time steps and
therefore does not lend itself to sucks
in description like the other ones you
don't get any structural inside by
deriving the optimal algorithm like this
you can derive it in time T to the K it
is good for small K but there is this
annoying dependence on the number of
remaining time step T okay so we will
move to the geometric horizon model and
from now on I am going to drive talk
about driving the optimal adversity in
the algorithm simultaneously for three
so okay before too we haven't derived
the optimal algorithm get right I'll do
that so this max minus average quantity
is translation invariant because if you
add a scalar to all the to all the
cumulative gains nothing changes
therefore I can normalize the gain of
the leading expert to be 0 right the
leading expert is at zero always and the
lagging expert is at some X which is
negative the optimal algorithms
probabilities are simply a function of X
right because it's simply a function of
the cumulative gains the cumulative
gains here is just 0 and X so it's just
a function of X P 1 of X and P 2 of X
and I am going to use f of X to denote
the regret you get starting at 0 X this
cumulative gains okay so what happens
right if you start at 0 and X adversity
has four different actions if the
adversary decides to advanced expert one
alone then from zero X you move to 1x
right the cumulative gains will move to
1x but one x is not in our format so 1x
basically has to be smooth you know
normalized 20 x minus 1 you spec one in
both nothing changes and then you
advance to alone you go from 0 x 20 x
plus 1 when you advance both 1 and 20 x
goes to 1 and X plus 1 right which again
has to be brought back to 0 and X
squatting one when you advance nothing
zero X station 0 X right so what does
this mean for regret right so you start
at X 0 X there is a probability delta
which it is the game will end then it
there in which case you will get a
regret of 0 because the leading leading
expert is at zero it accumulated gain of
the leading experts is 0 so max minus
average will be 0 so max is 0 average
didn't get any time to go we didn't get
any gain with probability 1 minus delta
the game will proceed so you will get
the max of these four actions are now
looking at the adversary which is the
optimal adversary when the algorithm
follows these probabilities p1 and p2
right so what is optimal adversity if it
follows expert one right then you go to
the configuration 0 x minus one in which
case you get a regret of f of X minus
one right you start at X minus one but
you should subtract p 1 because p 1 is
what the algorithm gets right what is
this plus 1 doing here the plus one is
for going from one x 20 x minus 1 you
subtracted this one and then that lag to
the regret so you have to add +1 there
and when you follow to the situation is
that you are at x plus 1 so the regret
is f of X plus 1 minus what the
algorithm gets this p2 and if you follow
12 then you go to 0 X you should write f
of X but since you went from one to zero
there is a plus one here as a minus p1
minus p2 and then if you do nothing you
just ate f of X f of X really means the
regret that the optimal adversary for
the best response adversary for this
algorithm p1 and p2 gets if you start
from cumulative gains of 0 and X right 0
is the leading expert X is a lagging
expert X is negative therefore if you
start there normally nor any human yes
that is why if you stiff user that is
why here is I say delta x 0 so if you
stop right away you get 0 right
okay so this is what it is and you have
to solve this recurrence first of all
you can ignore these two guys right why
because 1 minus p1 minus p2 is just 0
plus p1 plus p2 is one so these guys do
not do anything right you're just saying
at the same point these are useless
actions so you just have these two right
so you have a max of these two but still
max is pain to solve how do you simplify
this well we know that the optimal the
best response adversary for the optimal
algorithm is the minimax optimal
adversary and we just derived the
minimax optimal adversary to be this
50-50 adversary right and if the
adversary is just a 50-50 adversary it
has to be indifferent between playing
action one and playing action 2 and
therefore you should be able to equate
these two guys right so this should be
equal to this that's what i am doing
here right the minimax optimal algorithm
will make the minimax optimal adversary
indifferent between the two actions it
has at its disposal which is advancing
expert one or advising is for two alone
right and now since you have this you
can add this and divide by 2 which will
eliminate probabilities completely right
so we have this recurrence and for good
luck you also add the boundary condition
what you do it 0 is slightly different
now you have this complete system and
here comes the characteristic polynomial
for this right it is X square minus 2 or
minus delta x x plus 1 and the roots of
this polynomial are exactly the sigh i
talked about and if you solve this it
you get to know two roots i wanan so you
have f of 0 0 0 minus 1 well so forget
the what we do at the boundary condition
rate I mean let's forget that for now
you are right so let us forget that for
now let's go over what we did here right
so this is what we had here so we had
this for things and we two away to
useless things and therefore now you
have these two guys right now there are
two actions and the optimal algorithm is
baking optimal adversity indifferent
between these two and therefore I can
equate the regret you
from these two guys but then you go up
this recurrence and then we got this
then we average right and that throws
away the probabilities so you just solve
this whatever you do to solve it you get
the probabilities the X the exterior is
nothing to do with the other x ah ok
yeah so so I should I mean I should have
used y or something here for the
characteristic for another service this
X has nothing blewett's the excesses
this is in order to recurrence so you
know so you know that the solutions are
sounds of some linear combination of two
geometric yeah so let me just hide this
so this is all the recurrence and get
these probabilities that's all you
should do this is the second order
actually need two boundary conditions to
completely finish so I think's depends
on a 6-1 as possible right but the only
boundary you get is at zero right
because I mean so when do these
equations fail these equations fail only
when X is at zero in which case this is
0 plus 1 1 0 comma 1 is a meaningless
thing it says that the leading expert is
zero the training was saying in order to
solve complete the request you need to
boundary this is where you still have
some sleep primary linux
okay so okay I just true okay so if you
get two constants c1 and c2 and one of
these constant has to be zero otherwise
the regret will go to infinity okay so
the solution get one of these console
has to be zero if not the regret will go
to infinity and because of that you can
eliminate that you use that to eliminate
that and the other constant can be
derived using the boundary condition
which is solution of the recurrences
larger one that's right great okay cool
thanks okay so this is the probability
with which you play the leading expert
in the probability you play the trailing
expert so what happens for k equal to 3
well you have eight actions so you write
the max over all these things now what
you do right for k equals to 2 you had
the knowledge of what the optimal
adversary is and therefore you said that
I'll go and equate the regret you got
for these two actions for k equal to
three donuts much more complicated so we
wrote programs to see that the optimal
adversary was always doing either one or
two three or 13 or two ok so both were
optimal so you just advance one and
three together and then two separately
with probability half or one with
probability half and 200 separately
probability half both were optimal okay
most of the time that was what the
optimal adversity was doing therefore we
just want to do a guess and verify
strategy here right you know that this
is optimal you guess it is optimal so
you just equate the regret you get out
of these four guys and this is purely
out of simulations this is where the
optic adversity is helping you do this
and after equating this and writing the
all the boundary conditions what happens
when x 1 equal to x 2 and so on and then
getting pain for four or five hours
finally you verify that the optimal the
strategy you get by equating these two
guys indeed satisfies the max equation
and therefore what you got to the guess
you made was really correct and that
also will give you the
algorithm out you can solve for the
probabilities well cool yeah so for you
know therefore for at least 43 this
optimal for for it is starting to get
delta dependent the optimal adversary's
so it becomes complicated so 43 the
optical adversity is not delta dependent
for for slightly delta dependent but
asymptotically it is not delta dependent
it has delta goes to 0 it is the comb
but verse 3 13 or two for it no no for
for for HC you mean it seems to be
asymptotically optimal right well at
this point i do not have any doubts at
all that it is it is the asymptotically
optimal just question of pooing so can
you prove the list of the dependence
perform with by simulations basically I
mean yeah
okay this is what we get for K equals 3
so what's the optimal adversary there
are two optimal adversaries like I said
whether it is 1 or 2 3 or 1 3 &amp;amp; 2 okay
so but this is what we call the comb but
versity right because it is one and
three and two in between right that is
what generalizes easily so was the
optimal i will say for gentle k this is
the conjecture right if you prove this
it'll be cool basically the optimal
adversity we conjecture as Delta goes to
zero is simply this you are advanced all
there are experts with probability half
all the even experts with probability
half ok so this generalizes the optimal
adversities for K equals 2 and 3 and
this is again supported by computer
simulations even 45 an optimal or fix
arising as he posed yes mmhmm yeah yeah
basic yeah I mean a date you know why of
oil you get very close to t the
adversary will start behaving crazy but
for most of the time steps it will be
this right because that is where we got
all this insight from we wrote all the
programs for the finite horizon in for
the finite worries and we could see the
comb but versuri for k equal to three
for instance so does it for any pattern
for a large key does it was too late
young years what goes trimmed of the way
in it seems to be this your colon it
seems to be the comb adversity hear this
so so if your conjecture is right how
how quickly does more forget of weights
become up those k goes off to infinity
yeah we don't know how to calculate the
regret rate you have to calculate the
regret for this guy that itself is a
challenge so i'll say how to do it for k
equal to 4 13 and 24 and then you'll see
why it's a fight way it should increase
quay by one you can square the amount of
work used to do this one is not skill
yep it is a good point mean if you can
just compute the optimal regret or the
regret for this particular strategy that
itself will be something okay so what is
the connection between finite and
geometric horizons right you have all
these instance photometry karai's and
can you put them back to the finite
model so how r RT and our delta related
so the obvious point is that the regret
you get in the Delta model R Delta is
smaller than this infinite series right
because if I tell you in the Delta model
that after drawing the time this is the
optimal T then you can do something but
in the Delta model you do not know it
therefore you can't get any better than
what you can perform if I tell you the T
so one direction of obvious but the
conjecture is that as Delta goes to zero
it's basically the same okay the
adversary does not benefit out of
getting informed about the key withdrawn
from this geometric distribution okay
and again for k equal to 2 this is true
not just asymptotically but for all
Delta because for k equal to 2 the
optimal adversary was didn't care about
the number of remaining time steps it
was just independent of T and Delta and
everything therefore for k equal to 2
this delta T theorem is not just a
connection it's a theorem that overall
Delta but even for larger k we believe
that this is true and again this is
supported by simulations the more
information you're giving me because the
distribution is mostly about so even
though it's Osama okay this thing Ted
these two are as Delta goes to 0 that
you should converge but this does happen
actually any more information but it
doesn't matter because it doesn't matter
more because the what's up
yeah which exactly serve with you know
the Sun would swallow the earth is it
six billion years or so what if this
were true right if this were true then
we know how to tackle the geometric
horizon model and if you have our delta
which is computable geometries and model
you can just immediately read off RT
from this by multiplying square root of
50 to this again it brings true so I now
go to regret lower bounds right
basically going to compute what the comb
adversity does for k equal to 4 right
the 1324 strategy which i wrote there so
recall that this is the conjecture
strategy for four so just get a feel for
what happens right if you have four
experts if you advance to and forth
together this is what happens if you
advance once more they coincide if you
advanced own for once more to gets ahead
of one and four gets ahead of three
which is meaningless and therefore you
have to swap right before you get to
this configuration right now if you
think about this mr. it is for a few
minutes you'll immediately realize that
the distance between one and three will
be the same as the distance between two
and four okay in this advancing process
therefore to keep track of the four
experts it is not necessary to keep
track of all the four you can just drop
the fourth one so we're going to keep
track of the first second and third
because that will tell you where the
fourth expert also is and now because
this process is translation invariant i
am going to map it to another process
which will maintain the relative
distances between the two experts so
advancing one and three and two and four
doing this strategy is equal to the
following random walk of a particle
between a reflecting wall which is fixed
and doesn't move and another wall which
moves okay so let's say this is the fat
fixed wall which reflects and this is
the wall which moves so this ball moves
the right or left is probably half if it
moves to the right here it sits at where
the wall is if it tries to move one more
step to the right it will get bounced
back right it moves to the
most of the left keeps moving to the
left it will coincide but if it tries to
move one more step to the left e to stay
at the same point but the wall will move
okay so this side is the fixed wall and
that side is a mobile wall okay and this
you can think about it a few minutes
maps to the 1324 adversary right adverts
32 moving is like this ball moving one
step forward at expert two and four
moving is like the ball moving one step
forward experts one and three moving is
like the ball moving one step backwards
right now what do you want to compute
for this random walk well you want to
compute the max minus average and this
max minus average is a quantity which
does not change it increases in
expectation by half exactly when the
wall one this fixed wall and that
particle coincides okay only when there
is a swap something happens okay so what
you need to do is compute the expected
number of visits of this particle I am
saying particle to is not that there are
two particles but just to say apps to 2
p 2 2 w1 right this is what you need to
compute and again this is translation
wayne i am going to fix the wall one at
0 let us say w 3 is a negative L right
so what you need to compute is
separation of the two walls going from l
to infinity the sum of the expected
number of visits of p2 to w1 when w3 is
at minus L right is sum over all these
things you get the total expected number
of visits so v12 of l is basically the
expected number of visits when the wall
to is at minus n right so your wall
three there only two walls but NSA wall
three I mean the third one right so when
that is at minus L wats the number of
visits and that is product of three
quantities I am going to use i'm going
to set theta square root q delta delta
goes to 0 this I won't derive but I will
just explain what it is so q1 of l is
first of all the wall three has to reach
minus l if at all you want any visits
when w3 is at minus l so what is the
probability that it reaches minus l and
then there is a question of the particle
to basically after pushing the wall is
very close to the
all the we stopped the whole exercise
that's why w2 might never Douglas we
might never reach might never reach
minus it the second thing is you are
talking about the number of visits of p2
to w1 for that first of all the particle
P to should first preach 0 that's where
w1 is right should reach before reaching
minus L minus one right before pushing
the wall is that should happen now after
that there is a question of the number
of visits of p220 before the wall moves
one step further right so I'm going to
go ahead of all these things but this
probably is familiar right when p2 is at
minus l plus 1 the wall is at zero New
York minus l plus 1 this is the particle
and there is a mobile wall w 3 at minus
L right what is the probability that you
reach zero before you reach this guy
right so if there are no stopping it's
basically two or l plus 1 with stopping
its inch to or since help us one if
there is no stopping is two or l plus 1
okay so i am not going to delay these
other guys but you have all these things
so if you multiply all these things you
get 1 over cosh gosh cube del theta and
what you need to do is some this guy
from l equals 0 to infinity and this is
same as the integral if you integrate
this get the beautiful pie or four
outside so they a constant in front of 1
over square root 2 delta goes as k equal
to 2 it is half k equal to 3 it's two
thirds for k equal to four it is PI over
4 okay then could your computer and pray
that this is basically matches what your
simulation says you get RT out of
simulations you get our delta out of
this right you divide this you get
exactly square root pi or two which is
what you want right so this basically
tells me that this is the
the dress is only question of proving I
can simulate RT for small t reckon it's
tara t equals three even four oh sorry t
I mean 40 that depends on them k right
for k equal to three i can go till I 300
or 400 for k equal to 4 even lesser so
sorry when j is equal to 4 what t can we
do about 100 to 120 and that already
says this basically we we got this
constant pi over 4 before we derived
this I mean based on simulations and we
had one or two attempts before getting
this pi over 4 we didn't know what this
number was point seven eight we cut pi
over 4 is exactly that didn't have
enough precision to plug into the eggs
yeah so the file before people should
just recognize yeah meet you yeah so
this is the evidence for optimality of
the comp adversity we just go prove it
for general k that's the open question
from the stock thanks for listening the
other question so I'm just curious about
this computational complexity of this
sword I'm wondering is lecture that we
settle for something less than towards
of tonality or whatever okay so from the
discussion but the overall intuition i
get is like since you know that the
adversity is for in the optimal
adversity is pulling some kind of tennis
distribution so why can't I mean a
simple strategy would be so far k
greater than three or four so in that
case you could do like choose first so
you had at any time step you know you
know the order like which one is
reading second third and do you know
this already so juice first so take a
knee length so you are taking the arms
right right I said it you choose first 4
7 2nd 5th 8th and third 69 so I just go
in this holder I can do that right
hiding form is three subsets of subsets
of these arms so you are let's say that
you had but these strategy is suggested
as the conjecture is even simpler right
you just advance money in on a level
four star general I think for the player
or were they for the adverse ooh I am
perfectly clear I'm simple player okay
for the player what are you suggesting
as the release Lucifer at a given gains
configuration what will you do choose
one for 7-10 and so on you have the
solution for letting fee for what does
it mean to choose all of 14 7 and 10
equally likely or you want to advance
equally regular so that just was
advancing that is what the bursary does
when you have to pick right maybe this
question is best disgusting and offline
okay yeah yeah yeah probably of this was
fun so pure value is certainly available
for questions and pectin game</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>