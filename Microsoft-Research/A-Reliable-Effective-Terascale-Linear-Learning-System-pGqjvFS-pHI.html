<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Reliable Effective Terascale Linear Learning System | Coder Coacher - Coaching Coders</title><meta content="A Reliable Effective Terascale Linear Learning System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Reliable Effective Terascale Linear Learning System</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pGqjvFS-pHI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay well it's my pleasure to introduce
my pleasure to increase jon langford
newly of microsoft research in our
delightful new york city branch oh gosh
John has been around machine learning
for years doing things like I so map and
n CAPTCHAs and also doing fun sort of
learning theory things like the workshop
on error bounds less than a half i
remembered that was very fun you got his
PhD about ten years ago at CMU and it
was a tech back before that so here is
okay so this is not my normal style of
talk because i use do theory but last
summer we got upset that our learning
adams weren't fast enough we just said
to crank on it really hard so i'm going
to tell you how we got linea learning
working at very large scales and very
quickly so this is this is true story
when i was at Caltech just finishing up
i applied for a fellowship i think was
the heinz fellowship and they send
somebody out to interview me and and the
interviewer said what do you want to do
and i said hmm well i'd like to solve a
I and then the interviewer said well how
do you want to do that and I said well
I'll use parallel learning algorithms
and and he said no it was more bracing
than no yeah I think so okay so so I
obviously did not get the fellowship
even worse he was right to some point to
some extent the it is the case that in a
race between creating a better learning
algorithm and and just trying to make a
parallel learning algorithm often the
bed learning algorithm wins and it was
always winning at that point in time
so I'm going to show you a baseline
learn nothing I remember where we
started at this is a document
classification data set which is 424
megabytes compressed and if we look at
the dataset in detail we see something
very typical we have a class label and
we have a bunch of features which are
tf-idf transformed feature values and
then on the next line we see another
example and so forth right so this is
very typical and now I have a
six-year-old laptop with an encrypted
hard drive how long does it take to
actually learn a good classifier on this
data set we have two classes being taken
to be nice I my desktop at home which is
not six years old it takes about two
seconds on this machine maybe 40 seconds
or so so what's happening here is this
is progressive validation loss so we're
running through the examples we evaluate
on the example we add that into the
running average and then and then we do
an update right and I guess the
important thing about this is that the
deviations are like a test set we're
just doing one pass over the data and
turns out that for this data set if you
have a tf-idf transform when you do and
you have a very tricked-out update a
linear update her online update rule
which we have you can get an average
squared loss of four percent or point oh
four and then the it turns out on a test
set it's about five and an eight percent
error rate which is about the best
possible in delays done on this data set
as far as I know
okay so this is the baseline this is
where we're starting from I was only 31
seconds a very good okay so we're just
learning a linear predictor and document
classification is particularly easy
because words are very strong indicators
of document class there are roughly 60
million features that we went through
780,000 examples and took about the 31
seconds this time
ok so that's that's nice it's a but we
have bigger data sets so I'm particular
at Yahoo we were playing around with an
ad data set which had to Terra features
so I should go back a little bit let's
go back here this is one feature
everything that's not there is doesn't
count as a feature we're only counting
the nonzero entries in the day of the
matrix this is a sparse representation
so when I say to taro features meaning I
mean to Terra sparks features so to
Terra nonzero features right we still
want to learn a linear predictor it will
have a lot more parameters because you
need to have that and here's a few more
details there's 17 billion examples
there's a little of a hundred nonzero
features / example there's 60 million
parameters that we're learning and we
have a thousand node cluster to learn on
and the question is how long does it
take well how do we do it first of all
but more importantly just how long does
it take to do it in terabyte of features
no I'm going to tear our feature there
the 12 nonzero entries in your data
matrix you see 16 probably first
so those are the weights of your eyes
what's that yeah love you guys
so that there are that many teachers
that are nonzero ever or you mean that
you know always summed over all the
cases there's this many nonzero entries
in your data matrix so the number of
bytes if you if you did it use the bite
representation would be maybe 20
terabytes perhaps if 10 if it was 10
bytes per feature no no no the number
does that was me Dad essentially your
total your total matrix is 72 pin please
17 million rows and 16 million is
columns and the number of nonzero
entries is Tudor you have to worry about
over 10,000 times more examples
yeah so overfitting becomes less of a
concern but it actually is some concern
the issue is the sparsity so some of the
features just don't come up very often
and so it is possible to overfit and
those features even if you can't over
fit on the majority of the mass uh yeah
but did but let's just go back to the
demonstration right so I'm using zero
regulation when I do this and turns out
that if you have a very good online
update rule you don't need to worry
about regulation that much it's kind of
built into the online process itself
just roll so so the question is how long
does it take 30-40 seconds that was
fantastic oh I've heard of those extra
zeros really matter yeah well you guys
would be nice if I do that hit 70
minutes
okay so now the important thing here is
that if you just look at the overall
throughput of the system we have to do
multiple passes on this data by the way
because you know it's it's much for our
than the document classification Davis
said so if you take look at the overall
throughput this ends up being 500 mega
features per second each individual
machine in that cluster just had a
gigabit per second to Ethernet now I
don't know exactly how many bytes it
takes to specify a feature but or bits
but it's certainly more than two so we
are reading the aisle bandwidth of any
single machine in our network
starting from st absolutely asking no
there's a lot of tricks that we throw in
right but we do start from a ski and
because it's kind of human readable and
the buckle but the VW has a nice little
cash for Matt and I actually kind of
lied here because I was really using the
cast form of the data set which is of
course unreadable oops so that's 290
megabytes in binary okay so this is this
is the only example I know above a
learning algorithm which is sort of
prob'ly faster than any single machine
learning item that's ever invented in
the future because we beat the i/o
bandwidth right meet the i/o bandwidth
limit that's a finally an answer to this
guy that turned me down all right so we
had a book with Misha where we were or
people contributed many chapters on
parallel learning methods and for each
of these parallel learning methods you
could take a look at sort of the
baseline they compared with in this
features per second type measure and
then how fast their system was so this
was some people at Google they had a
radial radial basis function support
vector machine they're running on 500
nodes using the RCB one dataset those
same one that I just showed you and they
they had some speed up from a relatively
small baseline this is an ensemble tree
this is NEC guys they had a much
stronger baseline support vector machine
and they sped it up more than you might
expect they only had 48 nodes this is it
more than a factor of 48 this is a
logarithmic scale here and there they're
winning from some nice catching effects
this is MapReduce decision trees from
google they have some reasonable
decision tree baseline and then
just speed it up and pretty
straightforward way this is from
Microsoft this is krista and chris's
booster decision trees so they were
working with relatively small number of
nodes only 32 they had a very strong
base line and then I sped it up a little
bit more and then this is what I just
demonstrated to you and this is what I'm
talking about now so the baseline got
worse because the data set is raw and we
have to pass over the data multiple
times all right so we know what can
Oliver do a single pass we get a much
bigger speed up because we're dealing
with a thousand machines effectively
okay so how do we do this well yeah we
do things in binary we used take
advantage of the hashing trick who knows
the hashing trick here okay so that idea
what the hashing trick is that if you're
dealing with quickly if you're dealing
with these mini features type data sets
you can often just hash the individual
features to map them into some
particular fixed dimensionality weight
vector space right so when I say sixteen
million i don't mean there are 16
million unique features what I mean is
that i'm using the hashing trip to map
them to using a 24-bit hash right and so
what's scary here is that you get
collisions between different features
but turns out that if your features are
a little bit redundant that works out
pretty well no machine learning out of
them can learn to compress around
collisions we're using online learning
using some implicit features we're doing
a bunch of tricks to improve the online
learning we're using el bfgs to finish
up with the batch learning and we switch
between these two and then there's a
bunch of tricks and how we're doing the
pedal learning as well all of which are
aimed at either making the communication
faster or or making the learning a
little bit more intelligent in a
parallel setting okay so the key thing
that we did the most key thing is this
Hadoop I'll reduce which I'll go through
first okay so all reduces an operation
from MPI who knows all reduce okay just
a couple so the idea is you start with a
bunch of nodes each note has a number
and then you call all reduce and then
every node has the sum of all numbers
right
and then all the question is what does
the algorithm for doing that and one
such algorithm is you create a binary
tree over the nodes and then you add
things up going up the tree so one plus
two plus five is eight and ooh three
plus four plus six is thirteen then
eight plus 13 plus 7 is 28 and then you
broadcast back down the tree okay so
that's that's all reduce and this is
this is a very nice primitive in several
ways in a network in general you have
two things you care about one of them is
latency the other one is bandwidth
typically when in machine learning when
you calling all reduce you don't want to
call it them just one number you want to
call it on a vector of numbers and so
you can pipeline the all reduce
operation and then latency doesn't
matter right the other thing then
there's also a bandwidth and this
particular implementation of all reduce
is within a constant factor of the
minimum so for the internal node you
have 12 coming in one going out one
coming back into going out so that's six
total six times the number of bytes
you're doing all reduce on so that's
within a constant factor the best
possible and so we're decent as far as
is using a bandwidth okay and then the
last point is is actually the most
important one the code for doing this is
exactly the same as the code that i just
used here it's exactly i mean the i
wrote no new functions except for the
all reduce function itself and and i can
just what I and just look at my
sequential code and go ok I'll do it
already is there and there and then it
runs in parallel ok so this is one a few
times in my life when having the right
language primitive seemed like it was
phenomenal this comes after I spent
several years doing parallel programming
the hard way
it's very easy to experiment with
paralyzing any learning algorithm which
you want using all reduce okay so here's
an example if we're doing online
learning on each individual node what we
could do is we could just pass through a
bunch of examples doing our little
online updates I mean we finish with our
paths and then we call reduce on the
weights then we average our weights and
then we do another pass right and so we
just we just add this if we're doing
things the sequential environment there
be no all reduce and then we wonder
things in parallel so we just add in the
right our disk all okay so we did this
for the other items that we were using
this is simple stochastic gradient style
algorithm there's also well it gets a
little bit more complex we want to have
more intelligent online update rules
which keep track of some notion of how
much update has been done to an
individual feature and then you want to
do a non-uniform average of the
different weights because that makes
sense the weights which have seen this
feature more often and updated more for
that feature should have a stronger wait
in the overall average right so it turns
out you can do non-uniform averaging
easily enough you need to use two
already skulls and you can do conjugate
gradients and you can do L bfgs okay so
so that's all reduce that that's what
MPI gives you we're not actually using
MPI and the reason why is basically MPI
was not made with data in mind so
typically if you try to use MPI you'd
have this first step which was load all
the data which it would be pretty
painful and anyways the the data yahoo
existed on Hadoop clusters so you didn't
want to shift the data off the clusters
because the data was was very heavy so
what Hadoop would get give you is the
ability to move a program to the data
right and rather than moving the data
that's it's very nice as far as research
usage and then another thing that Hadoop
would give you it is automated restart
and failure so with mpi if one of your
nodes goes down typically the job fails
right and it's actually still true for
us with all reduce but it's not as true
and in particular it's not true for the
first pass of the data because we delay
initialization of the communication
infrastructure until after every node
has passed over its piece of data and
that means that the most common failure
mode of disk failure will get triggered
in the first pass that job will get
killed and then it will be restarted on
the same piece of data on a different
note and then respect to the overall
computation across the cluster all it
all it happens is things are slowed down
a little bit in the first pass all right
so this this deals with with the
robustness issues to a large extent in
practice my experience was that as long
as your job was less than 10,000 note
hours or so you didn't see failures
unless they were your own bug what's
that hours before this reckless
it was after this but I can't entirely
ascribe that to this there's another
effect which is some sort of convention
and unix land that the ports above to to
the 15 can be randomly used and we were
using those on purpose but the relations
going on anyways that change also
happened and then all her problems went
away yeah it was already than be
partitioned customs and then you have
multiple copies of each yeah that's
right so Hadoop has a file system called
HDFS which just stores everything
partitioned across the nodes in
triplicate yeah so 2.1 tariff features
yeah
this life two gigabytes Kurt now the
more like 20 gigabytes perhaps but yeah
the feature is not a bite feature is
more than a bite typically
so he right so it's not terribly much on
each individual machine that's good I'm
just questioning is mpi really really
horrible because it seems like if you
are delaying initialization on the do
think you can do the same
OMG i is basically not an option because
we didn't have the cluster and we didn't
have a separate cluster drawn MPI on and
NPI kind of I mean it doesn't have a
resource scheduling component like
Hadoop does so a big company that wants
to make sure that it's cluster is
getting utilized isn't going to want to
run NPI because they want to share
machines as much as possible in there
were various efforts at Yahoo to run MPI
on Hadoop it was never pretty okay so
there's one last trick which is very
important when you have a thousand notes
I'll reduce is a barrier operation that
means you run as fast as the slowest
node right so if you have a thousand
nodes one of them's going to be kind of
slow in particular it's common for the
slowest known to be a factor of ten
slower than that's just knowed maybe
even factor of 30 so Hadoop has a magnum
mechanism called specula texa cution
which says that if if one of your jobs
is running slow you restart the same job
in the same date on a different note and
then these two processes race so we take
advantage of that and that allows it's a
factor of 10 improvement in the overall
speed of the system right yeah
like
you have exact copies in different nodes
and you know the second one is starting
later than the first one because the
first one has been noticed to be slow so
we're going to lose a factor of two or
three which is actually what we observe
compared to a single machine performance
but we don't lose a factor of 30 and
that's that's a that's a big win and
some way know typically what happens is
you know data the data is on individual
nodes the resource scheduler tries to
figure out which nodes to to move the
computation to occasion that moves both
the data in the computation but mostly
just move the computation and often it
try it tries to over subscribe
individual nodes for performance reasons
overall throughput of the system and it
fails right it fails badly on something
particular note or maybe maybe some node
happens to have more than one piece of
the data right and in which case it gets
overloaded right so these kinds of
things come up very commonly in a large
cluster yeah straighten exclusive you
summary don't know wait we yeah you know
they gave us access to the production
clusters the actual production question
but the one the development clusters the
ones what were used right and they give
us access now they let us use a thousand
nodes but a even two thousand but but
exclusive access was too much they asked
for
alright so so this is a reliable
execution at about 10,000 note hours we
only went to about a thousand no towers
and so maybe they're larger scale
problems here that oh well one thing we
checked by the way there's a lot of
cheap tricks when you're doing large
scale learning you could for example
subsample the data and you know run on
the subtitle data turned out that for
this data set you actually did do worse
we subsampled and noticeably worse in
fact this is already a subsample data
set this is throwing out 99% of the non
clicks but we couldn't throw out any
more without losing performance
we can be in faster to use the sub
subsample tina's at the tassimo as
possible the things you a little tricky
is that the communication time is
non-trivial here so if u sub st. Paul
the data set and then you ran on a
thousand no to the subsample data set
the communication time would still be
eating into your total time budget quite
a bit you just spray sees a perimeter
center base yes
so that's like a second or something I'm
gigabit per second either network it was
in fact maybe about 10 seconds in
practice so because there's collisions
and whatnot also we have that factor of
six okay so when we're doing hmm no
wrong way okay so so that's that's the
communication infrastructure having a
decent communication infrastructure is
necessary but not sufficient for good
machine learning algorithm this is
radically better than MapReduce because
at least for Hadoop the iteration time
for MapReduce is about a minute and here
is about ten times ping so you can okay
so that mean just in terms of
performance it's much much better for
for synchronizing different nodes is
also much much easier to program with
all reduce which i think is probably an
even more thing and important thing in
practice yeah yeah the code is your own
already yes and we covered it as a
library so it is a library in VW which
is an open source project that we ran so
it's very easy for you to just use that
library it's one file that you just grab
and stick into your it compiles as a
library in fact so it should be very
easy to use if you wanted to cuz it
issues like somebody
they're cooperating with everybody else
broadcasting
I mean the failure melons have all
reducer justice to the same except that
the interface is set up to encourage
delayed initialization because when you
call it you call it with all the things
that require to initialize right it
works with specs of execution so it does
if two nodes say hey I'm node 3 in
computation for then it just takes the
first one and uses that and it ignores
the other one there in order to make
this all work you need a way to for the
notes to organize themselves into the
tree so the way that works is there's a
little Damon that runs on the Gateway
and then for every map job we point it
to the Gateway it so the job talks to
the daemon and says hey I'm job three
and and I'm three or four with some
particular knots which you need to have
distinct for each different allergists
invocation and then once it collects a
complete set of 10 for 20 for three or
four or four or four then it it tries to
be intelligent about creating the tree
so it sorts the IP addresses and then it
builds the tree on the sorted list of IP
addresses so that means in particular
that if you have the same job at the
same IP address then that tends to be
communication just inside of a computer
right it's not guaranteed it's not
perfect but but it tries to minimize the
interactive and with usage
for heavy always does it once the
initialization and then it just keeps
the soccer is open for overall usage so
once the communication infrastructure is
created if a node dies then the overall
process does but again that wasn't a
problem out to about 10,000 odd hours
okay so so now let's talk about the
algorithms are trying to do gradient
descent so it's brinda centas is the
core of all of these algorithms but
Graydon descent is not quite adequate
and the way I think about it not being
is inadequacy is to think about units so
it's kind of a physics point of view so
if you have squared loss you take the
derivative squared loss then looks like
this and the important thing is this
like a constant here and then we have
the feature and if you look at the
prediction this is the feature times the
weight so that means that if the feature
is double the weights need to have in
order to keep the same prediction right
and this is a feature so we're adding a
learning rate times a feature to
something which is inverse in the
features unit wise and that that unit
clash causes issues they try to make
thanks they try to divide but over bags
yes in fact that's one of the tricks
that's an old Elgar yeah it's a simple
thing all right so wa natural units of 1
over I and X I has units of I so things
don't work very well so you I base it
which is too far in particular direction
okay so we're doing adaptive safe online
gradient descent so online you aware up
you just update after seeing each
individual example adaptive is where the
update in direction I is rescaled
according to 1 over the square root of
the sum of the gradient squared for that
feature so you have essentially a per
feature learning rate where the learning
rate gets scaled down as you do more and
more updates to that individual feature
yeah
another for second order
second order Argos yeah so in terms of
performance my pression is this is at
least as good in terms of the actual
update rule it does differ in some
details so we're looking at the
gradients here so it could be there's
two reasons whether some of the
gradients is small one reason is because
you've never seen that feature before
which is helpful and I think the second
order granted sent reporters good at
that pretty well but a second reason why
the sum of the gradients might be small
is because every time that feature came
up you predicted correctly in that case
the update rules tend to differ in what
they do CW confidence waited yeah its
weak
books once a month
the real truth is I haven't tested
empirically dick compared the tube I
would like to do that but I haven't had
time anyway this is what we're using
that that's helpful the other thing is
the trick that you mentioned so
rescaling the features to where the
units work out overall that's it helpful
and then the last one is the safe update
so this is this is when Niko's visited
me we're worrying about doing only
learning with large importance weights
and the obvious way to deal with an
importance weight you want you want to
if you have an importance weight of two
in our example it's like saying this
examples twice as important as normal
and the obvious thing to do would be to
just put a two here right but that works
pretty badly because essentially because
you're trying to have a very aggressive
learning rate with online learning and
it makes it too aggressive so instead
what you can do is you can say all right
a example with importance weight too
should be like having two examples of
importance weight one in a row and you
can say well maybe this should also be
like having four examples with
importance weight a half in a row or
maybe it's like eight with imports wait
one quarter in a row and so forth and
you can take the limit of infinitely
mainly infinitely small updates you can
solve in closed form to figure out what
that update rule would be so the
important thing about this update rule
for our purposes is okay so this is it
in fact help you with with importance
weights we don't have any importance
weights here terms of that it also helps
with just importance weight always one
because it's not equivalent to just the
normal update rule it takes into account
the global structure of the loss
function it doesn't have to be squared
las resolved it for many other losses
logistic as well in fact they they the
starting example is actually logistic
loss so it's safe because if you have
these infinitely small infinitely many
Utley small updates the update can never
pass the label for any love of the same
loss functions and so
hinge loss four squared loss for
logistic loss the update never passes
the label so you never have a large
importance weight throw you out into
crazy land and that allows you to have a
much more aggressive learning rate than
you could otherwise have even with just
importance weight one you're not in that
shooting that locally late for each for
each guy is they're stepping through and
get them know it how do you enforce that
the can't you enforce that entire levels
welcome taking money so we're just using
cynical alia to make sure that combining
makes sense all right so um yeah okay so
we do online learning on each individual
node we use all reduced to average the
weights then we switch to lb-ft s so L
bfgs is not rhythm that did not learn
about as a grad student and I feel like
my education was missing something it
was just being formulated by bureaucrats
no no I think actually L bfgs was making
80 lbs yes sir yes yes
l bfgs was 70 lb of Jason was 82 easy to
remember that was so L bfgs is a batch
algorithm batch I rhythms suffer a lot
in terms of speed but turned out that
it's very good at at getting that last
bit of optimization done well it uses so
what you would want to do if you could
afford it would be something like a
Newton update bill it involves inverting
a hessian that you can't even represent
a hessian on this many parameters so
instead what you do is you approximate
the inverse session directly this is the
core approximation so you have the
change in the weights from one pass to
the next add a product with a change in
the weights any other changing the
weights inner product with the change in
the gradient in this if you think about
the units for a moment actually has the
right units in turns out to be a pretty
decent approximation to the universe
session then you build this up over
multiple rounds and you get a better and
better approximation to than recession
and then you can get a verge very
quickly to to a good solution okay so
now we use the average of this output to
initialize this which is a cheap trick
that turned out to be extremely helpful
ok so now use a map on the Hadoop job
for process control we use all risks
code to synchronize the state we save
input examples into a cache file for
fair later passes and we use a hashing
trick to introduce the input complexity
these are all very important things
individually but they're not related to
to the actual para lips and parallelism
and then it's open source in bhopal
Abbott this is the online learning
software that we've been putting
together now I guess online and batch
learning now
okay we did a few studies of this system
so we we had a smaller data set and we
were varied between ten and a hundred
nodes and then for each number of nodes
we ran ten times and then you can look
at so if things were perfectly linear
and their speed up then it would look
like this line we're not perfectly
linear in our speed up things you go a
little bit slower as you go towards more
and more nodes but you do get
significant speed ups so this is between
Tim's and 100 this takes a factor of 10
longer to compute or almost a factor
take longer to compute than this does
you can also look at the variation
between the men and men in the max
running time and you can see there's
some variation you have to expect that
on a cluster you don't really control
but it's not too bad okay so now we can
look at how this the sacrament works
there's two really obvious things to
compare with one of them is what if we
take our online learning algorithm and
we just do repeated averaging there was
a paper people published about this kind
of approach and you can see that things
do indeed get better over time as you do
repeated averaging of online learning
and it's still getting better out here
at 50 passes then another approach you
can do is you can just say okay I'm just
going to run l bfgs and it does nothing
for 10 passes and then then starts to
get better and better and better and
better and keeps getting better and then
you can do online learning for one pass
and you can switch to lv f GS and then
you're done at about 20 passes this is a
splice site recognition data set this is
this is one of the largest publicly
available linear prediction is
reasonable that data sets that I know of
it's probably the largest that I know of
you can also do online for five passes
and then switch over to El bfgs and you
see that you know maybe it tops out a
little bit later
so in our experiments that seemed like
one online pass was pretty good at
putting you near enough to the Optima
that the wfg s would would really nail
things pretty quickly we compared with
various data sets there with other
algorithms so there's a couple other
algorithms one of them I think I think
maybe Marty gave a talk about here
before the other one Lynn and ophir
worked on and and Ron and who's the
fourth author what's that oh yeah it's
right ok so Marty's algorithm operates
by you see an over complete partition so
a single example appears on one quarter
of the notes and then you learn
independently on each of these nodes and
the average things together at the end
right and this is essentially measuring
this is affected number of passes this
is measuring that the degree of over
completion of the partition right so as
five an example appears in five nodes
and then then there's the the mini batch
approach and here we're just measuring
the number of passes through the data
because we keep passing through the data
multiple times doing the mini batches
and yeah well L bfgs really helps you
kick that last bit of performance and
that's a that's a pretty big win
accommodation oh yeah by far so the
actual lbf Jess itself was like a second
or less and you read that window
two runs and all notes I think it's all
it's all reduce yeah so we we look at
sort of the communication computation
breakdown for for the system and it's
about 10 seconds to do the
synchronization it's about a second or
less to do the L bfgs itself the the
solenoid problem is still there so it's
about about half of the community of the
time is wasted in a barrier and about
half is for confident for computing the
gradient
just look in the paper I think the
default in VW is 15 but we often used
five or ten yeah
this is by the way I mean this graph
kind of understates the difference in
the performance between these algorithms
because if you look at computational
time this would be substantially worse
this willing to be much worse because
the communication complexity is higher
this one would be significantly worse
okay so so I'm not done we're creating
we decided to make a new machine
learning mailing list machine dash
learning because we had that at Yahoo
and I found it useful for interacting
with product groups of various sorts so
I think maybe people will be
automatically subscribed and you can
unsubscribe yourself but but if you are
talking to product group people they're
doing machine learning it'd be cool to
point this out to them BW you can just
search for there's a mailing list if
this external if there's demand enough
we can create an internal one I'm trying
to work with media to create a windows
version hopefully this week and in this
tutorial at nips which is off the wiki
so this is my first talk here on a
mission a few other things so there is
CAPTCHAs and ice and map that you
mentioned I've also worked on learning
reductions which is how to decompose
complex prediction problems into simple
prediction problems so the solutions of
simple problems gives you the solution
to the complex problems we're
implementing these in VW right now some
of these are also logarithmic time
reductions so if you're trying to do
multi-class classification most of the
common approaches take order k time
where you have k classes and so that you
can actually do log k effectively in
many situations we look at active
learning so this is the selective
sampling version of active learning and
I guess the the thing that we figured
out here is how to deal with noise so we
can deal with the same kind of noise
that you typically analyzed in
supervised learning settings with active
learning right and we have algorithms
now there are
very efficient and effective and then
there's a lot of work on contextual
bandit settings where I guess the the
motivating example at Yahoo was ads but
anytime a user comes to a website and
the website decides something to show
the user and then the user reacts to
that you can try to use a machine
learning to predict what the website
should be presenting to the user and
there's a bunch of issues related to
bias and exploration exploitation around
that that we've resolved to a large
extent right thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>