<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 3 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 3</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SyadRJf6j2o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright so this work is in collaboration
with song coolio lecithin and Chris Rea
at Stanford University and I'm Christmas
ah so let's say you wanted to do
inference on a factor graph often you'd
use Gibbs sampling because it's observed
to work well in many practical cases but
to use Gibbs sampling effectively you
need to know it's mixing time that is
how long you need to run it in order to
produce a good result unfortunately it's
difficult to decide what the mixing time
of a model will be because of a lack of
theory no one really knows if Gibbs
sampling will be feasible for a
particular model what we came up with is
a new way to measure a factor graph that
gives us a theoretical understanding of
Gibbs sampling this measured the
hierarchy with characters is only the
structure of the underlying factor graph
and bounding the hierarchy width is
sufficient to ensure that Gibbs sampling
will mix in polynomial time this further
is the understanding of the problem by
giving us a new class of models where
Gibbs sampling is guaranteed to work so
the hierarchy with condition roughly
breaks down factor graphs into three
classes to gain intuition let's look at
an example of each one factor grass with
constant hierarchy would prove Lee mix
in at most quadratic time factor grass
with hierarchy with proportional to the
logarithm of the number of variables
provably mixed in polynomial time
finally some factor grass with unbounded
hierarchy with linearly proportional to
the number of variables are known to
take an exponential amount of time to
mix to see how hierarchy with interacts
with real problems we constructed three
models for the same knowledge base
population task one in each of these
three categories in this plot the green
series for the model with hierarchy with
proportional to the problem size has a
much larger estimation error than either
the red series which represents the
model with log proportional hierarchy
width or the blue series which
represents the model with higher
constant I key width and not only do the
low hierarchy with models mix rapidly
but they also can produce high-quality
results a model with log proportional
hierarchy width has recently been
applied to real
problems yielding competition winning
results to follow on these results we
also give you a way to generate models
that prove Lee mix in polynomial time we
introduce hierarchical templates a new
class of factor graph template that
always produces models that have bounded
hierarchy width this lets us easily
construct models that proved Lee mix in
polynomial time so to recap we proposed
a new factor graph parameter hierarchy
wit with which gives sampling can be
shown to mix rapidly so if you want to
learn more about how to construct the
examples that work come to poster 747
tonight thank you I'll be talking about
joint work with Tamra Broderick and
Michael Jordan the title of our paper is
linear response methods for accurate
covariance estimates from midfield
variational bay's mean-field variational
bay's provides approximate inference by
casting inference as an optimization
problem where we minimize the Quebec
lupo distance between the true posterior
and approximating factorized posterior
this method has proved attractive
because of its faster on time on large
data sets however it's well known that
even when it provides good approximate
posterior means it dramatically
underestimates posterior variances and
by construction since the posterior fact
Rises provides no co variance estimates
as a simple as example of this consider
estimating the locations of a mixture of
a two dimensional two dimensional
Gaussian a sample dataset is shown on
the on the left there and imagine that
we want to get the posterior of the
locations shown in red dots we simulated
a number of data sets on the rightmost
graph each simulated data set represents
one dot and on that data set we fit an
mcmc model a gibbs sampler which we
consider to be the ground truth and a
mean-field model that assumes that the
posterior factorize is the graph shows
the standard deviation is it and as you
could as you can see the mean field
model dramatically underestimates the
standard deviation of
location component linear response
methods provide a simple closed form
correction to the mean field covariance
matrix the formulas shown on the left in
the formula you have two terms you have
sigma vb that's the original mean field
underestimated covariance matrix and h
is the hessian of the expected and
unnormalized log likelihood each of
these terms are generally easy to
calculate and sparse the sparsity allows
you to in many cases solve this linear
system in time that's linear in the
number of data points even when the
number of latent variables in your model
scales with the number of data and as
you can see on the graph on the right
the green dots represent the linear
response of standard deviation estimates
for that component on the same model and
they match the mcmc results at a small
fraction of the time we've demonstrated
that linear response methods work on a
wide variety of models providing
accurate mcmc like co variance estimates
but in variation of Bayes time the
linear response methods generalized
classical methods from statistical
physics to exponential family
approximating distributions the original
posterior need not be exponential in
fact the key assumption and the only
assumption that you need is that the
mean field approximation produces good
estimates of the posterior means we
encourage you to come to our poster to
talk more about the theoretical
background to see many more example
models and to talk with us about your
modeling problems we're interested in
using this on more problems in the
future thank
I mean gene June i'm working with
natural guitar than chop cyclin we
propose a new method called leighton
baines melding to combine two different
models where the where goes of the
models are connected by a deterministic
function when one model is called in the
water model so for example we use a heat
map model to model our sequence data as
here so the animal is called population
model so that the idea is that we use a
national survey data to model some
summary statistics tall of the sequence
s one example is the time of the edge
which the state's the ste in the wilds
days of of the two states in the hmm so
this could be just the normal
distribution and at at the same time
tall can be is a function of the ass
here is the just some of the sequences
so then we have the distribution tall
and the PS and also the function f here
so our questions to how to combine Pete
oh and PS so we could employ the
posterior regularization and also we
could is an odd method it's called base
melding so the idea is that on the one
hand we have the distribution Pete Hall
on the other hand we have this vision PS
and the function f so this will induce a
new distribution for tall which is Pete
hostile then we combine these two
different dissolutions at all into one
and pro back to the in the water model
in the space s so but the base molding a
serious there's no little wearable
one instead is quite often there's a
little bourbon Cassie so like the
mixture mixture model and we still can
use the base molding but there's an
intractable integration here instead we
propose the little busy building and
arrive at Joint Distribution whereas and
Cassie and we sure that there's no
intact blowing the grecian and smc can
be inferred simultaneously if entreated
to come to my poster number 25 thank you
alright so I'm Phil Bachman uh this is
joint work with my advisor doing a pre
cup I'm from McGill and the papers
called data generation as a sequential
decision making so primarily this paper
is kind of work in two parts I guess you
could say the first section of the paper
is a collection of mathematical
analogies providing kind of a framework
that might help people view a variety of
techniques that have been introduced for
training deep generative models in kind
of a more unified and cleaner way and
then likening those two techniques that
have been developed recently also for
performing reinforcement learning so
this is based around the fact that deep
directed generative models are powerful
they represent structurally complicated
distributions by composing simpler ones
and we interpret directed models as
policies for constructing data so these
policies essentially act by sequentially
setting or configuring the models Laden
variables in a sequence of decisions
where you kind of condition the outcome
of each decision on previous decisions
that were made and training these kind
of sequential decision-making policies
is just the domain of reinforcement
learning so on the slide here I have a
couple pictures of different
distributions where you're sort of
sequentially refining a very vague
distribution into something finer
grained and more precise so the second
part of the paper is just kind of
showing these ideas in action and
developing a couple of models for
performing the task called imputation in
which you're trying to predict the value
of someone observed data and then kind
of applying these techniques for
training these directed generative
models by this kind of guided policy
search so what we do is we develop
stochastic recurrent neural networks
that construct predictions sequentially
via iterative refinement so the models
kind of make a prediction for what the
missing data is they compared to what
they're looking at then may refine that
this isn't that prediction about what
the missing data is and they do this
repeatedly so it's kind of a stochastic
feedback loop that we're training to
produce the correct outcome by
constructing prediction sequentially
like this the models can naturally
capture higher-order structure without
having that post hoc Emma Roth's Emma
reps or CRFs so rather than training the
model on its own and then sticking an
mrf or a CRF or something
the end and kind of fine-tuning that we
just trained the model jointly and and
to produce this kind of structure in the
output distribution so we train these
models using a form of guided policy
search or equivalently at least in this
particular context you could also
interpret this as stochastic gradient a
variational base so I guess if you want
to hear more you can come and talk to me
at the poster the sort of mathematical
analogies in the paper are a little bit
dense and require familiar you'd like
familiarity with the surrounding work
that they involve so I recommend that
hello everyone my name is susan john
franco institute nyu my collaborators
are Anna cora monica and a young akan
i'm presenting a new method called
elastic averaging SGD which distributes
the training of large neural nets on GPU
cluster the basic idea is illustrated by
this diagram we have P workers run in
parallel an essential parameter server
the workers wrong SGD with natural
momentum asynchronously their last
function has an l2 penalty it has
elastic term that attracts the parameter
of local worker and the central
parameter toward each other our local
workers see the full dataset in
different order in order to reduce the
communication overhead each worker
communicates with the central server
every town mini batch update the central
parameter is updated with the moving
average formula whenever receives a
parameter of raw worker we now
illustrate the basic elastic averaging
property our quadratic laws in two
dimensions with Gaussian noise on the
Left we plotted trajectory of the global
average Prime at X in red and the local
parameters in blue we see that the
global average parameter has a smaller
variance near the optimum 0 on the right
we combine nesterov momentum method with
elastic averaging SGD we observe that
the central path in red is even less
observatory send the local parameters
moreover the whole system is more stable
due to the elastic averaging property
finally we apply both asynchronous ESG
D&amp;amp;E msgd to train large comminution or
neural network on imagenet data set our
baseline as msgd the natural moment is
GD on single GPU we also compared with
the downpour method of team and others
on for jeep views we see that the
elastic averaging STD in red has similar
performance as downpour in green but our
methods require 10 times less
communication remarkable after combining
with natural momentum SGD our method in
orange achieves not only the fastest
convergence rate but also the smallest
test error we first posted our paper
archive about a year ago we have learned
that a number of companies including
Facebook and others have implemented the
u.s. GD and are currently using it in
production to train large neural nets
however its theoretical aspect is still
not fully understood please count your
poster number 37 this unique thank you
for your attention
so hi everyone my name's engine and this
is a jaunt walk with Miguel a habit and
also migrate supervisor rich so zoobi
has given a great lecture this morning
about Bayesian inference and the core
problem in Bayesian inference
essentially is to compute the
troposphere however for most models that
we are interested in including your
networks the true posterior is in
trouble so now here we consider
approximations and in particular I show
the approximation structure on the right
hand side that we will replace the
typical like returns with simple factors
so I'm ideal lies way to update these
things we think is to first state
troopers the area and then you remove
one of the lock returns replace it with
with a simple factor and then you update
the simple factors to meet a new
distribution approximated to posterior
so this will give us very accurate
approximations because right now the
fetters can capture the effects of its
corresponding level turns on the
triple-t area however it is also an
internal procedure as well because you
also have the difficult turns in the red
box so in practice we run EP so which
takes the same idea but replace the
difficult like returns with simple
factors so now if he has much cheaper
updates and tended idealized procedure
and we often have analytical solutions
but more importantly if he can produce
most accurate approximations on a
variety of problems so it is a very
important techniques in approximate
bayesian inference however it is a shame
that we could apply this thing to last
girl inference problems because it has
order n memory so we solve this memory
overhead problem by proposing soccer CEP
where we tie all the local factors and
then run City on it so now we just need
to store one copy of the local fetters
that means we no longer have the number
n in the memory completa number so the
important solved
so the coal idea here is that now the
factors are approximating the average of
facts of the lack of talent on a true
pasti area and during training time each
iteration we will sample one like
returns to do so hassey and updates and
importantly we need to mention that the
relationship between stochastic EP and
EP is the same between stochastic
version inference and virtual inference
we test this algorithm on a variety of
examples and to summarize that
stochastic EP performs pretty much the
same as the original EP and both of them
are better than ADF a simplified
algorithm we also tested the memory
reduction numbers and in the largest
example we saved 65 gigabyte memory
comparing to the original algorithm so
to summarize this we finally managed to
scare this very accurate ETA written to
the large state has their settings so we
are very excited about this come to the
pasta and watch out to talk with me
thank you alright so I'd like to tell
you how we automated variational
inference using stem this is joint work
with my amazing colleagues Rajesh Andrew
and Dave so if we want to use
variational inference and practice today
we're kind of faced with the picture
that we see here we want to fit a
statistical model to a large data set
well we hand both of these things to a
PhD student she toils away at the
mathematics and the coding and the
debugging and many months later produces
a result if we make any changes to the
statistical model this poor student has
to repeat this process and this really
is inhibiting the use of all of these
nice variational techniques today we
propose an automatic tool to in some
sense replace the poor PhD student or at
least give her some free time it's
called a DVI automatic differentiation
variational inference and what we do is
we take the things in this list which
consume the time of the PhD student and
ultimate them using the techniques that
you see beside them
I'd like to emphasize that a DVI we've
implemented it as part of Stan which is
a problems to programming system and
I'll tell you a little bit more about
that in the next slide what a DVI does
is it paints a new picture for us we can
now take any statistical model that we
write in a probabilistic programming
system in this case Stan and instantly
get a variational inference algorithm no
derivations no coding this allows us to
close the loop our goal is to genuinely
rapidly iterative we explore new models
as we investigate large data sets and a
DVI is bringing us one step closer
towards that goal so what the stand code
looked like let me walk you through a
very simple example this is a toy model
a graphical model on the Left where we
have n observations xn these are
discrete observations so i went ahead
and place the poisson likelihood on them
the latent parameter that we'd like to
estimate is theta it's a positive real
rate value so we've declared it as such
in the parameters block and because we
can we make no no no conjugacy
requirements in a DVI I've placed a
weibull prior which I doubt anyone else
in the room has done so given this we
give it into a DVI and automatically get
a variational inference algorithm
nothing else is needed as expected a DVI
is faster than mt mt techniques
especially for data sets with large
number of observations and I again like
to highlight that a DVI is currently
part of Stan I encourage you to the
visit to visit the link below and let me
know how it goes this weekend tips thank
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>