<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Continuous Limit of Large Random Planar Maps | Coder Coacher - Coaching Coders</title><meta content="The Continuous Limit of Large Random Planar Maps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Continuous Limit of Large Random Planar Maps</b></h2><h5 class="post__date">2016-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5EwXrRSpaI8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so next is the
bill birnes lecture and the paper bomb
was a professor at the University of
Washington from 1939 to 1974 he has done
distinguished services to the university
as well as to the society he has served
as the president of IMS as well as the
editor for the journals of mathematical
statistics just the previous one for on
the annals of probability and annals of
statistics are the offsprings for that
channel and he passed away in 2000 and
in his memory a few years ago a brain
bounds lecture was established for this
noise where's the probability seminal
and it's a great pleasure and honor to
have jean-francois like AUH from
universal paris or say to keep this
year's parabola
he's going to talk about continuous
limit of large random planar maps okay
thank you for the presentation I also
would like to thank the organizers for
this opportunity so I want to talk about
scaling limits for random planar maps so
planar map suggest graphs embedded in
the plane or fact will be more
convenient to use them as embedded on
this video the idea is to work with
random object so to pick such a graph
uniformly at random in a given class I
will be more precise in a while and to
to let the size of the graph tend to
infinity and to study the scaling limit
of these objects viewed as metric spaces
for the graph distance okay so
essentially if you have a graphic and
uses metric structure distance on the
vertex set and take graph
which is larger and larger you can
rescale the distance and hope to get a
limit and this is what I will try to
explain in this lecture okay so why is
it interesting it's not really we we
hope this in in this way we get a
limiting universal object in the sense
that it's not going to depend on the
particular class of these kept objects
that we start from okay we also hope
that we get in this way you're an
interesting continuous model this is
what I will call the Brownian map
although as we should see it is not yet
completely identified in some sense but
essentially if we can study this
limiting continuous subject we also hope
that we understand better the properties
of the large discreet object a large
beta Maps okay so in the sense this is
very analogous to what we always do in
probability theory we use convergence of
rescale random paths to bring in motion
and we know that bonyen motion is a very
nice and important object and we also
know that if we understand well bonyen
motion we can derive information about
long random paths okay so in some times
we want to do something analogous but
instead of doing with random paths we
are going to deal with random graphs
okay so here is a brief outline of the
electron so I will insist in the second
part on the main technical tool that we
are going to use which is certain by
directions between graphs and trees okay
in the last part I will try to present
some more recent work about geodesics in
these objects so let me start with a
brief introduction to planar maps so as
I said before planar map is just an
embedding a proper embedding means that
edges do not cross so proper embedding
of a connected graph should say a finite
connected graph into the two-dimensional
sphere and something which is very
important that we look only at the shape
of the of the graph so we are not
interested
the particular bedding we identify to
embeddings if they correspond via dye
hiked on a morphism of the sphere so
it's really a combinatorial objective
genome so here is an example of a planar
map so something which you can define
because your graph is embedded is a
notion of a face okay so the faces are
simply the connected components of the
complement of the union of edges so in
this particular case you have 1 2 3 4 5
6 7 faces you should not forget the
external one because you aren't the
sphere in fact there is no external one
and I'm going to be interested in the
particular instance of planar maps
namely the so-called P leg you lations
again so P is an integer a fixed integer
greater than or equal to 3 and P
angulation is a planar map such that
each face as exactly the adjacent edges
so in the case P equals 3 this is a
familiar notion of a triangulation and
in the case P equals 4 this is what we
call quite on Galatian yeah so this one
for instance here is a quite angulation
ok you can check that every face as for
the center jeez it's also true for the
external face it's also true for this
face here's it may seem a little bit
surprising but when when you have a
situation like this when an edge is
completely contained and if you want in
is a understand to only one edge to only
one face we counted twice because we we
imagine that we go around the face like
this and we meet this edge twice ok so
these faces are so quite hungry looking
ok and perhaps last thing that I will
need we need a notion of a Huijin map
okay so hooting a map means that I
distinguish an edge which is also hunted
so find stands on this picture I shows
this edge and I orient it downwards and
then the origin of the root edge is
called the hold vertex ok so this is you
know science a combinatorial tracheids
much easier to find sent for enumeration
purposes it's much easier to deal with
rooted objects and one believes that
this is not so important for does not
match make such a big difference
for the problems I'm going to address
okay so so this is just a simulation of
a large triangulation okay drawn on the
on the surface on your morphic to the
sphere
so essentially what the idea is now is
well can we here we have hundreds of
edges hundreds of vertices can we
sometimes choose one such object at
random let the number of edges tend to
infinity and try to get some continuous
limit okay so what do I mean by the
continuous limit so this will be in the
sense of convergence of meeting spaces
so I will view my graph my planar map as
a metric space so this is very easy to
do I look at the vertex set V of M I can
equip it with the usual graph distance
okay so the distance between two
vertices is just minimal number of edges
on the path from the first vertex to the
second one and equipped with the graph
distance the vertex set is of course a
finite metric space okay now the idea is
to to choose the planar map M uniformly
at random for instance here in the set
of all hooted regulations with n faces
so let me emphasize that we identify two
planar map t they correspond via direct
remember feasible sphere so thanks to
this identification this set of all the
angulations with n phases is a finite
set so it makes perfectly sense to
choose of course one uniformly at random
in this set and we want to understand
how this metric space The Associated
metric space behaves when we let n tend
to infinity okay so a man is chosen
uniformly at random as I said before in
this set and we expect that
if you risk a the distance properly so
if you multiply the graph distance by a
factor tending to 0 as n tends to
infinity so you can imagine that you
assign a length end to the power minus a
to each edge instead of having edges of
length 1 so you do this risk and you
expect that risk admitting spaces will
converge when the number of faces tend
to infinity toward a certain
continuously and the meaning of this
convergence will be in the sense of the
chromophore staff distance which I will
record in a while okay so before doing
that two remarks so it's important to to
whisk in the graph distance if you want
to remain in the framework of compact
spaces okay it's also interesting to
study this convergence here without
risking okay
this leads if you don't do any rescaling
you will get of course in the limit you
will get an infinite random lattice an
infinite random graph and this has been
studied by various people in particular
or Mehadrin and audit rom and others but
this is not what I'm doing here you know
I want to stay to have a kind of global
limit and not a local limit okay
understand before we expect some
universality of the limit it should not
depend on the integer P so it should be
the same for pn4 triangulation for
quadrangular and so for more than the
whole random planar maps okay so very
briefly remind you of the gamma first
off distance so this is just the
definition of the classical lost of
distance between compact subsets of the
metric space now if you have two compact
metric spaces which are not a priori
subset of a bigger space
you cannot use of course out of this
tends to compare them but what you can
do you can trust there is a very simple
idea do to come off you can embed
simultaneously in your spaces into the
same big space okay so this is the
meaning of the picture you have your red
space you want your green space it to
you try you find
embeddings i1 and i2 you can always do
that
of e1 and e2 into the same space it's
very important that these are isometric
embeddings because I'd like preserve
distance is it's really copies of your
meeting spaces that you find into the
same big space and then once you are in
the same big space you can use the out
of distance to compare your two spaces
okay so this is what you do you minimize
over all possible isometric embeddings
of e1 and e2 into the same space capital
e so our stuff distance between the
embeddings again this gives you the
so-called the chromophore stuff distance
yeah and it has nice properties and in
particular if you look at the set of all
isometric classes of compact metric
spaces you can check that the gamma fast
of distance is indeed the distance on
this space capital gain this is not
completely obvious and moreover if you
equip cage with this distance you get
one a nice metric space of Polish space
that is both separable and complete so
now I come back to the problem I was
mentioning in the beginning of the
lecture so it makes perfectly sense to
study the convergence in distribution of
the vertex set of my graph V of MN
equipped with a risk a graph distance
viewed as a random variable with values
in this capital capital okay so it's
really the usual setting of having a
sequence of random variables taking
values in a Polish space and you want to
study their convergent into distribution
so this problem in fact was stated I
think first in this form for
triangulations by H harmony in his SEM
paper ok so well I can immediately tell
you what's going to be the right choice
of a so of course the parameter a is
chosen in such a way that the diameter
of the space remains bounded so here's
the diameter of the before rescaling
should be of order n to the power a and
it is has been known for sometimes
that's a correct value of a is equal 1/4
okay are these forked on Galatians but
as I said before it would be the same
for regulations in fact okay so very
briefly some motivations for studying
planar maps but there are lots of
motivations coming from combinatorics
planar maps are important objects in
combinatorics they have been so I think
since the work of 13 in the 60s there
are motivations from theoretical physics
there are called deep connections
between enumeration of maps and
expansions for matrix integrals and more
recently large random planar maps have
been used as models of random geometry
in particular in the setting of
two-dimensional of quantum gravity okay
and you can imagine there's a reason
work by your bed for the protein
structure field which is related to this
except they don't deal with span our
prana maps they use a different approach
involving as a Gaussian free field but
it is expected that sometimes both
approach it should be equivalent okay of
course as I said in the beginning there
are motivation purely from probability
theory in some sense you want to get
kind of analog of Brownian motion but
replacing path by graphs so a kind of
purely Brownian surface okay there are
other motivations from metric geometry
and also motivations from algebra and
geometry but ok will not say more about
that okay so let's become perhaps to the
description of the main technical tool
I'm going to use which depends on
certain by directions between maps and
trees so I need two different notions of
a tree first notion is a notion of penat
reapplying tree so here
why foreign you can view such a tree as
a general article tree for a population
that starts with an empty store I
represented here incest or by the symbol
empty set and then any individual in
this Jenna logical tree
being represented by a word made of
positive integers in the obvious way for
instance the children of the ancestor
are represented by the symbols 1 2 and
so on
children of 1 2 by the symbols 1 2 1 1 2
2 1 2 3 yeah so it's a obvious
definition so you can simply define such
a tree as a collection of these words of
integers and this collision has to
satisfy certain obvious rules but it
don't state them so what is important is
the notion of a planar tree is the fact
that ok first you have a root of course
I insist on here and you ever saw an
order just the other is put in the
description of the tree if you want when
you say that the children of the root
are 1 2 and so on
1 is the first child to be the second
child and so on so you have a
lexicographical order on vertices ok so
this is a very basic notion of a tree of
course in Terminator X now slightly more
complicated notion what I call a well
labelled tree is going to be a pair
consisting of planar tree first two and
then a collection of labels assigned to
the vertices ok so the labels here are
the numbers in red on my picture and
they have to satisfy certain properties
the label of the root is a grass equal
to 1 Labour's are positive integers and
when you move along one edge the label
can vary by plus 1 or minus 1 or it can
say stays the same
ok so for instance if you have an
individual here with label 3 its
children can have label 3 2 or 4 but
these are the only possible choices okay
okay so why is this notion interesting
because there is a nice by direction
between these objects well labeled trees
and correlations so I consider a set
capital T n of all well label trees with
energies and there is a bijection from
this set onto the set of all teed
quadrangular cones with n faces so for
the moment is not saying much is just
saying that
sets at the same cardinality but in fact
you get more into by direction explained
below you get the fact that if you start
from a well label tree you can construct
The Associated population capital M in
such a way that the vertex set of the
map is the same as the vertex set of the
tree plus one extra vertex which I call
partial D here so vertices of the tree
become vertices of of the graph and
moreover which will be very important
for us what was label on the tree
becomes the graph distance from the hood
vertex in the planar map okay so you
have this property here labels become
graph distances when you go from the
tree to the graph ok so before I explain
to users by direction let me mention
that there are similar by directions for
much more general planar Maps points and
for triangulations and so on these have
been studied by various people but in
particular by butchie's differential
school and detail vertical physicists in
fact exactly but I will stick to the
case of catan relations for explaining
this by direction because it's it is a
simplest case so here is an example so
you start from a well labeled tree it is
here and you want to construct this
quadrant galician which is here ok well
it's not yet constructed but the
vertices of the collaborations will be
the red vertices here so they are the
same if you want of course as the
vertices of the tree plus one extra
vertex which is here ok so I start by
adding this extra vertex partial T and I
assigned to this extrovert X the labeled
zero by convention so now what I do I
I follow the contour of the tree in the
way indicated by the arrows here
and in this way I will encounter of
course every vertex of the tree but more
precisely I will encounter every corner
associated with the vertex of the tree
okay so you will see 5/10 this vertex
here has three Associated corners one
here one here and one here and I will
visit all three of them doing the
control okay and the ruler as follows so
when I do this contour I start here from
the bottom of the tree from the incest
or what I do i I connect this vertex to
the last visited vertex with smaller
label okay so the first step of course
I'm at the bottom of the tree I have a
vertex with label 1 okay I have to
connect it to zero so there is only one
which is this extra via text partial D
here so I draw this red edge here at the
first step okay but then what I'm going
to do I'm going to move this way okay so
I arrived here I connect this vertex
label 2 to the last vertex labeled one
which is this one yeah there is not much
choice here but later there will be some
choice okay I continue I arrive at this
one when I have attacked label one I
connect it in fact to the unique vertex
labeled zero okay so then I go down like
this so a visit to gain this vertex
labor to but I visit a different corner
now okay so in fact what I do i connect
it by an edge starting from this corner
here to the last one which is now the
vertex here okay when I am going
downwards arriving at this vertex to the
last vertex labeled one which I have
visited is now this one okay this way I
drew this edge here okay okay and then
okay 3i i connect it to the last two
which is here to again I connect it to
the last one which is here one I have no
choice to 0 and you continue this way
until you have explored every corner of
the tree until you are finished the
contour and you can check that now the
graph that you have constructed
via this algorithm is indeed a
quadrangle Asian can you can check that
all phases of degree 4
by convention you hoot this correlation
at the first edge constructed in this
algorithm and in such a way that the
extravagant X partial D is the root
vertex okay so you will route the
calculation here and you can now verify
it okay at least on this example that
the red figures here which were the
labors of the tree okay but yeah how do
you make the connection is where D how
do you draw them in a plane you said
which edges to draw they're not
embedding in the plane
it's the tree is drawn into plane right
plane what's important is that I didn't
want to give too many details but if you
ask me but it's important study it's
time you drew an edge I should maybe
come back ok maybe ok what i'm doing
here i'm going down i arrived at this
vertex here I drew an edge of the graph
from this vertex to the last one which
is here and this vertex starts from the
corner at which I have arrived okay so
this is I draw this edge here like this
and you can do that in such a way that
edges do not cross operate you of yours
but it's not very difficult to check and
there is actually a unique way of doing
that so a planar map uniquely defined I
don't want to give too many details
because does it answer your question
so what I was saying that the red
figures here so which were the labels on
the tree you can check that they now
Quinn side with distances from the hood
vertex for instance this vertex here at
labels three and three is also the
distance from this vertex to the root
vertex which is here okay and this is
true fine you other vertex so now just
to summarize the our strategy will be
first to understand continuous limits of
trees this has already been done in some
sense in order to be able to understand
continuous limits of maps but there is a
difficult point which is that as I
explained before what you will
understand in the graph in the planar
map you will understand distances from
one specified vertex which is the root
vertex because I coincide with the
labels okay but of course this is not
sufficient if you want to to get
converters of metrics basis you you
should be able to understand distances
between any two vertices not only
distances from one specified vertex okay
so let me explain the asymptotics for
trees I'm going to need so you see a
serum which is basically due to the V
del deuce so you look at the set of all
planar trees with energies and you pick
one uniformly at random in this set I
call it 2 n so again you can view 2 n as
a metric space for the graph distance
the same kind of idea that I had before
but now I apply this idea to two tree so
you will change the graph distance
basically with the factor of 1 over
square root of n this converges in
distribution in the gamma first of
science as before
toward a certain limiting random compact
metric space which is called which is in
fact the so called Ciotti
what in your random tree ok so why is
this x notation te-de comes from the
fact that you can use a CRT and Mathews
is perhaps the best way of looking at
the CRT you can do it as a tree code
by a normalised bueno discussion okay so
let me explain that perhaps first to to
give a rigorous definition of the CRT I
should introduce a notion of a real tree
okay so what is a real tree well the
formal definition is here it's a compact
metric space at that between any two
points a and B you have a unique arc
meaning that there is a unique way of
going from A to B in a continuous and
injective manner of course up to a
parameterization okay okay there is a
unique path from A to B you want up to a
parameterization if you require your
pass to be one-to-one and continuous of
course and moral voice you truce kaanji
if you choose suitably the
parameterization your pass is isometric
to a line segment okay so this is the
definition of a real tree you should
think of a real tree as a union of line
segments just as in the picture which is
there of course this union has to be
connected it has to be a tree in the
sense that you will have no loops it has
to to have the shape of a tree if you
want but then you you get if you look at
such a union of line segments if you
equip it with the appropriate distance
the distance between a and B here has to
be 400 lines of course was a red path
which is here then you get a real tree
okay so really any compact real tree I'm
going to look only in fact here at
compact real trees any compact where
tree can be obtained as a limit and
increasing limit if you want of finite
union of segments such as on this
picture of course but of course it can
be much more complicated than this you
can have infinitely many branching
points and you can have even uncountably
infinite lis many leaves but basically
you should think of a real tree as an
object like this okay so in the discrete
setting it is well known that you can
code the planar trees which are
introduced before by dig paths or
contour functions and that you can do
the same for your trees okay so this is
the next slide you can
could a real tree and in fact you can
code any compact real tree by a function
in the following way so when I start
from the function okay you have a
function G which is non negative defined
on the interval 0 1 continuous of course
we start and finishes at 0 so this is
the red function here and you can
associate with this functional real tree
study perhaps I should give you the
intuition behind the construction you
imagine that you job the red graph here
is a strip of paper and you put some
glue below with the strip of paper so
below the graph you put glue everywhere
and then we imagine that you squeeze the
graph like this you push the right side
onto the left side if you want and so
what's going to happen if you do this
operation so what's important when you
do this squeezing you you keep the
vertical distances okay very important
that you don't change the vertical
distances so what will happen is that
two points which are the same level
below the curve or than this point and
this point and on your strip of paper
are going to be glued together ok and if
you do this gluing not how to imagine
that you get a tree pattern in this
example it would be a very simple
treatment right okay but the precise
mathematical definition is there what
you can do you can define pseudo
distance on the interval 0 1 the G of s
T is just G of s plus G of T minus twice
the minimum of G between s and T and you
glue well this not earlier distanced
only up pseudo distance so there are you
can have s different from T and did G of
s T equals 0 but what you do you
identify say T and T prime if the G of T
DT prime is equal to 0 it is just the
same as saying that G of T is equal to G
of T Prime and is equal to the minimum
between T and T prime so then as usual
what you can do you take the quotient
space of 0 1 for the success relation
you equip it with DG which is now a
distance and you get a real tree
something which is are so important and
you can have a convention for who things
is really you you hold it at the
equivalent class of zero okay something
which is important in this construction
is that you also get using this
construction you're still get a
lexicographical order on your tree you
will say that the vertex which is the
equivalence class of s comes before the
vertex T if s is smaller than T simply
the same lexicographical order we had
four discrete trees before you also get
it from your trees in fact via this
cutting okay so now I can just is just a
restatement of a new CRM in the limit
what you get is a random real tree coded
by your boy in education of length 1 and
this is a shot so informally you should
imagine that you have your tree you
could define what it means of making the
con to your tree like this and if you
would record the distance from the hole
in this evolution you would get the
point energy ocean is intuitive idea
okay so you just limits of trees now I
have to speak about limits of labels you
can remember in the discrete setting we
had Labor's assigned to the vertices of
our plane trees okay so I want to do the
same now for my continuous trees okay so
of course this CRT here is a trick coded
by your random function of course so
it's a random real tree so but I start
by considering a deterministic real tree
yeah Capital TD so something you can do
very easily just looking at body motion
indexed by the tree okay so this is well
it just turned out definition if you
want of born in motion indexed by the
line except that you replace the line by
by a real tree but otherwise exactly the
same so what what does it mean just
means that you you Han independent
Brownian motions along the branches of
your tree so
labor's evolve like Brownian motion when
you
when you move along the tree okay but of
course if you you look at two different
line segments two disjoint line segments
on your tree you have to use independent
bone in motions to describe the
evolution of labour okay okay so you can
do that for any we are tree if you have
some mild conditions on your real tree
you can prove that there is is there is
a Condit continuous modification of your
Brownian motion indexed by the tree okay
so this looks similar to what we had in
the discrete setting remember in the
discrete setting we the Labour's could
move could change by plus 1 minus 1 or 0
along each edge so it was a kind of trim
vector random walk so what I'm doing in
the continuous setting I look at 3 index
board and motion does the same except
that we lost a positive itical straight
okay if I look at bone in motion indexed
by the tree in this way of course it's a
Gaussian process it's not going to be
positive when it starts from from zero
at the root okay so we have to do
something else to take into account this
positive constraint and it turns out
that there is something very simple we
can do the the first idea would be just
to condition the labels to be
non-negative in fact it works but there
is an an even simpler construction which
I going to explain in the next slide so
I would state in fact to scaling limit
for well labeled trees was just to
remind you what was a well labeled tree
you know this plane tree with the red
labels which were positive and satisfied
the rules I explained before okay so we
take one one labeled tree with energies
uniformly at random so theta n R and V
and V are the labels so we escaped the
tree by 1 over square root of n just as
in I do serum of course not a surprise
and we change the labels by one of a
square root of square root of n ok so
why is it so remember what I just said
in some sense the labor's
evolve a little bit like to index to
random work you forget about the
positivity constraint okay so since the
height of the tree is
order one over square root of n sorry
the height of the trees of order square
root of n typically if you look at the
value of a label it will correspond to
the value of a random walk at the time
of order square root of n okay and the
random walk well centered went to work
of course at time square root of n is a
for a square root of square root of n so
this is where we get this n to the
power1 force which is very important
okay
okay so now if we do this rescaling we
can find explicitly the form of the
scaling limit and what we get when the
limit for the trees is a CRT it's not
completely obvious okay I will explain
that in a while and for the labels we
don't get Brownian motion index by
society but we get a process which I
call it Z bar so this body motion was
defined on the previous slide yeah for
deterministic tree but you can take the
CRT of course so what you get is this
process condition to stay positive and
there is a simple way of doing this
conditioning what you do you define Z
bar as the - it's minimal value so in
this way it's obvious it that it becomes
non-negative I said that the limit was
not exactly the CRT well it is a CRT but
you have to change the route okay you
have to reroute the CRT at the vertex
that means I call your Rasta which
minimizes the labels simply because in
if you want in the continuous limit you
also want the route to have a minimal
label which was true in the discrete
setting okay so you have to change the
route of the CRT okay but otherwise this
is a scaling image this already gives
you a lot of information about distances
in a random planar map okay because
remember that these Labour's
corresponding to distances from the hood
vertex ok so it should be the serum
constant which was proved by Sanchez
we're saying that if you look at the
maximal distance from the hood is a new
random calculation with n faces
you can rescale it by n to the power
minus 1/4
so introduce the by direction between
padam Gration and trees the maximal
distant from the root corresponds to the
maximum label okay so this one for ya of
course is one force is the same as a one
force here we are using to rescale
labels of course and you get a more or
less explicit form for the limiting
distribution in terms of body in motion
indexed by society it's possible in fact
to compute to have some compute moments
for instance we have a kind of Laplace
transform to have some information about
the limiting distribution so let the
immediately mention in connection with
universality so this wizard has been
extended to much more general planar
maps including triangulations and in
fact or so random planar maps where
faces do not all have the same degree we
can have different degrees in particular
by Gregory niyama underscore so this
core sauce okay so the next section I
will come back to program I started from
the programmer the scanning limit of
random planar map okay so this is just
to remind you of the notation so I look
at all who did 2p anger oceans with n
faces it's very important that I take
only here even integer to please okay
this is called a bipartite case okay so
this does not include triangulations
although although it's very likely that
the results I'm going to state also hold
for triangulations so I pick one
uniformly at random in this set I look
at its vertex set I wish again the graph
distance by one over ends about
one-fourth for CP is just a constant
depending on P and we get two converters
in distribution toward a certain random
compact between space in the sense of
chromophore daf distance unfortunately
it's only a sequential limit okay so
there is a
compactness argument there you get this
limit at least along a subsequence okay
I will come back to that in a while
still you can describe the possible
limit in a fairly explicit way you can
show that
limiting spaces I'm infinity which I'm
going to call the Brownian map later he
is the quotient space of the CRT well so
I have another equivalence relation this
equivalence relation on the CRT is
defined in terms of Brownian Labor's on
the CRT Borgan motion index position so
I use the same notation as before he is
born in motion integrity Z bar Z minus
it's minimal value and now I define the
current relation by saying that two
vertices are equivalent if and only if
they have the same label and if between
a and B the labels are larger this is in
fact similar to the equals relation we
used to define the coding of trees by
functions okay but here it's a different
setting so I should say what it means
here to say that T is between a and B XI
belongs to the interval a B it makes
sense because as I said before we have
also a notion of lexicographical order
on the tree okay so we can make sense of
this interval here okay so now this M
infinity is completely defined it's the
CRT co hunted by the sequence relation
now what is capital D now well of course
these is a distance and M infinity can
prove that it induces a caution topology
we can prove several bounds on capital D
upper and lower bounds and we can also
prove that the distance between any
vertex a and the root Rasta is the label
of a so it is just similar to what we
had in the discrete setting but we
cannot I cannot completely identify
capital D okay that's still an open
problem but we have some information
okay so before I discuss this open
problem let me explain to you perhaps
why we have this equation here that
comes up okay so you have to remember
the discrete setting okay
in the discrete setting when we did we
draw such an edge between two vertices U
and V
with who satin satin edge when you was
the last visited vertex before v with
smaller label because you this mean
certain in the interval between U and V
all the labels or the labels of these
red vertices here at least as large as a
label of V you may be that you go
backward on the tree and the first time
you meet a vertex with a smaller label
is this you see okay so this means that
we have this properties if you prefer
and now the the definition of the
current relation is a continuous limit
is just the analog of these properties
in the discrete setting okay so what's
happening if you want in the when we
passed the scaling limit is that we will
have this property here between two
vertices U and V which are very far away
from each other this will happen from
time to time and saying now that they
are connected by an edge in the quraan
relation or in the planar map well this
means that in the scaling limit because
distances are we scaled by a factor
tending to zero this means that in the
scaling limit they have to be identified
so this is the reason for this
identification of course what's
difficult to prove is the converse that
you don't don't identify more than this
okay I know in a sense you don't
identify identify so many points typical
equivalence class is a singleton okay
and you have only countably many
occurrence classes with the three points
in fact okay but the typical case is
single tone because you don't even under
defined anything
okay so as I said before you at least
you you don't identify really the limit
but you identify its topology just a
caution topology and discussion and open
problems are to identify capital one is
able to identify capital D this would
imply that you don't need to take
subsequent season messua okay and
otherwise you of course you will also
want to prove that capital D does not
depend on P that you are this
universality property same limit for
triangulation squad on relations and so
on okay although the limiting space it's
not completely identified the space i'm
infinity d you can prove a lot about it
okay so this place is called the
Brownian map the name was given by
marker Honoka D mu what a different
approach to the same object in fact not
dealing with a coma for stuff
convergence and Shina for inside two
serums you can prove about the berean
map you can prove that it has dimension
D is equal to four almost surely and you
can prove that almost surely it is only
OMA feet to the 2-sphere so in a sense
is not totally surprising because you
start from graphs drawn on the sphere
and you put more and more vertices more
and more edges and in the limit you get
something which is Obama fix to the
sphere okay not a big surprise maybe but
still it's not obvious because you could
you could imagine that in your random
graph you could have bottlenecks like
this in cycles such that both cycles of
both side story of the cycle have a
macroscopic size but such that the
length of the cycle is small in
comparison with the diameter of the
graph
okay and what this theorem tells you in
fact is that this does not occur in fact
for large planar maps okay so in the
last five or seven minutes I want to
talk about more recent results
concerning geodesics in the Brennan map
and maybe I can start by describing what
happens in the discrete setting okay
there it's very easy to to understand
how to construct geodesics while say to
the at least two
to the root vertex from any vertex going
to the wood vertex okay so you use the
by directions which I explained in the
case of quantum relations but there are
more general by directions okay so what
you do you start from V and I explained
before you you you go back what's in the
tree you look for the last visited by
attacks before view with a smaller label
with label L V minus 1 okay so find
santini 6 that part is going to be V
prime ok and then you start from the
prime and you do the same you go
backward on the tree like this and this
second is the last visited the vertex
before V prime with smaller label ok and
so on and if you do that remembers that
the Labour corresponds to the distance
from the root vertex each time you
decrease by 1 the distance from the
vertex so in the end of course you will
Rachel the vertex and you will get to
dualistic
of course your discs are not unique and
you can see that that you are not unique
because when you arrive at a vertex like
this which is not a leaf of the tree you
can't cross the tree in some sense you
can continue decide to continue this way
all you can as I did on the picture you
can explore what is above also and this
is why you don't have unique necessary
day6 but this is the way you get your
basics from the tree if you want well
now you can do exactly the same
construction in the continuous setting
ok now you have this CRT it's important
that you have the lexicographic order on
it and then you you start from any
vertex a here so it has a certain label
this brownian label z ba a witch queen
site with a distance from the root and
you go backward on the tree and for
every T between 0 and z bar a you look
at the first vertex that you meet going
backwards from a on the tree which has
labeled so it's exactly the same as in
the discrete setting okay and but of
course now it's a continuous curve ok
it's indexed by T in the interval 0 from
between 0 and z bar a which is a
distance between aliens and the root but
it's not only by the same formula the
discrete case you get a geodesic
and this called a simple dude is it okay
now there is a nice fact which is really
the key point the fact that except
possibly the starting point
simple geodesics will visit only leaves
of the church so remember what is
playing the discrete setting I explained
that non uniqueness of geodesics was
linked to the fact that arriving at a
vertex which is not a lift you had
several choices and yeah because you
visit only leaves at least informally
you can guest at least at all simple
delish will have kind of property of
uniqueness of geodesics okay and this is
what I will explain now if you have a
leaf the CRT you get two unique geodesic
while unique simple geodesic at least if
you start from the point which is not
the leaf say for instance this one you
can stop while either from the left or
from the right this makes sense okay
should define it in a more proper way
but you you can contact two distinct
simple geodesics and even three simple
geodetic when you have a branching point
okay so I'm using the fact fact you
cannot get more because three is a
maximal multiplicity okay so the key
result or so now is the fact that in
this way you got all your basics to the
root okay so here is the main result
about 36 so I need to define the
skeleton of the skeleton is jut the CRT
- its leaves it thought that if you look
at the projection canonical projection
here from the CRT onto the bunion map
remember the brain map was Distortion
this projection restricted to the
skeleton is yomama facin okay this means
that you you are not identifying any
point of the skeleton with a different
point okay if you want the only point
that can be identified in this quotient
are leaves of the shot
so you can prove that this is okay I
could tell you made of the skeleton and
there's a canonical projection we can
prove that scale as dimension to so in a
sense it's a very small subset of the
brilliant map so recall that the Braden
map M infinity had dimension 4
okay so this again is a kind of it's
okay till it's dent the brilliant map
it's a kind of dance tree embedded in
the brilliant map okay so why is it
important well but you can prove it that
this at scale is exactly what geometers
may call the calculus of the Brownian
map with respect to the root it's a set
of points while you don't have a unique
geodesic to the root and in fact what
you can prove is that if you take a
point X in scale the number of distinct
is X to the root is exactly the
multiplicity of the point in the
skeleton okay
remember that scale is Uma morphic to a
tree so the multiplicity is just makes
sense as a multiplicity in the tree okay
okay so this result is very analogous to
classical results of classes of a
Romanian geometry which go back to point
away and authors the control class is
always a tree for a surface which is a
umam of it to the sphere I shouldn't
mention that you have to the root does
not play any special all here the same
result hold if you replace the root by a
point typical point in the Brillion map
okay one minute
so other things you can deduce from this
research is confluence property of
geodesics if you have two points x and y
if you take a need where suppose an x
and y start outside a big ball of a ball
of radius Delta and you take any
geodesic from X to the root and it is it
from Y to the root they have to merge at
some point say before hitting this
smaller ball around the root so in some
sense there is just one gem of geodesic
starting from the root and the same
holds for any typical point okay okay so
maybe I will go very fast now because my
time is over you can apply all these
results
to uniqueness of geodesics in discrete
Penna maps discrete graphs okay
of course there you don't get uniqueness
but you get a kind of microscopic
uniqueness so this corollary tells you
find stands that if you take a point
uniformly at random in the vertex set of
your graph of course you will not have a
unique your disease but any two
realities will be close at another
smaller small in comparison with the
diameter of the graph gets become the
kind of make Oh stupid uniqueness and
you can also study points exceptional
points where you have don't have
macroscopic uniqueness you can show that
you have at most three mattes
macroscopically different geodesics for
exceptional vertices in your planar map
okay okay so I think I should stop now
thank you
so it's a pleasure to introduce our
speaker from the north this year we have
Gordon Slade from UBC and he's going to
be talking about weekly self avoiding
walks in four dimensions thank you very
much and I'm very happy to have this
opportunity to talk to you today about
this recent and ongoing work with David
bridges my colleague at UBC about four
dimensional self avoiding walks so let
me start just by reminding you a little
bit about ordinary self avoiding walk so
think about the discrete time model
first although I want to go to
continuous time shortly so SN of X is
the set of all self avoiding walks in Z
of length n that start at zero and end
at X and take nearest-neighbor steps
that's the Euclidean distance and this
condition is the condition that the walk
not intersect itself so that that's what
makes it into a self avoiding walk so SN
of X is a set of all n step self
avoiding walks that start at zero and
end at X and SN is the set of n step
self avoiding walks that start at zero
and end anywhere we'll be interested in
the cardinality of these sets so CN of X
is the number of n step self avoiding
walks from 0 to X CN is the number of n
steps self avoiding walks that start at
0 and end anywhere and there's an easy
sub that tells you that CN is growing
exponentially in the sense that the nth
root of C n converges to a limit mu
which is called the connective constant
and the measure that I want to put on
the set of self avoiding walks of length
n is just the uniform measure so look at
the uniform measure on SN so that means
that each self avoiding walk has
probability 1 over the cardinality of SN
which which is C n another interesting
quantity is the two-point function you
know when you have a combinatorial
problem like this it's often useful to
go to generating functions so the
generating
is this power series with coefficient cn
of X we'll call that G subset of X and
this for every X turns out to have
radius of convergence said C which is 1
over mu so CN of X is growing like new
to the end for all X not just CN now
what I'm interested in is critical
exponents and these are concerned with
the asymptotic behavior of various
questions that you could ask about the
this problem like CN grows like Mew to
the N we know that here but what about
Corrections to that leading behavior and
there's a critical exponent gamma here
which is predicted to exist and has been
measured numerically and all dimensions
and the mean square displacement so this
is with respect to that uniform measure
on SN what's the expected distance
squared that should grow like n to a
power which is called to new and if we
look at the two-point function right at
the critical values Zed see this 1 over
mu which is the radius of convergence
then it should be finite and decay as X
goes to infinity according to a power
which is written 1 over X to the power D
minus 2 plus ADA so this gamma nu and
ADA are examples of critical exponents
they're not independent at least that's
the prediction there's a relation called
Fisher's relation comes from the sort of
physics arguments that tells you that
they're they're related by that equation
so I'll show you a picture this is a
random self avoiding walk in on the
square lattice that takes a million
steps this is a figure by Tom Kennedy
and what you can see is that it doesn't
look anything at all like a Brownian
motion it's not like ordinary random
walk path would look like an ordinary
random walk path would look like a plate
of spaghetti and and this doesn't look
like that at all so it's different but
in high dimensions they look the same so
you can come up with a rough argument
that tells you that in more than four
dimensions self avoiding walk should
look like Brownian motion like this you
there's more than one way to do it but
maybe the easiest is to say that random
walk path
our two-dimensional and too
two-dimensional objects don't want to
intersect generically in more than four
dimensions and so if you tell a random
walk not to intersect itself it won't
care in more than four dimensions and
it'll just be a Brownian motion so that
is hard to prove but there's a some
theorems that many people have worked on
over the years that say that CN is
growing purely exponentially to leading
order that the mean square displacement
is linear in the number of steps the
critical to point function has the same
behavior as the green function for
random walks one over X to the D minus 2
and you have convergence and
distribution to Brownian motion so these
are old results and what I want to talk
about today is what's happening in four
dimensions these results were proved by
lace expansion techniques which do not
extend to four dimensions they cannot be
applied now the prediction from physics
is that the upper critical dimension is
four well that's also that little
argument that I just gave you and that
the asymptotic behavior for four
dimensions has log corrections to these
relations here especially these two in
that CN should grow like Mew to the N
with a log correction not an end to a
power but log into a power and the mean
square displacement should be a little
bit bigger than n but not that much
bigger just a power of logarithm and
well for the critical to point function
actually there's no logarithmic
correction that's what the prediction is
this goes back to our normalization
non-rigorous or a normalization group
methods from almost 40 years ago and
these logs appear also in the
susceptibility which is just the the
generating function for the sequence CN
it will diverge as zet approaches C from
below linearly but with a log correction
and the correlation length which is the
related to the exponential rate of decay
of the sub critical to point function
this e 1 here is a unit vector in the
one direction so GZ at any one is
behaving like e to the minus n over the
correlation length for Z less than Zed
see
of exponential decay if you're below the
critical point that will that
correlation length will diverge as that
approaches Zed see from below with the
square root divergence and then also a
log correction so one would like to try
to prove these things we have been
looking at a modification of the this
self avoiding walk that I've just been
talking about called the continuous time
weekly self avoiding walk so I'm gonna
describe it just in high dimensions here
which is what I'm interested in so you
start with a continuous time simple
random walk so what that is is it's it's
a process that instead of moving at
integer times it moves at random times
which are separated by independent
Exponential's so there's these
exponential holding times for how long
you stay at a place until you make a
jump and when you do make a jump you
choose uniformly from your 2d neighbors
and move to one of them so that's what
this ez Rho is its expectation for that
continuous time nearest neighbor simple
random walk with exponential holding
times and then we introduced the local
time of that process at a point u n Z D
which is just the total amount of time
spent by the process at u up to time T
and then we form this intersection local
time I of T which is the l2 norm squared
of this local time at you now if you
write out this LUT as an integral and do
it twice because it's squared you have a
double integral and you can actually do
the sum over u to eliminate one of the
Delta functions and these are just
kronecker deltas and you'll end up with
this expression here for the
intersection local time and you can see
what that what this does is it's
measuring the amount of time that the
walk spends at the same place and so
it's a measure of how much intersection
is actually occurring now we'll define
the two-point function of this
continuous time weakly self avoiding
walk in the following way first of all
there's a parameter G positive which is
giving the strength of the
self avoidance we'll be looking at G's
which are quite close to zero later on
and it's given by this expression so
what this does is it let's look at the
integrand here it's taking the
expectation with respect to this
continuous time simple random walk
started at zero we're forcing the walk
to be at X at time T and then we wait a
walk with E to the minus G times this at
intersection local time so the more
intersection that takes place the bigger
this exponent is going to be in the
negative sense and so the less weight
that the path will have then we want to
sum over T that's like summing over N
when we were discussing the two-point
function for the discrete time self
avoiding walk so now we have to
integrate over T and we used to have a
Zed to the N now there's an e to the
minus nu times T which is playing that
role so this is the quantity that I want
to be studying this is the two-point
function and well you can apply a
subadditivity argument to this
expectation here as well to see that the
susceptibility which would just be
defined to be the sum over all X in Z D
of this two-point function will be
finite if nu is large enough so that you
have enough exponential damping here but
it'll be infinite if nu gets too small
we want to study what happens right at
the critical value and here's the main
result so actually the title said D
equals four but the method works more
easily when D is greater than four so
I'm including it here in the statement
there exists a G not well which depends
on D such that if if G is in between
zero and this G naught then the critical
to point function with that positive
value of G will be asymptotically a
constant over X to the D minus two as X
goes to infinity with with some higher
order correction in particular there's
there's no logarithmic correction in
four dimensions here so I want to talk a
little bit about the method of proof
that we use
but before doing that I want to say that
we think that the this method has
potential to do some other things that
we're actually actively working on right
now one of them is to prove the
logarithmic Corrections for the
susceptibility in the correlation length
in the case of D equals four we haven't
achieved any of these three bullets yet
but they're things that we're we're
interested in and working on and another
one is to prove the same result also
with a small nearest neighbor attraction
so this is a model which is used in
modeling polymer collapse a polymer in a
pore solution so a polymer in a good
solution is measured by a self avoiding
walk but a polymer in a pore solution
referred the pore solution refers to the
fact that the polymer cannot stand to be
in contact with the solution and so it's
only other option is to be in contact
with itself instead and so there's some
competition between the self avoidance
which tends to push the lock make it
bigger and a self attraction which is
making it want to be next to itself and
that's a very hard problem actually that
has not been solved very much
and what Roland Bower Schmidt who's a
student at UBC has noticed is that the
techniques that we use in the proof here
can include the case of nearest neighbor
attraction and that's something that is
currently being investigated also
there's a model of self avoiding walk
strictly self avoiding walk in discrete
time with a particular weight associated
to each step and it's the long-range
steps I think I won't talk more about
that but this is something else that
we're working on as well the methods
that we use our renormalization group
methods rigorous or normalization group
methods that have been developed in the
course of some work that David bridges
has been involved in for about 20 years
including a paper with Evans and Embry
in 1992 and with Embry in 2003 that
solved this kind of a problem and other
problems including the end-to-end
distance problem with those logarithmic
Corrections that we saw earlier for the
case of a four dimensional hierarchical
lattice now I don't want to say
hierarchical lattices here but it's a
kind of a replacement of the hyper cubic
lattice by something with a sort of tree
like recursive structure that makes it
more easily adoptable to renormalize
ation group analysis and so that was in
those were important precursors for what
we've been doing there's also a totally
different approach using different
renormalization group methods by Hera
takashi hara and his student oh no all
right deltas the laplacian con in the
lattice
yeah I'll define it in a moment all
right so now I want to talk a little bit
about the methods that enter into the
proof of the theorem so we'll fix a G
greater than zero and we'll usually drop
it from the notation and we need to make
a finite volume approximation and
actually it's quite easy using standard
technique called Simon liebe inequality
which may be familiar to people from
percolation or easing model or other
places this has been around for a long
time now that allow you to show that
this critical to point function on Z D
can be written actually as the limit of
crit above a two-point function which is
subcritical new greater than new C
corresponds to subcritical here because
it's e to the minus nu so there's a two
limits that take place here there's an
infinite volume limit and then there's a
limit as new approaches new C so what is
this finite volume problem here this is
doing the same two-point function on a
torus so R is some number which will be
going off to infinity here in integer
and lambda is is the torus of side
length R and what this two point
function is it's the same thing it's the
weekly self avoiding walk to point
function on the torus so this is the
continuous-time simple random walk on
lambda that's what this expectation is
and this is the self-intersection local
time on lambda so we only sum over the
vertices on lambda here those are the
only vertices that exist as far as this
finite volume model is concerned so I
want to take this for granted and say
that in order to study this critical to
point function on Zed D it's enough to
study it on a finite torus and work a
little bit subcritical provided that
we're able to do it with sufficient
uniformity both in the volume and in new
that we can take these limits
now there's another formula for the
two-point function so this is a theorem
that this finite volume two-point
function can be written as a certain
integral and in order to say what that
integral is I need to make several
definitions which I think may look
unfamiliar to some of you so Phi here is
a complex field on lambda so lambda is
the torus and Phi is like a spin Phi of
X is like a spin sitting at X and it's a
complex spin so it's not like an easing
spin which is plus or minus one it's a
complex field associated to the points
in lambda and Phi bar so if Phi is u
plus IV at X then Phi bar is just the
complex conjugate this Delta is the
discrete laplacian on lambda so it just
compares the value of Phi at X with the
values at its neighbors usual definition
of the laplacian there and then we we
need to go to differential forms
actually so associated to this Phi X
which is sitting at a point X we think
of that as a complex variable and
associate to it a differential D Phi X
and there's also a differential D Phi X
bar and there's some scalar which ends
up going in front of them which is just
there so that it doesn't show up later
and then we define this differential
form which has a zero form just a
function here Phi X Phi X bar and then
there's there's this to form so IX II IX
bar which is multiplied together using
the the usual wedge product and the
wedge product is just a way of
multiplying together differentials and
it's anti-symmetric anti commutative
rather so that's the important feature
about this product is that it's anti
commutative it matters what order you
write things if you change the order of
two adjacent differentials then you
change the sign and if you want to write
this in terms of the real coordinates
then you you can you can write it this
way so in particular side wedge Sai is
equal to 0 because it's anti commuting
so we have to define this tau X which is
this this form and then there's tau
Delta X which is another form which is
kind of
like Tao except the laplacian minus the
laplacian has been inserted on and on
the four possible factors where it could
be inserted here then this wonderful
formula holds that the two-point
function for the weakly self avoiding
walk with continuous time is equal to
this integral now this integral requires
some interpretation I have to tell you
actually what it means it's an integral
over C to the lambda so it's a very
large dimensional integral what you
don't see at the moment in this formula
is a measure showing up and usually you
like to see a measure when you do
integration so you know how to do the
integral and and unfortunately or what's
what's very delightful about this is
that the measure is actually up in the
exponent so this tau u and tau u squared
and by the way I put the wedge in red
here to emphasize it and I'm never going
to put it again so when you see tell you
that's actually tell you wedge tell you
now the the the differentials are up in
the exponent so we have to interpret
that and I'll say what that is on the
next transparency but this right hand
side is something which in physics would
be called the two-point function of a
super symmetric field theory with a
boson ik field Phi and Phi bar so the
Phi and Phi bar the boson ik field and
the the differentials are the fermionic
field differential Fermi fermions are
anti commuting objects and the
differentials are are doing that job
here this has a long history actually
going back thirty years to work a
physicist Parisien sulla and mccain in
1980
Lewton jure from the mathematical side
LaShawn bridges evans and in briana
bridges Embree and i have a survey paper
from last year on this topic so i'm not
going to tell you how to prove this but
i'll try to tell you at least what it
means so what is the meaning of this
integral okay so the problem is we want
to interpret as an ordinary lebesgue
integral but the the measure is up in
the exponent and it's sort of all mixed
up so we want to bring it downstairs
and straighten it out so what you do is
you expand the entire integrand in a
formal power series about the degree
zero part that's the part that doesn't
have any differentials so if you have a
function of the differentials then what
you do is you just expand that function
in a Taylor series and that Taylor
series is going to terminate actually as
a Taylor polynomial because of the
anti-community you can't have you know
you only have a sigh in a sidebar at
every point in lambda and you can't you
can't have a higher power than two
lambda so for example if you expand eetu
the tau tau was was this five five bar
Plus Size sidebar then you just expand
out the size sidebar so e to the size
sidebar when you expand it you just get
one Plus Size sidebar because if you
were to square it you would get zero so
you keep only fact the terms with the
one factor D Phi and one D Phi bar four
for each X and lambda and then write
those in terms of their real and
imaginary parts U and V and do the same
thing for the D 5 in the D Phi bar
you're gonna keep only the terms that
have one factor D Phi and D Phi bar once
you've done that big expansion up here
for each X in lambda and then you'll use
anti coming to do v2 to rearrange the
differential so that you can write them
as the product D u 1 DV 1 times D u 2 DV
2 and so on and then you have a lebesgue
integral and so then you just do the
integral so I mean in practice this is
not what you would do if you wanted to
evaluate one of these integrals that's
the way that it's defined but instead
what you do is you prove properties that
these integrals have that come out of
this definition and use those properties
in the analysis and those properties
tend to turn out to be really quite nice
so for example if I define s of lambda
to be the sum over X in lambda of that
form Tau Delta the one that had the
minus laplacian put in plus M squared
times tau then if I integrate any
function of tau that can be integrate
so this tau should be regarded as a
vector whose components are tau X so
it's it's a vector with whose components
is equal to the cardinality of lambda so
I have a function of tau at all of the
X's that that integral just turns out to
be F of zero no matter what F it is that
you're dealing with as long as the
integral exists and if you take as a
function that you want to integrate
against this e to the minus s of lambda
this is s of lambda here Phi 0 bar Phi X
then you will just get minus laplacian
plus M Squared inverse 0 X which is I
mean the minus laplacian was in here and
the M Squared is here so it's coming out
of the out of this s so what will happen
now is that we're trying to prove the
asymptotic behavior of the limit of this
as lambda goes to Z and nu goes to Nu
see we're going to work with the
right-hand side from now on and simply
forget about the walks from this point
we won't see any more walks
we'll just see the integral now I have
to do something a little bit technical
here for the moment but the idea is
simple it's a change of variables so in
that integral
I just want it's just it's just
ultimately a lebesgue integral I want to
make a change of variables which is to
scale and to introduce a scalar here
square root of 1 plus Z naught where
said naught is just some real number
greater than minus 1 then if you do that
you end up with this this formula so
this is just a simple change of
variables I won't belabor it but you
know you had a Phi bar and a Phi here
and together those are going to give you
a 1 plus Z naught that's popped out and
the Tau Delta and the Tau were quadratic
in Phi and Phi bar and D Phi and D Phi
bar and so they're also going to produce
a 1 plus set naught and this tau u
squared is Kordich in Phi and in D Phi
and so you'll get a 1 plus Z naught
square root to the fourth power which is
where this comes from now it's it's very
convenient very often in in statistical
mechanics to
introduce an external field and and I
want to do that here because that will
allow us to move this phi0 bar and Phi X
down from below up into the exponent so
well there's various notation here that
it's not so important to follow the
details of it but essentially what is
important is this Sigma Phi 0 bar plus
Sigma bar Phi X which has been
introduced so Sigma is a complex field
here well it's not really a field it's
just it's just a scalar it doesn't
depend on X it's a complex variable and
it's been introduced so that when we
write e to the minus s of lambda minus V
naught of lambda here then in that V
naught of lambda is this term and if we
differentiate that with respect to Sigma
and Sigma bar then what will happen is
we'll bring down the finite bar and the
Phi X and recover the formula that we
had here so that's sort of the
interesting thing which has happened in
this line
otherwise it's bookkeeping so there's
this s of lambda which has been isolated
here the reason for introducing this M
Squared is that the laplacian is not
invertible on the torus and so we have
to add a little bit of a term here in
order to make minus laplacian plus M
Squared invertible so M Squared is a
positive number here and you can see
that actually it's related to taking the
limit nu goes to nu C is related to M
Squared going to 0 so there was the
ultimately the limit as lambda goes to Z
and nu goes to Nu see that we need to
take that limit as nu goes to nu C will
now be replaced by the limit as M
Squared goes to 0 so essentially what we
want to show is that this green part are
the green this red part here of V naught
is like a small perturbation of s of
lambda in the sense that well that it's
not playing an important role if it
doesn't play an important role if we
were just to eliminate it then we can do
this calculation and it's related to one
of those properties that I showed you on
the previous slide
and this limit just turns out to be
minus the laplacian inverse on all of
zed D which has the decay that we want
so our problem really is to show that
this V naught is a small perturbation
now I want to introduce a kind of
expectation it's not an expectation like
in normally in probability theory but it
has many of the properties these
integrals behave very much like Gaussian
integrals ordinary Gaussian integrals
even though they have this sort of
thermionic apps' aspect to them with all
these differentials all over the place
so given a positive definite lambda by
lambda matrix C whose inverse is a will
define s a of lambda to be this kind of
quadratic form in in Phi and in D Phi so
the psy is basically a multiple of D Phi
and so we're introducing this extra term
here and then for a form F like the
kinds of things that we've been
integrating on the previous
transparencies the expectation of such
an F with respect to this covariant C
will just be defined to be this integral
and it has the property that if F is
equal to 1 that this integral will be
equal to 1 actually for any a this is
one of these marvelous properties of
these integrals is that they're self
normalizing so that this the integral of
e to the minus sa times 1 is just 1 no
matter what a is as long as it's
positive definite so it has at least
that aspect of an expectation so the a
that we're interested in is minus
laplacian plus m squared and then i
could rephrase what it is that we're
trying to compute in terms of this
expectation just by writing that
integral that we had previously which
was one of these integrals now as an
expectation and we really profit by
thinking of this as an expectation even
though we're working in much more
generality than as usual in probability
theory because we want to do conditional
expectations as well in which case these
this expectation will have the value
it'll be form valued
it'll be different its value will be a
differential form not not a number one
of the ways in which they behave like
ordinary Gaussian integrals is with
respect to convolution properties so I
want to use the abbreviation I'll write
Phi for the for the pair Phi Phi bar and
D Phi for D Phi D Phi bar just with the
different font and and just recall the
the basic fact that we teach our
students as undergraduates that if you
can take a normal random variable with
variance Sigma 1 squared plus Sigma 2
squared it'll have the same distribution
as the sum of independent normal random
variables with variances Sigma 1 squared
and Sigma 2 squared and that finds an
expression also for for these kinds of
Gaussian integrals that we're dealing
with here another way of saying the same
thing is that the convolution of two
Gaussian densities is a Gaussian density
and so here's a kind of expression of
that fact for these expectations that if
I define theta of F so there's been a
doubling of the field here whoo there's
the original field Phi and psy now
there's another field side boson field
and a de
Fermi field so we can add Phi and X I
and Ada and psy and evaluate F on the
sum like that so that what this integral
does is it integrates out the X I and
the ADA leaving Phi and X I fixed by an
upside fixed rather and then the second
integral integrates out the the Phi and
the up psy so this is a a version of
this fact up here about ordinary
Gaussian random variables which is going
to be quite useful for us because what
we're going to do is we want to evaluate
the expectation with respect to this
specific covariance C which is minus
laplacian plus M Squared inverse we're
going to decompose that covariance in an
intelligent way into a very large sum of
covariances and we're going to write the
original integral that we're trying to
compute as an iterated
and that intelligent decomposition comes
from a result of bridges gwah danny and
emitter from 2004 which they used in
their hierarchical work so works for any
dimension greater than two we fixed some
large L and suppose that the lambda the
volume this torus has side length L to
the N and n will be going off to
infinity L will be fixed and large let's
see be- laplacian plus M Squared inverse
actually they worked on on Z D it works
you can extend what they've done also to
the torus on Zed D you can take M
Squared to be 0 on the torus you cannot
anyway it's possible to decompose this
covariance as a sum of n covariances C 1
C 2 up to CN positive definite that have
the property of being finite range that
means that C J of X Y will be 0 if x and
y are separated by distance of order L
to the J and what that means is that the
fields at points x and y are going to be
independent as far as CJ is concerned if
they're if they have such a separation
and moreover except for the last one
which is special so CN is a bit special
I don't want to talk about that but
until you get there the the covariance
is obey very nice estimates so there's
this something called the dimension of
the field here which is 1/2 of D minus 2
in four dimensions you might as well
think about four dimensions the
dimension of the field is just one then
the covariance is bounded above so this
covariance sub J is bounded above by L
to the minus 2 times J minus 1 so the
covariances are getting smaller and
smaller and so the fields that are
distributed according to that covariance
under the gaussian measure what that
covariance would be getting smaller and
smaller and well more generally smoother
and smoother so if you every time you
take a derivative a discrete derivative
with respect that should be a why there
with respect to either one of the
entries of the covariance you get an L
the minus J minus 1 for every derivative
that you take as well so we'll use that
fact I want to use that decomposition of
the covariance you get a decomposition
of the fields this corresponds to
writing your gaussian measure your
gaussian random variable with variance
Sigma 1 squared plus Sigma 2 squared is
X 1 plus X 2 so now both Phi and D Phi
become decomposed though so that that's
something that you have to prove that it
works also for these kinds of Gaussian
expectations that I'm talking about now
but it does so you can decompose the
field like this in such a way that this
expectation that you want to take is
actually the convolution or the
composition rather of these expectations
where expectations sub c1 will integrate
out X I 1 and DX I 1 and so on second
expectation will integrate out excite 2
and DX I 2 and by the time you're done
there'll be nothing left so we want to
write Phi J as what is left to integrate
after you've done J of these operations
and then you can well just by this
definition you can write Phi J's excise
a plus 1 plus what's left after that one
and then we'll define these sort of
partition functions Z 0 is e to the
minus v-0 that's what we initially have
to integrate with respect to C and here
we have a kind of progressive way of
doing the integral where it's it's only
been partially done the first j of them
have been done here and we'll call that
Z J it will depend on Phi J what remains
what hasn't been integrated out and the
differentials that go with that and so
what we need to do is to compute Z n
which is the full expectation and so
we're led to study the so cauldron or
Malaysian group map which is the map
that takes ZJ to Z J plus 1 by doing the
next expectation so that's what we have
to study and this gives rise to a
dynamical system
now when you're looking at a dynamical
system you want to know which directions
are expanding which directions are
contracting which directions are
marginal and not doing either one and
it's it's here that the dimension four
comes in in fact so I just like to say a
word about that let's focus first of all
about D equals four you look at those
covariance estimates for CJ plus one
that suggests that this field that
you're going to be integrating out with
that covariance is of size L to the
minus J because the covariance was was L
to the minus 2j and the covariance is
just the it's the variance because these
fields are centered so this field should
have size which is like the square root
of the covariance which is this L to the
minus J and moreover because of the
smoothness that field will be roughly
constant on a block of side length L to
the J because the covariance was its
derivative was was was very small so if
you look at a block be on in lambda of
side length L to the J so this is a cube
then if you look at the size of that
field raised to the P power summed over
that block well the fields roughly
constant and it's of size L to the minus
J we're taking the peeth power there's
this many terms in the sum that many
means L to the 4 times J and so this is
L to the J times 4 minus P and you see
that this is an expanding direction if P
is less than 4 this is going to be
blowing up as J increases if P is less
than 4 it'll be marginal it'll be order
1 if P equals 4 and it'll be irrelevant
it'll be getting smaller as J increases
if P is greater than 4 and well tau was
like 5 squared tau squared is like Phi
to the fourth and so those were the
terms which were showing up in the in
this v-0 the this potential that has to
be whose exponential has to be
integrated
so this is in a way as explaining why we
we don't ever see any tile cubed or at
out of the fourth or higher order terms
and in fact if you take other symmetries
into account including the supersymmetry
which is a sort of symmetry between the
thighs and the defies then you find that
the only relevant and marginal monomials
are precisely the ones that shows up in
in the V zero so this is somehow saying
that this V zero has put its finger on
the right things to be looking at and
the role of D equals four shows up
because if you look at tau squared which
is what multiplies G that strength of
the self avoidance you find that the Tau
squared term is is relevant if D is less
than 4 in irrelevant for D greater than
4 if you do the calculation this is how
it works out and so what this means is
that D greater than 4 is an easier
problem than D equals 4 and D less than
4 is a harder problem all right so we
want to look at this map a sub C J which
is taking us from Z J minus 1 to Z J
let's just try and talk about the first
one before we move on to the general one
so what this map does is takes a
function of Phi which is Phi 1 plus 6i 1
to a function of Phi 1 by integrating
out the excite 1 and also the DXi 1
so let's write Z naught at a single site
now as I naught of X which is e to the
minus V naught of X V naught was this
polynomial in tau X and tau x squared
and Tau Delta X and for a subset of
lambda define isay row of that subset to
be the product of the I zeros which
because we're talking just about an
exponential here will be e to the minus
the sum over little X in capital X of V
naught of little X and I'm writing that
in this way so this is a function of Phi
V not depends on tau which is a function
of Phi and D Phi now suppose I had some
other polynomial V 1 which is a version
of V naught whose coupling constants are
different so instead of G naught nu
naught and Z naught I have some other
ones which
just get to pick I want to pick them
eventually in an intelligent way but
they can be anything for the sake of the
argument that I'm presenting right now
and I want to regard this v1 as being a
function of Phi 1 so essentially what I
want to do is I want to think that when
I take the expectation of e to the minus
V naught of lambda I'll get e to the
minus v1 of lambda now I won't get that
there'll be some Corrections and I'm
trying to now investigate what is the
nature of those Corrections and as they
all have to be controlled so let's
suppose that we had some good guess for
what v1 would show up if you like it's
just the log of the expectation of e to
the minus v-0 and we're trying to
approximate it by a polynomial let's
suppose that we have some good guess for
it and then let delta i-b the difference
between a 0 at Phi 1 plus x i1 and i1 at
Phi 1 then we can take our partition
function z1 of lambda which is this
expectation of ID 0 and write it as the
expectation of the product of I 0 and
write I 0 as i1 plus Delta I 1 because
that's what it it turns out to be here
then this product can be evaluated so
when you take this product basically you
decide when do I take i1 when do I take
Delta I 1 that decision is is being
recorded by this capital X here which
tells me when I took the Delta I 1 and
this expectation only acts on X i1 which
shows up only in the Delta and so I can
pass it through the sum and through the
i1 and write it in this form and we end
up with what's called the the circle
product representation so this Z 1
lambda can be written now in this form
so here's the formula that I had a
moment ago what I'll do is X is
represented by these little squares here
and we'll go to the next scale so p1 is
polymer's on the next scale which
consists of blocks on this large scale
here so p
some collection of blocks and I'm just
essentially performing this some by
conditioning on what is the set of
larger blocks which contain these points
so I hear you is that set of larger
blocks this I 1 is in the background and
K 1 will be what it needs to be in order
to get that result so we're moving on to
the next scale and this formula is an
instance of a circle product so if F and
G are our forms which are associated to
polymers so pj are an element of pj is
just a subset of the set of all blocks
of size L to the J and what we've got to
here is a kind of a convolution which
shares lambda among F and G this defines
an associative and commutative product
and it's possible to write Z naught and
Z 1 in terms of this circle product and
I think I'm running short of time here
so I'll have to speed up a little bit
but here's the theorem I'm stating it
for D equals 4 there's a choice of
coupling constants so g j nu jz j lambda
j QJ which determines IJ according to
some formula so IJ is kind of like a
finite dimensional part of the dynamical
system that we're following in detail KJ
is an error term such that if we write
ZJ in terms of circle product of I J and
K J then when we move on and take the
expectation the new Z is again a circle
product of J plus 1 and K J plus 1 where
I J plus 1 is given in terms of I J by
this dynamical system
and so what we need to do is to study
that dynamical system and we prove a
fixed-point theorem for that dynamical
system which says that there's a choice
of initial condition Z naught which is
something which shows up in the constant
in the 1 over x squared decay of the to
point function in four dimensions and nu
naught which is what actually is putting
us at the critical point such that the
solution of this dynamical system in the
limits is driven to zero this is what
physicists call infrared asymptotic
freedom they're they're very good at
inventing names for things and from this
we and from well other ingredients which
I don't have time to explain we can
simply do the calculation of the
two-point function here's the formula
that we had for it initially and from
that integral representation by the time
you get to scale n when you do the
circle product it's rather easy because
it's a way of dividing up the space
between I and K but on scale n there's
only one block and so either I gets at
all or K gets at all and K we show is
essentially zero in the limit and I once
all of the integration has been done has
no field left in it and it's possible to
do this calculation and what we find is
that the limit is given in terms of the
inverse Laplace ian's on Zed for which
has the decay that we want to prove
thank you very much
so here the critical dimension is really
for it's not four and a half it's four
right sir is there a universal reason
why crystal lemons must be introduced or
mana and also they're not and there was
something that I skipped over here that
I really ought to mention find it yeah
so what you can do is take as your
underlying random walk not the nearest
neighbor walk like I did but but take a
long-range walk so that the you know
that's something that would be
converging to a stable law so that the
weight associated to a step from here to
a point X which is distance R away would
be like 1 over R to the D plus alpha
then if you as you vary alpha then you
have the effect of actually varying the
critical dimension and there's a formula
for what the critical dimension is in
terms of alpha I think it's 2 alpha and
there there's something quite
interesting here that I didn't mention
by Mitterrand scope ala paper in 2008 so
they did that with the choice alpha
equals 3 plus epsilon over 2 that model
has critical dimension 3 plus Epsilon
and they actually work in dimension 3
which means that they're slightly below
critical for for that model and are have
taken the first steps in constructing
this renormalization group flow in that
context so so that's quite interesting
it it it's analogous to studying the
nearest neighbor model in dimension 3
point 9 9 9
any other questions
evolved Canadian desert seas maybe the
simplest case would be bigger house
table walk where the peripheral
dimension is one
I don't think it's easier so in fact
I've talked to emitter about this and
and if you play with alpha then you can
make this DC to be essentially anything
less than four that you want and if you
so if you choose the right alpha well I
guess it would be 1 plus epsilon over 2
so that DC would be 1 plus Epsilon then
the problem is equally difficult so it
doesn't get any easier no that's right
the result the result should be true for
all g+ but our method is definitely
restricted to small G yeah
okay let's take this meter</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>