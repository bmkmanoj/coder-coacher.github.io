<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cluster Trees and Neighborhood Graphs | Coder Coacher - Coaching Coders</title><meta content="Cluster Trees and Neighborhood Graphs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cluster Trees and Neighborhood Graphs</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JySuW1TFeBw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'll be talking about
a bit about clustering and neighborhood
groves and so let me start with the
clustering part okay so what is
clustering you're given a bunch of
points and you want to divide them into
groups okay and now this is something
that arises in sort of two rather
different contexts okay
the first of them is vector quantization
which is integral to say audio and video
coding so over here the data points are
typically samples from some vast
underlying space typically a continuous
space okay for example each data point
might be a small sound snippet from the
space of all possible speech samples
okay and the goal in these cases is to
find a small set of representatives a
small finite set of representatives for
this really large underlying space okay
in audio coding for example you want to
find a small set of representative
signals and subsequently every sound
will be approximated by its closest
representative okay in such cases it's
completely irrelevant whether there are
any natural clusters in the data nobody
cares
all you want is to get good coverage of
this space okay you want to find a bunch
of centers that cover the space nicely
so I'll be talking about sort of the the
differ a different way in which
clustering is used and perhaps a more
common way in which is used where the
goal is to find meaningful structures in
the data
okay so sort of natural groups that
exist in the data and you know just to
give you another example from you know
this this domain of of sound signals and
so on when when a baby is you know first
born and
you know as you know day one it's you
know it hears all these utterances from
its mother and father and so on and
initially these audiences basically form
a continuum but by the time the baby is
six to nine months old
it has clustered these sounds which were
initially a continuum into the phonetic
categories of its mother tongue and
these are natural clusters and in the
sound space you know 40 to 50 clusters
okay 40 to 50 phonemes so that's the
kind of clustering that I'll be talking
about finding natural groups and data
and you know this arises all the time in
applications for example if you have a
new data set and you want to do
exploratory analysis of it
one very useful thing is to see if there
are natural groups in it or you know if
you have a company and you want to find
natural Coheed cohesive groups of
customers for things like targeted
advertising or you have a data set and
you want to get a clustering of it that
you are going to use subsequently for
semi-supervised learning or active
learning okay anyway so there they're
only sort of sort of endless situations
in which it's used and so you know let's
just look at this picture so we want to
find the natural clusters in the data
what are they
you know like like how many are there
are there two clusters it could be you
could have one here and one there or are
there three like maybe there could be
three you know or maybe more it's hard
to tell
right it's hard to tell what the right
number of clusters are and in fact there
are these these various criteria that
will give you a number
so there's minimum description length
and bayesian information criteria and
they'll tell you what the three clusters
over here or something like that but one
look at this picture should convince you
that you should be really skeptical
about such measures okay
there's obviously no right answer okay
and so what we'll be doing is we'll just
be looking at hierarchical cluster
so the nice thing about a hierarchy is
that it contains all levels of
granularity okay so at the top level you
have all the points if you want to
clusters you have that if you want three
clusters you have that if you want four
you have that and so on so we look at a
case where you build an entire hierarchy
over the points okay now this might seem
more difficult than flat clustering
because in flat bickers you know in
order to build a hierarchy it seems like
you have to do a flat clustering at
every possible granularity and then
somehow massage all these clusterings
together into a coherent tree okay but
actually in many ways hierarchical
clustering can be easier because it
avoids making some choices that are
difficult or almost impossible to make
choices like what is the right number of
clusters okay and so that's something
that keeps popping up again and again in
a lot of unsupervised learning that when
you move to sort of a hierarchical view
of things you often end up avoiding a
lot of difficult choices okay okay so
we're going to be doing hierarchical
clustering so you know as you know
there's lots of clustering algorithms
out there k-means em etc etc okay
one algorithm for hierarchical
clustering that's pretty well known is
the single linkage algorithm okay and
the one of the remarkable things about
this algorithm is that it's used in both
statistics and computer science
communities for completely different
things
the statisticians call it single linkage
and use it for hierarchical clustering
the computer scientists call it
kruskal's algorithm and use it to find
the minimum spanning tree of a network
okay so it's exactly the same procedure
used for two rather different tasks okay
and so let's look at what this procedure
is okay so this is due to Kruskal who
passed away a few years ago okay so this
is the way it works this is the way this
hierarchical clustering algorithm words
many of you are probably familiar with
it but let's go through a quick example
so over here you have 10 points
okay you start by merging the two
closest to them okay and so now you have
one cluster with two points in it then
you merge the next pair so now you have
two clusters with two points then you
merge the next pair and now the next
closest pair of points is this point and
this point so you end up getting a
cluster containing three points okay and
you proceed in this way and then you
have a hierarchy okay so at the top
level you have all the points if you
want to get two clusters you just cut
the longest edge and you get two
clusters if you want to get three
clusters you've got the next longest
edge and you get three clusters and so
on okay and you can get as many clusters
as you like so it's a nice simple
algorithm and so the question is is it
any good is it a good algorithm and
that's something that has been a little
bit difficult in clustering in general
you know there are there's a whole slew
of clustering algorithms out there and
it can be hard to tell which ones are
better than others okay so because of
this there's been a whole slew of theory
work where you know what has happened is
that people write down some properties
that they think good clustering
algorithms should possess and then they
analyze clustering algorithms to see
what which of these properties they
actually possess so this is actually an
enterprise that started a long time ago
in the 1970s mathematical biologists who
build these phylogenetic trees they
started with this this whole axiom of
property based analysis of clustering
and in 2001 John Kleinberg resuscitated
this so Kleinberg came up with three
axioms or three properties that every
clustering algorithm should have and he
found that actually there is no
clustering algorithm that has all three
properties but the one that comes
closest is single linkage and you know
now after that you know people came up
with other sets of properties and yet
again and again single linkage seems to
keep cropping up as you know a really
good algorithm but in practice it's
deeply flawed okay it runs into a lot of
problems
and so you know the issue is what is
wrong with all of this you know what is
wrong in all of these theoretical models
why do they keep suggesting single
linkage as being you know the right way
to hierarchically cluster and what is
wrong with them basically is that they
completely ignore the statistical aspect
of the problem and as soon as you view
clustering statistically you see what's
wrong with this algorithm okay and so
let me explain what I mean okay so when
you're clustering and data points these
are not n arbitrary points that have
materialized out of nowhere rather they
are n draws from some underlying
distribution from some unknown
underlying distribution for example the
distribution of utterances from a mother
to her child
okay so there's some distribution we
don't know what the distribution is but
the points that come from that
distribution and the mere fact that they
have been generated in this way gives
them special properties okay so in
particular one thing we would hope is
that if we got two samples of points to
random samples from the same
distribution and we clustered both of
them they would each yield similar
clusterings of the underlying space okay
so that's one property that we'd want at
least if the number of clusters were
large okay and further along that line
as you get more and more points you
would hope that this clustering you have
eventually converges to some underlying
clustering to some clustering of the
underlying space okay and that's the
property that we would call consistency
okay that as the number of points grows
these clusterings that you get the
clustering with n points n plus 1 points
and so on these clusterings actually
converge to something they converge to
some partition of the space ok so that's
that we would call consistency and we'd
also hope that this thing that they're
converging to is something meaningful
okay that's something that really tells
us something about the underlying
density from which the point should come
okay
now if you work in classification then
all this talk about consistency will
seem really obvious and naive and the
reason for that is that in
classification we have a very
well-developed theory of consistency
okay using large deviation bounds on the
VC dimension and all that kind of stuff
but in clustering the situation is much
more primitive and consistency has
proved to be rather elusive okay so just
as an example like if we take the
k-means algorithm this is probably the
most popular clustering algorithm in the
world
okay k-means so suppose you were to run
k-means on and you you're getting points
from some underlying distribution you
run it on endpoints then you run it on n
plus 1 points then you run on n plus 2
points do you converge to something you
know does the clustering converge it
turns out that the only thing is no the
only that's known is that the answer is
yes if you're clustering algorithm
always finds the global optimum of the
k-means cost function now that's a tall
order because optimizing the k-means
cost function is an np-hard problem even
for just K equals 2 even for two
clusters ok but even supposing you were
able to do this the thing that it
converges to is simply the k-means
clustering of the underlying density of
the ok and it's not at all clear that
that's meaningful or interesting ok so
basically the statistical theory of
clustering is pretty dismal all in all
and and with that backdrop single
linkage is actually not too bad
it is trying to converge to something
and the thing that it's trying to
converge to is something actually very
sensible and so let me tell you what it
is tell you what it's trying to do what
is it trying to converge to so here is
an example of a density ok the sort of
density from which you might be drawing
points I think most people who looked at
this density would agree that there seem
to be four clusters over here ok there
are these four humps over here and the
only thing that's unclear is where
exactly we draw the
like is this a cluster or is that a
cluster okay you know well the good
thing is that you know since we're
talking hierarchically we don't have to
make any of these choices okay this is a
cluster and so is this and so is this
they're all clusters okay
and so we can just define clusters in a
very simple way okay we say pick any
lambda pick any density level lambda and
look at the region where the density is
more than lambda okay so you're gonna
get a few connected components any of
those connected components are bonafide
clusters that's a valid cluster okay so
you've got some density you cut off the
density at some point and you get a
bunch of connected components that's a
bona fide cluster you pick a different
density level you get other clusters so
there you get a whole infinitude of
clusters but they form a hierarchy and
we call this hierarchy the cluster tree
and that is what single linkage is
trying to converge to it's trying to
converge to this cluster tree so this is
just it's just the the usual you know
path connectedness you know that there
is a path from one to the other so you
have this region where let's say the
density is continuous so you have a
region and you know it's just are they
connected
is there is there okay so just by way of
example let's say we have this one
dimensional density okay so we can pick
any density level let's say we pick
level lambda one at that level there is
just one cluster this is the cluster
right there okay we can pick a different
density level lambda two at that level
there are two clusters this cluster of
points this disconnected component of
points with density more than lambda 2
and this connected component with
density more than lambda 2 at lambda
three there are also two clusters and so
on okay and so there are infinitely many
density levels and so there are
infinitely many clusters but they form a
hierarchy
if you look at any two clusters they're
easy to either disjoint or one is nested
inside the other so that means that they
form a tree and so we get this infinite
tree called the cluster tree so I said
that single linkage is trying to
converge to this thing okay so what does
that mean and you know like in what
sense is it trying to convert so let's
say we draw a sample from from this
distribution and let's see what single
linkage is gonna do on that sample okay
so it's gonna start by merging the two
closest points okay so it'll merge 2 &amp;amp; 3
and we can think of that we can think of
this cluster is being a finite
approximation to that cluster right
there okay then it merges in the 1 and
we can think of this as a finite
approximation to this cluster okay or we
can think of this subtree as a finite
approximation to this cluster then it
merges those and we can think of this
subtree as a finite approximation of
this cluster right here then it merges
in a 4 and we can think of this subtree
is a finite approximation to this
cluster and so on okay and so it really
does seem like the tree that single
linkage is producing is somehow
consistent with the cluster tree ok just
sort of anecdotally but we have to say
precisely what we mean by consistency
and hardigan came up with in the in the
70s came up with a very sensible
definition okay and you know it's kind
of there's a lot of math over there but
let me just tell you what the definition
says okay I just tell you in words okay
so this is what consistency means what
does it mean for a hierarchical
clustering algorithm to be consistent
well what it means is that take any
density level like let's say we take
this level and look at any two clusters
like let's say this cluster in that
cluster okay so look at two of these
clusters now you have a finite number of
points and you're gonna hierarchically
cluster them what you want is that the
points that fall in this region and the
points that fall in that region
end up in disjoint sub trees that's what
you want okay so the points that fall in
this region they'll be in this case four
of them and the points that fall in that
region
end up in disjoint subtrees and indeed
they do this is one subtree and that's
another subtree okay so that's what
consistency means for hierarchical
clustering that's hard against notions
of consistency okay and we can see that
in this case the single linkage
algorithm really is consistent and this
is just with six points as far as the
definition goes we just need this to
hold as the number of points goes to
infinity okay now it turns out that this
example is not just a coincidence okay
Hartigan's showed in 1975 that single
linkage is in fact consistent in one
dimension okay so no matter no matter
what the underlying distribution is
there is some infinite cluster tree
associated with that distribution and
when you run single linkage on larger
and larger datasets you will converge to
the close to tree this is one of the few
nice results of the statistical theory
of clustering okay so we have this
algorithm single linkage that is
actually converging and what's more it's
converging to something that's
meaningful and sensible okay so this was
a really nice result in clustering and
so the next natural question is what
happens when the dimension is more than
one okay and unfortunately the news
becomes very bad immediately okay so
seven years later
Hartigan the same guy showed that even
when the dimension becomes two okay
single linkage stops being consistent
okay and he did this via a reduction to
continuum percolation which and I'm
gonna talk a little bit more about
percolation later on okay it's a very
beautiful argument but basically the
problem is the following okay so let's
say you have two high density regions
these like this and this these are
clusters basically right what we want
single linkage to do is to connect all
points in this region and to connect all
points in this region before it creates
any links between the two regions okay
that's what we want from consistency but
that's actually a tall order when the
dimension grows
because it's really easy to have a point
close to the boundary here and a nearby
point close to the boundary there and
just have that bridge created before
this side has been fully connected or
before the other side has been fully
connected it doesn't happen in one
dimension but as soon as the dimension
exceeds one it happens inevitably okay
and so single linkage stops being
consistent and so at that point Hartigan
you know pose this open problem find a
way to do consistent hierarchical
clustering in higher dimension and
that's something that we figured out a
few years ago four or five years ago and
since then there's been a lot more work
along that line and so that's what I'll
be talking about today okay so we found
that in fact you you can do hierarchical
clustering you can actually converge to
the cluster tree and the algorithm that
you use to do that it's just a very
small variant of single linkage it's
almost the exact same thing you just
have to tweak it a very little bit and
to see how you need to tweak it you just
need to look at it in a different way
you need to think of single linkage in
terms of neighborhood graphs okay and so
I'll talk about that yeah right so all
that happens is that it's much more
likely in in two dimensions you know
it's it's much easier for it to happen
it's because in one dimension these
boundaries between clusters are tiny
whereas in in two dimensions they
suddenly become much larger right
and so the prop and as n goes to
infinity the probability of one goes to
zero and the probability of the other
goes to one okay right so let's talk
about neighborhood graphs okay and so
these are these are really popular in
machine learning
a neighborhood graph is just a way of
construe
of capturing the the sort of local
geometry of a data set okay so you want
to capture the local structure of data
so what the way you do that is you
create a graph with a node for each data
point and you put an edge between points
that are close by and so you start with
this data set that is high dimensional
and chaotic and hard to understand and
you end up with this really nice clean
mathematical object okay and then that's
the starting point for what you want to
do next okay so if you want a cluster
you can look for small cuts in this
graph if you want to do semi-supervised
learning you label a few points in this
graph and then you propagate labels
along edges if you want to do manifold
learning you find an embedding of the
points that respects the neighborhood
structure shown in this graph and so on
and so forth okay so these are very
popular objects you take this
complicated data set and you make it
into this really nice graph okay so that
sounds very good and so the question is
how exactly do you build this graph like
which or how do you decide where to put
the edges when it turns out that they're
two popular ways to do it yeah Oh
regularization yeah so you know so for
example if you're trying to learn a
function over you know over the space
what you want is the function should
have similar values at neighboring
points okay so if you're learning a you
know just you just want to learn some
arbitrary function okay like
nonparametric regression you can
regularize it by asking that the
functions value at this point and the
functions value at that point are very
close together okay so that's an example
okay so here are two popular ways
there's basically two popular ways of
constructing these graphs and the reason
that there are two of them is that
neither one is entirely satisfactory
okay so it's just a question of finding
the lesser of the two evils for you know
via the task at hand okay
now the first way is to join any two
points whose distance is less than some
threshold are those are the points you
can end the second way is to join two
points if there are K nearest neighbors
of each other okay so these are the two
ways and let's see what their
shortcomings are okay so the problem
with the with the first method is that
also
you have clusters at many different
scales okay so you can have a situation
like this where there are three clusters
at different scales okay so let's see
what happens suppose you set R to be
something small well then you'll get
edges inside here but you might get no
edges at all inside this big cluster on
the other hand if you set R to be larger
you might get it you'd get edges in here
but you might end up merging these two
clusters okay so you just end up in
trouble either way now this kind of
multi scale structure is not
pathological it is the norm okay so if
you if you for example think of the the
MS data set of handwritten digits you
knows you have a cluster of zeros a
cluster of ones and if you actually look
at the radii of those clusters they're
all very different
you know the cluster of ones for example
there's not much variation in the way
people write once so that's a really
small cluster that looks like this where
is something like the eight looks like
that you know so multi scale structure
is the norm we have to deal with it
okay now this other scheme where you
connect each point to its K nearest
neighbors the hope is that those nearest
neighbors are going to be in the same
cluster okay and I think it would do
okay on this data set we will probably
do fine here but then you run into
another problem okay in large data sets
it's very common to have points that are
outliers or at any rate are outside the
central region of each cluster the
minute you have something like this you
could have these spurious connections
with this point which is a bit of an
outlier ends up you know being connected
to both this and to that and then later
wreaks havoc with whatever it is that
you're trying to do okay so anyway so
both these both these methods have their
problems and of course there is the
additional problem of having to figure
out exactly what R and K you should use
but the high level issue over here is
that the high level problem is that
you're trying to take something that's
multi scale and representing it in a
flat graph okay and that's just
inherently difficult okay so what's a
way around it well instead of just
creating one flat
how about creating a hierarchy of nested
groves okay that should get around this
problem and in fact that's exactly what
single linkage implicitly does okay and
so let's look at single linkage in that
way so here's the single linkage
algorithm and another way to think about
single linkage is that it's got this
knob that starts at zero and the
algorithm slowly turns the knob until it
reaches infinity this knob is the scale
okay and let's call it R okay so when
the knob reaches are what you do is you
join together any points that are a
distance R or less from each other okay
you join together those points and you
output the connected components that you
find okay and any singleton components
you just ignore okay and then when the
knob goes a little higher then you throw
in some more edges and then you output
those connected components and so on
okay so single linkage really is
implicitly creating this hierarchy of
neighborhood graphs okay now another way
to just sort of think about it is that
as this knob is being turned initially
none of the points are active okay the
only time a point X I becomes active is
when the knob reaches R of X I which is
the distance from X I to its nearest
neighbor when the knob reaches that
value all of a sudden X I is activated
and thereafter X I is always going to be
in the graph okay so each node becomes
activated at some point thereafter it's
always in there okay now and so this is
a nice algorithm but we saw that it's
not consistent so what do you need to do
to it to make it consistent and it turns
out that you need to change almost
nothing okay so let me show you the the
amended version okay so this is regular
single linkage and this is the amended
version I've literally just included
four more letters okay so what are the
changes okay so the
first change you make is that I said you
were turning this knob and a point X I
becomes active when the scale reaches R
of X I the distance from X I to its
nearest neighbor what you do now is you
look at the distance from X I to its K
the nearest neighbor okay this just
makes everything more robust and you
know we'll see why in a second okay so
each point gets activated a little bit
later okay
instead of when the knob reaches the
distance to its nearest neighbor you
wait till the knob reaches the distance
to its Kate's nearest neighbor the other
thing is when the knob is at are you
link-up all points a distance less than
R but you also link up a few more points
you also link up points whose distance
is less than alpha R and alpha here is
square root 2 okay and if you make these
changes then then this is consistent in
any dimension okay so this is a slight
modification of the algorithm and it
becomes consistent okay now now
consistency is an asymptotic property
okay it tells you what happens as the
number of points goes to infinity okay
and now of course in reality so we're
only working with finite data sets and
therefore you know we really care about
what happens for a finite number of
points okay not just as the number of
points goes to infinity we want we want
to know what happens when you have n
data points you know what kind of tree
does it return and then okay what
clusters does it discover then okay so
what we know is as the number of points
goes to infinity all the underlying
clusters will be found okay everything
is going to be found as n goes to
infinity but what we might expect is
that if there are some clusters that are
particularly prominent okay or if there
are some clusters that are salient then
they would be found pretty early on even
with very few points okay and so all we
need now is a notion of cluster salience
and using that we'll be able to get sort
of finite sample results okay
so so what we need to do is to define
how prominent a cluster is so there's
some cluster in the underlying density
in the previous you know early on we saw
those four humps in general they're not
going to be that salient okay but we
want a notion of how salient a cluster
is and this might seem like a
complicated thing but actually it's very
simple to characterize there are just
two effects that we need to take into
account okay so let me tell you what
they are this is the first effect okay
so this is a cluster and it's got high
density throughout okay so this is a
high density region but it's got two
sides that are connected by a narrow
bridge and so what will happen is that
in a finite sample if you have a small
sample it will probably just look like
two separate clusters okay so this is an
example of a high density cluster that
is hard to discover in a finite sample
okay and to deal with this we will also
talk about a buffer zone around each
cluster okay so for any set Z like this
cluster over here and any number Sigma
like point one Z sub Sigma is this plus
a plus a buffer zone of size point one
around it okay
so we'll use that definition so that's
effect number one and the second effect
is this so here we have two bimodal
distributions they both have two
clusters but in this case the dip
between the two is much more pronounced
than in this case so these two clusters
should be it should require much less
data to discover that there are two
clusters here than in that situation
okay so these are you know simple
observations and it turns out these are
the only two effects that one needs to
account for okay and so we'll define a
notion now of how separated clusters are
okay so I'll say two clusters are Sigma
Epsilon separated so here are two
clusters okay now we're not making any
assumptions about the density so the
clusters could be weird-looking and
that's why I've drawn this kind of
strange-looking thing over here okay so
yeah you take any two clusters and we
say that they're sick
Absalon separated if first of all
they're separated so there's some set s
that separates them and what I mean by
separation is that any path from a to a
prime has to go through s okay so that's
the separator set okay but moreover the
density along the separator is less than
the density in a in a prime so the
separator in is in a lower density
region even if you put a buffer zone
around everything okay even if you put a
buffer of Sigma around everything you
have this you have this gap in density
okay so the main thing to notice over
here is that no matter what the clusters
are they are separated for some
sufficiently small Sigma Epsilon okay
you look at any two clusters there will
be some if you any if you make Sigma and
epsilon small enough they will be
separated okay but if you have two
clusters for which the separation is
larger for which Sigma and epsilon are
larger then you will discover them with
just a very small sample okay and so
this is the result over here that that
one can show and and so here's well
here's what you can say that that
modified single linkage procedure I told
you about has the following property
that it and this holds uniformly for any
two clusters a na prime if you have two
clusters that are this much separated
and their minimum density is lambda then
after the number of points exceeds this
the two clusters will be separated in
other words they will show up in
distinct sub trees of the hierarchy okay
so we know that as n goes to infinity
they are going to be separated but the
question is how large does n have to be
before these two clusters will appear in
distinct sub trees of the hierarchy and
that's how large n has to be and you can
also show that that is as good as it
gets
okay and I'll say something about that
later okay and so and so one way to
think about it is that at that value of
n the hierarchy has these two clusters
and distinct sub trees the other way to
think about it is that the algorithm is
spitting out all these nearest neighbor
graphs and so there'll be some scale
are at which the graph has the following
property a isn't within one connected
component of the graph a prime is within
a different connected component of the
graph okay so that's another way to to
characterize that and so why does this
hold and this is a an exercise and
continuum percolation and so I'll just
talk a little bit about this okay so so
I'll just talk a little bit about
connectivity in random graphs yeah there
is any link between them they could all
think there are sort of separate numbers
they capture two separate things they're
just going to be small positive numbers
let's just look at the definition so
Sigma is the width of this buffer zone
and epsilon is how much smaller the
density is here than here okay so let's
say the density over here is a hungred
and the density in this separator zone
is 50 okay
then epsilon is a half because it's half
the density yeah
so the right so the k-means global
optimum however is a different
clustering so it's not the k-means is
not attempting to find the cluster tree
k-means actually is attempting to solve
a problem that's more in line with
vector quantization so it's got a
particular cost function where it's
minimizing the squared distance from
each point to its closest representative
and that particular cost function
happens to be NP hard to dissolve
optimality
unfortunately not yeah
k-means is actually much more suitable
in the vector quantization context than
in the other context of finding
meaningful structure yeah even though it
gets used in both cases yeah any other
questions okay so um so let me talk a
little bit about connectivity and random
graphs so I think most of you are
probably familiar with or any random
graphs because of the kind of random
graphs we usually encounter in
combinatorics and computer science okay
so in these random graphs you have n
nodes so you start by plonking down n
nodes and for each pair of nodes you
flip a coin and with probability P you
put an edge between those two nodes okay
so the randomness lie lies in the edges
there are n choose two random choices
you choose you flip n choose two coins
independently with bias B okay and you
get this random graph and the sort of
question that one asks about these
graphs is how large for example this P
have to be in order for the resulting
graph to be connected or to have a
component that's very large okay these
are the kind of questions that we would
call percolation now that's very
interesting but well in statistics the
kind of graphs that we come up with are
usually not of this kind but rather
they're random geometric rats
okay so what a random geometric graph is
that there's is the is the following we
have some unknown density and we
generate endpoints at random from that
density and then we put an edge between
two points if they satisfy some
criterion like they'll distance less
than one apart or something like that so
that's a random geometric graph okay
it's also a random graph but it's very
different in flavor the randomness is
not in the edges the randomness is
entirely in the nodes themselves okay
the points have been picked at random so
the amount of randomness is actually
order n
versus order n-squared so it's a
different kind of random graph it's been
studied significantly less but the kinds
of questions one is interested in are
exactly the same kinds of questions you
know like at what point do these graphs
become connected and so on
and so these are the kind of random
graphs that we will be dealing with okay
so when you're generating points from an
underlying distribution you inevitably
end up with a random geometric graph
rather than an area graph okay so yeah
so let's let's go back to the algorithm
and so what single linkage is trying to
do well what any of these algorithms are
trying to do is that initially they're
focusing on the high density regions
okay so single linkage the first thing
single linkage does is to take the two
closest points and merge them together
okay and it's hoping that this is the
highest density region of the underlying
density okay and that's you know it's
reasonable to see why it's hoping that
but the problem is that you know the
problem is sampling error it's possible
that the you know just by chance the two
closest points might end up being in a
part of the density that's actually far
from the highest region okay such things
can happen and one mistake of that kind
will completely derail the algorithm
okay so we need to make it a little bit
more robust and that's why instead of
the distance to the nearest neighbor we
look at the distance to the K nearest
neighbor okay so to to understand this
it's it's useful to look at large
deviation bounds okay so that mixture of
ananka's bounds for ball so it's let's
say we're in Euclidean space of D
dimensions okay what vc bounds tell us
is that in our D dimensional space okay
or in this room is a three dimensional
space if you look at any ball in the
space there are infinitely many balls of
course look at any ball of any radius
that ball if you will contain
roughly the expected number of points
okay plus or minus some error bar but
the error bar is very small okay and so
okay and so the remarkable thing here is
that this holds uniformly over all the
balls even though there are infinitely
many of them okay the key thing is what
the error bar is in the case of these
balls the error bar is about d log n
okay so if you have n points each ball
will contain the right number of points
under the underlying density okay so if
the density is F the expected number of
points is the probability mass of the
ball times n ok so that's the expected
number of points and the number of
points that you will actually observe is
that plus or minus D log n okay and this
isn't the case where okay and and what
that means is that since the error bar
is d log n we really should not look at
balls but less than D log n points okay
and so that's a reasonable setting for K
so there's no point at looking at just
your nearest neighbor you should look at
the if that's not a significant value
that's not a significant radius you
should look at the radius at which you
have at least D log n points and that's
significant that overwhelms sampling
error okay so that's the first thing
okay so now let's see why this holds and
you know I just wanted to give you a
quick flavor of the kind of arguments
that arise over here so we have these
two weird clusters okay and we're
turning this knob and all we need to say
is that these graphs you know is that
and whenever the knob reaches some value
certain points are active okay and as
the knob keeps turning more and more
points become active and at each setting
of the knob you put some edges okay and
we just have to argue that there's some
point at which this all the points here
are active and connected all the points
here are active and connected and
there's no way to get from here to there
okay and that means that you these these
points will lie in disjoint subtrees
okay so because of these density bounds
because this is higher density and this
is higher density and that's lower
density it's very easy to argue that
there is some point at which everything
here is active you know because of these
VC bounds these radii are of X I the
distance to its cave nearest neighbor is
really very significant okay and so
there really is some point at which all
of the points in here are active and all
of the points in here are active and
none of the points in here are active
and that means that at that point
there's no way to get from here to there
okay so they are separated the hard part
is to show that at that point these are
actually connected okay and so in in
these kind of analyses of of these
processes its connectivity that usually
is troublesome
okay so there's certainly a point at
which all of these all of the data
points in here are active all of these
are active and none of these are active
so they're separated and we just have to
show that they're connected okay and so
this is the situation we're looking at
we have a single cluster every point in
the cluster is active and we just have
to show that they're connected that
there is enough points in here that
they're all a distance R from each other
okay that you can get from any one two
or the other now this is kind of a bad
case because there's this narrow bridge
in here okay so what's the worst case
that's the worst case okay where the
whole thing is just one narrow bridge
and you have one data point at this end
and you have one data point at that end
and you have to show that these are
connected okay so you have to show that
there must be a point here and there and
there and there you know something like
that so there really is a way to get
from one to the other and that's where
that alpha comes in okay that's why it's
not that's why we end up can the larger
we make alpha the more edges we have in
here and the easier it is to show
connectivity and so we use alpha equals
square root two and it would be really
nice
if somebody could show that you could
just use alpha equal to one okay that
would completely remove you know but I
have no idea whether or not that's true
okay and so I'm not gonna go through the
argument so the argument is a little bit
subtle but basically you are you you
construct the path from one to the other
to show that within each cluster things
are fully connected okay and the open
problem is whether alpha equals one
works okay so one of the things I wanted
to mention is that you know so I said
that there is a lower bound where you
can show that you know so I said that
you know for any given cluster you need
a certain number of samples to for that
cluster to be detected okay and then I
said that you know there's a lower bound
showing that you can't do much better
and I'm not gonna actually tell you the
lower bound I'm just gonna tell you the
inequality on which it's based because
this is such a nice and powerful
inequality okay this is called fan o's
inequality and it's just a tool in
information theory and you know it's
really worth knowing because once once
you have a handle on this you will you
know you'll just be able to apply to so
many different statistical estimation
problems okay and so let me just tell
you what this is and you know this is
what we use and i won't go into how
exactly we use it okay so pharoah's
inequality is is presented as a game
between a player and nature okay so you
have nature and you have a player and
before the game begins they agree on a
predefined set of distributions pay the
one to pay the elk
okay so nature and the player have
agreed on these distributions and then
the game begins nature picks one of
those distributions but doesn't reveal
which one it's picked okay
the player has to guess so the player
doesn't know but what the player gets to
see is n iid samples from that
distribution
okay so nature and the player they've
agreed these are the distributions we're
talking about nature picks one of them
doesn't say where each it is and the
player gets n samples from that
distribution
so here's what Panos inequality says
fairness inequality says that in order
for the player to get the right answer
with probability at least 1/2 okay
the number of samples that must see is
at least this much it's log the number
of distributions divided by 2 times beta
and beta is the average KL divergence
between the distributions so if these
distributions are really far apart if
those L distributions are totally
different from each other
it doesn't need to see a lot of samples
but if the distributions are really very
similar to each other then it needs to
see a whole lot of samples ok so this is
a nose inequality and it's just one of
those things that is really useful and
nice okay and this is what can be used
directly to to show a lower bound for
this problem okay so let me talk about
what some of the more recent work on
this question has been so the stuff I've
told you about was from a few years ago
and what was shown recently is by the
tufa and Vaughn Lux Berg is that in fact
you can also use a nearest neighbor
graph for this okay so you can construct
a much sparser graph where you never
connect each point to more than its K
nearest neighbors okay in the graphs I
talked about when the knob gets really
far along there's gonna be a lot of
edges in there okay and what they show
is that you never need to connect each
point to more than its K nearest
neighbors okay so exactly the same
procedure but never go further than the
K nearest neighbors and then they show
that you get a similar consistency
result okay and the open problem over
here is to come up with other simple
estimators so currently these are the
only two algorithms that are known that
converge to the cluster tree and it's
very likely that there's a much larger
family of procedures
another another problem sort of a
another interesting issue over here is
that I was talking about this notion of
consistency and this is something that
hardigan came up with in the 1970s and
it's really nice I mean it it's it's
subtle and it really captured a lot but
now that we've handled this we can ask
the stronger notions of consistency so
what Harlequin says is that if you have
two clusters you want them to and then
end up in disjoint subtrees okay
reasonable but what that allows is it
allows you to also overly fragment the
data okay so it's just telling you that
different things have to stay apart but
it allows you to have excessive
fragmentation and I'll show you what
that means okay so when you have a
density like this this is the kind of
cluster tree you hope for by the way
this is what a cluster tree looks like
okay it's like an inverted hierarchical
clustering we're used to seeing
hierarchical clustering the other way
around okay so I mean when you run
something like single linkage of
complete linkage you know you have the
top of the hierarchy which is the whole
data set and then you split into two
clusters at three clusters and we
normally focus just on the top right and
then you know as things get further we
we you know our eyes glaze over and we
don't look at the rest of the hierarchy
cluster trees are the opposite you have
to think of them you know upside down
and the most significant parts are these
because these are the high density
regions this is where this is where the
information is sound if there are low
density areas of this space frankly you
have no idea what's going on over there
okay the part of the space that you can
really be confident about are the high
density parts of the space so it's
really kind of opposite to you know how
we normally see hierarchical clustering
but anyway for a density like this this
is the kind of tree let's say you
sampled four points one two three four
from the left and four points five six
seven eight from the right we want
something like this you know where
there's just a single fork and then you
grab
we get narrower but a fragmented version
like this is also permitted by
Hartigan's definition and so and this is
a problem that has been studied a lot in
the context of cross decrease in the
statistics community and so recently
there's been some nice work on that in
fact most recently by Belkin and Wang
okay so stronger notions of consistency
anyway so some more open problems so you
know so for a hierarchical clustering
algorithm we want the we want these
clusterings to converge to something
meaningful about the underlying density
right and this notion I think you know
meets that requirement it's something
that makes sense
and I would love to find any other
notion okay I don't know of any other
that that seems reasonable okay and I
think that would be really interesting
if somebody could come up with a limit
you know a limiting clustering that you
know is sensible okay so this is an
example of a sensible one that there has
to be some other I don't know what they
are okay and the second open problem is
that this is really just for densities
and what happens in discrete
distributions is there is there
something similar one can do okay so
that's all I was going to say
I'll go any questions yeah so I think I
think one of the issues with k-means is
that k-means is the algorithm of choice
for vector quantization where you have a
large space and you just want to divvy
up this you just want to cover the space
uniformly you know with points so that
the loss the distance from each point to
its representative is not too large okay
k-means is absolutely the right thing to
do in that case and it is used in that
context you know for in in audio coding
for example but if you're actually
looking for natural clusters and the
data then there's no reason why you know
the k-means cost function would recover
one of those yeah well you know yes you
get back this tree and then you just
have to look at it and try this side you
know what what suits you you know you
just got it
no it doesn't there's no way to really
answer that question you know I mean
there's well your D log n thing says
that so this this business of like
looking you know let's say you have a
data point and you say what is the
distance to the nearest neighbor okay
and you ask is that a significant number
okay that's hard to argue because that
distance has got a a high it's got some
expected value but it's got a high error
bar on it but if you say what is the
distance to the D log enth nearest
neighbor that's much more significant so
it's got
an expected value but the error bar on
that is much smaller okay so that's the
issue you know so in all of these things
we're looking at the distance from a
point to some neighbor and how far does
one have to look before that distance is
is close to its expected value you know
it has a nice concentration property
when they're when they're sparse so okay
so there are two things so the first
thing is that the thing I've shown is
only in continuous spaces okay so I mean
these own densities so if it's if these
are sparse vectors then what that means
is that they're just the union's of you
know these sort of lower dimensional
it's just you know it's just the a
mixture of very low dimensional
densities so you know you would just
apply the low dimensional result I think
it would be interesting to get a result
for so so one thing that has been shown
is that if the data lies on a low
dimensional manifold then you know then
the dimension that you need in here the
D is the dimension of the manifold and
not the dimension of the ambient space
and that was shown recently
see it up the other I mean so the thing
is that you know all of this work I mean
so so this is you know this would be
this would be the one in any dimension
but this is not yet reach the point
where it is practical and the reason for
that I mean so we've run it and it does
nice things and so on but the reason for
that is that like single link this is
just like single linkage and like single
linkage it is N squared
you know like and quadratic is like the
new exponential you know so it's you
know so so we're looking at ways to
really speed that up and and and that's
what's needed to really make it
practical great if there are no more
questions then I like to cause pretty to
comment Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>