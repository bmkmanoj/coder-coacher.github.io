<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Better Questionnaires with Probabilistic Modelling | Coder Coacher - Coaching Coders</title><meta content="Building Better Questionnaires with Probabilistic Modelling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Better Questionnaires with Probabilistic Modelling</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KCJSGhKnSvE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it is a legend who have with us
ricardo silva from UCL and ricardo
basically works on machine learning and
one of his important sort of research
direction is causality which we were
sort of talking about and invasion
inference so over to Ricardo okay thank
you can you hear me well okay first
thanks for inviting me to give this talk
not really going to talk about causality
today but towards more than general air
of probabilistic modeling within a
context of machine learning so to give
you a general idea what this talk is
about I'm going to use this running
example which is a big survey applied to
NHS in 2009 is basically about Steph
satisfaction so the staff members of NHS
had to answer about 150 questions I
think on a variety of different
indicators of satisfaction of your job
here for example we have a section on
your relationship to a reader the
manager so the different icons they're
trying to measure how well your
relationship of your mediate manager is
in different aspects like the weather
the manager provides feedback to you
where you think the manager is reliable
to rely on you have some need in so on
and so forth these questions most of
dinner given this so called liquid scale
we have some some ordinal encoding on
how well you agree or disagree with this
practically statements that you find in
each section so this is business
structure that data they collected here
they had about 150,000 staff members
answering this question as you can
imagine this is such a cut out quite a
lot of work to answer such a long
questionnaire and keep might think on
whether were the kinds of uses they make
out of this data how you say racial
measures such concepts and with so much
precision
they pay the cost of that is actually
taking some staff time for example so
what uses people make out of this data
here i'm going to focus in just one use
and this is going to be the stepping
stone for the methods are going to
introduce so it's not uncommon to have
some sort of pasta later latent rates of
interest in a population this latent
trait sometimes are very hard to define
so if your political scientist for
instance you might be interested in
ranking nation's according to their
levels of industrialization
democratization these are kind of
abstract concepts are somehow like to
measure that in one way or another and
that's how some people come up with
indicators such as gross national
product or some expert assessment of
democratization by asking experts to
write a country on how free the
elections are isn't just a simple
example so some time that some people
have to fill in these answers to these
questions to measure traits of interest
and then what happens is you might be
interested in the embedding of the your
objects in this latent space so here in
the right hand side a picture that i
have i have i'm plotting the data in
this latent dimension so if you have
about 75 countries and i have their
conditional distribution of these latent
factors given observations that come
from a questionnaire and the
distribution is over some real space so
what i have here here i have the sample
space for the democratization level and
here has some box plot capturing I think
ninety-five percent of the mass or the
distribution and I have a longitudinal
study so I have agitated over two time
points and this is a plot it is
organized sport in the countries by the
expected value of the industrialization
so this gives them that kind of summary
of the straights
we're interested in the latent
distribution given the observations so
one person thing here is about designing
measurements that measure the straights
such that what you have there is used of
I sent this that domain so here might
want to compare for example trends overt
over democratization distributions as
you vary industrialization factor is a
summary on how the two concepts are
associated in the population so in one
way what is happening is that all the
information that you are based your
conclusions on come from the conditional
distribution is latent variables giving
the observations okay so here my
assumption be out information that are
interested in here this domain is
postulated to be containing the
conditional distribution of the latent
variables so given this is my working
assumption how they're going to use
these models what I'm going to do it
should develop some methods by which you
can design better ways of measuring is
latent rates so I'm going to go through
two main topics here in this talk one of
them refers what I call other
compression if you are going to have
some sort of long questionnaire to
capture information about latent rates
it might be useful to you to design
short questionnaires for reason I'm
going to explain swing-off in terms of
machine low and in context context of
course is related to something that you
know executive Learning watching a
slightly an indirect way as you're going
to see a second main topic that is about
really unlearning itself so you might
have some background knowledge on how
the strength should be measured but the
background knowledge Medina perfect and
how can you extend an export-driven
model with some adaptive method better
capture the distribution of the latent
variables so this is the second part is
related to the ideas that you find in
graphical model literature on structure
learning the methodology for seven
to be based in some very well-known
building blocks such as graphical models
could mentor optimization problems with
inference okay so feel free to interrupt
me at any point I'm not sure exactly how
far will be able to go in the talk maybe
I want to have enough time to cover
everything but I plan to but it doesn't
matter you can directly at any point I
appreciate it so the first topic on
modern compression sometimes this is no
as in social science literature in
psychometrics literature is how to
shorten scales how to shorten and sets
of measurements that you design in order
to measure some latent trait is the
point of view of somebody who actually
answer some question layers like to call
this a method to minimize the annoyance
of whoever has to actually reply the
discretion layers so how does it work
what's the motivation and behind this so
you might have a long question that
reply to you why doesn't mean that is of
the interests of the scientist or
whether organizations collected that
data that the question is as detailed as
possible the reason for that of force
humans have to reply to these and there
is that it pay some sort of costs that
have to pay to actually filling this
data so if you have a very large set of
questions your refuse awaits might be
high some people might just say okay I'm
not going to reply to that please don't
bother me again or you might have people
who agree or replying to that and left
after wire they might get higher the
answers are not given with as much
confidence as if they had a fresh start
might induce some dependence witching of
your patterns of answers because of that
this might be biased your results in
some undesirable ways so might as well
instead of having date of lower quality
try to go focus on some more important
measurements those that carry most of
the information so we are so here that
we have a given model to measure some
weight
traits but this model can be simplified
if that much of a lot of information
okay so there's no be there'll be no
learning step in this case no estimation
I'm assuming that you're giving a model
I wanted to reduce the amount of effort
that takes me to provide them the
observations that would be fed in this
model ok so how it is going to be
formulated and crucially there we have
at least a few people in the audience
that will think exactly the same way but
is a very standard way of India of
expressing how I can summarize
information so I'm going to call here X
my set of latent variables prates like
my example industrialization
democratization i'm going to call why my
observations would be would be in this
context and such a questionnaire so be
interested in selecting a subset of Y to
preserve as much information the
conditional distribution of X as
possible so one way of measuring that is
where divergence measure between these
two conditional distributions average
over the distribution of your data and
web common measure of divergence use the
KL divergence okay so you have a key
ally versions between the conditional
distribution are given every possible
reservation and one condition submission
giving a subset or your reservations
decision variables here this problem is
your set Z that you have to pick out of
a collection of questions and here again
you have to be provided in some sort of
size for the set so say I want exactly
key questions to be preserved such that
distributions close to that you know
because I will not be concerned here on
the size of K how does this chose them
this is a trade-off parameter and pmsing
is given to you by whoever is interested
in this study yes is it actually
reasonable something that small as you
know
a priori we have no idea about
questionnaire design but it would seem
to me that in many cases one perhaps
comes up with a set of questions that
one finds reasonable and then later on
one does makes some exported sorry some
explorative analysis and tries to find
some sort of latent trades or latent
variables that could explain those
answers so you don't have this model yes
I or II oh you like about that so there
are different ways by which people
converge to some sort of lake in a very
well with some sort of model over the
patterns of answers so yes you can think
I'll think of a complete adaptive method
of people start from the measurements
and try to see which are the other como
explanations for the associations or you
can think that scenarios that people
design their questions to measure traits
of interest here I'm just assumed that
you have a latent traits that they are
target rates how are we paying this is
I'm being indifferent to that but you do
have a trace of interest okay and does
it make sense to constrain yourself to
the subset or might you be interested in
so just asking different questions and
then you'd be able to ask fewer
questions a strict subset of the
questions in your original set wire
right i mean they actually design how
much how you actually watch you're going
to measure is a much more complicated
much modem I mean domain oriented task
here I'm focus on subset first was it
much simpler but you of course just
changing the nature of the questions
another way by which can simplify your
set right my tiramisu bishops which one
set by a smaller set of a different type
it's just much harder to define problem
them this problem that starts already
some some work that was done before so
how can you tap in this previous work to
get a simplified questionnaire
right so this is a general goal
optimizing over subsets i can rewrite to
that KO divergence in terms of a more
direct cost function with explicit
binary indicators encoding whether
questions included or not so that eyes
are indicators where the question I is
included or not and here i am assuming
that observations why are conditionally
independent given x so given the late in
various defector eyes and this allows me
to write down that KL divergence in this
as this function where I have a linear
combination of these log conditional
probabilities and you have an entropy
term entropy turn over the DC be sure
there'll be so hobos in the set of your
choice okay so here this is basically
integer programming formulation and
under this constraint that they have a
choice of exactly K questions alright
and at this point is a very complex
nonlinear integer programming because of
this entropy term there so we can
actually see that this is equivalent and
to minimize the conditional entropy or
the latent variables giving a set of
your choice and in some situations this
is known to have some life's
combinatorial properties this situation
that i have here where observations are
independent given the latent variables
you can show that as a function of your
set your choice of variables to keep
this is a submodular function i'm not
going to get into detail here but
basically entails that there's some nice
profit psychics can exploit while
optimizing for the choice of z ok but
one difficulty here even under this
scenario is that the objective function
that is to calculate sometimes people do
this on combinatorial searches where
this entropy term is it is easy to
calculate is gaussian for example the
joint exhibition
then you can calculate easily what the
entropy or the joint gauss would be in
this case here i'm assuming a discrete
variables and later on we want us to get
actual binary for simplicity and in this
situation you cannot even calculate your
objective function because it depends on
this entropy term so the main goal here
is the first part of the talk to develop
some procedure which tries to solve this
hard combinatorial ization problem the
same time is try to approximate this
objective function and to that to some
extent where did the optimization
between the set you are going to pick is
intertwined with the approximation to
the objective function right so to make
things work concrete and focus on
particular model which is one of the
most common waters using the situation
which is a multivariate probit model so
simulating there is a jointly Gaussian
and without any loss of generality
generality can make them zero mean and
we can imagine having binary
observations which come from truncating
some intermediate Gaussian that is
generated from X so you have this
intermediate Gaussian variable which
comes from a normal distribution
conditioner legs again we've got lots of
general generality consumers ver
instance one so this is greater than
zero your outcome is one is less than
zero comma zero sum for simplicity
focusing here on binary answers just
because it's going to make some
calculations simpler but this is not
essential what is interesting about this
problem is that you can calculate some
small margins without much effort so you
can calculate the marginal distributions
of the wise easily we've got any Monte
Carlo method just a simple project
you'll make it and can calculate small
dimensional marginals easily to all
sorts of methods they can calculate
joint distributions of a wise for small
sets of wise in some relatively simple
way so it is a property that is going to
be important and when designing my
procedures
so what would be one way of trying to
solve this fella tackling this difficult
optimization problem on the simplest
ways is just pretend your data skousen
then just plugging the gaussian
approximation for that which in this
case the entropy reduce the law
determined okay and calculating the log
determinant is of course tractable i'm
assuming don't have that many questions
here so you can call it deserve that
much effort and this gives you
attractive objective function then can
throw it at any of your favorite
community optimization methods so for
example doing the greedy approach that
is often use sub-module optimization you
can apply that here i can calculate this
covariance matrix easily from this model
I don't need related to have something
like an EP for the simple model because
you do have the marginal distribution of
pairs of variables easily so you can
calculate them the conveyance and
independently and put them together in
the covariance matrix yeah done go on so
this will be the simplest strength to
their yes in the video optimization in
the full in the exact case you get a 1
plus 1 by E approximation in the in the
submodular function case then we were
finding the best guy in each iteration
in this case you are using an
approximate sort of algorithm to do the
greedy step so but are there any bound
on the accuracy of this step and if yes
do the propagate over to the final sort
of thumb that you get on the overall set
solution where I'm not sure about that
to be honest
okay so okay of course if you want to
try to estimate this by Monte Carlo
which of course is going to be up on
very difficult in general but I actually
don't clicking the monocle head to the
SMC because I going to condition some
other reservations so you cannot do
exact model a little bit after the mcmc
so then the Gaza Fox which is just one
of the first things you could try what
is cheap and they can just use the tools
of the trade deal of that em any
modification so I'm looking here salt
relatives to debt how can approximate is
entropy in a way that is also related to
your choice of set so one way of doing
this is look at this very simple and
relationship of entropy that you
condition a largest set your entropy can
only go down in every never increases
increase your reservation so with that
can build a very simple bound by for
example choosing a particular order for
your variables and it is order you can
decompose the joint entropy as a sum of
this conditional entropy s where every
element it is all this condition
everybody preceding it okay you can just
bound this quantity by using subset here
instead of the whole proceeding set of
variables this gives you a very trivial
upper bound on the joint entropy so
what's the free parameter here is the
ordering that you have so you can try to
get tighter balance by picking different
orders so that this is minimized okay
there are all sorts of methods for that
some of them coming just from the
literature on learning Bayesian networks
where essentially if you have a directed
acyclic graph the most difficult step he
is picking the right order to factorize
a distribution the all sorts of mexicans
it is step including some very simple
greedy approaches so here i am assuming
that I'm given a neighborhood size
depending on computing capabilities
choose
large your neighborhood can be and based
on that you're going to choose the order
and choose your neighbors within the
condition that they should precede you
in via particular size okay so here's
just using a standard tools of asia
network learning you can fuck slate that
entropy by optimization of or drains and
bounded sets so nothing here is making
use of them a choice of variables so
this is just a general optimization
procedure to get a joint entropy and
reduce that to some of a small entropy
terms okay now what do you do it is okay
once i have this order and i have my
choice of a neighborhood i'm going to
plug this into my objective function
here as a substitute for the joint
entropy so what you have what you have
there is an upper bound in the original
objective function here you can encode
the original joint entropy in terms of
indicator functions or which you guys
are going to be selected so you have
this term that goes over the other and
what the straw means is the entropy that
is given by the intersection or the
values that are in my neighborhood and
the values which are actually selected
okay someone including here is my
preceding conditioning set those which
are selected now of course is my
variable here has now been selected this
whole term is dropped out of the sum so
these various here mean their functions
of the region indicators which basically
a defined terms of product which there
was actually included in Z so the
regardless of what the subset of the
neighbors that i have here an ax picked
by my selection i can indicate this
function in terms of a product over
these original indicator functions
for every part of subsets of my
neighbors so if you have a subset of
size five here I've exponentially many I
have the power set of it to the power of
five possible combinations that will be
picked I'm objective function depending
on what am I said is ok so of course its
explanation is sized ever set but you're
assuming that you're choosing this the
size of this set according to your
computational capabilities so I'm
assuming here this is tractable in the
size of your problem so when you
configurations that I have a
corresponding combination here that
gives me the right entropy term
depending on which ones i actually used
and here this vertically step can be
again rewritten as a set of binary
variables was basically this saying look
at every possible subset of this return
the entropy term that corresponds to
those in Z and this means a product of
our indicators of elements of Z so it's
again a binary variable so a choice s
here I can have one big binder indicator
saying this is the term that corresponds
to that set and this big horrible binary
indicator can be written in terms of
linear constraints with respect to the
original variables so just give you one
example let's say that this set here
this set corresponds to very want to win
pre for example and this set is going to
be picked only if Z Zed for variables
one two and three is one okay so there
is a relationship between saying the set
of three versus picked and every element
corresponding to this set has is l
equals to one and can rewrite this as a
linear equality constraint which I'm not
going to get into detail here but
basically this combination can be
10 as a function said with a linear
inequality constraint which means this
whole objective function here can be
written in terms of binary variables
with linear constraints and this is
going to be linear in these binary
variables yes is it right the size of
the Nevada yeah i mean the candidates
set for any word is fixed in advance yes
when you can optimize over it right you
can basically find out the tightest
boundary absence so that would be sort
of relate to a maximum problem right you
maximizing overall objective minimizing
over the neighborhood say hmmm so yeah
okay but that is a you've not done that
well I fixing that events for the
authoring of the sets are going to be
okay and this is my approximate the
entropy is just plug it in there this of
course has consequences which am a try
to explain here so if you have of the
original model of course is a fully
collected model because when marginalize
elite and values induce dependence
between our variables so you can imagine
its distribution being coded some sort
of leg factorization directly see a
graphic factorization and the entropy is
uses some of this conditional entropy is
you can approximate this by restricting
than the sets which are conditioning on
okay so here for example I try to
approximate the distribution with a
distribution that has a sausage
structure ok where I can calculate the
entropy of this and what when it
combines two together in my variable
selection problem there are some
artifacts that are undesirable there so
for instance suppose I have only three
variables and the approximation that is
chosen by the other source in the
neighborhood search corresponds to this
lack of chain here ok so this will be
your approximation that works once the
results from having the whole joint
distribution
now when I'm searching for subsets I'm
searching for subsets of that okay so
suppose I'm choosing to out of these
three variables using this approximation
so you share what to choose variable one
in three for example they correspond to
and in reality if i'm using this
approximation what that should use is
the joint entropy or they the marginal
distribution y 1 and y tree together so
ideally this would correspond to using
my objective function that joint entropy
of this model or this model is the
marginal distribution of that model okay
from marginalized white to my
approximation entail they should be the
structure between y 1 and y 3 but you
might simplify it set up what I'm in
doing is since Y 2 dozen is not being
selected I'm just using this subset of
my neighbors that does include why to
which in this case is the empty set so
when I'm choosing my Z to be variable
one and variable 3 i'm using the joint
entropy of this deck model instead of
joint entropy oh that marginalized that
model okay so this is a simplification
in order to preserve an integer linear
programming problem we're doing them the
variable selection the right intertwined
between calculating marginal
distributions of our bags which itself
intractable so I cannot calculate a
margin distribute distributions directly
in any case so this is a simplification
of that okay so this says some of these
are the properties of course because
this is an evening loser an
approximation then what will get fukuda
marginalize my original vaccination so a
situation like that which alternatives
do you have well it is if it's hard to
marginalize a dagger in general and we
should look for simpler struck even
simpler structures than this which you
can marginalize efficiently with being
objective function okay
so the next step is looking for a
variation of this idea where it your
your pounds is more adaptive your choice
upset so one a different user of this
entropy bound is looking here at all
sets of size one and you can of course
get the tightest bound here by just
looking at every possible candidate and
picking the one we'd use you the
tightest bond right so instead of
looking at neighborhoods of a particular
size you look at those those of size one
the difference here is a minimization
problem instead of being predefined and
fixed before I build my integer linear
programming problem this is not going to
be used within the objective function so
I'm carrying this now will be my
variable selection combinatorial search
so here Michael notarial search will
include this main term instead of having
that neighborhood being prefixed and
again I'm not getting much detail i can
rewrite this function in terms of a
linear functional binary variables ok so
my choice of a neighborhood is a port or
the objective function and I can have a
tighter search within this family
because it's not prefixed before I
choose that ok so how does this
translate in terms of proximity models
so suppose I had four variables again
the numbers you indicate the actual
order but is chosen by the author search
procedure again the order that's still
prefixed what ravine is order the choice
of a neighborhood changes of Zed so my
left diagram there of the original
complete distribution where I would
select all variables and I can imagine a
rearrangement this tree structure when I
select only a subset so if you have a
subset which drops one of their
variables and is very
happens to be white free for instance
i'm going to my objective function is
going to be related to another tree
structure instead of just drop in on
this edge it is other three structural
just linked wife or to some other
predecessor depend on the strength of
the association that so when you choose
your said your tree structure here
includes new dependencies that didn't
exist directly your region joint
formulation okay this one is one step
further than the proof that the previous
attempt in a sense that this structure
depends on that Bell okay so these are
the two main methods and the Squanto
compared is against some other
alternatives one very simple primitive
that doesn't use any entropy term
consisting using the soft call and
reliability score which is defined for
Gaussian distributions but I'm going to
use this rebel if you score zero as if
my data was Gaussian so basically the
amount of variance it is explained by
the latent various for each one or your
reservations okay if your observation
was Gaussian then i have this linear
program here that just maximize the
sound ability scores over my choice of
subsets so this would be very simple and
comparison model approach to compare to
which doesn't require any interpreter of
course why not have an entropy term it
has some bad artifacts such as it could
choose for a single latent variable only
indicators of it or design indicators of
very high precision so we never had to
choose indicator for another late and
invaluable for example it is the case
because in that don't give any Center
for diversity there okay so evaluation
will consist on comparing different
methods on how well they can reconstruct
latent posterior expectations with
respect to using all observations why
okay
so we're going to the script explain did
some synthetic study first with two
different scenarios one where there is a
highest single to waste noise in a
measure model with one little smaller
scintillation noise the details i think
that people are not going to go into
details in this but other these
different scenarios just calculate how
well how close you get to get to the
expected conditional distribution
expected value of x in the conditional
distribution but this is a very common
functional of the conditional
distribution okay i'm going to report
something here that is some compassion
of respectively reliability score
measured at a mission before so here
higher numbers are better this
improvement on using that matter that
doesn't have an entropy term so looking
here on this particular corner this is
just the distribution of scatter plots
and this is just a box plot over 880
experiments comparing on then fixed
neighborhood with a neighborhood of
firing method a tree structure method
and the one that uses a Gaussian
approximation with respect to the
reliability score matter and there is
some not large but relatively consistent
improvement of using the tree
approximation compared to just using a
Gaussian approximation yes right this is
sort of the root really we think it'd be
slick the questions right the active
learning sort of which would be you look
at the best observation then condition
on that
so if you sort of embed the document
with active learning well a greeting
metal be basically starting from the
whole set and in doing the opposite
direction so in one sense i mean the
agreement already tertia procedure
didn't try to use a greedy metal
starting from the empty set in going to
their whole set I mean here you know in
advance how many points are going to get
sorry humming measurements it when i get
so you can just do that joint
optimization together okay okay okay
okay so here's just a scatter plot
showing the error comparing the metal of
that the entropy term against the three
bound method then you can see it very
easy to be the matter that doesn't
account for entropy of variability in
the choice of set because I'm short in x
and just say you apply this for that NHS
data too but using force a question is
latent variable the structure that
wasn't really designed doesn't really
explicit for that study just design my
own latent variable model structure by
looking sections of the questionnaire
and they are not showing those
correspond to different latent variables
and then doing the the search there and
again reason using a decreasing arrow
reconstruction by using the tree method
compared to the other methods although
they dip their relative gains quite
modest here okay so intense de
statements i'm just going to give a
general idea on the other side of it if
you do have a pre-specified model how
can refine it to give a better faster
data by doing some sort of aren't cheap
structure learning / / candidates that
include your background knowledge as a
backbone so well the idea if you do have
some sort of prep specification how
light envelops you to connect to
observations you might want to do a
modification of that by embedding this
is part of a search space for models
that do this relationship between
Layton's and observations so your search
space to try to define here for example
ways by which later in various can
connect for themselves or connected
observations in search along this space
to give something that gives a best fit
to the data okay now sorts of
complications here because you evaluate
how good the fate of a model is there
computationally expensive again the cost
calculator marginal likelihood is hard
so one approach for that is trying to
first modify your search space to be
simpler no way that is simplified a way
that makes a problem more tractable and
simplify a way by which evaluated these
models too so simple search space in a
simple way of evaluating how good these
models are is when st. shal decision to
be done before you even its begin to
tackle up feeling like that so once
choice of search space here so I just
skip this is saying that you do have a
pre-specified on choice of latent traits
that you want to target and you know
which variables are measuring which
traits but there might be all sorts of
infinitely many confounding factors that
also explain away the associations among
your reservations so be concerned on how
this infinitely many factors might be
connected to observations and try to
learn this partial structure phone data
and given background knowledge on this
original partition here okay so giving a
partition of how your your observations
connected to a particular trait and try
to patch it up very inflate them any
other
variables okay so it's a much simpler
search space if you think in terms of
how sparsely connected this might be so
this is really very sparsely collected
it might be more worthwhile to represent
directly what the marginal distribution
among these associations is once to
marginalize this pool of extra variables
that you don't know what priori so
sometimes this is represented by this
force by director structure what I'm
saying here this edge between these two
fellows means that they had some common
parent in this non-specified pool of
infinitely latent variables so if your
latent rates that will chose a priori
explained away already much of the
association we expect these remaining
and connections here to be very sparse
so in one sense this gives you a family
of a low rank plus sports connectivity
by which you can try to fit the data
okay so to make things more concrete
lies i can define a parametric form
again using multivariate models but i'm
going to use some slightly different
planet realization for the conditional
distribution all these observations
given the traits of interest okay so
instead of doing the in some standard
parametrization want to use these models
that would design a region by gene one
and brandon fray if i was actually here
at MSR in seattle i think so i define a
CDF conditional distribution function
for the reservations given the
pre-chosen latent traits and they define
a CDF is no way that are easy to
factorize they are to define terms of
product or small dimensional CDF's I'm
not going to get into detail why do we
learn to do something like this but you
can imagine being able to put together a
large emission CDF using small
dimensional city
and this gives you all sizes way of
calculating margins they're easier
calculator margins no model like this
for example I'd by two-dimensional
three-dimensional margins a very used to
calculate from cydia africa's can just
plug in the infinite value for all
variables you want to marginalize
automatically get a margin or that cdf
so how's this using this context i want
to skip this well if you want to see how
well the model fit you need to
marginalize our latent variables and
this is their expensive so instead of
using the whole joint likelihood and we
want to use pieces of the of the
likelihood that have a small size okay
so sometimes it is always marginal
composite likelihood you can do i can
calculate small marginals because of the
CDFI authorization gives me
automatically very easy way or
calculator modules i can put everything
together it's a composite likelihood
score and then maximizing this for
instead of the joint likelihood so for
simplicity and epidermis linking the
latent stood observations are pretty
desk points which i'm estimating by
point estimator but i'm integrating away
the parameters corresponding to those by
directed edges so there'll be a bayesian
penalisation for the dependency
structure that is due to the do this
infinite pool of latent variables which
are not explicitly we presenting okay
now the most it forward algorithm using
this composite likelihood is looking at
pairs of variables and then looking at
that some of the log probability over
all pairs in for a given graph structure
that tells me which by directive
structures should exist i have different
factors here as a function of that so g
is just my myspace structure that
remains after conditioning the latent
variables
milk crates and we have to optimize this
respect to G of respect to the
coefficients linking X to Y and the
covariance matrix of my latent variables
but marginalizing the parameters of the
dependent structure all those of that by
directed set okay so you can do this
vertical efficiently especially if you
have a very simple one parameter
representation for the dependence
structure where these things called
popular models which we're not going to
get into detail gives you many families
or dependent structures that can be
paralyzed by a single parameter soaking
etching to greater way sound is using
powdered sugar for example don't need
any multicolor method for that and in
here another nice thing about this is
because this is two-dimensional if you
have something like binary data discrete
data you only need to do one pass to
your data because you can catch there by
very statistics so each pair so maybe
some of you heard about spectral
learning methods for latent variable
models for example and the main idea is
once you partition a likelihood like
this if only came to sketch everything
that you need of a single pass your data
and this makes a fitting far far more
efficient then doing full maximum likely
likelihood for example there are
variations of this which I don't think
I'm going to get into detail but let me
just say that you can also use a young
like bound here do they use of ginseng
equality boundary in each term
individually so you can have a different
am conditional distribution for each
term okay and do some sort of yam fit in
here and why would we want to do this
instead of doing maximizing likelihood
directly is because here in this
function can try to incorporate
information from other variables within
your pear so there's a big still a
one dimensional integral con this being
the dependence parameter but here being
coding information from some other
variables of course the trick is how to
do this efficiently I'm not going to get
into detail here but you can adapt yem
ideas within your choice of average
distribution and do their bounce
separately for each term in a composite
likelihood okay so let me just jump to
some results so in terms of how you
apply this again my am here to give a
model that preserves interpretability so
if you are interested in particular
latent traits you don't you want to use
this information the information I
distrait some measure in some way by
particular measurement model so I'm just
refining this model under these
assumptions by adding these extra
dependency structure in a way that in
the model student a terrible but have a
better fit okay so I mean I mean
evaluating these in terms of fitness by
looking at the lake tanning bed in of
each data point in checking how how good
is latent features are in some
prediction problem so I fit this too
again the NHS data with some learning
the residual structure given that latent
traits that i specified my first section
and with that separate some set of
questions that are not including the
model use the latent in bed in you can
subset other questions to predict
questions outside that set comparing the
model where I do not do sorry I'm paying
them all the way do not do and anybody
wreckage structure search against their
mother wait I do it so this is comparing
the model that doesn't refine the
original latent trait structure against
the model that does refinement so in
terms of area under the curve on
predicting some
ization of the remaining variables here
there's a much of a change in min of the
holdout questions that I have but in a
few cases I get some major improvement
just showing this is by the wreckage
structure gives you some zing the
recreation is by my greatest threat
gives some better fit to the Joint
Distribution while preserving that
ability or the original traits that you
picked at the beginning of the analysis
okay sousa ha is a technique that is
saying that only a predictive ability
but also interpretive little bit of the
model that you obtain in the end so i
can't really make much of this is just
it just show how sports they structure
that was run actually is so these are
the blocks corresponding to the
partition of the set of 50 variables so
here are all children of the same latent
variable the red points here indicates
by directed edges will bring the same
latent and children and the yellow dots
here represent those by the red edges
across children of different lengths so
this is just showing that there's more
range structure that I have here is
combined with a very sparse structure
the remainder so integration way those
infinitely many variables that might
have cost is it's probably a good idea
because it structure so sparse it
doesn't make sense to try to add these
latent variables explicitly in your
mother space okay so this is basically
it what I look to talk about and they of
course main directions and this work in
terms of try to compress a question
there there are many things that could
was think of such as incorporating
certainty in the model so it just seems
a fixed model other criteria that we
might want to throw in of course
calculating the conditional distribution
letting there's just one criteria might
have other constraints to deal with
maybe some of these other constraints
can be easily added to the linear
program maybe not
adaptive and questionnaires is something
is not covered here all there are some
question is where the choice of
questions depend on previous answers so
how the other situation like that for
the second half way have on this idea on
patching up a pre a partially specify
leading created model with extra
collections whether there are the source
approximations that you can use they're
not to make this more scalable because
even though this is describes only one
pass to the data it's super expensive
procedure because they tater asians that
you make to update your mother is still
quite expensive so whether there are the
ways of improvement is still something
is work in progress okay and of course
experiments are all done by now rising
data and it was the original data side
binary or this extension is relatively
simple to ordinal data or more general
type right for this basically it okay
thank you very much yes so what are the
take-home message is if I want to design
a questionnaire what should I do suppose
that I have suppose that have a model or
suppose that I don't have money but
instead i have maybe a hundred subjects
who answered my question air what should
i do well first of all have to see how
useful at your little representation is
so you can do for example rather than
alice's of a subset and see whether it
is inform the information that you have
no conditioning solutions to
representative of what you have with a
full set so you can do for example
player difference choices of k so see
how sensitive your output is on that and
see whether we done and slightly so it's
one tool to explore how redundant
information is a person there again
everything here as soon as that you can
reduce your goals to functionals of the
distribution of latent variables I'm not
cleaning any in verse verse ality here
on how to design a questionnaire
only other this point of view where
you're doing the mood in some latent
rate of interest so this is one tool for
the in sensitivity analysis on how we
done on your questions is or another
thing is don't processor model has been
complete just based on the theory on how
you're measuring a latent rates there
are many spaces of models by which you
can search to see what's the best filter
data the space some design here is no
way that tries to preserve as much or
the original brand knowledge as possible
with some extra residual structured
search so you can see again how much
information losing by not using the
structure as may be an indicator that
maybe you should just patch it up with
this far structure or whether you should
actually destroy what you have and bring
try to find a different starting points
begin with if you cannot really patch it
up in a way that the fit is good enough
for you so maybe there's something
fundamentally wrong with the original
measurement model to begin with so these
are two ways by which you can do this
again I'm not saying that it should just
take the outputs there's a face value of
course not I believe this can be used
with some sort of sensitivity knowledge
tools so if I have even have a model
which use related variables and I know
what these variables are and I'm
designing my questionnaire why not just
ask for those making things or because
you can't right this is it that this is
the water points i'm usually for example
anxiety what is anxiety I mean there are
different ways by which you can measure
that so one example that I use one
sourcing test anxiety on the students
what preparing to take the eggs some
exam so you can ask them how anxious
they are respected to something but I'm
not as psychometrician cannot really
answer that there are people here which
are much better this but a different
ways by which you can try to probe that
in different aspects on society too so
might as for example do you feel
heartbeats changing before the exam why
if you're getting sweaty year before the
exam or something like that
you don't need any evening things like a
democratization we don't go out and
express what it country's democratic or
not directly because there are many
facets of that so you can as for example
how free the elections are the country
or how I'll free the prayers of our
country is and then can use these
indirect measures of some sort of trick
that would explain away discover the
fact these common observations there's a
very good question but again they're
much better people here qualified to
that in myself to answer that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>