<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop: The Default Machine Learning Platform? | Coder Coacher - Coaching Coders</title><meta content="Hadoop: The Default Machine Learning Platform? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop: The Default Machine Learning Platform?</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/chn2YUHeMSw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so the title actually has a question
mark at the end okay so it's not I'm not
stating that it's going to be the
default machine learning platform all
right so my name is Melinda and Erica
and in the last 20 years of my career it
has been split into basically roughly
two parts first half has been focused on
HPC and the second half has been focused
on what we call as the Big Data now
although I do not like that term at all
because then the next question is okay
how big it's take right so I don't want
to answer that question one thing to
note about my profile is that I have
absolutely no experience in machine
learn which basically makes me the
perfect candidate to speak at a
distributed machine learning workshop
but but anyway just a few people to
acknowledge of course is not my own
presentation it's been collected from
various people a lot of open source
development has gone and based on a lot
of proprietary work that has gone on in
building distributed data platforms a
lot of ex-colleagues to thank at Yahoo
research who basically provided me with
a with a lot of input on how they do
machine learning on this fledgling
platform at that time in 2006 called
called Hadoop and that has improved the
platform a lot a lot of my colleagues at
my current company called pivotal who
claimed to be data scientist another
word I do not like at all commercial
term right and and most importantly we
general and from Microsoft again Exia
whoo-hoo and I actually he and I
conducted a tutorial on doing machine
learning using MapReduce at kdd 2011 and
that prompted me to go over this entire
survey of different machine learning
techniques and how how they were
implemented on top of MapReduce so just
to clear of some confusion about where I
am from because the the program says I
am from greenplum EMC so yes I used to
be in greenplum EMC
and my division has been now spun off
into a new company and you start up
called
pivotal so that's where I'm from mainly
focused on Hadoop though so I've been
with the Hadoop community for almost
since its beginning actually I joined
Yahoo in 2005 to revamp its search
infrastructure and what came out of that
is essentially essentially Hadoop so
this is the actual picture of the first
first hurdle cluster we actually had
borrowed 600 machines from our Yahoo
search engine people who were going to
throw it away or sell it on eBay or
something like that and basically said
we are good on this new platform card
called Hadoop on top of that and when I
left in 2000 left Yahoo in 2010 we had
more than 45,000 machines containing 300
petabytes of data you know all the other
numbers probably already right so it has
been a huge success in the meantime we
had a very nice 500 machines cluster
literally sitting in a trailer outside
of the Yahoo buildings in a Yahoo
parking lot which we actually repurposed
as an academic collaboration cluster we
actually have had a lot of research done
on that mainly in the natural language
processing area Tom Mitch's group in in
CMU actually used this cluster a lot
observing their work actually gave me a
lot of confidence that given some free
software and free hardware good research
book will come out of it so I took that
to heart
I basically when I joined left Yahoo and
joined green block now portal we
established a partnership between
various hardware vendors and they
donated a lot of hardware so we set up
this thing called green plum analytics
workbench now pivotal analytics
workbench it's a thousand node cluster
running Hadoop and other distributed
data platform software it's a full
fledge Hadoop cluster it's a fully
managed Hadoop cluster you don't need to
do anything you just bring your data
bring your applications get your
research done it's basically just fill
up a form and we will provide you with
the cursor okay
so that's all that I've got to say about
the clusters aspect of it alright so a
lot of people out and refer to how do
mapreduce etc we all know when people
say you know Hadoop is slow or do
business Hadoop is that people are
mostly talking about and I would say
mostly talking about Hadoop 1.0 which
basically has MapReduce as the only
programming paradigm supported at scale
on this distributed systems platform
okay if you have to up limits when you
first do the batch processing using
MapReduce who take results of that data
and then publish it somewhere else in
order to maybe in HBase maybe in the
relational databases etc to make it
available to the real-time attractive
queries or just you know take a sample
of that loaded into an excel spreadsheet
and do some plotting visualization take
that data loaded into memory in an R
program and then do some interesting
machine learning work right so that was
Hadoop 1.0 Duke 1.0 limitations of
courseware that the system demons that
were supporting Hadoop like job tracker
and toss tracker
we're sort of hardwired with MapReduce
as the only programming paradigm that
they would support right they would not
support for example peer-to-peer
communication at all reduce like
communication nothing like that you had
to have a map you have to have a shuffle
and optionally we have to have a so
optionally you have to have a shuffle
and optionally you have to have already
right that was the only paradigm that
was supported but what happened is that
since this was probably the only
platform that was available at scale to
all these machine learning researchers
in Yahoo and elsewhere they basically
had no other choice but to repurpose
Hadoop for Hadoop and MapReduce for
their own work right so first of all
Hadoop has been optimized for data
throughput this whole notion of pushing
computation to where the data is and
mostly most of the times executing your
map tasks close to where the data is
already stored basically meant that you
could scale your number of map tasks and
accordingly you could increase the
bandwidth that was availability so 100
terabytes per hour processing all that
data in a streaming manner from the disk
using 500 mappers was was sort of common
okay the second thing and this has come
up again and again is that the
most useful feature of Hadoop especially
working on commodity hardware it is its
fault all of us right and I would say
that you know the the commodity
tolerance and sorry commodity hardware
and failure of those components hardware
components actually comes up again and
again it gets it's more than the share
you know mind share that it deserves
actually it's not that the hardware
components fail intermittently right yes
they fail that is about you know a 1
percent failure rate that we have
observed and especially because of the
spinning discs that are there in every
machine those are the only spinning
mechanical part those tend to fail made
very often 1% fail over over a month
right but your job typically runs for
maybe a day how many machines are going
to fail in a thousand or cluster the
probability is actually fairly low you
are all you know the probability and
statistics researchers you should
actually calculate this probability
right it should be fairly low I think
the main issue at that time at least is
that the earth since these clusters are
multi-tenant your your application is
not the only application that is running
on any node at any time there are other
poor programmers who have written their
applications which take over the entire
machine and kill your job that's the
biggest number of faults that we used to
observe in a production Hadoop cluster
so fault tolerance not only to tolerate
the clean hardware but really the
multi-tenant aspect of this cluster
actually killing or taking over the
resources from your job and making your
jobs fail that's the failures that we
observed most of it's not well this was
this was the problem the problem was the
resource isolation and problem still is
the resource isolation you can do good
resource isolation if you are willing to
pay the price for it for example you can
easily nicely partition a single
physical machine into a lot of virtual
machines and virtual machines with
VMware Citrix whatever these guys
actually offer a very nice resource
isolation that one means misbehaving
application inside a virtual machine
does not kill the other application does
not take over the CPU does not do you
know overflow its memory does not take
over the entire network by flooding it
with ten gigabytes of transfers per
second right that was the main issue and
that is why when some some some of your
code in particular what we observed but
that the map task was not getting enough
resources the task tracker will believe
that it has it is the dead and three
starter right so that's that's basically
the whole thing that was happened all
right
and obviously accepts in in counting
number of things right
in fact the hello world of MapReduce is
world talk right so counting is what my
produced really excels yeah yeah yeah
yeah yes absolutely absolutely
so in the initial days I have been
involved in running these clusters in
sort of 0.1 in the initial days the
software was fairly mature and in fact
used to have a schedule of unscheduled
maintenance window once every six hours
or something like that right and we
won't care about other users that are
running their jobs will just update the
cluster and then we'll get an angry
phone call from some user saying I
already have almost finished
why did you reboot the cluster right but
then later as the software matured more
and more we used to have a scheduled
maintenance window of two hours every
week in order to carry out all those all
those tasks and we had redundant
clusters in three different data centers
so we used to notify users in advance
saying that this cluster is going to be
down in the next two whatever next
Wednesday or something like that during
this time all this data is already
available in this other data center in
our other cluster you should go and use
that and then then things and and most
of the times now this is how things are
working the whole rolling upgrade thing
in Hadoop is still not okay all right
it's built on commodity hardware we have
seen various customer cases who have
replaced 20 millions dollars of Teradata
equipment with four hundred and fifty
thousand dollars worth of hadoop in AWS
so really it has democratized the whole
notion of of
again I don't want to say big data but
just data processing okay
and since Hadoop and HDFS in particular
offers a very cheap and economical way
of storing this data a lot of your
company's data is already landing on
this this cluster and is already
available and given its scale tens of
petabytes of data on a single cluster is
nothing to be scoffed at right I mean so
so since it is already there why can't
we basically use this for machine
learning as well I mean the manager of
my manager at Yahoo Eric ball spoiler
who later went on to found this company
called Hortonworks used to have a saying
in Yahoo search that I am buying this
CPUs for free right so already I have
all these bits and storing all this data
these CPUs are just idling around just
doing some mundane replication tasks why
don't I use those for for doing some
good good computation right okay and in
particular when we are talking about
machine learning I mean most of the
advanced machine learning techniques and
especially when they are implemented in
Java without you know giving any thought
to CPU efficiency and how they are
actually executing those instructions
they can soon become limited by
computation right not so much as data
because you are filtering out all most
of this data you are probably you know
throwing away a lot of this data you are
doing feature extractions in advance and
then after those features are extracted
those probably get that the data sizes
probably get a lot less but then you
have all these computations that we have
to do on those and then that can that
has to scale across all these different
machines right okay so what are
techniques that people use to to do
machine learning in in in MapReduce
right so that the candidate algorithms
that you have that were easily ported to
the MapReduce model are fairly simple
algorithms in terms of that their
communication patterns write data
parallel algorithms it's a given we are
basically all shared nothing you know
you don't really need to maintain a
state you can just indepent
independently operates on separate chunk
of data so so those kinds of algorithms
are the most suitable for MapReduce you
just say no shuffle is needed no reduce
is needed and then everything can be
processed inside a map right but then
not all the algorithms obviously follow
this but most most interesting
algorithms actually are not data panel I
mean I would say most algorithms are a
data parallel but the most interesting
algorithms are not data paths right and
so for those most interesting algorithms
MapReduce may not be a suitable may not
be a suitable implementation right but
relaxing some constraints are relaxing
constraints a little bit and this we
have seen a lot of times getting to a
approximate solution with more data
parallel communication patterns or
rather complete lack of communication
patterns are acceptable when the data is
large enough I do not have exact the
theory on that I do not understand I
have heard about it from a lot of people
who are actually nice off these machine
learning algorithms on the Yahoo scale
data right so so that is why I think
MapReduce became popular as a mechanism
for implementing these machine learning
algorithms inside of Yahoo
sure so so anything that involves
iterations which are more fine-grained
iterations are the algorithms that I
would call as not really data parallel I
mean not to map suitable for MapReduce
right and this has come up again and
again wherever iteration is there and
especially across iteration you have to
share state that is where MapReduce
actually sucks right so so that that's
where and that's that's why I'm saying
there was something else
oops all right so the the most
appropriate I mean the way people
basically took their machine learning
algorithms and and ported it over to run
MapReduce I think the the first
candidate was where you could actually
train multiple models completely
independently on the same data right
that that's a given that's basically
just come complete written independent
models independent modeling techniques
execute inside different mappers and
just just produce those smart models
okay you could train those in either in
mappers or introducers right so if you
can basically fit in all the data inside
sorry all the data inside a particular
process inside 'jv as a single JVM then
you could train the models in inside a
reducer right otherwise what you could
do is basically just split them or split
the amount of data that you are
processing in each of the mappers loaded
completely into memory run your machine
learning algorithms reduce the model and
then combine those models inside a
reducer right so that's what the the
most of the distributed learning
algorithms are programmed using
MapReduce it has also been used for
ensemble training methods where you
basically combine all the different
kinds of models or are trained different
models inside all the mappers and then
introduced in reducer you combine all
the models assumption is that the model
is obviously going to be smaller than
the amount of data that you are going to
train it up so the amount of data that
is shuffled between the mappers and the
reducers is going to be small okay
so parallel training of multiple models
is pretty simple you basic
we assign a model ID for each of the
model that you want to train you load
all this data into the data that you
want to train your model on into each of
the mappers each mapper is assigned to a
particular model ID it basically just
trains the model and says okay sends it
to the reducer the reducer basically
combines those models together right
distributed algorithms on the other hand
when they are put into MapReduce they
are more suitable for MapReduce if they
are computing 10 super records right the
reason being that if you are just
reading the record and doing a very few
amount of computations on each of those
records the the overhead are spawning a
particular map task on a remote machine
and then establishing a connection to
the HDFS and then written all this data
which tends to be kind of slow because
it is coming out of a disk right and
again going into the disk for the
shuffle phase so unless you spend more
compute cycles per record in an
algorithm those algorithms are not going
to be sufficient for for programming are
efficient when programming using using
mappings but what people have found is
that MapReduce has been a pretty good
fit for a lot of statistical query
modeling techniques and a lot of machine
learning algorithms or machine learning
techniques that are commonly used
apparently fit into this what we call as
the SQL which basically you are
estimating a particular function you
asked an Oracle okay how do you do an
estimate and you basically ask an Oracle
how different I am from the actual
solution the Oracle tells you something
to move in a particular direction and
you basically repeat that process until
you read that that particular estimate
right so again it involves iterations
but as long as your individual iteration
grain size is large enough you can
program this using MapReduce right
the the streaming speed per disc is in
practice you can expect to get about 50
megabytes per second right a 50 megabyte
per second that that basically is not
the actual raw disk speed but when you
actually do the D serialization and
finally get it to compute we are getting
about 20 megabytes per second right so
now within a second given the modern CPU
you can process up to a gigabyte right
easy and so that is what you need to
take into account is basically if I am
fetching this data at 20 megabytes per
second am i spending long enough time on
that 20 megabytes data to justify just
the launch overhead right you know when
you taste say your task launch over it
in Hadoop the way it happens is that the
task tracker wakes up once in a while
about every 3 seconds goes to the jar
tracker and says hey give me something
to do
right so when you have tons of those
right and there is a lot of number of
tasks to be pending of that that are
pending to be executed our large loop
job tracker being a single point right a
single demon is actually doing a lot of
calculations about ok which task is the
most suitable to be fit to be to be run
here right and that means the task
scheduling overhead is actually gated by
the by the task tracked by the job
tracker right yeah correct correct that
that's why it actually makes perfect
sense because the so first of all the
number of iterations need to be small
enough and the second thing is what what
state are you transferring between one
iteration and another iteration but then
there are I have I have a whole slide on
all that all these challenges in
importing models model training to
machine learning and some of the the
main bottlenecks for that is not only
this but every time you to read the same
data the disk that would be actually
becomes another button like that yeah
yeah
second your p2p communication yeah yes
that's right yeah
well then you have to be able to fit
your whole thing into Ben memory right
that's that's the main thing yes
yeah yeah exactly
completely completely I think I think I
should have given my agenda up front as
to how I'm going with this story which
is basically saying you know these are
all the experiences of people that
actually initially did MapReduce machine
learning in MapReduce found it you know
to be severely lacking and then moved on
to other things and those other things
aren't the ones that I want to actually
focus on okay so just to give you an
example of the commonly used techniques
in clustering k-means clustering you
basically have case centroids and you
have a bunch of data you have to fit
those two into these key clusters so
obviously the each MapReduce job in here
forms an individual iteration of this
iterative process okay in each of the
iteration you basically refine your
solution a little bit more okay and you
need a control program which is outside
of the framework MapReduce to basically
first of all initialize then say okay
now start one iteration wait for that
MapReduce job to finish get the output
from the MapReduce job take that output
of the MapReduce job and pass it on to
the next I mean there will be some
stopping criteria but take that output
of the job and pass it on to the next
MapReduce job etcetera right so okay so
the for the map in that case and this is
one iteration of that k-means clustering
is for the map a bunch of data points X
I some some initial estimate of the
centroids or the estimates that are
derived from the previous iteration of
the MapReduce job and then you are
basically finding out for each X I which
centroid it is it is the closest to
that's pretty much what it is doing
marking that X I with that particular
centroid and passing it up okay and on
the reduced side now basically you have
all these new x i's and the associated
with the old centroids now you take the
define this cluster by saying all these
X I am good
you know fine which is the what is the
central for that you update the centroid
that becomes the output of each
iteration that goes on to the next
iteration right okay so you basically
have endpoints and K as the clusters and
you have the distance computation is OD
especially where the complexity of OD or
the amount of computation that you do in
order to compute this distance if this
is high then this is a really good
candidate for MapReduce if this is very
low in that case the each iteration
finishes in a very short time in that
case your job launching overhead etc
will be will be really large right in
that case it's not a very good very good
solution for or candidate for MapReduce
well so based on these initial
experiences of really tackling large
data and really having at least some go
to machine learning methods maybe about
20 or so methods right a project was
born in again in the Apache Hadoop
ecosystem called Apache mahute Apache
mohith has been around for almost three
almost four years now
I think most of the times it is used for
recommendations engine with with
collaborative filtering zan and I think
frequent itemsets mining as well is a
very go to method in a passive mode
right the goal is not to make the most
performant machine learning library but
rather the most scalable machine
learning library right given the amount
of time that people put in to bring
these algorithms at least my feeling is
that most of the machine learning
algorithm implementations will always be
open source right okay I think it will
come up all right no problem so so it
will always be open source because
people are going to make modifications
and go tinkering around that right I
haven't seen a single machine learning
package well
sass aside which basically you don't get
the source far and he just straight away
start programming hey so having an open
source scalable machine learning library
was a was felt to be an atmosphere at
most importance and that's how a possum
who was not
mouth was
I would say unfortunately there hasn't
been a really you know consistent
corporate sponsor for a pie scam who
basically takes the ownership of mahute
and and keeps it keeps it updated easily
deployable easily operable etcetera and
that is why the development on Apache
mode has stalled a little bit but at
least still for somebody to get started
with machine learning on Map Reduce I
think that mouth actually provides a
very good candidate right these are this
is basically a list of all the
algorithms in mahute most of the times
as I said the item based recommendations
is the one that I've seen most often
being used inside of with mouth but
again the problem with mahute are also
the same things right even if there are
some sequential methods in included in
mahute the bulk of why people use mahute
is basically because it is a ability to
handle large amounts of data and to use
MapReduce and therefore with the hype of
Hadoop you basically sit in the Hadoop
ecosystem you are golden right so that
is why people may that is why ma who has
benefited but then the challenges with
mahute also remains I mean the same
challenges with MapReduce remain with
mahute as well is that MapReduce is
optimized for really large data
processing I mean most of the times you
are filtering data you are preparing
your data or doing machine learning
after you have prepared your data the
data size becomes small but in some
cases such as in Yahoo case for example
in the case of Yahoo we had something
like 10 terabytes of web logs and click
streams etc coming in every day so if
you had to analyze 13 months of data in
order to determine people's click
patterns over the years right that
basically becomes a really large-scale
problem and that's where MapReduce
really succeeds or really really excels
right
and again as everybody said MapReduce
the main problem is that iteration is
not a fundamental concept in MapReduce
and Dev and and most of the machine
learning algorithms are iterative and
therefore the the overheads of job
launching each each time you launch an
iteration essentially takes over the
entire efficiency that you have multiple
scans is a huge issue because my
produced does not support a good
in-memory caching solution so that you
you cannot have you know once you read
read the data if it is going to be
needed again for the next iteration so
you cannot keep it somewhere close to
you you can at most you can keep it
close to close to you on the disk
but even that disk is were I mean on the
local disk but even that local disk is
wiped out after your job finishes ok so
there is no real good way there is there
is we started a project called
distributed cache and that distributed
cache feature is actually in in the in
the Hadoop mainline code but on a
multi-tenant cluster since there is no
since this is distributed cache not a
resilient data set like spark has right
there is no way to guarantee that the
next time my job comes in and my
iteration starts I will find that I so I
won't be able to find that data in the
cache I will have to again read it from
from the disk and obviously as I refer
to in the k-means example you need a you
need a controller which is running
outside of the MapReduce framework
typically on a client machine now the
client machine also has a chance to go
down sometimes so you have to do you
have to implement the fault tolerance
for the controllers if you have really
long running iterations or long-running
iterative jobs inside a MapReduce
cluster okay
and and the the controller has to do a
bunch of things obviously it has to do
coordinate all these multiple MapReduce
jobs that you are running in each of
these iterations right it has to
implement a stopping criteria it has to
sequentially and typically this
controller will be sequential running
outside of the MapReduce cluster so it
has to again between the iterations it
has to go to HDFS look at the last the
data that or the output that the last
MapReduce job produce and sequentially
determined whether to continue or not so
especially when your MapReduce job which
is doing an iteration if it produces a
large amount of data in that case
obviously the sequential bottleneck
comes into picture right all right
the task initialization over its is
something that that comes up again and
again so it's not
the task assignments and tasks
scheduling on a particular machines all
the Program Files and all the libraries
like side files libraries etc that you
have to you need in order to run your
task they have to be downloaded from the
shared storage like HDFS localized
inside the tasks class directory and
that actually adds a lot of overheads in
the in the mapper and reducer tasks when
they are when they are started and
obviously map this is almost always
shuffle bound all the applications that
have seen so far in my life and
MapReduce
incur the maximum amount of overhead in
data being shuffled from the mapper into
a onto the reducer and the reason
obviously being that in order to provide
fault tolerance the mappers has to write
the data on to the disk and reducer has
to pick up this data from the disk from
and typically the remote machine
transfer over the network the network
used to be a huge bottleneck earlier
when we had only about a gigabit of
bandwidth within our cluster in that
case maybe about two gigabit of
bandwidth this may be about some six or
maybe about five disks or something like
that or maybe even less but now we are
talking about ten gigabits and now the
disk has again become the bottleneck and
network has sort of grown in bandwidth
so map actually gets executed closer to
where the data of fragment already
exists like input split already exists
but there is no locality between the
maps and reduces the reducers scheduled
completely independently so the the the
default scheduler in hoop has no notion
of where to scheduler reduce closer to
where the map is right so the reducer
and typically that is why the shuffle is
almost always on the network right the
map side efficiency comes from getting
data local to the disk but shuffle there
is no no other option
well the the jobscheduler in MapReduce
basically places the map tasks close to
where the split is and the split is
corresponds to HDFS data block which is
replicated on three machines right so
the job scheduler has a choice of three
machines in order to locate the map as
close as possible yeah oh yes yes yes
absolutely
it's coming from the disk so you are
going to deserialize it anyway right
yeah yeah so I if you look at the spark
working rdd's I think the one efficiency
in RDD actually are resilient data sets
actually comes from storing it in memory
but the second thing is actually this
deserialization and serialization cost
because really the the D serialization
cost actually plays havoc on the CPUs
the 50 megabytes per second per disk
output that input that I am getting the
reason that it is getting to 20
megabytes per second when it comes to my
map method is because a lot of that is
being spent in D serialization
the compression also bit serialization
okay
all right the the there was another
involve the talks basically there was a
notion of that unless the all the maps
finished reducers cannot start right so
you do not have this streaming kind of
architecture because now maps are
already constantly producing these
records which are transformed they could
immediately determine okay which reducer
it needs to go to and they could go to a
reducer and the reducer could
immediately get started there are a lot
of reasons why this was actually done
and fault tolerance and again giving
reasonable semantics in order to what
happens with the reduce reduced our task
is restarted for that reason we cannot
have maps streaming data directly to
reducers right and that is why there is
this blocking this this huge block
introduced between the between the
mappers and reducers right yeah
that that would be so you would
basically take at least the five format
conversion you will have to do right I
mean you'll have to put it fitted in
some some way in the file format right
this parts are you are you actually
storing it in a in binary right right if
that is the case then there is no
deserialization oh most yeah most of the
times people use the standard like text
formats and then sequence file formats
and then these are readable again only I
mean text formats are readable by
multiple languages but sequence file
format which is one of the very commonly
used format in Hadoop is readable only
by Java so what people have to do is
basically store it in data structures
which are java friendly right and arrays
in particular multi-dimensional arrays
are not very java friend right sparse
arrays you could represent by a column
row comma column comma value but then
when when it actually gets in there your
sparse array I mean in what format it is
going to be available to your sparse
matrix computation library might be
completely different plus this data is
shared across multiple applications
which basically means that you have to
choose the lowest common denominator in
order to store this data so that it is
possible to be used in multiple
languages and applications written in
multiple languages and multiple
frameworks so that's another thing lots
of reasons especially when it comes to
multi-tenant I mean this is okay I'll go
on a rant here but but this is the
difference I think is between doing
research to produce a paper versus
maintaining a huge system which is going
to be used in completely unpredictable
ways by people coming from all walks of
life and and antonin with completely
different aims and and producing
completely different output right and
that is why you have to take this I mean
you can you we could have optimized I
mean it's not that there is no room for
optimal week we could have optimized if
we knew what use case to optimize for
right
since this was a completely multi-tenant
cluster hundreds or thousands of
different use case
we had to resort to a lowest common
denominator and that's where all the
that's where all the inefficiencies I
mean most of the inefficiencies come
from them all right
so in order to implement iterative
algorithms in MapReduce the only shared
patient that you have to share the state
among different iterations in HD f is
HDFS right the overhead per iteration as
we talked about job setup is the big big
overload data loading again and again
you have to scan the source data and the
whole disk i/o involved in sharing the
state in the distributed system and
getting it back on to the next time next
map right obviously the world was not
you know just following MapReduce
blindly or for a Hondo Yahoo or Facebook
blindly they were actually suggesting
enhancements right a lot of these
enhancement where basically let's look
at one aspect of MapReduce and try to
fix that right that's base I mean the
inefficiencies coming in MapReduce and
let's try to fix that right basically
over the last four or five or four years
we have actually seen a lot of papers
being published the worker aggregator
framework actually came from Marcus
Weimer
haloo came from University of Washington
magda MapReduce online came from Yahoo
there is another paper called I am a
produced spark came from work Berkeley a
bunch of those but they try to modify
the MapReduce working model while trying
to first of all keep the connectivity
with HDFS because that's where your data
is if you have a machine learning
framework that does not connect some way
with the Hadoop distributed file system
that framework has very low chance of
succeeding right that's that sort of the
reality accepted by the marketplace so
they try to keep that they try to keep
some sort of fault tolerance aspects
right not all these enhancements that
are that are proposed actually have the
same fault fault tolerance property as
as the rest of MapReduce so I'm going to
briefly touch on only those only the two
that I found being actually used in in
in practice so Marcus Weimer sand and
Tyson conned his work on worker
aggregator basically just reduces the
shuffle phase completely right so to
just launch a sum app to this job a
bunch of map tasks
last and these map tasks essentially are
doing the iterations in each of these
tasks at the end of each iteration each
map tasks talks to an aggregator which
is just sitting there right as a demon
you tower talks to an aggregator
deposits estate the aggregator wakes for
all the P 1 to P and mark map tasks the
operation outputs to get in there that's
some combination of that and actually
communicates the result back to all the
mappers and then they could they start
the second iteration at the final output
then is written through the aggregate
under and the stopping criteria etcetera
is actually implemented in the aggregate
right so what it gets rid of is
basically first of all the state being
shared across as a through HDFS after
each iteration so that's one thing that
it gets rid of the second thing that it
gets rid of is actually having to
schedule this job again and again and
again part of every iteration the map
tasks are just staying there okay yeah I
already staged that all right the second
one which actually I liked a lot
personally when I actually read the
paper I haven't I don't know whether it
has been actually used in production at
all at least I like the name a lot right
it's called helu so it's basically
Hadoop optimized for iterative
computation it actually adds new api is
in order to be able to program these
iterative hadoop computations or
iterative machine learning applications
the algorithms very nicely okay so the
the second big part of a loop actually
is the loop aware task scheduling
right so it knows that a haloo program
typically goes through a bunch of
different loops right and in order to
provide fault tolerance the tasks have
to finish and then have to restart again
right so those two things are seemingly
incompatible and they actually try to
combine that by making the scheduler
aware that this task is going to be
again executed in a short amount of time
so probably I should reserve that node
for this task that is coming right that
reduces the utilization a little bit
obviously because
while that task is not running and we
are waiting for the controller to large
the task again we are spending some time
on the node which without doing anything
right but at least gives a sort of an
notion of efficiency in in scheduling
these iterations again and again on the
same same amount of nodes right the
second thing that it does is since there
is a high probability that the same
instance of the task in the next
iteration is going to be started on the
same machine in a short amount of time
now I can actually use the local machine
either memory or the disk as a as a
cache in order to share this data across
these different iterations I do not have
to go back to HDFS and start fetching
all this data again I can actually store
it on my local disk and if I have a if I
have if I have SSDs or anything like
that on my local disk or if I have large
enough memory in which the data that is
being written to the local disk is
actually buffered before the next task
attrition comes in so that I can read it
from memory and at a at a much higher
bandwidth so so the amount of time that
it takes in order to read the next data
for the next iteration is going to be
really small so so I mean let's just
think about one single job right I mean
so it is this is being optimized for one
single job that is coming in again and
again as part of these different
iterations that's pretty much what it
what it is right now given the dump that
given the way MapReduce divides the
resources among among different users
using either fair scheduling a faceted
schedulers multiple users jobs are going
to be cascading anyway right so so did
they the ha loop in particular does not
make any you know modifications to that
part it only makes modification to a
single job scheduling problem right but
now that this not now that I mean the
whole thing is now that since the
scheduler itself is loop aware it has
you know it can take the liberty of
caching this data and making sure that
it is going to be available again this
is the cache right this is I mean if
somehow that machine goes down or the
cache somehow gets wiped out you can
again read this data so only the cap
only the loop-invariant data in these
jobs those are cached okay and this is
the same principle that is used in
already DS right I mean jeez I think
inspire caddies came much later than the
Helou quark but her loop essentially had
had this this notion much much much
earlier this is actually Java serialize
data again it actually Ricci realizes
that data the only thing that it it gets
rid of is not having to read it from
HDFS again so trying to contact the name
node and then going to the data a node
and fetching it rather than that I am
just going to keep it in my local disk
and when I keep it on local disk I'm
good again go to serialize it and and
since this is taken again taken care by
the framework it does it in the local
boom boom you know the least common
denominator
so the yeah no no no so one should be
sterilized you get it into memory right
what what her loop is trying to do is
basically pushes pushes it again into
the local disk rather than as DFS when
you have to push it on to the local disk
there again Reece a realizing it right
so that that's one that's one if you a
fishin see that they they did not get
rid of the LEDs on the other case it was
in spark they got rid of that or at
least have plans to get rid of that
right okay so I think we have we have
talked a lot about about spark and were
to switch that switch away from spark
but what we have seen now happening is
that since all these deficiencies of Map
Reduce were brought forth by the
research community and even by the
industrial community right we had
already started rethinking about the
Hadoop architecture itself right so the
whole notion of MapReduce being the only
programming paradigm that that runs on
on top of Hadoop was already going away
right what what in one of these talks
have actually heard somebody say that
Hadoop earlier used to be something like
a regular phone in which you used to
just only be able to make calls
now with the now Hadoop has becomes out
of the smartphones now that clearly you
can develop apps on top of it I mean so
it has becomes out of our chest rather
than being a single tool right so all
that has been done is that the job
tracker and the task tracker framework
which allowed MapReduce to be run on top
of HDFS that has been now refactored
into a cluster resource management
versus MapReduce okay and since the
cluster resource management piece which
is now called young has been completely
modular lis separated out now we can
have multiple different programming
paradigms all that they need to do is
talk to yarn in order to gather
resources and after that they
essentially to take on the cluster right
so because of that what has happened is
that already we have seen a lot of other
programming paradigms or different kinds
of services I would say being
provisioned on top of Hadoop or on top
of Jana so so now do
has truly become sort of a smartphone
off of you know data processing
frameworks again for the lack of time
I'm not going into the deep architecture
discussion so as I said the job tracker
and the task trackers have now been sort
of separate modularly separated out
instead of a job tracker now there is a
resource manager which does not know
anything about Map Reduce all that you
ask the resource manager is basically
give me a container that can have two
gigs of data one physical core and some
experiments of bandwidth that's pretty
much it or something like that right and
then individual nodes run node manager
again node manager knows nothing about
MapReduce node manager basically knows
okay these are the containers that are
living on my machine are these
containers alive or dead that's pretty
much it
right so with this refactoring now
MapReduce becomes an application that
runs inside of this this framework so
the notion the the role of the job
tracker in the earlier hadoop Renato is
now simply an application launcher and
application scheduler that's pretty much
what it is now okay so taking advantage
of this you basically seen the 40-year
old
sequel language to actually make a huge
comeback on to on to huddle right in its
MPP form so what we have seen in the
last year in the Hadoop ecosystem is
basically a resurgence of sequel on
Hadoop offerings obviously pivotal my
company was one of the first ones to
launch something called hoc which was
the greenplum database running on top of
Hadoop but we have seen cloud era
Facebook map are with their Apache drill
Hortonworks all these companies actually
is clamoring with their own sequel
unheard-of solution the way sequel on
Hadoop solution typically works is that
sequel can be implemented very nicely
with this MapReduce like programming
model where you have a gang of
processors which are basically
communicating in a shuffle like manner
across different stages right and the
only difference with with MapReduce is
that instead of storing the data on the
disk between those two gangs and that is
materializing this data you basically
transfer it over the network right so
whoever has the most efficient
interconnect in the MPP sequel basically
vents right so that's basically where
we're variable so
it turns very similar to what a typical
MapReduce program runs so instead of a
map task and a reduce task everything is
basically a faceless it's called a
segment or something like that in case
of a Impala I think it is called a
partition and then these basically are
scheduled as close as possible to the
data which is right on the machine that
is running the data node so they are all
reading in local data and then once they
process data in some way they are
actually transferring this over the
interconnect which is rather than over
the network rather than pushing it onto
the disk and this is again sort of
getting future proof because the network
the network speeds are increasing much
more than the disk speeds right so again
the network bottlenecks are getting
lesser and lesser in in large large
scale commodity clusters all right just
to give you a performance numbers this
is preliminary benchmarks of all the
different I mean this is I'm just
comparing there two of them hive is
basically getting modified and and into
a project called stinger just getting
more and more sequel in that so I think
it makes sense to compare these MPP
sequel databases with hive
if you can see since hive uses MapReduce
underneath today it takes a long amount
of time this is the 22 T PCH queries and
the in-memory transfer based or network
transfer based MPP databases running on
top of Hadoop are actually significantly
better sometimes 50 to 100 times better
than that right nothing surprising there
okay but what these new MPP sequel
databases on top of Hadoop enable is
this whole notion of in database
analytics right during Hellerstein and a
bunch of people from green plum and
University of Florida and University of
Wisconsin Chris Rea from University of
Wisconsin they have been working on this
open source platform card or open source
machine learning libraries called mad
lib that actually are the set of
user-defined functions that implement
various machine learning algorithms
inside of the database right so the
database executors that means database
execution engine is actually running
close to where the data is and it is
actually running all these machine
learning
UDF's also inside of that that is the
whole idea of off madman and now with
MPP databases making a comeback on the
in the Hadoop infrastructure now the
hawk of the the kinds of hawk database
engines running on top AHA do we can
actually now support mad lib irony
running inside of that right so taking a
big picture as to where Hadoop is going
basically you can see that the maglev
algorithms are sitting right inside of
the hawk advanced database services
their Impala also has I mean this is not
a marketing talk the order has Impala
has also started supporting MATLAB
inside of that a bunch of different
functions are already already given
there the way to access Madlib is
basically through a sequel interface so
for people who are used to sequel it's
very natural for them there are some
people who are not used to sequel and
therefore you know going into the Select
statement and doing k-means inside of
select probably it does not make sense a
lot right but for people who are mainly
business analysts and want to sort of
you know do some machine learning I
think this is a solution for these guys
right ya know they have been told to do
this they read the manual and they have
been told to do this and they basically
try it out right so that's basically the
solution for them yeah yeah yeah let's
let's try to wrap it up okay just maybe
maybe a last example that I will give is
basically how to combine this MPP sequel
engines and are okay earlier what we
used to do is basically are used to get
the data from the database using an ODBC
connection load it into a data frame
inside of ours memory and then then do
the computation right
what pivotal are and this purity Allah
is again another open source project
it's actually available on cram you can
download it from there and and it
actually works with Madlib so what it
does is the interface remains the our
client we override the DV dot object
inside of our and basically do this lazy
evaluation of our functions using and
doing in database
right so the execution of the our
application is or most of the parts of
our which is actually dealing they are
our programs that is dealing with the
data is actually happening inside of the
database so that's the big difference
between regular R and and and this way
of doing things okay
and the parallel parallelism which is
how many shards is this data split into
how do I access these in parallel how do
i implement this particular algorithm in
parallel that is all access that that is
all I handled by pivotal are currently
it does not support the full our
functionality it only supports a few few
of them and I have a slide on that but
this the simplest way of doing this is
basically okay
instead of data dot frame I will say DB
dot data of dot frame give it a table t1
and then once that happens then I can
actually access all its columns and all
the types inside of the database
directly in in our functions okay so
it's also a wrapper for all the Madlib
or some of the some of the Madlib
functionality and then it also supports
a bunch of other functions or a bunch of
other operators in are already okay and
and most of these are implemented in
fact okay just to give you a
architecture of this this is basically
how it is implemented we derived from
debated objects whenever some operation
needs to be done on this debater data
dot frame through our query we basically
actually go and and do this in in the
using the Madlib wrapper functions etc
inside inside the database right not a
lot of interesting thing if you are
coming from the learning background
rather than systems background right so
the good part of the good good part
about this is that all the data stays in
the database right the are objects are
merely pointing to that they are not
actually loading this data into memory
so the memory limitation that okay if
you have a terabyte worth update of a
database out there loading all that
terabytes in in memory for a sequential
or machine and then doing computations
there you rather push those computations
inside the parallel database machine
itself right that is what is basically
happening
and obviously the database talks sequel
so from our somehow these sequel
statements have to be generated in order
to execute MATLAB or other parallel
machine learning algorithms inside the
database so all that is taken care by
the pivotal arc line
only the strings that is the sequel
statements and the results returned by
sequel statements those are the only
things that are being communicated
between R and the database okay so
that's where we are going
I have a few more slides and actually
just let me mention one thing all these
new I mean Hadoop clearly is moving
beyond MapReduce right and since all the
data is already there if your
computations are moving to the data then
your programming frameworks will be
moving to Hadoop that's basically what
I'm saying what I'm hoping will happen
and I think we have already seen a lot
of evidence by that we have actually
seen graph processing continuous event
processing like streaming of Britishness
like storm even long-running services
like HBase that are required for serving
real-time clients that's that's being
ported on top of yawn and a personal
favorite of mine because that combines
my first 10 years of HPC and next 10
years of Purdue into one which is which
I cleverly called
hamster is actually Hadoop and MPI on
the same cluster it's actually running
MPI jobs by scheduling yawn containers
and then doing openmpi computations
inside of there so that's happening
that's coming pretty soon in February we
are going to make a cheer and because of
hamster which is MPI on top of Hadoop we
get GraphLab for free because graph lab
uses MPI as its communication library
and HDFS interfaces or HDFS data
interfaces so since GraphLab
computations run directly on top of MPI
with Amster we get GraphLab for free so
that's all coming in February all right
you know
yeah sorry proto buffer so protocol
buffers is basically a serialization and
deserialization mechanism the already I
mean a lot of people already support
that there are protocols etc for
MapReduce and for it for a heart we have
a service called pivotal extension
framework that understands protocol
buffers the second one was distributed
hash tables distributed hash tables
actually there was one big block diagram
that block in that block diagram that
has skipped which was called gem fire XD
gem fire HD is in memory data cache
which is sort of a key value pair or you
can use HBase as a distributed hash
table right space is essentially
distributed - yeah yeah yeah
so there could be temporary tables there
could be temporary tables that are
created but only during the execution of
that program right so for example in K
means all these centroids etc that are
being stored are stored in a single line
temporary table right and we have enough
intelligence in there in in in the
database itself to sort of do garbage
cleaning and and clear those temporary
tables all right
thanks thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>