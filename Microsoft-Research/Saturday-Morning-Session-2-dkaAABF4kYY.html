<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Saturday Morning Session 2 | Coder Coacher - Coaching Coders</title><meta content="Saturday Morning Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Saturday Morning Session 2</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dkaAABF4kYY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
welcome back is this session on
compressive sensing and rolling shutter
the first talk is titled the flutter
shot a video camera for compressive
sensing of videos and Jason will give
the talk thank you is the sound okay
everybody's happy all right thank you my
name is Jason and I'm going to be
presenting our work on behalf of our
collaborators oshawa shook and salil and
what we did is we are interested in
taking a high-speed video but only using
a low speed camera to do it and what
we're hoping is that by doing this we're
going to be able to avoid some of the
problems inherent to using a high-speed
camera so first let's talk about what it
means to take an image so here i have a
my low speed camera and i'm taking a
picture of the scene I opened my shutter
I take a picture my shutter closest and
this works great for static scenes
especially if I use a tripod the problem
comes when I have motion in my scene
especially fast motion so here I have a
cat jumping at a toy I'm using my same
camera I'm going to open my shutter is
going to stay open for my exposure time
and then close and of course everybody
knows what what's going to result is
that in areas where there's a lot of
motion I get a blurry picture and this
is not very nice however in areas worth
noting moved in static portions of my
scene I have a really nice high spatial
resolution so I have to have this this
conundrum and we really see this in a
high-speed camera right so here I'm
going to try to take a video on a budget
so I'm a grad student I need to work on
my budget I have 30 million voxels that
I can use any way I want and one way to
take a what are you shown what I have
just shown a high speed high spatial
resolution image at 30 frames per second
but if I want a high temporal resolution
image so high speed
at a thousand frames per second my
spatial resolution suffers and as I
increase my frames per second I have
light throughput issues so we're going
to try to avoid that using computational
imaging and we're going to do that by
the introduction of this optical coating
block in our diagram or in our motion
model and if my optical coating is a
linear operator and I just take linear
measurements i can write this very
nicely as a matrix multiplication
problem where my observation y as a
vector is nothing more than a linear
combination of my real scene x that's
operated on by my coding block and for
this camera the flutter showed a video
camera this is a very simple operation
it's just going to be coded exposure I'm
going to open and close my shutter as I
take my picture so unfortunately there's
a small resolution problem here so other
techniques that have been used include
spatial multiplexing such as a single
pixel camera and you're in hear more
about how to use this or use a single
pixel camera for movies in the next talk
but this is a complex hardware and you
but you get full control of that a
matrix anything you want to do you can
achieve with the single pixel camera you
could also do per pixel shutter control
which is implemented on an el coss
liquid crystal on silicon mirror and it
gives you per pixel coded exposure you
could also use a purpose of sensor
control which is directly implementable
implementable on CMOS camera
architecture which you might have in
your cell phones in your pocket or this
being Seattle in your jacket pocket and
this because it's going to be directly
in from the phone on SI los it's a
single bumper pixel per frame and what
we want to do is is something a little
more simple and its global shutter
control so i'm going to open and close
my shutter throughout my integration
time
and which you may notice is it as we've
been advancing the complexity of the
hardware modification has decreased
unfortunately this also means that my
control of the a matrix has decreased
but it's something that we're gonna have
to live with so again here's my my
recovery model so given Y and a I want
to find X or some excess timate of X but
this is severely under constraint where
the number of measurements why is far
less than number of true elements n so
in order to recover my video I'm going
to have to introduce video priors to
help guide my reconstruction and I'm
sorry for the darkness here and various
seen assumptions have been used to
influence which priors we use if I have
periodic or quasi periodic seen encoded
strobing they've shown that we can get a
TX compression if my my scene a bay is a
linear dynamical system then I can get
20 to 50 X compression if I have linear
motion with known velocity then the
flutter shutter camera which is the
antecedent of art of our work has been
used to recover d blur images but for a
more general class of motion that
doesn't have these these assumptions
some notable works shown here we're a
little bit more limited to six to 16 X
compression so with the video priors
that I'm going to use are twofold so I'm
going to assume that I have two classes
if my scene motion is locally linear so
in small patches i can approximate
motion as linear motion and this is true
i'm going to use a union of subspaces if
i have general motion then i'm going to
use the different reconstruction based
on TV minimization so here is a video of
the lenin image it may not look like a
video but it is a video but it's moving
with zero velocity
so if I were to look at a patch of this
video an 18 x 18 x 24 patch as it
evolves in time and i would look at any
X Y pixel and plot its intensity over
time I would notice that it doesn't
change another way to show that is I'm
going to look at the xt or the YT slice
XD syt slice whichever you prefer and
we're noticed that it's completely flat
and what this suggests is that these
patches actually lie on a subspace and
that if I were to learn the subspace of
patches with zero velocity then I could
recover this and so that's what I do if
instead the lenin image moves down at
one frame or one pixel per frame and i
look at my video cube that same patch of
the eye i notice that all the pixels are
still following the same motion but the
angles changed which implies that i have
a different sub space so i learn that
too and in fact I actually learned this
I learned 521 different velocities so at
40 angles and 13 speeds up to about 2
pixels per frame and so if i have a
high-speed video i can approximate it
using the subspace and also what this
means is that my high dimensional patch
18 x 18 x 24 can really be expressed
using a lower low dimensional subspace
which is going to be incredibly
important for compressive sensing so if
i take my high speed video and i apply a
linear operator which is exactly what my
a matrix is I get another subspace I get
it so I go from a high speed subspace to
the low speed subspace and this is where
we want to be so I have an observation
patch again this is all patch based so I
take little patches and then I overlap
them an average in at the end so i take
my patch I project to the nearest point
in the subspace get the coefficients I
bring them back to the high-speed
subspace and get my approximation so
really the the only challenge is making
sure that we're hitting
to the right point the nearest point and
so here for some some unknown velocity
that's not part of those 521 that I know
I show us so here's a simple
reconstruction and we noticed the error
between our estimate and the the ground
truth is a minimum at the nearest point
and even if it's even if there's a
little bit of noise or a slight
perturbation we're in the right
neighborhood so our reconstruction
should be should look good and so that's
what we have here here's that that whole
video which I've taken patches out of
that are overlapping moving at that
unknown speed and the reconstruction on
the right and so I am able to recover
this with a peak signal noise ratio
about 35.5 DB but not all scenes follow
this locally linear video assumption
that I have so if for more general class
of scenes we notice one we notice a
couple of things videos like images have
a sparse distribution of their gradients
and we know that the TV norm has been
used to help an image deep blurring so
in a 2d case so we want to extend that
to 3d unfortunately we at the time that
we were doing this we didn't we couldn't
find a really nice fast 3d TV minimizer
so we ended up having to we ended up
using a package in MATLAB and that we
did this on xt slices so we would so for
each slice we would reconstruct using
the TV minimizer and then evolve in
space and then add these together so
before I show any of the portion of the
experimental results here a couple more
simulations if I had here i have a
advertisement moving to the right again
x is ground truth or left his ground
truth right as our reconstruction and
here we're able to recover with a piece
of noise ratio about forty point six DB
for more general motion such as this
scene where we have a dancer is about
the clap and a chalk dust this form a
chalk cloud is formed and recover with
my TV minimizer
and we noticed that the the peak
signal-to-noise ratio is a little bit
lower to 28.7 and this is suggestive of
the fact that in our linear motion model
we have a richness of the scene that
we're taking advantage of and here we
have a more general scene so the flutter
show that our video camera F SVC can be
directly implemented on machine vision
cameras so in our lab we have a flea 3
grayscale flea three camera from Point
Grey and we implemented a very simple
setup and we're operating at about eight
frames per second due to some inherent
limitations of the camera and we have
here on the left is for observed frames
so I'm going to take observe frame patch
18 x 18 x 4 and recover a video patch
which would be 18 x 18 x 24 say overlap
i average them so here's my video and
here's a close-up so we see that are the
first bit of the word lollipops is
blurred and if this were a low speed
camera this entire thing be blurred and
as we evolve in time all of the letters
should remain blurred and so here's our
reconstruction and a zoomed in version
of the reconstruction so again we've
gone from four frames for low-speed
frames and recovered 24 high-speed
frames and of course we can do this in
multiple chunks of four so if we took a
longer video we could recover a much
longer video so kind of in conclusion to
wrap up what we want to notice is that
global shutter control suffice is for
high-speed video recovery and what this
really means for us is that as we've
been able to reduce the hardware
complexity from some of the some of the
rigid or first papers we Mabel to reduce
the complexity and only an a minor cost
and quality but
this camera our camera is much easier to
implement on existing camera
architectures thank you oh of course so
the the original simulations did include
noise and in the the camera is there's a
there's a plot in the paper the cameras
fairly robust to additive Gaussian noise
I mean that's that's what we simulated
anyway and and they work they find the
it finds the correct subspace up to i
think is like point 01 a standard
deviation of point 01 on 021 scale yep
right yes so the the recovery is is
simultaneous and what what we do is as
good so in our first case the linear
locally linear model we're observing our
database of high-speed patches using the
same coded exposure so we're blurring
this database that we have to generate
our low speed subspace so when we
project onto that subspace and recover
we're actually taking care of that d
blurring and interpolation all at once
and when we do the TV reconstruction
it's a similar thing as in that we
handle both of them at the same time I
didn't show the the steps for for
brevity
right here sorry mm-hmm excellent
question sorry for not making that clear
no we don't we do not need to know the
motion direction and that's a terrible
way to go about doing this so in the
maybe some other time there goes hey got
very lucky so what we do is we in the
local model we actually exhaustively
search over the database and because
this is formulated as a matrix recovery
it'sit's very low cost to search all of
our patches it actually takes longer to
load the data into memory than it does
to search so we searched exhaustively
overall 521 different velocities and
find the the estimate whose observation
error is the lowest yeah yeah so the the
the observation a right here takes our
database which is very large I mean so
it's high dimensional it's 18 x 18 x 24
dimensional the independent from the
picture itself yeah so yeah so it's
independent of the picture itself and it
searches and and each patch is found
individually so in the in the
reconstruction results which this is not
happy to go through you'll notice that
there are some parts that are static and
the thing moves so for those patches
that are static it finds that the
appropriate velocity to pick is 0 and
for the other for the motion it finds
the correct velocity things are just one
more question that's just big one yep
it's a good question so we are we are
working on on that part right now so for
the for the locally linear for the
locally linear model we need to step
back one level because right now we're
only modeling without occlusions in our
in our database and for the for the more
general motion for TV it just turns out
that the camera the way that we
implemented it we didn't use a
ferroelectric shutter and our frame rate
was very low and I was unable to move
things that fit the the model so at
eight frames per second I found it very
hard to move things in any sort of
pattern that wasn't like jumping
completely outside of the frame that was
a personal experience limitation I think
okay so you just heard Jason tell you
about how to take a low speed video
camera say something like a 30 frames
per second camera and convert it to a
high speed camp or something like a 200
frames a second camera and the idea was
that during each exposure of the slow
speed camera you introduce a temporal
code and this temporal code allows you
to convert this load temporal resolution
camera into a high temporal resolution
camera I'm going to talk about something
complimentary here we are going to talk
about cameras which are very poor
spatial resolution so if you have very
few pixels in these cameras and we are
going to introduce spatial mask spatial
multiplexing mask that allow us to
convert a camera that has low spatial
resolution into something that at high
spatial resolution and we are going to
call these cameras broadly a spatial
multiplexing cameras what I'm going to
talk about is some of the basic
trade-offs that come about in the
context of these cameras and I'm going
to derive a video compressive sensing
algorithm that works for these spatial
multiplexing cameras so what are spatial
multiplexing cameras I think the example
I am going to use for rest of the talk
is a single pixel camera as its name
suggests a single pixel camera as a
single or director a single photo diode
that's due to some wavelengths of
interest and
to continue something that's been going
on all this talk here's my code Jason
just mentioned that we have a resolution
problem and clearly with a single pixel
we do have a resolution problem when we
deal with single with single pixel
cameras so the question is how do we
circumvent this resolution problem the
idea is we introduce a spatial light
modulator and this particular spatial
light modulator we introduce is a
digital micromirror device so the camera
works as follows light from the scene is
projected on not to the sensor but to
this digital micromirror device at each
pixel of this bike Romero device you
have a tiny mirror which can be in one
of two states when it's white the light
from that pixel goes towards the photo
detector and when it's black the light
is just discarded so what we end up
getting for any particular pattern on
those on the on the mirrors is that the
photo detector sums up all the pixels
that are white so what we get is a
linear measurement of the scene and by
flipping through random combinations of
these mirrors what we can end up get as
different set of linear combinations of
the pixels in the same or different
linear measurements of the scene now
once we obtain enough linear
measurements we can invert to obtain the
sea in either using least square
techniques or more compressive recovery
techniques right I think the main reason
why this camera makes sense when we are
sensing in non visible wavelengths
typically in many way our wavelength
bands and shortwave infrared and medium
wave infrared sensing materials are
costly as a consequence it's very
difficult to build sensor arrays of of
the resolution that we care about so
instead of single pixel camera tackles
that by putting us single sensing
element at the resolution we interested
however gives a spatial resolution by
introducing a light modulator in the
optical path way the problem is that at
any time in soon we get exactly one
measurement and if you have to if you
need multiple measurements recover a
scene we need to integrate over time it
accumulate measurements over time to
recover the scene and this somehow makes
the assumption that the scene is static
over a small duration of time and if
then the scene is static if the scene is
not static or not slowly moving or a
small duration of time
what ends up happening is that every
measurement is of a slightly different
sea and the question is how does this
what is how does this affect our
recovery algorithm how does affect our
recovery process media interest in
seeing images and videos and that's what
I'm going to talk about for rest of the
talk so let's do a simple thought
experiment we start off with a
time-varying seen in this particular
scene you have two vehicles that are
just crossing each other I'm going to
sense the scene with a single pixel
camera and every time instant I'm going
to measure exactly one measurement and
recall that as I'm sensing measurement
the scene is slowly changing during this
measurement process so I'm going to take
a window of W measurements I'm going to
take WWE the number of measurements I'm
going to take and given that W
measurements I'm going to recover the
scene as though it were a static scene
so I'm going to forget the fact that
things are moving take W measurements
and try to recover the sea as so it's a
single static image or the question I
let's see what happens when we do this
process to answer this question we need
to actually look at different time
scales suppose i take very few
measurements aw is small if W was small
then I'm taking very few measurements
therefore the time window over which I
took these measurements is small if the
time window is small it means things
have moved very little and therefore I
expect to get very little motion blur
however because of the fact that I have
very few measurements I end up getting
very poor reconstruction clearly if I'm
sensing a high-resolution seen you need
lot more measurements and you do not get
that by taking very few measurements at
the other end if you take a lot of
measurements say we take we take
measurements over a long window of time
we do end up getting a lot of
measurements so we can go ahead and try
to recover the scene at very high
spatial resolution however because our W
project means the window of time over
which we took these measurements is also
going to be large and have consequence
what ends up happening is as dimension
blur in our recovered research and I
think the straight of his fundamental
almost all video cameras its basic
trade-off between spatial resolution and
temporal resolution at which we r sensor
seen in the context of a single pixel
camera we end up getting this resolution
is also tied to the number of
that we take so at one end we have very
low spatial resolution but we also have
higher temporal resolution and the other
end when we take a lot of measurements
we have very high spatial resolution but
much poorer temporal resolution that is
a sweet spot unfortunately the sweet
spot I gives us very poor so both
temporal resolution and also the
reconstruction error is very high right
what we ideally like to have is that
that sweet spot to be as low as possible
so that we get good quality recovered
image and to be as to my right as
possible so that we get good temporal
resolution so that's what we are aiming
for and the question is can we how do we
go about getting better trade off from a
device such as this another point to be
made here is that the sweet spot moves
is dependent on the scene if you have
faster moving object it moves to its
right and if you have slower moving
object it moves to the left and so on
but for this for this particular since
I'm fixed my scene we can continue with
this particular example so the question
is how do you move the sweet spot low
and how do you move it to the right
right so that gives us better temporal
resolution and uses higher quality
imagery the answer lies in the fact that
we are trying to recover a single image
at a time our videos are clearly more
ridden than than that that is and this
is clearly and we all know this due to
success of the video compression
literature and the idea is that if you
need to exploit motion in the scene we
need to do things like motion estimation
and motion compensation things that stay
up there compression algorithms to to
somehow reduce the number of degrees in
the freedom and not have to solve for
individual frames at a time can we do
this the problem it turns out is not as
simple in the cup in a case of
compression we have access to the frames
of the video so we can compute motion we
can do all sorts of fancy motion
compensation in estimation techniques
however the context of sensing this is
not as easy because we do not have
access to the frames of the video right
we do have an initial estimate right we
do have an initial estimate that comes
about from that what happens if you use
this initial estimate to recover V to
recover motion and try to do something
with that
is that it turns out that because our
initial estimate was very poor our
motion estimates are poor as well so
this is the chicken and egg problem that
we need to solve we need to get very
good motion estimates if you want to
recover good estimates of the sea
however to get good estimates of the to
get good estimates of motion we need to
have we need to have the scene at a good
nice reconstruct in a very nice way and
this chicken and egg problem is
something that is not easily broken when
we do this kind of naive reconstruction
techniques I'm going to skip over a lot
of details here but let me go through
the intuition of how we get through the
solution I think the first thing to
acknowledge is that when we have blur in
the scene blur reduces the amount of
sparse structures in the scene and
compresses and saying all of
comprehending techniques and
traditionally have read on sparse
approximation on an ounce bar signal
models and so on so maybe the first
thing to acknowledge is that sparse
approximation algorithm do not
necessarily work well when you have
motion blurred scenes and so why not try
something simpler just simple least
square recovery so I have a linear set
of equations i'm just going to invert it
without any spots priors this is not
easy you could do this but the problem
with this knowledge doesn't give you any
better trade-offs because we have a seer
sensing and seen it of the high spatial
resolution to get enough compressive
measurements will have to wait a long
amount of time and that means you are
going to get very poor temporal
resolution and a lot of motion blur so
the next idea is we are going to release
/ recovery but not on the scene at its
full spatial resolution but you're going
to artificially reduce the resolution of
the senior going to send the scene at
the lower spatial resolution and the
idea here is that if you have a low
spatial resolution we have a lower
dimensional problem a lower dimensional
problem means we need lesser number of
linear measurements and if you have
lesser number of measurements it means
we are going to take measurements over a
smaller window of time which means
you're going to get a highest temporal
resolution can you just do both of these
and get away with it the answer is still
no we cannot use random matrices that
have traditionally been the focus of
compressors and saying
in sampling the problem is random
matrices are a condition if you date a
large random matrix a Gaussian matrix
its singular values DK linearly which
means when you use it for linear
recovery all in the when you use it with
linear occur if you get huge amount of
noise amplification and the details of
this are in the paper and there are a
lot of crappy images or if you are
interested in looking at the answer lies
in something that was mentioned by Dave
radius today we need to use our matrix
we need to use a class of metrics that
are called harder mad matrices now what
about matrices and this is work that
goes back almost half a century to
century back are orthogonal which means
the singular values of flat there's no
noise amplification but orthogonal T is
not all that we require clearly an
identity matrix provides orthogonal T we
need something else we need maximum like
throughput and Hyderabad matrixes
provide that in the context of sensing
matrices what are sensing matrix is a
matrix is that do not amplify light the
matrices whose entries are bounded by
identity they take values between minus
1 and +1 and among the space of those
matrices are Ahmad is optimal for least
square recovery so what happens if we
now use Hamid matrices but at a lower
spatial resolution the trade of that we
get the sweet spot now moves both down
and to the right so we have gotten we
have gotten better reconstruction at the
same time we are also enabled a higher
temporal resolution right so we know
that we estimate that we get but to
obtain this estimate we had to sacrifice
spatial resolution and that's what we
are after we want to recover the scene
at full spatial resolution and the
trade-off that hard ahmad gives a sort
of strange if you want to increase a
spatial resolution you necessarily have
to increase the temporal resolution
because you're using least square
technique so at the switch what you get
a good result but at a low spatial
resolution if you try to increase the
spatial resolution immediately motion
both creeps in and then you get the
humans that are no longer acceptable
what we would like to have is all the
properties that Haddad provides are a
great initial reconstruction at a lower
spatial resolution with high temper with
high tempo resolution but nonetheless we
would like to keep the properties of
random matrices which are they enable
tl1 recovery with a sparse prior they
provide they provide reconstruction's of
seeing at full spatial resolution and
what we would like to have our
measurement matrices that guarantee both
of these properties simultaneously the
question is how do we go about building
such measurement matrices and we we have
a solution and it's a simple solution we
call them dual scales and sing matrices
here's how we go about doing this we
start off with a row of the Hadamard
matrix so this is just a single row of
the audubon matrix shaped into say 32 by
32 block we up sample this to a final
resolution and on this by nearest
neighbor upsampling once we absent
elytte we had a random high frequency
component to this and take the sum of
this to obtain this measurement vector
and the idea is that this random
component is created in a very specific
way it's made so that when you down
sample this matrix you end up with the
row of the harder mod matrix that you
started off it right so what is this
matrix guaranteed it has all the
properties of Adam are at the lower
spatial resolution nonetheless it has
high frequency content that we will
eventually use in a compressive recovery
technique so the patterns that we put on
the mirror put on the mirror of our
single pixel camera look like that you
can see that there is if there is a
multi scale structure to that you do see
the low-frequency Hadamard blocks in
this case it's scrambled that's why
doesn't look like that and on top of
that there is a high-frequency sort of a
noise speckle thrown on top and that
comes about because of creating matrices
that have these with this kind of
structure and this measurement matrix
allows us to derive video compressive
sensing algorithm that breaks the
chicken and egg problem that I mentioned
earlier so how do you go about doing
that first we may obtain compressive
measurements using these dual scale
matrices once you get these compression
measurements we can take any unit of W
measurements where W is chosen by the
resolution of the Hadamard matrix that
we use to recover an initial low
resolution estimate of the scene right
so this is very fast all we do is take W
measure
apply in verse Hadamard transform and we
obtain those results now once you have
these results we can now go ahead and
run our favorite motion estimation of
this particular case an optical for
optical flow algorithm and now the
motion estimates are much better and if
once we have these motion estimates we
can go ahead and solve for the entire
video all over again our now with motion
constraints on top and what does motion
really where does motion help us so you
imagine two frames of a medium if you
know that certain pixels in one frame
appeared in the second frame you can use
that as constraint to reduce the number
of independent variables that we have so
clearly what we're motion helps us is we
can take a video where things are moving
and sort of register think so that we
are now it's almost like sensing a
static image and sensing a static image
is something that we can do very well
with a single pixel camera all we need
to do is get enough measurements and we
can recover the scene very well so here
is a quick of simulation result so what
you see here is the ground through on
the on on your left the initial
reconstruction but now up sample it by
cubic in the middle and you can already
see that that's very good but once you
add motion constraints on top and solve
for the video all over again you end up
with the final result that looks even
better so we implemented this on the lab
setup that we have so this is a single
pixel camera setup that we have in a lab
this is the target that we use a
metronome that object mounted on it the
particular photodiode we use was an
India's photo detector the census in
short wave infrared we are sampling at
about 10 kilohertz or for every second
we are getting 10,000 compressive
measurements and we are aiming to
recover a scene that had a spatial
resolution of 128 x 128 so we are
looking at a compression of about 61 61
x here's a recovered result top row you
see the initial estimate that we get of
doing this the Haddam r inverse on
windows of our matrix is the initial
estimate that we get once we estimate
motion across those frames and rerun our
video record algorithm you get the
bottom estimated healers video of the
recovered result
just to compare with what happens when
you do when you just purely random codes
and try to get an initial estimate these
are the frames that you get on real late
at the top rope so you just take pure
random measurements and take blocks of
compressed measurement and try to do a
linear recovery and in contrast even for
fast-moving object we get much better
results and I think this shows the power
of both the measurement matrix that we
use which would provide an initial
estimate as well as the power of optical
flow based recovery that really helps in
both d noise but also reduce the number
of degrees of freedom and provide better
image reconstruction so the key
ingredient they're basically two key
ingredients in this particular algorithm
the first is the design of these normal
measurement matrices that have these
dual scale structure and this somehow
helps us to recover get a very good
estimate of the initial scene and when
we couple it with state-of-the-art
motion models that video compression
algorithms use we get very high quality
recovered results this is one of the
first to a best of my knowledge of
practical I'll video record album that
works for the single pixel camera and I
think at the heart of this is
acknowledging the fact that the scene
actually changes during the measurement
process between every consecutive
measurements and there's been a lot of
work in this area including many data in
which I co-authored where we just ignore
this and simulate with 30 frames per
second video and that clearly doesn't
reproduce the kind of real data that the
kind of real later behavior that happens
when we use a single pixel camera
limitations so we do need to know have
some idea of what the speed of object
motion or their at least the speed of
objects in the scene because that tells
us why the sweet spot occurs and what
the what the size of the Hyderabad
matrix needs to be we could of course we
are looking into multi scale
construction that can sort of circumvent
this where we do not need this prior
knowledge another assumption if you're
making is that the motion is revealed
that the low resolution estimate if we
have very fine structure say like why is
moving about we do not detect it in the
lower resolution estimate and that's
something that it's not clear how to go
about doing that and finally everything
depends on how well we do motion
estimation so currently we are just
using of the
alpha optical flow algorithms that do
Paraguay's optical computations but
clearly there's a lot of room for
improvement here we can definitely do
multi-frame optical flow to really make
the motion estimate more robust and a
lot of this takes in the order of
minutes to recover clearly we could do a
lot better there as well in terms of
metal recovery algorithms at saket thank
you for a couple of questions yeah so we
sure so we do get a lot of light in
because at every time instant we're
getting half the light in the scene end
right so in that sense because spatial
multiplexing really helps us handle low
like in fact if you most of these most
of the real data was collected in what
you would call low light situations it
works reasonably well why I'm not going
to use this sensor standard video camera
I can get very high video great question
the patient is why would you want to use
the sensor clearly we want a lot of
effort in terms of building this
hardware and so on and my cell phone
camera you don't have one he provides
much better video quality than this so
what that's the place for this
particular technology the answer comes
about when you are sensing a non visible
spectrum so think of sensing say
megapixel video at 30 frames per second
in infrared or in a shortwave or medium
infrared the best of my knowledge vga
infrared cameras cause upwards of fifty
two hundred thousand dollars and i'm not
sure if there are megapixel video
cameras that sends in infrared and
that's exactly where this technology
finds its niche it's in and it's
generally true of compressive sensing as
well sensing has to be costly if you
want to make if it's useful and you're
the cost comes about because in gas
building sensor is using in gadgets
extremely costly while in the case of
single pixel camera we need exactly one
photo detector two hundred bucks and
Todd labs and we had this set of filming
on a question that came before when you
talking about more noise that's not so
you're talking about noise in the during
measurement process v so we have not
done a complete analysis in terms of
looking at noise but I can show you
results we control the amount of
illumination the scene like reduce
illumination by a forth and so on and it
works reasonably well but to answer
question we have not even done a
systematic study of that but it's
something that definitely in the works
one last question
so the a question is we know that videos
have some sort of subspace structures to
it am I trying to exploit them some
sense i think that i think at the heart
of it is well i'm not directly
exploiting those those kinds of models
because I all I'm using is the yes no
yes in a sense I'm assuming that the
video sort of band-limited otherwise the
aliasing that happens because of
introducing the high frequency pattern
will will give very poor initial results
so in some sense i'm assuming that the
video is is not too i think this will
not work on edge images if i think that
lecture dance vibration ok let's stand
cashman again the last talk of the
session is titled calibration free
rolling shutter removal and matthias
grundman will present hey my name is
Matthias and I will be presenting joint
work with the vac Daniel and earphone
about calibration free rolling shutter
removal in video so to motivate the
problem let's look at a following
example so this is a video that we've
taken from a mobile android phone that
we deliberately have been shaken quite a
bit and what i want to point out here is
that you see due to the no due to the
nature of the sensor that you see
non-rigid distortions in this videos
that go beyond what what you would
consider average camera shake and our
goal is basically to understand undo
this camera shake so again on top is the
original shaky video and at the bottom
is our calibrated result and you see
it's still a video because when you look
at the person walking up on the up on
that little bridge and one ingredient of
our technique is is basically that is
completely calibration free or that it
also doesn't assume any knowledge about
the camera so our goal here is to come
up with a novel technique that allows
rolling shutter removal in casual web
video and in order to do this it needs
to be calibration free no knowledge
about the camera it needs to be also
fully automatic so beyond the simple
click of a button by user there should
be no further user interaction involved
we achieve this by proposing a novel
mixture model of home ography and we
evaluate that compared to a previous
approaches by conducting a user study
comparing against six other AUSA's and
our close to top are showing you that
our technique is basically applicable to
real time performance and show you a
little life implementation that is on
youtube so the nature of rolling shutter
stems from that there are two kinds of
sensors so the classical sensor like a
ccd sensor uses a global shutter it
reads out and captures the image at one
instant in time however if you have a
rolling shutter sensor like for example
CMOS sensor that you find in most mobile
phones but even in high-end cameras like
the red camera the image is red at one
scanline at a time and if if your motion
model does not account for this
difference in the capturing process you
basically do poorly so to motivate is
let's look at the following example so
this is a rolling shutter video courtesy
of baker at all in cpr 2010 and it's
taken from a helicopter with a rolling
shutter camera and you see these wallet
distortions and now if we just apply the
our previous video stabilization
approach that just uses a global motion
model then you basically see that it
stabilizes the videos that it takes out
some of the shake of the helicopter but
the wobble distortions are still there
so the issue basically stems from the
fact that the motion model is now in a
case of rolling shutter not global with
respect to the frame but varies across
the scan line so you need to come up
with a motion model that is basically
dynamic and can adapt to that nature and
at various over the scan lines and what
do I mean by motion model
so in our case we start by expecting
feature points and for example coyote
future points and then just tracking
them so they're basically you measure
for a bunch of corners displacement with
respect to the previous frame and now an
hour if we try to do now what we try to
do is basically fit a parametric motion
model to this discrete set of abstract
feature points so that you basically can
map every point of the image to its
corresponding point in the previous
image and so this can induce a warp so
that you can effectively then understood
the rolling shutter motion now how does
this motion estimation work in this case
I show three different frames and I
indicate the nature of the rolling
shutter by basically displacing the
capture time of the scanline so that's
why you get this little slanted line now
if you assume for a moment that you
would have ordinary global shutter in a
global chatter what you basically have
is in case you have a calibrated camera
matrix where the camera matrix is known
then the camera matrix is determined by
the intrinsic parameters and the
rotation and translation of the camera
at the instant you captured that frame
and in case you use a 2d motion model so
you look at a feature match x and y you
basically have a mapping between x and y
and for example in 2d if you assume that
there's no translation then the camera
matrix is invertible and you have this
relationship between x and y by by
assuming that X and Y image the same or
the identical 3d point of the seat now
in case you have a rolling shutter model
this changes a bit now the issue here is
that the rotation matrices are not
constant with respect to the frames
anymore but to actually change across
the scanline so this is indicated by
this dependency of the rotation matrix
on the scanline of X as X and then also
the scan line of Y sy and we now try to
basically come up with a little
simplified model that describes this
relationship so what we need is of
course we need a higher degree of
freedom model higher degree of freedom
model compared to for example home
ography with a degrees of freedom so the
first step that we can do if we takes a
relationship from the previous slide as
we can basically lump the five degrees
of freedom of the intrinsic camera
matrix K and Sweetie with the freedom of
the rotation together in this home
ography which is just a linear sui boys
free transform and this filmography
still depends of course on the scanline
of X and it also depends the second one
on the scanline of Y and now we make an
important simplification to make this
problem tractable what we will do is we
will drop the dependency on Y so we
basically assumes that all points in one
scanline get mapped to another scan line
in the other frame this simplification
is of course not always true right in
case you have heavy rotation or you have
heavy scale between the two frames it is
violated but in video in general these
changes are rather small so we found
that this simplification is still
applicable so then what you basically
have is that what you basically have is
you have one home ography that describes
the mapping between x and y but this
filmography depends on the scan line of
x now in order to estimate this this
model you can imagine you would need to
estimate 10 ma grafite for every scan
line and of course you don't have that
many feature points so to simplify this
what we do is instead of estimating one
home ography for every scan line we
discretize the image into multiple
blocks and we will estimate one whom
ography per block now you see if I were
just estimating independently you would
get these discontinuities between blocks
and the walk would look weird so instead
what we do is we simply say that the
home ography is a mixture of the space
home agra fees that we estimate for
every block with linear weights that
only depend on the scan line of the
feature point and these were
it's a unknown a priori you can just
compute them what we do is we basically
use Gaussian weights so these weights
are centered around the midpoint of each
block and they basically vote for each
feature point how much they influence it
and the nice thing about this is now you
have a linear model describing the
nature of rolling shutter and you still
can like estimated with some kind of
extended dat everything is linear some
implementation details we use ten blocks
across the image we use a sigma for the
Gaussian weight that is ten percent of
the frame height and one thing you need
to do is you need to add a regularizer
because if one of the blocks has very
few features it becomes numerically
unstable and in order to discount for
ground motions what we do is we embed
this into a iterative rear weighted
least square scheme and the nice thing
about this is in like in the previous
example there was a cars and driving
through the scene later on and you you
see we can nicely discount this
foreground motion if you don't do that
you basically get a warps that would
follow the forward motion and and would
look yet over time back to the original
video from the helicopter know if it
takes this video and I just applies
motion model I just described I can
basically nicely undo the rolling
shutter distortions so it can adapt to
it and you also see like the little
input sine dancing around this is the
foreground or the iterator freeway Tory
square discounting it I picked two
previous approaches to compare here just
very quickly basically forcing in
rinkeby is a visual odometry approach
which assumes the fixed camera matrix
that has been calibrated and then in
instead of homography is estimated over
blocks they basically assume that this
rotation matrices are smoothly varying
across the capturing process and they
use spherical linear interpolation Baker
at all from cvpr 2010 uses something
similar to ours in a sense that they
also have blocks and then there
per block translations and they
integrate those now the difference is if
you do interpolation on integration what
is important is you need to account for
the inter-frame delays interframe delay
is a little time between capturing the
last gain line in the frame and then
capturing the first scan line in the
next frame and this is important because
if you use interpolation integration you
have bounced and you need to get that
right in our case we don't have we don't
use any of those so it's its inherent
recalibration free we evaluated this
with a user study based on 54
participants comparing to six authors
and we basically showed everybody's
original and then left at a and let it
be and we asked them to say which
methods a preferred of any or the
original and the the arrangement was
randomized and here's the some some
comparison that are not blind now any
more to hear this is a big at all
approach I should note Baker can
actually estimate the inter-frame delay
automatically from a video if it
contains wall so it needs to be a
rolling shutter distorted video this is
a force and ring that we approach here
on the Left which which is a calibrated
approach so the inter-frame delay there
is actually estimated in a lab with a
pulse laser post pulse sorry that is
lying at all which which uses some kind
of interpolation of a fine transforms
across the frame
here we compared to the approach of
capping co at all it's a check report
and Stanford which is basically based on
the force and ringer be paper but
instead of estimating the rotation from
the visual data they use the gyroscope
to build in the mobile phone to
basically read that out here we compared
to our previous work in cvpr 2011 and
you've seen that before and finally we
compared to subspace video stabilization
which is a transaction of graphics last
year and these are the plots for each of
the six officers that we compared to
read basically indicates they the users
prefer our approach blue the approach of
the other author and then green no
preference and you sees it like in the
majority of the cases users preferred
our uncalibrated approach compared to
others now I want to show some
challenging examples that we obtained
from YouTube as a citizen tube in
particular so these are videos where
casual users just you know report live
from seen in this cases versus jet crash
in Virginia Beach just some weeks ago
and this person for example riding on a
bike so you see these heavy wobble
distortions and it's basically hard to
watch as this one is actually even more
extreme it's people that that are
excited of course in the moment this is
challenging because what you have is
here for ground motion you have a lot of
lack of texture you have these crazy
emotions in between where it shouldn't
break down you know missing texture and
the sky region and the technique is kind
of can deal with a lot of the challenges
ok and the end i want to show you that
this is applicable to real time
implementation so this is a little demo
i captured from youtube at youtube.com /
editor so we implemented the technique
with the editor here the the helicopter
video and now if i basically click on
stabilize its doing stabilization and
rolling shutter removal in real time
another example and if you want to try
it out by yourself it should be
available end of next week
yeah and this concludes my talk
questions time for questions yeah um no
in general in general what we use is a
warp so in the end of the day you will
introduce some kind of blurriness on the
pixel level but in the cases you have
seen actually the resolution was high
enough that this shouldn't be a problem
here yeah
on youtube we have these enhancements
and the editor and end of next week you
will be available to upload your you're
rolling shattered video I mean we the
stabilizers already in place but it will
basically account for the rolling
shutter distortions as well as a demo
right and YouTube that happen on the
cloud control sequences readout would
you choose the role in the bottom is the
easiest
dispersion free
basically the the readout and the CMOS
sensor is something we we keep you know
given in this sense and we just adapt to
it we don't design camera sensors there
are good reasons to do that for example
I think there's actually like a pre read
out in the CMOS sensor that can allow
you to do exposure control which you
don't happen to CCD sensor but that's
not really our expertise so basically
our video stabilization approach or this
is using our previous work basically
dust cropping so we trade content for
motion stability alternatives are that
you allow out of bound areas in which
case you would need to do some kind of
motion in painting we don't do that
mainly because you know we need to
produce the results fast and motion
painting takes a little while so we use
a simpler version where we just do
cropping for stabilization yes I see
with you is better than our videos
so basically for the automatic version
it is applied as applicable so we have a
technique that basically knows when to
apply rolling shutter and we're not know
it's a completely blind because the
metadata and video is not reliable you
know you can get some from mp4 but in
general you don't know that so it's it's
more general than that okay there's
nothing else so let's thank all the
three speakers again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>