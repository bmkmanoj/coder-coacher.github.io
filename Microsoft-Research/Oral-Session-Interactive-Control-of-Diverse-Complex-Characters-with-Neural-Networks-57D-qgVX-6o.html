<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Interactive Control of Diverse Complex Characters with Neural Networks | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Interactive Control of Diverse Complex Characters with Neural Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Interactive Control of Diverse Complex Characters with Neural Networks</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/57D-qgVX-6o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay our next talk is a paper by eager
mode at Kendall Lowry
got an Andrew ziram Popovic and Emanuel
Todorov is about interactive control of
diverse complex characters with neural
networks thank you very much so yes
thank you for the introduction as I said
I'll talk about interactive control of
complex characters and in general the
problem that we're going to be tackling
is to synthesize complex movement from
simple goals and the goal here is for
example to move up the character to a
particular location that I'm
interactively controlling with a cursor
and different characters might
automatically adapt different strategies
to satisfy this goal strategies from
flying to swimming to bipedal walking to
quadrupedal walking and we would like
this complement strategies to be
discovered fully automatically without
the need to for manual design or manual
specification of motion or engineering
and we don't want them to come out of
say mimicking of existing motions that
we provide I would like to have them
discovered de novo and being able to do
something like this has host of
implications and fields of robotics for
autonomous control of versatile robot
platforms in biomechanics and cognitive
science to be able to say have Paul
perform virtual experiments and study
the effects of various equipment or
prostheses or surgical procedures and
also in computer graphics to be able to
automate the enemy animation in films or
interactive games now the interactive
controllers that I'll talked about in
our case are going to be represented by
control policies and you're probably
fairly familiar with this topic but just
to make sure we're on the same page what
I mean by a control policy is going to
be some function
we'll call pi to note that has some
parameters theta that is going to map
the state of
agent for example like all the joint
angles of this bipedal character the
joint angle velocities and the target
velocity that you want this character to
walk yet and it's going to output some
actions for example all the individual
joint torques applied to the character
and we're going to apply those joint
torques and we're going to end up in a
new state and once again we can query
the policy repeatedly take a look at the
state the policy will output an action
and if we keep repeating this we're
going to get the behavior of the
character that will satisfy the tasks
that we want now there's been a lot of
research done in finding these control
policies one way you can think go about
it is to pose it as a general search or
optimization problem leading to say
policy gradient methods and a lot of
that work but in the general case that
is a very difficult optimization problem
and has typically been limited to fairly
low dimensional systems and somewhat not
particularly complicated behaviors so
the other way you can go about it is to
incorporate more of the additional
structure if you know something about
the behavior so for example if you know
it's going you're going to be creating
walking controllers you might create
some specially designed state machines
or if you're going to be creating
swimming creatures you can possibly
express the policy and say a composition
of sinusoidal or cyclic functions but
these again limit the generality of the
approach or of the different platforms
that you can apply this method at and
require a lot of manual insight for a
lot of human insight so the way that
we're going to try to approach this
problem is make use of Touma two
approaches is make use of trajectory
optimization and accuse of supervised
learning and I think we're all familiar
with supervised learning but I think it
would be worth to talk a little bit
about trajectory optimization and either
introduce a review it
so in trajectory optimization for
example let's say you have a character
agent Asian character that starts in
some initial configuration let's call it
q0 containing say all the joint angles
of this character and it has some tasks
some tasks that you want this character
to achieve so for example move the
center of mass over the ledge and as
what we'd like to have is find a motion
that satisfies this high-level task and
the parameters of the problem that we're
going to be optimizing over is going to
be a collection of these poses of the
character from time 0 to T and also in
addition the contact forces that are
that this character applies to the
environment and vice versa and this
optimization is going to be subjected to
some constraints so such as the
kinematic constraints say making sure
that the elbows don't bend backward and
all the joint limits of the character is
satisfied also the dynamics constraints
making sure that the transition between
any two poses q1 say q1 to q2 BAE's the
laws of physics and also some additional
contact constraints that enforce that
will enforce consistency between poses
and forces I don't want to go into too
many details of this for more details
you can find me at the poster but then
basically we can impose these
constraints in our case we impose them
as soft constraints and just fold them
as additional objectives and the
resulting problem as then can be solved
with standard existing local gradient
based optimization methods in our case
we use quasi 'm we use gauss newton
method to solve all of these problems
and as a result what you can get is
actually we've done a lot of work on
this and in particular after very
careful treatment of contact we're able
to find optimized trajectories that
solved very very complex movement
problems
again without any particular clever
initialization or any additional input
just from a few high-level goals like
where the final kill or to find a
location of the character should be but
for each of these movements that you see
it's only specific to a particular to a
particular situation where the character
starts at a particular initial state and
has a particular goal but what we're
after is learning policies that are
applicable everywhere not just for one
situation so what's one way that we can
think about going and going about
learning these policies well one initial
thought that you can have is we're going
to pose a collection of problems say 1
to N and a collection of situations and
each of these situations we're going to
optimize find an optimal trajectory
using the methods that I've just talked
about find a solution find a motion so
for example character walking backwards
1 meter or say another task where the
character is walking forward 2 meters
and we're going to collect state and
action pairs from all of these solve
trajectories and we're going to use our
favorite function approximator such as a
neural network for example to fit the
general mapping from States to actions
and hopefully that approximator will
generalize to unseen cases like walking
backwards 1.5 meter or something like
that so the problem with this is that
this collection of trajectories this x1
to xn can be very difficult or even an
inconsistent data set to learn and
that's what we found in practice like
this approach did not work well because
so for example one way there are many
ways to walk forward one meter you might
like say start with the left foot or you
might start with the right foot and some
trajectories might be doing it with one
way or some trajectories might be doing
it with the other and if you're trying
to fit a single for a particular inch
for a particular state you're trying to
fit
deterministic or a unimodel
policy then it's gonna try to do
something in between stepping with the
right it's stepping with the left and
that's not a particularly sensible
strategy so what the strategy is that we
were proposing instead is making sure
that the policy that we're learning
actually feeds back into the trajectory
optimization problems and so one way
that this can be posed is if you think
of the problem jointly optimizing say
policy parameters and this collection of
trajectories where the objective is that
each of the trajectories must be
satisfying those goals like getting to a
particular location while satisfying
laws of physics but also have an
additional term that says well the
actions along each of the trajectories
have to be what our policy would output
along the states of that trajectory so
you basically have this additional term
that says trajectories have to stay
close to the policy and in practice of
course like this is a very large
optimization problem and we can
decompose it into a collection of
independent trajectory optimization
problems holding the policy fixed but
just having this additional quadratic
term and then we simply use regression
on the fixed collection of state and
action pairs from the policies and we
keep if we keep alternating this we
actually end up observing convergence
where the policy actually ends up being
able to reconstruct these state these
trajectories that we observe and one
very nice thing is that this can be done
in a distributed and very scalable way
so in practice we can actually have this
very large collection of trajectories
running on a cluster of machines where
each machine is for example responsible
for a synchronously optimizing some
trajectories sending the data to another
machine which does supervised learning
with existing neural network
implementations and you never actually
need to put the entire data set into
memory the data set can actually change
we can add new trajectories or new tasks
on the fly or remove them as
necessary so I think we've only begun to
explore the future possibilities of this
but I think that is a very exciting
aspect to this approach and just to be a
bit more concrete and than our
implementation the policy as I said
we're going to be using neural networks
although other function approximator
could potentially be used and as an
input we're going to take as I've said
the joining goals of the character and
the contact forces and the policy is
going to output how these quantities
change so it's contact with how the
joint angles and forces change so one
interesting aspect of this is that it
ends up learning a model of the dynamics
under the optimal controls and it can
possibly be used to make predictions of
what the future will look like without
ever invoking a simulator so imagining
how you would move like in the future
and so this is one example of me
interactively controlling the target and
you see that what is a tray like what is
a collection of those poses is
trajectories predicted without ever
touching the simulator and oops and
another aspect that we found to be very
important and making this work is
injecting noise to make these to make
this training robust so one issue is
that or something that you would have to
encounter is policies if you're just
training them on this collection of
trajectories they may not be exposed to
what they should do around the
trajectory States so for example if I'm
at a state s I'm not sure if you can see
it here I am in a state us and I know
what to do there but if I didn't lead
ev8 or drift to another new state or
nearby s plus epsilon I need to be able
to
make an adjustment on what what to do in
that nearby state and we have I don't
want to go into the details of this but
if you assume that this correction to
the action is going to be linear you can
very efficiently calculate for what the
action should be and this corrupted or
noisy state and this idea is I guess
somewhat similar to denoising
auto-encoders that Yoshio talked about
at the tutorial yesterday but here the
task is different
instead of corrupting the input and
trying to reconstruct the original
version of the output here you're
corrupting the input but there is also
it also induces a particular change in
the output that is not just being able
to recreate the original and we've also
found it important to also add noise
into the neural network training itself
so for example we could have used
dropout we've used additive noise to the
units instead and just to show you what
would happen if you were to try to
optimize policies without using this
noise you can see the tasks kind of end
up being successful sometimes but
they're very very like me entering and
when you do this policy optimization
with noise the task always ends up
succeeding very very quickly so because
it knows what to do around nearby states
and this is kind of what it looks like
for a very small data set that we're
learning on training without noise kind
of meander but training with noise ends
up succeeding really quickly and now to
show you some examples of what we're
able to do and talk more about them in
detail in this case we basically have a
collection of boxes essentially actuated
by some motors at the joints and the air
has a particular medium in this case
that of air and we're not assuming
anything else
we're not for example assuming that
these joints are actuated by cyclic
state machines or a collection of
sinusoids what's driving this behavior
is just entirely a general four layer
neural network that was trained with
this with our algorithm and it's able to
achieve the tasks such as moving
backward and forward and when then using
the same neural network architecture
using exactly the same algorithm what we
can do is just build another character
and change the environment so that the
medium is that of water which is quite a
bit denser and we can get very different
behavior strategies coming out without
doing any manual changes and we can
control not just say the location but
also the heading and that is
incorporated as as an input into the
network and again using the same exactly
the same network in architecture we can
set up a humanoid that is able to
contact with the ground and has access
to those ground reaction forces which is
how it actually ends up locomoting and
again there's no motion capture or
anything being used the trajectory
optimization discovers that to move
forward you need to make a series of
motions that looks like a footstep and
then the policy ends up learning and
generalizing that you can also try to
have different morphologies such as this
slightly shorter character or there's
much much wider character that still
find strategies to move and also
quadrupedal characters to get to a
particular location and also turn around
or the areas like spider like creatures
and one thing I should say is that the
resulting policies are also very
coherent so as I showed before as you
change the target location you're not
getting very wildly different strategies
so you can have some confidence that
these resulting motions of the policy is
going to roll out are going to be real
and they're not going to do something
unexpected which is good for safety we
can also incorporate additional sources
of information for example like sensors
like kind of a folio field around this
quadcopter that says how close am I to
the obstacle and get policies that move
to avoid obstacles we've also briefly
tried implementing this on physical
robots where we're only working with a
sensor information that's available to
the robot and not any additional sources
and here the goal is to get the torso to
a particular orientation that is
controlled by my co-authors mouse cursor
and there isn't anything for example
we're not making use of say end effector
or Jacobian transpose control or any of
those ideas Jesus learned just coming
out automatically and here the goal is
to simple move the end effector of the
hand to a particular location that is
once again controlled interactively by
the cursor we'd like to be able to try
locomotion behaviors unfortunately all
the policies we've tried to do require
access to contact forces and we've just
only put in the contact force sensors
onto the robot so we're really excited
to try to do look emotion next so to
conclude we have been able to find a
method to automatically discover move
policies for movement behaviors with a
very scalable implementation that can
distribute the workload across machines
we were only using 40 machines so far
but there's can potentially scale it up
to a much larger number and it didn't
require any manual policy like manual
design of policy structure or any
initialization for the motions and in
the future would like to be able to
transfer the work across domains in
particular to more work on applying this
in the physical world study house
sensors affect the behavior and use more
rich sensory information such as visual
input so thank you
it's probably time for one question okay
so go on yeah so this reminded me a lot
of guided policy search in the sense
that you're kind of finding some optimal
trajectories and then trying to learn a
neural net that can reproduce them can
you just like briefly compare and
contrast yeah in fact this work was kind
of going on in parallel with guided
policy search but I think there's a lot
of similarity and that is really great
work I think one of the differences in
practice ins of being that we use a
particular trajectory optimization
method that is quite a bit I think is
able to you find strategies for
movements that involve a lot of contact
and that allows us to try to get at the
motions that the other method was not
able to do before but I think there is
in general there's a lot of similarity
between the two approaches also with
regards to district scalable distributed
implementation I think there's there are
some things with get a policy search
that making use of a DMM makes it not
entirely obvious how to implement that
but there are potentially ways so yeah
thanks
crates let's thank the speaker again
thank you
you
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>