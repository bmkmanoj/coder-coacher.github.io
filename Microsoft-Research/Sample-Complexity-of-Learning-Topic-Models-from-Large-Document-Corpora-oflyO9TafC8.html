<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sample Complexity of Learning Topic Models from Large Document Corpora | Coder Coacher - Coaching Coders</title><meta content="Sample Complexity of Learning Topic Models from Large Document Corpora - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sample Complexity of Learning Topic Models from Large Document Corpora</b></h2><h5 class="post__date">2016-07-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oflyO9TafC8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the second hook is x qi lu from IRC and
he asked me to introduce him as just
cheerio from is good morning everybody
so first i will be thanked organizers
for inviting me to speak so pardon me
for the title I told me as boring as
what I it seems so we're going to talk
about text analytics and a big data
setting and I think the previous talk
clearly summed up why in nutshell
startups will come up out of analytics
and big data etcetera right so so this
talk we are going to talk about
interesting problem in text analytics
that of extracting topics from a large
document collection and was happy to
hear the words like unsupervised
learning etc so this would be an example
once always learning so so this is a
picture of what anybody any guess listen
I just trying to make sure that your not
escape nervous this I jokingly you can
say this is the picture of papers on my
desk after the examination no joking i
scraped it from internet somewhere so
imagine this is a common sight in most
of our desks especially p unorganized
people like me so one of us often one
things right if if there was a tool
which could help organize these things
you know in unsupervised fashion won't
it be the good no so so the main
challenge is that the main challenges
okay one good thing to organize it okay
higher you get ten guys and each of you
look at the paper and put it in this
bucket that bucket that bucket right so
that's not going to that's not possible
you want your automatic scheme and
that's hard the N and so there are two
reasons are the two dimensions of this
problem first of all what are the
methodologies you want to deploy for
such a problem that also not very clear
emerging theme and the methodologies
which we currently have a severely
challenged if a number of documents are
but launched so what can we do and this
is being and I mean this is one of I
would say
if text is if your data is in text and
this the men today's challenge for sure
so topic models is a very interesting
attempt at this problem so these models
attempt to discover hidden themes in
document collections in a minute we'll
try to make this more clear now if you
have some it if you understand the
themes in a document you can use them to
label them automatically tag them right
I once you have tagged them you know
then you can then you can use the stats
to do you know browsing in a searching
and and storing them efficiently
etcetera right so so so and so and these
models do not require supervision so
given so many advantages that is why
this is almost I mean my colleagues in
Google and Yahoo tell me that this goes
into the back end I have except the
anecdotal evidence I can't tell you
what's going on there further than work
there but this has been now industry
standard in in organizing large text
collections so so just to set some
example like just to set some
definitions right sort of topics so
suppose I tell you I show you a document
on which you see this following words a
run innings it season game any idea what
the doc point would be sports baseball
and cricket so now if I say the document
of I came out a new york times or since
so basically this is long as a baseball
or cricket if it was time so it could as
well good times of india then it will
cricket an audio break so so the main
idea is that just knowing this few words
you can I a okay this could be about
cricket or baseball so that's what you
are talking about here right so so for
example I mean you know if you run in
such models in topic models over you
know say some data set maybe in this
case you're running on New York Times
data set so so obviously you can see the
first
five words this run inning hit season
game we talked about it that's more like
baseball the second one could be you
know I mean about cooking and then there
is this patient drug dr. cancer medical
of course it was not something a health
care right so can you automatically
extract them that is the main idea sort
of right so now as you see these are
very interesting and useful things so
people have built some topic models says
one great example which I find is
advertisement for topic models I mean
I'd not sure how much of it is used so
imagine Wikipedia right I want to browse
Wikipedia so what you can do is you can
take the weekend tell you it a dump
right through topic models so you can
see I don't understand you can read
those words carry the same household
population female family median right so
so this is this is a like these are the
words I'm finding right there's another
set of words let's say war force army
attack military something with measuring
things now so for example in this
particular case if you so these are
basically some topics you are defining
publisher digital now as I saying
collection of words now which are which
all appear in poker heavily right so if
you go and click there you know you will
see the following documents these are
document titles coming up so this is
great now you want to say ok I am coming
to pull your document i wanna see
related topics working to township
household population that's different
angle topic so this gives you a very
easy way of browsing large document
category just to show one use but you
know but now effectively all back and
search is now driven by this topic
models that viewed from industrial
search companies can tell us better so
having convinced you that these are you
know great tools now let us see how can
I learn them how can i find so the bill
this the subject of this talk so we
briefly talked about what are topic
models now will introduce you to a very
you classic technique or latent semantic
sing where this is started and then we
will talk then if you get time we will
actually talk about what i mean by
learning talk some finite sample number
the sample complexities this is our
recent work with power avi canal and
drop it I said that's the outline so so
basically all they started in 1960s and
when the first shoot first notion of
topics sorry document started emerging
double collections so at that point
people said okay I have a collection of
documents and document is relational
words etc I have this model now the main
thing is can I find information what is
informational said I'll ask I'll maybe
ask a question and you have to answer
this they're right I'll give some
keywords and near to find me the nearest
documents so at that point the
formulation was I give you a document
find me the closest documents this 1970s
yeah so then so so then people then came
up with this model which are called
vector space model so it's what is it
would this is the level as do the
following thing let us encode the image
document the presence or absence of a
word for your account that i can
represent by a vector and then you look
at your query that also represent by
another vector then try to see which
which documents are closer to that query
and return it back to you this is this
is what i will call as keyword search
now very soon people found that this
works sometime but not always so if I'm
looking for example is my for example is
you are looking for something like
Jaguar j gu er a Jaguar now it can each
other the car or Jaguar danimal is not
very clear I just went from that word
right then it was a Jaguar adventure and
say Jaguar fast speed you know so this
quarter's helps right and then so
sometimes these are not so apparent just
by words 1 / 1 can have okay people can
write the same word there are
what would say many different words that
can always happen so all kinds of things
came into play right so that point
people came up with this interesting
idea that vector space models and may
not be that good so the cave is very
touching and availability it in semantic
indexing and that completely
outperformed this keyword search so
let's look at it very quickly to set the
tone for this talk so we'll as we
discussed for us you know the data is
document election will input the data as
a matrix and here is your math comes and
that's what we know so so so here let us
assume that each document is a column I
am sorry to do this in the morning see
each document is a column and each row
in the column is a word now what you
fill in the row they can be depending on
what models you have for example the
vector space model can fill in the
absence or presence of a world or you
can count the number of words if you
wish all right all right so so for
example let's say that I have counted
number of words and divided them a total
number of words the total number of
words is 100 and there are nine words
the first world it's a point zero night
and all that right so some of them
should be one something like we can do
other things also right so finally given
a document matrix so the purpose or
input in a matrix a right so then people
found is that ok there is this
interesting tool called from mathematics
linear algebra or singular value
decomposition what you do is you write
this matrix into a factorization MDS the
three matrices right and now so so
remember each column of a is a
documentary so you can see each column
the mutant has something like AI into
some matrix into a sign but SI and I
think of it is a new representation of
this document okay so now what I do is
now a curative a query good q is given
to me yeah so what I do is I project the
query into this using this md by this
formula and then I can pick a similarity
score between Q &amp;amp; A
q is a query document an AI is that the
document in my data sets so now what I
can do is if this number is high than AI
is close to Q this is very similar I
will take this argument now I can use
this code for each grade L I can use
this code for all the documents in the
corpus and return to you the closest
ones now this completely outperform the
keywords efficient so now but you are
such a win does Q transpose the idea
that is the main thing if you are
mathematically so i said as i said right
so people are wondering this working
where but why why should work well Evan
what did i do when there is no linear
algebra i knew the two light on it why
so this was saying I so maybe then since
that may be the matrix M is encoding
something to semantics maybe do it was
projecting wanna say by projecting the
documents those columns of em see
there's somewhere the words so it is not
really aligned along each word it is a
direction of which is mixing up to three
words right so maybe this is catching
semantics this is what they are thinking
yet see this is 1998 so people but
applying it using it no one understand
now as you said 30 years right these
took 10 years 10 years later there's a
very interesting paper came up a classic
paper who's that me explain this the
widest work so now so then said okay
maybe what you should do that each topic
sorry each document has a lotion ball
topic will define it in a moment right
so and if each topic has some very few
words that is you take the matrix the
matrix write a matrix you have this
words all the speakers what are you
showed no bad inning season game etc and
you see there's a large fraction of the
probability mass is sitting on those
words that is this normal a very high
okay in such a case we can prove really
show then the keywords are outperformed
by this and that other braid exploration
and of course there are many math here
right essentially this gives a very
interesting idea so then topic is
nothing but a priority distribution over
words the first time since came out
right so sort of document not so
document is this is nothing but maybe I
am rolling a dice with the number of
phases where d is the vocabulary em
throw em draws and each time a word
shows up is Putin occupant so under that
model and all this was like all this is
true so I have eight minutes so let me
just speed up so sorry now is not the
question is this is 2000 right we will
understood great but what is the
assumption they made they said each
document has only one topic there is the
sumption right will flash to and
slightly he's not going to go to one
topic that is that is each draw is from
one distribution now the question became
is that ok so now here is a paper you
know the sword art and cover designs
obviously the set-asides not going to
work here so we will have as we were to
you know as we are trying to apply a
cyclic quickly found what okay this is
breaking down in these places so we need
to have models which can go behind that
which can handle multiple topics in a
single document so so for example you
know let's not get politically but let's
say let's say there's a document we are
talking olin Jet Li we are talking about
Chidambaram we are talking about BCCI
see they are both on the BCCI board but
about corruption here so this document
even this is a document comes out if
this out BCCI scandal see but here they
are not an opposing sides correctly sir
click Add is what politics it's also
about corruption right so unless I will
not we would handle this correct so now
what do we do so this is a
interesting open question and now in
came up every favor fantastic answer in
2002 where they actually took the idea
of probabilistic the topic is a
probability and net had to fit actually
a full generative model of a document I
mean very complex model I will not have
time because I am racing against that
it's five minutes so so we'll just say
that lda is this great tool but it's a
brave prognostic ok so now so here is a
tool and it has one deadlier can do so
they build a model of science that is
they taken the science magazine or last
hundred years this took this on the data
set the run this model so it's
discovering you know so they tracked how
this topics will change so for example
you see 1960 you to see water fish
marion oceans what is it so in the
marine science right somebody marine you
know McMahon in biology so how that
topic has evolved you know just to give
you another example how can visualize
you know large government election so I
spend much time on this so essentially
the idea is this so I so the main idea
was that ok LSI grid tool but it can
only handle one topic one document and
you save a new tool which can do
multiple things so what does it do it
says that ok for example assume that
only two words your topic this to have a
visualization so now so let us put some
weights on these so this is you can find
out this house there are three topics so
you can put it as a triangle and now
what it is doing is what is LD a new
topic is doing is is putting some
weights here putting some weight there
including someone who ate there and
turned that linear combination of that
that is is trying to find out a point
inside the China under the new
distribution and now they're trying to
drawing documents according with that
distribution okay so this is that that
the util gives them the idea or the
flexibility to handle multiple topics
but document okay so obviously on the
flip side great it would explain but
from a theory computer science is
nightmare everything is np-hard
everything the sense that ok I give you
a document tell me which to which topics
there and be hard how will I find topics
were lodged up on corporal np-hard okay
but but these guys are designed it you
know so they give mcmc techniques Markov
chain Monte Carlo techniques which I
have which are which does not guarantee
but works fabulously well and this is no
by far now is the industry standard and
this is news heavily David score so so
now so often it is that so given the
importance of this so often does really
interest in understanding that okay can
we do better can we do can you develop
algorithms we're can guarantee that if
there's a topic I'll find it and will
not be stuck by this and we are de
sumption right so so that is this recent
interest and that's where our results
were so so so so basically what what I
mean what is the recent last 23 years
this is happening so basically as you
it's now saw that M is a matrix each
column is a topic and you're putting
weights on them right and the weights
are random so given IM matrix you
randomly putting weights on them we
generate a new doc new distribution or
distribution will generate a document
this you do it many many times remember
each time the weights are changing all
right but if i show enough documents m
is not changing right so probably I can
still be able to guess m so that's the
main idea right so now open this is not
always possible until I see make
assumptions on em so i will quickly
point out so this is a break should pay
for a phenomenal breakthrough what they
said was that okay if you assume that an
M matrix right each column of em column
means each is this a word right in there
is a word which occurs in no other topic
but of course only on that topic for
example if you see there's one bad
happening and that can only mean of
course not little forget forget the
forget animal but say bat only happens
is I'm looking at real sports let's say
so if I see the word bat it must mean
so the probability of occurring a bat
opening any aerobics is 0 right so i can
so you can use that to anchor the this
is like a signature what if moment i see
this i know create a song that
particular topic is of occurring so
using that and some pure mathematical
tools you know well Rhoda and all that
give a fabulous algorithm which has this
kind of sample complexity and now thing
to notice so they conjectured that you
have to go beyond SVD right because SVD
LSA was as video it can only do one
topic right remember LSI was SVD based
you can only do one topic right so these
are you dead okay we have to go go to do
other things and all that going on is VD
now I have exactly one minute so i'll
kill you our result so if you hear smart
okay I not sure the audience will like
it but okay so let's see let's put a
result and then we'll see we'll take
some questions so our result is we make
some assumptions will not tell you what
what is I'm sorry weaker than the
previous ones what do you suggest is do
some threshold II do SVD and i can meet
your sample complexity and the solid
ultim and so if you want to hear that we
can be happy to talk about for another
20 20 30 minutes and i think i'll stop
here and take questions yes
hi I'm linkage from to Blighty deli so
I've been studying this topic models and
it's interesting that we can use
co-occurrence and try to figure out some
semantic value out of the documents and
you as you say it's quite reasonable
that it helps in search and other kind
of applications but i wonder how
interesting are these topics and how do
you make these topics interesting
ultimately there is a manual
intervention for labeling these topics
right so so you have these five words
and from that you said it's either
baseball or cricket or whatever right so
systems may not come up with such
interesting topics it might just find
another term which is not so interesting
for labeling that topic what is ooh so
your question is that if I run it on a
general corpora I may not get good
topics per se so talk modeling has
issues one of the issue is that
currently that if I do not set the
number of topics correct I make it
topics which are garbled words okay so
some amount of hand tuning is needed to
that effect so we are not there the full
full you know the whole things and so
this is clearly a very evolving research
area but the interesting point is the
example which I showed two examples I
should wikipedia and science proven
examples on large document collection it
is giving good results right so often so
which basically means that if you
randomly take some you know collection
of documents I'm gonna be able to find
out the tongues may or may not become
coherent okay but but probably if the
documents have some link there indeed
some topics right del dia model you know
does discover some things and for
example on the new results or were a
result or our result you know now are
trying to get can tell you more
precisely that if the document has this
characteristics we can we'll find them
so that that's happening so it's clearly
this is going to be dominating the the
field for some time now thanks
yeah I can hear you just a small
question you said about your result that
you use some kind of thresholding
followed by this vide yeah is it do you
mean threshold on the columns of the
matrix is haunted so what I do is I data
is a matrix right data will given to us
is a matrix from that ought to do
whatever I have to do No Deal give me em
right so what we do is I take the a
matrix and do some thresholding
operation remember the lsi orgy Willis I
was you factorize the a matrix bias
video and that took them where they took
them right so now we're you're
suggesting under some assumptions that
you threshold that a matrix then apply
is VD then I'll then also got to do some
you K means clustering and it gives me
the M matrix this is any intuition why
this objects the intuition is another 10
minutes so basically okay the ideas okay
let me spend that since Satish said I
can take some more time so we make this
interesting assumption which we differ
so first thing to notice so this i'm
okay i can i take a few more minutes so
let me bore you with some details so you
see so here is the interesting parameter
called p naught p naught is what this
set is that the word bat the probability
of the word bat occurring and the word
bad occurs only in that particular topic
and in a where else right so Steve it
was six is dominated that's what I roses
are this s is number of samples I need
to recover an M hat which is way up
salon close to the original topic matrix
right so this is horribly it says
horrible thing right so okay so now we
make a sort of a different assumption so
the assumption we make is as follows and
it is in light in line with lsi
assumption what we assume is maybe the
bcc example the good topic
good document to think about it may have
politics it may have corruption it may
have to get administration but
fundamentally it must be what cricket
and this topic should dominate it right
so we understand let's think about those
document corpuses where the be multiple
themes and so the user word thing theme
is basically your appalling s topics so
da cunha have multiple themes but one
theme will dominate ok this is something
I make on the daughters that is the
weight on that particular theme to be
very high okay now if you see if it is
if i push it to the extreme I recovered
lsi right okay lsi was it the only one
topic at walkers are they would say
which addresses 0 isn't it so I make it
high I make it high not what not to one
since i since i won't make it to one
another math is very difficult and you
know but the essential idea is that that
is where the thresholding helps me in
discovering those dominant topics you
know so that's the intuition if that ok
ok so let's thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>