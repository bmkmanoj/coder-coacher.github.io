<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Approximate Differentiable Renderer | Coder Coacher - Coaching Coders</title><meta content="An Approximate Differentiable Renderer - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Approximate Differentiable Renderer</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4B06sKUt5dY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um welcome guys thanks for coming Monday
morning I just want to introduce Matthew
topper quickly who's coming to visit us
from tubingen and the max planck
institute he's in micro black slab I was
just visiting recently and got to see a
lot of cool stuff and I was very
impressed by Matthew and his work beyond
his research I think he's really
creating tools that other people are
using within the lab so we sort of like
this what did they call it a keystone of
the lab I think providing things like
automatic differentiation and this open
di renderer really enabling the research
there so I should say that he liked it
is MSC at Brown this masters I found and
he came over with Michael black when he
when they started this lab into being
good so that's some do it alright well
thanks for the introduction
can everyone hear me the mic gone
terrific thanks for inviting me it's
really nice to be here
Cambridge is is lovely what I've seen of
it so far today I want to talk about the
open dr which is an approximate
differentiable renderer that i've been
working on with Michael black so one
question that can be asked is what what
is computer vision and one way of
answering it is that you want to get the
state from a set of observations or from
a set of images there are some different
ways to think about computer vision but
one is to split it into some
discriminative methods and some more
generative or inverse graphics based
methods so for a discriminative method
you maybe have weaker modeling but it's
easier to train for a particular task
whereas for inverse graphics you may be
able to have stronger modeling make more
use of something like Bayes rule here
and have better introspection there's a
really interesting place in between
these two which I won't talk about today
but I'm very interested in being able to
bridge inverse graphics with
with more discriminative methods but
today I'm going to talk about an
approach for inverse graphics and the
way that we're going to proceed is at
least to begin with is just to render
and compare so the notion is that we can
render a scene and compare it against an
observed image and see whether they
match and we can do that by having some
function which will render the image
given some set of parameters and
minimizing say the sum of squares
differences between that function and
the image so that's not the only way to
set up this problem but that's the one
that we're going to start out with so
open D R is a is easy to use
differentiable rendering framework which
allows you to do three things it allows
you to approximate a rendering process
differentiate through that rendering
process and then get estimates of
parameters for the for the system and so
first I'll talk a little bit about
approximating the rendering process
stepping back from inverse graphics and
just talking about graphics graphics can
be kind of a pain to actually program
and I think those of you who have done
graphics programming know that it can
take quite a few lines of code just to
draw a single triangle and for those of
you who know OpenGL this even uses glut
so this is this is a ton of code just to
draw a single triangle that does no
modeling and no no inference so with
open D are going through the just this
forward process no inference yet you can
see it takes not so many lines of code
to draw a texture mapped spherical
harmonic lit scene of earth and I say
that this is 20 lines but as far as the
code actually necessary to set up the
renderer
it's ten there's an argument to be made
that renders and there api's were not
designed for the computer vision
community right and that that it's
useful that it's fruitful to think about
designing renders that that are so this
is the the forward rendering process
and one interesting part of all this is
that if you then want to get the
derivatives of the pixels with respect
to some kind of underlying parameters
it's just one additional line right so
here there's this setup of this scene
and then I've added one line in order to
to get the derivatives and then notably
I've asked for the derivatives with
respect to the vertices but I could have
put in the vertex colors or the camera
rotation or translation distortion all
kinds of other things and it would have
given me back something that would let
me estimate the derivatives either a
Jacobian or a linear operator depending
and then here's the inverse graphics
part here we can actually minimize let
me see if this has a have a pointer oh
yes it does so here we can actually do
what we're interested in which is to
estimate parameters so these are the
parameters that are gonna vary over the
course of the optimization but we could
have more right we could have inserted
the vertices in here or any other kinds
of things and then this is the objective
that we're minimizing here with the
dogleg gradient base method so part of
the dream is to be able to concisely
specify some forward model and then fit
it to image evidence the formulation for
this renderer is that we have this
rendering function which is a function
of the per vertex positions the per
vertex appearance and the camera
parameters where the camera parameters
are some sort of open CV style camera
parameters including distortion focal
lengths the the standard ones basically
the same as the open CV setup I should
note that we have other kinds of renders
besides color renders we have a depth
render and a boundary renderer but today
I'll be talking about how the color
render is constructed
one thing I should make note of is what
we're using for our per pixel light
integration so our model for that is
essentially OpenGL or I guess DirectX
has this to some extent to which is to
say that each pixel it's not that it has
no point spread function but its point
spread function is is the impulse right
so a color is taken from one one point
rether than taken from from an area this
ends up being less of a problem because
in practice because anti-aliasing can be
used we can render at a very high
resolution and then the blurring and
down sampling necessary to perform
anti-aliasing is a simple linear
operation that we can differentiate
through but it is an important
approximation and it's one to be aware
of and it it does enable this easy
hardware acceleration I think it's
important to note though that on the one
hand approximation is definitely a part
of this work right we make
approximations and try to explain them
but we're very open to replacing those
approximations with other approximations
to make it better or to make it faster
so I've talked a little bit about
approximating our rendering process and
now I want to talk about differentiating
it one question is why would you want to
even do this why would you differentiate
this process well it's good for local
search right so if you are basically in
the basin of some objective having these
gradients can help a great deal in
finding getting you towards a better
answer faster and it's nice when there
are many degrees of freedom another
point is that it's not useful only for
search it can be used for say to
estimate the posterior of some function
with something like with a hybrid Monte
Carlo so it's not restricted to to just
finding a one estimate or a map estimate
or something
but it's also important to note that
differentiating through the rendering
process for computer vision applications
has absolutely been done before so for
example a blends and better did it when
they estimated the shape parameters
light light position and camera position
for foursome heads
jalapeno also did it for for planet
surfaces
although the emphasis there was a little
bit more on Bayesian modeling and
Delacorte a has a very nice thesis
exploring ways in which one can
differentiate through the renderer for
the purpose of hand tracking so these
are these are nice solutions to specific
problems and the problem that I have
with them is that they basically have
not been released with some kind of a
general framework for solving new
problems so although they're they have
nice formulations and did a nice job
there's nothing that I can download to
apply to to to a new problem and I think
that that can be valuable so part of the
work here is in designing a framework
such that people don't have to to
reinvent the wheel so I'm going to start
with the basics of what it means to have
a differentiable renderer so we would
define a differentiable renderer as
something which produces pixels produces
derivatives of pixels with respect to
underlying parameters and represents
some kind of a plausible imaging process
so you notice without the third
requirement it could really be anything
right and just some differential
function so it has to in some sense
render and so on the left-hand side you
can imagine that there are some pixel
intensities one one intensity per pixel
and that you want to know how these
brightnesses change with respect to
these controls so if one were to pull
these controls up and down we might be
interested in how it affects the
pixel intensities so this is a movie but
before I show this movie I want to
explain it a little bit because there's
one part of it that's a potentially a
little bit confusing this is going to
show just the rendered image and these
sliders are going to start to change and
you'll see that the rendered image
changes as the sliders change so that's
seems kind of reasonable I think and
these sliders represent some spherical
harmonics parameters and some distortion
parameters the less straightforward part
is this window here this window here is
showing the derivatives of the of all of
the pixels with respect to whichever
slider is active so if this slider we're
moving back and forth then this would be
showing the per pixel derivatives with
respect to that to that particular value
so so here again we can see the the
parameters going back and forth the
image changing and then the derivatives
being shown with respect to the slider
that's moving I apologize I think this
this should probably show the show
what's what's active here but it should
give you a sense of of the fact that we
can render and create derivatives
there's another kind of interesting
point here which is if you watch this
closely you may not notice that there
are two kind of types of things that
you'll see one is this these kind of
derivatives that look like the image
itself these I would say our appearance
derivatives and the other of which look
like spatial filters right they look
like some kind of a spatial gradient and
those are essentially geometry
derivatives so in a minute I'll go more
into that but but one can divide these
these derivatives into this the
appearance and the and the geometry so
why is this hard one reason that this is
hard is because of occlusion handling as
you can imagine if you have one surface
on top of another you have this boundary
right this occlusion boundary and it
seems like that that might not be
differentiable from the get-go
another issue with with implementing
something like this is that if you want
something fast it's at least currently
my belief that you need to interact with
with the hardware that you need to have
the GPU on board for something like that
fast isn't always the goal of course and
I think that there are approaches that
have less approximations than this that
can can do a nicer job given more time
but but I think that this this approach
strikes a nice balance of performance
and fidelity another thing that makes
this hard is that just a render is not
enough so if I give you some
differentiable renderer you don't
actually want that right you want to
solve a computer vision problem so you
have some objective on the outside that
you may want to solve and then on the
inside you may not care about the the
say vertices you may care about some
other formulation you may care about
shape or pose or some other parameters
that drive the process so put it put
another way a render is not enough
another question that I get asked about
this is why not just do finite
differencing right that can be awfully
fast and one reason is is illustrated
here so if you take this and I've zoomed
things in to make it clear what's
happening if if you take the cylinder
here and then perform finite
differencing with respect to in-plane
rotation you're going to take the
difference of two pictures and that's
going to end up giving you something
like this so you can see at the center
you end up predicting that there are
note that there are no changes with
respect to the rotation and then on the
edges on the sides you end up saying
that the the differences are going to
span multiple pixels and that's not
really so desirable it suggests that if
you want to make finite differencing
work it has to have an epsilon which
depends upon the pixel and depends upon
the the parameters of interest which
makes things trick
I think and then here with opened er you
can see that the the derivatives are
restricted to the to the boundary of
interest one question I get asked about
this process is many people think that
rendering is not differentiable and they
come to me and say how can you do this
this is this is impossible and some
people think it's trivial tribulus
differentiable much of it is definitely
differentiable so for example the camera
projection part of this and the leading
equations typically are differentiable
the biggest concern I would say are the
discontinuities from from projection or
the the occlusion occlusion boundaries
and and the bottom line is that it
depends on the renderer so some
renderers are differentiable and some
renders or not and it depends on how you
set things up so next I want to talk
more about the formulation of the
renderer specifically with regards to to
the derivatives so here we still have
this output image and we it's still a
function of camera parameters of
vertices and of the per vertex colors
but we're also going to introduce this
variable U which represents the
projected positions of the of the
vertices so then if we want the
derivatives of the image with respect to
these inputs it's just a matter of
multiplying up these partials in order
to get that these seasoned appearance
does not include lighting great question
great question
so appearance sounds a little bit vague
and it's it's vague on purpose because
it's intended to include the effects of
albedo lighting and everything that can
possibly change the the eye space color
so the appearance is really the color
that's coming from the computer monitor
and going to the person and typically
appearance is going to be made some
function
of albedo vertices and whatever a user
wants to set up as as their lighting
model so it's not albedo it's not
lighting it's something it could be any
right exactly it could be anything and
it's exactly intended to be able to be
set up in that way no it is absolutely
so global illuminate so if you can write
it great but I think it's going to be
challenging so the silly answer would be
of course you know go ahead and write
that and the practical answer is that's
very difficult that's right so it's a
good question though so this is the in
some sense the real world version of
this previous tree here this tree is
generated by the by the framework so if
you have your renderer any CH object you
can tell it to show its tree and that
will draw out the expression tree so for
example the camera you can see as a
function of camera parameters like
center of projection translation focal
length and so on here we have appearance
set to being spherical harmonics and one
can see that spherical harmonics is a
function both of some parameters that
control the the lighting aspect of it
but it's also an indirect function of
the vertices here right and one can also
see that the camera projection process
from 3d to 2d is also a function of the
vertices so this in other words this
program that we saw earlier can also be
represented and shown in this way so
it's it's worth talking about how to
compute these different kinds of
partials some of them are easy and some
of them are difficult the appearance
partials so the derivatives with respect
to a are quite simple and the reason for
this is that the the user is simply
specifying these per vertex colors and
what's displayed as an interpreter
palacios of those so if you want the
derivative of a pixel with respect to
all the vertices
all of the vertex colors it's just a
matter of finding the three nearest
vertices and denoting how this color
changes as a as an interpolated function
of those three vertex colors so that's
that's a simple part and I should also
say that this appearance well as I said
before is intended to be a function that
that's constructed by by the user
although we have some prefabbed ones
yeah and then these three vertices yes
so I don't know if I was completely
clear about that so the process is that
for each pixel one can identify exactly
as you said but I just want to restate
it one can identify the the triangle of
interest and then one can find or
actually render the barycentric
coordinates of that pixel within that
triangle and then consider that color to
be just that interpolated linear
combination so the geometry partials are
the hard part one part of the geometry
partials actually peace this is a very
easy part and this is just talking about
the derivatives of the 2d projected
locations with respect to the vertices
and the camera parameters and this is
simple because it's just matrix math
right you have some you know intrinsics
matrix and extrinsic matrix and they can
all be multiplied together in a way
that's fairly straightforward the the
difficult part and the part that I think
brings up the most discussion is is this
so it's the the derivatives of the oh
didn't mean to do that the derivatives
of the image with respect to these 2d
vertex coordinates so how does the value
of each pixel change when you in 2d once
you start moving these moving these
vertices around
so the first thing I want to present
because I think it's the simplest is the
thing that we don't do which is to
simply say that each pixel has a point
spread function which just amounts to
the area distended by its pixel right
that one frustum and then one can look
at one one can explicitly integrate how
much light should come from all of the
polygons that project into that square
right so here you can see we have we
have this polygon and this polygon and
this polygon and one can not only add up
that light in order to decide what
should be shown in that pixel but one
can also differentiate that light with
respect to to the position of these
vertices now one unfortunate thing about
this is it at least it's my belief that
this means throwing away standard
rendering pipelines so standard
rendering pipelines like to render these
impulse function type integrations right
and so so we're gonna do something
that's different but but it's worth
keeping this in mind and understanding
that this is in some sense a nicer but
sometimes slower way of doing things
okay so we're gonna take a different
tack and we're going to to use something
that poggio suggested back in 1996 which
is just to use spatial derivatives okay
now technically speaking this is finite
differencing but that's very misleading
because it's not finite differencing
with respect to the camera parameters or
the vertices or or the appearance this
is just spatial so all that work that
all that's going to be required is one
pass of say so Bell filtering in two
dimensions in order to get all the
derivatives so it's not that for each
new parameter you add you're going to
have to do more finite differencing this
is just one
or two passes of the computation of
spatial derivatives so why why are the
spatial derivatives useful here one
thought experiment that can be used to
show why they might be useful is to
think about just as a thought experiment
what should be the derivatives of the
pixels with respect to the translation
of an image say under a computer monitor
so you think about say that I don't know
if you're familiar with the Ken Burns
effect which you have a monitor and then
there's going to be an image which is
slowly translating in some direction
another thought experiment is what's the
derivative of an image with respect to
dense flow and a third thought
experiment is what is the derivative of
the pixels in an image with respect to
the 2d positions of some kinds of
controlling vertices well if you think
about this for a minute
you realize that this first question can
help solve these other two questions
which is to say if you want to know what
happens under the translation of an
image under a monitor you can just
translate it by one pixel and find out
right and that amounts to taking the
spatial derivatives in the X direction
or you can translate it in the in the
vertical direction take the difference
and find out
so one ends up simply doing spatial
filtering in order to know what the
changes of an image given that some
local patch is translating so this works
this works reasonably well for within
object gradients so if you have an image
in which you don't have any occluders or
anything like that you've got just got
something like a sheet this is something
that that can work reasonably well and
it can work reasonably well sort of
without a lot of assumptions about your
renderer so you could have a ray tracer
or something else but the problem that
that comes up is that once you have
occlusions it doesn't work anymore at
the at the occlusion boundary so it with
the at the between object gradients you
actually want the derivative of the
image with respect to two surfaces not
one so it breaks down so we have to fix
it
so to fix that we're going to introduce
the following cases we're going to say
that we want within object gradients on
the interior pixels and then between
object gradients on the boundary pixels
so to go forward with that we're then
going to classify every pixel as either
being a boundary pixel or an interior
pixel and then we're going to we're
going to use that in order to to special
case things so the cases are as follows
for the interior pixels far away from
any boundary we're just going to use a
spatial filtering very simple that's the
simple case for interior pixels that are
neighboring the boundary it's going to
be very important not to cross the
boundary so what I mean by that is that
we would want to say that right well so
in in practical terms if there's a
boundary to the right of the pixel we're
going to use a spatial filter which does
not include the boundary one way of
thinking about this is that we're just
saying when we're on the interior we
only want the derivatives to be with
respect to the to the interior right
then for boundary pixels we actually go
back to using the regular kernel and the
model here is that we're modeling the
difference between an occluding pixel
and an occluded pixel by an including
pixel and the pixel next door so this is
there's a difficult thing to describe
but it but it does work in practice but
so I guess I want to get questions
though to the extent that you feel like
this is unclear because I want to make
this clear
so this is the same kernel as this is
but there are different but for a
different reason so so this kernel I
think is a little bit easier to think
about why that's happening right the
interior case I think is a little bit
more clear this case what we're doing is
we're saying that instead what so what
what should you be doing what are we
approximating right well we we would
like to take some linear combination of
the including pixel and the included
pixel right on a boundary so I hope that
that's clear so as this boundary moves
we'd like to change how much of an
occluding or a how much of an including
polygons color and an included polygons
color we're going to take gonna take
we'd like to take a linear combination
of that the renderer does not give us
the occluded color however right so
although there may be tricks to get
around this what we have access to is
the occluding color and something right
next door so the assumption here is that
that there is that the colors don't
change too much in the background or the
foreground so here's an example imagine
that you have a red ball on a blue
background
well modeling so although we're
interested in taking a linear
combination of the the red and the blue
that are on the foreground in the
background we can actually look right
next door and let me think how to
describe that what we would like to be
able to do is peek around the corner at
the occluded but we're modeling it by
what is right next to the occluded so
yes you could render with and without
the including object but then you have
to solve some kinds of combinatorial
problems about rendering with and
without all objects right and so
depending on what the relationships of
the objects are and there's another
issue here which is that the
the kind of currency of opened er is not
objects it's triangles right and so you
have to think about and I'm not
disagreeing with you right I think
that's that's a plausible way of doing
things it seems to me that you're making
an assumption there's essentially a
constant assumption for the background
the occluding object and the included
object in order to be able to apply this
kernel and doesn't that essentially
assume that the gradients then are zero
on each object which makes which then
makes taking the finding the gradients a
little bit right so and in principle it
seems like this would watch the whole
thing right but in practice the gradient
between these two objects is much
stronger than the gradient within the
objects right in price and especially as
you render at higher and higher
resolutions right unless you have some
fractal like you know colors in your
object the smoothness is going to is
going to increase along the way so part
of the argument is that as you increase
this resolution this this becomes a
better and better better assumption but
but it is definitely an approximation
and as I said earlier we're open to
replacing it with other approximations
we just want to stay aware of the
performance because this can get slow
you know differentiating through the
rendering process can be can be slow
okay so this is just an example to try
to elucidate this so if we render a red
circle on a green background we expect
that if the green background is moved
nothing should change in the image right
we assume that this green background is
is much larger than the screen but if
this red circle in the foreground moves
then we would expect that pixels should
change so if we use if we do no special
casing and take derivatives with respect
to the foreground occluder then there
are no problems and by that I mean that
finite differencing we reproduce the the
results of finite differences without
this special
casing things don't work with respect to
the background object with respect to
the occluded object so we'll get the the
wrong derivative with respect to the
translation of the background object and
then with the special casing it's fixed
again so these are definitely choices
with trade-offs we're not currently
modeling a sub-pixel geometry there's an
argument that if there's a an entire
world inside of each pixel that one
might not want to model that right that
if you want to restrict your computation
time in some way that it's not always
worth modeling that and there's an also
there's another argument to be made that
as you render at higher and higher
resolutions one can use anti-aliasing in
order to get rid of the detrimental
effects of this of this approximation
but these are trade-offs well how much
do you want so so I think 2 or 4 times
anti-aliasing does a very reasonable job
at getting rid of some of these problems
but I think one great example for where
this would be a bad approach would be if
you have images from say a satellite
that's going around Jupiter or something
like that right in that case or are
around a planet surface where you want
to reconstruct that surface well in that
case you have essentially a ton of time
in order to reconstruct it and you want
absolutely the best job possible right
you're willing to to throw a lot of time
at that then then I think it makes sense
not to use some of these approximations
but but the nice thing about any
aliasing is you can do it as many levels
as you want right and the the GPU is
fast right the graphics cards fast these
pipelines are built to build to do these
things quickly which is good so the
third thing that opened er does is to
allow you to find parameter estimates
this is of course the thing that we
actually
in the end and I should say that we
don't just want a render out of this
process right what I think people want
to be able to do is to construct a whole
objective they want to work on higher
level features than just pixels
many people think pixels are just
terrible features and there's a good
argument to be made for that and and
it's useful to have the renderer sit
inside of some oh not again to sit
inside of some objective function okay
so we would say that a good
differentiable renderer should allow
easy to specify models facilitate
objective formulation and scale to
larger problems so I want to say a
little something about scalability so
this is the the expression tree as I
showed you from the previous example so
this is the code example here and this
is the expression tree and this is the
expression tree for our connect fitter
which I'll show to you in a minute it
includes I think six six depth renderers
six color renderers our scape body model
a number of priors it's a it's it's a
fairly large objective and I think it
would have been difficult to build in a
piecewise manner without having a simple
framework that would allow us to to put
things together
I want to make brief note of this thing
that we've also included which is called
chumpy and chumpy is an auto
differentiation of framework in python
and the basic idea is that it is the
auto differentiated version of numpy and
numba is a linear algebra library for
for python so here we can see code on
both sides they're doing the same basic
things but you'll see that this has got
NP and this has got CH here so you could
say where you can see we're inverting a
matrix
we're computing the SVD of a matrix and
then we're differentiating through that
process so the notion is that if you
know numpy you can use chumpy but also
get the derivatives at the end it's not
the fastest framework out there but it's
quite easy to use and I think that's for
the purpose of prototyping that can be
valuable open dr's meant to allow you to
take your render and compute
differentiable features of of an image
so if you want to compute say an edge
image right so you want to not compare a
rendered image against the observable
but a filtered image say against a
filtered observable you can do that very
simply here this is just taking the
difference between most of the image and
most of the image next door in order to
compute a very simple gradient or here's
another example in which image moments
are computed and then the the difference
between the moments of some observed
moments and the moments of the image are
minimized in a gradient based manner and
there's no no magic here right this is
just basically constructing the problem
as you would in numpy and then
minimizing the objective using the
renderer as as a primitive in and of
itself in order to get work done I think
it would also be interesting to do say
discrete cosine transform but I haven't
done it so I shouldn't talk about it so
next I want to show some of the results
that we obtained one of the big things
that we did with this was a body fitter
experiment in which we inferred the
shape pose lighting per vertex albedo
and camera translation in order to well
it was actually for the purpose of
measurement prediction but I think you
can see that the that the results are
not terrible so this is the these are
the observed the faces are blurred out
which is why they look like that and
then this is the this is the simulated
and next I'm going to show a movie which
flips through
- these yep this is a sample from a
genital model a great question so so
this is this is a literal observed image
one thing that's not being shown in here
is the depth image so we have the color
image and the depth image one confusing
thing that I should clarify this the
reason that this background looks
photorealistic is because we took a
background shot so we have a shot of the
scene without the person in it depth and
color right and this helps a lot with
modeling and convergence right if we
didn't have that that would make things
it wouldn't converge as well so this
movie shows a lot of different things
which I'll explain and I can pause so
what let me say something about this so
what this is going to show is it's going
to flip back and forth between some
observed images and some simulated
images so that you can see them overlaid
and up here it's going to show the
albedo and the lighting and then just
the albedo that's estimated of the of
the person in 3d so that's it's going to
flip back and forth so that you can see
some of the results that that we've
obtained so yes exactly yeah I should
have pointed that out so this is with
with escape so it's parameterised by by
a set of pose parameters and a set of
shape parameters I think it's probably
about 50 on the order of 50 pose shape
50 pose parameters and 50 shape
parameters the big part of this
optimization actually was the was the
per vertex colors because there are so
many vertices so we have a variable in
there for for every vertex color and
something and a smoothness term which
says that nearby vertices should have
colors that that are close to each other
right another thing that I wanted to
point out is something about the the
performance of this system the
performance I guess in general it's hard
to characterize because it depends on so
many things right it's going to depend
oh sure
for the for the body fitter yeah you
know it doesn't it doesn't matter too
much basically if you just have the you
know so it's it's with the mean shape we
do have the gender of the person but the
mean shape roughly in that pose so we
could not here's an example of something
we couldn't do we couldn't initialize
here and fit data like this right so we
do initialize with the body model turned
roughly in the direction that we expect
them to be but because the background is
in there the convergence is quite good
there's another thing I should mention
though which is which is kind of
critical which is we're not comparing
images to images observed to simulated
work we're comparing pyramids to
pyramids so because that's such a sort
of simple linear filter and because the
number of pixels just doubles right when
you go from an image to a pyramid and
because that helps our convergence so
much we make a lot of use of that and we
find convergence to be quite slow
without that exactly exactly so it's
it's not it's not so much coarse to fine
as coarse and fine
now in practice if you want things to be
fast it makes more sense to have a
course and then course and a little bit
fine and then coarse and very fine
because at the very first step you're
going to be guided mostly by the course
but but if you don't care about runtime
then doing it course and find the whole
way with
all the levels of the pyramid throughout
works fine so I guess put another way
for the fastest fitter it makes sense to
step through resolutions of both the
body model and the number of pixels but
for the purpose of just getting a good
result you can leave the body model at
one resolution and leave the number of
pixels at one resolution you'll get
something that's that's just as good
okay so about performance I did want to
compare the performance of rendering
versus the performance with derivatives
with the caveat that that this is a
little bit difficult to measure in a in
an all encompassing way and maybe
someone will have a suggestion about
this but it depends the performance is
going to depend on the number of pixels
on the number of polygons and on the on
a lot of things about the setup of the
rendering process and so it's it's hard
to to characterize in a really general
manner but for this kind of toy problem
given a thousand 24 triangles and
keeping in mind that here is 640 by 480
this is showing the difference between
rendering and and and differentiating so
the the one bottom line could be that if
you're you only have six parameters then
only using finite difference would be
about twice as fast but with more on the
order of I guess this would be about
1500 parameters because there's three
components per vertex that's 80 times
slower and it's also worth remembering
that although you can perform finite
differencing it may not work that well
for certain things like in-plane
rotation right so it could be faster but
it may not work as well so what's next
one thing that we want to work on is
improving the the core right so we would
like to have it be more accurate to
include global illumination
to have explicit objective smoothing if
the comparison of these pyramids is an
approximation for that then we might
want to to do that more explicitly and
to make it faster I think all of those
things are interesting and you know
making it broader could also be good and
what I mean by that is we make a certain
set of approximations but adding another
renderer that makes a different set of
approximations maybe less approximations
could also be be valuable but another
important aspect is bridging open D are
two other other kinds of approaches so
we are subject at the end of the day to
being in some Basin so if we're not in
the base where we want to be it won't
work so we're very sort of aware that we
may have to use sampling methods in
practical situations to solve real
problems we're still kind of interested
in using hybrid Monte Carlo as a way to
explore space and then there's also this
class of related work in probabilistic
graphics programs which we think is kind
of interesting in specifically this is
kind of interesting because this idea is
kind of the same of saying we want to
concisely specify write a program for
the purpose of computer vision in order
to invert the system or characterize the
posterior in the system in this
particular work though they're really
focused on being concise about the setup
of the probability distribution so you
can see there are things here like
laying position and Road width and you
start wondering well where is the code
that that actually instantiates this and
I think the place that open d'art is
simple is probably where this is a bit
complicated so I think it would be very
interesting to combine some of the
probabilistic programming kinds of
approaches with open D are going going
forward so these are really
complementary works and and we'd like to
yeah to combine them if you want to
download open D R and you have a Mac or
Linux maybe then it's it's pretty simple
it's just a pip install away and and I
hope that that people will try it out
and use it and and bridge it to other
methods and and give me feedback on it
so in conclusion we've shown that open
dr can allow approximating of a
rendering process differentiating
through that process approximately and
that it can allow parameter estimation
but it doesn't solve everything but it
is good for high dimensional problems
and local problems right right so so
using this with with video right one
might want to use not just like six
frames but six hundred six thousand I
think it's a very interesting question
you know you have all of this redundant
information or close to redundant
information for neighboring frames and
yet you'd like to make to make use of it
well there's some work that's going on
to do that at back at my lab but I
probably can't talk too much about it
yes I guess the answer is yes I think
that's right right well you know the the
thing that comes immediately to my mind
is how do you handle this explosion of
parameters right so if you're going to
solve it all as one problem and maybe
you don't have to maybe you could solve
it sequentially or in some kind of an
altar
Manor but but that's that's the the
immediate concern to come to that comes
to mind but maybe you're thinking about
sort of integrating things like flow and
movement or the differentiable you know
video volume render or something like
that and we've thought a little bit
about it but it's the struggle is with
keeping it fast enough and keeping
everything in memory so that everything
is yeah so image is also highly
redundant within the image spatially
right and I guess existing renderers are
mostly built for humans to look at the
output so they don't want to render just
partial images but have you thought
about getting speed ups by just focusing
on certain parts of the images or
sarcastically subsampling certain parts
of the images because these objectives
that you fit normally seem like large
sums over all the pixels in image
yes so so as first stochastically
sampling and I think that's that has
been done in this is was one of the ways
that initially some of these problems
were solved because back then there was
no way that all of this could could be
fit into memory I think I think it's a
good question you know they're they're
going to be certain parts of the image
that are going to be maybe more
informative than others boundaries for
example you make care more about
boundaries you may care more about
heavily textured areas and maybe there's
a way of reducing the size of the
problem by getting rid of some of the
parts of the image that you don't care
about or that are maybe somehow already
explained by the set of parameters I
think that's absolutely right and and
maybe that's a way of getting around
this this kind of explosion of at least
in the output space right yeah well you
chose to build it in Python and what the
consequences of that decision are sure
so we built it in Python because python
is is easy to use
easy to read and for prototyping
purposes we didn't want to spend a lot
of time writing a sort of systems-level
code when you know we would rather be
able to prototype things right in in
ways but there are definitely
disadvantages to Python the threading of
course is kind of terrible in Python and
when you build an auto differentiation
framework in Python you're stuck with
slowness of Python right and it's it is
a real issue because I think for a small
project python is great and for a
medium-sized project python is great but
then if you try to get much bigger than
that and your performance sensitive
right and you have all these primitives
in play you saw this graph that was up
there each one of those is a Python
object well so so it's oh well the last
time that I checked 80% of the the cost
so this is very kind of off the cuff
right because it's very problem specific
right but 80% of the cost of evaluation
was you know in doing things like
stacking matrices and shuffling objects
around was in not in the not in the dr
but in this auto-da-fÃ© framework and not
for any particularly great reason so I
think if there were you know I think
Julia Julia was suffering less than 20%
of the total anything sorry the OpenGL
renderings probably oh the OpenGL
rendering is a very small percentage of
the time and I think you know earlier
the the frames per second that we're
getting right now are maybe I don't know
a hundred frames per second given I
don't know maybe 640 or a thousand
something like that and that's quite low
right for the
number of polygons that we're dealing
with so so we definitely have not
reached the potential of this by any
stretch and I'm very curious to know
yeah to know what that potential is it
probably means bringing it to another
language though yeah but it's nice to be
able to prototype yeah oh yes sorry I
mean if you would take a small toy
example of only one image with a very
simple texture maybe a linear
interpolated color
mm-hmm would you get the another to go
derivative with your family well so
there are a couple so one answer is we
haven't checked that so one could
construct the analytical derivatives by
just having the polygons right adding
them up and finding out what they are
and I know that they wouldn't match
right I mean of course they're not going
to match exactly I guess the question is
how well will they approximate these
analytical things and furthermore what's
the I think this would be a great graph
to have as a function of say any
aliasing what's the convergence does
does it actually converge towards some
kind of perfect and how fast you showed
that only taking finite difference of
the previous and then the next rendered
image gives you some some sort of yeah
two pixels or something with whether it
were they where the derivative is active
from the bad ingredients you got only
the the derivative active on the
boundary of the object just this come
from multiplying the partial derivatives
of geometry and pixel appearances or or
how do you reduce the the support of the
relative to the poem so so put another
way why why don't I have the same
problem as finite differencing is that
sort of roughly the
getting really the support of the
derivative on the boundary of the object
only maybe amounts to 2 multiplying 2
partial derivatives where one has
Steve's support only there for example
the geometry why don't you first tell us
what the analytic derivatives are for
that image so so the so you want to know
the analytic derivatives of this
cylinder given that you have some box
know your function so the derivatives
with respect to my function are not 0
everywhere yes yes that's right and that
that's one of the approximations so with
OpenGL an infinitesimal change in some
kind of underlying geometry control
parameters will change nothing right the
image yes yes yes that's true so at the
boundaries nothing would the boundary
would not change but you're absolutely
right so within the object things will
change so it will change nothing it will
change nothing right so if you take
finite differencing with some
infinitesimally like incredibly small
jump nothing will change but part of our
approximation is to is to approximate
what that change would be if it did
change okay not with not with the Box
PDF but approximating the derivative of
the image pixels with respect to the 2d
vertex locations by spatial filtering
okay and so what this is essentially
doing is it's saying one could take the
output of any renderer and say that the
derivative of this this output pixel
with respect to some small translation
into D right is approximated
by the spatial derivative so it's
important this is important to realize
right this is a big approximation
because OpenGL right the the boundary it
as you say it will not move with an
infinitely small movement of the
vertices and yet our derivatives will
indicate that that there is change and
that's important in making the making
the practical aspects of optimization
work</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>