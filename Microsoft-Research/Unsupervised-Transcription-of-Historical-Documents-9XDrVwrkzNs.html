<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Unsupervised Transcription of Historical Documents | Coder Coacher - Coaching Coders</title><meta content="Unsupervised Transcription of Historical Documents - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Unsupervised Transcription of Historical Documents</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9XDrVwrkzNs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi it's my great pleasure to welcome
Taylor Burke Kilpatrick to visit us from
Berkeley Taylor has done a number of
interesting work ranging from a diverse
topic such as like were linemen
summarization and more recently on
recognizing text from from from images
so and then I have a number of us have
been using things like for example I
feature-rich GM that it has developed so
I'm very exciting and have him here and
without further ado is Taylor thanks hi
so this is joint work with Greg Durrett
and Dan Klein my advisor and this talk
is going to be about transcribing images
of historical documents into text so let
me first just give an example of of a
historical document I'll be mainly
talking about documents from the
printing press era so here's an image of
a document that was printed in 1775 and
it's part of the proceedings of the old
bailey courthouse in London I'll let me
read just a little bit of it Jacob
Lazarus and his wife the prisoner were
both together when i received them i
sold 11 pair of them for three guineas
and delivered the remainder back to the
prisoner so researchers are interested
in being able to do analysis on
collections of documents that look a lot
like this one but in order to do many
types of analysis you first have to take
these images and turn them into text and
so that's what this talk is about but so
let's let's see what happens though when
we take this particular historical
document image and plug it into a modern
off-the-shelf OCR system i'll use google
tesseract as an example
so here's the transcription you get out
of Google tesseract I've marked the
errors in the transcription in red and
you can see there's a lot of errors in
fact more than half of the words here
are wrong that's a pretty terrible error
rate and that's weird because Google
tesseract is really good on modern
documents most OCR systems are really
good on modern documents so next to help
explain why existing OCR systems perform
so badly on historical documents let me
just quickly cartoon how a typical OCR
system might work so here's an image of
some digitally typeset modern text
restored modern so the first thing at LC
our system will often do is try to find
a baseline for the text in the image
once it's found the baseline it will try
to segment the pixels in the image into
regions that correspond to individual
characters so tesseract actually does
that by identifying connected components
okay so once the image is is segmented
into characters the system will then try
to recognize each character segments
individually and that usually happens
using some kind of supervised classifier
that's trained on a single font or maybe
a collection of fonts so this kind of
pipelined approach works really well on
modern documents that are digitally
typeset so that white space is really
regular and where the font is known but
we've seen that that kind of approach
fails really badly on on documents like
this one so now let me tell you the kind
of the three primary aspects of
historical documents that make them
really hard for existing OCR the first
problem is that the fonts in these
documents are often unknown so here's a
rendering of the word positive in a
historical document we can try to line
up a modern serif font with the first
two characters for this particular
modern serif font doesn't quite line up
because the descend
the P isn't quite long enough try a
couple more modern serif fonts they
don't line up in fact no modern font is
going to perfectly line up with this
image because the font used in the image
is an ancestor of modern fonts an
extreme case of this problem of unknown
font you can see here with the use of
the long s cliff so we don't even we
don't use this cliff anymore so we don't
even have a representation for it in
modern fonts okay a second problem that
occurs in a lot of these printing press
air documents has to do with with the
baseline of the text so this example
reads the death of the deceased and you
can see that the baseline of the text
kind of wanders up and down as you go
across the line the glyphs kind of hop
up and down and so this actually has to
do with the way that these documents
were printed using the printing press so
humans take you know templates character
templates and align them using a
mechanical baseline and there's some
slop in that mechanism and as a result
you get this kind of noise in the
baseline okay a third problem has to do
with inking levels and the fact that
they vary across a lot of these
documents so this example reads rode
along in silence and on the left of the
example the word road is under inked
it's so under ink that the glyph for the
character D has actually divided into
two different connected components on
the right there's a large degree of over
inking and bleeding ink so much so that
many of the glist have now come together
into one big connected component and
this again is a result of how the
document was printed maybe the ink was
applied to the roller bar and evenly or
maybe pressure was applied unevenly when
the document was printed so here are
four portions here are portions of four
different historical documents I one
from 1725 1823 1875 and 1883 and you can
see that the three problems I mentioned
are present
in these four documents and in fact if
you look those three problems are
present and pretty much all documents
from from the printing press era so our
approach is going to explicitly deal
with each of those issues in turn so the
way we deal with the problem of unknown
font is that we're going to learn the
font in an unsupervised fashion from the
document itself we're going to deal with
the noise that is a result of the
printing process by jointly segmenting
and recognizing and doing that in a
generative model that explicitly models
the wandering baseline and also
explicitly models the varying levels of
ink across the document so now I'll give
you a high-level depiction of the
generative model that we use it's
actually going to generate all the way
down to the level of pixels so first
thing we do in the model is generate
some text character by character and
that comes from a language model and now
conditioned on that text we're going to
generate a bunch of typesetting
information and so this is going to look
like a bunch of bounding boxes that tell
you how the glyphs are going to be laid
out so specifically first thing we do
for the character p is generate a left
padding box so this box is going to
house the horizontal white space that
you see before you actually get to the
glyph for the character p then we'll
generate a glyph bounding box repeat
which will house the glyph for p and the
vertical white space above and below it
and then the right padding box will do
the same for our and the rest of the
characters so now we have a kind of
layout we're also going to generate for
each of these glyph boxes a vertical
offset for the glyph that goes in that
box and so this will actually specify
the baseline of the text and allow it to
wanders it goes across the document and
then finally we will generate an inking
level an overall level of ink in each of
these glyph boxes and that similarly
will allow us to model the wandering
levels of ink across the document yeah
variation so we're actually going to be
learning the glyph shapes and so once
you have once you've specified the
baseline and you know the shape of the
glyph in terms of its pixel layout that
then defines the top of the glyph as
well so that's enough to specify that
would become more clear as I'm going to
dive into increasing levels of detail
here and okay so then once we have all
that typesetting information and the the
text condition on all that we're going
to actually start filling in pixels and
this will happen in the rendering model
so we're actually laying out the pixels
for the glyphs and then here we've
generated an image of the text alright
so I'm going to name some of the random
variables involved I'll call the text e
and that's going to come from POV the
language model and the typesetting
information I'll call T and that comes
from the typesetting model p of t given
e and then finally the the image itself
the actual grid of pixels I'll call X
and that will come from the rendering
model P P of x given ENT and so the only
observed random variable in our model
it's going to be X the actual image
itself both a and T you're going to be
latent random variables and during
learning will marginalize them out and
when it comes time to do transcription
will fill them in with inference okay so
now I'm going to tell you more details
about each of these three components in
turn let me just say that probably kind
of the first thing that popped into your
head about what each of these components
might look like is basically correct
with one exception there's going to be a
little extra level of complexity in the
rendering model and I'll tell you about
it when we get there all right so first
let's talk about the language model
we're going to have a random variable
e-sub I corresponding to each character
all right and these random variables
will be distributed according to a user
nice smooth character six grand model
that will train on a large corpus of
external text data so this is going to
resolve a lot of ambiguity
you when we're doing this in an
unsupervised away the fact that we have
a strong language model train not a
bunch of data okay so now let's talk
about typesetting yeah so we played with
a couple different language models
you'll you'll see them in a second one
is out of domain it comes from giga word
and the others in domain for some of the
historical corpora that we looked at so
the in domain one is from that period
and it was manually transcribed there
was a big curatorial effort that like
actually transcribed a bunch of these
documents manually and so we can use
that to sort of see what the penalty is
from using auto domain data okay so I'll
talk about the typesetting model and
we'll look at the typesetting process
for the token net index I and it works
the same way for each token so the
character random variable at index is is
e sub I as we've seen and let's say that
that's an a so the first thing we're
going to do in the typesetting process
is generate a random variable L sub I
which specifies the width of the left
padding box and also by is going to come
from a multinomial then we'll generate a
random variable G sub I which is the
width of the glyph box and that's again
going to come from a multinomial and
then finally we'll generate the width of
the right padding box again from a
multinomial and remember the left
padding box horizontal white space right
panel box horizontal white space glyph
box for the glyph and the vertical white
space and so these three multinomial are
specific to the character type a they
govern the widths of tokens of type a
and the distribution of horizontal white
space around tokens of type pain and so
that ladder thing can capture effects
like kerning we're going to have similar
multinomial for every character in the
alphabet and together all these
multinomial are part of our
parameterization of the font that we're
going to learn in an unsupervised
fashion display phone what research is
the real perp just you
out of the these pictures that box waist
sorry other spots the green pathways the
glyph box with yes what are you really
using it army clicking by a city uh oh
here it depends on the height that we
choose so we down sample the resolution
of the images so that each line is 30
pixels high and then I believe that we
capped the glyph box with it 30 the same
as the height so yeah I think that's
correct that's actually what we use in
the model 12 30 that's in terms of
pixels ok so we're also going to
generate the vertical offset as I
mentioned before and that will call V
sub I but that's going to come from a
global multinomial and then finally
we'll generate the inking level for this
glyph box d sub I again from a global
multi-domain ok so now we're going to
try to fill in the pixels for this
particular glyph box token and so far
we've generated the width of the box the
vertical offset of the glyph that's
going in the box and the inking level
for printing that cliff in the box and
we're trying to fill in the grid of
pixels which is which is X so there has
to be something in our model that tells
us the shape of the glyph for the
character type a because actually we're
trying to print an a here because we're
saying that this isn't a so that part of
our model is going to be a grid of
glyphs shape parameters and here I'm
showing you the particular setting of
the parameters that we learned in our
model on some document these glyphs
shape parameters are going to combine
with the current glyph box with the
current vertical offset in the current
inking level to produce an intermediate
grid of Bernoulli pixel probabilities
that has the same dimensions as the
particular glyph box token that we're
trying to fill in and once we've done
that we can just sample the pixel values
in the glyph box from the corresponding
Bernoulli's so the special thing about
this approach is that our
parameterization of glyphs shape is
actually independent of the particular
width offset and inking level for the
token we're trying to
generate and that means that we can we
can fix the glyph shape parameters in
the model and then vary the glyph box
with to produce tokens with different
widths we can change the vertical offset
to generate Uggams with different
baselines and we can change the inking
level to generate tokens with different
levels of ink all while keeping the
shape of the glyph the same and this is
important because it means that when
we're doing learning when in the
document we encounter an instance of an
under inked a and then later in the
document in count encounter an instance
of an over inked a both of those sources
of information can influence our notion
of what the shape of an egg lift should
look like we don't have to have separate
grids of parameters for under ink days
and overing days and it turned out that
this this kind of shared
parameterization that we're using was
really important to getting unsupervised
learning to work and his task because
after all we're going to try to learn
these glyphs shape parameters
unsupervised style okay so here's what
I'm going to go into a little extra
level of detail the key to getting this
parameterization to work is how you
define this function that map's the
glyph shape parameters along with the
width offset and inking level down to
the intermediate grid so I'll tell you
about that process for a particular row
of the glyph shape parameters how it
generates the corresponding row of the
bernoulli pixel probabilities so called
the the glyph shape parameter row five
and the Bernoulli pixel probability row
theta and first I'll tell you how we
generate the leftmost Bernoulli pixel
probability in in the row so in order to
generate the leftmost guy we interpolate
the glyph shape parameter row five with
a vector of interpolation weights alpha
that shape according to a Gaussian that
centered over to the left of the row of
glyph shape parameters once we've done
that we just apply the logistic function
and that gives us the actual Bernoulli
pixel probability now to generate the
remaining Bernoulli's in this row as we
move along to the right
we just interpolate with different
vectors alpha letter still shaped like
gaussians but now we're centered
proportionally along the row so this
means that the Bernoulli pixel
probability position J is generated by
interpolating with a specific vector
alpha sub J and now our parameterization
looks like this theta sub J is log
proportional to the inner product of
alpha sub J and Phi now it turns out
that we can get the various effects that
I talked about earlier of varying width
of tokens varying the vertical offset
and varying the inking level by having
these interpolation weights actually
depend on the random variables G sub I
visa vine d sub I alright so I'm not
going to there's some more detail to be
talked about that I'm not going to talk
about it right now what I do want to say
that before we move on to learning is
that you can actually view these
interpolation weights like fixed feature
functions in a locally normalized log
linear model and that means that when it
comes time to learn the glyph shape
parameters Phi we can actually use out
of the box on supervised learning
methods so that's a nice property of the
parameterization here hey that's I you
pick the stand up
and just rescale done to whatever your
car GI yeah so that's the effect that GI
would have so you'd have a specific so
for the position J you'd have a specific
set of alphas that would be used to map
down the full width cliff shape
parameters to a glyph box depending on G
so suppose you wanted to map down to a
cliff box with with ten you'd have a
specific set of interpolation which they
would do that and G would pick that out
but you define those ahead of time
similarly VI is just going to pick which
row that you're pulling from the
trickiest thing here is di so it turns
out that you can because you're pushing
this through a non-linearity by
rescaling everything by changing the
interpolation weights and by adding bias
you can kind of change things like
contrast and darkness levels and so you
specify for each of the discrete
settings of these random variables you
specify ahead of time the kinds of
interpolation which that you will use
and then those are actually they can be
treated like feature functions but they
can actually do all these effects that
we want using this single
parameterization of shape five that's
that makes sense so those a lie
yeah yeah so another question is that
you're working your baseline wondering
and also you're ready Sims I from your
in your generative model they are
independent they are ya what they say
there's reason to believe they shouldn't
be right yes especially the base
language yeah also inking they should
all be kind of slow very and even I'll
show you an example later where because
of three-dimensional warping is you get
near the bindings of books there's
actually slow varying in the change of
the width of the glyphs as well so we
don't explicitly model any of that and
it works really well but we thought
about ways to extend it we think we've
seen a couple examples a couple errors
that we think it might actually lead to
improvements but there aren't a lot of
errors that we think of it actually fix
oh yeah but I mean it's interesting I
mean we can talk about it it's
interesting how you might do it because
it gets to just do it naively you end up
adding a lot of states to the semi
markov model you can get really slow and
this is already pretty slow but you can
think about using some kind of mean
field approximation because you know the
offsets are basically independent of
what's going on with the language model
and so it might sit appropriate
approximation okay so obviously talk
talk about learning so we use e m to
learn the the font parameters and so
that again means learning both the
multinomial is governing the layout and
the glyphs a parameter specifying glyph
shape and we initialize IAM using font
parameters that come from mixtures of
modern fonts so we just took all the
fonts that appear on the ubuntu
distribution and mix them down and in
order to get the expectations that we
need for EM we run the semi Markov
dynamic program because the model itself
is actually an instance of a hidden semi
Markov model and then is it just your
own way
yeah uniform the initialization mixture
yet so there were by hand we identified
some weird ones like comics hands
there's like a bunch of like Zapf
dingbats or whatever and we just said
those don't count but the ones that
actually look like text we just
uniformly put them together and so to
make this fast we took a course to find
approach to inference okay so now you
know basically what the model looks like
and you know how we do learning
inference before i show you experiments
i just want to show you two examples of
the of the system kind of in action so
here is a nip it of a document it reads
how the murders came to so if we train
the model not using am on the entire
page that this snippet came from we
learn some font parameters and now with
maximum likelihood inference we can fill
in the text random variables and get a
transcription so this is the actual
output of our model how the murders came
to we happen to get this one right we
can also do maximum likely to inference
on the typesetting random variables and
use that to kind of peek into what the
model is actually doing here so here's a
representation of the models prediction
about the typesetting random variables
the blue here corresponds to padding
boxes and spaces and the white boxes are
the cliff boxes so you can kind of see
the layout the glyphs that actually
appear inside these white boxes are the
Bernoulli pixel probabilities that the
model chose to use to actually generate
the pixels that it saw yes space so the
way we model space is we say it's a
special type of glyph that has fixed
parameters that are just white with some
small probability of black for noisiness
yeah so we don't try to learn that but
so that this this means then that that
space is generate a glyph box which is
blank and then to padding boxes with
surgery which is blank which is kind of
like there's a identifiability problem
there but it didn't doesn't really cost
problem
determine the width of a glove box where
they being has been very overrated for
example a man re
starter against him would be a wide flip
fox are and in would be to lift promises
but if you're given a so how would you
distinguish how do you distinguish
between if you're setting up the glove
box how do you distinguish between the
case where so the language model the
language while the language model would
tell you hopefully yeah I mean if yeah
and some of the errors we get our
because the language model is ambivalent
about stuff like that and you will see
cases where ya confuse can you tell me
again you're testing procedure do you
see a document and then you run through
your bottom oh yeah and then you get a
global inc multinomial parameters the
global baseline from which is right and
then so basically you to impress on
without attachment so we're running in
to get the sort of out parameters and
then we do maximal good inference to
transcribe nice okay there's no
supervision and we're training and
testing on the same document cuz it's
totally unsupervised because we have to
learn the font in that document before
we can transcribe location turns out to
be enough yeah so ideally we'd be able
to do this on whole books and actually
update the language model parameters as
well but right now it's a little slow
but we're gonna make it faster so well
so i want you to see in in this example
and the British you type setting that
we're actually we're capturing the
wandering baseline the models using the
vertical offset random variables to do
that which is good the model is also
using the inking random variables to
capture the over into H on the left and
the underrung Ian in the word done so
it's cool to see that the model is kind
of doing what we intended to do i want
to show you another example this snippet
reads taken ill and taken away i
remember and again we use the M to learn
parameters on from the whole page that
this this snippet came from and we get a
transcription and it turns out again
we're right in this case and then here's
the predicted typesetting so here I want
you to see that again we're modeling the
wandering baseline
this is something I mentioned earlier
the wandering baseline in the input here
isn't a result of the printing press and
how it worked this is actually a result
of three dimensional warping as you get
near the binding of a book and so we can
capture that but what's more cool is
that because the page begins to face
slightly away from the camera as it
recedes in three-dimensional space those
characters are actually thinner and so
are because we can generate glyphs of
different width we capture that here was
the thinner be in our so it's kind of
cool that we can capture this effect
because these effects are kind of common
in a lot of these historical documents
actually be not horizontal
no so we we don't model the fact that
they have slanted we're just it's an
approximation were saying yes they're
kind of moving down and getting thinner
asleep yes yeah basically doesn't have
an angle it just has it's just an offset
cuz they're that red dog show yes yeah
that was just yeah I'm just like laser
pointer ring with the ones okay so now
let me talk about actual real
experiments aust result on two different
tests eps the first was taken from this
old bailey corpus that i talked about
earlier the old bailey courthouse in
london and here we chose 20 images at
random distributed across years each
consisting of 30 lines and that formed
our test set and we manually transcribe
these guys to get the gold data and the
second test that comes from a corpus
called trove and so this is a collection
of historical newspapers from Australia
that's curated by the National Library
of Australia here we took 10 images at
random again each consisting of 30 lines
well we comparing into two different
baselines the first is Google tesseract
which I mentioned earlier the second is
a state-of-the-art commercial system
called abbyy finereader and this is
actually the system that was chosen by
the National Library of Australia to to
automatically transcribe the documents
they had an intro I'll show you
experience with two different language
models the first is out of domain as I
mentioned this this we trained on 34
million words from the New York Times
portion of of Giga word and the second
language model is in domain for our
dataset and this came from manually
transcribed text from the Old Bailey
Corvus 32 million words
restricted it to just a New York Times
portion that he would yeah so it's a
character 6pm model and it seemed like
more data wasn't helping and it was
making reading stuff in slower but yeah
ok so I'll show you results in terms of
word error rate on our on the first test
set which was the Old Bailey corpus so
here bigger bars are worse so Google
tesseract on this test set gets a word
error rate of 54.8 so it's pretty bad
it's kind of like we saw in that small
an example this is getting more than
half of words wrong abbyy finereader
does a bit better it gets a word out
rate of 40 on this test set and our
system with the out of domain language
model gets a word error rate of 28.1 and
with the in domain language model gets a
word air rated 24.1 so that's a that's a
pretty big reduction in air it's
actually more than a fifty percent error
reduction compared to Google tesseract
on this test set here's the the second
test set trove here google test iraq's
doing even worse this is kind of a
harder test set in a way it's getting a
word i rate of 59.3 now a be finding
here's readers getting almost half of
words wrong 49.2 in our system with the
outer domain language models getting
word error rate of 33 and so that's not
quite a fifty percent error reduction
compared to Google tesseract but it's
kind of close okay uh so I just want to
go back to the original example and sir
using the other names knowledge gateway
us we never ran it we could have we
figured that it would have been more in
domain that nyt was for trove but not as
in domain as it was rolled Bailey
because it was actually for the Old
Bailey corpus would it make sense to use
card error right
yeah we have those numbers to in the
paper we went back and forth about what
to present the carrier rates character
error rates are much lower and that's
what people usually present but they're
kind of misleading because once you get
a you know a couple characters wrong in
a word it makes it hard to search for
makes it hard to read I don't know I
think it's better to see where errors
but I can't i think the character error
rates are in the tens something like
this I don't totally remember thank you
using a six load six grand Missy all
right let it rain yeah character yeah
yeah why don't these words so we did a
lot of experiments actually after we
published the paper on trying to boost
the language model because we thought
like you did you know less and
ambiguities better it's actually hard
the problem is that your state space in
the semi Markov model gets huge and so
you have to do different approximations
we started doing beaming and the beaming
kind of interacts badly with models that
have these kind of two levels of
generating words and then characters we
found and even when we waited long
enough to get more exact results we saw
that it wasn't giving a big improvement
for the additive main language model
even for the endo main language model it
wasn't giving huge improvements which is
a little we're not totally sure we
expected it to give bigger improvements
but it didn't so here's that original
example again so just to give you an
idea of how legibility gets better this
is google tesseract with more than half
of words wrong and this is our system
with fewer than a third of words wrong
this is our output on that on that
document you can see that it's it's
easier to read okay next I just want to
show you some examples of the fonts that
that we actually learn so first here is
the initializer for the glyph shape
parameters that we use for the character
G you can see that it is a mixture of a
bunch of modern fonts there are kind of
two different types of
dominant jeez that are visible here the
first is the type of G that has the
descender starting on the left and the
second is the type of G that has the
descenders starting on the right so here
are the final parameters that we learn
for these glyphs shape parameters after
running am on various documents from
across different years and you can see
that in all of these historical
documents the type of G with the
dissenter on the left is used and we
were able to learn them we're also able
to learn more fine-grain distinctions in
glyph shape as as glyphs shake varies
from document to document and across
years there's one more cool thing that
we can do with our model I want to talk
about and this has to do with the fact
that we're actually generating all the
way down to the level of pixels we can
treat obscured pixels in the input as
either better a result of over inking or
blotch tank or even tearing in the page
we can treat obscured pixels as
unobserved random variables in the model
and then during learning we can
marginalize them out and during
transcription inference we can actually
make predictions about their values so
here is a portion of a document that we
found that had severe bleeding ink from
the facing page you can see that's here
these big capital letters they got
superimposed we went and manually
annotated that bleeding Incan red that's
why it's red here we told the model
during learning to treat all the red
pixels is unobserved and during
inference to try to fill them in and so
here's the predicted typesetting that we
get on this portion you can see that we
correctly identified the word mouth
which is cool and we made reasonable
predictions about what pixels should go
there we also correctly identified the
missing d that was totally obscured in
the word found and again made a
reasonable prediction of what the D
shape should be that goes here because
after all we know what these look like
in this document because we learned the
font on this document
so that's cool and we're looking at
there's there's future applications that
you could think of doing integrating
this model with different kinds of
reconstruction techniques that we're
looking at ok so now I'll conclude so
unsupervised font learning we've seen
that it can yield staley results on
documents where the fonts unknown we've
also seen that generative Lee modeling
these sources of noise that were
specific to the printing press arrow
documents it was was really effective
and finally uh we're trying to make this
into a tool that so that historians and
researchers can actually use it we're
working on that now the biggest problem
is that it's slow right now we're trying
to get fast and thanks yeah so one
observation a few slides back when you
showed you know a comparison of the
Google recognition and your recognition
there's one yeah yeah actually so when I
look at it it looks like especially
names yes so this is why we're think of
adapting a language well so these are
like court transcripts so it's almost
like a script the names occur over and
over again and will transcribe the same
name completely differently every time
we see it I do want to like that I think
that this is funny if you see what we
will test did fool morphs that's one
point that I think it's funny that it
said that but yeah so some kind of
constraints or we thought a little bit
about this so that that you don't
transcribe the same name differently
each time if you add to that constraint
that might improve things or if you
simply try to adapt the language model
yeah I was maybe an ie thought but I was
I was thinking if you had a separate
character and remodel or capitalized
that's a good idea Thanks because I mean
the names often you know they're there
they may not be English they may
yeah coming in from the right now just
there's a lot of character and grand
yeah bilities that are probably very
different in names than they are in
English chemical oh no sense and
sometimes especially if the name isn't
severely degraded you'd be better off
like ignoring the language model all
together maybe like for example you can
see here this looks like the language
model likes it but it probably doesn't
but the pixels don't like it so it's a
good idea but you try us to a model on
the Bothan documents yeah we did and
just a little bit and it does you know
the character the character irate some
other documents in our system and other
systems are tiny it's like you know less
than 2% character array 1% character
array and so it seemed like ours was
like about as good as Google tesseract
the thing is ours is so much slower than
Google tesseract that you should just
use a different google tesseract if you
don't want to document how slow it is
right now right now it's about 10
minutes per page maybe 20 minutes and
that's not a pretty fast desktop but it
turns out the actually the biggest
bottleneck isn't the dynamic program
which we've made really fast now the
bottleneck is actually the computation
of all the emission probabilities for
all the different spans because there's
actually a you know many different types
of templates you could actually used to
print things you have all different wits
different office that's different inking
levels there parameterize in a really
simple way but you actually have to
compute how each of those possible
templates would fit with the input and
so that's a big computation you can
write it as something that looks
basically like a matrix outer product
and so just doing that and our code made
it faster I mean do is like cash flow
cavity and stuff but we're also looking
at putting it on a GPU because it's sort
of ideally suited for a GPU but that
said we're worried that historians or
humanities researchers aren't going to
have like some desktop some gaming
desktop with like three GPUs in it so
we're also looking at approximations
that you could do to speed it up on the
cpu it's solid question relating
but it's try me like you're wherever is
much bigger than the character already
which seems I like I don't have any
comparative it but seems like there is
some sort of error correcting it for two
if you may be also relate to the
Patrick's question too it's like if you
have some were base color language model
right you actually try to recover from
the characters like maybe your whole
word just needs a couple characters yeah
and can you sell can correct yeah we
could spit out some kind of lattice and
then rescore with some high and graham
word something like that yeah it's
probably good idea people do do a lot of
post-processing and I think they see
improvements and OCR outputs so the
Google tool you or for the attitude you
get a scope each character is so yo you
get out right you didn't get a skull
what's the second most possible
irritated oh yeah no we didn't let you
mean like investors or something like
that yeah yeah we didn't turn it three
you that you have that and you just
smooth we stay with your mother probably
would do pretty well yeah yeah that's
possible yeah it's a good idea planning
to sell your toe in the other one former
city yeah we know we don't I don't know
not really so what my questions are how
is it that possible that people
transcript and 32 million yeah I don't
know it's pretty may they said it's like
one of these uh it's not it's not paid I
think they set up a website and people
come to the website and I think it's fun
and they do it and they got lucky that
enough people got interested that they
did it yeah yeah it's kind of amazing
so have you talked to some some
potential consumers like I post written
something ok yeah no yeah we could call
it no yeah more about Ally the social
scientists yeah so we've been in contact
with a couple somebody at Berkeley yeah
and we're trying to figure out what so
we looked at their documents and they do
look a lot like these documents and so
really now it's a matter of us making it
fast enough and making the it interface
easily usable for them you saw a gloss
around but but you definitely seem like
much good idea definitely one semi good
idea that when you try to let's say you
try to learn the hopeful and you keep
updating your language model yeah we
trade language model of a solid by you
initialize from that you're not right
but then they can get
whatever it is how you got yeah so
what's the bottom that uh oh so since I
mean we could have tried it but we
figured that because we're training on
single pages that's not enough data to
actually get signal in the language
model we might be wrong maybe you could
get something like names because they do
repeat but we figured we need it to be
fast enough that we can train on larger
portions at a time so you also imagine
cost of five so you see cause some find
themselves different muscles so with the
course if I was in turn yeah that's
something we're looking at two though
but no of course the fine was in terms
of increasing that the order of the
language model over time doesn't that
makes the dynamic program pretty fast
and so and then you prune using the
lower order one and we use max marginals
vision they eventually I know yeah and
so so this is one of the approximations
we're thinking about there's there's an
older literature on something called
document image decoding and this is also
this old-school kind of generative model
that's similar to this and that they're
they're treating OCR is a noisy channel
model and they came up with some
interesting algorithms with with Minka
back in the day something called
iterative iterative iterated complete
paths where you find so you all those
emission probabilities I talked about
which we're so slow for us to compute
you compute them all at a lower
resolution or with some sort of like
column projection to give you an upper
bound on the actual probabilities and
then you find the Viterbi path and then
you you update all the guys on that
victory path with the exact scores and
then you recompute the battery path and
so once that converges your guarantee
that you found that right got the right
path but you haven't had to actually
compute all those emission probabilities
and so that's that's a place where the
kind of course to find pixel level stuff
could could be really useful
excited what happens with the M
interactions like if you just did
without the dating the parameters just
from the initialization point
so what's the score of the initializer
what yeah yeah it's it's it's like 10 WR
worse maybe more than that so it's not
it's not it's not totally failing but
it's pretty bad it's like maybe as bad
as a be fine reader better than Google
we probably due to the joint dip coating
and then the ability to model up the
noise yeah i think the joint and then
the ability to do baseline stuff so
you're the question i'm wondering why
you didn't ask this one foot so it seems
once you have a model that is this
flexible to deal with my so i don't mean
the next frontier with your handwriting
yeah and I mean there's obviously huge
you don't demand that tribe medical
plane right so we thought about this and
the reason we haven't actually done
experiments is we think we suspect it's
unlikely to work because we've noticed
in air analysis that the places where
our model fails are where you have a
large region where there isn't any
single column of white space between
characters right so then it's on the
first pass vm you get the segmentation
wrong and then the learning all goes
wrong you can get away with this
happening where a couple characters
leave together but with something like
handwriting recursive yeah or even
Arabic OCR even digitally types of
Arabic this could be a big problem so I
mean it might work when we've thought
about ways to I mean maybe with enough
random restarts like you could start to
you know do better but in our initial
experiments we weren't helpful
to the left and back</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>