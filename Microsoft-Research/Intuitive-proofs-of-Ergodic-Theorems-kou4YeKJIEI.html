<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intuitive proofs of Ergodic Theorems | Coder Coacher - Coaching Coders</title><meta content="Intuitive proofs of Ergodic Theorems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intuitive proofs of Ergodic Theorems</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kou4YeKJIEI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone good afternoon
so at Russell's request or suggestion I
researched various various proofs of the
ergodic theorem and the sub additive a
gothic theorem so these are classical
theorems going back almost a hundred
years in the case of the ergodic theorem
and to the 70s for the surviving or
gothic theorem the original proofs you
know were quite work quite hard since
then many shorter and simpler proofs
were found some of the shortest proofs
using the maximal ergodic theorem are
not so intuitive so I want to present
proofs that are perhaps more intuitive
following works of kamae Benjie Weiss
Katznelson and by King and Mike Steele
so it'll be product of least so the
ergodic theorem of berkov can be stated
in a measure theoretic form or a
probabilistic form in the public forum
we have a stationary ergodic process so
XJ so
but this process is a stationary meaning
that the the law of any sequence say x1
x2 up to xn is the same as as the law of
a shift X 2 X 3 to X that's 1 and you
can this is true for N and it follows
from this shift invariance
that you can shift by more than 1 in
other words if we look at the
transformation T that sends just the
sequence X J to the sequence X J plus 1
this transformation is measure
preserving and so our measure is a
probability measure and then we'll also
assume this assumption can be can be
removed but in the applications often
the process is ergodic which means that
if you have any if so we assume that the
process is there got it which means if
if we have an invariant event or an
invariant function so if F is any
function of of the process which is
measurable and satisfies the right
measurable
and and this 15 Darien so f is point
wise equal not just in distribution but
point wise equal to its composition with
T then then F is constant almost
everywhere and and this assumption is
satisfied in many cases in particular
and it's in particular this holds holds
in the independent case but in many more
cases so if the XJ are independent then
in this invariance invariance certainly
implies that its concept okay so in that
setting the ergodic theorem would just
say that if we look at the partial sum
so let me write more generally so the
partial sum of size L starting at K will
be the sum of XJ plus K when J ranges
from 0 to L minus 1 okay so we'll be
interested in the partial sums from
various places and then we have the
normalized partial sum or the average al
of K is 1 over l SL of k
and then the in the theorem states that
and of K for any K but it's enough to
talk about the end of one this converges
to the mean say the expectation of X 1
of course by stationarity they all have
the same in this converges almost surely
that's the work of a gothic theorem
later we'll discuss the sub additive
version okay so all right so let's me
questions on the statement if there are
two there really two parts to the
statement one is that this actually
converges and then which is the maybe
they in general the harder part and then
identifying that the limit is as you
expect the mean of X 1 any any questions
or something unclear it's a property of
the distribution of the XJ and of the
transformation P ok so so maybe you know
worried about what this means you can
instead of invariant functions you can
think just of invariant events or an
event B is invariant if T inverse of B
equals B and it's equivalent to just
assume that all invariance events have
probability 0 1 so that's another form
of a good the city and ok yet yet
another form is that you know for any
two sets of positive measure if you
apply t enough times to one it will
intersect the other at the positive
measure so you cannot have kind of two
sets of positive measure that don't see
each other when you apply T ok so that's
so that's about the statement so so I
want and said there are several short
proofs on to give the one that is most
most intuitive and also generalizes to
the sub additive case so
so I said this proof it starts with the
ideas of the turrican I then a there's
who wrote the proof and non-standard
analysis then this was simplified back
it's Nelson and Weiss and and then by
keen and it's basically his version
they're done so today okay so so first a
so we want to so first I want to say
that it's enough it's enough to consider
positive variables so we may assume on
the XJ are non-negative because in
general you just separate into the
positive and the negative parts and
prove it separately right so you just
write X as a difference of a positive
and a negative part and right to work
with each one separately and get the
result so once we prove everything here
is kind of linear so once we prove it
for the positive part we prove it for
the negative part you can just subtract
and get it so we it's enough to deal
with this case yes no the limit is the
limit is the same but again a n1 is the
limit in right we're summing these are
partial sums from different locations
right so so the fifteen dances of the
distributions right so X 1 is not equal
to X 2 adjust its distribution is the
same so the distribution but so a1 is
definitely not a two but it has the same
distribution okay so yes of course and
also you know easy to see that they must
have the same limit so a in right so I
wrote here a n1 but it applies to NK for
anything this
is the limit right as n tends to
infinity right but okay so so so look at
the limp soup of the a ends and
okay so I said I'm going to try an
exclusive as intuitive as possible which
means not exactly the same as the short
as possible so on to first consider an
easy case which will then generalize so
okay maybe one more definition before
that so we have the limb soup and with
fix and alpha which is less than the
limb soup by the way this limb soup we
our priority don't know that it's finite
at all even though so maybe I should
have we want added added the assumption
to have stationary or got it and I want
to assume that it's integrable so assume
that the expectation of the variables is
finite so it's like that okay
to make this statement meaningful okay
so so we look at this limb soup which a
priori don't know it's finite so we know
it's in you know 0 infinity it might at
this stage be infinite but we'll prove
it's finite if we were talking about the
limit would be immediate that it's
finite form for tooth lemma but we're
talking about the limb soup so there's
something still to prove okay so fix
alpha which is less than the limb soup
and then let's say L of K this will be
random variables these are the first
time that you exceed that the averages
exceed this in soups are the first L so
that al of K exceeds alpha
okay alpha is certainly fine a finite
number yeah okay so so by the definition
of limb soup this number is certainly
finite but it might be very large you
might have to wait for long so so case
one is when these numbers are uniformly
bounded so so this is very special you
know it happens for some organic
sequences like periodic sequences but it
will be a good warm-up for the general
case so suppose at this L of case are
bounded uniformly bounded by some L this
is just a special a typical case and we
see what to do in this case and then
generalize from that so so in this case
we just take the partial sum and write
it so basically we were the idea is to
take the interval from 1 to N and cover
it by intervals where the partials where
the averages exceed alpha so so we know
there is some interval here where the
average exceeds alpha and this length of
this interval right so this is this L of
1 the length of this interval is at most
L and then we find another interval
where the partial sum exceeds alpha and
so on and we know that all these
intervals are no longer than L so so we
stopped the first time we cross n minus
L and this the composition of this
series let's write it now in more
formally so I want to say that the sum
of xj j from 1 to n this is going to be
bigger then the sum i from 1 to some m
of
partial sums so this will be partial
sums from some a K from some number okay
that's right
these are L of ki act ki
so and I'll write some more and then
explain this is going to be bigger than
then the son of L of ki times alpha
which will be bigger than n minus L
times alpha and here where each time we
choose ki so we start with k 1 is 1 and
ki plus 1 will just be ki plus L of ki
right so each time we start at 1 we find
an interval where the partial sums are
large then we just go to the next right
so we find this interval then we go to
the next point find a good another good
interval and so on so all these partial
sums are larger than and their length
times the average right because this is
greater than alpha so we get this
inequality and then and the length of
all these intervals will exceed n minus
L because we just keep going as long as
we can and we just have to stop once we
exceed n minus L it's possible that the
length next interval will overshoot n so
we stopped at that point we don't take
the next interval okay no I didn't this
inequality is you know completely
obvious after you've seen it and maybe
confusing the first time you see it so
please stop me because this is so
apologies for those doing it's trivial
and apologies to those for whom is
confusing but anyone from the second
group wants to ask something this
because this is really the crux of the
whole matter so
we shouldn't mention the ball doesn't
depict
yes that's important
thank you so so that I should have
mentioned that a bar is a constant so
thank you so that's right so a bar this
limb soup Thanks mmm so maybe at this
point I should have mentioned so a bar
is so a observe note that's a bar equals
a bar composed with tea it is if in
other words if we this is a related to
an oops comment so if we start the
partial sums from one or from two it not
only it has the same distribution but
point wise it only the partial sums only
differ by the first variable so when we
divide by n and take a limit or a Lim
soup it doesn't matter
so point wise the link soup of the
sequence starting from the first or the
Limp soup started from the second when
we divide by n clearly easy to see it
has the same limb soup so so we have
this invariance which means because
we're working in the ergodic case same
so a bar is a constant or mustard okay
so thanks offer I should have commented
on that
right yeah so so it's so they we wanted
only to differ in the first term so what
maybe I'll say this so-and-so a you know
a and of one my you know equals a and
minus one of to aim right guessing of
one because SN minus 1 of 2 plus a plus
x1 ok so that's better so because you
know the last term we don't control well
but listen now right so now divide and
so so a n of 1 equals you know n minus 1
over n n minus 1 of 2 plus X 1 over n
and now it's safe to take names oops ok
so this is to formally justify this okay
so what offer pointed out is that if we
just work with averages of length n then
partial sum of length n from 1 from 2
they differ by last element which kind
of varies in time so we don't control it
well we could but it's easier just to
look at the just to use this identity
which compares partial sum of n terms to
partial sum of n minus 1 terms and and
now once we have it in this form we
really can take we really can take the
limbs soup and see this goes to 0 this
factor doesn't matter and so we indeed
derived this identity
okay so
okay so in this case we're basically
done at least was the existence of the
limit because and we can even easily
finish the rest because now if you take
and if you take these two sides right
you divide by n and take a limit so you
get that the so here okay so the limit
of I'll just write it out 1 over N some
xj j equals 1 to n is going to be well
we take this divided by n and take a
limit for limit as n tends to infinity
so this is greater than alpha okay and
of course if we could do this for any
alpha less than a bar then we would get
the lemmings equals the limbs up and so
the limit exists but of course this
assumption that L is constant is a very
restrictive so now I don't want to spell
out the details in this case this case
was more just to see the key the key
argument in the clinic setting now we're
going to just do that same thing in the
general case so so in the general case
we can't assume that these L case are
bounded but what we can do is you know
given given epsilon small we can pick we
can pick a large number L so that the
probability that L K this probability
doesn't depend on K but the probability
that this is bigger than L will be less
than Epsilon
okay again by stationerity this
probability is the same for any case or
could I put here just out of one okay so
the point is this L of K is a finite
number so it's a finite random variable
but so I can put in a large of the
probability this variable is bigger than
that is less than Epsilon
and now we want to modify the process so
that sorry so XK star will be XK in the
case when L of K is less than L these
are somehow well-behaved cases and in
the bad cases so I'm going to look ahead
and if the situation is bad so we need
to wait too long then we're just going
to modify the process input an alpha
here okay this is the modified process
XK star and I want to write it as XK
plus ZK right so ZK is usually zero just
when we are in this case ZK will be you
know alpha minus XK so so we define X K
this star this way and now they end and
define this you know right so so al star
of K are the averages averages of the XK
star X right so XJ plus K okay from 0 to
L minus 1
and and L and then we have L star of K
is the first L so that these averages al
star of K exceed alpha and now we're in
a good position these are always at most
L X star thank you
yes that's the whole point these are
averages of the X stars and and now
because L star of K in fact it's usually
it's L of K if L of K is less than L and
it's 1 and so and so L star K is going
so L star of K what is it it's L of K if
L of K is less than L and it's 1
otherwise so it certainly always at most
at most L so now the previous argument
applies and just partitioning the
partial sums am i from so now we're
going to take n which is much much
larger than N and we're going to take
the sum so the sum J from 1 to N of XJ
star is going to be bigger by the same
argument from before because we're a
situation n minus L times alpha K so
it's convenient that all the variables
here are non-negative so so terms that
we throw away here at the end we know
they're not negative ok so it's exactly
the same argument from before now prove
this proves this inequality which is
really they you know the ki KI
inequality of this step right any any
questions so it's exactly the argument
because we are in that situation where
we only have to sum at most capital L
terms to get the you know to get the
average that we want all right so now
what we can do with this several things
so first let's take expectations on on
both sides so we get that n times
expectation of x1 star is greater than
alpha and what can you say about this
well look it icon starts only differs in
this case so it's at most a this is n
times expectation of X 1 plus Alpha
Epsilon because the difference is just
the expectation of this of this z1 and
that difference is at most alpha and
with probability at most Epsilon okay so
now now we're in a good situation we can
take we can divide by n and take a limit
as n tends to infinity and we'll get
that expectation of X 1 is greater than
and 1 minus epsilon times alpha removing
the epsilon to the other side and this
we could do note that here it's now it's
X 1 it's not X 1 star anymore so this is
true for any epsilon so you get
expectation of X 1 is greater than equal
alpha but this was true for any alpha
less than a bar so we can conclude that
expectation of X 1 is in fact a greater
than a bar which is a powerful
inequality here because a bar was the
lame soup so we conclude that this limp
soup is in fact finite and bounded above
by the expectation okay so now we're
almost done we just still have to argue
that the limit of the X's gives us the
it's also though so a bar and to this we
go for this we just go back to this
yes
yeah but the point that we used it is I
mean at the end we - we use the fact
that this is true for any epsilon right
once we had this right we got this
inequality and I would say this is true
for any positive epsilon so ex1 is in
fact greater than alpha okay and then we
said this is true for any alpha less
than a bar so in fact x1 is
bigger than a bar so now in order to go
back
notice that the XR have two pieces have
this X in the Z so let's make sure we
can control disease well and for this
we're going to use what we've already
proved for the process X we're going to
use it for the process Z so aim so we
expectation of so so this fact that
we've already proved implies when
applied to the process Z which is after
all just another stationary process in a
stationary ergodic process that the
expectation of Z 1 is going to be a
bigger than the limb soup 1 over N some
J equals 1 to n of the ZJ and
so the disease are a function or a
function of the X process and so so
they're also also good so any if you
have if you have an inner gothic any
ergodic process in you apply and you
apply a function of that then you also
get an ergodic process and you just
check that from the from the different
from the definition thanks Russ so
all right so sin so so this is something
it should be added since disease are a
function and an invariant function of
the of the original process they're also
an ergodic process so we get so we get
this inequality now we want to apply
that going back to our key inequality
upstairs here so so that inequality
tells us that maybe I'll also write that
expectation of z1 we know is that most
Alpha Epsilon
okay now let's write this inequality as
a in the form a 1 over n some J from 1
to N of XJ plus some J from 1 to N so
the ZJ is at least n minus well over N
times alpha
okay now now we want to take limb in for
both sides well here it just converges
to alpha what can we say about a about
the limit of this side well they on the
one hand it's bigger so so we take the
lemons and on the one hand it's bigger
than alpha from that inequality on the
other hand it's certainly well what can
we bound it above it's not true that the
length is bounded above by the sum of
the limbs but it's certainly true it's
bounded above by the limb in of the of
the axis plus the limb soup of disease
this is just true for any two sequences
when we add them within bound
like we can bound above the lemmings by
the lymph soup of one in the limit of
the other but right so this is limit of
a and of one-plus and this we know so
can be bounded above by Alpha Epsilon
this is greater than alpha and so we're
in the same situation we want it before
we move all types on to the other side
and so we get that the this so so this
limit is in fact greater than alpha so
this so it follows that this slimming
phase in fact greater than a bar so it's
equal to a bar and we have the
convergence and we've already verified
that what
I still have convergence to
all right so a bar okay so hmm so here I
guess I've explained why a bar is at
most is that most of the expectation why
is I didn't say why a bar is why a bar
is at least the expectation so this so
this is the final note so so so this is
not the fact that it's at most
expectation we've proved now in
in the bounded case if the axis were
bounded we certainly know that a a bar
would be equal to e x1 just from the
lebesgue bounded convergence theorem
because all the averages will be bounded
by the same bound and so and so in in
general a bar is bigger than the limit
of one over N some J from 1 to N of the
minimum of XJ with M which is right so
this is now a bounded process so this
gives us expectation of X 1 minimum M so
a bar is is bigger XJ minimum with a
large number n is what
right so again in the bounded case just
lebesgue bounded convergence theorem
gives you that the expectation of the
limit is limited expectations in the
unbounded case you just truncate you get
this inequality and now once we have
this inequality you can let M tend to
infinity and you get it a bar will also
be greater equality exactly X 1 just by
taking em to infinity ok so any
questions about this alright so I said
the are of the proofs this one is
particularly well adapted to
generalization to the severity of case
historically when the kingman was first
proved the serve additive ergodic
theorem in the 70's the proof was much
harder so i'll go on to that if there
are no questions on this case
and despite the names of active a gothic
theorem I'm going to prove the super
additive version so but it's just up to
negate taking minuses so we're going to
so so what's what's a super additive
process to me I mean so maybe first it's
just one word on the kind of places
where sub additive a gothic theorems get
applied so one place is when you in
first passage percolation you have say a
lattice and you want to and on the edges
you have random variables which indicate
passage times and you want to find T of
zero n is the time to go from 0 to n in
general T of MN might be the time to go
from m to n on the x axis so we have two
points you know em and and then on the x
axis we look at all possible paths that
go from one to the other for each path
we look at the total passage time of the
path and we minimize over all these
paths so on the edges are endowed with
independent random variables which are
you know passage time with these edges
so so these random variables it's is
easy to see that there's a relative so
the time to go from 0 to n is certainly
bounded by the time to go from 0 to M
plus the time to go from m to n because
here we're considering a larger ensemble
of paths that go from 0 to n here we're
considering passed it on the right hand
side we're considering paths that go
from 0 to M but have to go via the
intermediate point M so here we're
minimizing over a larger ensemble of
paths so this is the kind of quality and
then you want to show that when you take
the time to go from 0 to n you divide by
n this actually has a limit
and the limit is non-random so it's
almost really constant and it's equal to
the limit of the expectations this is
one kind of application another
application is for random walks on
groups and you want to show that the
random walk has a speed so you have some
Cayley graph you're doing a random walk
in the calligraph and you look at the
distance from your starting point to the
end point in your walk and again you can
check that that satisfies such an
inequality and so get existence of of a
limit so I won't talk now about more
applications but rather go to the formal
statement since I want to finish in time
so the sources of additive Ghatak
theorem of Kingman and I'm going to
state it in super additive version so
again we can think of some underlying
probability space and so if some so in
this case the probability space is just
all the edges on all these the random
variables that indicate the passage
times of the edges and you can think of
a transformation T from Omega to itself
which is measure preserving and and then
the important things are random
variables y MN so you can think of these
as say the negative of these passage
times okay and why I mean they satisfy
the sub additive inequality so just
write y 0 n is that is I'm sorry the
super additive so this is bigger than y
0 M plus y
so so that's one assumption and also the
shifting variance in distribution so why
I'm in composed with T is why M + 1 + +
4 so you see in this situation the
transformation is just shifting the
random variables and and you see that
the time to go from n plus 1 to n plus 1
it just has the same laws the time to go
from M to n it just corresponds to the
shifted passage times so this these are
the assumptions and then the conclusion
is that if you take what
so no this is this is not equality in
distribution this is this is the actual
random variables this measure preserving
that's okay so you should you only think
there's really just one sequence of just
like in the ergodic theorem yeah so so
here you should okay so there you think
of their basic variables as these is ymn
this or y 0n and then from y 0 n you can
end the transformation you have all the
variables say but the transformation
just shifts the underlying space so then
but then the conclusion is that there
exists the limit of y0 in this limit
exists almost surely now beta is some
number it's not in this case it's not
minus infinity but it could well be
infinity okay the number is why these
are finite numbers but I didn't assume
integrability here so certainly averages
could go to infinity okay
and also if beta is also the limit of
the expectations
thank you everybody
okay so
the proof is you see it follows the same
same lines as before so so we're going
to so first if you look at the variables
y tilde MN which are Y and then minus
the Sun Y K minus 1 K came from M plus 1
to n then you could just see that the
super additivity assumption implies that
this is no negative okay so leave that
as a little verification you just
recursively apply this assumption again
again and remember this assumption
together with the one on the right
implies that we have the super activity
along any interval if you take the
interval and break it into pieces
y of the big interval is bigger than
wise of the the the sum of the Y's of
the pieces right and and you just keep
breaking it up until you get to to this
Y on intervals of length one and so so
this is non negative and
okay so and and this partial sum can be
treated with the with the ergodic
theorem so okay if so okay there's if
this if these variables have infinite
expectation then you easily conclude
that every that the limit is infinite if
they have a finite expectation then you
can just use the ergodic theorem that
tells you that the averages of these
will go to their mean and you just
reduce the case of why I'm into the case
of Y tilde so because of such a
definition in so this allows us to
assume that the original wireman are
non-negative
yes so
right so let's
yes that's right okay so
okay so let's forcibly if you assume
that these Y's are assuming that these
are finite okay so thanks so then all
right so this allows us to assume that
the wireman are non-negative so now we
just continue with no negative variables
and define as before so a so guess now
call it beta the limp soup
of the one over n y0m
okay we fix alpha less than beta and
define before l of k is the first first
l so that when we take y from k to k
plus L this is bigger than L alpha and L
star of K so now L star K will be L of K
if L of K is less than L and one of L of
K is bigger than n okay now the same
logic that we've used already twice
before will allow us to bound from below
y0 n okay by something n minus
essentially entering cell times alpha
I'll write it in an extra - the Sun in
overall
so I didn't tell you how we choose L but
you can already guess already done month
so in the sum over all K so that L of K
is
okay and here we choose lb as before we
choose L so that the probability of L of
K to be bigger than L is less than
Epsilon
and then we so this L star K we don't
really use them they just kind of remind
us of the argument to get this
inequality to get this inequality we
take the as before we take the interval
zero N and we start we look from zero we
look do we have an interval here where L
of K is less than L if so we're happy
and then we continue but maybe here when
we look here the L of K is bigger than L
then we just take a single ton and we go
to the next point okay so but this
single stone was a special point where L
of K was bigger than L then we go to the
next point and so we overall we cover
the whole interval from n to n my from 0
to n minus L by good intervals where L
of K is less than L and bad single terms
so when we had when L of K is bigger
than L we just take that single term as
and and jump to the next so overall from
the good intervals will get n minus L
times their times their length but we're
going to lose and here yeah so I have
we're going to lose from these so we
have this also are the alpha multiplies
this as well when you're write it this
way
n minus L minus the sum all of this
multiplies alpha right so the total
length of the good intervals is at least
n minus L minus the sum of the bad
locations the number of the bad
locations in the bad locations we had to
go to the next point okay this this is
some of the key to this proof but it's
similar to the keys to the previous
proof that's why we went through that
maybe but you see it's not it's not not
it might it kind of does give some
negative information but this is not a
right not least here I'm not computing
probabilities this is just a
combinatorial point-wise inequality says
we gain alpha times the length of the
good intervals and we lose I mean but
what is the total length of the good
intervals it's at least n minus L minus
the number of the bad Singleton's every
time we see a bad singleton we go to the
next maybe that's another bad singleton
but then it will just enter into this
sum so the total length of the good
intervals is at least what's written in
the parentheses here and then from them
we get this times alpha okay
yeah essentially essentially the same
what is different here is that we didn't
do actually a modification of the
process for this we just kind of paid
the price here but we're in a better
position than before because we already
have their gothic serum and we for the
burqa forgot a theme and we're about to
use it okay so that's why we didn't have
to go through the same thing because now
when we divide by n and take and we want
to take a lemon you see these are now
these are just these indicators are just
in a sequence of station because
everything is a function of a stationary
of a stationary process these indicators
project they're also stationary so if we
divide by n and take a limit we know
they converge to the expectation of this
indicator so and that expectation is
just the probability of this event which
is smallest less than epsilon so so thus
if we take divided by n and take le
means
what do we get well here when L is
constant so when we divide by n we're
going to get at least alpha - well alpha
times 1 minus the probability of L say
of 1 bigger than 1 which is bigger than
L of one bigger than L so this is bigger
than alpha times 1 minus Epsilon okay
and at this point we use the work of
ergodic theorem for these random
variables which again because we're in
the stationary situation these
themselves from a stationary sequence so
we can apply the Berghof ergodic theorem
here and now we're done the limit here
is greater than alpha and this was true
for any alpha less than beta so the
limit equals the value so so the last
comment is why is the limit the same as
the limit of the expectations so you
always have right so so we we already
proved so the limit of y 0 n over n
exists and equals beta so so then from
from fatu if we take expectation we get
that the expectation of the limit which
is beta is is that most the limits of
the expectations
but this is a super additive numerical
sequence so the limit exists so so the
limit exists and and so beta is at most
this limit in the other direction just
observe that if you take Y zero and say
K times n you can break this up into n
intervals of length K right so see if
you take this and divide by n and take a
limit this will be bigger than the
expectation of Y zero pain
okay because just from the severity of
inequality you can or the super additive
inequality you can break this up into a
sum of n summons each on intervals of
length K and and then for these summons
you apply the ordinary ergodic theorem
and to get that when you divide these
this is sum of n summons when you divide
by n you'll get this limit this is true
for every case so now let's take this so
here I thought of n is tending to
infinity and K is constant so again
right so I have this and this is true
this is this is exactly our limit beta
and it's bigger than this for every K so
taking the limit gives us the remaining
inequality we need okay since promised
to finish at 5 I won't really discuss
more applications now but let me stop
here and wait for any questions yes we
had some in this inequality here right
what did we do we take the interval 0 n
and we broke it into good intervals and
bad Singleton's
right now in the bad Singleton's all I
said is well you know we don't get the
good contribution but we know we get
something at least zero so that's why I
could write this inequality so this gave
us what is in the parenthesis is just
the total length of the good intervals
and those all give us alpha times their
length the other things I don't know but
it's non-negative
but also the last year that's right yes
because we stopped before the end and
okay so that's where where that gets
used not the first proof the first proof
along this argument so the King man who
was classical so this is so so this this
line of proof started with a
non-standard analysis proved by the Toro
Khmer in 1982 and then this was you know
so if he cuts me also in venture wise
read that proof understood it and
understood how to remove the
non-standard analysis but they still but
it and then this was further simplified
a bit by my keen and Mike Steele who
gave these are essentially these
arguments for the Gothic for the birkoff
case and a sub additive got the case you
want to compare two other proofs you
could so say direct probability book has
a proof for the severity of ergodic
theorem slightly more general version
but this one applies to most
applications this is the what I pro two
is basically the original kingman
version and but the proof that the red
gives which follows Liggett is really
much harder to follow and and to
remember so here I think at least the
idea is pretty easy to remember the
proof if you assume very secure I mean
but you prove a completely different
statements so it's if you want to verify
that that statement is actually
equivalent it's much longer
yes sir this is a way to avoid maximal
theorem so so some there is a very short
proof of the work of ergodic theorem
that comes from the maximal noble
experiments for and this was one of the
roots of the original proofs and
initially it was thought oh this
reductions so easy so the maximal
ergodic theorem must be hard but then
Garcia came up with a very short proof
of the maximal ergodic theorem so if you
combine those there is a very you know
there is a proof even shorter than the
one I presented going via maximal
ergodic theory but that one Latin is
more mysterious I'd say even even to the
experts
right right that's not that one that one
doesn't translate directly to the
Serenity okay thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>