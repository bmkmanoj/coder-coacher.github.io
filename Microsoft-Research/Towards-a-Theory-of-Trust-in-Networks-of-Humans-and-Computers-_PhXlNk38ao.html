<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards a Theory of Trust in Networks of Humans and Computers | Coder Coacher - Coaching Coders</title><meta content="Towards a Theory of Trust in Networks of Humans and Computers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards a Theory of Trust in Networks of Humans and Computers</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_PhXlNk38ao" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to first thank Microsoft
Research nan Chi University and qian jin
university for hosting this event so
thank you it's a real honor to be here
in this beautiful city of tengen or with
lovely weather today so i really have
and looking forward to giving rice talk
to all these eager energetic young
students in the audience to share my new
ideas and new questions i'm going to
talk about a theory of trust towards a
theory of trust in networks of humans
and computers and the talk i'm going to
give is going to ask more questions than
answer questions and this is I think
especially appropriate for this audience
it's going to point to new fundamental
questions insecurities that I think
challenge all of society today so let me
start I'm motivated by actually a
problem that men while blom a Turing
award winner asked a couple of years ago
we have to recognize especially
appropriate in the theme of the 21st
century computing computing naturally we
have to recognize that the computer of
today is very different from the
computer of yesterday the computer of
today actually is made up of humans and
machines we have now networks of humans
and computers that are solving problems
solving problems that neither can solve
alone so this is the computer of today
and one could ask some deep scientific
questions in this context what is the
what is computable by such a huge of
network of humans and computers but I'm
going to focus on a different question
and this question is how can i as a
human being
trust the information that I read over
the Internet a very common situation is
we go to our machine we look something
up in Wikipedia and we read what is
displayed why should we trust what that
Wikipedia page says that's the question
I'm asking and in trying to disentangle
this question we will see in my talk
many questions that are unanswered many
problems in computer science that we
still have to answer but probably the
most important part of my talk is not is
that it's not just about computer
science the most important part of my
talk is that to answer this question we
have to bring in the social behavioral
and economic sciences to understand a
broader theory of trust in this context
so the insight is that computational
trust defines trust relations among
devices computers and networks
behavioral trust defines trust relations
among people and organizations and a
theory of trust for networks of humans
and commuters needs to include elements
from both so let me decompose the
question how can I as human trust the
information I read over the Internet let
me associate messages that I receive
over the Internet as information and let
me associate receiving messages as what
I read over the Internet and so we're in
a more familiar territory now when we're
talking about receiving messages and now
let me decompose the question into two
questions one is is the communication
channel over which I receive messages
secure the second is
how can I trust the sender of the
messages I receive or another way to put
this is how can I trust the person who
wrote that Wikipedia page why should I
trust the site that hosts that page and
so on let me start with a simple
communications model because this will
let me get rid of the first question and
put it aside so we have a sender and a
receiver sender will always be on your
right and the receiver will but the will
be Bob and he'll be on the left we're
going to assume that we have secure
private available channels we're going
to assume that we have the computer
science technology and the computer
science science to discharge this
assumption we're going to assume we have
a trusted pass between the application
or the user and her machine and a
trusted path between the receiver and
his machine and we're also going to
assume that between the computering
devices that are communicating with each
other that there are penetration
resistant interfaces this means that I
basically have secure communication and
so I can gray out the first question of
is the communication channel over which
I receive messages secure because I'm
going to assume that I can implement all
those properties i just mentioned on my
previous slide that leaves the second
question how can I trust the center of
the messages I receive so that's what
I'm going to focus this talk on and
we'll see that this question the main
question boils down to the act of
trusting the sender where you should
think about the sender as some person or
some organization
okay why let's let's say I'm the
receiver um and why don't you even want
to ask this question the reason is there
must be some value for me as a receiver
to want to interact with some sender I
must be gaining something from this
interaction and be willing to risk some
costs for having this interaction
otherwise I can just not interact at all
with anyone and then I don't have to
trust anyone so there must be some value
gained by the receiver in interacting
with the sender and this value must
outweigh the costs so that's what this
slide is supposed to indicate the value
underline the act of trusting the sender
if the receiver trust the sender and the
sender is trustworthy then values gain
for both so I gain information and the
sender monetizes on my clique if the
receiver trust the sender and the sender
is untrustworthy then the value gained
must outweigh the cost to engage so the
receiver risks getting now where for
instance but I take that risk because I
think I'm going to get some good
information and finally if the receiver
suspects the sender is untrustworthy
that I want to engage and then no value
is exchanged so these are the ways to
think about the axioms underlying this
theory of trust so let's first start
talking about computational trust how do
we build computation how do we build
trust in the computational elements in
in our system and then I'm going to talk
about behavioral trust because we'll see
that even if we could do the best job
possible for ensuring computational
trust we still need to rely on something
else to trust
the sender so the elements of
computational trust our isolation
correctness and recovery these are all
familiar kinds of properties that we
study as computer scientists and these
isolation is when a receiver could
isolate himself from any sender so
regardless of what the center is or who
the sender is or what message is
received we'd like to achieve that
because if you can achieve that then you
can trust anyone and you can trust any
input but we'll see it's hard to achieve
correctness means independent
verification of the code that the sender
might send some somehow parts of certain
messages might be code and we actually
want to want to verify properties that
the code and we know computer science
ways of doing that verification and the
third is recovery where it may be that
we receive a message and we get into a
bad state and we have some way of
recovering back into a good state and
again in computer science we have
techniques to do fault recovery and so
on so we'll see that all of these
elements are necessary for providing
computational trust but they're not
sufficient so let's just look at the
first one isolation from sender so we
have a we have one way to possibly
isolate ourselves from the sender is
verify every any input message that we
might receive and I said if we can
verify any input message that we can
receive in terms of the properties in
the behavior of the the code that might
be in there then trust of the sender is
not needed however let's look at how
possible is isolation from any sender
can input always be verified well the
input might be anything from a piece of
text or two piece of code so no
arbitrary code cannot be verified I
might ask the prop
for instance does this piece of code
that I just received halt and we know we
can't do that we can answer that suppose
the input can be verified is
verification always efficient well no it
might be that the the message is a
solution to some co NP problem and we
know that verifying the solution will be
inefficient suppose input the input is
we can verify it efficiently is it
always practical and again there are a
recent results on by par no actually an
ACM dissertation award winner a from
Carnegie Mellon who is now at Microsoft
Research who shows that it may be
possible to for instance send a piece of
encrypted code over to a server say the
Microsoft Bing that might say do a
search over a large database of pages
and send back a result on that the
receiver can check really quickly so we
know we can do this however to do it is
not quite practical yet because these
kinds of techniques rely on a fully
homomorphic encryption which are that
technique is not quite practical yet but
it gives you a hint at what might come
in the future ok suppose input
verification is efficient its practical
and is it always scalable even hear the
answer is unfortunately no and we have
security models from decades ago the mid
and late 1970s that have come up with
ways for us to define how we can check
certain conditions on input messages
with respect to a particular security
model the problem is in the Internet
today not all organizations and entities
and individuals buy into a single
security model so not one security model
fits all
okay so now we're in the situation
isolation from the sender is hard
suppose the sender can provide evidence
of trustworthiness so now we can't when
the receiver really needs to interact
with the sender and the sender is going
to promise to send over not just the
message but some evidence that what
isn't that message it should be trusted
by the receiver so this is a situation
and here the most the hardest example is
when the message contains a piece of
code and so we might want to ask about
how to verify that piece of code with
respect to certain properties and the
situation is if we have complete sender
trustworthiness no isolation is needed
and the input can always be accepted and
you'll see that we still can't do this
so for instance is it practical not
usually Co correctness proofs are not
scalable their first often limited to
small configurations or limited to a few
properties um so I might for instance
pass a piece of evidence would be a
would be a proof that the code is
correct with respect to the
specification so that I can as a
receiver check the proof but this kind
of fruit carrying code is often
restricted to certain kinds of
properties another problem with scaling
this technique is that techniques we use
to do the correctness proofs themselves
are often human intensive they often
require interactive seema improving so
the real problem but even if we could do
full verification of code and and
provide a proof for evidence that that
code is correct is that that code might
still have some input parameters uninst
an she ate it so some where someone or
something is going to instantiate those
input parameters so we still have some
reliance on that someone or something
so how do we do it today we rely on
reputation services third-party
recommendations outsourcing trust
relations to get a sense of whether the
human is trustworthy or not and I think
this should give you a hint as what
Scott's to what is going to come in
terms of the behavioral trust we need so
providing complete or irrefutable
evidence that the sender is trustworthy
is hard suppose a receiver can detect
and recover from ascenders
untrustworthiness okay so in this case
we get an input message we get ourselves
into a bad state and we figure out we
detect that we're in a bad state and
then we need to recover from this to get
ourselves into a good state again is
this feasible practical and scalable by
the way all of these seemingly
challenges are great questions research
questions for you to be thinking about
so the answer is no not usually it might
be possible in certain situations for
instance a lot of the work and
fault-tolerant distributed transactional
databases have figured out how to do
this that's why we can go to an ATM
machine on and do transactions online
however there are situations where it is
impossible to undo the effects on the
standard example is in fact when you go
to the ATM machine you were to draw
money and then you can't undo that
you're hardly going to give the money
back so that's an example of i/o where
those effects cannot be undone okay so
we're in a situation where we can't have
isolation from the sender we don't have
sufficient ways to provide trust worthy
on this evidence we can't always recover
from that input
well suppose when we do get that input
we punish the sender so here what we
need is if if we want to have deterrence
we need punishment and for punishment we
need accountability so what that means
is if i do want to punish the center
sender i need to know who to punish um
and uh this is well known this sort of
implication the necessity chain but we
also need sufficient punishment to deter
and sufficient accountability to punish
so this is actually a new idea okay is
deterrence always practical and scalable
and here no not always what deters human
behavior or what deters human
misbehavior is an outstanding question
that has been debated by the legal
community for centuries so let me just
summarize where we are in terms of the
act of trusting from a computational
point of view we don't have this
isolation from center we don't have a
way to provide complete irrefutable
trustworthy this evidence we can't
always recover from that input we don't
have a complete of ways of deterring
from sending people deterring people
from sending that input so but this is
of course the Internet of today so is
there ever a way that we can ever be
safe to trust the sender and this is
where the new idea of combining elements
from behavioral trust from the social
behavioral and economics alliances come
in so let me just tell you right now in
terms of the theory trust so far in
terms of the computational elements
we've got cryptography verification
fault-tolerant computing all of these
have been studied for many years to
provide elements of a computational
trust to give us secure communication
waste to do isolation a ways to prove
programs correct provide sender
trustworthiness and do recovery but we
need more we need to define trust among
humans and so what we're going to do is
look to the social sciences where they
have studied this question for many many
decades the actor trusting what should
the act of trusting mean and hundreds of
research articles have been published on
this very topic but not in computer
science in economics and social science
so it is we should as computer
scientists be looking outside of just
what we understand about security and
computing and looking to what these
other sciences have developed on in
terms of theories of trust and so that's
what I'm going to explain in the next
few slides there's a notion of
behavioral trust specifically that i'm
going to give an example of and this
notion of behavioral trust says
axiomatically that there are beliefs and
preferences and that's all there is and
that's quite a powerful statement so
let's just look at a single model for
behavioral trust I'm taking this from a
paper by economic scientist fair the
center is now going to be I'm going to
call that sender a trustee so it could
be the bank ebay Google Amazon Microsoft
some third party that you trust you
trust the bank with your money you trust
a Google with all your information and
so
on the receivers the trust door so I'm
the one say bidding on an auction on the
one consuming some information on the
customer of a bank and the model is
going to be based on a one-shot game and
so what I'm going to do in the next
slide is explain what a one-shot game is
so there's a dealer and there's a sender
and a receiver and the dealer gives ten
dollars each a to to the sender and ten
dollars to the receiver and the game is
as I as a receiver I get this ten
dollars and the rule is if I give my
sent my ten dollars over to the sender
the dealer will multiply by ten dollars
by four so that means the sender would
receive forty dollars for me in case one
if there's if we cooperate there's a
win-win situation so if we cooperate I
send my ten dollars over the dealer
multiplies my ten dollars the receiver
the center gets forty dollars and his
responsibility is to take all the money
he gets from me and from the dealer and
divide by two and send me the other half
so in this case the sender gets ten
dollars from the dealer forty dollars to
me that's fifty dollars divided by two
that's $25 and the sender if in the case
of cooperation sends me back twenty-five
dollars so i sent the sender ten dollars
and i got back 25 so as a receiver i get
twenty-five dollars gaining fifteen
dollars and the sender keeps $25 also
gaining fifteen dollars over
or what the dealer sent so this is the
win-win situation now of course there
are other cases so for instance I could
send my ten dollars over and the sender
might decide to keep my ten dollars and
I lose so I get zero dollars as this
receiver I lose ten dollars and
meanwhile the sender keeps fifty dollars
so the sender gains forty dollars that's
not really nice but that's case to case
three is I might decide I don't want to
play this game so i get my ten dollars
and I'm not sending over any money and
the sender guess is ten dollars and
nothing happens in this case by my not
sending over ten dollars to the sender
there's a part of me that says I don't
trust that sundar and so no trust no
value gained and the same thing is true
on sender side no trust no value gained
so this is an example of a one-shot game
and these games are used in spades in
economics and social science to try to
understand human behavior to try to
incentivize certain behavior so this is
just a very simple example to tease you
into thinking about what trust might
mean um there's let me before I go on to
the next part of the experiment let me
just show that the possible value
outcomes of this game are analogous to
what I talked about before in terms of
senders and receive it receive receivers
gaining value in interacting with each
other so in the game example of the
trust or trust the trustee and the
trustee is trustworthy than the trust
so you are better off because before
executing the protocol cooperation pays
off if the trustor trust is trustee and
the trustees untrustworthy than the
trustee is better off in the trust or is
worse off that is the trustee has strong
incentive to cheat in the absence of a
mechanism that protects the trust or and
then of course the third case was if the
trust door suspects the trustee will
chief then he's not going to engage and
thus no value is exchanged and this is
completely analogous to what I said
earlier in terms of the sender and
receiver interacting in networks okay so
in fact this one shot game is also has
also been used as the basis of some
human experimentation to understand to
tease out some of these elements of
behavioral trust I already mentioned the
22 main ideas are beliefs and
preferences but in fact there are two
different kinds of preferences and this
game that I'm going to explain this
experiment I'm going to explain I
teases out those two kinds of
preferences so the continuation of the
game is the dealer has 20 punishment
units that he gives the receiver and the
and there there's three different cases
of how the receiver might punish the
Thunder so in the first case it costs
the receiver to punish the sender so if
I find out that the sender cheated on me
then if I want to punish the sender it
cost me one unit one punishment unit and
the sender then is punished by two so I
it costs me something to punish the
sender but the sender is doubly punished
more than me that's the first case so
that's the cost model in the second case
it's free for me to punish it doesn't
cost me anything um and the sender is
still punished by two units and in the
third case it doesn't cost me anything
it's just symbolic that the sender is
punished so that doesn't cost me
anything and the receiver is not
punished by any dollar amount either and
it turns out that this is human behavior
now that we're measuring most receivers
pay the dealer to punish cheating
senders so first of all this experiment
had 15 subjects so one subject didn't
even want to play the game of the 14
subjects 12 on what was willing 12
subjects were willing to pay to punish
the Sun the descender in the cost model
but everyone all of them were were what
it was free to punish wanted to punish
the sender and then in the symbolic case
not so many receivers felt like they
really needed or wanted to punish the
sender so now let me tell you about on
yet another case that was part of this
experiment and this will make a
distinction between two kinds of
preferences what's called risk
preferences and a behavioral but
betrayal aversion so in fact in this
experiment the subjects the human
subjects were monitored their brains
were actually monitored so PET scans
were taking of their brains as they were
playing this game and it turned out that
that there's certain parts of your brain
that are known to be associated with
reward satisfaction so it turns out that
the PET scan of the receivers brain
showed reward satisfaction when the
the sender when the receiver punished
the sender so there this is called
betrayal aversion at the aversion to
being scammed or cheated and the reason
that in this experiment and in this
theory of trust for humans the reason
that that people in the social science
and economic sciences make studied this
was to be able to make a distinction
between betrayal aversion and risk
aversion and so it it the fourth
experiment that was run was when the
sender was not a human the center was a
computer and the dealer would tell the
receiver oh well you didn't get your
money back but it was just a random
process that I kind of randomly decided
whether he would send your money back
and so in this case the receiver didn't
feel like I mean punish a computer it
didn't it didn't it receiver didn't feel
like he really needed to punish a
machine as opposed to punish a human
being who cheated on him and so the in
this fourth case of the experiment there
there was no board there was very little
desire to punish on and the snow little
lower reward satisfaction interestingly
this what is your brain is actually can
be chemically altered to control
betrayal aversion so there again this is
very interesting to me when I read the
literature that the social science is
actually
run these experimentals experiments on
humans and do try to chemically alter
the brain to measure the degree of
betrayal aversion and I again this is
very distinct from risk aversion so let
me just summarize the experiment's
results the trust her or the receiver is
willing to incur a cost to punish and
the amount of punishment inflicted was
higher when the punishment was free the
trustor or the receiver derived
satisfaction felt rewarded proportional
to the amount of punishment inflicted on
cheat on the cheating sender and finally
when the trustor the sender is replaced
by a random device like a computer then
the receivers desire to punish was
negligible so what we have is a theory
of trust for humans based on beliefs and
preferences and there are two kinds of
preferences risk preferences and social
preferences so in terms of beliefs and
trustworthiness this is usually measured
in terms of probabilistically and that's
just the probability that I'm going to
play the game so 1 out of 15 people
didn't play the game in terms of risk
preferences this is really um noting
that when the machine was a sender I it
didn't the degree of risk aversion was
was low and then in terms of sort of
betrayal aversion or social preferences
if the cheater was human and of with the
sender was a cheater and a human then
the receiver really felt like punishing
okay so I'm going to wrap up what i'm
really trying to argue is there's a new
way of thinking about trust in this
world of networks of humans and
computers and it doesn't it's not
sufficient to look at just computer
science to understand
this notion of trust we need to bring in
we need to look to other sciences that
have been studying trust among humans
and organizations for decades so we have
a notion of beliefs and centers
trustworthiness be preferences and
aversions that are either risk or
betrayal we have counterparts to this in
the computational world the interesting
question for us now if we understand
this theory of trust on about humans is
how do we bring this into the
computational realm how do we invent
computational infrastructure a
computational mechanisms to actually
support on these theories of that
include beliefs risks a risk aversion
and betrayal aversion so that's really
the punch line we already have some hint
that some of techniques that we have
from proven code correct will help
provide trust worthy evidence that might
have some something to do with beliefs
in the sender's trustworthiness with
respect to risk this is well if
something bad happens you try to recover
from what's bad so maybe we can recover
from sender non-compliance but the
really new idea to both the social
science community and the computer
science community with respect to this
question of security and Trust is what
is out there in terms of computational
mechanisms and computer infrastructure
to give us the support for detecting
betrayal and punishing the sender and
even or deterring the sender from
betraying or figuring out accountability
so that we would even know who to punish
and so we have in practice ways that are
practical enough for some aspects of
correctness some acts the aspects of
recovery but we don't
of at least from computer science a
serious way of thinking about betrayal
aversion we have things like third party
recommendations services and reputation
services those are kind of like those
are ad hoc mechanisms and what I'm
really seeking are more systematic ways
of thinking about how to provide a
theory of trust that includes betrayal
aversion so this is really what I'm
trying to state in terms of people
interested in doing research and
security in the past we built new trust
relations derived from old we had
trusted third parties transitive trust
relations delegation today or in the
future where we're going to need to
build our new security infrastructures
that promote new trust relations that
are possibly unmediated there's no end
to end argument and these are akin to
today's reputation services and
recommendation systems and so there's an
analogy to transportation which is even
a bigger point that I'm trying to make
and that is there's a new way that we
should be approaching security research
it's not so much there are bad guys and
good guys and the good guys have to
always keep up with the the bad guys
it's it's not so black and white there's
an economics an ecosystem in this
network of computers and humans that we
have to be understanding and so security
can bring us new mechanisms that can
bring new economic value to interacting
with each other over the Internet so the
goal is to seek security mechanisms that
create new value not just prevent loss
and that's the close of my talk thank
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>