<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Approximating the Expansion Profile and Almost Optimal Local Graph Clustering | Coder Coacher - Coaching Coders</title><meta content="Approximating the Expansion Profile and Almost Optimal Local Graph Clustering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Approximating the Expansion Profile and Almost Optimal Local Graph Clustering</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sZBhvecPf9k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
ok so we delighted to have Cheyenne tell
us about approximating the expansion
profile hello everybody so today I'm
going to talk about the expansion
profile ethic how to approximate in
expansion profile at the graph and local
graph clustering algorithms this is a
joint work with local service on so let
me I'm going to start with motivating
the problem and then I'll talk about the
results so I will today I will talk
about a clustering problem so suppose we
are given an undirected graph G that may
for example I present friendships in a
social network and they want to
partition it into good clusters ok so
try to talk i'm going to assume that the
graph is d regular but the results will
generalize to non regular graphs and ok
so i'm going to define it rigorously but
by good partitioning by good clustering
we mean a good cluster is a said where
it is highly connected inside and
loosely connected outs so let me let me
now define how we're going to measure
the quality of a cluster we're going to
use expansion so for a set s the
expansion of s is defined as the ratio
of the number of edges leaving us over
the summation of the degree of vertices
in s since we assume the graph is the
regular the denominator is just d times
s so so for example here in this picture
the expansion of this set s is one-fifth
so the expansion is always between 0 and
1 and if a set has a smaller expansion
means that it's a better cluster ok if
we wanna see me think of the expansion
as the probability that a random walk is
started at a uniformly random vertex of
s
leaves it in bonus tip all right so so
it's just clear this is we're gonna use
these definitions throughout the talk
the expansion of the graph is also
defined as the as the worst expansion of
any set in the craft this is also called
Esparza Scott of the graph ok so the
smallest cut problem has been very well
studied and there are different out
different variants algorithms but let me
talk about the trigger type inequalities
which would be of interest to us today
so sugar type inequalities and
characterized as far as a Scott of a
graph in terms of the second eigenvalue
of the normalized adjacency matrix so
they Solomon amendment and I'll improve
that Phi of G for any graph is between
one half of 1 minus lambda 2 and its
utmost root 2 1 minus lambda 2 so in
fact this proof is constructive so we
can get a get a set of this expansion so
this means that you can you can obtain a
order of one of the root for
approximation to as far as the Scott of
the graph so so why this is important
because this guarantee is independent of
the size of the graph okay nowadays
since we have to you know deal with
massive grass it would be very good if
we can have approximation algorithms
that are independent that apart with the
approximation factor that are
independent of the size of the graph
right and of course its algorithm the
rounding algorithm is very simple it can
be implemented in nearly linear time so
all you need to do is that you need to
compute the eigenvector the second
eigenvector of the normalized adjacency
matrix then embed the graph on a line
based on the value of the vertices in
this eigenvector sweep this line from
left to the right and find the best cut
okay so so in practice people you know
use this algorithm to kind of to
partition the graph and so so they use
it kind of recursively the first they
find a sort of as far as a Scott
unfortunate as part of Scott divides the
graphing to two then
is recursively on each of the two sides
okay until the graph is partitioned into
you know a good cluster but this may not
do well in practice because of these two
things the first thing is that although
this algorithm is nearly linear time in
may you may end up to spend quadratic
amount of time for running it because
you know that there is no guarantee on
the size of the output set it could be
that after running linear time you may
just end up separating three or four
vertices from the rest of the graph okay
so you may have to run this algorithm
order of n times to partition the crap
and on the other hand it could be that
many of the small communities are
misclassified by this algorithm because
you know for example once you do this
you separate this giant component may
end up misclassifying a small
communities because you know no matter
which side of the cut they choose it
doesn't change the value of this the cut
that you're selecting that much okay so
so in order to overcome with this
deficiencies stimulant thing suggest
that the following interesting problem
is called the local graph clustering
problem so suppose we are given a vertex
U of interest in a massive graph and we
want to find a non expanding cluster
around you in time proportional to the
cluster size okay so for example think
about the the facebook graph the whole
massive graph and let's say you want to
find for example the cs theory as a
cluster in this trap and we are given to
us for example the facebook page of you
all okay so all we can do is that we can
go to the Facebook page of you or I
don't know be fast and we can look at
his friends and then you know we can go
to the Facebook page of his friends and
do this again and again but we are not
allowed to jump in this network so
basically the question is how should we
explore the neighborhood of you to find
this
trust and before talking about the
results note that if we can solve this
problem we kind of you know get rid of
this deficiencies so in fact we can we
can do the clustering of a graph by you
know running this algorithm over and
over and we know that every time that
the amount of time that we spend is
proportional to the size of the cluster
that we that we are gay and we're going
to get so the running time would remain
linear and on the other hand we know
that that if we start from some small
community then hopefully we're going to
find something very close to it okay so
so still mounting answer this question
kind of and the prove the following nice
guarantees so i should also mention that
they use this algorithm in their linear
time nearly linear time laplacian store
so they prove the following for any
target set a if we are given a random
vertex you inside a then the algorithm
can find will find a set s mostly in a
the following guarantees the expansion
of s is at most screw tough i like
yougov n and the running time is order
of the size of the output 1 over 5
squared poly log so we call this one or
five square poly log n at the board pay
more per volume ratio this is basically
the ratio of the running time of the
algorithm and the size of the output so
ideally if this if this ratio is
constant then by applying this algorithm
we we can can we can have a nearly
linear time partitioning of a crash
twice 5855
is 196 yeah mostly in a is you know cuz
almost 1 minus epsilon for some constant
epsilon alright and also this algorithm
is randomized so they can do this with
some constant probability in fact they
cannot prove that for every vertex in a
this is true they can prove that for you
half of the vertices in a they can do
this so this result has been improved so
Anderson sugar length improved the
approximation guarantee to root pi log
in and the worker volume ratio to 1 over
5 Polly Logan and more recently
understand and prayers improve the work
per volume ratio to 1 over root for poly
log ok all of these results like the
underlying idea for all of these results
is the is to run a random walk in the
net from you so basically they they
searched the neighborhood of you by
running an analog but they use very
different techniques as you can see here
like a simultaneous the truncated random
walks and their social nalang use the
approximate PageRank vectors and NSN and
prayers use evolving set so today i'll
talk about evolving set pasts and we
will use that today so ok so what do i
want to show you from this table the
thing is if you look at the
approximation guarantee of all these
algorithms unlike the chigger inequality
here there is a dependence the
undersides of the graph ok so it has
been an open problem if if one can find
you know a local graph clustering
algorithm without any dependents the
undersides of the graph and that's the
essential to give you a local variant of
the sugar inequality because it would
kind of give you the same guarantee as a
sugar inequality with keeping ok so this
is the basic distant object of this
start and kind of solve this problem so
so we prove the following prove that for
any targets at a if you are given a
random vertex you in a then we can find
a set s such that
expansion of S is at most exclusive
expansion of a over epsilon and the word
pair volume ratio is at most a to the
epsilon 1 over root for poly lager okay
so so this indeed kind of provides a
local variant of the sugar inequality
because both the approximation guarantee
and the running time is independent of
the size of a graph it just depends on
the size of the cluster that that you're
trying to find ya learning times they up
to this Foley Logan yeah I do so one
more thing is that this algorithm
generalizes that the previous result in
a sense that here you can choose epsilon
whatever you want so if you choose
epsilon to be 1 over log of a you can
replicate the previous result so in fact
there is a trade-off between the running
time and and the approximation guarantee
by choosing a larger values of epsilon
you will obtain a better approximation
guarantee with the water running time
what am i choosing smaller value of
epsilon you would get a better running
time and a voiced approximation
guarantee ok many question
but so before talking about the proofs
or the algorithm I want to talk about an
implication of this result which is
interesting in the theory community so
the so there's a close connection
between this problem and the small said
expansion problem so let me first define
the expansion profile of a graph for for
any foot it will be a parameter between
zero and in halves then Phi of MU is
defined as the most expansion of any set
of size at most mu so for example five
and a half is just the as far as the
Scott of the graph ok so it's math
expansion problem asks if you can find
approximation algorithm for for this
parameter Phi of MU which is independent
of the size of a graph ok so by cheer
inequality we know that we can do this
for four new being in half so basically
here the question is if you can do that
for any values of mu mu which is some
constant exercise
for any constant because we're depends
on the size I'm not sure no food for
every meal it cannot depend on miu miu
it was yeah I give you a mule in the
input you give me an approximation
algorithm yeah it could depend on n it
doesn't matter but that you may depend
on the size of G but I want the
approximation guarantee to not depend on
size of G so that fast efficient
guarantee cannot depend on mu for
example it may depend on fire from you
but may not depend on new one of you is
always between 0 and 1 so so it may
depend on Phi of me it could be for
example for some crazy function of Phi
of me with that you know so so why this
problem is of interest to us first of
all like the one range trip conjecture
that this is an np-hard problem so they
conjectured that for every value of Rho
there exists some delta much much
smaller than road such that is NP hard
to distinguish whether Phi of Delta n is
closed one or if five del time is close
to zero and more importantly they prove
that this conjecture improves the unique
gains conjecture so so in other words
this means that if you want to design an
algorithm for the unique game you should
start from this monster expansion
problem because this is that this is an
easier this is an easier problem than
the unique games every algorithm if you
design a polynomial time algorithm for
unique games you will obtain a
polynomial time algorithm for the
smallest expansion problem
yeah if you wanna yeah not if you want
to refute this you should start by
refusing this there's no no not for the
proof so then it could be that you could
design algorithm for this one but not
for uni kings but ok so so our a result
as a corollary imply a approximation
algorithm for this problem so we can
prove that for any values of mu and
epsilon we can find a set of size at
most M you to the 1 plus epsilon and
expansion at most root PI of me over
absol so this does not refute the
conjecture because the size of the set
that we find is is larger than mu if you
could prove for example that size of s
is within a constant factor of new then
we would have refuted the conjecture the
running time is again if if I have a
vertex of the of this target set then
it's going to be sub linear it's going
to be just proportional to the size of
the sir it's it's basically corollary of
the after algorithm so as I said
previously our algorithm can our local
graph clustering algorithm can ensure
you that the output set has a large
intersection with the the target set so
in fact this gives you the intersection
that would set like mute the minus
epsilon fraction of the output set lies
in the in the target set my view was
blocked
hi
you are going to start with the tiger
said no algorithm we can run the
algorithm from every vertex in the graph
and just find the best we don't need to
start from the targets it I mean then
the running time would be worse yeah if
you know if you yeah it depends on that
but for here we don't need to you don't
want to optimize the running time we
just want to find that an approximation
to the problem different natures
actually she's not look at the algorithm
do this we'll start with the vertexes
will have order so I mean that's so the
idea is that if you have a local
algorithm it's a good hope that you you
can solve this problem because you know
a local algorithm should will let's
start from the neighborhood of the
vertex and then it expands to all of the
vertices so so you may hope that you
know along the way you find a small non
expanding set so that is why these two
problems kind of related to each other
hey for example it could be that I mean
I cannot prove it could be that if we
can get rid of this a to the epsilon
then you could have solved their
response expansion problem ok so this
result is also proved independently by
quark and love using different
techniques ok so if there is no question
let me go on to talk about the algorithm
and the analysis ok so all that so you
said that that it's still possible even
over there sighs it with your salon yeah
yeah we don't think I mean yeah it is it
is possible and in fact even the
algorithm that I'm going to present may
do that but we cannot prove it really i
will mention by enough talk that the
current algorithm we don't have any any
tight example for it it could be that it
can refute this conjecture don't know ok
so so this time i put more main focus on
this and this your mi so I try to forget
about the running time I'm going to show
you how we can approximate this massive
expansion problem ok I'll give you some
ideas also how we can talk about the
running time too but I mean ok so let me
let me now talk about the algorithm for
a couple of minutes like 10 minutes so
we use the same machinery the evolving
set process that understand and press we
are used in their work I will define the
evolving step process in a minute but
just I mean as i will show you later the
algorithm is again very simple we just
run this process and and we return the
best that I defined ok so what is the
process the evolving set process is a
markov chain on the subsets of vertices
in the graph ok so let me let me tell
you how we can evaluate for finding them
a markov colonel so for a set s we first
choose a random threshold you uniformly
between 0 and 1
then we include all the way all of the
vertices where the probability of going
from v2s in with a lazy random walk in
one step is more than you so for example
if if you is if you is less than
one-half then the new set would always
be a superset of the old one why because
because this is a lazy random walk so
the probability of going from everywhere
takes inside s to s is more than one
half so the news that would be a
superset of the old one and if you is
more than one half the news that is
going to be a subset of tall so let me
let me show you an example it would be
more clear so let's say for example we
wanna run this process and this cycle
okay so we start from this graph I say
the first first threshold is pointing to
so we include the vertices that have
probability more than pointing to of
going to that single vertex these races
has probability one quarter right so
include these two vertices okay then the
next threshold is 0 point 7 this is not
going to change so the video one has
probably one of a staying side
before it's what you have always point
to yeah so it's about Patricia sorry
so then the next one is point one you
increase again the next time is point
eight now you decrease because you know
these guys has probability only
one-quarter of going to s so you
decrease the set then you again decrease
and then you decrease to the empty set
okay so it's not hard to see that this
process has to absorbing states either
the empty set or the full set so since
we want to run this process you know for
a sufficiently long time to find a good
set we would like that the process
absorbing the full set the whole set ok
so in order to avoid absorbing in the
empty set the condition on the process
to get absorbed in the full set and we
recall this new process the volume bias
evolving set process it was really funny
random mckinsey run so so so here is the
mark of Colonel for the volume bias
evolving set process you just need to
multiply the original mark of Colonel by
the ratio of the new set over the also
this follows simply by the you know the
fact that we've always said pass that
the size of a set is a martingale a
volume is set process and you use the
using the loop h transfer okay so let me
not prove it but yeah let's this is
going to be the mark of Kenner for the
new so in the rest of the talk whenever
you use this hat notation this means the
the volume biased process and I i don't
think that reasonable
this one no it could be like this one
could be larger than s ya know yet yeah
the problem see would never be larger
than one yeah this follows by the dupage
transform but for example you may see
that this is to never absorb in the
empty set because if if as long as empty
disk is going to be 0 so it for sure
never absorb in the empty set may just
absorbing the full set and it's also not
our to see that it is in this isn't in
the Mark of Cain so summation of
probabilities are one okay so let me
tell you the close connection so as I
said there's a close connection between
this process and random walk process and
in fact diaconis and Phil first started
this process in order to compute the
mixing time of random walks so so
they're in the paper they prove a very
nice coupling between this process and
the lazy random walk so they define is
coupling XTS key that has the following
properties first of all XT is always
inside st so that random walk is already
inside this and moreover conditional st
being some particular set s the random
block is uniformly distributed in that
set
okay so so as an example note that once
the process absorb in the whole set the
random wat is mixed so what this means
that if you want to compute the mixing
time if a random work you can just run
this process and compute the absorbing
time of this process yeah upper bound
you up around the absorbing town of this
process
okay so also understand and prayers in
their paper use this coupling to prove
that one can simulate the developing
step process so so they proved that for
any sample path s0 s1 off to st we can
simulate this in time essentially
proportional to the size of st so size
of SC x routine Polly locket so in fact
what kind of what they do is that they
look at the we look at the vertex they
do one step of the random walk and then
they condition on the set and the new
set to contains this new vertex and see
how it should change so so the upshot is
what this me this means that you can you
can run this process for a sufficiently
long time without you know without being
you know terrified after a running time
right it is okay so you just need to you
just need to prove that the process is
is good it's going to give you a small
non expanding set but there is no
problem with the running so so in fact
our algorithm we just do that we just
for a small expansion we just run this
process from everywhere text of the
graph and we're on each copy once the
length of the work is sufficiently
largest at least it's not of me over for
you and if any other copies finds a
small and expanding set then we just
return it and victor so very simple we
just run the process from each vertex
find it look so so let me now talk about
the analysis i'm going to i'm going to
show you that there exists the vertex V
such that if you run this process with
some nonzero probability you're going to
find the not
small on expanding set okay so before
going into the mathematical notation let
me give you a high-level overview of the
proof so the proof uses the following
two observations the first one is that
the evolving set process grows rapidly
on expanding sets and a second one is
that the process cannot get kind of get
trapped in non expanding state cannot
you know leave them very easily so kind
of putting these together what you what
you would like to say is that ok the
process so if this is the target said i
want to say the process goes fat very
fast until it hits mine you know target
said then it has to spend some time
there so i should be able to find its
targets ok so so for example let's say
this is think about this dumbbell graph
at say you have to expanders connected
by an edge i'm going to start the
process from this vertex so you may get
some intuition by looking at a random
wall so the process kinds of do work the
same as a random walk and we know that a
random walk in the first log in a steps
you know very rapidly mix in this
expanded so the process kind of do the
same thing very rapidly expands and
covers the whole set and then for quite
large amount of time he can just we just
added need some vertices would be very
close to a and then we should be able to
find it will be able to fine
ya know there is a difference between
random walk and so yeah I good random
but it may take a lot of time for it to
hit this vertex but by the process may
may get increased but but then it would
decrease the reason is that you know
from this point of view like at this
time the you can think about running
evolving step process from a single
vertex right but if you run an evolving
set process from a single metric with
high probability it will go to the empty
set this new process is not going to be
volume biased anymore because it always
has this big mass at the left right so
for example if I have this latest if I
include this neighbor his neighbors then
the size wouldn't change so it's the
same as if you as if you you were
running the non volume bias process from
this vertex and so with high probability
you were going to send it very small so
lonely be retained in the same next time
the uniform threshold happens to be
below this very small region
then you have a point if the chance when
I stepped in there goes into the set
above the threshold normally look is
this about the festival yeah includes
that is obstruction of its neighbors in
this current set about this version is
about right and so this vertex is very
unlikely to be in the second iteration
yeah it's how they stuff because it's a
date or not but no directrix this to say
in any way I don't think I'm neighbors
like that right to stay cuz it's a naked
under right let's poke with you have no
but if you don't it a couple of times
then you would know this movie but it
won't do those guys this time I don't
say what the neighbors from for each
labored check free to go in thank you
know I think I think that I fate to
think about it is you know suppose I
just I just run the volume that not the
volume is the evolving set process from
a single vertex okay then then the
process is a martingale so so the
probability 1 over N the process would
absorb in the empty set sorry we
probably one minus one over the property
with absorbing it empty set and only as
positive 10 variance will get the whole
set now my claim is once you get here
then the process is this new process
from the point of view of this expander
is just as if you run a non volume bias
process because because you have a giant
component at the left so this ratio is
always going to be one
okay so let me now tell you how we can
make this quantitative so we're going to
prove the following statements and i'm
going to show you that this is
sufficient for a proof so the first one
is the following we're going to show
that for any time T if you start the
process from some vertex V 44 anywhere
fix their the minimum of the expansion
squared of all of it of all the sets in
the path is at most a lot of lot of s T
over T with high probability
okay so so in other words if if for
example these expansions are large then
you should have you should have a large
set by this time right now so again so
this means that if you are going through
the expanding step you your your set
should go very fast but in the second
statement I'm going to I'm going to tell
you that I'm going to show that for for
quite a large time the state cannot grow
very fast the set should remain small so
so here we're going to show that if you
choose to very large something like
epsilon log of MU over five new then all
of the sets are a small all of the sets
in the sample paths are at most new to
the one class epson okay so so these two
basically contradict each other one of
them says okay either you find a good
non expanding set or then you're done or
if you don't then your your process
should go very fast but you know you
cannot grow very fast because there's a
there's a target there's a target not
expanding set to you so this means that
with some probability you should go
slowly so you should be able to find a
small on expands so let me tell you why
this why this proves the theorem it's
very easy just plug in alpha equals mu
in the top then by Union bound you can
prove these two both of those of the
things statements a care with some
probability me with the minus epsilon
over to something then then what what do
you get from it bottom line you get all
of the sets are a small they have size
at most mute the 1 plus epsilon from the
top one you get there is it there exists
a set of size at both of expansion at
moes root 5 over epsilon because put t
equal epsilon of me over fire feel whole
thing is going to be roots for grabs
ok
so so now let me tell you how we can
prove each of the statements so I'm
going to start with the top one the face
i'm going to show you how we can show
the the process clothes rapidly in varna
step okay so this is an this is based on
observation due to more recent press so
they show that for any set s there if
you look at this expectation expectation
of the root of the change in one step it
is at most 1 minus 5 squared a cell for
example think of the Phi being a
constant this means that in one is that
your new set is constant times more than
the old one mmm you should have to
handful yeah
so share so this is with respective
volume bias process you can write it in
terms of the non volume is one just by
multiplying it by s 1 over s and then
you get this okay but now this is this
is not hard to prove so in fact the
proof simply follows from the fact that
the volume bias process is a martingale
the size of the setting the volume wise
process is a martingale so so this means
the reason is that if your threshold is
below one half your set your new set
would be 1 plus 5 more than the old one
in expectation and if it is more than
one half it would be 1 minus five tools
1 in expectation the reason it just
follows from the same reason that the
random the probability the random walk
remaining inside the set s is 1-5 it's
exactly the same reason so having this
in hand you can just prove this using
Jensen's inequality
right so now how can we use this to
prove that you know totally like overall
the the the set grows rapidly so let me
call this ratio sigh of s this is a
ratio just depend on s right now the
idea is is to define a martini so so we
define this ratio of 1 over root st
times the product of the size functions
of s0 up to st I have very easy to see
that this is a martini because because
expectation of Mt given you know s0 up
to st minus 1 is you can write it as
expectation of you know root s t minus 1
root st root HT minus 1 times the size
function soil s0 up to self s t minus
one condition on the above and then you
know if you take this thing out it's
just empty of minus one this thing is
you know this thing is a sigh function
this is empty of minus one times one
over sorry office t minus one times
expectation of root is t minus 1 over s
okay it's easy to show I'm Tori about so
so what this means is that okay in
expectation this mark this MT is going
to be one so by simple application of
Markov inequality you can you can show
it's going to be less than any other
with some large probability 1 minus 1 1
over alpha so plugging in the values
you're going to get the following with
somehow with probability 1 minus 1 over
alpha the product of one 1 over 1 minus
5 squared is less than alpha times
square root of SD and you know just if
you just take a look at it Emma for both
sides you're going to get this
inequality so varies
okay so let me now talk about the second
part of the pool so here we want to show
that the set gets trapped in on
expanding sets okay so let me let me
instead of moving at all of the sets are
a small let me just proved that the last
one is a small okay and because the
process is growing if the last one is
small essentially all of them should be
useful so how we're going to prove that
the last one is small so I'm going to I
want to prove that the last like for
example pat s 0 of 2 sts see that most
move to the 1 plus epsilon with this
probability some investor anomia
probability of new for some very large t
so here the idea is to use the coupling
between the between the process and the
random walk so my claim is this is
equivalent to the following I just need
to prove that a random walk it started
from this vertex V is going to be inside
a with some like I need to prove that
there exists some set a such that the
random walk a status from V we're going
to be in a in tier steps with some
probability some nonzero profit amuse
the mighty subs why it is two are
equivalent just use the coupling so you
know that let me recall what what did
the coupling say said the distribution
of the random walk is right it said it
says condition on the evolving set
process being equal to some particular
set the random walk is uniformly
distributed on that set now if I look at
the distribution of the like the
probability that a random walk is that
is in some set a this is equal to the
expected fraction of the you know their
section of st with a right because you
know you can easily go from the evolving
set process with random what if i just
look at the distribution of the set i
can just you know apply the uniform
distribution instead and then take the
average this would give me the random
wattis tribution now if i want to
the probability of being in some set a I
can just look at the set distribution
projected onto the set I take the
uniform solution and then take the
average us with give me I'd probably now
this this tells me the right hand side
is a small but I know the left hand side
is a smaller than the ratio of a over s
T so because a I take a to be a small HD
can be at most M you to the epsilon more
than a so HD can be at most mute the
oneplus eps
so I just need to prove this question
which is just a forget out there
involving set process is just a random
walk so just I just need to prove that
there exists some set a and as you might
guess this set a will be my target set
so I want to prove that for my target
set there exists a ver take such that
the work is going to be in that set
after two steps with some probability me
to the minus epsilon I'm going to prove
something stronger is that proving that
the block is in some is at T is at a at
time T I'm going to prove that the work
never leaves a up to time T all right
and I'm going to prove it with this
probability but you know these two are
essentially the same if you take tea to
the epsilon log of mu over phi of you
then you see these two are exactly the
same
so so we just want to prove something at
some property of the random walks you
just want to show for every set a there
exists a vertex V so I said that an
unlock never leaves this gives a with
probability 1 minus Phi of a to the T so
if again I'm not going to make it even
stronger and I say I'm going to prove if
if you choose we uniformly at random in
a it never kind of leaves a with this
probability now this should kind of
recall you off of a DVD of a definition
of Phi of X so what was the definition
of Phi of a it said it's the probability
that the random walk leaves a and 1
minus Phi is the probability that the
random work remains inside a so
intuitively this should hold because you
know if if the probability that the walk
leaves the settee at each time was
independent of each other you would
exactly get 1 minus 5 so you want to get
you want to say it's even better so here
is some extreme example where you get a
quality in the above so let's say you
know remember that each vertex in a has
on average d times Phi of a edges going
outside now let's say that's the case
for all vertices so all vertices has
exactly this much amount of edges going
outside then if you do one instead of
the random walk the distribution remains
uniform inside a because everybody has
the same number of edges going out
rather okay you do 1 SL of that of the
random walk and you'll cut that look at
the distribution remains inside a ok you
know it starts from a uniform
distribution in a now do one SF of the
random walk from from each vertex
exactly the same amount would go outside
social project that probability back to
the set a you get uniform distribution
but again if you condition I need to be
in the city listen yeah condition
so yeah this is espadrilles yeah so in
some ways yeah so in general somewhere
Texas might have more so what we want to
say is that intuitively if some vertices
have more edges going to the outside
then this probably the probability of
being on them should only decrease
because if you are at if you're on those
vertices you will certainly leave the
set faster so after some number of steps
your probability should be more
concentrated on the vertices with less
number of edges going outside so you
should have a higher probability of
remaining inside so how can we prove
this it's also again simple so so we let
P me the transition probability of the
lazy random work then this thing is just
equivalent to the following the little
parts this for you so you of a is the
uniform distribution on a so start from
the uniform distribution of a we do
wanna step of the random walk we project
it back I of a is the identity matrix on
the city so you project the walk back on
the set we do one more step with project
click back we do one more step until the
time T where we just add up the
summation of probabilities and this is
exactly that and then the right hand
side is just you know this thing see
with by removing the tea and we know
that this is because we know that 1-4 is
the problem is that one is the random
walk remain inside the city so I just
need to prove this equation
and I can even reduce it even more so
let's let X be root square root of this
uniform distribution then and also
define this p IA to be a matrix Q then
the left hand side is just X transpose Q
TX right i can write this square root of
x square root of x
alright
and the right-hand side is X transpose
QX to the T by by symmetry so just want
to prove this thing all right and this
is all so simple to prove so you just
need to diagonalize q and use the fact
that it is a power system it semi
definite matrix so so you can write x
transpose Q of X yeah Q is not symmetric
but I can make it symmetric simply by
putting another I of a here so yeah I
cannot express plus q QX as summation of
you know X transpose V I where V eyes
are the eigen I can vector so let's say
vivan up to VN are the eigenvectors of q
and lambda 1 up to London eigenvalues
then this thing would be exactly this
outside this is quick
and the right-hand side is just
the whole turkey so you just want to
prove this thing is more bottom one and
this is again simple to prove because
this is just the yen sins inequality
these eigenvalues are non-negative is
the summation of these guys add up to 1
because X is a unit vector so this is
the ski insanity
alright so I'm almost done to pack this
equation holds for any X and Q so let me
finish the talk so again we proved that
for any new and epsilon our algorithm
can find a set of size at most new to
the 1 plus epsilon and expansion root 5
over epsilon this has been the first
approximation to a small expansion
problem without loss in the expansion so
previously there has been many works
even with many people in this room where
where they they could could give an
approximation algorithm with preserving
the size of the preserving the size of
the set the size of a target set but
losing in the expansion
this one no it's not of an overview log
of n know it besides this plugin it's
not a be nominated yes logs yeah no yeah
that is this is the bracket here and
also improved that a chinger cut can be
computed in almost linear time in the
size of the target set thus containing a
local variant of the chagrin call it so
let me give you some open problems and
finish the talk so perhaps the main
remain remain trysting open problem is
if one can find a approximation
algorithm for Phi of music is
independent of the size of the graph so
in particular this is a nice question if
you can if you can prove that there with
some University polynomial probability
mu all of the sets in the central part
of the evolving set process are at most
order of mu so here we prove that there
are at most new to the 1 plus epsilon
and and we managed to get this size mute
the 1 plus epsilon after output set now
if you can improve this order of MU then
you would refuse to the smallest
expansion contraction another problem is
that so these works they all use the
semi definite programming relaxation not
the random walk it also would be
interesting if one can kind of replicate
our results using the semi definite
programming relaxation it would you know
may highlight some new ideas for the
problem in terms of the local graph
clustering problem one interesting
question is if one can generalize these
two weighted graphs currently we don't
know none of the algorithms we
generalize to weighted graphs and the
other one is that you know by if you use
a traditional spectral clustering
algorithms as I said you may miss
classify the small communities but here
since you can using local graph
clustering since you can run the
algorithm from every vertex in the graph
you can hope to find overlapping
communities in a network
but so there has been although there is
a quite a lot of practical interest in
this problem there hasn't been much of
the theoretical works and it would be a
very interesting direction to work on so
what do you think of this using SDP
relaxation because I'm till you get
chico california state university yeah
so so this this result they use SBP
relaxation and if you don't want that
set to be large you can just you can
just plug in meu to be in half and then
you get cheated inequality
and
but but then there is no guarantee on
the other side of the set it could be
very large
sorry about way to grass didn't you say
the beginning that you didn't need
regular graphs is any regular but we
need unweighted never have to be simple
girl yeah we do to be simple yeah
right
I mean is that so clear how you should I
mean so all of these algorithms kind of
depend on the number of edges in the
graph but hmm and you have a better
answer in the weights or conductances
you just into the devil's multiplicity
of edges okay wrong is very same come I
think you can run the algorithm but can
you can you bound the running time
yeah pulling thumbs he stands willing
board is for you wait yeah I mean the
point is it and I might be that the
analysis would work I just not after
people have done this and I would
I don't know if it is say that inherit
difficulty on the problem or now if the
graphs are directed that's important
yeah so the graph are directed it's much
more
so then you mean for weighted graph if
the weights are small it's a safe he got
very small then it's exactly you can
just not interpreted as multiple actions
and bound algorithm exactly did remove
difference if the weights can be very
large the algorithm still it works it's
just a matter in the analysis yeah that
is my only know we have very large edges
versus very small very large waitress is
very small weights then now I don't know
it could be the kindest words yeah yeah
the running time but the fascination
guarantees would work</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>