<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session 9 | Coder Coacher - Coaching Coders</title><meta content="Oral Session 9 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session 9</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FZnn6BuABrQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so our next talk is going to be is
called actor critic algorithms for
risk-sensitive MDPs by prashant LA and
Mohammed Gotham's a day and Muhammad's
going to give the talk so thank you for
having me here I'm currently a leave of
absence from energy of working at Adobe
research in California and it's a joint
work with our postdoc back in France
Frusciante la as the title implies this
talk about is about managing risk in
sequential decision-making problem
sequential decision making under
uncertainty arise in everyday life in
problems like navigation process control
finance and medical diagnosis and many
interesting sequential decision making
problems can be formulated as a
reinforcement learning problem in which
an agent interacts with a dynamic
stochastic and in completely known
environment with the goal of learning an
action selection strategy that we would
like to call it a policy in order to
optimize some measure of its long-term
performance this interaction between the
agent and the environment is often
modeled as a Markov decision process or
MVP which is a five tuple in which X is
a service state is the set of actions
the capital R is the reward random
variable of taking action in state ex
who's expectation is the little R and P
is the dynamic of the system which tells
us if you take action anus state X what
would be the distribution over the next
States and p 0 is the initial
distribution of the system so what's
important is policy and policy is a
mapping that look at the system what has
happened in the system from the
beginning up to now and try to tells us
what to do and which action to take of
course we are more comfortable to work
with and are interested in a stationary
Markovian policies that are mappings
from states to actions or distributions
over actions so one of the common
criteria in marketing sequential
decision-making is discounted optimality
or discounted reward MVPs so let's go to
the system at each time you are in the
state X and you have any strategy new
and then using this strategy to interact
with the system and you generate a
trajectory of the system and along this
trajectory you observe rewards and then
you put these rewards together waited by
the discount factor to keep things
bounded and then you have a quantity
which is called the return of this
policy at the state X and if there is
any intrinsic randomness in the system
this quantity is a random variable
therefore in a standard risk neutral the
objective is to find a strategy that
maximizes the expectation of this random
quantity which we call it the value
function of course weighted by the
initial state of the system for
simplifying the notation I assume that
the system has only one initial state X
naught and therefore the deuce neutral
is trying to find the policy that
maximizes the value function at X naught
I want to have a footnote here that I'm
using some in this talk but you can
replace all the sons with integrals and
everything is going to go through so we
are not restricted to discrete systems
here another popular optimality criteria
is the average rivard optimality an
average of art of the policy is the
reward / a set of following that policy
which can be also written as the
multiplication of the stationary
distribution of the system over state
action pairs times the reward function
and again here the risk-neutral or
standard objective is to find a strategy
that maximizes the reward / step from
the literature we know that for both
discount at an average board these
optimization problems have a mark
stationary markov and even deterministic
policy then we have a repertoire of
algorithms in order to exactly or
approximately find these policies both
in average and discounted setting okay
so now in many problems we may want to
go and want to step ahead because look
at this so let's think about the
discount that case let's imagine that
you learn a policy finding the optimum
policy now you go to the world and you
have one life to live and you executor
your policy and this is the
result that you get is the return if
this policy has a lot of deviation and
this value deviate around the stand
around the expectation it may not be
desirable in some problems and this is
when we would like to control this
variation so when we talk about risk you
immediately think about finance
investment or sometimes about burning
made of but in many other sequential
decision-making problems risk is
important for example you may decide to
deviate from your shortest path in order
to avoid a dangerous Highway or you may
want to lose in terms of the performance
of your system in order to control is
variability because it might be
dangerous or the rest of your system so
therefore we are looking at the criteria
that penalizes this variability in use
by a policy because it might be
undesirable for us or in other word we
are trying to minimize some measure of
risk as well as maximizing our standard
optimization criteria that we were
dealing with so now what is a risk
sensitive criteria we have some work in
the literature we have expected
exponential utility which has direct
connection with mean variance trade-off
a very natural and well studies set of
criteria or the variance Lee related
measures that have been very well
studied and now people are looking at
percentile optimization and percentile
control but the open an important
question is to construct a conceptually
meaningful risk criteria which at the
same time is computationally tractable
that optimization problem is nice
unfortunately the all the results coming
in this front are not very good from the
early work by shoaib Alan Phil are two
very recent work in our community by
shimon orange on testiclees at icml 2011
and when i say bad news which means that
this solution to the risk sensitive
criteria may not be a stationary
Markovian policy that we lie or
calculating it might be something hard
ok so now working on this and basically
trying to restrict ourselves to
different set of policies
trying to approximate them has a long
history in operation research much less
work in very unfortunate turning a
machine learning community and which
what discriminate us with operation
research is an operation research they
often assume that they know the dynamic
of the system while in machine learning
they would like to consider the case
that all the information that we have
from the system is from the samples that
we obtained by interacting with the
system some results in this sensitive RL
are the work by vivek work are
unexpected exponential utility and a
recent work from the shiners group that
they look at the several variants
related measures similar to what we do
in this world but they restrict
themselves to stochastic shortest path
problem and to policy gradient methods
so the contribution of this work is we
look at both discounted an average of
our MDPs from more general than their
work we defined for each case we define
a measure of variability for a policy
first which in turns induces a set of
variants related risk sensitive criteria
so here we are restricted in this work
to variously variants related risk
criteria for this criteria the proposed
actor critic type of algorithms to
optimize their sensitive criteria for
each algorithm establish the asymptotic
convergence of the algorithm I'm not
going to talk to show any proof and talk
about that convergence in this talk but
I would be happy to take question at the
post there but i'm only mentioned that
asymptotic convergence that we show that
we converge to the local saddle point of
a performance measure and saddle point
comes from the policy parameters and a
Lagrange multiplier and then we
demonstrate the usefulness of an
algorithm using a traffic signal control
problem for those of you who may not be
familiar with actor critic actor critic
is a very elegant naturally inspired
reinforcement learning algorithm which
works based on two components one
component is called actor which is in
has a is a class of parameterized
stochastic policies and is in charge of
control and exploration of the system
and tries to improve the policy by
tuning the policy parameters in the
direction of gradient of a performance
measure critic the other component at
each time tries to estimate the value
function of the current policy so it's a
predictor and
passes this to actor I say okay use it
to estimate your gradient and I'll take
your policy parameters so next up to
here I want to deliver the message the
main message that what we are doing in
this work now I talk a little bit more
technical let's look at it discounted
reward setting the random variable is
the return in risk neutral people look
at the expectation of this random
variable try to optimize that the first
question is what is the measure of
variability the variability that the
consider here is one of the nicest more
meaningful but harder to deal with in
terms of mathematically and being
tractable which is the variance of this
quantity so the measure of variability
is the variance of this point if you
want to remember any notation from this
slide is this you is the expectation of
return square so I call it Val the
square value function so if V is the
value function use the square value so
now any combination of these expect is
this mean and variance any meaningful
combination of this is a risk criteria
that our algorithms can handle for
example maximizing the mean subject to
variance to be bounded by alpha or the
sharper ratio which is used in finance a
lot which is the mean divided by the
standard deviation so our algorithm can
handle any of them so I just focus on
one of these cases which is maximizing
the expectation subject to variance to
be bounded so we used the lagrangean
relaxation change the constraint
optimization to unconstraint
optimization and of course one needs to
evaluate the gradient of this L with
respect to both theta and lambda defend
in Quetta accenting lambda and we are
final the great envious vector lambda is
fine it's easy to calculate the gradient
with respect to theta requires
calculating the gradient of the value
function at X naught at the bliss state
initial state and the square value
function so in this equation the only
thing that is important is this pi gamma
and pie tilde gamma are the gamma and
gamma square discounted visiting a state
distribution of the markov decision
process induced by debt so now for those
of you who are familiar with actor
critic in risk neutral we only have the
s gradient and we can estimate this
gradient with one trajectory of the
system unfortunately right now because
of two reasons one we have these two
different sampling distribution PI and
PI till then and second and second in
order to estimate the gradient of a
square value function we need to
estimate the gradient of value function
at every state because of these two
problems we cannot estimate the
gradients with one trajectory of the
system and what we do is we look at this
simultaneous perturbation methods that
are very elegant techniques to estimate
the gradient of the function using two
simulated trajectories of the system one
using the current parameter theta the
other one used using the perturbed
parameter theta plus beta delta beta is
Anna scalar Delta is a vector of
independent random variables if these
random variables are the matter is
simultaneous perturbation stochastic
approximation spsa and if it's a
Gaussian is a smooth functional and
these are the way that they estimate the
gradient the elegancy of these methods
comes from the fact that no matter what
is the size of the gradient only true
trajectory is enough to estimate the
great okay so this is the structure of
the algorithm this is an incremental
algorithm we generate two trajectories
trajectory one using the running
parameter of the system theta trajectory
to uses using the perturbed parameter of
the system and then we will have the
critic which estimates now both value
function and square value function and
passes this estimation to actor to
estimate the gradient and update both
the policy parameters and the Lagrange
multiplier so I ignore these formulas
the only thing that I want to emphasize
this is the formulation in the critique
so for critique now we need a method for
policy evaluation in as Peter explained
temporal difference learning TD is a
method that we used to evaluate a value
function but we don't know whether this
machinery works for the variance related
quantity that you so what we were
working on for both discounted and
average but the shiners group published
it at ICM a 2000
and 13 is to show that TV actually works
for the variance related quantities so
they showed it for the stochastic
shortest path we needed for discounted
an average we basically extended it but
this by itself was a contribution that
showed that the critic by itself works
for the variance related quantities and
then this is the actor and updates the
party supporters and the Lagrange
multiplier what is important is in order
for us to guarantee the convergence of
these algorithms the 3s step sizes the
Zeta 3 the step size for treaty theta to
a step size for policy parameter and
theta 1 the step size for Lagrange
multiplier should be in a way that the
step size for critic is in the fastest
time scale the theta for data the policy
parameters on the intermediate time
scale and the other one on this flow is
time escape then we can converge we can
guarantee the convergence of this
algorithm so it's a tree time scale of
stochastic approximation algorithm but
we don't like about this is we would
like to estimate everything with one
single trajectory which here is not
possible so we need a simulator of the
system the good news is if we fix the
Lagrange multiplier and don't optimize
whether a regular party prior we can
estimate the gradient in one trajectory
and have a two time scale stochastic
approximation so basically you have to
choose 10 different Lagrange multiplier
optimized for each choose the best so
for the average of art I go very fast
the situation is very similar the only
thing that is important is what is the
measure of variability for average we
work so we'd want to optimize for
average the measure of variability that
we consider is how much the immediate
reward deviates around the average
that's the measure of variability which
is called the long-run values so again
any combination of this row and lambda
can be can be optimized but are not
worry them so again one notation that
you should remember from this slide is
Etta the row is the average aboard Etta
is the average square reward and we need
to deal with this coin again for the
case of maximizing average do constraint
on variance to be bounded we need to
calculate the gradients now the
gradients requires calculating the
gradient of the average and average
square but the good news is in the
average reward we can estimate these
gradients with one single trajectory of
the system good news and this is the
structure of the algorithm and so we
estimate the value on the average an
average square we have the TV algorithm
we have the critic which calculates
those and actor update so again in here
there's eight afore and Zeta 3 are done
the same time scale the first time
escape Zita to inter mediate a Zeta 1 is
slow so it's a three time scale
stochastic approximation if you want to
fix the Lagrange multiplier don't
optimize for it it's a two time scale
stochastic approximation so experimental
result we apply this to a traffic signal
control problem that that cars are
coming with a Poisson distribution and
we want to so so far i talked about
reward and maximization this experiment
is cost and minimization so that the
lower is better so we want to minimize
the number of cars in the system which
indirectly minimizes the delay of the
cars in the system and so the we
formulated this problem at both average
you are done discounted I just show the
results for average but the same for
discounted the way that you should read
this result is look at the one on the
left so we run an algorithm we learn a
policy the executives policy several
times and each time we calculate the
average word or or the return so we have
a number of these values we calculate
this mean and variance we represented as
Gaussian so nothing is Gaussian your
gumption has been used just for
representation as you can see the the
fat Gaussian is the risk-neutral
algorithm and it's mean is a smaller so
it's better but it said when we go for
risk-sensitive we lose a little bit in
terms of me but we get a thinner
algorithm and less variance and the one
on this on the right side is the average
Junction waiting time that for both
algorithms so at the beginning the
waiting is zero but the cars are coming
if we don't have a good control these
graphs should shoot to infinity but
their control basically flattered
flatten it out and you can see that the
performance of the risk sensitive is a
little bit worse than the performance of
the risk-neutral but we reduce the
variance so I just wrap up here and
future work is looking at
interesting a sophisticated
risk-sensitive criteria and of course
looking at finite time bound for these
algorithms but unfortunately you don't
have finite time bounds even for
risk-neutral actor critic methods so it
might be a little bit orthogonal so
thank you for attention I'm ready for
question and we are hiring both interns
and researcher at Adobe research if
you're interested to know about it I'll
be happy to talk to you thank you sir we
have time some questions but those who
are going to the spotlight presentations
immediately after the stock would you
please come up and stand behind on this
side thank you so none do you had it
done thank you my question has to do
with meta control what Peter alluded to
in his last slide it seems to me that in
going from a three time stochastic
approximation to a two-time one you
decided to use a fixed grid for one of
the parameters now that's one could
think of doing much smarts to control
their because if you're going to be
using as a grade you should use say
based on optimization or use contextual
bandits to actually select the best
parameters moreover spsa has a lot of
tuning parameters which are actually
hard to tune in practice so what would
you think about having a meta policy
that is base a on contextual bandit that
would allow you to tune all these extra
parameters would it behave well I
haven't talked about this that's that's
that's a good point I honestly haven't
talked about the controlling having
basically a meta control separate from
for the Lagrange multiplier separate
separate than the policy parameter I
need to think about it but definitely
this undesirable because we remove one
of the optimization criteria and just
look at the grid but you make the
algorithm more desirable for
reinforcement learning that we like to
estimate everything in one project if
you don't like to have simulator or
anything but that's that's a good point
i need to think about it i honestly
haven't talked that i haven't talked
about this point but have you seen such
a thing that in optimization they
separate the
lagrange multiplier from the rest of the
optimization criteria have liked it
hierarchical type of way were no chef
okay yeah I have a small question
regarding the initial performance of
your approach so in a risk environment
is usually very difficult to actually
apply something that that's online
learning and I just have a question how
you actually solve that problem like if
you'd go to a traffic light problem it's
not that big of a deal if the
performance at the beginning is not that
good excuse me but if you think about a
more sophisticated problem where you
actually risk running running a robot or
damaging something it might be very hard
to actually apply your approach right
now I know can you comment on that I
don't see why the method should be more
dangerous if you want to apply to too
because whenever we have exploration
roboticist don't like us but I don't see
how there is sensitive might be more
dangerous or than the risk-neutral at
the beginning yes because we have a lot
of exploration and our policies are
naive then it might be dangerous no
matter is what but the question here is
after you learn your strategy and I say
okay you have one life to live and go
executive in the in the real world you
don't want to deviate too much but maybe
it's your unlucky day but you still want
to be at least don't hurt your system or
damage so it's after the fact during the
case I don't see any difference between
the two cases we can we can talk later
but that's okay let's thank the speaker
again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>