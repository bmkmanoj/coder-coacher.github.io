<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Declarative query processing in imperative managed runtimes | Coder Coacher - Coaching Coders</title><meta content="Declarative query processing in imperative managed runtimes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Declarative query processing in imperative managed runtimes</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wtv8cs1Fr0Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
yeah hi everyone it's my pleasure to
introduce Stratus who's read there at
the University of Edinburgh some of you
might know him because he's held his PhD
scholarships with microphone he's having
another one with systems networking and
yeah technically okay so yeah thanks for
the invitation it's its first time I've
been in Microsoft actually well in
Cambridge so um so what I'll talk about
today is this notion of things that I'm
guessing should be near and dear to you
oh so this notion of integrating more
database like functionality into
programming languages effectively so so
here's my I do database systems by the
way so some of the things that it might
say may not be too popular with you
that's okay that's fine just don't throw
something at me right or you know if
this is how you do things here that's
fine as well so so i can tell you my
view my own little view of my own little
world right so the way that I think
people build applications and you can
correct me if I'm wrong so what we
usually do is that we have some kind of
managed runtime and it could be C sharp
net it could be a the Java Virtual
Machine it could be your compiler and
your operating system stack all sorts of
things and you have basically two
separate entities into that runtime
which is your application logic so the
kinds of things that you want to do for
your in your new program and then you
basically manage the volatile memory
heap and what you do then is that you
start building your own data structures
and your own little you know AVL trees
or whatever trees you like or hash
tables or all sorts of nice things and
those data structures basically map well
to the kind of processing that you want
to do at the application level and you
use your ma you're volatile memory to
basically store your data structures
what happens is that at some point your
data structure
become too big to actually fit into your
main memory so then you have to start
offloading things to disk and I'm using
SQL and secondary storage while I'm
using secondary storage there's a
general general term but I'm using SQL
as a serialization protocol because pay
i do databases that's the language that
I speak right it could be anything else
but the takeaway messages that you need
to have some kind of protocol that
basically takes your nice in-memory data
structures and maps them to something
that's block oriented disk oriented and
it's it needs some kind of translation
layer in between to perform the
translation right so that's that could
be done automatically and it could be
done through some well it's usually done
automatically through some high-level
interface like JDBC if you're using Java
or link or some other application some
link provider so on so forth now what
changes is that the data structures are
now too we're now getting enough memory
to actually have our data structures
completely fit into main memory so the
only way the only reason why we would
need this secondary storage
representation an SQL or some kind of
serialization protocol is so we can
target persistence okay so we can make
sure that our data structures now can
survive a system crash or can survive
our system being shutdown and rebooted
so on so forth but what changes now is
that our memory heaps are becoming
persistent because we have non-volatile
memories we have persistent memories we
have all sorts of technologies that have
historically been five years down the
road we haven't seen them yet but in
five years we'll get them that's that's
perpetually true but so what is really
interesting is that if you have enough
memory to fit your data structures and
if you can ensure that your memory your
byte addressable memory is also
persistent non-volatile then chances are
you don't need
the right hand side part of the picture
so what I'll be talking about basically
in in this talk is some of the things
that we've been doing over the past I
would say five or six years at algebra
with with my group in terms of exposing
quite a bit of that functionality at the
application level so the first thing
that I'll talk about is this job is this
work that we did on cogeneration for
Just In Time Protocol violation and
that's actually partially funded by
Microsoft so we started by compiling
sequel SQL down to see and then using
that representation to to run and
evaluate queries but once we figured out
that this is not really where the the
interesting bits where we started moving
towards language integrated queering and
i'll spend quite a bit of time actually
discussing what sort of things what sort
of changes you need to affect to the
link runtime so you can get these
cogeneration effects and fast query
performance and the last the second
parts that I'll talk about is how we
deal with persistence in the context of
that of that sort of architecture so
I'll introduce this notion of right
limited algorithms for persistent memory
effectively I'll only be talking about
two algorithms that most database
researchers have been beating to death
over the past few years hashing and
sorting and we'll see how we can
implement those algorithms in the
context of non-volatile memory but byte
addressable non-volatile memory using
some cogeneration as a way by which we
can better optimize the structure or the
performance absolutely if you can assume
by deducible persistent memory well and
you have kind of a like a cycle by cycle
snapshot of everything is happening so
why is the mini research to do switch a
machine off switch it on again and there
it is in the state elastic there must be
something else you're not telling us so
one of the things that might go wrong
and I won't be talking about this is
that if the reason why your machine died
or was switched off is because you had a
problem like something like that then
you're basically inheriting it right so
the manor that you turn it back on again
you still have corrupt memory so just I
was tell
aleksandra before we moved here your
data is still lost but persistently lost
okay our basic assumption for this talk
should be real despite adjustable
persistent memory you just use it for
everything and you got to worry about
that was it that would be a correct that
would be a correct assumption but i'm
not talking about i'm not going to be
talking about recovery too much in this
talk there's a certain separate part of
of our work that has to do with recovery
to do well um so most of the algorithms
that we've been taught they are dealing
with in terms of databases have been
optimized for disk i/o try to run a blog
level algorithm over byte addressable
persistent memory you'll end up missing
something aid like ninety percent of
your performance right there so there
are quite a few changes that you need to
affect in that respect so there's
nothing to persistence after you just
say database are brothers when you're
running them in memory need thinking
about a different way fine okay so and
by the way if you have any questions
just shoot right so so we'll talk about
these algorithms for for joint
processing effectively over persistent
memory and I'll spend quite some well
not so much time dealing with some of
our ongoing work on extending this work
in terms of and manage collections and
manage runtimes and workload driven data
placement so so this is you know
databases 101 for those of you who are
not familiar with database systems so
basically in my mind again database
systems basically have for Bay for
standard building blocks the query
engine the transaction manager the
storage manager in the Recovery Manager
and you can think of those four basic
building blocks as sort of orthogonal
that means that if you know as a
database researcher I choose to work on
the query engine than any performance
improvements that I managed to to get
over the query engine I've have little
effect on other parts of the system so
the added bonus is basically incremental
and it should be the case that if I
decide to change the transaction manager
I shouldn't really have to change the
other parts of the system that come
along with it
of course this is sort of an idealized
yeah sorry just just to go back to
silence cuisine is the assumption that
you still want to go through a database
even when there is persistent memory
right this is just an introduction for
what databases do so we'll be taking
some of those building blocks away as we
go along right so this is how databases
work this is as I said my own view of
the world coming from a database
perspective what I'll be arguing is that
you actually need to throw some of those
techniques out just because you're
dealing with something that you are not
optimized to deal with okay but yeah
great great observations all so far so
as I said these are relatively
orthogonal aspects if you change
something on one part 1 block of the
system you don't necessarily have to
change everything this is a bit of an
ideal situation may turns out that you
know if you're implementing your own
query engine part of the parameters that
you get for running your queries is a
transaction context and you need to
update that transaction context context
as you go along for every type of access
that you perform over some primary data
structure in the database system you
need to have some kind of log so that
affects the recovery algorithms so on so
forth but you know for for all practical
purposes we can assume that they are
relatively independent parts of the
stack now when it comes to query
processing what happens is that we have
this pipeline or transformations that
take place whenever you send an SQL
query to to a system so the sequel
queries for sparse do you have an
absolute index 3 you rewrite it you come
up with a logical plan the optimizer
takes over it produces a physical plan
and then that physical plan is
interpreted by the query engine to get
the results now the first time that I
showed this picture to some of my
colleagues who were doing compilers
their job basically dropped because
there was they were always under the
impression that sequel is not
interpreted it's actually compiled but
it is interpreted the reason why it's
interpreted was that back in the first
days of relational systems they did try
native compliation of SQL it
just too slow so they gave up on it and
nobody actually revisited for quite a
few for quite a quite a long time but so
as we said so the query is going to be
fed through this pipeline at some point
the physical plan is going to be
constructed which is a plan of operators
so something like sort merge-join or
hash join or some kind of access method
that scans a B+ tree something along
those lines yes sorry massage it takes
into account the data a year yeah so you
can still take into account the data if
you're running the compiler right but
the thing is that a relational and a
relational query engine needs to have
quite a bit or hazard its disposal quite
a few more pieces of information when it
comes to access patterns when it comes
to staging computation so even though it
started even though compliation was
rejected as an idea at the beginning
because it was too slow the complexity
of database systems grew so that meant
that you had you know all this baggage
to drag with you so you couldn't really
afford to revisit everything right um my
sir both of them take a query and
produce a better query is the only
difference that the optimizer can use
physical information somehow about I
don't know what about they are
statistics value distributions memory
allocations so how much memory you have
available it's written so the rewrite it
so you can think of the rewriter
basically addressing the fact that most
programmers write bad code right so the
rewriter will just take a rescue L query
and convert it into something that makes
a lot more sense and then the optimizer
will optimize that representation my sis
telling stuff might be some more
rewritings available absolutely so
actually it's a sort of so it's not a
loop so you don't optimize the query
multiple times through the actual
optimizer you just well the optimizer
emits a physical plan and that physical
plan is is just trusted that is going to
be the best one or it's going to give
the performance that you need what
really happens the query optimization is
that
and that has to do with the shape of
query plans effectively you can you have
a large collection of query plans that
have really bad performance and you have
a small collection of plans of execution
plans that have decent and comparable
performance so what the query optimizer
does is that it's her eyes to steer you
away from all the bad choices because if
you manage to get to one of the really
good plans then any potentially better
plan would probably give you something
like a one-percent performance
improvement which is something that you
can avoid you don't really need that
okay so even though you can continuously
optimize the query you choose not to
because you're really talking about some
large differences in performance after
optimization so so what we did was
basically introduced this notion of what
we call the holistic techniques so the
Assumption there would be to have a more
template inspired approach when it comes
to pre processing so I mean though in
the same way that some language like C++
gets a template for a data structure and
then instantiates that template based on
the data types and the type of
processing that it's performing we said
that what if we have a similar approach
to query processing and have templates
for algorithms in the same way that we
always have templates for algorithms but
instead of just having a whole bunch of
virtual function pointers that allow us
to map the query to various different
data types actually have some way of
instantiating the template at a pert and
in doing that we might as well start
collapsing various operators in the
query plan so that you can make a whole
bunch of nested calls and eliminate
function calls that way so a typical
example of why you needs or why we think
that you need techniques like that is
the level of data exchange when it comes
to tuples records being exchanged
between two operators in a query plan so
effectively what happens is that for the
operators of a query engine to be
composable you need to have all
operators abide conform to the same
interface so that's what we call an
iterator interface standard
iterator interface right but what
happens then is that for each record
that needs to be exchanged between two
operators you need to make a function
call okay so a function call actually
hurts performance but if you know that
you're going that you are always going
to be making the same function called
between two operators your models will
collapse both operators into a single
called constructs and you can do that
because you're going to be generating
the code okay sure so you can have I
have an example in a couple of slides of
automatically code generator generated
because of a visa sure okay so basically
what we're doing is that we're treating
SQL as a manager on time and we're doing
just in time code generation for that
managed runtime so the the overarching
the new architecture let's say of our
system now is that again you have the
parser that takes an SQL query fits it
to the optimizer the optimizer comes up
with a plan but now once you have that
plan you take it and you generate code
for that you fade it to a compiler and
then you apply all the compiler level
optimizations that you couldn't apply to
begin with or you hadn't applied to
begin with you get a new binary link it
dynamically loaded run it give it a the
evaluator in the evaluator just goes to
this get the data and returns the result
and just to give an example of generated
code this is automatically generated
codes by our system so we've figured out
that there's a single template that you
can use to so the input okay so a
typical example would be something like
a selection so a selection is basically
for this one yeah so for this one I'm
interesting with the entities so I can
see so right so so the name of the
algorithm is sore joint so I don't know
if that helps at all but the input is
two sorted lists that you merge them and
for every pair of matching records you
evaluate a joint predicate some equality
predicate so something like give me all
records in table 1 and table 24 for
which the value of attribute one in
table 1 is equal to the value of
attributed to in table 2 so that's a
joint algorithm so what you do is that
you have so
both tables table 1 in table 2 on the
joint attributes a1 and a2 and what you
do is that you make a synchronized scan
over the two records over the two tables
and evaluating the joint whenever you
have a match between the two okay
another example or the simpler example
would be something like a selection so
that's a filtering operation right so
scan the that table and only propagate
to the output the records for which some
predicate is true now imagine that you
have two selections one after the other
okay so you have a selection on the
object attribute one and another
selection attribute to what a typical
query optimizer would do or a typical
query engine would do would be to have
to selection operators one after the
other and for every record that would
satisfy the first predicate they would
need to be an iterator call or sorry for
every the top-level operator would make
an iterator call to the second operator
for every records that would satisfy the
selection predicate so that's a function
call then you would evaluate the second
selection predicate and then you would
propagate the tuple what we're saying is
that if you're doing cogeneration you
might as well look at that see that you
have one filter feeds into second filter
and collapse those two into a single
construct so these are examples of the
optimizations that we are and we perform
okay any more questions on sure you said
the list of sort they are the two tables
are sorted right so is out of these
dynamic properties that you BTM because
we can generate the code for sorting as
well or any query engine would actually
have some operator that would soar the
inputs before it would feed it into a
merge joint being a function call I mean
is it because it's an indirect virtual
call that makes it difficult or if it
was a known call then generate code that
you know the
Oh juice it must be a virtual function
called because you don't know the times
when you're programming the operators
when you're developing the operators
that say did the specialization based on
type but still left it as a function
call in and let the the c compiler in
line if it wants to but it was still
make a base for the second potter right
yeah but then but then you you mean so
you're doing two things one year going
and figuring out what type it should be
and the other is your sort of manually
in lining it fun and i'm just wondering
is that second step something that you
really have to do or just a secret is it
is right so it's it's something that the
compiler can do itself absolutely but it
cannot do it when you're compiling the
server because it doesn't know what sort
of queries you're going to be fading to
it they'd beat that the pilot free but
not willing to do that for some reason
no no that's what we're doing that for
that so you're going to build a compiler
of your own yeah to avoid having to
invoke somebody else's absolutely okay
you could invoke somebody else's that
would be fine to that well that would be
fine as well yeah it's it's just that
we're choosing to basically collapse as
much information as we can into a single
cold cause refusing to implement your
own compiler you're losing out although
you know 29 years or twenty two hundred
nine years of FL all right ok so maybe
they'd be like so maybe i said something
that it's not true right so uh so we are
using a compiler we are generating this
code this C code and then we're fading
that code to a compiler but you are ok
right it's just that we are trying to
inline as much code as possible so the
compiler doesn't have to reverse
engineer all this information yeah so my
experience from Stewart something is
that it's a bit unpredictable when the
compiler is going to doing lying you and
and you know the more continuous code
you expose him the more it's their
locations gonna do so that was our
experience as well but it was our
experience with GCC right so I don't
know what other compilers might be doing
yeah so it seems to me that you know you
can feed all those hints or you can try
to make the compilers life was or you
can ask the compiler to do also
compiler optimizations for you but
unless you lay out the code in some way
that it can easily figure out that these
compilations are applicable it's not
going to do them because it tries to
avoid basically mistakes potential
mistakes so so just things to note here
is that the fact that we have nested
loops all over the place and nested
loops are good as far as the compiler go
so again large blocks of code that the
compiler can optimize in one go fixed
strides so we don't really if we have
information about our records instead of
making function calls to extract
attributes from records we can just
directly point to the correct location
and a record the references and always
have fixed rides when we're accessing
the various attributes so and so forth
and type specific computation if we know
that they're integers why make a
function call to compare integers we
might as well use integer comparison so
these are all things that we can afford
to do just because we're doing things at
the run but just because we're
generating code for the actual query so
that was work that we did for SQL and a
few years ago we started boarding this
idea to link effectively so the classic
setup in the thing on disk right so this
looks attractive is everybody doing it
now um well you guys hopefully you guys
are Microsoft is doing like we are so
that's cool oh yeah so now everybody did
we do each other the style yeah well you
know love well okay so you do interpret
for dynamic queries but you're using
cogeneration for stored procedures
packet and uses stored cogeneration i
know that SI p uses it for all queries
there are some research prototypes that
actually do it for all sorts of queries
all sorts of dynamic queries but yeah
sorry I using a traditional paraplanner
yeah so it's cardinality based it's a
card another vehicle model yes it does
that lead to complications because it
might be making decisions based on
intermediate cardinality ever just go
away absolutely yeah it does okay it is
but we haven't so if you pick your
workload well now
you don't run into this problem so if
you're using something like t PCH which
is completely predictable then yeah you
don't really have these problems but if
you have a completely dynamic workload
where your value distributions
frequently change then yeah you might
run into problems so the thing that
we're doing just now is that or we've
been doing for the past few years is
that we've been porting this idea to
link right so I don't have to tell you
how link works but just to just as an
illustration you again have you the
application you have a query engine and
the data store that exchange information
in terms of innumerable collections and
what you do is that you have providers
that basically take some link
representation and they map it to the
representation that the data store uses
and one of the standard providers for
for link is linked to objects and the
idea there is that you have in memory
collections you know standard lists or
something along those lines and you
perform queries using link as the
description language for as the query
language for over those collections so
you're not really relying on a
relational engine or anything of the
sort you're taking your in-memory
collections and querying them on the fly
so what a link to objects codes looks
like is a little bit like this so this
is a typical example from a schema that
resembles to PCH actually so what you do
is that you have a list of orders you
populate it in some way and then you
expect you you express some query
statements over that list of orders
where you iterate over it and you only
filter out certain data for which the
order date is greater than the first of
January 1999 and for that you assign
some kind of discount okay so what you
would do then is that this is just a
specification of the query so then you
need to consume the results of that
query through some iteration so you're
saying for each variable in the query
statement so for every binding for that
iterator variable consume the query
result
so so this is as I said this is just the
Declaration of the statements and just
because you have the declaration doesn't
mean that you're going to execute it it
just means that you have a
representation for it so only when you
start iterating over the result do you
start executing that query so what
happens now is that you have a situation
similar to the one that I was describing
by you know gestures a few minutes ago
so the idea now is that you still have
your orders storage layer let's say and
then you need to have multiple iterators
basically iterate over that storage
layer to consume the result applying
some kind of medical minimal computation
at each step okay so first you apply the
word operator and you just consume your
input filter it and only retrieve the
the records of that input that's have I
have an order date greater than some
threshold and then from those you apply
some second level of computation and
that's how how things go right so this
is the equivalent code on the top left
corner so for each records in some
source if the predicate is true yield
that result okay so so what we do then
is that we're saying if we know that
this is what the query is going to be
doing we might as well generate a
different kind of code for that not
necessarily see codes but just c sharp
go to begin with so we can start
collapsing these computations into a
single block construct so instead of
having two operations we might as well
generate a single block of code that
performs both computations on the fly
surprisingly now if this is not
something that link to objects did okay
so they were actually going through all
the motions of running multiple multiple
operators at a time so the architecture
again is similar to what we had before
so you still have a query tree you have
a code tree so some representation of
the innumerable xand operators over over
the in-memory collections and then you
can generate either c-sharp code or C
codes we'll see
it well once you can generate the
shortcode going to see code is not as
straightforward as you would expect but
it's possible as we will see once you
have a c-sharp codes or C code
representation of the same query you can
convert it into a dilla dll dynamically
linked library loaded dynamically run it
and get the results what we also have is
a query cache so the first time that we
see a query week a if we see it
again we don't have to go through this
song and dance of free optimizing it and
regenerating code for it and so this
seems to be working just for standard
conversion from of link queries to
c-sharp code yeah and we're invoking the
compiler of the seashore compiler from
from the runtime thank you it's probably
much more directly you can use yeah it's
just that at some point you need to look
at the sister source code C sharp source
code to figure out that what you're
doing is correct right so that's the
only reason why we're doing so so the
bad news is that if we have this kind of
representation that we're sort of
limited by the performance of C sharp
that doesn't mean that C sharp is a bad
language it just means that it's not
optimized for database query processing
so a typical example again would be
something like a collection or a
collection of Records whereas if you
expect those records to be laid out one
after the other in a main memory
database system this is not the case now
so you have this references that
potentially point all over the place and
that means that any notion of locality
can be sort of you kissed away at that
point so what we want to do is have
something that resembles the database
memory layouts which is something that
we can do things which is something that
we can process in a much better way so
what we would like to do is effectively
use a value type representation of our
records now if our records are
structures that's fine because that's
exactly what's the shop does so if we
have value type representation that
means that
these records are actually stored as a
contiguous memory main memory region so
then if we generate see codes that
actually iterates over that region
that's it we can show this what is that
thing you're saying yeah yeah absolutely
Drexel will be contiguous but the bid on
the right all the records of this
relation also particular Saturday yeah
absolutely that's a separate matter like
dad's us have we are you already talking
about the little horizontal about guys
you one of those you want those pika do
so you want the horizontal guys to be
continuous in main memory yeah yeah one
record they haven't gone to it on yet it
moves the successive records right so
your rights or so i'll still
contiguously that so you have a database
but not a seashell rights or so
attributes and records need to be
contiguous in main memory so if you're
talking about a structure right then the
attributes of that the fields of that
structure will be contiguous in main
memory right and if you've allocated a
large number of those structures than
all of those records all of those
instances of those structures will be
contiguous in main memory as well means
he shall be with us judges you'd have to
sort of allocated some kind of available
you gimme yeah so if if the right so if
the in-memory collection is an array of
structures right and you've already
allocated it then that makes sense right
you don't know the size of the array you
true but you can be dynamic in that way
if you're willing to pay some extra
penalty of having a dynamic array you
can get away with it right so if it is
indeed the case that we have structures
and an array of structures and we've
already allocated all those structures
then we can effectively reuse our
technique for converting SQL to see you
know modulo linked to perform to do
things in the in the same way at the end
of the day now the problem is if you
actually have an array list or some
collection of objects that's really what
makes things complicated so what we do
in those so this
this is the notion of saying that you
can have some kind of seed sharp level
cogeneration and then for the heavy
lifting of the query you can so for
instance things like join processing or
aggregation you can use sea level
cogeneration combine the two into a
single construct and somehow manage to
get the best of both worlds in terms of
performance the reason why you would
need to have both C sharp and C even in
that case is that for certain types of
processing something like a filtering
operation it's a bit it's a bit too much
trouble to actually generate C code for
that you're going to be spending most of
your time generating the C code as
opposed to evaluating the query so you
want to stage your computation in that
way so what if the input is now not
structures or arrays of structures but
you actually have collections of objects
so so the difference there is that you
need to have some way of converting
those if you want to use C C code to
evaluate the query you want to have some
way of converting your in-memory
collections to arrays of structures and
what we did was some build some
substrate that would allow us to
exchange information between SI SE
runtime and a c-sharp runtime using
fixed buffers that actually take care of
the translation between C sharp types
and see structures and then we can apply
again our techniques for cogeneration
over C constructs to value the queries
so what staging looks like it is the
fact that you first need to know what
data is relevant and what well what
records are relevant and what attributes
of those records are relevant so if you
have that information then you can sort
of copy all the records all the
information that you need from your
in-memory collection on 2 AC like array
of structures and once you have that you
can again apply your code generation or
our code generation techniques and if
you do that this is one of the only
slides of results that i'm going to show
you
so these are t PCH queries and what we
have is the normalized execution of this
mixed runtime in comparison to the
standard linked to objects runtime so
linked to objects is the blue line
compile C sharp is the red line so just
by compiling our queries down to C sharp
we get something like well in the best
case are seventy seventy percent
improvement on average we get about
forty percent performance improvement so
that these are queries two and three if
we have anticipated the fact that will
be performing query processing over some
collection of structures and we have
laid out our data structures that way
then we move on to the green line to the
green bar so for that we have really
dramatic performance improvements so we
can get up to ninety percent performance
improvements on all those three queries
if we manage to have laid out our
objects or our data in some way that is
conducive to processing with C but if
we're not that lucky we're actually have
some technique that allows us to copy
data from the c-sharp runtime to a
c-like representation and then apply
cogeneration using this mixed mode query
processing and in the best case again
have a ninety percent performance
improvement but on the average and q2 is
a typical performance improvement over
all the queries that we've tested gets
up to something like sixty to seventy
percent performance improvement yeah
these are sort of full scans right mmm
food scares and aggregations and some
joint processes as well okay if you're
looking an index lookup do you do that
in the c-sharp will receive no no no so
we assume that we don't have any indexes
so these are all standard scans right so
of course if you have something like an
index and you perform a single look up
then no run time is actually going to
save you right so it's not as if doing a
look up and see is going to be that much
more effective than doing a liquor look
up in c-sharp so the assumption here and
then no and the majority of t PCH
queries are scanning queries so they
assume that you're doing analytics as
opposed to
point look ups or something along those
lines like stay no or the link optimizer
from Essos so we have right so so steno
is one of the things that we tried to
compare it with we surprisingly enough
we didn't have the implementation of
that but they're doing things that are
fairly close to what we're doing but
they're not really targeting query
performance they're mostly dealing with
representing things at the c-sharp level
in a better way and I could be
completely wrong on this one but our
impression was that the you know
generating going all the way down to see
just because you wanted to get as much
performance as you could out of the
system is not something that was one of
the primary goals but I could be
completely wrong on this one and we
didn't have access to the implementation
was the thing assistant called link
optimizer but I consultancy called meses
that I should take a look at that I wish
it were another another project would be
something like Harry but again they're
targeting different parts of the query
pipeliners closure reduces as well as
bad I am aware of those but you know we
having to play the comparison with dry
add link because obviously you know it
wasn't us in the mode they was thinking
about the MapReduce style world but
obviously it was thinking mode efficient
and very large data battle and running
queries over that right and it did a lot
of transformations and abetting C code
and c-sharp code and recompile girls and
loading the DLLs and distributing into
different machines and so on so it's a
it seems like a more obvious starting
point than link to objects absolutely so
one of the things that we're looking at
just now is basically using more
expressive workflow languages as opposed
to SQL and then we will be comparing two
dried link because we're going to be
mapping quite a bit of our processing to
MapReduce workflows and this is the
obvious choice so over here we're just
saying you know in the worst oil in the
typical case you have a developer who
has their own data structures and
they're going to be using link to
objects to
begin with so how can we improve that
okay but you're absolutely right that
dryads link is a another comparison hook
back we need to make computation so in
your sage computation you're taking
essentially a managed object and you are
extracting the fields that are
interesting from it and representing
those in unmanaged structure to make it
efficient if you considered just simply
doing the opposite so we'll keep all the
data on managed and use the dotnet
property style API is to make thick
objects in the rare cases where it
doesn't matter when you're going to
inefficiently access the object from the
seashore world so we haven't thought of
it it's definitely something that's
where we're looking into just now so one
of the things that we're doing just now
is taking this approach all the way to
to one extreme and saying that
everything is just unmanaged memory as
far as we're concerned and we have a
wrong implementation of collections that
allows us to perform these computations
as quickly as possible actually pop
across it and stage when you actually
execute the query do you iterate over
that data many times or something
because it seems just intuitively I mean
once you go to the effort of copying
then all the the nonlocality you mean
you've paid all that price like all
those catfishes you have to the FTE you
have to pay those but you're just a
physical yeah but you can mask some of
those differences right so what we what
we do is that we don't cooperate in one
go and we have multiple threads some
kind of producer-consumer analogies so
we generate chunks of data so that's our
c runtime can process those chunks while
at the same time we're copying other
data and if this if this is a
long-running computation then in the end
we'll end up winning where the produces
is just that much better even if you
already have to go over there
and but by the way this is not just for
C sharp right so Java sucks even more so
but you probably know that already so I
don't think I have too much time
actually they go through the second part
oh sorry there's a question first so
which part a the link bar tower sequel
to see both I don't compare all the
runtime equator to the evaluation right
so if you if you if you give me a slow
enough compiler I'll give you more data
right so then I'll win again but that
that's a great observation it's in some
cases if you're talking about some query
that only runs you know in a in a few
milliseconds and you have all the
compiler optimizations turned on you
might actually lose so you might be
spending most of your time just
compiling the query but the
counter-argument to that is this is a
one-off costs so you only have to pay
the first time that you see the query if
you have recurring queries that's why we
have this query cache you can just
invoke the execution managed code so i
could come in we do the home instead of
going always see just using jit compiler
um so we yes and no the reason why we
can do that is we don't have access to
dotnet right so we well we can't
generate a dotnet level code CLR right
externally completely externally or at
least when we started this job with this
work we didn't we couldn't now no again
but this is from two years ago really
right so this is worth it started two
years ago and at that point no no
dissolve in libraries for rock island
code on the fly and don't know either
using reflection or right everyone but
like remix by code as you go along my
code as well that's that's pretty much
what we're doing right
we're talking about well okay so we have
we are invoking the both the c-sharp
compiler that generates my code right
and the c compiler if we have to do
either of those you just generate by by
the memory right so so we haven't done
that I haven't looked at that so more
questions so should I wrap things up
because yeah okay so i can so i can give
you basically so if there's a single
thing to take away from from the second
part of the talk is that i'm not going
to be talking about is this notion of
what we call the right limited algorithm
so so imagine that you have you know
non-volatile memory and the key premise
is that updating takes longer than
reading so writing is more expensive
than reading and let's assume that you
know there you can sort of quantify that
that ratio so let's say that writing is
10 times for instance more expensive
than reading what that means is that in
the same amount of time that I need to
generate an intermediate result which is
something that we frequently do in
database systems I can reread the input
that many times so if right to read the
right to read ratio is equal to 10 that
means that in the same time that I need
to generate some partition or sort my
input I can read it 10 more times okay
so the Assumption based on that
assumption we have a whole bunch of
family of algorithms that allow us to
trade rates for rights dynamically and
what we do is that through so i'll skip
the algorithms through some runtime we
threw some well-defined api it turns out
that you can express all data database
query processing algorithms using that
API and it all comes down to five calls
basically partitioning merging splitting
and inputs filtering and input so and so
forth but what you do is that in a
typical children
fashion is that once you have expressed
your code using that API you don't
actually compile it you just record the
workflow of that of that code and
whenever you need to access one of those
collections that you generate on the fly
you can make a decision whether you can
whether you want to materialize it so
let's take something like t0 over here
we know that in order to materialize t0
we need to scan T and partition it using
some hash function that that's what the
workflow the control flow graph
basically gives us right so now I have
an option I can either materialize it or
I can choose to rescan t a much larger
input and apply the computation to
reconstruct it so what the runtime does
is that it dynamically keeps track of
the reads and writes over each
collection and it decides whether it
needs to materialize it or not so yeah
we will track the accumulated number of
cache line reads and writes and decides
based on the operation whether we can
that whether we need to materialize
whether it makes more it is of more
benefit to actually materialized that
collection or we can afford to
reconstruct it from its input and we can
do that at all levels of a query plan
effectively so for instance if you want
to generate the final plan you can
replay the entire trace of computation
starting from the end of the primary the
principal tables in order to in order to
compute that so we've also tests it's
quite a few implementation until
alternatives in terms of exporting and
vm exposing nvm to to the system so the
tip the first would be something like a
ramdisk a persistent memory file system
by Intel having our own I think a
standard dynamic array representation
where you assume that you're using a
standard memory allocator and then
you're accessing or jury you're just
using non-volatile memory a standard
dram or you can have an optimized memory
allocator which is the last
representation over there that is a
memory allocator aware of the fact that
it's allocating data over
non-volatile memory so it generates
chunks of it returns chunks of memory
that are a lot better laid out for the
various types of algorithm so the goal
of this work was not so much to show
that you know you can have the best
algorithms you can think of or the most
high performing algorithms you can think
the highest performing algorithms you
can think of when you're operating over
a persistent memory it was to show that
you can choose the right intensity of
the algorithms without compromising
performance so what this complicated
graph basically gives you and you have
your buffer memory size on the x-axis
and performance basically on the y-axis
is the fact that you can have you can
take your standard you know really
optimized external sorting algorithm if
you're sorting data and you can get the
same performance at twenty percent of
the right cost of that algorithm because
you now have a runtime that allows you
to dynamically keep track of your reads
and writes so you can choose when you
want to exchange trade some rights for
extra reads or if you are not really
interested in you know leveling out the
wear of your device so you can afford to
perform acts or rights you can then get
some more performance that way and the
fact is that regardless of what sort of
right intensity the developer or the
programmer wants to have you can still
get decent performance you can still get
as good performance as you would have
gotten if you were only optimizing for
are you pipelines process my plans are
relatively shallow yeah because you
don't memorize too much not much so it's
a combination of memorization basically
and dynamic cost-benefit analysis and
the fact that you know your work clothes
are not going to be I don't know I'm not
going to be spanning a few you know
megabytes of memory to or gigabytes of
memory to traverse them that helps as
well
so right so ongoing work so this is
something that I alluded to at the
beginning so what we're doing just now
is that we're saying that if you really
want to do things correctly then you
should have a chunk of on manage memory
managed as a database system really and
you need to have your own memory
collections exposed to the developer and
then all those nice properties that you
rely on to get the performance that you
want in cogeneration for link are
actually taken care of from the
beginning other things that we've been
toying around with is you know things
like instead of having an array of
structures having a structure of arrays
so vertical partitioning effectively so
we can do all sorts of things on the fly
if we are allocating memory ourselves
and finally data placement for
non-volatile memory the notion now is
that you have some kind of hybrid system
where you have you know your your DRAM
and your persistent memory your
non-volatile part you have your still
managing collections and what you need
to decide is what to place where so you
can have some intermediate result that
you know is only going to be access to
one time so something like in a
partition or some intermediate run when
you're doing external sorting so even
though you're accessing it as a file you
might choose to put it in diagram just
because you know you're only going to be
accessing it once and throwing it away
afterwards so so just to summarize the
entire notion is that large memories
mean that processing is now going to be
becoming even more memory bound the fact
that we have non-volatile memory coming
into the picture and unfortunately this
is not something that I talked about too
much means that we need to set up our
data structures in some way that
guarantee persistence and need to be
accessed and need to be accessing your
non-volatile way and we also need to
change our algorithm so that they make
as much use of the characteristics of
non-volatile memory as possible so what
we're proposing is management's approach
at all levels so having decisions being
taken at memory allocation time about
how to lay things out and during run
time having decisions in terms of what
to place where and what sort of code you
need to run in order to evaluate queries
in our case as efficiently as possible
and so I'll just so acknowledgments the
students did all the work I'm just doing
the talking and collaborators Gavin
baron who used to be here Marcelo
central Alexander from now on and Vijay
back at Edinburgh and you know this is
the shameless plug so if you know of any
good PhD students who are interested in
doing a PhD either in data science or
parallelism please ask them to send them
to send me an email and I'll just leave
the summary here and thank you so you
have you more rain Adele a minute for a
question I have a comment it's nice to
think of all these al SQL queries being
generated by JavaScript program and then
going through your analysis to undo all
the bad work they've done before that's
so another that's a very good
observation it doesn't necessarily need
to be you know JavaScript it could be
any your favorites or least favorite
scripting language but the the
observation for this work was that you
know we looked at the use cases of
database systems and then the vast
majority of cases you have some poor guy
typing in information in a form right
and then that gets translated into an
SQL query so in in the vast majority of
cases your SQL is really fixed so that's
why cogeneration actually starts making
a lot more sense and of course you can
undo all the bad effects of bad
programmers with still thanks figures
place thing to speak Lansing thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>