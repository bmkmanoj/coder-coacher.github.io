<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Sampling from Probabilistic Submodular Models | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Sampling from Probabilistic Submodular Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Sampling from Probabilistic Submodular Models</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kRgCUp33uLs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay welcome back we've got two great
talks the first talk is on something
from probabilistic sub modular models
it's by al kiss go to verse hammered
Hasani and andreas Krause and al case is
going to be giving the talk thanks ok
thank you hi everyone so my name is Alec
is go to us and I'm going to talk about
sampling from privilege six of modular
models so this is a joint work with
Hamid Hasani and Andres kasi at ETH
Zurich so first I want to motivate these
kinds of models so let me start with an
example application so imagine you have
a big collection of images and you want
to summarize it so you want to select a
few representative images from this
collection one objective that has been
used for this task is facility location
and I want to go through this in detail
here but on high level it encourages a
combination of covering the image
collection as well as having a diverse
summary so if you call the set of images
V then for any summaries of wherein
subset s of V this function would give
you a score f of s now interestingly if
you look at adding a new image so the
benefit of adding new image to an
existing summary want here and compare
this to the benefit of having the same
image to a superset of the original
summary then according to this objective
the first benefit is always greater than
the second one so this intuitive
probability of diminishing returns is
called sub modularity and sub modular
functions have been used to model
notions such as coverage and diversity
while their counterparts super modular
functions have been used to model
notions such as smoothness and
cooperation and of course the problem
both problems of algorithmic minimizing
and maximizing some modular functions
are extensively studied however there is
very little existing work on actually
defining probabilistic models using some
modular and super modular functions and
here's an example of how that might be
helpful so rather than just optimizing
the function and getting one single
summary what you can do if you have a
probabilistic model is just sample from
this model get a bunch of summaries and
then let the users select the one that
suits his or her needs so we would like
to define for basic models using some
modular
super modular functions but also there
are standard or realistic models that
have been used for some tasks in the
past so here is another example
application which is about segmenting
and imaging the foreground and
background so ideally you would like to
get something like this year so black is
background why this foreground and
because it's really hard people have
been looking to actually using
probabilistic models to get the
probability of each pixel being assigned
to foreground or background so this
yellow and red regions are the in
certain regions about the labeling and
of course once the newer model people
have been looking at it just taking
damage constructing a grid of pixels
connecting the neighbors with edges or
with pairwise potentials and then if you
call the set of all pixels V you can get
the probability distribution over
subsets of foreground pixels where the
probability of a set of a subset s is
proportional to e to the sum of all
these pairwise potentials now what it's
interesting to look at is also if you
can use some richer information of the
image rather than these pairwise
potential to get better results so one
such example of what is called what are
called higher order models and comes
from for example using super pixels so
you first acquire this higher this
colored regions here the super pixels
and then if you look at one of them what
you can do is define a higher order
potential I here and if you use as Phi
say a convex function what this encodes
is the tendency of all pixels in the
super pixel to be assigned the same
label so if all our programs are all our
background you get the value of Phi max
otherwise you get much lower body and
using this then you can define a new
distribution which is a proportional to
e to the sum of all these super pixel
potentials and for example from some
recent work by jalanan krauser you can
see that this higher all the potentials
actually achieve this much much crisper
probabilistic labeling than pairwise
ones so to summarize my two major points
up to now on the one hand we would like
to go beyond sub-module optimization and
actually use of modular functions to
define for ballistic models and on the
other hand we would like to take
existing probabilistic models and equip
them with higher order interactions so
both of these points lead us to consider
a class of models that we call probably
six or modular models or PSMs for short
and they are distributions over subsets
of the ground set so
the probability of a subset s is
proportional to e to f of X where F is a
modular super modular function and Z
here is just a normalization constant so
let me make a quick connection to Markov
random fields because you're probably
all much more familiar with those so as
I said a PS MSR defines the ground set V
of size n as a modular super modular
function f and then we get the
distribution of a subset so you can see
the same thing equivalently as an mrf by
defining a binary random vector X of
length n and then you have a set of
factors fly I each of them with scope CI
so CI here is the eyes click in the MRF
and then what you get is the
distribution of a binary vectors where
the probability of a vector X
proportional to e to the sum of all
these factors so when I talk about model
order here what I'm referring to is the
maximum clique size of the corresponding
mrf now let me also quickly show you how
to spread the class / ballistics and
modular models generalize and many well
studied models of the past so i'll start
with product distributions here which
are just distributions over independent
sets of random variables next we have
attractive pairwise mrf so for example
isaac model spots models etc likely a
segmentation example that I showed you
before their counterpart are repulsive
pairwise mo revs and which encode
notions of diversity another class of
models that we used to encode diversity
are detrimental point processes and now
log sub modular distributions generalize
this diversity musing part of this model
space here and their counterpart lock
super modular distributions generalize
attractive models so in the end the
union of these two classes we call /
ballistics and modular models now of
course what we're interested in doing in
these models is inference so compute
various marginal probabilities say the
probability of a pixel being assigned
forget to foreground or background or
conditional probabilities for example
the probability of an image being
included in our summary given that we
have already selected a few images to be
in a partial summary so of course people
have been doing various kinds of
inference in the past so we know that
exact inference is tractable only for
limited subclasses say product
distributions DPP's restructured MRFs
etc but there it's really hard
even for a simple Isaac models now lower
during my revs also a model class that
people have extensively studied and
they've come up with algorithms such as
liberal belief propagation mean field
etc but all of them are actually
exponential in the model order so they
are still not feasible for general psms
now in last year's nips jalan krauser
were the first consider the general
class of PSMs and proposed a variational
approach for inference in this class of
models so what we're asking here is
whether the usual alternative to all
these deterministic inference methods
which is of course sampling and is
feasible psms and what we can say about
that in particular we're interested in a
Markov chain Monte Carlo a sampling so
as you might already know the idea here
is that we have a state space Omega and
a transition matrix P with corresponding
station of distribution pi and what we
want to do is define a Markov chain as T
that moves that makes local moves
according to the side transition matrix
P in our case the state phase will be
the power set of B and we call it will
construct the solution matrix so that
corresponds to the gibbs sampler and
then of course the stationary
distribution that we are interested here
is the PSM distribution so for a simple
for a small grant set of size 3 here
this is how our states which would look
like so just all any subsets of the
ground set and let me quickly show you
how the gibbs sampler would work here so
you start from an arbitrary state s 0
and then let's say you have done T
iterations and you've ended up at state
and the single don't stay to hear so the
first step is to select a random element
from the ground set so here we would
have three choices either one two or
three which correspond to three
potential transitions that we can make
so say here we select V equals three and
then the next step is to compute the
conditional probability of adding or
removing this element to a current set
and the final step is to actually flip a
biased coin and make the transition or
just stay at the same state as before so
if you look at all possible transitions
in the state space you get just this
subset lattice here according to the
gibbs sampler and of course this is also
as i told you before equivalent to just
a binary hypercube of dimension equal to
the size of the graph set now of course
what you want to ask about this
markov chain and the local moves that it
makes is whether it converges to the
stationary distribution and this is
usually measured by looking at the total
variation distance between the
probability of the markov chain and the
stationary distribution in particular we
look at the maximum of that over all
possible starting states so it is well
known that under some mild assumptions
that is a goat icity the stationary this
deliberation distance goes to zero
asymptotically so the more interesting
question here is actually how fast as
this converge and this is measured by
looking the mixing time so this is just
the minimum amount of steps that we need
to get epsilon close to the stationary
distribution so unfortunately mixing
times for general PSMs are exponential
in the size of the ground set and in
particular the exponential even for
general pairwise models as was shown by
Jeremy and Sinclair 93 so what we are
doing in this paper is established some
sufficient conditions that guarantee
subjects financial mixing times of the
gibbs sampler on psms so to get our
first result I need to show you a couple
of definitions first so we call a
function reset function f modular if
this equality here is satisfied for
every subset a and B of the ground set
and we call it sub modular if the
left-hand side is greater and super
modular if it's the other way around so
now using this we can look at the amount
by which this inequality is violated by
a general function f and just take the
maximum of this over all possible
subsets a and B and we call this
quantities that I have here the distance
from modularity now secondly you can
take any sub modular super modular
function and decompose it into a
constant term and modular term and a
normalized monotone sub modular term and
if you look at what this means in terms
of the distribution it just means that
the PSM can be written and as a factor
of a product term and an interaction
term so a term that contains all
interactions between elements ok now we
can get to our first theorem of the
paper where we use both these things so
what we say here is that for any PSM
which is defined through a sub module or
super modular function f we get a bound
in the mixing time which is quadratic in
the size of the ground set so we have
this n squared term here but it's
exponential in the zetas f so this
distance from modularity and not the
here that the self is the lower case so
we just care about the monotone
now you might think that if it is that i
have actually increases in n then this
exponential term might blow up and this
might be indeed the case for some
examples but there are also many
interesting cases where this that I've
is say constant in n so let me get back
to our super pixel example here remember
we have one factor one potential by
super pixel and then our function is
just the sum of all these super PACs of
potentials so it's easy to show that in
this case our distance modularity so the
zetas f is bounded by L so the number of
super pixels at times Phi max so the
amount of smoothness that we're trying
to impose and note you that this L might
be around 50 say for this image here so
if you are just fine max you can make
this exponential term be fairly small
but if you were to actually and see this
as an mrf then you would get model
orders that are say 10,000 or so so the
cliq size would be around 10,000 for the
largest super pixel now let me also
quickly show you proof outline of our
theorem here just a high level picture
so we use a method called a method of
canonical paths and i'll use your a
slightly larger state space of grounds
at size 4 so the idea is to set up at
low routing problem in this state space
graph so here with different colors i
show the probability of each state
according to the PSM distribution and
the what we want to do is for every pair
of states we want to route an amount of
flow equal to PA times PB from each
state a to each other state be so the
first step is to to construct what is
called a canonical path between each
ordered pair of states so we will use
this path then to route flow from A to B
for example here what we could do is
just add or remove elements in numerical
order for example starting with a we
would add element 1 remove element to
add element 3 remove element 4 and we
are at be now imagine doing this for
every pair of state in the state space
the second step is to define a capacity
of each edge which will be proportional
to the probability of making this
transition that the edge corresponds to
according to the gift sampler so here I
show this with different widths and on
the edges and finally we can define a
quantity called congestion of an edge
which is just the total flow that goes
through the edge according to our
canonical pad construction / the
capacity of
the edge so here I showed this with
different colors now the reason we care
about this congestion quantity is
because a theorem by sinclair in 92
shows that you can bound the mixing time
of the keep sampler by bounding the
maximum congestion among all edges
announced a state with graph so what we
do in this paper is bound this maximum
congestion of a PSM using this quantity
this distance from modularity is at the
f now let me quickly go through our
second theorem and I don't want to get
into detail here but roughly speaking
this theorem gives us a better bounds in
the mixing time but under stricter
conditions or under special structure of
the PSM so he will use a different
quantity this gamma F which we call
maximum total influence and what the
serum theorem says is that if this
quantity gamma F is smaller than 1 then
we get a mixing time which has a
dependence n log n on the size of the
ground set and there's also this 1 over
1 minus gamma F dependence in front we
also show in the paper a simple way to
bound gamma F if the corresponding
function f can be decomposed into a sum
of simpler functions now for people who
are familiar with these concepts this
kind of theorem is closely related to
what are called the Machine conditions
for uniqueness of Gibbs measures and
also closely related to influence
matrices and their norms furthermore and
Erebus skinny and carboxy in
contemporary work two hours and show the
similar theorem which is not directly
comparable but still has a similar
flavor to to this theorem here so
finally let me show you some
experimental evaluation so our goal here
is to compare to the against variational
approach of jalanga and crowded from
last year so we use the exact same
models that they used in their paper two
of them are log super modular models and
one of them is log super modular so here
I'll concentrate just on the third model
which is an e to the facility location
model so what we do is compute marginal
our conditional probabilities of this
form here where s starts from an empty
set and iteratively increases so we
conditioned on more and more elements
also we constrain the ground set size to
be at
was 20 so we can actually compare
compute exact marginals and compared to
them so in this plot I'll show you on
the x-axis the percentage of condition
elements so the set of the and now the
size of a set s and on the y axis we
have the mean absolute error compared to
the exact marginals so if you look at
the two flavors that the variational
approach comment comes in you see that
both of them start with a relatively
higher and as we conditioned on more and
more elements the error decreases if you
look at just a hundred gives iterations
you see that we get fairly consistent
error across all conditioning regimes
and if you increase the number of keep
situations this error gets lower and
lower so you see that we can beat the
variational methods across most
conditioning acceptable condition on
almost all elements of the ground set so
to sum up we've looked at the class of
models called ballistics and modular
models which generalize and fu well
studied subclass to the past and we have
shown conditions under which we get fast
mixing times of the kips sampler and
these models so we think this gives us a
principled way to actually identify
higher-order classes of models that are
amenable to efficient inference and also
we have the first indications that sub
or super modularity I can actually lead
to faster mixing times so for more
discussion please come to our poster and
number 70 thanks
so do we have any questions for elkus
so so there are other ways of measuring
distance from modularity besides your
gamma F and EMF is hard to compute and
they're things like curvature if you
actually thought about for example other
more computable ways of measuring
distance from modularity so okay so
first of all this quantity that we have
defined you might be able to analyze for
some cases and actually get some simpler
way of computing it now in terms of
curvature it doesn't directly come into
this results here because some of the
way we define the probabilistic some
modular models give us additive
distances from modularity whereas
curvature itself somehow is has a
division in there right and this doesn't
come up if you look now if you look at
your kind of models that you had
actually be fine so the the sub modular
point processes there you might actually
get something that might be related to
curvature so that might be an
interesting direction to look at another
question
so I actually have a question the cases
that you looked at you assumed a finite
set of indicators but for some of the
examples in your set for example the
terminal point Percy's you can have an
unbounded number have you looked at
whether you can construct similar
guarantees without a finite state space
oh no our whole formulation somehow
depends on some modular functions which
are at least as far as I know usually
defined on finite grant sets so no we
haven't at least not yet lift up these
kings one more question
just a just one quick question I think I
missed something so what I'd like to
understand is it seems to me that this
is very similar to a covering problem in
which if I select a node then i can't
select any of its neighbors and that's
like in an Ising model those are really
hard problems to solve so I'm not really
sure how how you're doing the inference
in such a way that you can get anywhere
because it's like a odd cycle you know
right okay so yes I agree they are in
general very hard problems and that's
what I meant when I said that in general
you get exponential mixing times right
so what we're looking at is which cases
or which subclasses of these models you
can actually do efficient infer insane
so we want some a current sum sum
theorem that says that if this holds
then you actually get efficient
influence so we're not claiming that for
all such models you can get efficient
inference of course this is very well
known to be a very hard problem in
general okay thank you so much sure okay
let's thank helices again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>