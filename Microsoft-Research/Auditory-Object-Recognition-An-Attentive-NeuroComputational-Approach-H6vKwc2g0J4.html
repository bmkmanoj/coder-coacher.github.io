<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Auditory Object Recognition: An Attentive Neuro-Computational Approach | Coder Coacher - Coaching Coders</title><meta content="Auditory Object Recognition: An Attentive Neuro-Computational Approach - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Auditory Object Recognition: An Attentive Neuro-Computational Approach</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H6vKwc2g0J4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
today we have chaos Patil who is a at
the end of his PhD studies in John's hop
Johns Hopkins University in Baltimore
and he's going to talk about auditory
object recognition without further ado
gallery have the floor do I need to be
standing here or I can work if you're
worse okay thank you all for coming
today today I will be talking about
auditory object recognition so this is a
brief outline of what I will be going
through our brief idea of what auditory
objects are what are the different kind
of representations for these auditory
objects finally after that I'll talk
about some statistical analysis where
did experiments to evaluate these
auditory object representations and I
would also like to talk about the
concept of attention in terms of our
auditory objects so what our auditory
objects auditory objects are in the
Oxford English Dictionary they are
defined as object in general is defined
as something that can be placed before
or presented to the eyes or senses so
you can see from this that the
definition or the identity of an object
is interlinked between the perception of
the object so there is no unique
identity of an object by itself it has
to be it is intertwined with the
perception of that object as well so you
can see why there could be difficulty
representing different of auditory
objects moreover these objects for
example consider this image so objects
can go from let's say for example one
particular instrument here or you can
consider this whole section of violence
or you can consider the whole image as
an auditory object oh sorry a visual
object
so this leads to difficulties in
representing different objects also this
another added a layer of variability
where let's say for visual objects you
can have different orientations for the
objects you can have different
manufacturers for these instruments can
add different lighting conditions and so
on so similarly for auditory objects you
can consider like different to different
instruments can produce different sounds
whereas the same instrument can produce
different kind of sounds depending on
the playing style for example of vibrato
versus a plane mode of playing so there
can be a lot of variations in auditory
objects as well so the main difficulty
comes from this temporal aspect of
auditory objects so consider this this
scene where there are three different
acoustic objects playing at the same
time but what happens here is you all
the three objects get overlapped and
that's what enters your ear so the
temporal overlap of these objects can be
a difficult thing but are we are able to
easily perceive the different
instruments present are different
objects of this acoustic scene
so I would now like to move on to the
kind of auditory object representations
I will give you a brief overview of what
is there and what we are proposing here
you can think of one simple way of
representing auditory objects is just
the waveform itself so that's the
topmost row here people have tried to
extract different features from the
waveform it could they've used a
features front leg zero crossing or
energy amplitude and so on another way
of representing these objects is the
spectrum level which is basically the
energy present at different frequencies
for that object and again people have
used different features at this level
like spectral flux or a spectral slope
and so on finally as a hybrid between
these two that's the bottom
representation here which is a
time-frequency representation and this
is the most standard technique used for
majority of the auditory object lip
analysis where people generally consider
a window of time slice of the
spectrogram and then derive different
features for each time slice and this
window is then shifted over time so you
get different representations which are
like ms ccs and LP cc's but what you can
notice is that these are these don't
capture all the variations and nuances
in the spectrogram and they just capture
whatever occurs in that small time
window so these may not be extremely
effective at capturing the
characteristics of an auditory object
also neurophysiological studies have
shown that in the auditory cortex
neurons are sensitive to various kinds
of features they can be not only just
frequency in time but they can be
sensitive to different kind of patterns
in this
program and you can have a sensitivity
to page you can have sensitivity to
harmonious City and so on so this
suggests that we should look at some
something which looks more than just a
short time window in the auditory
pathway there millions of connections
happening and these are the week all the
processes and just below the auditory
cortex all the processes until then we
call it as the subcortical processes and
the processes in the auditory cortex we
call it as the cortical processes so the
subcortical processes are well studied
and it has been well documented and well
modeled so there are models for the
different processes up till that stage
so in this talk will be mainly focusing
on how we can model or derive
inspiration from the processes in the
auditory cortex just to go over the
brief steps of modeling the subcortical
processes the incoming wave form is
analyzed on along a bank of filter back
Bank of asymmetric filters which are
uniformly placed on the logarithmic
access so what this does is it analyzes
the energy present in along different
frequencies and we then to sharpen these
filters we apply a lateral inhibition
that you can consider this as a
derivative along frequency so this
effectively sharpens the bandwidth of
the filters and finally there's a
smoothing operation which which mimics
the processing in the midbrain where
there's a lock of loss of phase locking
so we end up with something which is
similar to a spectrogram it's we call
this auditory spectrogram so here we are
going to talk about what we can do to
model the cortical processes so we came
up with a filter Bank which looks like
this so what this does is it analyzes
the spectrogram
for various different patterns so these
patterns look like this so what is going
to do is it's going to look for energy
Rises and energy falls and so on and we
are doing that at different scales are
different what we call it as modulation
so for example this one is very slow in
time whereas this one is extremely fast
in time so that these are capturing the
different modulations in time and
similarly you can see the same thing
along frequency so once we have this
Bank of filters we can what what this
effectively does is Maps this time
frequency into a high dimensional space
where the different axis could be this
spectral profile the temporal profile
time frequency and you can even think of
adding different feature dimensions here
like pitch and so on so one quick
motivation of why this could be a useful
representation so what we did here is we
have we collected many examples of cello
and flute notes and we we averaged the
representation so this is the average
representation for these two instruments
in that high dimensional space or the
cortical space and you can see that most
of the energy for cello lies in a
different space as compared to the flute
so this clearly shows that if you move
to this high dimension space the two are
auditory objects which are overlapping
in time are now really well separated in
this space so you can think of doing
multiple things in this domain
so now I will talk about how we are
going to and analyze the effectiveness
of such a representation for different
tasks so the first task I am considering
here is the speech recognition task then
I later talked about Tampa recognition
and so on so these are this reputation
this is the same as the
uh
kind of sharma Neiman as granny other
people are doing yeah it's aimed at
giving the word whatever you know when
we move from the cortical stage here is
different from their model so they have
filters which are modeled on the actual
neural responses whereas minus just to
generate 2d cover filter Bank so it's
it's similar you can go back well mostly
back so this is quite typical this just
says this is crucial yeah this is the
chakra manner okay in the next slide so
this is all is something which kind of
stuff to start over yeah
so how many features you can extract
with those filters how many of those
locals so if you want think about this
we have around 200 of these filters or
220 filters and so what that does is if
we have what we have multiple frequency
channels so is going to represent it
into a really high dimensional space
because of that so pretty much you apply
that filter on the spectrogram in there
the output is do we have something like
this in that particular frequency slaves
face yes so that it's going to the
output activity is going to show how
much of a match we have for this kind of
pattern in this spectrum so you have
four dimensions time frequency and
spectral and temporal yeah
so I'm still because I thought freezing
I'd like they had just one of the sort
of successful
so the difference is in the Paris sorry
so the differences in the
characterization of these filters so
they use the filters which are actually
modeling the neural field like the
neural responses whereas minus a generic
oh so the shape filter shape say oh
there is a yeah specific morrow or just
like oh yeah the motivations yeah so in
both cases you end up a representation
like that yeah 3 here yeah probably
ok
e
so one note on this when we are doing
some kind of statistical analysis a high
dimensional representation has its own
pros and cons for so what happens is a
machine learning or the back end stage
cannot handle effectively high
dimensional space so it's around the we
have like 28,000 features at the end so
it cannot effectively handle this many
feature so what we will have to do
something depending on what is the task
so for the speech we notice that
preserving this temporal information is
extremely crucial because if you want to
recognize a speech you need to have the
sequence of phones which are on the
order of like 100 milliseconds so thus
temporal characteristics is extremely
crucial so and also the machine learning
setup cannot handle a high number of
feature dimensions again so what we do
is instead of modeling these neurons as
a filter Bank we are going to model
whole group of neurons so by that what I
mean is so just to give you a motivation
look at the spectrogram here and what we
do is we take a cross section of the
spectrogram a long time and this is one
second in duration so you can see that
they're five distinct peaks in the
overall temporal profile and similarly
in the spectral profile you can count
there like four peaks in this pair in
the spectral profile so if you take a
two-dimensional FFT of the spectrogram
what's going to happen is all the energy
here is going to be localized because of
this characteristic so the main peak is
around five Hertz and in the spectral
domain it's below what we call as one
cycle per octave so this motivates that
instead of in this two-day FFT domain
instead of having multiple filters what
we can do is we can capture
the region of speech which is captured
the region which contains only speech
information so what we did then was we
have so to try which regions are most
effective to do speech recognition we
have we tried three different filters so
we call the suboptimal once p centric
and suboptimal to the actual filter
shapes are given here on the temporal
axis and the frequency axis the dotted
line is actual shape of the filter so
what we are testing here is how much of
this region we need to capture so the
results of these three filters is going
to tell us how much we need to capture
so the test that I'll give you the
details of the experiments the
spectrogram is around 128 is 128
dimensional so if we do that modulation
filtering we still have a 128
dimensional representation so then we
append the delta features on that so we
end up with a 500 dimensional feature
representation and we test it on a
phoneme recognition experiment on
dimmitt and during testing we are going
this is a mismatch condition so we are
training unclean and we are testing on
additive noise cases so only the test
set is corrupted with additive noise
from noise X database and for the back
end we have hybrid hmm MLP set up and
the hmm the mlps here are stacked so we
have 22 h mlps which are stacked so the
end we have around six layers of neurons
so the results of that experiment would
you can see here consider that if you
are just considering the clean case
obviously keeping the entire information
is the best possible case so if you are
trying to reduce the information you are
going to lose some of the some of the
performance but when you go to the noisy
case when you're adding different kinds
of
nice so what happens is if you keep the
whole space the noise all the noise is
included and you're going to take a huge
hit in performance but well when you are
going to when you are reducing the space
and trying to concentrate only on the
speech we find that the speech centric
filters is the best and one thing to
note here that suboptimal one is always
less than suboptimal to so suboptimal
one if you go back as is removing some
of this speech regions over here a
suboptimal tours including some of the
noise so it turns out that removing some
of the speech regions is more hurtful
than including little bit as much and
then we is that
okay yeah so we have so what we do is
the first MLP we just have so we take
these fight well features and we map it
to the phonemes and the second one we
include context on that so we have I
think ten frame context on that so we
are trying to include some yeah yeah
yeah then again predict can you return
you're both sides back when you show the
truth World War okay here so technically
what you do you gather at the entire
spectrum convert it into their own
relation to me apply the filter
converted better spectrum and then
basically filled out the regular of
speech from them yeah we can reach it as
the second to the speech enhancement
algorithm
because from the spec program I can't go
back to them blue vein and see kind of
the noise missing yeah beep so it's
little bit tricky to go back from the
this is the auditory spectrogram so we
try to go back from there and we like me
try to actually listen to the sound and
it wasn't that much obvious that it's
doing speech enhancement at the signal
level but you didn't listen good yeah
yeah yeah so how it sounds a little bit
noisier than you expect to or if you
compare just the the noisy
reconstruction versus this one we
couldn't find that much obvious
difference in the quality of the sound
yeah but also so that like the steps to
going back from Earth this auditory
spectrogram to this signal is little bit
of an estimation involved in that it's
not straightforward so I think use the
circle speech project which is 25
milliseconds per frame and then the
second step right yeah
so there's but there's something about
this approach that requires any of that
other representation I guess that's my
so it's not seemly you can expect around
do it to the enmity of it but yet do
something really I do yeah we are
considering only those modulations which
are important for speech so effectively
what the original representation I was
talking about is going to sample points
in this space and if we do that then we
are going to increase the feature
dimensionality so another way of
thinking of the same thing is where you
want to sample it and just extract that
region face unless deciding which of
those
no we don't pick those features because
the speech back-end cannot handle like
that much dimensionality so what we do
is instead of picking those points in
this region we pick the relevant region
in this space so we are not the
motivation is kind of the same but we
are not using that feature set so we
then compare this with two standard
approaches where we apply MBA on mfc CS
and pl peas and we are able to
outperform both of these standard
techniques on clean and all the
considered noise noise cases and just a
note that in our technique we used the
feature dimensions don't vary that much
between the baseline and our approach so
MBAs mean variance armor technique so
what that does is MA it combines
cepstral mean subtraction variance
normalization and some kind of ARMA
filtering which is similar to raster
filtering Elvis as he has been Alex
I species lived
no we compared it with some other
features which will I'll talk about a
little bit later but I don't know what
you have in mind hey dr. subtraction we
little filter like advanced funny yeah
I've be compared it with advancement and
is one of the difference correct go back
yeah that your system has an input that
has nine frames protect the future just
uh oh yeah yeah
chuckling sniffle builtin I mean it's
people usually do that and then reduce
the dimensionality
so so on average we see that speech
centric filtering out performs this mba
technique by twelve to fourteen percent
so there's some evidence in the studies
which look at the auditory processing
and there's some evidence that the
processing actually happens in multiple
streams so the the brain actually
divides the incoming information into
slow dynamics or slow temporal dynamics
versus fast dynamics and they process it
and it's later the evidence is combined
at a higher higher level so we wanted to
do a similar divide and conquer strategy
so that the machine learning stage can
take a maximum advantage of each of
these streams so how we divide the space
into different streams is based on these
three principles the first one is
information encoding where each stream
has to have enough information second
one is complementary information or
technique principle where each stream
should be unique compared to the other
streams and finally noise robustness
where each stream should contain only
the high speech highest in our speech
regions so the way we do that is again
look this is the same modulation domain
and we divide that into these three
streams and so if you see the
information encoding principle according
to that we divided the streams which
have at least sixty percent of the
energy in this entire region then each
stream has some unique part compared to
the other streams and finally they don't
they're still localized on the speech
region so this has to we hope that this
gives noise robustness so an example of
how these three stream of the three
filtered spectrograms would look just
like this so the first one is including
only the low temporal and spectral
modulation so it's
smooth in time and frequency the second
one is allowing for higher frequency
modulations so you can see it's little
bit more faster a long frequency and the
third one allows for a little bit faster
a long time so it's it's not as smooth
as the first one again the back end is
almost similar to the first experiment
so instead of having one so we have the
same setup for each of the three streams
and we combine the evidence using the
product rule so we're combining the
posteriors at the very end and we test
this not only an additive noise but in
this case we tested it on channel noise
and reverb artists shale River Bridge
reverberated test conditions in this
case we compared it with two other
baseline one other base line which is
the ETS I advanced front end baseline so
on this one they they do voice activity
detection and they're doing the estimate
the noise and they're doing some sort of
noise cancellation as well in etsi so
etsi if you look it's it's it's been a
it's it's built for additive noise so it
does really well an additive but it does
not do as well on the other kinds of
noises but when we compare our approach
we see that we are doing we beat all
these systems not only unclean but all
the other noise conditions as well by a
significant margin
so as I said etsi has these additional
advantages of voice activity detection
we don't have anything like that and we
are still able to beat those by around 4
29 19% and depending on the noise
conditions and if you analyze the
different streams we saw that stream one
alone can beat most the baseline on most
of the noise conditions and stream 2 and
3 then add on to this to give us the
best advantage so now I'll talk about
how we did the we adapted this auditory
objector presentation for Tambor
recognition typically when people look
at Tambor they are trying to identify
what are the dimensions of time where so
when I say time where it's defined as
anything which identifies the instrument
and it's not volume and pitch so
anything apart from volume and pitch
which you can use to identify the
instrument so a lot of studies they look
at so they collect human studies with
the human subjects rate how different
they perceive to instruments and based
on this in let's say perceptual distance
of the instruments they map it into a
two dimensional or a three-dimensional
space then they try to find features
which correlate with the dimensions of
this space so they are going about and
finding the dimensions of Tambor in that
approach here we take more the reverse
approach where we define the dimensions
of timbre and we see how well it matches
with the perceived distances but first
we will do a quick analysis of this
representation on the classification
task so here we keep the full
representation so we are keeping this
full high dimensional space because to
identify the timbre of an instrument
it's really key to have all the spectral
and temporal variations and we can
afford to lose the 10
Perl fine fine structure so we keep the
whole space and we test this on our CW
musical instrument database sir this has
11 instruments and it has like close to
2,000 notes / instrument need for each
instrument they have three different
manufacturers and they have multiple
playing styles and they play all the
possible pitch values on that instrument
so we are really testing if our system
can generalize to this so what we do is
we take this high dimensional
representation we try to reduce the
dimensions to a manageable amount using
a tensor SVD and for the back end we are
going to use svm classifier with RBF
kernels and I will talk about the last
point later so when we do this we are
going to compare this sort of
representation with two other
representations so the spectral features
here is just the auditory spectrogram
and we use the we use that as the we
also tested it on sim on features like
mfcc so they all give something in the
range of like seventy nine percent so
this second feature said that we
compared this actual neuron record
neuron transfer functions that were
recorded in University of Maryland so we
borrowed that and this has a I think
thousand two hundred neuron transfer
functions so we just applied those to
the auditory spectrogram and we use the
activity of that to classify the time
well and we saw that that's able to do
quite well as compared to the
spectrogram but the problem with that is
it doesn't have the full range of act so
it's just thousand neurons but our brain
has millions of neurons so it's not able
to accurately capture the entire range
of possible neurons so if
we use a model of this our model we see
that the we are able to get really good
representation for these instruments so
this shows that the these instruments
are really well separated in this region
so I was talking about how how these
instruments are separated in this space
so what we wanted to do is to see if
this model captures the perceptual
distance that human subjects have stated
so so we went and went ahead and had
human subjects rate different how
different they perceive these
instruments from this RCW database and
we have some sort of matrix which tells
how different these instruments are and
we then take the distances in our model
and we just do a correlation with these
distances and we see that when we do it
with the baseline features they are not
correlating that well with the human
distances and but with our model we see
that the correlation is also pretty high
so this shows that this representation
is not only able to separate the
instruments but it's also able to keep
the distances between the instruments as
we perceive it so then this was just
based on isolated tones of instruments
so we thought how we can use this in a
in a more generalized situation so you
might have so if if you go and buy a
musical recording you might have
something like this
so this is a artist playing appear
playing an instrument and whereas art
model was trained on something like this
which is a clean clean tone which is
which has the full rise and the decay of
the tone so it's allowed to decay
completely and it's we captured all the
dynamics there so if you test our system
on this just using uniform window so you
don't care about what's actually there
so you see that the classification
accuracy drops drastically from 98 to 70
so this shows that so these recordings
were be actually collected from
commercially available CD so there could
have been some differences in the
recording equipment and so on but but as
I saying since these notes are isolated
notes we think we can do better if we
can extract the notes from these from
this continuous recording so we tried
existing techniques for onset detection
which tell you the boundaries between
two nodes but they are not able to
generalize well to this database because
we have recordings from various
different studios and different CD so it
was not able to generalize because I we
think that it's because most of the
systems work on the signal level
representation so what we did was we
came up with this idea so here what we
are going to do is if you look at one
particular note you can see that the
harmonics are placed along different
points in the frequency axis so if we
come up with the template which mimics
the placing of these harmonics along the
frequency axis and if we see how well
this matches how well the
which template for each pitch matches
the spectrogram we can get some sort of
pitch estimate and we mark those regions
where the pitch estimate is changing as
the possible times for onset detection
and also the degree to which it matches
is we call it as Hardman icity and we
typically see that between at the
boundary of two notes that have Melissa
T is low because they're overlapping
harmonics so we also matched the can we
mark those candidates where the Harmonia
city is low as possible onset onset
boundaries so we then combine the
evidence from both of these streams to
get reliable estimates for an set for
the pitch bound for the note boundaries
then what we do is we then extract we
will segment this continuous recording
along these boundaries then if we do
note extraction we do the classification
and these extracted notes we see that we
get a performance improvement of close
to nine percent but to take care of the
differences in recordings and other
changes in the statistical distribution
we can also adapt the back end so what
we did was we have a few examples of
these we take a few examples of these
extracted notes and we adapt the svm
boundaries to this new new distribution
and if we do that we see that we can
further improve the performance by
around six percent so this this
obviously has the advantage of having a
few examples that it has seen whereas
the no text action this does not have
that advantage so
a few observations we were able to not
only represent a musical tambor in this
space but we are also able to capture
the distances between the instruments
well when we did a study of the marginal
representations led do we need to keep
the whole space or can we marginalize it
along different dimensions we saw that
the whole space is indeed required and
we also saw how it can be modified to be
robust to out of domain conditions so
now I would like to talk about the
concept of attention in this space so
when I say attention they're usually two
kinds of things that are thought about
so one is the bottom-up attention where
you are not doing anything voluntarily
but let's say there's a loud bang
somewhere in the background you direct
immediately diverting attention to that
so that is the bottom of attention and
we are going to talk about something
called less top-down attention where I
give you the task to pay attention to
some particular sound or some particular
object and without this task you won't
be able to recognize this but once you
once you given the task you will be able
to recognize it so that's the top-down
attention that we are going to talk
about so for example in the visual
domain if this is the visual scene that
is there and I ask you to pay attention
to this painting on the wall so you will
do something like a field of view sort
of thing where you're able to restrict
your faculties only to that particular
object and you're also able to change or
move this field of view to a different
object like if I ask you to pay
attention to the flowers so how does
this work in the auditory domain so for
example from this waveform you won't be
able to tell that there are multiple
objects so I will just play the sound to
you
059 1 0 0 so I think notice there
multiple speakers in this recording and
if I now ask you to repeat what the male
speaker said I am pretty sure nobody is
able to do that but now i give you the
task to pay attention to the only them
only to the male speaker and let's see
if we can do that again a hero by 9 10 0
so you're you are able to pay attention
to the mail and you are able to say it's
3855 and so on so this clearly shows
that there is something that's happening
where you can focus your view from one
object to the other even in the auditory
domain similar to what was happening in
the visual domain so we wanted to
implement this in our model so the some
studies on what actually happens in the
brain they show that this is a transfer
function of one particular neuron in now
in a ferret and the ferret is now asked
to pay attention to some particular
event and when it's doing that task the
transfer function changes so this
clearly shows that the representation in
a auditory cortex is not static so the
representation is changing whereas our
representation was sort of a static
representation so we want to implement
some kind of adaptive technique to take
care of this so the motivation is like
we saw two different objects occupied
two different spaces in this in this
space so let's say we had animals and
music and they are occupying different
regions so if we had to pay attention to
music if you had some prior knowledge of
music that it's there in that location
you can you can focus or highlight that
region in the space because they are
well separated so that's the hypothesis
and the way we implement that is we
collect a
during the we collect some prior data
for that instrument or that object and
we have the mean representation of all
the activations for the all the filters
and what we then do is if the task is to
pay attention to animals we use that
activation pattern as a boosting or you
can call it the gain adaptation of for
these filters so we adapt we boost those
filters which are most active during the
target class and we suppress the other
filters which are not so active and
there's also evidence that the higher
stages of processing also undergo
changes during Top Gun attention so we
can think of that as something like the
our statistical model is adapting to
this new task so we we do a map
adaptation in this case to given a few
examples we map it up the target
distribution to the new conditions so
the the way we test this is we have a
BBC sound effects database so this is a
quite a big database with test 12
classes and 46 hours of data and during
testing what we do is we artificially
mix the target class with other target
classes so there are multiple sources
occurring at the same time and we tell
them Taylor model to pay attention to
only the target and and we see how well
it does at this recognition so the in
this case the feature dimension we were
able to reduce it to 113 as compared to
the music where we had to keep more in
the back end as I said is the GMM
classifier and we use map adaptation to
adapt the models so when we up so the
performance here is measured in D prime
so this takes care of not only the not
only the true positive but also the
false
so it takes care of the false alarms and
the true positives so we are not just
looking at how many times you identify
it but how many times you wrongly
identified it as well this is a more
robust measure we compared it with this
msec baseline where we are averaging the
statistics for the entire duration of
that recording we take the first second
and third moments of the mcc features
and use that as the feature vector we
also recently compared we compared with
more standard acoustic features that are
there in the literature and we found
that they're all comparable to this to
this range of performance and in our
model without any add up without any
attention no boasting no adaptation of
the gm's we see that we are already
getting a big gain in performance so
this shows that probably our model
already has these objects well separated
so we are able to do that but when we
apply boosting that is when we apply the
gain and the filters we see that the in
the performance improves but not that
much so this shows that probably the
some change in the statistics that the
gmm back-end is not able to handle it
this is the gain boosting and when we
just do the adaptation and no boosting
we see a higher jump in name performance
so this shows that this may be because
that for doing the GM adaptation you
need some sample examples of the test
condition so that's why it might be
doing better and finally when we combine
both of these techniques we get the best
performance so this shows that when we
boost the filters we need to actually
adapt the GM m's as well so we are
getting the best performance when we do
both of these techniques all superwash
that are teaching our civilized
it's super super nice to know that you
already have some time some time but
yeah I'm sorry so how does boosting work
like exactly where workers
so let's say we had those two we had
like many filters so we collect the
further target class so given a target
class we collect from the training data
the average representations are the mean
activity for these filters and we
normalize it to be in the range of let's
say 12.5 the we can we can control this
parameter so these activations are now
in some range and we use that as the
gain parameter for the filters and we
want to pay attention so let's say
filter one was active for speech and
filter too was not active for speech but
it was active for machines so when we
apply this and we pay attention when we
asking to pay attention to speech so
filter one will still be on or it will
still be active versus filter to will
responsible
so we saw that this cortical
representation is able to we got on
forty five percent relative improvement
on the baseline and this mechanism
tation was a able to also facilitate an
attentional mechanism where we could
selectively attend to the targets and we
showed that these attentional mechanisms
provide eighty-one percent relative gain
that's a huge improvement and more
recently we were also able to not really
change the gain of the filters but we
are also able to adapt the shape of the
filters depending on the target we
showed that this is this was more useful
for adverse conditions when the target
to masquerade sure is really low which
machine the set of the truth you got
more filters yeah so you are thinking
about I think a different set of belle
of those based filters we do not ship so
I give it so that base let's say we have
a model train like this given a target
we are changing the orientation of this
filters little bit and in a manner so
that its most active during that target
class so if you change the file make a
more suitable offer well yeah
so what were you design use filters to
actually use Illidan approach for you
all to be manual designers so it's just
a you can think of it like a 2d wavelet
transform so we have the base functions
and we just
so in conclusion we presented a
multi-dimensional possibly over complete
space which can represent auditory which
was motivated from the auditory system
processing and this space was we had
sensitivity to frequency temporal
profile spectral profile and we showed
how this space can be a basis for many
different sound technologies like speech
recognition tramble classification and
seen classification and also how the
space facilitates the facilitates the
creation of attentional mechanisms and
how we think that such a mechanism can
be a basis for tasks like event
detection or keyword spotting and so on
these are some of the publications which
resulted from this work so you have any
questions education yeah on ego again
any any recognition slides so the
mission I compare the difference is so
that mfc
or the system for gmm systems
for this Beach yeah so I gave that
speech recognition initial
so those two first columns were human
noticed in the back in distance eNOS the
hybrid it's my medical ok
one window notc into it yeah hmm I
repair it yo man I might be taking
posteriors squirt with it and that one
is just
many days
no the back end is the same for mfc seas
and our system right this piece is
featured in Quetta the MLB Fan Expo
thank you here yes Tuesday yeah so we
still have that for MSC see that so it's
exactly the same only the input features
are changing the other question is look
like all the the other task to use this
tensor SVD when you use it here no
because here the feature space feature
dimensions were manageable it's still in
the finder the feature dimension was 500
so we have a MLP with input layer of
find it that's good you get this manual
Drive yeah yeah so here we try just take
that big thing in yeah that was the very
first name of protein
Nate way
yeah so so I think in that case so what
happens is i think these the features
are redundant i think in some way and we
are not able to capture this so we lose
some sort of temporal structure so let's
say we have filters which are quite
broad and we are trying to find the
correlation the convolution and time so
we are losing that the time the output
will lose the temporal structure so we
are getting some smooth response so we
are not able to predict the phone
sequence from that yes one of your early
slides the motivation slide you showed
that we can listen to an orchestra or
three musicians playing it and recognize
the instruments yeah get a computer do
that
the rest can we do this automatically
are their programs out of there has
there been research published that
successfully taken orchestra our
multiple instruments and separate the
sound an adjustment
there are some studies which do I am not
aware of studies which do exactly
identifying the instruments but they do
some sort of on this multi multi source
environment they do different processes
processing like identifying the genre of
the musical identifying different
characteristics of the music but maybe
not exact finding out which instruments
are playing the work you should
attention right yeah it couldn't that
just be applied to the problems you can
separate the male from the female
speaker or various sources could you
just apply that system to that problem
I think the way we formulated is given a
target so given you have to pay
attention to piano we were able to tell
to better extent fpn is present or not
so we would have to then apply this for
all the sets of instruments and then we
can we could we could in that case we
can tell which instruments are present
so we'd have to run the system on
different targets and see which of these
targets are present I think that would
be very interesting application to see
come on
and once we started to talk about other
applications of pretty much a new future
incentive can extract from the outer
signal how applicable is this for Adam
pass cleansing getting a nonverbal cues
from human speech emotion speaker
identification even gender age yeah do
we see Apple application for such for a
disability Joseph for such application I
think definitely I can see some
advantages of using such a
representation because we are we are not
after capturing so in time where we were
after capturing anything that is apart
from pitch and volume so we are trying
to capture all the characteristics of
that instrument so I think for an
example if if in the musical instrument
I tried just doing the broad class broad
instrument tracking me so we can is
recognized if the instrument has is a
string instrument or a wind instrument
or a percussion instrument so so by
using similar features so that shows
that it's not only capturing one
characteristic but we are able to
generalize it to capture different
characteristics of the sound so the way
you are saying like if you want to
capture me rephrase to do a little bit
so we're saying is
you can separate or detect the presence
of dangers as a cello or
yeah whatever
how had some recognition rate
if I had gave me three cellos and asked
you you know which of these three was
playing when what do you think you're
reach out so I think if you are given
three different cellos with which have
slightly different characteristics I
think we should be able to capture that
the menu differences between the
instruments as well because
we change the end if I think if you have
different manufacturers for the so you
are talking about different kinds of
instruments right different 44 in a
cello you might have different so I
don't know much about Chavez you know I
think I'm a lowly expensive 120 lacuna
Sistani values grounding for some lower
prediction I think I think another good
example for that would be a given a
cello you can play it in different
styles so you can have the vibrato style
or so that the musicians use that to
kind of it to a motor yeah the same
musician same instrument but he is
trying to try to express his emotions by
different style of yeah separating the
three shovels
cello from a food
I think
I mean like it may not be in the 99
percent range but I think given if
there's unique characteristics I think
we can do a really good job of capturing
these characteristics so I guess you
know the obvious other one is an eye of
three similar you know 3 30 year old
male speaking
recognize which one is late
yeah I think we can looks like the
number of human speech so yeah but the
optimistic because the tamper between
the male and a female that's quite a bit
different than between myself and Mike
right so there's all question surreal
yeah I mean it's just so we actually did
because verification experiments using
features similar to this okay and we
were able to beat most of the standard
approaches but did you do it under the
cocktail party condition where there
were two people speaking and you said
out of the ten people in the database
tell me the two people who were speaking
at probably had we had background
distant like we had background and Isis
Bible nights inside a ship the Norse is
very different you know officer low
Rumble music rasiya since it's a very
attractive approach both the idea of
filtering for the signal and the damn
thing but you know I think there's the
fascinating stuff is how far you can
push it the idea is elegant but
I don't have any sense that you really
know how widely applies under more
challenging mission valley will actually
even if you can distinguish and so far
sort out speech no speech because in
most of the cases at least today we have
one speaker talking to the daleks
historic reach the completion right or
even if you can't technology initiatives
belongs to speaker 128 or none of those
those are the typical scenarios which at
least for garmin kinda fun process
getting applicable in real systems
for questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>