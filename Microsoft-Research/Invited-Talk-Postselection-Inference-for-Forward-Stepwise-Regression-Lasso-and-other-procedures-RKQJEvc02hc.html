<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Invited Talk: Post-selection Inference for Forward Stepwise Regression, Lasso and other procedures | Coder Coacher - Coaching Coders</title><meta content="Invited Talk: Post-selection Inference for Forward Stepwise Regression, Lasso and other procedures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Invited Talk: Post-selection Inference for Forward Stepwise Regression, Lasso and other procedures</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RKQJEvc02hc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning again and welcome to the
beginning of the second day of nibs I
this today starts with the leo breiman
lecture who's invited speaker is
Professor Rob to be shirani
it is my great pleasure and honored to
be introducing Rob tipsy Ronnie who is a
professor of statistics and of
biomedical data sciences at Stanford
University from certain tips ironic
hails from Ontario where he began his
long involvement with statistics at the
University of Waterloo after a short
stint in mathematics he receives his
Bachelor degree in 1979 and a master's
in statistics from the University of
Toronto a year later in 1984 studying
with Brad Efron he obtained his PhD from
Stanford University he then spent the
next 14 years back at the University of
Toronto working in statistics but also
in preventive medicine and public health
scientist and in 1998 he returned to
Stanford to continue his work in Bolton
Public Health biomedical data Sciences
and statistics to cite rub-ons words it
is an exciting time to be a statistician
today because of enormous computational
tools at our disposal how best to use
these tools is a fundamental challenge
for our subject and the focus of my
research this rich research has had wide
impact it has affected the problems many
of us are working on today and has been
widely recognized Rob is the winner of
the cops a word that stands for the
Committee of presidents of the
statistical societies award and it is
given for outstanding contributions to
statistics by statistician under the age
of 40 the 2012 statistical societies of
Canada's gold medal at the steel world
he is also a fellow of the Institute
format of mathematical statistics the
American Statistical Association
the Royal Society of Canada and the
member of the National Academy of
Sciences in addition he also received
the Alumni achievement award from the
University of Waterloo I will not take
the time of this talk to list Rob's
contributions to statistical learning
which most of the audience present knows
however perhaps not so many here know
that Professor tip Shira and he is
claimed with equal force by another
community the biomedical studies this
community his series of paper on
significance analysis of the microarray
data have received ten thousands of
thousands of citations today and in fact
on research gate his total number of
citations is 78,000 almost he's getting
there so without further ado I invite
Rob to Shahani to talk about recent
advances in post selection statistical
inference I'll try this Mike I hope I
can be heard I'll try not you to drift
away from the mic boy this is a big room
normally I'd like to have questions
during the talk but that's not gonna
work like so I'll look forward to
questions after the talk I'm really
really happy and honored to be to be
chosen to give the Reimann lecture both
because it's a important lecture here at
nips but also because leo breiman was
one of my heroes he was a friend and
colleague and had a big impact on my my
life in and as i'll talk talk a bit
about Leo and my history with Leo along
the way so the work come and talk about
today is joint work with a lot of people
that are listed here most notably
jonathan taylor was a who's been a
driving force in this work and a lot of
other faculty and colleagues students
who are mentioned along the way so first
of all a little bit of history as mayor
mentioned i did a PhD in statistics in
84 at Stanford I'm Canadian
so I came back to Canada to the
University of Toronto at that time you
know and I was just prepared to be your
sort of your regular applied
statistician and then
geoff hinton who was at computer science
at Toronto and Jeff Jeff and uh a lot of
his students and postdocs some of them
whom were here like Zubin who spoke
yesterday and a number of other people
who summer here and they had a lot of
impact on me because they really had a
very different way of thinking first of
all a Jeff for those you know Jeff and
everybody knows Jeff he's I find Jeff
very hard to understand what he's
talking about he never he rarely would
write down a model he's always at the
board writing boxes and funny arrows and
and and I understand he's saying bye
nobody's saying must be really important
fortunately I had people like zooming
around to translate what Jeff was doing
into something I can understand but what
were they were doing was fitting models
with more parameters than observations
which was for statisticians at that
point was just absolutely crazy we
thought these guys were nuts you know
they're gonna you can't do that you're
gonna over fit you can have an error of
0 well of course they had ways of doing
that without overfitting and the the
main thing is they were fearless they
would tackle problems which to us look
really really hard and things wouldn't
we wouldn't consider even trying to
address so they had a big impact on me
and on the community of statisticians
starting in the mid the mid-80s
so my history at nips having met Jeff
and the group at Toronto I get
interested in machine learning community
and starting I think around the early
90s I can't exactly remember might be
late eighties I started attending nips
fairly regularly in Denver with Lee
O'Brien and Jerry Friedman and Trevor
hasty we were sort of the token
statisticians there and it was much
different meeting there was really I
think a few hundred people at the
meeting the sound systems were were
minor celebrities I'd say it's two
people you guys just you guys are real
statisticians and they want to talk us
about stuff that was neat it was also a
little bit easier to get a poster
accepted it seems it was my is lease my
memory I remember Jerry saying the
currency is ideas that's what I really
liked about these meetings you know you
go to the stats meeting at least at the
time and people talk about the
asymptotic properties of estimators and
I'm biased etc whereas it nips meetings
and it's stayed that way today they
actually a poster a talk had an ID in it
and actually a new idea
for something which may seem obvious to
guys is a good idea but for a
statistician this was refreshing so
Jerry and the whole group is really we
liked the way that the nips ran things
especially with the poster sessions and
lots of beer it was a really is a great
way to spend an evening and they learned
a lot in a in a small period of time but
I hadn't been for about 15 years I think
Terry asked me essays it's the first
time you've ever been to nips and well
it's been a long long time
what's my recent experience well I for a
while I didn't think about nips and then
I was with students started submitting
papers to nips no luck
try it again no luck this is honest
tries to write yet again no luck
and I I gave up for a while basically
and then I waited for a while and I got
invited so and maybe it's because I'm
getting older okay whatever I'll take it
I'm happy to be here so that's that's
one way to get to nibs okay so what's
this talk about so this talk is going to
be I think quite different for this
conference
it's about inference it's statistical
inference what I mean by inference I do
have to explain because I know in
machine learning it has a different kind
of meeting it I think it means to sample
parameters from a posterior distribution
force a decisions what that means is
construction of p-values and confidence
intervals for models and parameters fit
to data so we fit a model to data we
have we have a parameter that we're
interested in or parameters and we want
to say something about how strong the
evidence is and usually that's expressed
in terms of p-values for a null
hypothesis or confidence intervals for
the parameters it's free cuentas I'm
going to talk about not nan a nan
Bayesian as contrast to a zoom and
talked about yesterday
and it's gonna be different the word
deeps can appear only once in the talk
and I'll give $5 u.s. to the first
person who spots that okay so what what
is inference I've said what difference
is but how does satisficing see the
world well a lot of them for a lot of
them this is the way we think of it most
of the world is inference to us and
prediction that little thing on the side
is sort of an after
thought machine learning of course it
flips it around right and I think of
this meeting it seems even more than I
expected it's really mostly about
prediction right because now we have big
data big computers very efficient
algorithms for doing predictions in real
time and inference maybe has been a
little bit more even more marginalized
well I want to hopefully convince you
today that first of all a that it's
important and B that there's some new
tools for for tackling the problem
now Leo jumped right in the fray and
around 2000 again the influence really
for Leo was the machine learning
community and nips and he really kind of
balled outside assistants saying you
guys know we're worrying about the wrong
stuff for using problem probability
models are the basis for everything and
that's too limiting whereas these people
machine learning the model is the
algorithm don't worry about the up model
they just say here's some data here's a
sensible algorithm and here's a good
answer and he has a quote here saying
you know that this commitment by
statisticians to probability models has
that two irrelevant theory questionable
conclusions and it's kept scientists
from working on a large range of
interesting problems so the machine
learning community had a lot of
influence on Leo and the rest of us
really making the point that we should
think more like this if that prediction
is really important
and-and-and-and inference not as
important well I'm gonna try to flip
things around today I mean I obviously
agreed with Leo and that also had a lot
of impact on my own work and a lot of
other others in setting statistics but I
do want to say that this piece is also
important I'm gonna try to try to remind
you of that today and then show you some
new tools for handling the inference
problem and I'm actually gonna call it
Leo again on the other side where he
actually worries about this problem so
Leo is very portable why is it for it's
important well it in many situations
simply we care about the identity of the
features for example in a medical study
with biomarkers if we won't know which
genes relate to cancer which genes for
example will increase the chance that
that a treatment will work for a patient
which gene profile right then we care
about the actual genes we don't it's not
not just a prediction problem although
prediction is clearly a part of it we
want to know are those features that
have been identified
actually the right features are there
the ones that are that have a command
calls a relationship to the outcome so a
lot of this is still very important
especially medical science and there's a
crisis in science in reproducibility
john I&amp;amp;I tests of stanford is now made
very famous with this paper and he
spends most his career on this why most
published research findings are false
now that's a bit pessimistic and people
have taken some problems with that but I
think the general feeling is that he's
right that much of science is not
reproducible and here's an example that
he gives in one of his papers where this
is relating single snip mutations to
diseases each one of these colored lines
the disease and along the horizontal
axis is a time axis the original study
is on the left with the cumin of odds
ratio one being here and either being
less or more than one as a is
significant and then as time went on
people tried to repeat the study with
meta-analysis getting more data and you
can see what happens almost every case
is the odds ratio that was thought to be
significant by the original authors has
retreated to one so in a great majority
of cases people things that look
significant are presented is as
significant in a paper years later the
the finding goes away right so and this
seems to be more than norm than the
exception and there's a lot of people
think about this and working on this
problem now I only make purpose clear
part of the problem is non-statistical I
think - and this is a much longer topic
but I think one of the problems in
science is that the incentives are wrong
for authors and journals to get things
right right the incentives are to get
things done quickly to get things on
first so you get the next big grant you
become famous you get tenure to get
things if you make a mistake and your
shoddy with your work there isn't really
a lot of negative feedback because
typically that's discovered years later
and you're on to something else so I
think that it's really a social problem
the way things are set up largely but
part of the problem is also statistical
because nowadays with large data sets we
searched through through a large number
of models to find the
just one but we don't have good ways of
assessing the strength of the evidence
having done the search so the tools I
want to present today are to help with
the statistician that the the data
scientists to assess the strength of his
own evidence both for his own sake and
also for publications so that the the
reader can get an idea of just how
strong is that what we're seeing here is
it really signal or is it matter is it
more matter of the fact that he serves
through a large number of models so
today's talk will report on some
progress on the development of tools for
this problem and the first paper in this
topic is we have mentioned I'm Canadian
I mean I'm a US citizen now but I'm
always always be a Canadian as well
first paper was Richard Locke our
colonel Khaliq I'm Simon Fraser Jonathan
Taylor actually is a is from Montreal he
did his PhD in McGill with with Keith
Worsley and now it's been my colleague
at Stanford for a number of years my son
Ryan actually is a statistician he was
graduate from Stanford a student of
John's he's actually he actually comes
to these meetings or he went to the
workshops last year and will start
coming more often he's a statistician
and also in the machine learning
department at Carnegie Mellon University
and there's me and I younger day okay so
but I want to make clear that this is a
contribution of lots of people and this
work started a few years ago at Stanford
and has been ignited by contributions of
truth some terrific students will 15
who's at the meeting and could be in
this room
I can't really tell has made some
fundamental contributions to the work
which I'll talk about along the way as
well as Jason Lee and ucae son both of
whom are coming or are here already and
a lot of some former students who have
now some of whom still at Stanford and
some are have moved on to faculty jobs
so this is a very exciting area and a
lot of students involved and will be
continue to be involved so some of the
key papers this was the original paper
that the Canadian group published this
last year this paper which I'll talk
about shows how to do
inference and p-values for the further
lasso for fixed lambda and this one does
forth stepwise regression and then two
papers by will really set the the
theoretical framework for this work in
the exponential family paper last year
on optimal inference which I'll discuss
briefly and also a paper we just
finished actually a couple days ago
which talks about how to extend that to
sequential procedure so there's a whole
bunch of papers and you can look at my
website if you're interested after the
talk to get more of a detailed look at
it and what it's like to work with John
teller um it's what I really mean by
this cartoon and I have polyhedral ammo
here which I'll explain in the few
slides he's brilliant and sometimes hard
to understand which actually a bit like
geoff hinton
but in a different way john is really
he's quite mathematical very good at
mathematics and sometimes has trouble
explaining what he's doing to mere
mortals but it's a real joy to work with
john issue you should invite him to nip
some time he'd give a great talk and
there's a lot of a lot of fascinating
work he's doing so let's let me give you
an outline of the talk i'm gonna talk
about the well what i want to tell you
what i'm talking about everybody told
you in detail what post selection
inference is all about
and off to to running examples forward
stepwise regression and the lasso and
then i'll talk about a simple procedure
for for doing post selection inference
in these in these cases which is quite
remarkable there's no sampling required
and the former is explicit so i can give
you an explicit formula and there's
software which i'll refer to later which
solves the problem for for stepwise
regression the lasso and other other
simple supervisory methods and then
we'll dig deeper into the exponential
family to see exactly what these are
doing and in the process will come up
with more powerful procedures okay then
i'll talk about some new kind of more
funky ideas data splitting data carving
and randomized response
finally here I'll talk about the
sequential problem having having
computer set of p-values from a
sequential procedure how do you control
the false discovery rate and if there's
time but I don't think there will be
I'll talk about principal components
okay so what is post selection inference
well it's kind of an old way in a new
way to do statistics right the old the
old way is and this is what we were
still taught we still teach and Tec in
our textbooks in our courses you devised
a model you collect the data and you
test the hypothesis so again this might
seem very very strange to you especially
younger people but in the old days we
had small data sets and we actually
ahead of time knew what we wanted to
test we knew what parameters what what
what features won't look at so we
devised the model first then we
collected the data and we tested the
hypothesis and for that the classical is
kind of p-values that we all learn in
our early courses they're fine because
the hypothesis is fixed ahead of time
it's not based on the data and this is
sort of the old way of doing these maybe
say pre nineteen eighty but nowadays of
course this is all crazy because we what
do we do well we have much larger data
sets we're not sure what we want to what
we're looking for right so we collect
the data then we select a model of some
sort and then we test the hypothesis and
that boss is often it's it's based on
the model we saw right it depends what
what variables we happen to select in
this in the second step determines their
pastas we're going to test this is post
selection inference right and this is so
this is the way that science is done
today and now the question is can we
provide the kind of tools that we have
for the classical problem for the new
way of doing things now what's the
what's the point here the point is that
if we use classical tools which pretend
like we had pre specified the hypothesis
they're not going to work for post
selection inference and I'll show you
some examples of that I mean in
particular when we search through a
model we're cherry-picking for the
strongest effects so if we then having
found those effects if we use a
classical tool which pretended like we
hadn't searched our p-values are going
to be way too optimistic right if we fit
up if we fit an over fit and then use
classical tools which which which ignore
the fact we've searched then are all the
properties that we think we have will be
off the type one error the confidence
limits etc so we can't use these tools
for this situation so our goal is to
come of today's talk is a sort of a
first foray into how we come up with
post-selection inference tool
and Leo actually he's very cordial he
referred to this actual problem of using
classic hypothesis testing for adaptive
situations as a quiet scandal in the
Cisco community I like that because not
often statisticians are involved in
scandals but Lisa it's a quiet scandal
okay but you know this was the nineteen
nineteen ninety three or something and
then it's gotten much much worse because
the amount of data and fitting has
increased a great amount okay so that's
funny get some details now so for most
the talk I'm going to talk about
supervised learning let linear
regression here's our features our
outcomes on the the standard linear
model and this remind you forth stepwise
regression is sort of a basic way of
famous model which is a greedy algorithm
you start with just the constant the
model you find that predictor that's
most correlated with the outcome you you
put that in and then you move on to the
next variable you take the residual you
find there vary with us most correlated
with the residual and you get a sequence
of models starting from the null model
to the full set of predictors then you
have some procedure for determining
where along the path you should stop
like AIC criterion or cross-validation
etc so that's a greedy forward stepwise
algorithm the last so the other hand is
what it's it's um it's it's least
squares with a an absolute value penalty
on the coefficients with a penalty
parameter lambda so this discovers a
trade-off between the fit and how
complex the model is right lambda 0 you
get a get fully squares as lambda goes
to infinity this the coefficients are
shrunk to 0 so we're gonna talk about
the last sort of in two different modes
for fix lambda that we've just fixed
somehow or maybe chosen that
cross-validation or what's the so-called
least anger aggression or Lara algorithm
which considers a path of lambdas
starting with a very large lambda which
gives you a null model up to a very
small lambda which gives you a much
denser model okay so these going to be
our two learning algorithms that we're
gonna analyze so let's see just an
illustration of the problem here's this
a small data set eighty-eight
observations
eight predictors is to actually predict
this is in our elements of this
co-learning textbook
this is trying to predict the PSA level
of men have had the prostate removed
based on number of factors that the men
have we've measured on the on the
patient and and there let the left
column assist the fourth step wise using
the naive p-values so what does that
mean that what we're doing here is
taking the coefficient that was entered
dividing by standard error we get a
z-score or a Zed score and we compare
that to at a normal table and we got
these p-values for these variables this
guy went in first and then second etc I
want to first make the point that these
are not valid p-values that's again the
that's where the whole point of the talk
these are these this is a greedy
algorithm and we put in the strongest
variable to each stage whereas the
theory that gave us these p-values
requires the fact that this this
hypothesis namely say this right was it
was specified ahead of time so it's not
it assumes that this is not the best
variable on a vapora traffic's terrible
that was specified ahead of time so as
you can guess right these p-values
should actually be larger right because
the bar isn't set high enough these
should be compared to the strongest
p-values from a set of eight not just a
fixed one right so this is right there
the basic problem of the whole of trying
to apply classical inference to adaptive
estimation on the right is what I'm
going to present today we these are
adjusted p-values that have exactly the
right product we want for example this
guy this point over 1/2 is a P this p
value is given that i've selected this
variable first what is the this is a
test for that the partial regression
coefficient of this variable is zero in
this model with it with two variables
etc along the way you always were
testing for the coefficient that just
just came in the model given all the
other things that would that were
entered ahead of time and ads we're not
we're not surprised to see these
p-values are larger right because we've
set the bar higher which is appropriate
given the fact that we've done applied a
greedy algorithm so these these have and
the remarkable thing I'll show how we
get these these are these are exact
p-values in finite samples as long as
the errors are normal these have exactly
the right
post-selection coverage and they're
simple to obtain so I want to show you
that how to get that actually in a few
slides okay
second oh sorry I should have said this
before this just I said these guys were
too optimistic that they don't provide
the right type one error well here's a
quantitative demonstration of that fact
here are the p-values is still a
simulation with ten variables the true
very to model is null so there's no all
coefficient of zero there's no signal
here the p values from this naive test
this is a quantile quantile plot here's
the 45-degree line they're too small
they're way too small okay
matter of fact if you think the type 1
error is 5% if you said it's a test of
5% you actually tap on areas above 30%
so this is not a small mistake we're
making it's a big big mistake and it
gets much worse if P gets larger right
as it's gonna in most data sets the the
red line is the quantile quantile plot
from these these guys which are
calibrated correctly and so hence they
have a uniform distribution as they
should right this is the quantile
quantile plot against the uniform and
there they are the red along the 45
degree line so main point is that the
IEP values are not a little wrong
they're really wrong and we can we can
get correct p-values without much effort
it turns out okay second example and
I'll show you how we get these these in
a moment that I want to show you these
are the final result first this is the
lasso with fixed lambda this is some
data on a response to a drug for HIV and
the predictors were mutations at various
sites the lasso with fixed lambdas shows
in this small example seven variables
here's the active set there's two things
here these are confidence intervals the
dark brown ones are least squares
intervals okay so it just pretends like
we did least squares with these seven
variables that we didn't select anything
that this is what we started with right
the beige intervals are the selection
adjusted intervals from this same theory
and you can see there they're typically
longer as they need to be and what they
have an exact coverage property it says
given that we chose this active set from
this data set of
the size a few hundred these have these
cover the true value with probably with
these with the prescribed probability
again exactly in finite samples it
doesn't even asymptotic and doesn't
matter what your feature matrix looks
like it's very a very general result so
I want to show you how we get those so I
got to back up some more and say what
exactly do we mean what's our former
goal of post selection inference well we
can see it like this having selected a
model based on our data for example for
it stepwise regression may we run three
steps we'd like to test a hypothesis for
example that the variable that for
stepwise put in at that step has a
coefficient of zero now note that the
hypothesis is random this is this this
bothers people and as a it should bother
for at least a while right this
hypothesis wasn't thought of ahead of
time it was suggested by the data right
it was determined by what whatever we
put in at that stuff that's okay because
we're going to condition on the
hypothesis and if we have a rejection
region for the test say T of Y in R for
example this might be that the z-score
is bigger than two an absolute value
what what property do we want the test
to have we want the property have the
test that the tests have the property
that the probability of rejecting given
the model and the null hypothesis is
that most alpha so this is this is
post-selection type one error right
right because we conditioned on the fact
that we've chosen that model and the
corresponding hypothesis and so given
that given we chose in this model and we
want to test for this variable we want
the test to have the right size type one
error under those conditions right and
this this course does not hold for the
classical inference because it doesn't
condition on this
so the post-selection inference is
asking a harder question right okay so
well you might think well let me let all
this stuff we've got we already have
methods for this well what are those
methods data splitting is one popular
method which is which does have its uses
for example you can fit on one half of
the data split the data in half fit on
one half
find your do the inference p-values on
the other half
well for me that has a a basic kind of
practical problem right in the sense
that the model you get change changes
depending on the random split right if
you work with people that collaborator
it's like I do in medicine and I do a
random spit of the data and I say dr.
Jones I got these five variables he said
hey wait a minute you did I ran a split
at step one what if you do a different
random Swit oh I get these other seven
variables right and that's not I mean I
I realize data is random but that's not
a practically sort of useful procedure
because he's gonna want to know well
just what is my model like what what
model should I analyze and write about
so that's kind of the basic problem in
terms of Terp rotation of data splitting
a second is a loss of power right
typically the medical date especially
the sample sizes are not that large
if we cut the data in half we lose a lot
of power I'll have more on this later so
data splitting is useful but doesn't
isn't a panacea for this problem you
might have say let's just turn the
computer onto lots of permutations and
bootstraps well I don't know how to do
those permutations and bootstraps work
well for the global nonprofit says that
all features have no signal but how to
do them beyond beyond that is not clear
and no one's really figured that out so
it's not just a matter of doing more
computation it's how to figure out how
to do the right computation okay so a
key mathematical result is the
polyhedral lemma
so here's here that this Lummis is going
to give us the p-values and confidence
intervals that we just saw for the last
1/4 step wise in close form here's
here's how it works
suppose we have a response vector which
is normal with mean mu and variance
Sigma and we make a selection so this is
like the outcome of our regression we
make a selection that can be written in
called polyhedral form in other words
there's a matrix a and a vector B so
that this is the selection and it turns
out this is the case for the things we
just saw for its stepwise regression and
last so etc they can be written in this
form and I'll explain on the next slide
why that is namely if in a fourth
stepwise regression what's happened it's
basically a competition involved in the
inner products right at the first step
what happened we put in say X 3 and
that's what we run it for K steps then
at each each step we had a competition
between the inner products of a
Abell that was entered and the inner
products from all the other variables
with y and the variable that was entered
was the one that had the largest inner
product and that's a linear function of
Y right and each state so each taste you
could summarize that the results of the
competition of Ford stepwise as a series
of competitions of involving inner
products and that has a polyhedral form
so I need to tell you what a and B are
just you can believe me that you can
write that out in that fashion it's a
function of X and all the things that
you selected
so this polyhedra form is very
convenient it's also true for the lasso
turns out for fixed lambda or Laura that
you can write the selection in polyhedra
form there is an A and a B that
summarizes your competition you've just
run ok so here's the here's the lemma in
one slide and the proof will be in the
picture of the next slide given the
selection so let's suppose I've run
fourth step wise for four steps for
example and I want to infer about the
regression coefficient of the fourth
variable
well I condition on the selection not
there's the the selection event I choose
an ADA to test what I want to test for
example ADA transpose Y might be the
partial least squares estimate for the
selected variable the lemma says the the
conditional distribution of that
contrast given the selection has a
truncated normal distribution so that's
what all this notation looks like this
is meant to be a normal distribution
with me this mean this variance and
truncation limits given by these
variables okay these values where does
this come well first of all if if we
believe this then and this is a
mathematical fact but we can use this
now to get p values for whatever
contrast we want and also to get
confidence intervals and and and and
this is the the formula on which we base
the p-values and confidence intervals
I've showed you in the last few slides
so where does this come from
this truncated normal it's really this
picture so the selection event is a
polyhedron that's this yellow polygon
upjohn okay so remember that this
polyhedron said this is a set of wise
that would give me the same the supposed
to run fourth step wise for four steps
it's the set of wire that would leave me
days to select the same variables in the
same order it with the same
signs okay that's what this set is
so I want a condition on that and and do
an inference of cetera based on the fact
that Y lies in the set because I got
that selection now and what I care about
is some contrast a to transpose Y that's
a in this direction so I'm asking how
large can a transpose Y be given that
I'm in this polygon it's best polyhedron
okay so how the the method works to
predict that is the proof works is that
you condition on the orthogonal
direction and now rather than say I'm in
this polygon it's a complicated event I
can condition on where I am in this
orthogonal direction and now we get
where along this line so now we simply
have a truncated normal with truncation
limits given by these two numbers and
these numbers are computable given the
the selection and given a tit
so you can think of it this way without
selection a z-score has a normal
distribution with no no balance right no
support is the whole real line but given
we've selected from the data it now is a
truncated normal distribution the
truncation reflects the fact that we've
done this selection so we know that we
have to set the bar higher so we know
that this score has to fall in certain
in certain ranges and that's reflected
by these these limits so this is it's a
quite an amazing result and it gives us
in close form p-values a confidence
intervals for the lasso and forth
stepwise which are exact just based on
this simple mathematics office heavily
based on the normal distribution so
moving this away from the normal for
other other error distributions is still
a challenge it's approximate in those
cases so again here's the picture from
the lasso which we saw before and that
polyhedral emma gave us the beige limits
these these could these confidence
limits which again have posts election
coverage properties right having
selected this set of variables they
cover the partial regression
coefficients with the prescribed limit
which i think is ten percent with that
an error of 10% or coverage of 90%
okay now you might say hey wait a minute
when I run the lasso I never use a fixed
lambda I use cross-validation so it's a
random variable and that's a good point
well we really want to condition not
only on the active set but the choice of
lambda which is random um
and current work by these students is is
trying to figure that out
it is doable and it's not clear how much
difference it makes to do the proper
conditioning because after all selection
of lambda is kind of orthogonal to B to
the the amount of fitting you're doing
so it's still still a open problem okay
well how do we improve the power so I've
Sarge a simple formula involving no
sapling which has the right type one
error but type 1 errors only have to
picture right there's power there's how
long your confidence intervals are well
as a general sort of mantra the last you
Commission on the more power you have
right if you condition on more you take
away more the variability in the data
it's like it's like making your sample
size smaller so you're on a condition
unless to leave more data to vary and
that gives you more power so I pulled a
bit of sleight of hand here I
conditioned on this orthogonal direction
in order to make the problem
mathematically simpler right to give you
this line but the question arises do we
need to do this conditioning and every
is there some we're losing power by
doing so and this paper by will and
company really figure this out and I
won't I'm I want to cover more in some
some broader topics and in detail but
essentially they talk about two models
the saturating the selective model and
if you have a more general model which
the mean is not is not assumed to be a
function of the variables you see in the
linear span of what the variable you're
seeing then the conditioning and that
previous picture is exactly right but if
you're willing to make a reduced model
assumption that the model I've selected
is actually the true model you can
commission on less and get a more
powerful test the the downside is you
have to MCMC sampling but for this
audience for staffs audience there be a
groan this audience have no problem
right so how much difference does it
make well if you have two signals of
equal strength here's the the p-value
for the first variable entered of more
in stepwise the saturated from that
picture from the formula has much less
power than the selected which require
which is the most powerful test it turns
out for this situation so you can do
quite a bit better if you want to do
more computation and I think I think in
most case it's worth it
okay now I wanted to move on to sort of
I think some newer ideas which are ways
of increasing the power and I guess but
from from a philosophical point of view
are quite interesting too and their data
splitting data carving and randomized
response so how to improve the power
even further I've just shown you quickly
that there are some techniques which
give you exactly the right type on air
but again that's only part of the part
of the equation right we like to have
things which have more power and shorter
confidence intervals
well here's two ideas for that which I
want to talk about briefly now data
carving so and I'll show you in a
schematic on the next slide but the
basic idea data carving is we withhold a
small proportion of the data at the
first stage of selection and then use
all the data for inference and the
random randomize response is sort of
philosophically assembly but rather than
really holding dataview add noise
so you add noise the data before
selecting and then use the the unknowns
data for for inference and differential
privacy methods which are quite
interesting I have a close relation to
this randomize response idea where
you're trying to hide some of the
information the reason for doing it is
different in this case because here we
want to actually improve the power as
opposed to make the analysis private so
here's a picture sort of showing the
different different approaches data
splitting right we split the data in
half the first half were used for
selection the second half for inference
okay again useful but we lose a lot of
power cuz we're leaving a lot a lot of
data on the table the that I talked
about techniques which use the whole
data set both for selection and
inference and we adjusted correctly so
that we got the correct type one error
data carving says it looks like this we
leave out a little bit of data in this
selection stage now why is that because
we want to we don't want to reveal the
whole data set we want to leave
something fresh for the the inference
stage we would hold some data and that
turns out to actually improve things
quite dramatically the
by by withholding a bit of information
at the selection stage we have some
fresh information which is available the
inference stage and this makes the this
increases power and and increases power
it makes the conference in of a shorter
adding noise is a sort of a smooth
version of data carving where we use all
the data but we rad them with wholesome
data we add some noise to the data right
and that's like withholding data after
all right if you have add some noise the
data I'm not showing you all the data
I'm showing you a blurred version it's
withholding information in a sense in a
similar way to day to Cardon but it's
smoother because I'm using all the data
but rather I'm just varying the amount
of noise so I do the selection last so
for step wise based on the noise data
and then I do the inference based on the
fresh the unknowns data so these two new
ideas I think they're quite useful for
trying to improve the the the confidence
intervals here's an example comparing
data splitting this is the HIV mutation
they had again with a large number of
predictors each of these little box
these boxes is the the least squares
estimate of a crowd man chosen by the
lasso and here are the confidence
intervals from data splitting often
quite wide data carving the other hand
gives you quite a bit shorter intervals
right
and why is that well see data splitting
is using a lot less data right data
splitting is using half the data only
for the selection and half for the
inference whereas data carving is
reusing a large proportion of the data
so it's sample sizes are larger and
hence the power of these composition was
a shorter and then here's a comparison
of all three data splitting data carving
additive noise in a in a selection
problem and this graph is kind of funny
but good is is bottom bottom right
bottom left so we're looking at the
probability of screening it's the
probability of getting all the true
variables in this simulation correct and
this is the type two error the number of
false negatives and okay and data
because he data splitting his way up
here much worse and data carving
additive noise do quite a bit better
than with a slight advantage of
noise so I see we can improve these
quite dramatically by these tricks of
either leaving out a little bit of data
or adding noise to the data okay and the
last topic is fourth stepwise regression
stopping rules in the last five minutes
I showed you this table before right
this was the I started off with this
this was the naive p-values the adjusted
p-values and I argued that these were
not appropriate I hope you at least
believe me on that and that these these
adjusted ones have the correct type one
error but then the questions okay so
what do I do with these yeah I'm a data
analyst I I run this sequential p-value
procedure where do I stop right how do I
select a model and how can I do so with
them how can I select a model that has
the correct has some property that I can
guarantee for example false discovery
rate control so let me show you now how
to get false escape how to a stopping
rule a new stopping rule based on
sequential p-values that guarantees
stopping and it's this is work with more
students at Stanford and so this is
false discovery at all but it's
sequential so those of you familiar with
it with the false discovery rate and the
BH rule we can't apply the BH rule here
because there's an ordering right we
have to reject the hypotheses in order
right we have to start at the top and go
down we can't for example it decided to
reject this guy and not someone above it
right this is a sequential hypothesis
problem so we need a different procedure
and this is just a formal statement the
problem basically the procedure is
called fourth stop so it's very
intuitive it turns out here it is it
says well compute the p-values thisis a
selection adjusted p-values and then
keep going as long as their average is
below alpha ooh with one proviso the
average is taken on this transform scale
this is what the theory tells us to do
but that's basically you know we want to
accumulate the information right we
don't want a for example the stock
wouldn't say one p-value goes over 105
we want to look at the this accumulation
of p-values and when they uh no and
overall they're too large we want to
stop and that's what it says to do just
stop when the average p value exceeds
alpha the average being taken on this
transform scale and that has has we can
guarantee false discovery control at
level alpha and I want to mention that
there's a recent work of Liam barber on
accumulation tests which are provide a
more general form of this this stop you
and can sometimes provide more power one
proviso this theory requires a p-values
to be independent under the null
hypothesis turns out that the the P
values computed here are not independent
but in a very recent paper by will and
the rest of us whoops we actually
submitted to archive a couple days ago
we have a way based on the on the
selective model of producing sequential
independent p values which can therefore
be used for that rule so long and short
of it is that we have a procedure which
gives you P values which are independent
under the null hypothesis in which you
can feed into this procedure and get a
stopping rule with guaranteed false
discovery rate control I'm getting the
hook looks like okay so I will I was
going to talk about PCA but I will skip
that but let me just tell you that it's
possible do this for principal
components analysis where the problem is
to choose the rank of a matrix right and
you can think of that as sort of a
fourth stepwise regression on the eigen
vectors problem through those to be
harder but a generalized form of the
theory can be applied so I'm going work
and there's lots of it group variables
asymptotics bootstrap lots of fun stuff
we haven't yet got to even the logic
model cuz is nonlinear this turns out to
be hard and other more complicated
nonlinear models are not yet tractable
but I think the field is wide open for
for for generalizations of this work to
other more complicated models so an
exciting new area I hope I've conveyed
some of the excitement at least I have
and a lot of our students and graduate
students take note oh there's the okay
there's the appearance of deep I thought
I'd better get a trademark on that
before someone else does I'm not sure
I'm not sure what it is but it looks
nicely a great idea
and finally if you want to read more at
least until my son started working at
statistics I had a pretty unique name
but if you google list you'll get me or
Ryan and and you get you'll get a
pointer to lots of papers in this area
there's an R package so we have UNCHR
and the public repository there's an R
package called selective inference which
has a lot of this implemented and the
codes in pretty good shape and I'd be
happy to if you find bugs please say
something nice first and then tell me
what the bug but it's a it's a package
which will continue to develop and
support and we have a new book which is
actually on sale here and them at the
meeting with Chapman &amp;amp; Hall which is on
the lasso and sparsity but I I mentioned
here because there's a chapter on
selective inference so that's one easy
way to get more details from what I've
talked about and happy to take questions
thank you very much
thank you for an exciting talk and we
are ready to take questions now please
step to the microphone or get a
microphone somewhere so that everybody
can hear you oh okay thanks for the
great talk my questions at the beginning
your slides with the the emphasis on
prediction and versus inference and vice
versa
yes is that somewhat of a false
dichotomy that our field has in the
sense that it seems like those two
things you know essentially go hand in
hand and that you don't want inference
with that or you don't wanna do
inference on a model that doesn't have
predictive power and then you don't want
to do prediction necessarily on a model
that you can't infer anything about the
parameters are interpret so can you give
us your thoughts on that well I think I
think Leo's paper to address that I
would only partially agree with your
statement I think your statement that
you don't want to do inference on
something doesn't that predictive power
in medicine for example when we look at
the relative risk of say a dietary
factor on cancer the predictive power of
the model is terrible right we have very
little predictive power but we still
care whether you know eating six hot
dogs today is going to increase your
chance of colon cancer so I think a lot
of models a lot of situations we trying
to address relative risk or actually
done in very low predictive situations
but I would agree the more generally
that you know if you have a choice try
to use models to predict well and use
those for inference and vice-versa
Thanks hi yeah my name is Maura thanks
so much for this great talk I really
enjoyed it I have a quick remark and a
technical question so the mark is that
there's actually an entire workshop
devoted to this question on Friday it
has some of the statistics speakers like
will 15 as well as computer scientists
who's worked on this who have worked on
this I'm she learning people so for the
many of people of you who were intrigued
by this talk there's more to come on
Friday the workshops called adaptive
data analysis so check it out on the
program so the technical question is
have you considered sort of multiple
stages of adapt
inference for example you do one
hypothesis test and now you want to test
a different subgroup or a different
analysis what do you do in that case so
thank you for mentioning the workshop
which I wish I was here for it I have to
go home tomorrow but I'm happened here
does it workshop in this area and I
expect some really nice things to come
out of that workshop as far as the
multiple multistage analysis and haven't
really thought about that but in
principle you just have to condition on
there on this sequence of events at
every stage so in principle I can see
how to do it whether computationally
it's tractable is another and another
matter but it's a good question I have
also have a question so in very various
parts of science people usually have
usually have this kind of common
practice that they are doing statistical
tests and gappy values but they are
doing or the false discovery false
discovery rate correction so what do you
think of this practice oh that's a good
practice and I think what you can you
can view what I talked about today is
sort of an extension of those kinds of
methods to regression so the false
discovery rate approaches are really
from marginal testing you have a bunch
of items you want to test how a bunch of
features you write pollicis testing over
the group and now you want to ensure
some proper like false discovery rate
the work today maybe I should have said
that tries to bring that kind of
inference those ideas into regression
where now you have to worry about
correlations between variables
sequential testing etc so this turns to
take false discover it from their
marginal to the - to regression yeah
thank you one more question
hi thanks again Rob for the great talk
so one way to think of your work perhaps
is that you are extending the sort of
the classical view that you fix the
procedure beforehand and then analyze it
to more complicated procedures those
that involve in some sense two steps
first you select the very most like an
forward selection and then you do the
regression or work first you tune the
parameter and then you do the the
regression one sort of soft question is
whether like by having to analyze this
basically you are basically going back
to the same to the same situation and
you might be at every time point you
might be a couple of steps behind the
kind of procedures that are being used
in practice
so you'll be analyzing the procedures
which are already no longer being used
because machine learning people or other
solutions have invented other procedures
which you can't analyze for now is that
a question is a question do you think
it's the right sort of way to to
approach are you just kind of doing more
of the same or and how where however can
you ensure that you'll be sort of ahead
of or together with practice well I
agree with you but I mean I make what
you have to walk before you can run
so I mean we're starting maybe where
we're analyzing procedures which are
five years old but we have to start
there and hopefully we'll we'll start to
catch up with what cut would comment
with current practices I mean it's
possible course to make current current
practice so complex that we can never
analyze it but one has to try it
somewhere and I think we're starting at
a sensible place as chair of this
session which is a leo breiman
talk I would like to also add that the
statistical learning and data mining
journal is preparing a special issue to
another leo prime man's work and if you
are interested in submitting an article
please talk to me and now without
further ado let's think again Rob -
Ronnie for the
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>