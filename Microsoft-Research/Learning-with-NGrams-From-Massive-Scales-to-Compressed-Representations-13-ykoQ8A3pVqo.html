<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning with N-Grams: From Massive Scales to Compressed Representations [1/3] | Coder Coacher - Coaching Coders</title><meta content="Learning with N-Grams: From Massive Scales to Compressed Representations [1/3] - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning with N-Grams: From Massive Scales to Compressed Representations [1/3]</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ykoQ8A3pVqo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay it's my pleasure I'm gonna see if I
can pronounce this name correctly uh my
pleasure to introduce Chris Peskov he
goes by Chris can I say your last name
right yeah he's going to tell us about
learning with n grams today thank you so
uh thank you for inviting me to present
today it's an honor to be here um as
that I mentioned I'd like to talk to you
about my work on learning with n grams
and this is joint work with my PhD
advisors Trevor hasty and John Mitchell
much of my work has guided by this quest
to develop a new kind of deep learning
that I've lovingly started to call
Dracula it stands for data
representation and compression using
linear programming approximations and
and to preempt the question it is a
backronym as the name suggests this
paradigm takes classical ideas from
compression to not only learn efficient
ways to store data but actually to
derive features that are directly useful
for learning and unlike conventional
deep learning which is very non convex
optimization criterion that's hard to
really produce any theory for Dracula it
can be expressed as a binary linear
program and so it has a lot of structure
that we can leverage to make various
computational and statistical statements
about it now this journey to understand
Dracula has led me down a number of
interesting avenues one of which was
reading a book on suffix trees by Dan
Gus field this is an amazing book if you
get the opportunity i highly recommend
it suffix trees are seldom seen in
machine learning it's what kind of the
notable exception in the kernel trick
setting where they're used to define
pairwise similarities between documents
other than that it's really more of a
bioinformatics thing but in studying
suffix trees one of the things I noticed
is that the bag of n grams matrix one of
the fundamental representations used in
natural language processing actually has
a lot of structure that's entirely
encoded by the suffix tree of the text
that you're using to represent and so I
use the structure to actually provide a
fast multiplication algorithm for bag of
Ngram matrices and I'll
briefly touch on how we can use this to
gain some massive speed ups in learning
with n grams the other direction that
I've gone into heavily is optimization
here i think that there's a lot of
wonderful math that's currently being
developed but also a lot of math that
was developed in the 60s and 70s that's
kind of being rediscovered and leading
to a number of really fast algorithms
for classical problems an example of
this is a very recent work of mine in
which i take duality theory and
optimization in hilbert spaces and use
this to define a new learning paradigm
that i call fine control kernels its
goal is to lie somewhere in between the
kernel trick and explicit
representations so if you consider the
kernel trick really the whole point
behind that is to store and manipulate a
number of terms that's proportional to
the number of training examples that you
have it's fairly independent of the
number of features that you have sure
sure sure the kernel trick is kind of a
dual representation for learning
problems in which rather than storing a
feature matrix for every it which which
encodes a values for every example and
feature a pair that you might have the
kernel trick just stores an n-by-n
matrix and being the total number of
training examples that you have in it
kind of implicitly encodes features now
in doing this since we're manipulating
and storing a number of terms that's
only proportional to the number of
training samples that you have you kind
of you know that's that's an advantage
but also a disadvantage it's no longer
compatible with variety of regularizer
such as the lasso which induced sparsity
and are very useful for producing
interpretable models really the lasso
requires that explicit representation
but now you're stuck dealing with the
number of terms that's proportional to
the number of features that you have and
if you have many many many features more
than you can possibly store you'll run
into trouble so fine control kernels
leverage this time space trade-off in
which we only ever store a number of
terms that's proportional to the number
of training examples that we have but we
compute but never store a number of
terms that's proportional to the number
of features that we have and in doing
this the math kind of falls apart and it
allows you to to be compatible with
regular risers like the l1 regular
reiser that induce sparsity and it also
leads to very fast algorithms and so in
this paper I developed such an algorithm
and had use it to provide a number of
computational guarantees on the actual
learning problem and so it kind of opens
up new doors for what a new door for
what we can do with many features and
most recently I've gotten into
polyhedral competent torques to try to
understand the structure of Dracula and
we'll see what kind of work this leads
to now what connects all of these works
is the fact that they focus heavily on
learning with n grams and the reason
I've spent so much time on this
particular feature representation it's
exemplified by a paper from a colleague
of mine a few years ago in which he
sought to simply establish baselines for
a variety of natural language processing
tasks in which he trained simple unigram
and by graham models to see how they
compared against deep learning and a
variety of other representations and
what was the huge result out of this
paper was that these simple models
actually won in almost every case or at
least competitive with state-of-the-art
methods so this is in comparison to a
variety of deep learning methods
hand-tuned rule based methods and
methods specialized to NLP I believe it
was in the transactions of the ER in the
ACL yeah if you look it up it's called
the baselines and by grams it's a great
paper yes it just means that it wasn't
that a result wasn't available in the
paper intuition that most we would find
that surprising because they would have
expected deep learning yeah
I think um I mean was your lesson much
less data than there is available today
how much training you know member so in
general these these particular problems
the largest one had about 50,000
training examples so there weren't
massive scale problems but uh yeah so
that that might help explain part of
that but I'll get to the large learning
scenario in a second um this effect is
seen in other areas too for instance
there's a recent paper on metagenomics
in which the authors again take just a
simple bag of n grams representation and
use this to get results that are
comparable to methods that are much more
finely tuned for problems in
bioinformatics one of the interesting
things that their paper also shows that
for their particular problem using
longer n-grams help performance
substantially and but here they ran into
some real computational issues they they
heavily relied on the vole pal webbot
and a large cluster of computers to
train with up to 12 grams and I would
argue that you know you generally don't
see such long and grand models because
of the computational burden that they
place on learning in NLP people rarely
go beyond trigrams so very briefly I've
heard of gab gamers sure yeah that yeah
I'd love to talk to you about those yeah
Gregor she isn't wearing that we look at
peace it's not too important just beyond
that they're kind of more finely tuned
methods for for the bio informatics
problem um so this computational burden
you know people have argued that maybe
longer n-grams don't really help these
NLP problems but in the paper with the
fast multiplication algorithm we
actually trained on some massive data
sets so these were beer reviews a one
and a half million beer reviews from the
beer advocate dataset and you know
millions to tens of millions of reviews
from amazon.com and in this case it
actually benefited the more data we had
the better we were able to leverage four
and five grams and so long n grams can
help performance this it's kind of
disputes that claimed that short n grams
are the only thing that's useful and the
key here was that our algorithm allowed
us to train this model very efficiently
about as efficiently as we would have
trained it with just using you and grams
or by grams in grandma's bed that's Ya
Allah Ya Ya Allah so a very briefly
introduce this end gram multiplication
algorithm and and show you its use case
in machine learning okay so suppose they
give you a bunch of text reviews let's
say of movies and each of them is
labeled with whether or not the reviewer
liked the particular movie and ask you
to predict based on the text of the
review the associated label so one of
the first things first and those simple
things that you can probably do is train
a bag of engrams model on this so what I
mean is a bag of n grams matrix is our
feature representation that stores a row
per review and a column for every n
Graham so an n-gram is just a substring
that appears in any of your documents
yeah well up to up to up to some some
maximum length that will well allow on
and each entry in the matrix just counts
the number of times that that n grammar
substring appears in that particular
document it's just the frequency count
matrix and what you'll probably do is
then train some kind of supervised
learner with this and the modern thing
to do is pose this as an optimization
problem in which you learn some vector
of coefficients that's somehow
predictive of your target if you look at
what we're really doing at the core then
any optimization virtually any
optimization procedure is going to solve
this with the exception of cyclic
coordinate descent is really going to
rely on matrix vector multiplication as
its core operation to learn essentially
all these up all these optimization
procedures like gradient descent
quasi-newton methods even some of the
more recent operator splitting methods
will just add various combinations of x
or x transpose x a vector
until they converge yes like SGD serg or
in december you said all those will do
that very infrequently yes oh so there
are definitely classes of algorithms
that don't rely on matrix multiplication
quiet quiet as directly or they use
other means but there's kind of a large
class that I do they said matrix all you
made a sizable u matrix vector
multiplication yes major exec remote
that's that's that's what I always
referred that's what I always mean in
this context yeah is that I'm include X
that is that multiplication like a
sparse multiplication so did that no
estimate include the fact that you can I
created it up yeah yeah so this this is
with sparse or or dense really whatever
is yeah I actually wanted to have a cute
graph here showing you know for various
algorithms and learning scenarios what
the timings were spent on multiplication
but it was generally always above ninety
five percent some you know typically
closer to a hundred percent so it's kind
of boring so I just said it instead so
with this in mind let's let's look at
what the multiplication time for a bag
of n-grams matrix can be if we define n
to be the length of the corpus so the
total number of words in each of the
documents then there are examples for
which it requires a quadratic number of
operations and and also space to do
sparse multiplication with bag of n
grams matrix and uh by leveraging the
suffix tree structure what we realize is
we can actually get this running time
down to a linear time and I'm not going
to get into the details of this
algorithm because they're primarily just
painful but uh we were actually able to
come up with a custom-tailored data
structure that really only captures the
necessary information to perform this
multiplication and in fact it provides a
yes to the they go then
and on the length of the yes I'm sorry
and is being used in two yeah yeah any
weather is capitalized or not but for 48
grams would there be constant they go
depending on the number eight yeah
excellent question so that's that's
basically how people battle this this
quadratic time essentially the running
time is let's say K times n where K is
the maximum and Graham length and so if
you to get this running time you
basically considered like the set of all
n grams or something about as large
letters meet again here the DK so each
of these are documents and so this is
just the total document length as
measured in the number of words in it or
whatever level of granularity you're
operating at we're not that familiar
with suffix trees I don't want to get
you the details of the algorithm but can
you give us the court in sight this to
be sure well so it turns out that
basically the there are two big
components to this the first is that bag
of n graham matrices typically contain a
lot of redundant columns that we can get
rid of and the second is that once it's
because of kind of the theory behind
suffix trees but a lot of times you you
have to end up actually it turns out
with with these strings where if you
have a substring and then it's followed
by a character and these always appear
in the same location so they have
identical columns among your dacha
identical frequencies among your
documents this is actually something
that necessarily happens for any fixed
corpus the other big speed up is the
fact that once you remove these
redundant columns you end up expressing
the matrix essentially just the linear
combination of columns corresponding too
long and Graham's based on the suffix
tree itself so all of this ends up being
just coefficient propagation up and down
a tree that's it so the the recursive
pneus leverages the nested pneus the the
redundancy is is just kind of a an
interesting side effect of in
are you familiar with suffix trees okay
so you think would be great if you could
try to explain this stuff assuming that
none of us are familiar with suffix
trees I just think that there are
probably some people here who are there
probably enough people who aren't that
okay if you could try to okay sure
essentially imagine yeah this is kind of
only be the teaser for this paper of the
talk but all I'll explain that briefly
um so in essence imagine that I have the
string X a X a you know just I have this
repeated then the way it we're done in C
occurs in this case is if you look at
every time I see x I'm also going to see
XA so at any given nod essentially these
these always co-occur and so among all
of my documents these would be these
would have an identical distribution or
identical columns and and and that's
what leads to your redundant columns and
grand matrices it's actually something
that if you remove these redundant
columns you generally end up with a
better learning performance to in terms
of accuracy the second component of this
is the fact that it is leverage as
nested pneus so the fact that you know
um if I you know if in order to get the
frequency counts for let's let's move to
a more complicated example suppose I now
have XB XC XB XC in this case to get the
frequency counts for X I could sum over
all the frequency counts for XB + XC and
that that leads to this kind of nested
pneus and essentially this tree
structure that we utilize to multiply
any matrix vector product with with the
with any bag of n grams matrix
for each back in the first singled
getting for all the day for all the
documents yeah so so you yeah yeah it's
a fairly big speed improvement um so the
suffix tree though is kind of only
conceptual in this work and suffix trees
themselves tend to have this nasty
constant overhead associated with them
but we actually came up with a
customized data structure that allows
you to efficiently store your data in a
persistent manner that's essentially
just fine tune to learning with n grams
and this data structure does all kinds
of pre-processing when given a new
learning problem it performs feature
normalizations figures out which n-grams
will be useful for the learning problem
and then outputs are custom bag of n
grams matrix and kind of cool about this
is this isn't just some kind of
complexity result it actually has real
practical improvements too so this these
are some memory benchmarks for a
multiplication routine and the reason i
don't have timing benchmarks here is
they essentially follow the same pattern
as memory all of these multiplication
routines are just the linear sweep
through the memory and the first is on
natural language data it's on the beer
data set and there are a variety of
representations that I'm comparing
against that essentially they come from
whether or not I've removed redundant
columns and whether I leverage the fact
that I'm dealing with frequencies so I
can store integers rather than doubles
in all these cases though my
representation is essentially close to
independent of the maximum and gram
length that you use and it's far more
efficient it's about three two times
three to five times faster than or three
to five times more memory efficient than
the next best method and that's what
allowed us to train at such massive
scales on just using actually a single
core on my own desktop computer to get
the results for the earlier results are
presented the the other improvement the
other data set that we looked at was
from snip chip data so here we had data
on about 200 people there chromosome one
and you can think of this as a binary
screen this data has a very different
distribution than natural language would
and here we got some remarkable
improvements so as the Ngram length
increases we can the suffix tree
multiplication algorithm can be
thousands to tens of thousands of times
more efficient than doing
is naively and and grand models are you
know cropping up in some areas of
bioinformatics but I would say that this
doesn't just allow us to learn faster
this now opens up a new door for what we
can do with n grams and be very curious
to investigate this further and any
collaborations that they might be
interested in yes so those type huge
emotion yeah yeah those I mean those
these top curves are essentially
impossible to represent so for this I
actually just you can use the suffix
trees to calculate the number of entries
the number of nonzero entries and the in
the naive matrix and then and then do
ugly gateway to the tape without
actually yeah I mean for for what I
could I actually constructed the matrix
and but you know everything lined up so
yeah those are faithful yes previously
should an N squared to end improvement
yes it could be a factor like
proportional to the size of corpus and
your crap seem to be showing the top one
is like a tenfold improvement on one
goes up to maybe thousand fold or so
yeah 10,000 so it looks like the
improvement factor is closer to the
arity of the end grams rather than
besides the entire corpus do first of
all is that a correct reading the graphs
and secondly do you have insight into
why maybe sure yeah so excellent
question and that kind of depends on the
on the structure of the data so this
this quadratic improvement is you know
for carefully chosen counter-examples
should wear suffix trees are great and
and but that have many distinct and
grants you're right in that it kind of
seems to depend on on some factor of the
maximum and Graham length and what that
is is there's there's typically a
certain n-gram length of which you you
have a lot of distinct and grams going
on in natural language it tends to be up
to about 7 grams and in this NLP and in
this bio example it was closer to
thousands or tens of thousands after
which things kind of calm down and and
the distinction between the tree and and
the naive methods stops being as large
so it essentially most of the
information is being captured by this
this initial set of Ben grams and and
that's where the speed improvement lies
was at a time bounder was that also
space though it was time and space yeah
ok so what I'd like to spend the rest of
the time focusing on is Dracula so like
to introduce you briefly to what this
new deep learning paradigm is and to
give you a sense of some of the problem
structure and theory that that that we
can say about it so I'm first introduced
the criterion and then show you its
structure of its solution surface and
actually visualize it this is kind of
unpublished work so it's kind of
exciting to get it out there for the
first time and I'll do this by means of
showing you how to express Dracula as a
binary linear program now the reason I'm
visualizing its solution services for
anyone who's played around with GL m-net
one of the things you get out of it are
these regularization paths that tell you
how the models coefficient how the model
coefficients change as a function of the
regularization parameter and in some
sense what we're going to do here is do
the same but for a more sophisticated
regularization path that involves many
parameters and finally I'll conclude
with some experiments showcasing its
features so suppose I give you some text
and you want to compress it probably the
simplest thing you can do is set up a
shallow dictionary compression scheme in
which you store a dictionary of
plaintext and grams or strings as well
as a pointer set indicating where you
should paste copies of each of these
n-grams so as to reconstruct the data
losslessly so here I'm assuming that
everything is reconstructed perfectly
under this representation we can improve
upon this substantially by making it
deep and this is motivated by a very
simple example in which I have a string
of the letter A replicated an
exponential number of times in this case
the the best that the shallow
compression can do is to store an
exponentially long string in plain text
and to use an exponential number of
pointers so the overall representation
length is still exponential it's
somewhat unsatisfying but imagine now
that
have a scheme that's allowed to compress
its own dictionary so in particular
dictionary and grams can be expressed as
a sequence of characters and pointers in
this case the dictionary strings I'm
going to create our kind of the sequence
of strings that is exponentially
increasing in size but every time I
double the shrink size i only use two
pointers what this results in is a total
linear number of pointers or a linear
representation size so an exponential
improvement over the over the shallow
compression scheme and that's exactly
the idea underlying Dracula Dracula is
this deep extension of the shallow
compression scheme in which we allow it
to compress its own dictionary so in
this example we've gone from storing all
and all these engrams and plain text
it's actually noticing that X a B and X
AC occur quite frequently in the corpus
or in the document and they both share
XA so we can we can simultaneously
reduce the total number of pointers used
to reconstruct the documents but still
keep a reasonably small dictionary and
you'll notice that this is also
equivalent to a particular kind of
directed acyclic graph so we can use
this notion to very precisely define the
depth of our network and and draw some
parallels with deep learning networks
now the way we're going to use this to
extract features for learning is very
simple we're going to create bag of n
Graham's features and what's slightly
different though is that we're going to
for a particular Engram that appears in
the dictionary we're going to count the
number of pointers that use that engram
to reconstruct the original document so
this is different than the traditional
bag of n Graham's representation which
for instance 4xa would count all of its
occurrences in these documents that be
five and contrast here we only have one
pointer that uses XA at the very
beginning of the string and so it gets a
count of one in the corresponding
feature representation and we have this
you know this thus far just discusses
the actual document says nothing about
how we might take into account
dictionary structure yes
impression goal is just to compress the
document or is it press all points all
and be able to vacuum it is so sure the
great question be the compressions point
is to compress the documents but the way
it's it's a dictionary based compression
scheme so it's going to store a
dictionary and it's also going to
compress that dictionary to gain further
space savings so the reason that they
only comes once is because xeb and ex-ac
took care of the other areas exactly
exactly yeah so we you know by by
constructing this representation we're
kind of forgetting about the strings
that are used to construct those n-grams
and well I don't want to get into it too
much we can actually compute the exact
same feature representation for our
dictionary in grams and we can then
define a kind of diffusion process that
results in a regularizer that takes
dictionary structure into account but
what's kind of interesting what was kind
of an interesting result from this work
was that the way you construct your
dictionary really only matters if you
employ some form of regularization in
essence its structure it gets cancelled
out much in the same way that a
unregular eyes the generalized linear
model will cancel out feature
normalizations if you don't add any kind
of regularization term my intuition is
off but it seems like you might in many
problem settings want to actually count
each of those X's even when they're
followed by a B or C so is this the fact
that you don't you're saying under some
circumstances whether you do or whether
you know I'm saying under some
circumstances you you may not want to UM
didn't some of you made very well like
when I'm thinking of biology poems I
male I would think that I wouldn't want
them at sure yeah and and I'll kind of
discuss the representations you can get
with Dracula but but you can get these
like redundant representations where
where you do count things multiple times
um yeah
that must be pretty lost but I thought
that the point was that you can not
count those and then just when you learn
ex post like that it's taken into
account that they're included within the
other that right so that's that's what
taking into account the way the
dictionary structure would do so you
yeah present is it but oh well it's it's
it's it's kind of one extension so if
you think about traditional deep
learning stuff you you typically will
take the the top layer of the network
that you learn and use the features from
that or you can try to take into account
the whole network in a more
sophisticated manner and this allows you
to do that by the dictionary diffusion
process or you can just take into
account the top layer corresponding to
which pointers directly correspond
directly reconstruct the dictionary
documents nice example just understand
so so there's two problems one is that
you may or may not you have to keep
track of how many times XA occurs and
another one is that I like a bee or not
sorry DX which are cursed out if you are
interested in bx that's not even showing
up at all yes yeah yes you're going to
somehow smartly learn the right
compression Langston whatever your it's
a supervised problem you're facing
whatever your classification is you'll
see are what is the right compression
yeah for that that's yeah that's that
that's actually why I'm going to get
into its solution surface because you're
going to want to fine tune your
representation maybe you do want to take
into account you know every instance
that things occur and so you know kind
of one of the questions I'm going to
look into is well what happens when we
try to fine tune these representations
the things behave in a reasonable manner
you've done down as we learn the
representation with the supervised
labels or you have like a pre-training
phase and then you find
so right now for this work it's it's you
just have some kind of hyper parameters
that you might tune over a grid it turns
out so you would I mean the the way I
explored it here was just tuning the
high parameters without the label data
and then this gives you a number of
representations that you can choose from
and then you can do model selection once
you have a supervised learning problem
it's not yes right now this is
unsupervised yeah so in it actually
turns out there's very simple extensions
where you can kind of add a supervised
term to this and that's kind of a
follow-on paper that I'm working on
referred a couple times to the
dictionary diffusion process yes are you
gonna explain more about what that is um
I'm gonna it's not the folks of the talk
yeah so so yeah just just the the the
interesting point about that was the way
you reconstruct your dictionary really
only matters if you employ a
regularization like for unsupervised
learning here so if you think these is
words mmm you be fighting common phrases
so the good the good way a good
dictionary would have phrases that are
naturally phrases you wouldn't be
raising it up this radio exactly yeah
hopefully you're finding motifs yep
exactly all right so now that we have a
sense of what the structure underlying
Dracula a little bit I can actually show
you how to express this as a binary
linear program so let's go back to the
shallow scheme in this case one of the
first things I need to do is figure out
which n-grams are present in my
dictionary and the way we're going to do
that is consider some universe of all
possible n-gram that we might include in
the dictionary and teach of these
n-grams we're going to associate an
indicator variable telling us whether or
not its present in the final compression
second part of this is to figure out the
pointer set and here again essentially
we're going to consider some universe of
all possible pointers and to each of
these potential pointers we're going to
associate another indicator variable
telling us if it's in the compression
and this actually creates quite a few
variables if you think about it because
we now have you know for any particular
Engram that might appear in the
dictionary and for any particular
document in our corpus
we are going to look at all instances of
that engram in the in the document and
and that's going to generate a set of
pointers and then we're going to iterate
this process over everything sure sure
so T is just the variable I'm using to
to correspond to the end grams in the
dictionary so you can think of T as
directly representing a dictionary so T
sub s here is a particular n-gram and T
sub s is one if if that engram is in the
dictionary and p is just a pointer you
can think of a pointer as a triple
specifying the document that it can be
used in the at the Ngram that it uses
and the particular location the main
takeaway of this though is that we're
creating a lot of indicator variables
corresponding to potential dictionary
terms and and pointers using those
dictionary terms with this we can now
define the basic building block of the
compression scheme which are these
reconstruction modules and what a
reconstruction module does is it takes
in a fixed dictionary and for a
particular document it tries to compress
that document optimally in terms of
using engrams that are only present in
the dictionary and so what it assumes
that you're given some kind of cost
vector that specifies the cost of using
any particular pointer and why we may
want to have different costs depending
upon the pointers is something I'll get
to in a few slides so it's a great
question it's interesting because you
know classically you kind of think of
this as as storing as trying to minimize
on disk space so performing compression
and that corresponds to a particular
setting of the cost vector weights but
ultimately since we're in a learning
scenario we're going to try to maximize
a prediction performance and it's you
know one another one of the questions
that I that I've been trying to best yet
is well when do learn entation xulai
natural cost-saving representations so
it's it allows us to to explore the
learning domain but kind of take some
ideas from from
the compression domain compression and
regularization help yeah i mean we're
hoping that by doing some kind of
common-sense compression we're
essentially going to extract useful
features so the we can then write this
out as a binary linear program in which
the objective is simple it's just if we
include a pointer we pay the price of
including that pointer in the in our
representation and the two main
constraints of this binary linear
program are a reconstruction constraint
which forces a lossless reconstruction
of the original document so every
location must be accounted for in some
manner and the second constraint in blue
states that we may only use n grams that
are present in the dictionary so if a
pointer uses an Engram that's not
present in the dictionary that pointer
is disallowed and of course all of our
variables are constrained to be binary
now if you look at these constraints it
turns out that they're totally
unimodular and so this problem actually
already has a lot of structure to it and
that we can simply cast that as a linear
program and it will always give us a
binary solution so this problem is
actually easy to solve and in fact its
equivalent to a particular kind of
network flow problem that has me
wondering if a classical computer is
actually the best way to solve this I'll
leave it at that and with this we can
now define the I'm not sure yet more
I've been looking at analog circuits for
this this is just one one kind of
fanciful thing I've just taking a look
at but that you can set up analog
circuits for flow problems and so
sometimes these things would be like
millions of times faster and practice so
you know I'd be curious where that leads
to I it's it's kind of a sephora it's a
kind of trellis with with with backwards
with with units with head with backwards
edges as well but i'll i can oh it's
like a trellis yeah it's it's a
particular kind of a directive network
corresponding to two what's present in
your document
I'll take that one off line if you don't
mind yeah so the way we get to finding
an optimal shallow compression scheme
and we call this compressive feature
learning in our first paper is by
jointly learning over what's present in
the dictionary and so here this variable
T we're now learning over that as well
and we simply share this dictionary
among all of the reconstruction modules
for all of our documents that's that's
what this first term represents and the
second term represents the cost of if we
include a particular n-gram in the
dictionary since we have to store it in
plain text we have to pay that price
kind of that one-time cost of storing it
now when we learn over the dictionary
and pointers jointly this problem
becomes NP complete so you know that's
kind of a bummer because the original
problem was so reconstructing was so
easy but there's a lot of structure
there that we can leverage and actually
use this to construct a number of fast
homotopic algorithms that give
approximate binary solutions and in
particular it also gives us a really
easy way to take any continuous solution
and round it to an explicit binary
solution so we can extend the shallow
scheme to become deep in a very simple
manner essentially we think of
dictionary n-grams as these conditional
documents that only need to be
reconstructed if they're present in the
dictionary and so we preserve the binary
linear programming structure of the
problem and dracula's objective is
expressed by simply replacing that term
on the right with a more sophisticated a
reconstruction cost for how much does
this Engram require to reconstruct given
my dictionary essentially some represent
some representation of this yes it looks
pretty different than a typical deep
network like that a neural network yeah
yeah it's it's really based on
fundamentally you know different I mean
yeah i'm calling it deep because we we
move from this you know shallow scheme
to having multiple layers
alright so well actually one of the
things that we that you've noticed now
is I've introduced even more cost terms
into here so we now specify costs for
the pointers used in documents and
pointer and costs for the pointers used
to reconstruct the dictionary and so
what I'd like to characterize a little
bit is the solution surface of this
problem and if you can imagine in the
learning setting now maybe I've gotten
some kind of compressed representation
but I'm given a learning problem and so
I'd like to fine-tune that
representation what I mean by that is
I'd like to wiggle my costs parameters a
little bit to see if I can get in some
representation that maybe employs more
redundancy if it's necessary or a less
redundancy that ultimately provides
better features for the learning problem
just didn't back up a second so the the
goal here is you're given let's say just
you had a collection of beer reviews yep
you're a bunch of here he is your goal
is to come up with a representation of
these reviews yes which you can then use
to do what which we then use to extract
features in this bag of n grams matter
and then use that downstream on some
kind of learning problem editor features
that you would typically get or is it
more 66 what is it what is the goal of
this up hopefully better features yeah
at a minimum you know it can kind of
serve both purposes sometimes you can
use the speed-up training time by
selecting only the corpus of documents
classes really good features yes and
then yeah for for supervised learning so
um so this so I've defined as I
mentioned a lot of different costs here
and let's let's kind of simplify things
down to a common sense cost scheme so
we'll assume that all dull pointers used
to reconstruct documents uniformly have
the same cost all pointers used to
reconstruct dictionary and Graham's also
uniformly have the same cost except for
yoona grams of used in the dictionary so
you know grams now play the role of
characters there are these somehow
atomic elements that are that are
special and so we may want to allow them
to be discounted essentially we say well
you can you can write this and Graham as
a sequence of characters that are cheap
or you can replace some subset of it
with with pointers from from
order n grams and these parameters
actually give us have a very
interpretive little effect on the
dictionary and resulting representation
so the increasing the dictionary pointer
cost net increases the cost of any
particular dictionary and so it's kind
of favor dictionaries with smaller terms
of them essentially simpler dictionaries
it plays this quasi l0 regularization
role in the problem but one of the
subtle effects that it adds is this kind
of grouping effect and that because I
can leverage existing and graham terms
i'm going to prefer to include n-grams
in the dictionary that that can be
expressed as as a combination of engrams
that are already present so in the
example I had X a B and X AC both in the
dictionary well they both share XA so
I'm likely to Inc once I have X a and
the in the dictionary and likely to
include either one because they're
simple to reconstruct the other
parameter alpha which is the character
cost simply modulates the dictionary
depth so as alpha gets small relative to
the cost of any pointer it becomes
cheaper just to store that engram in
plain text rather than reconstruct it
using pointers and so it directly
shrinks the depth of the dictionary and
if you tune these parameters you
actually can get a variety of
representations so one of these is if
you set everything to be negative you
get this maximally redundant
representation that you know tries to
waste as much space as possible but in
doing so it includes every pointer every
and gram and you if you then carry out
the bag of n grams a feature extraction
procedure you end up with the
traditional bag of n grams so in
particular your question earlier that uh
maybe I want to use replicated copies of
maybe I want to count every time that XA
occurs you could do that essentially by
setting the document reconstruction the
document pointer cost to be negative it
would it would it would do that
automatically freshly for
like particular length grabs right like
say you only wanted the exhale you
didn't care like you couldn't sort of
differentially get the property sure
that's that is true but you know you
could then move to a more fine-tuned
costing to allow that capability this
isn't the case that this algorithm
interest rate negatives you could just
use a different elgar yeah I mean it
would be a little bit of a waste to use
them but it's it's just interesting that
this is a valid solution to our problem
we can also get a completely shallow
representation if we just set the
pointer cause of the yet point across to
be very high relative to the character
cost but in general the kinds of costs
I'm going to be interested in this
particular context are ones that
correspond to some notion of true
compression and that they save on disk
space and this is attained by setting
all these constants to be non-negative
so in light of the earlier question I
posed what happens when these parameters
vary slightly as as a function of T that
we're let's say we're trying to
fine-tune each of these costs to the
particular learning problem at hand so
we're going to generate a variety of
feature representations and see which of
these provides the best features for a
learning problem by varying its
parameter T in order to understand how
the solutions very we're first going to
look at the continuous relaxation of our
problem so this is formed by requiring
that all variables be between zero and
one and are no longer binary in this
case I end up with a simple linear
program whose constraint polyhedron
includes a variety of nonsensical
fractional solutions but it's
interesting here is that if you can
obviously this other set of all dracula
of all binary dracula solutions is valid
for this relaxed form and since there
everything is constrained to be between
zero and one they actually are vertices
on this relaxed polyhedron so if we take
these vertices and take their convex
hull we end up with another polyhedron
that has only binary vertices and this
corresponds to a polyhedron for the
original dracula problem in essence we
can view dracula as a linear program
over a sufficiently constrained
polyhedron that only has binary vertices
now
less p equals NP or you know the in
general this might be difficult to
express algebraically but it is a
polyhedron nonetheless in fact the
shuttle gomery theorem shows us that we
can take this this polyhedron the
continuous relaxation with many spurious
vertices and prune it down via by adding
a sequence of linear inequalities to the
to the system that is actually cut away
useless vertices until we get the final
binary polyhedron and I was able to use
some theory from suffix trees to define
a series of cuts that get you at least
part of the way there with this now we
can leverage some machinery from linear
programming so we can view Dracula or
its continuance relaxation as linear
program and kind of use the same tools
to answer questions for both regimes in
particular I proved the theorem and the
paper that it'sit's proved in a much
more general setting but for our context
essentially tries to answer the question
what happens to the representations as
we vary across parameters continuously
and I'll actually show you what happens
yes
dimensionality give this LP relaxation
yes is equal to the number of potential
and grooms plus the number of potential
pointers that you might have okay so
good do you have a curious actually
solving this thing yes it does one have
to I shouldn't have to apply some tricks
in order to be able to solve high
dimensional problem even in the
rationalization yeah yeah absolutely i'm
that's its kind of why I've gone into
optimization so much is there's a lot of
problem structure there that you can use
but you do need to use some tricks to
handle everything correctly and one of
these is moving into a dual space where
you can kind of be independent of the
number of pointers and just work with
length the documents as an example yeah
so the upshot is after applying all the
optimization trips you note for like
eight grams on the corpus of English
texts yeah the selfie is actually it's
multiple distractible yeah I the shallow
version of the LP I solved a few years
ago on on the beer review so that was
one and a half million reviews and that
was in all honesty me still kind of
learning some tricks and optimization so
that algorithm can be improved on
substantially so this is actually for
the first time ever a Dracula's actual
polyhedron corresponding to the text
that I've that I've been using it as an
example through this presentation this
is a kind of three dimensional nonlinear
projection of all the vertices that we
can reach as optimal solutions of of the
cost scheme in which our parameters vary
in this manner and it's dual is shown up
in the left there and the important part
about that is that each of its faces
corresponds to a vertex on this original
polyhedron and assuming our cost
functions never becomes identically zero
we can without any loss of generality
just assume that our cost function lives
on the surface of this duel polyhedron
so the first part of the theorem states
that as long as we remain on the
interior of of the face of this duel
then the vertex the optimal solution
remains the same it's
just constant and in this case this is
the actual solution corresponding to
this somewhat deep compression of the
text as soon as we move to another face
of the polyhedron the representation
changes and it's now constant as long as
our cost vector relies on that other
face and so in this particular case
we've moved to this is kind of
completely shallow representation so
thus far I've shown you a scenario where
we have this kind of piecewise constant
behavior of the solutions and the way
transitions occur is for that brief
period when I am let's assume for
simplicity that I get from one phase to
another only by crossing edges this is
actually fairly easy to enforce and
practice for that brief instant when I
lie on that edge where my costs lie on
that edge of the dual polyhedron both
solutions corresponding to both of the
vertices here are valid solutions and in
fact any convex combination of them is
also valid so you know we essentially
we've got this piecewise constant
behavior in which transitions occur with
brief periods of non uniqueness of the
solution and what matters in our context
is that what the states is if I'm at a
particular vertex if I only move along
edges the only other solutions that I
can move to are ones that I'm directly
connected to via an edge so the
solutions i can jump to by fine-tuning
my costs are entirely dependent on the
combinatorial structure of the
polyhedron and in particular i'm not
going to jump to arbitrary solutions
that i'm not connected to so for
instance this one back here corresponds
to the maximally redundant
representation in which i include every
single pointer it's not possible if I if
I'm moving on these two faces and I'm
continuous to to jump this as a solution
unless I go all the way to the back of
the to the back of that polyhedron so
there is some nice predictability at
least in what's going to happen ok so
I'll very quickly go through some
experiments showing the efficacy of the
features I'll skip the unsupervised
experiments but the point of this one is
that features actually can provide some
interesting
star plots of NLP data this particular
experiment uses used the shallow
compression scheme to extract features
for reviews written by about 10,000
authors on the beer advocate data sets
of this was quite a few reviews and and
there we were able to use the LP
structure to the Train fairly quickly at
this this was running on just a single
computer and it took a couple of days to
sweep over an entire parameter grid and
I used resulting features in an author
identification task in which given a
sample of each author's writing I had to
predict which which author wrote that
particular sample yeah it's a little
over 10,000 so there's quite a few um
yes yes it's it's the baseline of acting
randomly is quite low and you know grams
and trigrams do reasonably well but what
was what was amazing was that we doubled
accuracy using these compressed features
for this problem and this is curve on
the left is it kind of shows what
happens as the as the character costs in
the dictionary changes and it's it's
kind of implied by our LP theorem that
that the performance that the
representations shouldn't change too
much from one from one shift to another
but it's nice to see that in practice
this is the case so we get this very
nice validation curve as a result so
that to me was you know it was good that
uh kind of had some common sense about
this problem so next experiment and this
is the final 1 i'll talk about looked
again at the author identification
scenario but here to change things up i
took 10 authors from from the reuters
data set and I first parsed there's
their writing into part of speech tags
and then compress those there was no
notion of context anymore it was just
grammatical structure that we were
trying to identify and the goal of this
experiment was to see okay we we've seen
a case where shallow compression can do
better than n grams it seems to the
naive and grams it seems to identify
particularly useful features does adding
depth to the mix
in a particular cost saving scenario at
benefit us as well and this is some
small evidence that this may be the case
so as expected the shallow compression
scheme does better than the all n grams
representation CFL is the same as is the
shallow scheme so CS setting the
character costs to be very cheap yeah
but as the for various pointer costs we
actually saw an improvement on the
relative to the shallow scheme as well
by going deep and these results are kind
of preliminary and that the deep version
of the Dracula I'm still working out a
fast algorithm to solve I used Groby
actually to solve it so it was even in
this setting girl be managed to I was
impressed with that was able to solve it
but there are a lot of specialized
algorithms we can build for the deep
setting as well and I've shown you
briefly is an introduction to uh to
Dracula and and briefly touched on some
of the results of our fast and grand
multiplication paper um going forward
one of the most immediate things that
I'm working on is a fast algorithm to
solve Dracula that really leverages its
structure and my investigation into
suffix trees and duality was no
coincidence both of these are major
components of an algorithm currently
developing that allows it to scale to
truly massive dimensions another act of
avenue of research for me is isn't a
structured matrix multiplication i
believe that there may be a surprising
number of feature of kind of traditional
feature representations in machine
learning that have structure we can
leverage to multiply quickly and
expedite learning and i'm actively
looking for these some i'm currently in
the process of publishing an algorithm
for fast multiplication with the
binarized bag of n grams in which
nonzero entries are simply replaced by
one a longer term what I'm very focused
on is moving into the continuous data
realm there i think dracule will truly
be able to shine and kind of I'd like to
put it up against some of these more
traditional deep learning networks to
see where it stands on those problems I
now continuous data is is going to pose
a whole new set of problems because in
text we get away with
suffix trees because an a is just as
dissimilar to a be as it is to a see
with continuous data you have to use
things like error tolerance and
invariance to draw connections between
samples and I'm excited to see what kind
of algorithmic challenges this brings
and how I can solve them I'm also
interested in massive scale optimization
problems so if you have hard problems
that you're struggling to solve come
talk to me I'd love to see if I can
apply some of my knowledge to solve them
and if not I'd love to learn new
optimization theory to figure out how to
do it and finally when I think about
some of the most important structured
data sets that we have out there
immediately think to bioinformatics
things like DNA reeds and protein
sequences and so I'm very curious to see
how my existing set of algorithms it can
benefit this this data but I'm also
eager to see what kind of specific
problems come from this data and how I
might use this to to further develop my
algorithms so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>