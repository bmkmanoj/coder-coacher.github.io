<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Real-time Augmented Reality Processor and a Smart Glasses System | Coder Coacher - Coaching Coders</title><meta content="A Real-time Augmented Reality Processor and a Smart Glasses System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Real-time Augmented Reality Processor and a Smart Glasses System</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8M9BTehLZsI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon everyone so thank you for
coming over to the top and I'm very
excited to introduce mr. Callen Kim here
he's a first year PhD student at Christ
South Korea working on energy efficient
hotter architectures that token children
and the silicon implementation today
he's going to talk about his on low
energy augment reality chip and its
application to a a glass prototype all
right please Thank You Jo today I'm
gonna present you a real time of
materiality processor for a smart
glasses system so this is the outline of
my presentation first I will start with
the introduction of augmented reality in
the introduction speed and energy
constraints for classes system will be
explained next the target I will do
buckle is augmented reality will be
explained in the next section and then
chip implementation will be presented
with two key features they are Pascal
pipeline mini seemed architecture and on
your network based scheduler for
two-dimensional mesh network on chip
after that I will show you the chip
implementation wizard and it's system
implementation and they conclude my talk
first introduction before defining AR I
want to explain you a recent mobile
system hair dye there is a certain
relationship between a user and a mobile
system mobile device and real and the
rearward a user controls the device and
get experience through UI UX by analogy
by analyzing information given through
the sensor in this relationship we want
to
minimize the control while maximizing
the experience to minimize the control
we need an intelligent device by
removing inconvenient controls also it
has to support seamless natural feature
and responsive interactions to maximize
the experience for this reason augmented
reality is proposed to realize this new
paradigm AR is widely used in a variety
of applications the potential
applications are entertainment and
next-generation UI UX and the
combination with location-based services
the Dolans company XP ericsson mobile
categorize they are technologists in 244
levels according to their computation
complexities the verse 0 is hyper hyper
linking hyperlinking is not actually a
are but it was a trigger for air
technologies when we take a picture of
the QR code in the paper on the paper
the corresponding website pops up level
one is macro base they are recognizing
the marker on the paper a 3d object is
displayed 3d CAD tools such as
SolidWorks support this kind of AR for
an example or designer a house designer
can show his virtual house design
everywhere with a smartphone and the
printed marker however markers limits
the AERS application nevertheless most
of mobile services support only level
one AR yet level two is Macaulay seer
recognition without markers makes a our
algorithm more computationally heavy but
militia can be applied to
a variety of applications for this
reasons McCoy Sarah is actively studied
in the recent computer vision research
but really implemented in the mobile
systems lastly level three is augmented
vision augmented division is a
completely wearable air system since
it's fun factor is very small like items
while it has to support level to a RI
withum level 30 level ever three AR is
still on that a near future technology I
realized level two malicia for the smart
class platforms this page briefly
explains the constable michael is a our
algorithm that i implemented AR is an
augmentation of real world and the
virtual world seemingly but kaliesha is
composed of two steps first the target
object should be recognized and then
it's 3d model is patched and displayed
on the device so a 3d car animation is
displayed on the figure of the book to
realize vivid and realistic book reading
now we can see crochet chi minh on all
sides instead of a 2d image however
there are some hardware requirements
when implementing a are in an head
mounted display systems first thing is
high throughput I in the previous slide
I already explained that the AR consists
of two stages which are recognition and
visualization in the past backrub HDR
was actively investigated and a marker
on the object makes
Commission task so trivial however since
attaching markers on all objects are is
an impossible solution recently the
bakken easier is the mainstream in the
computer vision research in mccalla CR
for the general general object
recognition we have to rely on natural
feature extraction algorithm which
requires more than 10 times higher
computation compared to the macro based
they are so it is almost impossible to
realize it with real-time operation on
the current mobile system this page
explains the speed speed limit of the
modern application processor when
implementing a are then in this second
reference using Snapdragon s4
scale-invariant feature transform or
shift is implemented which is one of the
most widely used feature extraction
algorithm despite the the input image
resolution is quite low and without
without processing data base matching
the overall throughput is only six
frames per second according to this
reference the the reported performance
is optimized with mobile GPU and opencl
library because pictures section is a
only of only a part of recognition based
AR the food chain of air cannot be
implemented with the more than a piece
like snap to export to remove software
optimization issue in the previous slide
I assumed an ideal implementation wizard
in this page in the Assumption quad-core
processor of 1.2 gigahertz is employed
and each core has a
128-bit neon processor in this
assumption all four neons are assumed to
have one hundred percent utilization
since neon can process simultaneously
eight integer operation at a time its
peak performance becomes 38.4 Giga
operation per second or gobs with when
the recent vision processors with
hundreds of jobs are taken into
consideration its peak performance is
quite low even with 100-percent sim
utilization also the image resolution is
an important factor as well processing
in low resolution degrades the
recognition rate as shown in the left
graph it is a study of recognition rate
by the size of the size of target object
so highly solution is required for the
high recognition but it causes tight
hardware design constraints such as
throughput and own shape of chip
bandwidth the second requirement for
implementing a RNA in on a change the
system is low energy compared to the
previous boobear system PDA and
smartphone hmd has a smaller better size
to read right however hmd requires more
computation power to support complicated
macalistair functionalities therefore
there exists a huge energy gap between
the power supply and power demand
therefore we need to
fill this gap with the hardware
energy-efficient hardware implementation
to make my point clear I met I measure
the speed and the power consumption of
omagh for which was integrated in the
recent smart glass product I used only
for multimedia panda boot instead of the
smart classes form for the convenience
here is a specification of MA for it
will press at one point two gigahertz
and it contains arm cortex a9 ok I
tested these four algorithms which
composes they are as you can see here
the frame rates are very low far below
30 point per second of your time
operation and the power consumption is
also quite high considering the glasses
small better sighs there for high speed
and low energy requirements for the high
speed and low energy requirements are
high throughput and low energy hardware
implementation which is dedicated to
dedicated to level two Melissa are is
required this slide summarizes my
contribution first Earl real-time AR
processor chip is designed it features
test cable pipelines in cluster and
congestion aware that token ship
scheduler this is for high-throughput
implementation and this is for the low
energy consumption it's pic performance
is one point 22 tops and it concerns 381
milliwatts on average also this chips
evaluation board is designed and it is
integrated inside the prototypes smart
class prototype of smart class
its name is Kay glass and it has 800 x
600 microdisplay and it can sustain for
our AR operation here is a short
demonstration video of k glass this is
wearing image of chaos first we test it
we tested it in the bookstore in Christ
here is the micro display output it can
recognize each magazine on the chefs and
when we pack when we pick up a car
magazine the Porsche Cayman is detected
and the 3d model is displayed on the
left side and additional informations
are on the right side you can see the
other side of the car by rotating the
magazine
and detailed information of the chip is
shown here this is power consumption and
frame rate and the second place is a toy
store virtually visualized toy figures
are first we recognize the front of the
boxes and virtually visualized 3d toy
figure is shown so we can see the
assembled toy figure without actual
assembly sorry there are some jittery
comp and comes from the camera pose
estimation error and some kind of using
optimization is required ok from now on
attention based macula CR will be
explained in in this section we divide
on Malastare process into four stages
which are visual attention stage and key
point is Tecton stage in elgin batching
and post calculation stage last visual
rendering stage first we select region
of interest from the input image to
reduce the local reduction workload
processing workload for the further
stages and then key point vectors are
extracted only from the selected
otherwise the extracted key points are
utilized here first object matching
utilize the instructor key points to
select object in the database and post
calculation stage is to calculate the
pose of the two front camera in the hmd
finally the rendering image is displayed
on the device
a lot of vision processors I have
adapted our selection as a
pre-processing all right selection which
is also called visual attention select
only a meaningful reason from the input
image to reduce the processing work code
from the of the following stages the
smallest unit of the ROI the reason of
interest is our small rectangular image
tile usually between eight by eight
pixels and 32 by 32 pixels type
processing enables a smaller on-chip
memory because it the chip does not have
to store all of the total input image
and also independent type processing can
be implemented with the help of
paralyzed hardware so we can speed up
the overall air processing this page
shows the effectiveness of visual
attention combined with feature
detection algorithm the number of
features ducted the number extracted
features are reduced from 702 100
however the the only features from the
backgrounds are removed so the overall
observe recognition accuracy does not
degrade it however conventional spatial
domain visual attentions require heavy
computations for example in rural
entities or visual sellers map it
generates 7 image pyramids which
requires a lot of on-chip SRAM and
tankers time consumption and the
center-surround operation is performed
to get the finer
feature map and find feature map and
feature maps are are integrated to
generate the the sailors map due to its
heavy computations it spends too much
time and degrees overall throughput as a
pre-processing with your attention must
not consume a lot of a lot of time to
reduce the amount of computation visual
attention algorithm in frequency domain
is proposed its name is face spectrum of
catania video transform or pick up t it
generates only four channels of of image
but it does not need pyramid image
pyramids instead the fourth channels are
fury transformed into the frequency
domain and only the region of interest
whose face whose phase differences our
large are selected this page shows the
time reduction of frequency domain in
african domain visual attention to
visual attention I Williams are
implemented in the PC environment and
the special domain visual attention
spend 0.4 second and featured section
time is reduced by half as a result only
60 point zero point six second is saved
which is not large as a pre-processing
on the other hand special the frequency
domain visual attention spans 0.08
second with the similar attention gain
so in in terms of throughput frequency
domain visual attention is more
appropriate for the high throughput
applications another simple technique is
used to increase throughput
previously observer cognition and comma
tracking utilizes different kinds of
feature detection algorithms as an
example of the vehicle engine utilizes
shift and the Karma tracking utilized a
fast algorithm however feature detection
algorithm concerns a lot of time as
shown in this graph so simple language
modification enables the feature
detection algorithm sharing and it
increases throughput a lot so with the
helpful visual attention in frequency
domain and feature extraction sharing
the processing time is reduced by
seventy-eight percent and the overall
throughput is becomes 4.5 frames per
second however the the performance with
software optimization is not enough for
the real-time operation so additional
speed up is needed to achieve 30 frames
per second performance to accelerate a
are more to the real time operation I
design the dedicated chip named pune yar
poner is an abbreviation of basic
on-chip the track for augmented reality
it is developed to accelerate the muckle
is a our algorithm that I explained so
far previously various kinds of visual
vision processors will develop to
accelerate computer vision algorithms
mostly related to observe recognition
here and there pick performances are
hundreds of gobs and consume less than 1
watt except touch pass chip and several
techniques of hardware and algorithm
will
Lloyd to enable real-time of the
recognition but their performances are
not enough to process vocalist they are
in real time in this page overall
architecture of the proposed AR
processor is shown there are 32
processors in the chip and they are
connected with two-dimensional mesh
network first visual attention processor
or blue AP is responsible for the feed
board visual attention and scale-space
processor or SSP and kdp and DGP are
responsible for the feature instruction
and key point matching accelerator or
kma selects the target object in the
database and pose estimation process or
PEP calculates the pose of the camera
and finally the rent the restoration
Lester ideation processor or RP is
responsible for through the image
rendering although this processor
executes the fruit chain of AR of
michael is a barclays augment ability
i'm going to focus on the first two
first two steps because they include the
most of computation and let talk issues
from now on the proposed hardware will
be explained in detail to exert a our
data level parallelism Oh dlp and test
capitalism or TLP are exploited the AR
process has both of high dlp and TLP
every image style is processed with the
same series of the operation which
corresponds to TLP so for the dlp
acceleration
we can use vector paralyzes vector or
polarization due to its data
independence on the other hand for the
TLP which process consists of several
tasks and they have dependencies between
them in this case pipeline technique is
employed can be applied to accelerate AR
process now we have two kinds of persons
to be accelerated first dlp is about the
acceleration inside the stage
heterogeneous many sims peas are
employed to accelerate it stages TLP
there are six types of heterogeneous
same course and their their functional
units are dedicated to the corresponding
stages function and the number of cores
are determined by the degree of data of
a peloton of each stage for the TLP task
level pipeline is is employed over the
whole layer process similar clusters are
tightly pipelined with hike or
utilization by running them
simultaneously however pipelines in
architecture causes or simultaneous and
massive data transactions between
clusters because the task the very
pipeline incurs the produce task
producer and test consumer relationship
therefore network congestion problem
arises again arises and an intelligent
routing rule is required in the figure
heavy data transactions are are marked
with red arrow and these two
transactions are the largest
the transaction first I did some
numerical analysis on the network in my
trip to detect key points we stacked a
Gaussian pyramids and it accounts for
the largest portion of the data
transaction usually it's size is ten
times larger than the original input
image size second portion is key point
descriptor suppose that we have two
thousand key point in the current frame
then 256 kilobytes of descriptors should
be generated and then transferred to the
to the other clusters as a result one
point 28 gigabyte per second internal
bandwidth is required on average with
eighty percent processing time margin
222 so to process a AR in real time in
fact the amount of internal data depends
on the complexity of input images here
are two examples the first low
background clutter image has only 70
region of interest and 700 feature
vectors on the other hand the high
background color image has more than
twice processing workload compared to
the previous one this kind of local
variation has direct in fact on the
direct effect on the network congestion
and and it exists even in the
consecutive frames in the video frame in
the video stream so this graph shows the
actual data transaction inside the chip
measured with a rear rear video stream
its average value is 1.8 giga byte
sometimes it rises dramatically over the
network capability bony arse that your
capability is calculated by multiplying
energy frequency bid with data frequency
and bisection bandwidth its value is the
it's valerie's one point 32 gigabyte per
second which covers the average required
bandwidth but cannot deal with these
kinds of network overflow with based on
our pre design analysis the required
bandwidth utilization temporarily exists
exists the network capabilities since
this network overflow incurs network
congestion making real-time operation
impossible so we have to handle this
network overflow to eliminate network
congestion we need to know the reason of
the congestion as an example let's take
a look at this two clusters key point
detection cluster or KDC transfers the
detected key points to the descriptor
generation cluster OTG see in this case
this cluster KDC is task producer and
and the tgc is a task consumer as shown
here in the left figure 3 task producers
created the tasks and the number of
tasks are 21 and 1 and we have four
valid consumers as shown here normally
the task assignments are are are carried
out with greedy task assignments that
the congestions cannot be avoided even
though the
to the mesh network provide multiple
data path so with the greedy task
assignment letter congestion does not
disappear completely since network
congestion depends on the routing path
we propose a joint attack scheduler or
LNS for an intelligent routing to
intelligent routing method in on n NS in
the NS during that talk predicts the
workload of future of future workload
and core mapping scheduler determines
the routing path with with the help of
the workload prediction general donor
network and the neural net will be
explained in the next slide so let's get
back to the previous example to
eliminate network congestion p6 should
be suitable outed 2z nine instead of c6
it has a longer data path but there's no
content there's no network congestion
therefore we need to predict accurately
whether the producers create task and
how many tasks will be created created
to do congestion aware routing and to do
to do the prediction on your network is
is designed it has a 16 dimension input
and 32 hidden those are are in your
network with the with the workload
history input it runs with the time
series learning so it its input is a
workload history and the output is the
next workload of the
producer so here is the recognition
result of your network scheduler so
thanks to the rocket prediction of
neural network core mapping scheduler
can do congestion minimum task
assignment in contrast with first-come
first-serve assignment method when
producer core p.m. and finishes the task
and then send migration recast to n NS
and then it gives the assigned consumers
ID in this case CN and the datas are
transferred to consumers course local
memory the producer and consumer mapping
is carried out during the producers task
processing and we have minimum network
congestion and to hide the timing
overhead old prediction the prediction
The Critic shut up the mapping results
are stored in that in this table and
this slide shows the overall network
regulation regulation flow with
prediction results from the your network
scheduler pre assignment is performed to
minimize network congestion when task
producers finishes their task on
migration request is transferred to the
post scheduler and this is where we
compensate for the error of your network
prediction then we the weird task
assignment wizards are mapped into the
table and task my creation is performed
this slide shows the resort off and NS a
2d mesh and you see with conventional
greedy task assignment cannot avoid
inherent network congestion due to the
limited
number of loading path to satisfy real
time constraint of every frame interval
express the NOC traffic's across all of
the available energy links when pic
bandwidth utilization is in choir is in
touch sorry encountered here the the pic
little congestion czar spread to the
available network links so with the help
of the neural network scheduler 24.4
percent network congestion is reduced
now I will show you the chip
implementation results the proposed chip
is fabricated with 65 nanometer CMOS
technology and it occupies 4 millimeter
by in millimeter size the proposed chip
consumes 381 milliwatts on average and
778 milli watt is a pic power
consumption it's pic performance is one
point 22 tops and one point 55 tops per
watt energy efficiency is achieved for
HD videos clip compared to the previous
vision processors are it adapted tightly
pipelines in clusters and 2d mesh
network on chip topology with
intelligent pendant with regulation with
this schemes the application can be
changed to augment ability which has
more which requires more computation and
power consumption this slide summarizes
the performance achievement of this
processor compared to previous vision
processor 3.5
eight times higher peak performance is
achieved and it obtains the heist energy
efficiency of 1.57 top spot and 12.7
believe what religion per frame also it
is tested it is measured with the HDL
know it is measured with HD video since
the proposed processor is a prototype
implemented under the academic project
65 nanometer process technology has
chosen considering the fab availability
to examine the potential power reduction
with the state-of-the-art fabrication
process we estimate the power reduction
by the process scaling more than half of
powers can be saved with 28 nanometer
fabrication process and it can
contribute to the longer air processing
time I used this for references 22 to
calculate the process scaling and this
slide shows just shows the resolution
scaling my chip is target my chips
target is HD resolution but many of
computer vision algorithms are targeted
for the VGA or QE GA so for the fair
comparison I complete i compute the
resolution scaling so for the visually
dilution it requires 127 milliwatts and
I'll cook it and the chip will occupy 10
point seven millimeter scale for for the
vga processing this slide shows the
organization of K class we have a
touchpad here and and this is a front
camera and it's a micro display with 800
x 600 resolution the main board contains
the proposed air processor and host
processor for the image for the video
input and output and a pga is utilized
to connect this chip and the host
processor its own because the host
processor runs android and this is for
the it works as a mouse i will just
touch input device yes we do ok ok a
hardware specification of k glass and
other commercial reaching the systems
are compared in this page the battery
life is compared when Eric Asian is
fully operated k glass achieves out
about 44 hour with the Arab armm
application while other systems sustain
less than one hour this epson has an
extra battery pack with with the size of
smartphone so we this here is the here's
the algorithm implementation which are
related to immokalee CR
this is the conclusion of my
presentation on hmd system is necessary
software optimization is performed and
the result is 5.3 a 4.5 frames per
second and consumes more more than one
wats and to achieve 30 frames per second
performance and and achieves low-power
operation the hard way optimization is
performed the chip features data and
test clever pair ism and your network
scheduler lastly hmd system is carried
out which can sustain for our AR
operation thank you now do you guys have
any questions so open for pressure at
one time yes question right the chip
that you did that appeared to build on
previous designs is that correct yes
some of components of in my chip are
from from this processor but but there
is a new components for augment ability
such as camel pose estimation processor
I have to design a new processor for for
that kind of new evidence for those new
parts would that be taking previously
existing cores and sampling them in a
new fabric or do you completely you
actually it's changed the basic
instruction sets but it has a different
sim its extension because each algorithm
has different kind of Harrisons so it
has a different difference in the
extension ok thanks I come any other
questions yeah that's thank the speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>