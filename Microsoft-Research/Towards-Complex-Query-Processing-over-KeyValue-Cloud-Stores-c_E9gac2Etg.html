<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards Complex Query Processing over Key-Value Cloud Stores | Coder Coacher - Coaching Coders</title><meta content="Towards Complex Query Processing over Key-Value Cloud Stores - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards Complex Query Processing over Key-Value Cloud Stores</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c_E9gac2Etg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so we are pleased to welcome today Peter
John tefillin from University of Patras
I known Peter for quite some time
whatcha going to say exactly how long
what do when I say no no superiors but
now being out University of Patras for
about 10 years and he was at the
technical universe to creep before that
and before that Simon Fraser right and
before that University one right peter
has worked a lot of things during his
career but mostly I would say it's been
things that involve distributed systems
and data management file storage that
kind of stuff managing data in a
distributed context in various ways now
plus other things so there is going to
talk about how to extract more value out
of key value stores welcome thank you
thank you very much well thank you for
the invitation I'm really happy to be
here and present some of the stuff we've
been working on the last year when I say
we mustn't forget to mention my
colleagues Niko's dermis is a former PhD
student of mine and yannis and George
are basically masters students working
with me at the University of Patras so
the title of the talk is it's a bit iffy
towards complex queries on Q value
stores so what we'll try to do is first
I'm going to present the overall
framework what the philosophies behind
what we're trying to do and what that
saves our approach and then go into a
bit of more length into two particular
types of queries that I think are
interesting and see how we can process
them efficiently over key value stores
on the cloud and that will end the talk
using presenting for five slides
referring to the major conclusions and
the things that are interesting that we
think we've learned from this on top of
another solution that's good for this
particular type of query and it's fast
and so on and so forth
so the general framework is that we're
talking about data management services
from the cloud and their artists is come
in in the form of queries of various
complexities and we typically have this
well-known trade-off between storage
costs and query processing times and the
idea here is that we would be willing to
pay a bit more in terms of storage costs
if that could save us money in the long
run and talking about money we think
that the value to the enterprise will
come from cooley execution so the more
queries are better the more money would
be made and so the idea would be that
either the provider or the client comes
up with smart ways to invest in storage
to build up interesting indices that
will help them expedite the query
processing if you thinks about the
overall driving philosophy of the work
is we want to do some work towards
real-time queries on the cloud the state
of the art falls short of this we've
seen the last four or five years papers
published in the major database
conferences trying to do query such as
joints and other things that are
basically using MapReduce in one way or
the other to accomplish a task a good
way to describe the talk would be no
MapReduce for no SQL query processing
don't quite believe that but towards the
end I will mention what I mean by that
and to what extent I believe that so the
idea is the basic driving philosophies
what I want to do MapReduce want to
built indices so the question is how to
build indices and we want to build the
right indices and the key design one of
the key design decisions for us is to
have simplicity simplicity design of the
of the index implicit in the way that
you process the index to come up with
the answer to the query and of course we
always realized that what we're going to
do is going to be planting into a big
system there is a lot of smart folks out
there that are coming up with valuable
things in terms of infrastructure
or so we can use that we can basically
piggyback on that in order to come up
with a better system and of course if
its scalability always play a big role
and I'll describe I'll talk about that
later in more detail so the first part
is a bit more details about interval
indexing and query so there is a bunch
of different applications out there I
mentioned a few here that basically
refer to temporal queries or interval
queries in general whether it refers to
time or not independently of that so
there are different types of query of
quiz out there we can call I call them
intersection queries such as in a web
archiving system I'm interested to find
out pages that started or finished
within a particular time interval for
some analytics reason or I'm looking for
events that are completely contained the
events panel specific time interval that
this type int value is completely
contained within another time interval
or I'm looking for a security event say
we have a terrorist attack a knock on
wood which spans over five hours and I'm
looking to find as to what interesting
went on in a week before the start
including a week end after the start so
these are what call them containing
queries in general the query types look
like this so we have an interval
beginning at time point so the first
part are the sort of containment quiz to
contain queries it's obvious what it
means these are the contained queries or
this green huh does it so yeah it's all
there the green intervals the
intersection quiz a basic ways that are
crossing the query interval either from
the left or from the right those are the
purple intervals any particular type of
interesting queries called so-called the
stabbing query where you basically have
a single point with the begin and end
time points are the same and you want to
find out this particular time point
which are the intervals that are being
crossed by it so what we will be doing
is we're trying to come up with indices
and query processing algorithm using
these indices to expedite answers to
these queries so we're talking the the
basic infrastructure that we're assuming
our cue
value cloud stores its basis our
reference the basic idea here i'm sure
most of you are familiar with this is we
have an eight space master these are
basically tables or parts of tables if
you wish and all of this our data nodes
which are called region servers in in
HBase lingo and inside one of these
there is a men's store these key value
stores are optimized for high right
through ports all rights go into memory
and eventually they're being sorted into
this particular file formats with
particular indices another dumped into
the Hadoop distributed file system
underneath so this is the general
systems infrastructure what we're going
to be working with so the first question
is okay so we want to support now these
are interval queries Israeli native
support provided by HBase to do this so
here i have an example i will be using a
running example through this which is a
web archiving scenario this is most of
our data comes this is where most of our
data comes from and this is actually
what's paying for this for this research
because it's a big european project in
the terms of about four million euros
over three years it's basically for
partners throughout europe and buttress
is in charge of basically coming up with
the indexing part and the query
processing part on these indices for
that so here I'm having a region server
data node storing different regions or
tables there okie could be the URL of
the site or whatever there is a whole
bunch of other things that we don't care
about for the purpose of this talk and
we're also having a beginning and an end
period this could be whatever this could
be representing different crawling times
what this thing was alive every time you
get a new CRO of the same page basically
new timestamp starts and when the same
page is called again it's changed and
that's when the interval ends and so on
and so forth but in general we have a
beginning a net time point associated
with every with every row key and the
road key here is a URL okay and
similarly here so if I have a query that
comes into the system that basically
says give me all the interesting things
that happen in this time period then if
you look into the times you'll see that
it hits on both of this vision server
and what can add space do with this it
can basically you can run a filter you
can do a get operation specifying a
filter and basically the filter is a
predicate that basically says when you
grab a roll look at the beginning and
time intervals to see if they're
interesting with respect to this query
interval okay pretty simple stuff
nothing great the the bad thing here is
that obviously this is inefficient and
costly why because it grabs every single
row it applies the predicate if it's
okay it keeps it otherwise it not it
goes on and on for all the region
servers in parallel but still it has to
touch all the rows and especially for
queries for low selectivity selectivity
is an issue here I'll touch on that
later and we have quite a few of those
lost selectivity queries so the question
is can we expedite this okay so there's
a couple of ideas here all right let's
play the first idea is what we call a
time point and end points index so
here's my Road data again I have a rocky
which could be the URL or whatever it's
how is it store tonight space and I have
a beginning and an end point for it's
one of those different Road keys okay it
could be other data here we don't care
about it that's why I'm not showing it
so what we're doing basically is we're
coming up with an additional table if
you wish your family so we grab every
roll this is done using MapReduce
processes in parallel and what you're
doing is for every different endpoints
that we see we create a new role and the
key here is this particular value right
so there is a run on my table with value
one does one does 2010 whatever okay and
also in this other column family that
I've created here I'm putting in the the
corresponding endpoint okay similarly
I'll grab this the endpoint that would
create a new role for this and if the
other column family output the left and
point of the interval okay pretty simple
stuff so i will keep grabbing all the
rows creating all these new rows and
this would basically serve as my index
this segment of the table is the index
for
answering interval queries on my raw
data set so let me give you an example
of this so this is my index that I've
built over the previous example and
suppose I have an interval query that
comes in that basically says I'm
interested in this time interval tell me
what's interesting in there okay so
there's a couple of things going on here
underneath first thing is by design
hbase can do really quickly scans scans
of rows so by placing everything by row
ID here I could really identify segments
of interest really quickly and get all
the roads really quickly riding on what
eight space provides me already okay so
if i look at the at the end points of
this interval in the query then this
basically gives me a segment of my
endpoints index and the idea here is or
looking at the results for example i see
there's different things going on here
for example i see that is this item B
but both starts and finishes with in
this segment of the index there is this
item a that's basically it's a cross in
query a crossing interval from the left
because it started out over over there
and finish this in here the results of
this thing's see that started before and
it finishes you putting this query
interval and there's a couple of things
that start with in this particular
segment and finish the outside of it
okay so the result set that I get from
looking in here is a b c and f I don't
get D and B is also part of the result
in the sense that if you're interesting
the containment queries okay so in order
to get D as well I would have to look
either onto this side or on to this side
the idea would be by scanning my index
from the beginning of time till the end
of the segment specifying the query I
would be able to get all the interesting
things to happen in it or similarly by
scanning my index from the beginning of
the of the interval of the query
interval to the end of my index I will
also be able to get everything and I
would rely on the fast scan operator of
eight space or any key value store to
provide me with the answers note that
for a particular type of query like
Crossing queries from left and right or
the queries are completely intervals are
completely contrary contained in the
query interval then this thing is really
fast typically such it gives me a small
index of a small section of my endpoints
index and I can get all the relevant
data pretty fast if I have to do a stop
in query then I would probably have to
scan huge portions of this and this is
going to be huge okay so that's the take
home message from here so we'll come up
as a second help for this we've come we
looked into the literature as to what
our data structures and there is a lot
of data structures here indexing methods
to be to be using for for interval
queries so we selected one that's one of
the prototypical segments for doing this
and it's called the segment III and what
we've done is we've provided the key
value coding a key value representation
of the segment tree on the table and
we're using MapReduce faces which are
actually optimized in ways that i hope i
have time to point out in order to
create based on the road data table
that's given to us stored some ordinate
space Randy's MapReduce processes and
come up with the key value
representation of the segment three
which we call this Mrs T thing so in
terms of computing the elementary
intervals which are needed in order to
process the segment three queries it's
pretty much oh it's not it's not
particular difficult so what we do is we
splitting the space and we're giving
these data items to 22 different mappers
in this example these matters are
basically producing all the different
endpoints in the in the beginning end
times and some something is going on
here so everything is sorted and in the
end what comes out is what we're
starting to hbase is once we store it
all the end points these are basically
the elementary intervals and based on
that we have a second or by the way here
we'll also have realized that we have
all the data that we need to build the
endpoints index so we build the endpoint
index as well in terms of writing it as
a separate table in eight space so now
that we have the elementary intervals
it's we feed them into a second phase in
a map
this process is in the different mappers
and it's mapper basically will create
its own binary tree on top of the of the
basic atomic order order elementary
intervals again this is this is not
particular difficult and then we join
all these different separate sub trees
being produced under one heading under
the one common route to give you an idea
of what it looks like this is what the
their modesty index looks like innate
space so let's go through it so what
we've done is we have this coating of
the tree in a table and we basically
faithfully follow the standard
algorithms for using a segment three in
order to answer it's about queries so
suppose we have a query that gives us
this a stabbing queen of this point we
know what the root of the tree is and
the road is here on this table this
table basically stores arrow key and the
rock is what is the time point with
which every particular node in the tree
is identified so when you build a
segment three every node in the tree is
identified with specific time point for
the route this time point is basically
the median of all the end points if if
they're sorted okay for every subtree
the root of the subtree is the the
median of all the end points covered by
that sub tree that's a basic idea and
also for every node in the tree we have
every node in the tree is associated
with as with an interval and the
interval is basically the union of the
intervals associated with their children
with a notes children and this is
differentiable defined and so we have
these two pieces of items there okie is
this unique time point and then would
basically have other information every
node in my segment three can have a list
of different intervals that are placed
on that note so we have that and once we
have pointers to the right and left
child of every node okay note here in
terms of traditional index pointers this
point is adjust keys different keys into
my cable okay so once this query comes
in I'm comparing this date here with
this date it falls to the right of it
first of all
we'll grab whatever is in that note
that's a unique characteristic of
segment trees that to answer stopping
query what I was basically do is follow
a route from the root of the tree to a
particular leaf and all the know and all
the segments that has restored of these
nodes that will collect it they're all
part the grantee to be part of the
result okay so i will compare this date
here with this date and I will decide
whether to go left or right this date
here falls to the right of this so we'll
go to the right side first I will grab
the items stored there and I will keep
it from my result list then I will grab
the right child and then basically sends
me over to this role and I will do
exactly the same thing compare this with
that and this falls to the left child
there is nothing to grab there's nothing
stored there so then i would go to this
and I would keep continuing this it's
the same process so this actually has
this note is this particular interval
stored there so I will keep the interval
and then I will go to the right side and
that basically refers to a leaf node
that it's an elementary interval and
there is also something stored there so
we'll grab there and this is my answer
sir for a video yes yes because inside
these are different endpoints right
there's only one end
so and this actually I need that because
to define the message space keys all
right so this is basically what's going
on here is specified traverse the three
chapter every node in the tree is there
is a row on my table okay and the cool
thing here is how many traverses I'm
going to do logorrhea swing to the with
respect to the number of things that I
put there okay so that gives me AB down
I predict a predictable bounders to my
latency okay so interval coil is pretty
much go the same I'm showing this
example to illustrate something which is
another characteristic of segments trees
which is now with the bad side of
segments trees so again i'm having an
interval here i'm going to my route and
I'm not comparing this interval and I
see that all of this falls to the right
if I remember correctly of the time
point associated with with the root so
once I grab be for the result that will
go to the right child then going there I
will continue there is nothing there to
grab so I will basically see that this
interval now spans both the left and the
right children so I have to descend both
down on both of them okay so then I grab
those and you can basically the process
follows if the keep track so I don't
miss anything so I collect all the data
the point here is that if I have
interval queries of slightly further big
length depending on how my sigma 3 is
built I may have they sent to many nodes
in the tree and that will help me right
every descent that I do if it sounded
visit is a get in 8 space that cost ok
so for stopping queries it's great
because I do I don't know 20 30 gets and
I'm done and I grabbed everything all
right but when I have to grab a whole
portion of millions of nodes doing
millions of gates for a particular query
this vegetable kill me we will see that
in the performance results so now i have
both indices okay which one is better
and it turns out this was actually
didn't you just turned out it was my
design after spending some time thinking
about these data structures is that we
can get both of the best of both indices
what we're after here is to realize
which created type is good for what
square f
its index is good for what query type
and where the query comes in senator
query route the cooler to the particular
index and if the query is too demanding
is complex and once both containing and
contain tagore and crossing like we said
then decide which index to use for which
part of that okay so this is what's
being played out here okay so basically
for the interval query 8b I do a quick
scan on a be typically this would be
small a small how to identify small
section of by endpoints index and but as
we pointed out in the example earlier
i'm still missing something so to get
what i'm missing is a basically i'm
doing a stop in query on an Mrs T about
doing the stabbing queen either I would
get the missing parts okay I'm going to
get the missing parts i would get some
overlap but i can filter those out every
time I visit a note okay so if I am
again I am I to visualize all these
intervals then the purple intervals are
the ones that are getting from an API
and the green intervals are the ones
that I'm getting from them Mrs T okay so
this is the idea so the next logical
question is what covers now with up with
update and this is a big problem right
so what we're trying to do here is
basically piggyback onto the basic
electrical key value stores i'll give
you write throughput okay based
basically in or in all of the work and
the work done i'll talk about later i'm
mixing a value and then i'm having
things added to it so adding things to
it to associate with a particular value
is just adding another column for this
okay i have right through before that
that's given to me okay i don't have to
worry about disc notes being filled out
or blocks or whatever again we'll get
back to that so what we're proposing to
handle the updates we're proposing to
have this updates index the updates
index it's functionally the same as an
endpoint index in other words it's a
table that i can scan in HBase okay the
key difference is this thing is much
smaller okay so everything whenever
something comes in let me have to
install this so this is my regular
endpoints index and this is my updates
index that I have so when I have to
insert a new
record with rocky a and intervals
between 9 and 19 I will go to nine
another something here and we go to 19
and nuts and not something they're right
this is what the endpoints it was
working and I will also do the same on
the updates index basically the updating
this is supposed to be something very
small that's going to tell me I don't
want to be building the the tree the
tree building is a cost here costly
process and segments trees most of this
interval structures are static so I have
to rebuild them again from scratch so
what I'm doing is I'm dumping all the
updates in this small index here and
what my query comes I will write in
parallel so I get the old stuff plus the
new stuff that's been added but doing
quick scans on this small index here so
that's the basic strategy so if I'm to
delete something I want to delete record
with key X so I'll go and delete it from
that point index and I would basically
add tombstone records for that so when I
scan my parallel in panel in my updates
index and I see a tombstone record and I
go to the tree and it gives me interval
x I will use a tombstone record basic to
filter that out so that's basic idea so
in terms of stopping query what's going
on how do we possess a star bigquery now
or an interval query now that we have
this updates index we run the query on
the varsity why because it's very fast
i'm going to logarithmic number of gets
and at the same time scanning the UI
from the beginning of the of the UI
index until a the query time point this
is small the scan is fast so i'm not
going to pay a lot so ended up looking
to add things that have missed from the
from the tree or things that have to be
removed from the tree and if i have an
interval query then I can run my query
on the tree or I can run it on on API or
both and again in parallel I go and grab
what's new in the updates index recall
the update index is small so it doesn't
hurt the four months that we've tested
that i'll show you some results so in
terms of some experiments we have a
close of these domains of the not
particularly big so we use the three
five and nine node class dressed in a c2
with Porter Oliver
or Oliver code there and in terms of the
album's we implement it is the analyst
for building and accessing the indices
for preprocessing the support from the
native support for eight space provides
and we also implemented hive running on
HDFS we also did it on ornate space but
that was way too slow even slower than
one hive and here are some results I
mentioned that what we're building we
have this map will do spaces when we're
building these structures and we have a
simple version and an optimized version
if we see just the unoptimized versions
the takeaway message here is that we
have we see scalable performance as we
increase the number of nodes going from
two to four to eight data nodes we see
things scaling nicely and we also see
big improvements when we go to the
optimized instead of the to the
unoptimized version so these are fairly
big improvements and again we see a nice
scaling with when we throw more money
into the problem by bigger clusters in
terms of stubbing queries this is
basically what's going on this is the
database filter this is what we get with
hive hive basically runs huge MapReduce
jobs behind it again touching in
parallel though every role in the system
here's what we get from api only if we
execute the query on epi only here's
what we get if we execute the query on
on the segment three and of course if we
use both we're not going to get any
improvement for this ok so again we see
that for stopping queries we get a
factor of two or three better
performance from from running it on the
tree here running one we called
intersection queries or what we did is
we looked into the data set and we
randomly picked a hundred quiz I think
with we picked a hundred random one week
intervals and because the data set is
very dense this basically is a lot of
intervals in the particular data set so
immediately you see what's going on to
the tree right this is the the effect i
mentioned earlier right going down the
three you have to visit basically a big
portion of the dose and have to grab
huge numbers of of it of us give us from
late it's one of them again api stays
consistently
certain bound and if we do both that is
I only do on the segment three I don't
do the whole thing but only to the
stopping query and I do the remaining
when API things are again improved
considerably be yeah we're talking about
a few million from from two and a half
six million intervals I don't remember
to be honest the question it because we
have different versions of this but I
think what we did was we just put onto
the tables innate space only the
attributes I would care about so we
don't just have all the records there
from the original data set if you are
too if you want to put that and because
they've also have the text associated
with this and so it would be huge so
again pretty much so this is the one we
called for the demos data set again
we'll see the same scenario the tree
really is good performance for stabbing
if you go to so here what we did for the
demos I forgot to mention is that we
created synthetic queries to be able to
be able to play with us with a
sensitivity to the state activity of the
query so we design queries that we knew
manually we're going to give us
twenty-five percent of the whole data
set to see what's going on and we see
again the trends that we have seen
before both is a clear winner if we run
the query the the big interval query on
both indices and get the best of both
and even when running to seventy-five
percent selectivity and this particular
data set again was it pretty much the
same same behavior okay so last set of
experiments is when I'm basically
running in the face queries in the face
of updates so now the new player in the
scene is the so-called index the updates
index so what we've done is we assumed
that with the queries when the query
start playing a certain percentage of
what was originally in my indices are
now being stored in the updates index
and this percentage was built from five
to thirty percent so this ninety-five
percent here basically means that
ninety-five percent of my of my data is
in the three and five percent of the day
lies new data that's not in the three
items in the
dates index so when a pretty comes in
remember it goes on the pedal it goes on
the three and it goes on to the API or
it goes it can go it has to go in
parallel to the updates index to get the
new stuff ok so the update the index was
was varied up to thirty percent of the
of the original size of the tree of the
original number of of intervals and
again the thing to notice here is that
it's pretty stable so we don't get big
any deterioration going on in terms of
time because of accessing in parallel
the updates index so this pretty
standard we see the same behavior
everywhere this looks like differences
but they're really within the
statistical range so it's not really big
differences if you look at the into the
y-axis ok so in terms of the conclusions
for this part this is the first crack on
doing interval creation key value stores
these interval queries are becoming more
and more popular basically driven from
the need to do temporal analytics and
all kinds of things like this so we came
up with a few indices that can help us
solve the problem we build them with
MapReduce jobs maybe index processing
query processing out of utilizing the
same disease which ever needed
maintenance scheme that does not look
out queries and queries see the new
things that are being that have been put
there and we've seen big performance in
terms of the native support or in terms
of running MapReduce jobs you're not
reporting cost
right sorry about that ok so these is
finally father found a fireball well
you're right for example in the other in
the other part of the work I was we
explicitly also reporting bandwidth
which can be an indication to the
throughput that can be achieved here of
course nothing else was running into our
clusters right so the thing that was
running little cluster which was
basically our hundred queries so I I
have a good feeling that if I were to
report through put up with basically big
getting the same behavior because of
this so even if one more if it is partly
particular index or this particular
strategy is very resource family it will
not be showing here because there's
nothing lot of things going on
concurrently but you're right if I were
to set it up in a real class there was a
lot of things going on something that's
basically waste bandwidth around would
basically might kill my throughput so
but they will not be there is no
serializing points like after the
performance in other questions yes let's
see
civ carving a niche hurry like and
inadmissible to action context right
here do you provide any reduction in
standard interaction comments ok so this
is a big a big discussion so the
question is what kind of consistency yes
the Bing zone here in terms of the
regular queries is whatever the cloud
gives me basically what HBase gives you
is like read committed consistency so in
terms of our updates it's also easy to
solve that what you saw read committed
consistency ok so that's about it now
there is a lot of work going on that
basically trying to improvise the snaps
of escalation consistent I consistency
semantics or even Purcell's eligibility
semantics into with the night space and
that would be easy this is part of why
we did this right so if there's anybody
underneath us in the infrastructure give
us another way to define transactions
and help for example serialize our index
updates with a raw data fine for us we
just cook that because everything is
implemented in aids based lingo with
just using the api's we're doing
everything ok so the part 2 in about 25
minutes support to refers to run joint
queries this crowd is probably knows all
about it so this is a typical SQL
template for this typically we're
talking about an anyway join their
different models most models assume that
one of the attributes in your table is
being used to define the score at it for
a particular record ok and the key point
is that when you're joining tuples you
have to compute like an aggregated score
that comes from the two relations that
are being joined and this is typically a
monotonic application function like
summation or something like that so
would be using summation without loss of
generality here so the motivations here
are basically two different again
techniques to do this the first thing
we're going to do the respect some work
here on decentralized this is an aside
let's be some work on decentralized
run join algorithms the most frustrating
thing when i was reading those works was
the fact that they don't do the simplest
way of doing this in a decentralized
environment there is a very straight all
most straightforward way to be able to
do this in a distributed environment and
all of them do very complicated sampling
approaches with cute mathematics too so
that the sample is correct with these
types of guarantees but I bet good money
that they would lose against the simple
approach so let me tell you what the
simple approach is and what is more
sophisticated bloom filters histograms
which is a novel status constructs for
on Jones I would coming up with but
those are all about okay what is the
basic idea the basic idea is I have my
raw data okay some records some Road
keys here's my joint attribute value and
here's my score plus other things that
are in character ok so the base key here
is the record key whatever that was
designed to be so building and avert the
score list so this basically this thing
but build in terms of score so the wrong
key here is a score ok and so this is a
rehash of this delivered scoreless so
the basic idea is we're building this
week hustlas inverted Silverton index
right with based on score sort the score
inverted index so we built this using a
MapReduce and we start fitting but is of
rows from this inverted index list so i
have two relations that i want to join
and compute the top-k joint i'm going to
go to every is l of this relation and i
start bringing in batches of rows and
when I bring these buds of Rose I can
perform your favorite algorithm for
centrally producing a top K result at ok
John result in this case the alias at
all vldb or three algorithm which is
pretty much a standard in the area so
when I bring a new batch of rose I will
check every row there I guess all the
roads and I brought previously to see if
there is a joint and if there is a joint
I will compute the aggregated score for
the joint and then I will check to see
if there is a chance
that any other records that i have not
bought yet can make it into the top k
result okay and if so I will continue to
facing batches if not I will stop all
right this is they basically this the
Allah Fagin threshold algorithm applied
there for for the traditional top kek
query if you submit monotonic right we
give right I wrote that but I did not
mention that right so here is a typical
example of how God this works I'm
assuming here that the batches are just
one bringing a row at a time so I'm
going up a big the first Rose I'm
looking into the joint attribute values
there is no match there so I'm going to
break the second row and I'm going to
try to match this without both of these
rows there I'm seeing that it's out of
value 17 there is a joint so i compute
the score by just adding the different
scores there so that that's one result
then I'm also going to join this one
with you with the first two from the
other relation and I'm seeing a joint
attribute value 12 and I'm also
aggregating the scores there so this one
is obviously the the highest one so this
is my top K my current top one joined
result and if we look closely you will
see that nobody else that I can bring
from from this row and down can have a
score the higher than 183 that's why I
stopped this is the threshold criteria
okay so this is pretty simple stuff and
we'll see that it's working very nicely
and you can do this in any district the
system even know DHD that I've seen
peer-to-peer solutions for this it's
really easy to do like is it use what
other people have done for the
centralized processing you're bringing
in with batches and it works so the
battery thing of the previous algorithm
is that you bring in tuples and you
don't even know if they're going to join
right so depending on the distributions
of the scores and the joint attribute
values this may really kill you okay so
the goal is try to bring in mont on only
those topless for which you have a
pretty good guarantee that they're going
to be in the John result and here's
where our statistical structure is
coming to play we're using histograms
where
the package of the histograms reflects
color ranges okay and what we're going
to be putting into those histograms are
joined at a bit values all of that will
become more clear in a minute and I
don't want to just keep the frequency of
the histogram because then I have to
make assumptions about the distribution
of the joint attribute values within the
score range and I don't want to do that
because in practice that those do not
work very nicely so what I'm going to do
is I want to keep every single attribute
value that one into a particular bloom
filter sorry that we need to particular
bucket okay but because this can be huge
I'm going to use a bloom filter to
summarize this data so here's what the
structure looks like i'm having two
relations here i'm showing only the
joint attribute value and the end the
score value so if i were to build a
bloom filter histogram matrix as we call
it for our one what are we doing we're
processing every row at a time so we see
that the joint value is a degenerate
values a and the score is one so I'm
having a bucket for every score rains
every tenth of the score say its course
are normalized this case okay so this
here is a bloom filter representing the
contents of the bucket so I'm going to
ask a it's going to give me to this
position at the bit in the bloom filter
and set that did this case in this
particular example I'm using counting
bloom filters to simplify it so again
here i'm going to have see and be seen
from the score that it refers to the
first pockets i'm going to go to this
bloom filter and set the bit that
corresponds to the house of see okay so
i keep doing that for all of these here
point 82 force to the second hands
pocket so if I husby I go here and i
said this bit into the second prom
filter so this is the basic idea so I
keep doing this I thought I hit skip
that animation so please bear with me
for a minute
okay so and I've also built similarly
similarly the equivalent buckets using
bloom filters for the other relation now
what's going to be going on during a
query processing is up going to be
fitting the bucket at a time starting
from the high-end buckets so first I'm
point of it I'm from benefits the bucket
that refers to the everything that has a
score between point nine and one from
this relation and from this relation now
look into those bloom filters if I just
do a bitwise and I know this is a joint
here there is no joint I have something
that classes here something has is here
some circus is here no so I don't have
to build anything and I have to bring
anything now we're not go and build this
bucket here I will compare it with the
content of this and do a bitwise and
again and I will see that here in this
position I have two tuples of of the
second relations that get a value that
cast into this position for the joint
attribute and one couple from the first
relationship again a double with joint
attribute for custom did here so I have
a joint is out there actually I have to
join reasons because i have two tops
from this one with with grass while
you've been in this work in this case
and one topper from that one but has
value be okay so i have a john result
because b is the joint attribute value
right and i had a top of from here with
a joint attribute value B and the top is
from here from the joint attribute value
B
right I have I have accounting bloom
filter okay but you're right there are
four positives so it could be a bit more
so it's 2.02 or something but I'm always
on the safe side I may be big something
more but I'm not gonna miss anything
because of the false positives but let's
forget the post post because it actually
is a big discussion here so here I have
a structure that can tell me what i have
a john is out what I need now is I can
also associate with these two join
results a high score and a low score
using basically the score range for from
the bucket and I use those for my
threshold criteria to decide if i need
to bring more buckets okay so this how
it works and so the idea is we create a
bloom filter for each one of the of the
score ranges that we care about and we
store it as one column in a row it's a
blob of bits actually it's a big
discussion here if you're going to have
counting bloom filters you're going to
have playing boom filters how many has
functions are going to use and what kind
of compression out what you use here so
we've done believe me a lot of work here
and we are still going with plain bloom
filters with gollum compressed which are
and we use a golem capacity presentation
of them I don't know many details it's
my colleague Niko's I'll actually found
all of this I designed the structure
since the algorithm so the other idea
then is we need a diverse mapping let me
just explain what that is so here i have
my relation some housing these values
and i'm putting them into my bloom
filter and here are different positions
so into position seven I had dejes into
position cyber for example right and I'm
keeping a track that I have to fix the
casting to hear but I also maintain what
we call the reverse mapping in other
words when the person who the note the
Quirinal that collects this bloom
filters says that at position k i have a
joint result there has to be a mechanism
that you can go back and say give me
whatever has been to position k so here
basically i have a table where the keys
are the
position indices the nonzero positioning
this is on my bloom filter so if
somebody comes to this guy and says give
me whatever has to push it in 100 did
you bloom filter because I have a match
he can go here and grab all the relevant
topple information such as the topple ID
the joint attribute value in the exact
score okay again this is a nice table
ornate space I can do quick gets multi
gets and all of that so the algorithm
works like this we fetch a bucket from
each of the tables starting from the
high end of the range we compute
basically an aunt of all the different
bloom filters and we'll use the scores
to figure out whether we need applying
the first coat like want to keep
bringing more buckets in if so we repeat
the process else we stop so the joint
here that is no joined yet which
basically do a bit wiser on and a bit
west end and we see whether or not we
have enough results for the joint and
where you have enough results for the
joint we'll go back to the table and say
give me all the tuples light up like
this that has into position 100 like I
was saying earlier ok so again for the
data for the for the performance we took
from the DPC aids two relations the line
item in the port elation and was to
create a synthetic one for reasons that
I will explain shortly we have pretty
much the same setup and we're doing top
gate joints were case then after 10,000
we also have high volume up reduce and
implemented too slow I'm not even gonna
bother so it and the simple idea with
inverted scored list and unjoin
algorithm use but is referring to one
percent five percent ten percent of the
complete score mass so here what the
results look like the green bars refer
to the bloom filter histograms approach
and here we see that the simple idea the
ISL idea works nicely in the cells that
at least there is always one
configuration in this case the one
percent score mass configuration that
always beats in terms of query times the
bloom filter histogram
in terms of bandwidth oh and here's back
to to the point that you made earlier in
terms of bandwidth this is not the case
the compression because boom fittest of
Somerset's anyway we see that from 10 to
30 times better improvement and so this
would tell me something about throughput
in it would definitely tell me something
about costs at she also brought up David
because most of the charts models now
charge you for whatever whatever
operates are you doing so if if i'm
doing like for example the new DynamoDB
charts model if i'm doing an operation
and bind base I'm provisioning for this
message Pacific read capacity read
throughput capacity as they call it so
for every read of one kilobyte that I do
I pay ok so the more kilobytes that you
have to read it or transfer the query
this translates into bugs ok so both in
this in this it's not exactly what you
talked about in terms of flu put an
exact dollar values but it gives some
indication towards that why didn't this
current novel works if we
I think that it would I think the this
particular significance to action would
work yes it just happened that the
environment that we looked at was cloud
values Q value stores but I think they
would look at regular a sequel databases
so now an interesting thing is looking
into score distributions is actually the
different scores and how many tablets
from the original relations felder so
the reason why the simple idea beat the
bloom filter histograms was that because
it could stop somewhere around there and
it did not bring a lot of stuff before
it stopped so that's why it stopped so
what we did is basically we reversed we
flipped this around this is the
synthetic part of the relation and
indeed we see that this guy is always
better even in time and in terms of
bandwidth with your even Big Apple up to
50 times savings so conclusions the
first crack on run joints or key value
stores we think as David pointed out
that the statistical structures can be
used in other in other environments as
well the reason why we particularly like
this is the ISL is a simple idea
everybody is comfortable in CS with
building reddit indices and it seems to
be doing a good job there are some
configuration issues it says how much to
bring and every was every bad but again
this is there is no system that doesn't
have this tuning parameter problem with
respect to the bloom filter histogram
matrix we have great bandwidth savings
which translate them to cost and the
query times vary depending or or or
whether bringing in the huge filters
will actually pay off so if the
distributions are charged that you
really have to go deep you may have to
bring the whole filter and that may
actually not pay off so actually with
you we're looking into find really bad
distributions and negative correlations
between the joint attribute value on the
scores in order to see when this will
not be better compared to the to the
baseline approach
so the third part of the talk for about
5-6 minutes but i still have I want to
basically go through a number of things
that are beyond the traditional
conclusions that one season in this type
of research things that we've learned
working with key value stores and trying
to build indices for them so what we're
doing is we're having a key value
representation of indices basically that
means we whatever it is we put a little
tables everything for us every index is
an H base table and we use the API
provided by eight space or whatever
whatever key value store to get up so
what does an index need especially for a
for a key value environment for this
types of application you need fast
comedies in the lists you have a value
and then you have something else up
kisses value and something else that
gets is fine and something else cases
value this is easily done by HP right
almost key value stores you can easily
add columns a net keyed accesses either
for exact mats simple get operations or
multi getting operations or for scans
and again all of these actually
optimized key value stores were or exist
for these types of workloads so they're
great for building your indices and this
is actually a big departure from related
work most of the related worked on
bodybuilding indices as I said because
what they're doing is the d-line
MapReduce job to do joints with
developed amis ations or won't they try
to build some index that is that there's
been a couple of works they basically
are too low into the block level of the
desk okay and then you have problems
with updates or complete indices
disappearing but if you want to delete
an index are you the little index okay
you're left with big holes in your and
your actual physical stores of the data
well in the building lyrics
so costly build the indexes at the start
that you really have a hard time
advertising that caused us to the ring
you're right this is a classical problem
I mean should I build an index for this
or should I not build analytics for this
one of the things that we're looking at
is that for some like if you are to run
a MapReduce job it's so long that will
actually be faster to building it
expressed it just use the flat query but
I do not have any hard numbers on this
but it would definitely be the case I
mean you can easily imagine this huge
longer rounding them up with those jobs
right so built the index for 20 minutes
or 30 minutes or an hour and then run a
new query which will take a few seconds
it would still be better than 15 hours
of the running what's a map with this
job so that's one possible answers but
hey I did what we really need here now
and I'm trying to get that later it's
some kind of an optimizer right should I
build the index and when I if I build it
so i use it or not and i'll get to that
in a in a later point so this other
thing that it's actually closer to my
heart now is the focus is all divided
between main memory and disk indices so
there is a lot of smart people are
working computational geometry problems
for example x CS theories and they've
come up with this really nice cute data
structures the database community never
looked at those because they die there
are four main memory applications some
of the main one the main memory database
is back in the 90s it get a big lift
some people started looking into them
but there was this all divided okay so
my idea is if you're using key value
representations of this most of these
problems go away there is no divider
anymore why because my index pointer
just points to a row and this row can
have as many columns as you want
representing the values that fit this
particular row so i don't have to worry
about don't splits and and emerging and
and film factors for my for my block on
disk any of that age basics care of that
and it does a good job in taking care of
that okay so
wouldn't that have some the details have
some impact on your potential
performance where are you arguing
everything is washed out given the cloud
of agencies
partly but ok so the potential
performance is what the participate
Foreman says when I add stuff I add
columns this is really done fast because
key value stores do this I 183 stuff I
just to get on columns which again there
are physical entities there that can do
this really fast or do scans so I'm not
really paying a lot so the whole point
of building the city's do surgically go
and act specific rows or do segments of
rows together using scans and the key
value stores are fast for these things
fully buying what you're saying because
the fabric of that word food and that
never seemed to be to apply to normal
daily basis no because there you see if
I have a table I cannot just add columns
in the table it's a different data model
see here I can just add columns so if
I'm bit inverted legs and okay so I want
to find out which items the the
attribute value a is associated with
which items as new items come in adjust
our new columns for that in my key value
store this is really fast just the main
memory access and when i get i just go
and get this value and i will get all of
those rolls or all of these new columns
that have been added I cannot do the
same with with sequel tables so this is
the new and this way i think is an
interesting something interesting back
up that comes out of this and i can make
the device like that go away
and the point is that there is a lot of
smart people are there like I said that
have really come up with this but
they're dismissed by the database
communities being old school or only
main memory so we can use these
structures and actually would in other
words that we're doing we are using some
of these structures for massive data
sets as well nothing the normal database
sense but at least on tables and key
value stores now on Big Data indices and
MapReduce now MapReduce will trump your
index if you have high queer
selectivities in other words if the
result is a big portion of your data
anyways okay and they go do it
perilously and if you're careful about
how you do it so there is not of copying
and reading back between the mappers and
reducers and all of that map readers
will be a winner there so the speak is
remember the cute I know in the
beginning no monthly dues for Noelle
queries well it's actually we're talking
mostly about complementing MapReduce and
in fact our goal is and we will welcome
to type of this problem together with
any of you that are interested to work
towards an optimizer so I have a new
query that comes in what kind of
statistics do I need and how do I decide
whether I should run democritus job or
just use one of the synthesis or because
it's just building the experts as we
were saying earlier and run the query on
that and when it cost models flood which
is not an easy thing to do to the extent
that these things changed a lot okay but
there's also throughput costs because
MapReduce may win with respect to
response time because you have this
massive it is embarrassing
parallelizable mappers and reducers
going on a bug across thousands of
machines so they might actually do a
very good job in a response time but it
might hurt your pocket again if the
charts model charges for whatever you
read and if you have to touch on every
single record in the data store you can
have to pay for it so your win here
might actually hurt your pocket okay so
another thing is that key value stores
are read silenced and that really means
two things one thing is that they're not
read optimized they would basically
right optimize they were designed for
right intensive applications for
extremely high right through port
this means i said two things the first
thing is we need the indices to do that
if we're going to if we talk about
queries okay i'm going to ask something
that's really silenced as i call it then
you better have indices to go and get it
fast the other thing is that we're
putting with works like this the key
value stores were stressing them to the
indexing task in the sense that we do a
lot of gets and gates are not the reeds
right this how you read from a key value
store and those are not optimized so
using trees for example with logarithmic
depths might help you specifically
subjected to the predictability I will
do 30 gets to go down from the root to
the leaf for the billion nodes for a
billion size data set but still if we
having talked about interval queries
like we saw in the segment 3
representation in the key value store we
still get to visit a big number of the
nodes didn't get for each one of them
and that would still hurt performance so
I think there is money to be made this
is wise looking at alternative kinky
value representations or you basically
store your trees or whatever in a way
that you can get at them quickly with
scan operations so how you play around
how you define your key so that they
will be stored physically together and
quick scans over then we'll give you the
results without having to do explicit
gets sequentially okay so I've gotta
stall thank you for for inviting me and
I'll be glad to answer any more
questions in here questions
no more all right I just had one with
the segment index something so I send
you one one one advantage of the segment
indexes that you can map it the ordinary
indexes that you see inside key value
stores
yes I could do the same with the
interval three but with Interpol trees
those are similarly because they still
have the same atomic elementary and
elementary intervals we just chose
segments trees for reasons that have to
do with the efficiency even though there
are more storage hungry and they run
into apple trees and there's other
things right here i'm talking about
what's being indexed here in that work
is a one-dimensional object an interval
so if I like you've done work for
example where you you know you build we
have the interval and something else so
you combine like a b3 on the kids or
some matters or some attribute and you
want to have an interval associated with
that as well so it would mean to seem to
see how that would map we can't give
into a key value of the presentation as
well indexing both start and end points
and doing intersections in the bottom
results
well this is the end points in this is
close to that so for every interval
basically I'm having a role that's a
start another row for the end point so
it's very close to that and this is a
similar like back in the temporal
database of whether they were in their
height they were like nauseous like the
time index I think one of them was cold
so basically the idea was the same it
created the different endpoints but then
then they would build something like a B
plus 3 over that here I don't have to do
that because in essence if I have this
endpoints index hbase does binary search
on there on this so it's like I have a
binary tree over my head points so I'm
getting up for free so yeah there are
very similar ideas that have been played
around for some time and this is a very
deep field because just going into the
historical database of the temporal data
bases with both Felicity time in
transaction times humongous literature
there but all of them come down to the
same thing to similar things rather
familiar with the actual implementation
of each place so i was wondering why did
you claim that they have beat challenge
was chilly but all of them or all of
these key value stores they're optimized
for rights what does that mean that
means that when a ride comes in it's
just within mm cast somewhere
periodically one of these things filled
out what's happening is it sorted out
and it's put into something to building
into a block ready to build into disk
with an additional index by row here so
whereas you giving updates coming in all
the time you having different of these
the so called aids files and this can be
sent to disk everywhere so when I get
comes in you may just have to just get
all of this age files to figure out
which is the recent get that is invited
to get using in essence the right the
Knox tracks would file system for
example is is something that actually
permeates all of this design philosophy
here it ate me H basis HBase is based on
BigTable which is as long structured
merge trees so those those partition
files should be merged back together
eventually eventually it was only if
you're reading stuff that's been
recently written that you have this
fragmentation problem pull stuff from
the recent blog as well as from the
older you don't always get the problem
okay so the idea is the actual
implementation is the hip they use bloom
filters so you're asking for a
particular Road key so they use bloom
filters to decide which of the different
age files that have created which is our
the blobs of data that's written to disk
actually have this role in them okay and
if they have not been compacted to say
it's five so to one big H file okay if
it's a if it's compacted you just grab
the index of that in you go and get the
roll if it's not be compacted if you
have to bring this so then you have
tuning decisions of how fast do the big
compaction versus a short compaction and
so on and so forth and eventually you
get hiccups when you do a lot of reads
over this so you see a good performance
and then of a sudden you get do and
what's happening in the background
age-based basically trying to reconcile
so once it's a big one child there's
everything's faster yes you have control
over that in HBase or is it just I think
you do I think but I'm like I wouldn't
bet my money on it configuration
parameter specifies how frequently that
compaction but you don't know that's the
point I mean having quite awesome that
you don't know how how the heck would
you know but what about the right time
is this is classic a problem for most
systems right we have all these
parameters and we don't know how to tune
them so HBase falls in that category
that's why I called it depicts honest
we're almost to have to solve so we give
you a friend yes and if you find unity
of algorithms i will show you buddy i'm
doing great yeah yes you talked about
the update index as part of your dogs
did you perform any experiments on like
the scalability like how what kind of
update rates you can handle you know
this is actually in the list to do it
makes absolute sense but this is
basically riding with what HBase can
give you okay but it makes perfect sense
to be able to know that and one of the
things why we're actually doing this in
other words staying at the high level is
because there's a lot of work going on
here and it could be that the three
months from 94 on the same experiment
that you mentioned will get different
results because it's a huge community of
communities contributing stuff when
there's a lot of smart people working
here all right thank the speaker and
angry thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>