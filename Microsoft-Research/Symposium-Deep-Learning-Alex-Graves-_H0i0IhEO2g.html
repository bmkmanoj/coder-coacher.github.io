<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - Alex Graves | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - Alex Graves - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - Alex Graves</b></h2><h5 class="post__date">2016-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_H0i0IhEO2g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so while everybody gets seated I'll
start the introduction of Alex graves so
alex is a physicist and a mathematician
and a computer scientist I think and he
did his PhD at it SIA with jurgen
schmidhuber and that followed up by two
postpositions one in the tu in Munich
and one with geoff hinton in toronto so
he now works at google deepmind and he's
working on recurrent neural networks for
sequence modeling applied to speech in
handwriting and much more and today he
will tell us about his already famous
work on neural Turing machines Thank You
max so yes I'm going to talk about
neural Turing machines closer
better so this is joint work with many
people at deepmind to numerous dimension
but they know who they are and I'll get
straight on with the basic principle of
the neural Turing machine which was we
wanted to turn neural network into
something like a differentiable computer
by giving it read/write access to
external memory and the kind of cartoon
picture of this system is that the
neural network plays the part of the CPU
in a normal computer and the the memory
which is just the matrix of real numbers
plays the part of the RAM and the hope
is that if you put these two things
together you get a computer that is able
to learn programs or algorithms just
from input and output examples and back
propagation of course and another way
another way of looking at this is that
what we want to do is take a neural net
and change it so that it separates
computation from memory and we think
that this is a really a crucial thing
that is it's it's necessary for this
kind of system to learn algorithms so
the to go into a little bit more detail
the architecture consists of what we
call a controller network which can be a
recurrent Network or a feed-forward
network we typically use in LST m-net in
fact so the controller receives input
vectors and gives out output vectors
just like a normal neural net but it
also interacts with a large matrix a
large memory matrix of real numbers and
it it interacts via these attentional
processes that we call heads so this is
kind of where the Turing machine part
comes in so the read heads basically
take information from the memory and
return it to the controller the right
heads take information from the
controller and use use that to modify
the memory and the kind of key the
guiding principle and some way behind
all this was to make sure everything was
differentiable so I think we were seeing
this more and more on deep learning that
as long as you can back propagate
through it you can learn it so as long
as you keep everything differentiable
you're safe and so of course we don't
want the network to
modify or access the entire memory at
once we wanted to selectively focus on
specific parts of the memory that we're
going to read from or write to and in
order to do this we need an attention
model and I call this an introspective
attention model because it's it's a kind
of a form of internal attention rather
than external attention that we usually
think of and the the the way this
attention model is is is defined is that
the the controller outputs various
vectors that are used to parameterize a
distribution which we call a weighting
over the rows or locations in the memory
matrix and this weighting is there's two
basically there's two main attention
mechanisms at work in the neural Turing
machine one of them is based on content
content based lookup and one of them is
based on the actual location or address
within the memory so content based
lookup is by now pretty familiar
actually in the last two years this has
become kind of it's sort of swept the
board of at least recurrent networks if
not deep learning in general and the
basic idea is that you your neural
network emits a key or a key vector and
this is then compared to a whole bunch
of vectors that you have so in our case
it's compared to to the the vectors in
the locations of memory in other systems
as compared to a bunch of embeddings but
it's basically the same principle and
you compare it using some similarity
measure that gives you a score for the
similarity of your key to what's in the
what's in the memory and we use cosine
distance for this and then once you have
these similarities you generally want to
normalize them so that you have a nice
kind of balanced distribution over the
over the possible vectors and there's
one last component we have any
normalized with the softmax one extra
component we've added is sharpness
parameter which basically just it's a
number greater than or equal to one
which just multiplies the similarity
scores across the board and therefore
sharpens if that number is high it
sharpens the focus of the controller on
a small piece of memory
so it gives the network a way to decide
how how precise it wants to be with the
information it gets back and you know
the basic principle here is just this is
a associative lookup so what you're
getting back is the thing that's similar
to what you put out now a these
location-based addressing is something a
little bit different and it's not
something that I've seen in any other in
any other deep learning systems so what
we wanted to do here was to - where so
whereas content-based addressing and no
ignores completely where something is in
the memory and only cares about what the
contents of that actual vector are this
one does the exact opposite it doesn't
care what the contents are it only cares
where it is in the memory and it works
by outputting a shift kernel which you
know kind of a fancy name but really
it's in our case it's just the softmax
over a set of numbers between plus and
minus well often plus and minus one
maybe plus or minus five whatever and
this gives you kind of like a little a
little simple filter that you can
convulsing waiting to produce a shifted
waiting so if we look at the picture
over here that heat map represents a
waiting where you know the the Reds the
red lines are places in memory that have
a highway that are being focused on and
the blue ones have a low weight and are
being ignored and so if you took that
waiting on the left and convolve it with
a shift kernel that had all of its all
of its probability mass concentrated at
one point say a shift of +10 and
basically you'd end up with the one on
the right and and it wraps round at the
borders of the matrix if you're curious
so it gives a way of kind of taking
something that you've the taking a
weighting you've already generated and
pushing it around up or down okay so the
reason that we wanted to have these
multiple ways of addressing the memories
that we wanted to give the controller
several different modes of reading and
writing the memory and these modes have
quite you know a nice correspondence to
the data structures and data accesses
that we use in normal programming
languages so if the if the controller
isn't only emitting a content key then
what would the memory is basically being
treated like an associative map this is
the straightforward association between
the
in the content if if both the content
key and a location shift is specified
then it's it's almost like the chunk of
the memory is being treated like an
array so the key finds the head of the
array and the shift the content shift
then indexes into it and if we only have
location and we're completely ignoring
content then all we're doing is
iterating from the last point so we have
something like an iterator like a list
iterator
and this gives the network quite a rich
set of ways of interacting with a memory
and we'll see in some examples how it
actually uses these to solve problems ok
so once you've got the way things then
you have to define you know how you're
going to use them to actually interact
with the memory the reading is very
simple it's just you know you just take
a weighted sum over these these memory
rows and that you know you that's um you
combine that sum together to get a
single read vector which you then return
to the controller so basically you know
if if the waiting is very focused on one
point then this read vector you get back
will strongly resemble the the memory at
that one point and so writing is
slightly more complicated and what we
kind of did here is we modeled the the
this on the input and forget gates in in
an LS TM network and we said we
decomposed writing into an arrays and an
add so first of all the write heads as
well as omitting the parameters used to
determine the waiting's they also emit
this arrays vector which uses the set of
numbers between 0 &amp;amp; 1 and an ADD vector
and the arrays vector is used to kind of
scale the the existing rows in memory
towards 0 and then the add is added on
top ok so let's have a look at the the
first simplest algorithm learned by MTM
so we started putting these in in a kind
of visual language we spent a lot of
time analyzing you know what the network
was doing and we call that n TM + + so
basically what we've got here time goes
from left to right the left column shows
the the right weightings of the network
and the right column shows the read way
things and again these ratings are just
heat maps so basically if it's all black
with a single white square that means
that the waiting is completely focused
at one location in memory so you can see
it's very clear here there's this dag
line in the memory that means that the
the network chose somewhere to start
writing and then just iterated one step
at a time using the shift operator and
while I was doing that basically it was
writing down the the information that
came in to the input sorry I should have
defined the past here first the task was
take some random sequence and read the
whole thing into the into the the
network and then reproduce the whole
thing in order and it can do this you
know basically the pseudocode for how it
does this is on the right it uses this
the iterator to write down all of these
inputs in order then jumps back to the
start of you know where it started
writing and then uses the same kind of
iteration scheme for the reading so just
reads everything back in order and you
know the important thing about this so
just just copying a sequence of a fixed
length is pretty easy you can certainly
learn that with with lsdm but if you've
learned a copy algorithm then it should
generalize the arbitrary lengths and
this more or less does so you can train
it on say sequences of length one to ten
and this is what happens when you
generalize incrementally up to 120 and
if you look at the targets versus the
outputs you can see that there's some
mistakes but the network is basically
you know capturing the structure I mean
it's more or less reproducing the inputs
okay so slightly more you know the next
step after that was to say well can you
copy something end-times and this is
like a little little sort of very simple
form of a for loop so basically now
you're given you're given an input
sequence and a number and then your task
is to reproduce that input sequence that
number of times so we know how it does
the you know the reproduction of the
sequence that's just using these
iterators what's kind of interesting is
now that it has a number to store it
does this by iteratively adding
something to the memory at each step so
and I'm kind of want to stress this
point because this is one of the key
differences the ability to write to
memory as well as read from it is one of
the key differences between NTM and
memory networks and other kind of
tension based memory systems let's say
and I think there's a lot of cases where
it's really necessary to write and
here's actually here's an even better
example of that so this task is is
somewhat complicated to explain so bear
with me basically you get an input
sequence so at the start of every
sequence you have a set
Engram and Engram transition matrix
maybe it's five grams or something I
don't know n n was fixed and then you
generate a sequence from that transition
matrix a stochastic sequence and the
network must attempt to predict what's
coming next
oh dear I'm running out of time and
basically there's an optimal way of
doing this which in you know there's a
straightforward kind of Bayesian
algorithm for doing this which involves
keeping count of how many times you've
seen a particular Engram and the network
does this by waiting until it sees an
Engram so until they've seen them you
know the context of the last five ones
are zeros and then writing to a
particular place and you can see from
these green arrows that it's writing
again and again to the same place and
that means it can keep count it can keep
a count of how many times it's been
modified so again it's using its there's
writing as a fundamental part of this
and so I think I will skip to my last
slide which is some new work that we've
been doing now I hope this is clear to
everyone it's a little bit faint I know
and the task here so we've started
looking at is you know rather than just
looking at kind of sequential data can
we get the network to learn algorithms
on on graphical data and so what we've
done is we've fed in random graphs to
the network just as secret you know
unordered randomized sequences of edges
and then we ask questions like can you
find in this case can you find the path
from the shortest path from node three
92040 and this is the path that's
highlighted by the gray nodes and now
I'm gonna put this unfortunately it's an
animated gif which isn't very easy to
display so if I go back to the start
here so first of all we send it the
query which is just it just we just say
to it get from three 92040 for O and
then we give it about ten steps of
planning so a bunch of computational
hops here because obviously there's no
way it can solve this problem in one go
it has to iteratively look through the
memory and it's very interesting what it
does if you see after the first step of
planning is looking at all of the links
going in to the end node then
that it knows it has to reach and then
it starts looking at the links going out
of the start node and at the same time
as it's doing this sorry I should
specify on the left the ones highlighted
in pink on the left are the places that
it's reading from and the ones on the
right are the places that it's writing
to
so as well as reading what's coming out
of this node it's it's also writing down
something about maybe it's maybe it's
keeping track of the fact that it's
already expanded out this node I'm not
sure and then well it seems to muck
around for a while that goes back to the
start back to the beginning and some
point here it starts going a little bit
further so you can see now it's looking
at things going into the node that's one
step before the end and similarly it
starts looking another step away from
the beginning so it's now looking at the
sorry the outgoing links from nine nine
to I think it is and then I think it the
next step after that it is it's planning
time is over and it now has to give the
answer and it gives the answer it starts
so again goes back to the start node
like it has to looks at the outgoing
edges moves to the next node along the
gray path looks at the outgoing edges
and this time it's is definitely writing
something as it does this I think it's
keeping track of the fact that it's
already going through these nodes and
shouldn't return to them and it goes on
like that until it gets to the end how
much time have I got left 22 seconds
okay that's a good good perfect okay so
in summary you know we already showed
with the first paper that NPM can can
learn some interesting basic algorithms
just from input and output examples
things like copies loops even sorts
dynamic Engram inference and so now we
started looking at or tasks more complex
tasks on you know more complex types of
data so we're looking at you know graph
tasks algorithms on grass question
answering on graphs and also looking at
using reinforcement learning form or you
know general kinds of planning problems
thank you
so we have four to five minutes for
questions
timer ah thank you I like to thank you
for describing this very nicely please
speak in the Oh yellow is this better
okay better I am wondering if you need
two very very carefully design the
curriculum for this thing to learn
anything at all yes we are actually
using curricula now so once we came to
these these more complicated type of
problems like graphs and things like
that a curriculum helps a lot and I
think if you want to start building up
composite algorithms for example it's
it's obvious yeah so I think I I
personally to go back to the question
that the panel I think curriculum
learning is something that's it's just
going to become the norm soon everyone's
gonna start doing it thank you hello
I thought that your work was
particularly interesting I'm a question
about the content mechanism the content
base addressing so you mentioned that
it's kind of like an associative map
however an associative map where you
usually do is you use the key to recall
the value and in your situation the key
in the value are the same thing aha so I
wonder if you I mean it seems like it
would be cleaner to have a completely
separate set of units do they recall you
you couldn't do that but actually this
is this is something I'm going to
discuss in more detail at the RAM
workshop on Saturday but actually if you
partially specify the key then at least
there's something like cosine distance
the part that you don't specify that is
you leave at zero that then gets filled
in with the value so you can use this
single content mechanism to do key value
retrieval and in fact the the network
has to do that to solve these graph
problems okay thank you
so you may have already answered this
but why didn't you have sort of like a
absolute random access mechanism where
you just you just asked by location
directly as opposed to have a kind of a
relative shift from the previous
location oh it is it is a relative shift
no I why not have an apps and absolute
because it doesn't scale I mean if you
want to you know you want something
where you can expand the memory
dynamically and if you have to say say
you have a soft max over locations then
you need you know you need as many
parameters as there are locations so the
memory is not fixed the memory size I
mean in practice it is fixed but in
principle it shouldn't be the memory
should be you know kept separate from
the from the network right the number of
parameters shouldn't grow with the
memory high sense for a talk it's really
interesting and I have kind of dummy
question I'm not sure that I got an idea
how do you enforce your NTM to store
some specific information in the memory
like in grams but this is whatever we
don't force it at all so it's you know
pure and and supervised learning
so we basically we trained them the
problems and if it solves them we try to
work out what it's done thank you yeah
sure I just want to mention that you you
said that there wasn't much work on
location based addressing but your own
work on like handwriting generation use
location-based addressing and we've been
working on the Android end to end speech
recognition attention with attention
over the last year and it uses a
combination of location and content I
attention based
I'm glad you told me that I thought I
only used content both okay that's my
mistake okay very last question timer
thank you
for the speech because you really want
to bet most mobile that's what I thought
when I saw the system I was amazed it
only worked not only call me for
translation you just need content
location make sense keep it short please
random access in digital computers
skills very nicely because he use a
string does a binary notation any chance
that might work
you-you-you mean like it's logarithmic
like yeah you talk about the length
yeah so we actually tried having
explicit prefixes that it could search
by so like it would have had there was a
fixed binary prefix a prepended or
appended to every vector but it didn't
seem to use them in practice I don't
know if that answers your question
I'll come back and ask more later okay
okay so thank you very much oh yeah we
can give it another round of applause
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>