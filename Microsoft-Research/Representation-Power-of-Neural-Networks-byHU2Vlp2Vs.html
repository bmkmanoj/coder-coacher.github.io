<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Representation Power of Neural Networks | Coder Coacher - Coaching Coders</title><meta content="Representation Power of Neural Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Representation Power of Neural Networks</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/byHU2Vlp2Vs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by Microsoft
Corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
okay good afternoon everyone so I'm very
happy to have met with tear gas key from
University of Michigan so many of us are
interested in deep learning and trying
to understand what are its theoretical
underpinnings and matters will try to
tell us something new about those hi
thanks a lot and I'm very happy to be
here it was very nice to be invited by
Sebastian and I also say that I I like
this topic very very much and not not
just because it's become popular but
actually I think there's a lot of just
independently interesting things about
this problem and I think there's a lot
for basically everybody to contribute to
it just from from almost any standpoint
even just a purely mathematical one so
before I tell you what's in the talk and
any sort of summary most this talk will
be first principles so I'll just tell
you even even what a neural net is and
specifically the kinds of neural nets
we'll talk about today so a neural net
is just a way to write a function as a
graph so it's a computational graph that
works as follows so the graph has some
kind of multivariate input and then
there are nodes and they're
computational nodes the way they work is
they collect a vector from all of their
parents they wait for their vectors
there are parents to compute something
then they perform a linear combination
of what their parents did and then they
apply a fixed a fixed nonlinear function
Sigma so a class of neural nets the way
it's defined is we fix the network
layout you've noticed I left out some
edges so it's just some collection nodes
in some some collection of edges that's
fixed and the Sigma is fixed what we
vary are these linear combination
weights and one of the standard choices
for this nonlinear function Sigma is is
this funny function which is identity on
one side and 0 on the other side of 0 so
this functions actually become very
popular lately it's kind of the most
popular one right now and when I started
thinking about this problem I thought
well you know I might as well just try
to get caught up and use this one
because it's popular in practice but
fascinatingly this was actually
beautiful
with mathematically so this this will be
the nicest one for us to work with today
and if you follow any of this literature
there's all sorts of other words people
use to describe the kind of fanciest
most complicated version of neural nets
that are popular and we won't we won't
be discussing those these are words like
pooling and convolution we won't be
discussing that today so this is just a
very simple version of a neural network
okay so this is what the talk will cover
the well we'll try to get a handle on
exactly what this class of functions is
because I've just I just defined it
symbolically but it's not at all clear
what what these actually look like is I
very very all these linear combination
weights so thus we won't understand what
these actually look like what this class
functions is so first couple cover a
classical result which is that um they
can can fit continuous functions and
this result has a very strong limitation
this sense that it does not tell us
anything about what we gain from having
multiple layers of these things so in
practice and especially lately people
build these very deep circuits but but
these Cosco results only say something
about in fact something with only two
layers so so because of that we'll talk
about two results that that do tell us a
little bit about the benefit of depth so
one is one of my personal favorite
results in in actually the entire
machine learning in statistical
literature which is the computation of
VC dimension of these of these functions
and don't worry if you don't know what
that means we'll also I'll also explain
what VC dimension is and then the second
one is what I call exponential
separation it's basically a case where
if you allow yourself to have multiple
multiple layers then you can get away
with using exponentially fewer nodes or
logarithmically as many nodes and so
this is in terms of in terms of hype
this is the this is the new result if
you want to call it that but on the
other hand it's actually as I'll say in
the closing remarks when I give a lot of
open problems the new result is actually
the I'll tell you guys it's actually the
wrong result it's not it's not nearly
what we wanted to prove so the problem
is still very open so so like I said
there's lots of room for everybody to do
to be brought to all of these to all
these things okay so just to warm up and
start this whole set up very slowly when
I say that we're gonna fit a continuous
function I have to say what fit means so
one of the standard ways to say we fit a
function is in the LP sense so you're
familiar with it this is an LP norm and
it's just it's just basically an
integral and the point is that often we
would like this uniform sense which
means for every point in our domain we
are some epsilon away this one's a
little bit weaker we can average so that
we can give up entirely on some small
regions of points so this one is a this
one's easier to satisfy this one's
harder to satisfy oh and there was
little pictures that's kind of the
average sense and this is the uniform
sense and then in the later sections
we'll actually care about what I'll call
the classification sense of fit which
means there's a problem we want to
classify and we're gonna care about how
close we get to classify incorrectly on
this problem okay so continuing the
warm-up theme let's just cover a very
simple kind of the simplest possible
setting so what if we only have one
layer and of course we only have one
layer and I'm only doing a univariate
prediction so this is really just one
note it's known that with one node what
can you fit with that it's a simple
question and it has a simple answer if
this function Sigma is is monotone then
basically if it's monotone in this
linear combination I almost have an
indicator on a on a half space there's
some half space and I'm gonna be going
up basically along with the normal
vector of that half space so another way
to say that is I clearly cannot fit
arbitrary continuous functions because
consider this one if I'm correct over
here I have to be small over here so
that means I'm large over here so if I'm
correct on this I'm wrong on either of
these two if I'm correct on one of these
two I'm wrong on this one so there's
kind of a kind of a direct argument
tells us that in either of the sets the
fit we have to make a pretty substantial
error so this was kind of a maybe as a
stupid trivial example but there are two
reasons it was valuable so one is we
prove to that one layer is not
sufficient and it's gonna end up that
this is actually tight so with two
layers this is all we're gonna need and
the second thing is that in in this
lower bound we were we had a function
that basically has one bump we have this
half space and a monotone functionalized
we're basically fitting something with
kind of one bump and it's gonna happen
in all the results today that if we
build a shallow Network we basically
need to have the same order of nodes and
number of bumps in the function so so
this principle will drive all the lower
bounds today okay so to make they make
things a little bit more interesting let
me tell you about one of the kind of the
I considers to be the folklore proof of
how powerful a neural net is this isn't
quite as strong a result as the standard
one people throw around to say that
known as fitting a continuous function
but I find personally find this result
to be extremely illustrative so let's
say you want to fit a continuous
function from zero n to the D so the
hypercube - to just 0 1 and in this
picture I've made it even easier I have
the red read the thick red regions are
supposed to be where the function is 1
so it's 1 in those regions and it's 0
outside so if I want it to fit this with
a neural net I claim that it's trivial
if I can fit a box with a neural net so
by bunk oh so in in the picture or yeah
so it's it's from the interval from it's
from o 0 1 squared so it's from the
plane and then I've just drawn the level
curves so the two thick red so it's one
inside yeah sorry it's one inside here
one inside here 0 else you have some
painting in the Bargello it is yeah so
you have to yeah I was just trying to
simplify the picture so you near the
boundary close from what does irrelevant
in a session yeah very quickly okay
it's a pickling yeah you should call it
or you could call it an ISIL line and I
was just you know at rest at scale one
but I'm okay yeah somehow drawing
pictures is their whole sorry but so if
we had a neural network that could that
could l1 fit a bus and in the nail P
sense fit a box then I claimed this
problem is trivial and the reason is we
just we just grid the space and we just
fit each one of those boxes because then
we can just add these all up I can add
things up I can take linear combinations
three on their own net so I've reduced
the problem of fitting this function to
just the task of fitting a box and let
me just say there's kind of a common
theme that I'll come up here which is
that I'm building basically a gadget out
of the neural network I would have a
little tiny neural network that fits a
box and I'm gonna sing look at the span
of those things so I just like calling
it a gadget okay and to to to fit a box
is also very easy for us so just suppose
that this nonlinear function Sigma is
very close to the indicator function
we're gonna see how to fit it with just
two D plus 1 to D plus 1 nodes and the
the hint as to why it's 2 D plus 1 is
because a box is an intersection of two
D have spaces so because these neural
net single nodes behave roughly as an
indicator on on a on a half space got to
D half spaces so what I can do is I can
just take one dimension and I can put
one of these I can put one function with
each each of these normals or along each
of these normals so it's two in here and
one one in each of these I can do this
for all of them I end up with this and
now I'm in good shape because if I just
apply one more and I can threshold it at
two D minus 1/2 and it'll so only this
region will will be satisfied so
basically it is intersecting together -
D - do you have spaces and then that's
kind of what you get after that so with
this kind of fuzzy reasoning we've
a contused function from your one to d2r
with 2.5 letters I say 2.5 because I'm
just using the linear combination part
of the neural network so with with two
layers I fit a box and then the span of
these can fit any confused function from
zero you're wounded ax T to R and then
if I apply another non-linearity then
I'll be at three layers but then I can't
fit any um but then I'm iris constrained
to be zero of my range is constrained to
be zero one okay so this is the this is
kind of the folklore result and so there
are a couple there are a couple of
problems with it so so one is that s was
kind of pointed out because everything
is continuous we have a lot of we have a
lot of fudge factors on these boundaries
I can't exactly do these these these
hyper rectangles I have a little bit of
fudge on the ends and so that's why I
have to do an LP type fit because I have
to allow myself to kind of make some
errors in the boundaries if I wanted to
do a uniform fit then in the supremum
sense then I then I wouldn't be able to
use this bar argument it wouldn't work I
also want to say how old this proof is I
consider this proof to be as ancient as
mathematics basically if you look at
Jordan content or or definition of
lebesgue integral or any of these things
you see these kinds of these kinds of
box arguments I'll also say that notice
the way the proof worked I claimed that
we really didn't use anything about
composition of function under illness I
built up a basic class of functions and
I looked at its span and so in fact if
you look up how to prove for instance
that boosting is consistent data boost
auger than whatever you call it then one
way to do it is to take decision trees
of a certain size you make them have two
denotes then they can fit boxes also and
so the same proof says that boosting can
fit arbitrary decision surfaces so again
this is only a vector space argument it
is not an argument using anything about
composition of functions we constructed
a basis class then we reasoned about its
fan
okay and so we had a gap we had two
point five layers the upper bound we
didn't have an a uniform fit and so we
have a gap between this and lower bound
so we can close this gap with the so
this is the result that everyone
actually cites when I say nomads can fit
any function II they cite this result by
this guy George de Banco from 1989 and I
don't I don't know how it is for
everybody else but even though I see
this sighted basically infinitely it's
like in every paper I've actually never
seen anyone discuss it just for reasons
I I can't really comprehend so just just
sicom it but I actually like this proof
a lot it's extremely clean and every bug
that you that you and every little kind
of nastiness and sloppiness and
everything I just said is gone from this
proof so and I will say that I'm I'm
watching the clock and so I'm gonna rush
a score I'll just give the whole thing
in detail it's fine it's very clean
so this set up of the proof is very
similar to the last one I'm going to
build a gadget I'm gonna build some kind
of primitive object out of just
neural-net nodes and I'm gonna reason
about the span of this thing and before
I kind of used very hazy reasoning I
said fit things with boxes grid this
space Jordan content but we don't need
to do any of that because vector spaces
are such well understood objects that I
can just there's there's theorems that
can use I don't have to save you know
approximate things so the proof itself
is on using functional analysis that
might be why it's not discussed much
because the way it stated is actually
may be a little bit impenetrable but um
I'm actually gonna give a hilbert space
version of it and the hilbert space
version you can just read it knowing
what a vector it if you know what a
vector is and what faggus theorem is you
can just know understand the proof so
okay so here's the proof the first step
is you see is you you could prove that a
single neural net node is what I call a
correlation gadget and let me I'm like
I left something out of this slide I
have to tell you what Sigma is all we
need for Sigma in this proof is that on
one side it limits to zero on one side
of limits to one the whole function is
bounded and measurable this is all you
need it can be zero
you do any stupid thing you want and
then it can go to one so it's
approximation of the indicator but it's
a very weak approximately and of the
indicator so you have to believe that
this what I call the correlation
property so give me an F that's non zero
continuous F that's non zero and F in it
has been l/2 it has to be an able to a
function but give me an F then there
exists if it's non zero then there
exists choices of the linear combination
parameters so that this integral is non
zero so for any nonzero function I can I
can kind of kind of detect some
structure in it and interesting enough
the proof uses basically the same box
fitting argument I gave earlier the
effective way the proof goes it says if
the function is continuous then I can
kind of build up boxes I can integrate
the function using boxes but those boxes
have to have nonzero met they have to
have nonzero measure otherwise the
function is zero so the proof of this
actually embeds the previous proof
technically it has to use Fourier
analysis but it's the same it's using
the same thing under the hood this is a
lemma and then here's the in the Hilbert
case it's just a just almost a direct
argument so I have all of these so these
correlation guys these are just all the
functions here as I vary w 0 and W so I
look at the span of that ok that's a
subspace I'm an infinite dimension so
it's not might not be closed so I take a
closure of it l2 says yeah in this is
one of the places where in the full
proof you have to work in a uniform
topology and this kind of I guess so
anyway so it's the is this closure but
now it's a closed subspace it's a closed
subspace and I'm at Hilbert's spacing
and talked about things like perp
vectors yah Thackeray and theorem all
this kind of stuff so the the theorem
will be that the closure of the span of
these things equals the continuous
functions by
definition of closure this means that
for any epsilon I have an element in
here that is that is uh epsilon close so
the proof goes like this so I say give
me any continuous function and I can
project it onto my closed subspace and
then I can just look at this this
difference if I click the difference of
the two sorry okay so this implies that
the contrast functions on 0 under the P
is a closed subspace of no it's just
it's not so what you can see is the
continuous functions is a subset of
disclosure but this cannot be the
quality no any continuous function right
just I'm talking about the out two
continuous functions so but the closure
needs in the topology of the inverses
which is able to closure of some
subspace yeah if that has to be equal to
the set about the business function it's
not instead of all could you spot that
is a statement this is the I'm giving
the weakened version of the proof which
is only for Hilbert spaces I'm now
giving the whole the whole uniform
version of the proof so I don't
understand that statement that equality
there which is a closure that's fine
equals what you're right I should have I
should have justified the right-hand
side with only Hilbert spaces it's so
that the full statement uses the the
uniform code beautiful closure native
from topology the best thing is the
continuous functions themselves are
inside the Hilbert space they're not to
help your space because it's not
complete yeah but but it is the inner
product space so you can do closure in
there yeah since since I see that yeah
well what I'll do is after I state
because I mean it's clear to me that you
understand this very well so after I
give to the Hilbert version I'll tell
you how to translate all the lines into
the functionality version this was
well let me give you know I'm this this
is the this is the audience I was one I
mean this is this is great because I
have a background in his functional
analysis myself I have to ask myself at
some point why I so rarely see details
of this proof discussed and the only
thing I come up with is that is that
people get scared when they see the
phrases like hahn banach theorem and
they have to you know that this thing
for instance if this proof you know so
you said I used Fourier analysis but but
you have to take 48 Fourier transform of
a measure not of not of a function and
so this already is something that you
know for instance isn't well covered in
my grad analysis textbook so but you are
right that I that in this case I left
out too much detail that equality
doesn't hurt so I apologize for that
that aside let me just see how the rest
of the proof goes so so now let's let's
look at this thing so I know that I know
that F minus G is orthogonal it's in the
orthogonal complement to this subspace s
and that means that the inner product
between this and every element in s
itself also has to be zero
but then I can use the contrapositive
this if that's zero that means that for
all of these that's zero which which
means that there's nothing outside of
that means that my perp is 0 itself
every perp element of values to zero so
that that equality holds so now that
that's been said let me explain just
quickly for the experts in the audience
how to translate this thing into into
into what it actually is supposed to be
so I cannot I don't use the ltte space
but I use this the correct topology for
continuous functions is the set of
continuous functions by the way under
the restriction 0 1 to the D to R it's
that that's important cuz it's important
that I'm talking about contused
functions over a compact set so this
together with the uniform norm which is
that super norm so that is a Banach
space and the important thing is that
the dual of this space is the set of
route on measures on this thing
signed rat on sign measures and so then
this step so we cannot we cannot take
projections but I can use the Hahn
Banach theorem to get what's effectively
a Perfector and and so then I can still
use the same reasoning over here so know
that that was that was an excellent
point and um okay okay yeah yeah nice
view I feel very bad that all right so
this is this is the quick summary so two
layers is enough in a very strong sense
to sell infinity sense and and also
notice what we actually constructed in
this theorem these correlation gadgets
this is exactly what you're doing in in
boosting algorithms for instance you
basically find the weak learner which is
most core labor the thing this is an
algorithmic proof you know you greedily
you would you wouldn't you're
constructing this fit you would actually
pick the most correlated thing ever
every iteration and so that this has
been done this is actually what I would
argue popularized to great extent these
greedy these greedy methods this is big
paper by Andrew Barron from 1993 and
just a funny remark we can fake make
this algorithm deep in the following way
I can I can take a deep Merrick where
what it does it devotes part of every
layer to copying the input forward and
so I'm just and so then I can just every
time I would do my boosting type thing I
would just have the nodes just kind of
go like that and while this is kind of
stupid because that means I have you
know D nodes and every and every layer
doing the copying maybe there's some way
to compress it and okay the problem
though with this is that there were
there's not a good understanding of the
number of nodes the number of layers
trade-off or of function composition in
general so we haven't it's nice that
this result is true but and you know I
really like the subhankar proof but we
have not really gain that much about the
heart of the heart of the problem so the
next section like I said our results
very dear to my heart and because I've
slowed down a little bit I'll just kind
of state the highlights here
but but I will second that these are
absolutely beautiful results and I find
them personally very surprising so I'm
just going to assume everyone knows what
VC dimension is so the question is so
now that I gave you these networks and
so suppose that in those networks for
every one of those sigmoid functions is
um is just the indicator zero one the
question is what is the VC dimension and
for me the fascinating thing is that
it's literally ignoring a log factor it
is just the number of parameters so it's
this first one and this to me is
interesting thing about because a simple
perceptron so only one node is also
theta of W now this one is actually that
this one's W log W W the number of
parameters the network so all the edges
so nothing this this schema where I use
that function like just the indicator it
doesn't reflect the structure of the
non-linearity whatsoever okay the proof
is actually kind of a straightforward
induction now if I allow these that I
said these kind of popular
nonlinearities now the VC dimension
changes and not only does it become just
the number of parameters times number of
layers the theta was only closed by
Peter Bartlett and he hasn't even typed
it up yet if you look in his book
there's it's not completely nailed down
yet so so this is already fascinating to
me that only increases just
multiplicatively by the number of layers
so yeah I personally find this quite
fascinating and I have to say not only
do I think the proof is beautiful but I
would argue that Bartlett himself loves
the proof because the proof is actually
the cover of the book I'm actually
serious I'm not making this up this this
might look like some kind of stain you
know some kind of radioactive staining
of a of a neuron with like axons or
something that's like it's actually not
if you look in the book in chapter 8 the
figure appears and he just kind of
doodled on it to make this I haven't I
haven't
about this yet but but um literally the
proof of how this works is in there and
this is an amazing proof yet most gentle
square that you ever see mine you just
don't see that if we do not have a bound
on a number of layers the only upper
bound we know right now is W squared yes
but but for these piecewise blends being
piecewise polynomial this if the Sigma
is piecewise polynomial as we change
what Sigma is all sorts of terrible
things start happening yes yes I'll just
save one brief thing you might say ah
I'm sure regularity assumptions holds so
let's make Sigma concave convex goes to
0 goes to 1 and I can impose a
smoothness dot on it turns out I can
choose one of these to get vc-dimension
infinite with only three notes so this
is a very delicate business and how do
you actually prove these vc-dimension
bounds you have to use really
high-powered techniques so for those of
you in the audience that know sart's
theorem and know bazoo so bazoo theorem
talks about counting intersections of
polynomials and high dimensions and it
starts to make sense why that would come
up so these proofs are amazing
so if um if you do have time after or
later this week you can ask me because I
I love all these results a lot okay so
what we know so far is that a flat
network can fit any continuous function
and we also know that the number of
functions we have in the classification
sense how many classifiers we get it
doesn't actually grow that fast with the
number of layers but what we don't know
is what these functions actually look
like so in an attempt to get a sense of
what the functions with many layers look
like and how different they are with
what you can get with a flat network I
asked and answered the following
question so the setup is as follows you
give me an integer K
so this is going to be a result that
holds for all positive integers can you
give me a K I can construct two K points
with a following property any flat
network with less than two K nodes in
the network will have error at least the
sixth and the result itself will
quantify what flat and all these things
mean there there are no it's actually
not even to use asymptotic notation it's
a very it's a very clean easy to prove
and easy to state result and it's not
just a separation from flat and deep
it'll be for any arbitrary number a
number of layers and then the the
punchline will be that if you give me it
ends up two K layers I can get 0 error
with just 2 K parameters and something
called a recurrent net which is a fixed
small network so network of constant
size and then I take its output and plug
it back into itself and I do this K
times it'll also get zero error and the
reason why I care about this audition in
addition is just understanding this
class of functions is because in a
statistical sense I know that this thing
has exponential the VC dimension of this
thing so maybe there's some hope for
learning these functions from data well
and the thing to contrast this against
is the switching lemma and related
circuit complexity results which get a
similar which get a similar trade-off so
yeah I'll talk about this more maybe
offline ok so let me tell you what the
class of Sigma's that I deal with is
actually yeah okay I'll just do this so
this is just a class that has two nice
properties will slightly generalize that
constant and an identity function and
also I can use this class of functions
to inductively reason about what every
layer in every node in the entire
network is doing that's why this will be
a convenient class to work with so I
call this function T T sawtooth but as T
pieces so I take the real line I
partition it into T intervals possibly
of course they're two of them have to be
infinite and it's a fine in each of
those pieces and I don't require the
function to be continuous so it can it
can have discontinuities so it can be
like a piece and then another piece that
doesn't connect in another piece so this
is T a fine and two examples are so this
kind of popular function is piecewise
offline with two pieces but then you can
come up with other examples so again
this is only a univariate example so
decision tree kind of business
meaningful as usual but this is a tree
with t minus one knows we'll just have
its T sawtooth so this lower bound will
also apply to different algorithms so
for instance just lower bound also
applies to boosting but you might say
doesn't matter exceed you in a very a
problem but it's still there's all holds
so reasoning about these functions is
very easy so first if I have something
that's s sawtooth so it's piecewise
offline in s pieces and I have another
function which is T sawtooth
so it's key stays offline and T pieces I
claim that the summation of these two is
just s plus t minus one sawtooth and the
proof is just to look at every time the
slope changes so every one of the pieces
over here and I noticed that in each one
of these pieces they're both they both
have a fixed slope so I can just count
the number of time to change peaking at
two of them so that's actually t minus
its S Plus t minus two changes they both
agree on the first interval so that an S
Plus t minus 1 on the other hand if I
compose the two together it's it's
times T sawtooth and the way the way to
see what goes wrong is so I compose this
with this so this comes first so I take
the T sawtooth function a look at any
interval and any one of the T pieces
that define it well if I take that
interval and I map it through the
function I get another interval it's
it's a fine in that piece but that
interval I mapped you can't hit every
piece in this thing so for every
interval of the second func of the the
first of the function you apply the
second one to sorry its composition
should be defined the other way for
every for every piece over here I get s
pieces over here so together I get s
times T so and if I apply an inductive
argument to it to a neural network this
is actually quite easy to see so what
you do is you look at any note any node
in the network and I take I take all the
functions that that plug into it so if I
met some if I meant some layer I'm at
some layer J right now all the all the
nodes plugging in Twitter our
inductively going to be TM to the J
sawtooth I add together M of them so I'm
gonna be m TM to the J sawtooth I apply
my non linearity TM x TM to the j
sawtooth the whole thing is gonna be t
TM j plus 1
sawtooth that's the index ID that's the
so that's the inductive step with the
proof so and so saying it again a
network with a t sawtooth non-linearity
m nodes in each layer and L layers is TM
to the l sawtooth and that the point of
this is that the number of bumps the
number of pieces in the function grows
exponentially in the number of layers
but only linearly in the you know you
know all the other parameters so this is
actually what's going to make the proof
go through we're building these bumps
much more quickly with with composition
than with them with addition
okay so this s it turns out is it's
basically gonna complete the proof flip
the lower bound so I'll prove the next
slide ulema the lemma just says that if
you give me any sequence of 2k points
with labels alternating so the
prediction promise from the reals to
zero one you just give me any sequence
of any sequence of reals and I label
them zero one zero one zero one
alternate fast as possible and the claim
is that the claim is that any T prime
sawtooth so it's piecewise offline in at
most T minus T prime pieces has to have
error at least this with oh so I should
say what it is two to the K minus two T
prime over three times two to the K so
yeah so I'll prove that on the next
slide and the reason this completes the
proof is because suppose that you have
use your network structure satisfies
this inequality um so if you just plug
this in you get that your error is at
least the sixth and to make sense of
what so this is where I'm gonna say the
concrete version of what the lower bound
actually is so to make sense of this
quantity let's say L equals two so so
two layers and let's say we're with this
oh do I put uh yes sir I wrote all this
out here so if I use this this thing
which was zero on one side and identity
on the other side which is to sawtooth
so T is two and let's say I used two
layers then if I have less than two to
the K over 2 she knows where air is at
least the six if I make it root K and I
have less than two to the root K then
then I once again have error six this is
pretty rough I will say actually this
this is this is very much improvable
so I guess theory audience the raw
cursor video has this very nice
improvement of switching lemma from you
said it's this fox right yeah so that
one actually says that if you just bump
down the number of layers by one you
still get an exponential gap
that isn't implied by this result so so
the kind of separation get from his
results over you know circuits and
whatever AC zero it's a stronger okay
okay so the way so here's how we prove
this lemma up here so I have my t saw T
sawtooth function I guess I said T Prime
and and I have 2 to the K alternating
points so because I'm talking about
classification and classification I take
everything that's above a half and I
make it one and everything below make it
zero so all that matters is where I
cross zero base but where I cross a half
basically so that means I get a function
which is piecewise constant in two T
pieces
the reason it's two is because at the
discontinuities I can I can also cross
so that's why it's not T saw to see so
at these two T 2 2 T so the
discontinuities actually nuke the
constant and the bound is not tight with
constants in the continuous case okay so
now notice that if I just treat each of
these intervals as a bin then the number
of bins that get at most one point has
to be at most the number of bins so I
have at most two T things with a single
point and that means that the number of
bins with at least two points it's n
minus 2 T it's the total number of
points and Landin bins with at least two
points as n minus 2 T and the reason
this completes the proof is because if
you're an interval that gets at least
two points they're an all training label
so you have to make error at least 1/3
in the limit it s close to 1/2 but it's
at least 1/3 so I just divide this by 3
and it completes the proof sorry that
was the end oh yeah so yeah it's so yeah
it's just a it's just a counting
argument that's all it is
I thought the proof would be much more
complicated and so you noticed that
there you know there are no you know
there's no there's no oh it's not like
it holds for certain can others its it's
just that easy
now let me tell you really quick how the
upper bound works and this one's even
easier so the upper bound is I have to
find a function that's either in care
repetitions of a constant size network
as the recurrent case or is that a kale
air network with K parameters that fits
these points exactly and it's really
easy so I take this function and I wrote
it out tediously up there just to
establish to you if you're doubting that
it is just three nodes and two layers
but this is the easier way to read the
functional form of it so just little
pyramid I know the question is what
happens when you compose the pyramid
with itself it's it's quite easy so it
looks at points that are less than a
half multiplies them by two points that
are bigger than a half it reflects them
so it's literally all it does it's just
it's a and then by induction so then so
then I can just take the set of points
with alternating labels to be the the
bottoms and the tops of this function
and that's really good um one one one
thing that I found kind of fascinating
because it wasn't by design it was just
kind of an accident so you could argue I
mean this isn't I know how okay you can
argue that this is effectively an
approximation of a Fourier basis you
give me a ki I construct these kind of
high-frequency piecewise defined
functions and there is a fairly recent
survey on neural nets where they also
just asserted that that a Fourier basis
is easy for a deep network also you know
the switching lemma that was parity
functions that's the Fourier basis over
the boolean domain so I don't know if
everyone's doing for a basis cuz that's
the first thing you think of but
interesting coincidence okay so we can
close from here so to summarize so first
we gave a couple classical results where
we can fit any curious function with the
shallow Network then we pointed out that
that the VC dimension at least so number
of classifiers we can construct does not
grow too quickly so it's just linear in
the number of layers and and then we
gave this case where there do exist
functions where you have to blow up the
amount of parameters exponentially in
order in order to fit them with a
shallow thing and of course that doesn't
contradict the VC result in any sense
because it is just one class of
functions it's not describing you know
it's not saying that in every case we
can kind of reduce the complexity by by
a log we know logarithmically there's
kind of a nuance that's lost in the VC
characterization okay so I have a bunch
of kind of random remarks that are just
things I found very interesting so at
least for this construction if if I use
these indicator functions that the
result is false you don't increase the
complexity at all when you do
compositions
I mentioned this to Sebastian earlier he
point out immediately that I'm using
only the univariate case and I can't
actually tell you that in Multipure case
it is it is slightly different but but
already I found it interesting that that
um that you have to use something that
all the reproofs I know for these upper
bounds not just that pyramid function I
showed you but other proofs I know they
need a continuous function but like I
said earlier that class has infinite VC
dimension so you have to be very careful
which continuous functions you use oh
sorry yeah so this has oh I didn't
mention this this is used as a lemma in
the proof of the infinite VC dimension
for a very specific non-linearity
okay so another thing and this is a
result I'd really like actually think
I'm gonna probably try to prove it
sometime this month is so we constructed
a single function that we know is very
expensive to to construct with something
that's shallow the question now is what
are some other ones and the result I
would like to approach is can we define
a notion of independence and rank so in
other words let's say I just have a
grab-bag of functions that I can
represent efficiently with with a
multiple neural network and
inefficiently with a shallow Network is
there some other function now which I
can't represent is let's say a linear
combination or just a composition of
these other ones now that I can throw
into this bag we can kind of keep
increasing the set of functions I can
characterize because we didn't had all
characterized all the functions that
have L layers and M nodes in each layer
we didn't even remotely characterize
that function class but maybe there's
maybe we can find a couple more of these
functions maybe not just these piecewise
a fine before you transforms but maybe
there's a couple others that together
they actually do characterize the entire
clip that's what I mean by like dense in
quotes there is something I really like
to answer another thing is is what
actually is this function so a lot of
you know Leon batoo I
I gave actually this talk in Facebook
about a or sorry I gave a version of
these results some other stuff at
Facebook and Leon he said to me that the
thing but he was very fascinated by this
by this pyramid function and so this
pyramid function is it's pretty funny so
if you take a symmetric function G and
compose and look at the compass in G
composed with with with the pyramid so
what does it do if G is from 0 1 to 0 1
if I go so it pre composition so I go
from 1 to 2 to the minus K I'm gonna
replicate am i replicate genes the
condensed version of it and then from
here to here so from 2 to the minus K 2
to 2 minus K plus 1
I'm gonna
I'm I'm gonna get the reverse of it
since I said symmetric I'm gonna
duplicate it so buddy this peak
composition is gonna repeat the function
two to the K times so it's this period
just it's like a period operator or a
looping operator so and I so I say that
there's prior work if you know about
this coma Groff Arnold representation
and result it uses space-filling curves
and fractals and there are dark so this
is a this is a paper there are people
that claim the results are relevant but
I say actually that it captures a lot of
this kind of interesting structure
that's also in this pyramid map and then
so I think in general there's a lot of
room to develop just nice theories about
what composition of functions does so
some oh and by the way I'm not making
fun of function analysis because I
actually spent way too much of my PhD
reading functional analysis books and
using it but but now I realize wait like
I really need to know about more than
just vector spaces so but there are a
lot of interesting fields and and maybe
in during the questions section which
we'll get to in just a moment people can
just tell me about other ones but I feel
like we can just keep doing so much more
so of course in an TCS there's circuit
complexity results so another family
results that I literally know nothing
about I just found out about from Luca
trebizond blog is a su an in additive
combinatorics they look at they look at
the following problem you give me a
group and I look at a subset of it not a
subgroup but a subset and then if I look
at it just happens when I take the group
operation just apply it to itself so you
take the group some sorry some subset of
the group not a subgroup so I'll call it
s and I apply all the elements
themselves I get call it s squared s
cube how does what is the rate of growth
of this thing so me this is very very
similar to this how many functions do I
get us I add more Alerus problem so I
think there are lots of fields that are
attacking this problem of exactly what
is this function class I see yes you
play these compositions I think there
there could be so that this for me is
where this is just a purely beautiful
mathematical question because I still
don't understand what these competitions
are and
of course from our computational
perspective what we really want is some
structure that actually helps us design
algorithms some structure that algum can
pull out you know something maybe like
that correlation thing but a correlation
that works in a multi-layer fashion so
so for me that would be the the best
that'd be the best kind of structural
result okay we're done maybe just a
comment so the result that you told us
it's far from the separation entry
complexity right because it's just
uniform not only univariate it's also
it's not the worst case in the worst
case it could still be that shallow and
deep out the same thing yeah there could
be certain functions that have that
property I just gave one function fire
whereas it when what sir video did is
that he showed that in the worst case
there is four yes yes I well I have to
trust you on that i I thought he did not
show that but I just maybe don't know
the result won't of I thought he did
something kind of like this which normal
where he gave a specific function which
is hard to approximate with with one
less layer but maybe maybe I'm wrong
does anyone know should be clear I
thought the result was a specific class
he calls it the sips or something
functions where it exactly exists in in
a in a depth K circuit but then if you
go down to K minus one circuits you can
only get 1/2 minus little o of 1 close
without getting without having an
exponential blow-up in the size of the
circuit that was my understanding of the
survey D result but you know
yeah I feel bad saying X I'm not trying
to diminish the result I think it's not
actually giving copious lights on VC
dimension you just noticed there were
two results to celebrate yes go ahead
you can I just want to recall you want
to admire the book cover that's all
right
I didn't do this very well did I spent
more time talking about the book cover
than about so there was two Sigma's you
said talked about right yes so indicator
is just the size oh yeah yes yes yes so
just it just yeah so it it was so it was
this one this is the answer so it's it's
it's number of parameters Sigma is just
indicator yeah the mixed and this is if
I use that one that is popular now and
the reason I picked this function class
was because for this we do have a theta
we do have a we do have a tight upper
and barrel where you can ask about what
other function classes I just want to do
so long without these the two numbers
below the person like a multiple-choice
like okay this could be the dancer or do
you expect something with them or wait
there so you're asking why I thought
these were relevant bullet points
including I could have tried to sneakily
construct this talk where I try to make
it sound like my result is actually
showing that there are tons of these
functions where it takes exponentially
as many nodes so if it was true that I
always could take any function as
exponential size with a shallow network
and compress it down to a linearly size
deep network that would that must imply
that that the VC dimension is
exponential because I'm getting all
those functions they have to live
somewhere so if my result was not did
that make sense as I explained or was
that
so what this you can interpret this
result is saying that my result which is
for a fixed class of functions there
actually aren't that many of them there
aren't that many of them then the second
bullet point is because if you look at
all the existing bounds a lot of the
upper bounds not for this function this
Sigma but for many others actually are
quadratic and I won't call out names
because maybe that's unfavorable if
conjecture is wrong but but people like
I rate you know way way up
they have conjectured to me that it that
it actually is super linear and for a
super multi linear so quadratic in some
cases which I have only the only
intuition I have that that might be true
is what those people told me so it might
actually be it might exploit Radek in
some real cases yeah yeah including ones
that people care about yeah it's not
just pathologies but they and I mean an
interesting case is if you saw my
reasoning was doing these little piece
why thinks I'm out bins right
discretizing you know what what if you
know what if that thing is you know 1
over 1 plus e to the X which is which is
the Sigma which is the common thing
people use for well 1 over 1 plus e to
the minus X that thing we can still
prove it theorem but right now the only
upper bound there is actually quadratic
or actually sorry that upper bound does
not even depend on the number of layers
so actually exhibiting a dependence of
number of layers is is often tricky
a better result in years ago I think I
couldn't be here something about that
that I resolved in years ago saying that
the New York networks the size of the
waves is more for me the number and the
size of the network
you mean the Magnum magnitude of the
waves
yes so Kenny connect the dots how does
it that the results representatives me
you mean how does it connect these each
dimension results or how does it connect
back to my show that the complexity of
of the clasp represented by your nets is
dependent more on the this one claim in
this paper depends more on the size they
are like the l1 norm sorry like the l1
norm
yeah the l1 knows all the weights as
opposed to the number of words so how
does it kind of out you put it in
context here
sure so it's it's possible that what I'm
gonna say is wrong because I don't know
exactly what he meant by that but I'll
tell you what I do understand so it
depends on what exactly you're giving a
balance the VC dimension is about in the
classification error if we were for
instance caring about the logistic loss
or the exponential loss or hinge loss
for one of these for these we should use
something like a router mark complexity
result if we use the router my
complexity on when we think about the
rom and complexity of this loss class so
I'm looking to run more complexity of
the loss composed with with the
predictor in that case then the norm of
the weights will come out naturally so
if you care about those kinds of
problems for instance you care about
regression then that quantity is the one
that will come out naturally the reason
I do consider to be apples and oranges
is because gnomes you really do care
about classification and these are theta
belts they're tight
so not only that there really are cases
where you want something out the other
so one one thing I was telling Sebastian
about earlier today is that this fun oh
ok I won't joke but though that composed
pyramid function and constructed it's
got to to the Cape it's got 2 to the K
up and downs in the interval that means
it's Lipschitz constant is 2 to the K so
there are some ways to analyze has cost
functions that would just blow up
of the complexity but if you care about
classification then then that's not the
right estimate so I would say that it's
problem dependent when you think about
the two four bounds and they are I would
say kind of apples and oranges I don't
find that answer entirely satisfactory I
just kind of summarized for you the
results about one of the results but
other but I think this is a very
interesting question actually yeah I
understand precisely what is the norm
you know that characterizes the capacity
of these neural networks this is
essentially open so weight but bitter
battle indeed okay I guess 15 years ago
was indeed I suppose the l1 norm over
the entire network was here to normalize
the entire level but you could think of
things which are much finer because you
know the last layer and the first layers
they shouldn't be represented in the
same way in this no it's a showing that
if you have a boundary there's a mob
right unboxing so one you can express
the Rademacher
so you can get the dimension free rather
macro complexity down if you control the
weights of the entire neural network
either in a one-dimensional just like
you know just like one day oh this is
different for multi layers that you can
expect presumably to have a much bigger
class where you still have a dimension
three bundles or rather macro complexity
but where the norm is not going to be
the same in the last day or if I can
make one more comment here if you look
at the original Rademacher and Gaussian
complexity paper Bartlett Mendel's say
there's a very nice PC there's a very
nice Primerica plexi down in there that
I do not see discussed much particularly
never see discussed in summaries of run
amok complexity on a two layer Network
so for a two layer network so the valve
one of the big okay so I was making it
sound like VC versus run amok Chris a
question about whether you care about
real valued objects or 0-1 objects
basically but there's another big deal
which is of course ranma complexities
distribution
pendant and so he has a bound in there
that actually depends on sparsity
structure a lot and so then he has an
really really really nice theorem in
there for uhm basically the broader
markup licks you get is this s log n
type effect that you always expect as
far as things you know what I mean it
sort of being rude ended so it's an S
login so that's the number of non-zeros
and so so there is this other benefit
abroad more complexity but it does allow
us to have these nice distribution
dependent things all right thanks man</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>