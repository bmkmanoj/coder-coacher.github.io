<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Rethinking Storage and Networking in Next Generation Racks | Coder Coacher - Coaching Coders</title><meta content="Rethinking Storage and Networking in Next Generation Racks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Rethinking Storage and Networking in Next Generation Racks</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nnJctXmElmo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
right good morning everyone thank you
making the right room we had to move
because of the drilling work next door
cannot guarantee an hour of silence in
here we will avoid all I it's a great
pleasure to play later go talk to us
this morning about his work he's been
with us for lunch and a half years I'm
sure most of you know as I go through an
excessive introduction if you don't know
you shame on you for not saying it's a
bigger than that so with that Sergei
over to you rethinking storage
networking in next-generation ranks
thanks for the introduction hi everyone
and so my talk today I'm going to
describe the work I've been doing a
during my postdoc over the last two and
a half years in building more efficient
Iraq's but before we dive into the
technical details let me give you a
brief overview of my background so my
experience with you guys started while
ago back in 2011 when I joined as an
intern to work on an in-memory key value
store I called Jacqueline hide and this
was an extremely positive and
constructive experience for me because
back then I had to for the first time
design and implement a system from
scratch so I really like this approach
and so one and a half years later after
defending my PhD from inner ear in Paris
I joined as a postdoc to work on
designing next-generation racks and data
centers so basically the high-level goal
of my work has been to rethink how we
design racks to make them cheaper to
build more efficient and higher
performance as well and for that I've
been looking at hardware trends that
were emerging in data centers and
applying them at the scale of the rack
so I like to think of the research I've
been doing as a combination of a clean
slate design where basically i rethink
how to efficiently the most efficiently
possible design a system that is
targeted at some work load requirements
and often that requires a clean slate
approach where hardware and software are
co-design attract scale
for some requirements but then it's also
a good thing to implement the principles
that have been isolated during the first
phase and show that on the realistic set
up deployment this this principles apply
well so during my postdoc some of these
principles have been published in a
top-tier conference paper so this is the
paper about Pelikan in basically
defining how to efficiently provision
and manage resources at at the scale of
the rack but in addition to that and say
this is something I'm very proud of we
also had internal impact with with this
research so we have a working prototype
of one of our racks currently deployed
in a data center and we also have a
Microsoft patent so this for me defines
what good systems research should look
like a combination of open research to
explore different possibilities in a
clean clean slate way and then real
implementation to show how this works in
real set up so I'm in this talk I'm
going to describe two different project
that hopefully will illustrate the
approach and and of course this is work
done in a large team of people and so
for each of the project i will describe
i'm going to highlight my contributions
to the project but first a little bit of
context so it's all about increasing the
efficiency in data centers you can think
of it as increasing performance /
dollars in this cat the scale of the
rack and if you look at how iraq looks
like today it's basically very simple
it's a collection of blade servers that
share the same enclosure each server is
connected to a single top-of-rack switch
with an ethernet cable and each server
has a set of direct attached resources
such as storage in memory and if you
look at that it's a very partition
design where each server is basically a
shared nothing module so for example
resources like power and cooling are
supplied on a per server basis
so each server has its own set of fans
it has its own power supply unit and the
direct attached resources like storage
and memory are not by default shared
with the rest of the rack and if the
server fails all the resources that are
attached to the server become
unavailable so what can we do to improve
performance the efficiency of this of
this design well there's been this
emerging vision of rack scale computers
that consist of thinking of the rack as
a whole unit of design management
deployment and operation and this allows
to remove to co-design resources within
Iraq remove unnecessary hardware and
therefore increased density and reduce
capital costs but also because resources
are shared we increase the utilization
and so we're use the operational costs
as well and because we can now think at
the rack scale we can really optimize
the system for data center workloads and
increase performance as well and this
vision is backed up by several trends
that are emerging in data centers today
so the first one is that the rack is
increasingly becoming the unit of
deployment deployment in our data
centers so the data centers are becoming
so big that it's not longer
cost-effective to deploy server by
server we within entire racks or even
collections of racks sometimes the
second trend is that within each rack
the density is increasing thanks to
hardware integration and this isn't
particularly visible on silicon because
in each server now increasingly with the
CPU have additional features such as IO
controllers net well I'll memory
controllers and sometimes even
increasingly the networking is being on
the same silicon co-located with the CPU
which makes the server smaller and
therefore we can put more of them in
Iraq and the final trend is that we
increasingly see hardware vendors
targeting specific products at the
the center workloads this could be
something like archival hard drives that
are optimized for specific access
pattern to data that we see in data
centers could be programmable Nick's
that enforce data center wide policies
in hardware on Linux or it could be even
custom CPUs sometimes so in this talk
I'm going to describe two projects that
illustrate the challenges and the
benefit of the approach of rag scale
computing so the first one is Pelican
which is a storage rack for an
frequently accessed data and in Pelican
we isolate the principles of how
resources should be accurately
provisioned for the workload and managed
carefully at the scale of the rack and
this is a big collaboration with
Microsoft Cambridge Redmond and as your
storage but here in Microsoft Cambridge
I would like to thank Richard Austin and
end and in the second part I will talk
about networking so X fabric is a rock
scale network that optimizes the network
at this at the scale of the rack by
reconfiguring dynamically the physical
topology of the network to match the
work load requirements and this is a
work with nick chen done claro q
williamson and rostrum here in Cambridge
but first let's talk about Pelikan so
pelican is a storage rack designed to
store data that is unfrequently accessed
it's written once and then read rarely
and we know it represents a significant
fraction of the data we store in the
cloud and so we really need to make sure
we store this data efficiently so if you
look at the storage technologies
available today you can see that every
technology makes a specific trade off
between latency whoops sorry between
latency of access and costs so for
example SSDs are very low latency they
have latencies in the order of tens of
microseconds but they are very expensive
then you have slightly cheaper about
slightly higher latency enterprise disks
finally commodity drives and in the end
you have tape which is
tremely cheap but offers latency of
access in several minutes to several
hours and because of this specific trade
off each technology makes its naturally
well-suited to store a particular type
of data so for example SSDs and
enterprise are great for hot storage
when our data which is very frequently
accessed and requires low latency of
access slightly cooler but still warm
data is well serviced by commodity
drives and finally tape because it has
such a high latency of access is only
good for archival workloads which is
basically written once and never read
back so what about cold data then which
represents a large fraction of what we
store well none of these technologies is
well suited to store called data and
what happens is that tape has too high
latency for this called data that is
still accessed from time to time and
therefore cold data ends up being in a
hot or warm here and this is perfectly
OK for performance because we have high
throughput and low latency but it's very
bad for cost because these tears are
provisioned for peak performance and
therefore cost per gigabyte is really
high so instead if you want to improve
this we would like to have a cold tier
that is low-cost enough to store
archival data in compete with tape but
also low latency enough to store called
data at the same time and that's what
Pelican does so pelican is a building
block to build such it here and if we
compare it to tape it has a cost which
is comparable and but offers better
performance because it's a disk-based
appliance and to reduce costs the key is
to write provision resources within iraq
and by that i mean that we need to
provision just enough resources to
enable efficient work load operation but
no more so for example instead of using
commodity drives Pelican is designed for
archival and SMR dry
that are specifically targeted at on
frequent access we provision just enough
power cooling and bandwidth in Iraq just
enough to satisfy the requirements of
the workload but not for peak
performance so for example we don't have
enough resources even to have all the
disks active within Iraq for compute we
just have enough for efficient data
management and no more so we remove a
lot of servers out of the rack and we
end up with just two and because they
are just two servers we can directly
connect them to the data center and
remove the top of rocks which as well so
intuitively what we did here is that we
moved away from this very partitioned
traditional rack design to something
which in which the resources are
converged and provisioned just for the
requirements at the scale of the rack
and so this allows us to increase
density and reduce costs because we
remove unnecessary hardware from the
enclosure and also because we because
the dirac is designed just for the
requirements of the workload the
performance is kept and therefore the
power consumption is lower and the
operational cost is reduced as well so
that's how Pelican looks like this is
one of the earliest prototypes we have
it's a converged design in which power
cooling mechanical storage and software
are all could design together for the
cold data were called it has eleven
hundred and fifty two disks in a 52 you
rack which is a standard rack in data
centers and the density that we obtain
is much higher than in traditional disk
based storage in this particular
prototype because it has it uses for
terabyte drives is able to store / 5
petabytes of raw storage within a single
rack and the key of the design and the
insist on that is that resources are
right provisioned for the workload and
in very practically this means that at
most
eight percent of the disks in Iraq can
be active concurrently and doing that we
reduce the total cost of ownership and
make it comparable to tape but because
it's a disk-based appliance the latency
is lower however right provisioning
induces constraints on resources within
Iraq and so because not all the sets of
this can be active at any given time and
therefore the software stack needs to be
specialized to handle these resource
constraints at the scale of the rack and
in particular there are two key
mechanisms within the storage stack that
needs to be constrained where this is
the data placement that determines on
which disks the data will be stored and
the i/o scheduling that determines in
which order requests on the data will be
serviced and the design of these two
components is my first contribution to
this project but let's first look at the
impact of right provisioning on
resources so if you think about a fully
provisioned system well all the disks
are active at all the time so there are
no resource constraints and in contrast
in the right provision system each disk
is part of a domain for each resource it
consumes so for example here the disk D
is part of a vertical cooling domain and
horizontal power domain and the key here
is that each domain has only a limited
set of resources for all of the disks it
contains and so a disk can only be
active if there is enough resources
across all of the domains that it is
part of for all the resources and in
this example there are only two domains
but in pelican we have four we have
power cooling vibration and bandwidth
and in a general right provision system
you could imagine having an arbitrary
number of domains depending on the
resources that requires and this domains
in pelican have induced resource
constraints so we have for example only
two disks that can be active out of 16 /
power domain
one active out of 12 / cooling domain
and one out of two / vibration domain
and these listed here are hard
constraints meaning that if we violate
them something bad can happen to the
hardware or to the data so for example
if we were braid if we am violate the
power constraint basically a fuse will
trip with in Iraq leading to 16 disks
being unavailable for a period of time
and can even lead to increased failure
rate for these discs because they have
not been spun down cleanly when the
power went down and so these are hard
constraints but there are also soft
constraints like bandwidth which if we
violate it we only have performance
degradation and so for both hard and
soft constraints the software needs to
enforce these constraints and operate
within them efficiently and the first
component for the software is the data
placement so in Pelican we store blobs
that are basically large chunks of data
and we erase your encode them and this
means that we fragment the blob into a
set of smaller fragments and we add some
redundancy blocks redundancy fragments
to this data and the erasure encoding
guarantees that if we store each
fragment on a separate disk we can
survive several disk failures because we
have some redundancy and we are able to
reconstruct the blob and for in Pelican
for performance reasons we also enforce
that all the disks that store a blob
need to be concurrently active to access
the data and so here for example we have
one blob that is for as an example
stored on three randomly selected black
disks and we can see the impact on the
cooling and power domains that this
decision has now in a fully provisioned
system we don't have this impact because
we don't have domains there are no
constraints and so we can basically
select any random sets of disks for any
blob and this has no impact on
concurrency now in a right provision
system we cannot spin up any
sets of disks concurrently because some
sets have conflicting resource
requirements so and because we enforce
that sets of discs need to be
concurrently active to access the data
it means that if we randomly select two
blobs to read if their sets conflict we
won't be able to access these blobs
concurrently and so we end up having to
serialize the access and so the
challenge here intuitively that
placement has an impact on concurrency
and we need to minimize that for any
randomly selected blobs the probability
of conflict well that's that's what we
need to minimize and this is not a
simple this is not so simple to do and
to show you that let's think of a very
simple random approach for the placement
and see why it doesn't work well so for
random placement let's assume we
selected the disks of blob one randomly
and now we need to select disks for a
second blob to be stored and so every
time we will pick a disk at random for
blob to the probability that there will
be a conflict with the discs of lob one
grows with the number of disks one say n
+ 33 in that in that example and now as
we go further the probability increases
and eventually the with high probability
will select a disc that will conflict
with one of the disks of blob one
meaning that will need to have
sequential access for blob one and blob
two and so if you think about it in the
general case if we store up each blob
over n disks the probability that two
random blobs will conflict will be
growing in way with n square and so this
practically means that if we select a
wide enough erasure coding scheme like
something like 10 or 15 fragments / blob
we end up with a very high probability
that all the requests have to be
serialized so in Pelican we are doing
better and we are basing the placement
on the intuition that
it's much more efficient to concentrate
all the resource conflicts across over a
few sets of disks leaving them
completely independent from the rest of
the rack so in Pelican we partition
discs into groups such that each all the
disks in the same group are able to
concurrently spin up and we are aiming
to enforce the property that to randomly
selected groups are either fully
conflicting for all of their disks or
are completely independent and I'm going
to now show how we can build such groups
yeah are you saying you're optimizing
for average page retrieval time that the
expenses and very bad worst case or did
I misunderstand me know so what I put on
the conflicts in one place we mean
that's the data that's stored there
could have could be fully serializing
what I have a very long trip yes but the
idea is that basically we are like a
partial conflict if only one disks in
among to set is conflicting is as bad as
if they are all conflicting so it's
better to have um yes you see the point
right yeah maybe I don't understand is
originally I thought you said I'm going
to split my blog among 11 disks but I
only need 10 of them to be alive in
order to reconstruct the blob yes okay
so one isn't bad at all the thing is
that we are spinning up all of them
because we need to perform some checks
to see whether the data is still valid
so we spin them when we access data we
spin them all up but in some sense of
you you've over you've added some error
correction but you don't need all of it
oh no but in the erasure coding you have
relatively small number of array of
redundancy so you still need a large
majority of the disks to be active to
access the data well you don't need them
all but in our design we are accessing
the mall because we're really careful
about whether they're basically I'm
scrubbing on the disk and seeing
the data is still got it but but you're
right we don't need them all but we
still need a large majority so for
example we're using like 15 plus 3 and
we need at least 15 to be active that
means it turns the hard constraint into
a soft constraint because it's if only
to present at the time you don't do you
fuller oh yeah yes this is true but this
complicates the scheduling because you
need to to use an off-the-shelf standard
coding scheme could there exists an
optimized code in seeing the students
particular constraints in your system
somebody's our research is unit this is
uh so we use something of the shelf but
they possibly oh yeah so we use
something we need to erasure encode at
streamline we need to be really fast or
even something that is proving to be
efficient and fast yeah so yes so how
can we form these groups well if here's
the side view of the rack and if we
select one of the one discs on bottom
corner we can see how it uses a power
domain and cooling domain now if I
select one disc up in the diagonal we
can see that it shares no power and no
cooling domain and therefore there is no
possibility of conflict and therefore
these two discs can be spun up
concurrently now if we take all the
disks in this diagonal we can see this
is true for all of them so all these
discs can be concurrently spun up and
therefore this black diagonal represents
a well-formed group of 12 disks so if we
don't take the same diagonal but shifted
one disk down we end up with the second
group which is also well formed but more
importantly it has the property that
each disc of the gray group will
conflict with one of the black disks
meaning that we completely concentrate
all the conflicts between these two
groups and this also means that these
two groups are independent from the rest
of the rack so you can see that they
share no cooling domain with the rest of
the rack they still some share some some
power domains but remember that in
Pelican we can and we each power domain
can have enough can have two disk
spinning and the for this doesn't
represent a conflict so all the
conflicts are concentrated in this
darker area for these two groups and
this is also true for all the rotations
of this black diagonal meaning that by
forming these diagonals we actually end
up with by having 12 groups that are all
fully conflicting with each other and
are all competing and from the rest of
the rack and this is what we call a
class in Pelican and so if we now in
force that we store each blob in a
particular group only so a blob doesn't
span different groups we see that the
probability of the conflict of two
randomly selected blobs is now in the
order of N and another nice property is
that these groups fully encapsulate all
the constraints which means that at
runtime when we need to spin up disks we
don't need to worry about all the fine
grain constraints for each for each of
them we just need to know in which group
the blob is going to be and we know
automatically all the conflicts so this
simplifies the scheduling significantly
and now we only reason about groups so I
showed you an example with 12 disks but
the in telecom we have 24 disks and we
have 48 groups but the same principle
applies here we have classes we have
four of them that are independent and
therefore the concurrency is for if the
request if requests are randomly
distributed across classes sorry and in
pelican a blob is stored over 18 disks
and this is due to the erasure coding
scheme we use and that has been picked
to fit with the workload requirements in
terms of durability and throughput as
well
so for so this was the data placement
for the scheduler key the key constraint
the key challenge is that it takes a lot
of time to spin up a group so from what
we measured this was quite a while ago
but basically it takes around 14 seconds
to spin up a group and this includes
spinning up all the 24 disks mount their
file systems and perform various checks
before doing I oh so if we service
requests over one class we can only have
one group in that class out of 12 active
at one time and this means that if we
have a queue of requests on that class
and we've process it in 50 order with
relatively high probability will end up
requiring to spin up a new group after
each request and this is really bad for
a utilization because basically we'll
spend our time doing spin ups so instead
of doing that we are actually batching
requests per per group and this is done
to perform a decent amount of i/o once a
group is up but the challenge with
batching is that implicitly it it it
triggers some reordering compared to the
five order because some requests and the
batch will be processed ahead of their
five order and this can lead to
unfairness and so in Pelican we bound
the batching time the amount of batching
we do to ensure some fairness and this
is the trade-off between throughput and
fairness and this is a user-specified
parameter finally the scheduler can
handle different types of traffic by
weighted fair sharing so for example for
rebuild and client that can happen
concurrently right so to evaluate a
pelican we to evaluate the impact that
the right provisioning has we evaluate
pelican to a system in which all the
disks are up simultaneously and this is
a fully provisioned system because there
is enough resources for all and we call
it FP for fully provisioned because this
is really
to build because it requires a lot of
resources and a lot of cooling within
the same rack we actually use a fine
grain discrete-event simulator the to
compare both systems and we first cross
validate the simulator with our existing
Pelican hardware and it's a simulator
design and implementation is my second
contribution to this to this project so
I'm going to present three matrix the
rug throughput the latency and the power
consumption for these two systems and we
used an open loop workload basically
it's a Poisson arrival process and we'd
perform a parameter sweep across a range
of different workload intensities by
varying the lambda of the personal
arrival and we were very the request
rate up to 8 requests per seconds now
that's that may seem small but because
the requests are over 1 gig blobs this
actually represents 64 gigabits per
second of workload so first for the
throughput we have the average
throughput in function of the workload
rate so this is lightweight workload and
this is a very heavy weight workload and
we first present the results for the
random placement I was describing
earlier so basically the it's the same
pelican rack but we replace the Pelican
data placement with the random one and
we can see that regardless of the
workload it performs extremely badly and
this is because we end up having a lot
of conflicts between requests and and so
we basically end up processing them
sequentially now if we look at FP we
have the inverse performance because
it's extremely good because everything
is provisioned for peak and in fact the
only bottleneck is the bandwidth out of
the rack this is 40 gigabits per second
and it is saturated when the workload is
intense enough now let's have a look at
Pelican so for Pelican we can see that
we are able to handle right provisioning
extremely well because up to four
requests per second the performance is
actually identical to FP and this is
despite the fact that only eight percent
of the disks in Iraq are active
and this also illustrates the importance
of designing the hardware and the
software together because if we don't we
end up with performance like this if we
don't take if we don't manage
constraints efficiently at runtime now
of course after a certain point Pelican
plateaus and its performance is below FB
yeah it's so how long is the cue the you
inspect I think it's 500 500 requests so
we yes so yes but you're right this is
fairness dependent so here we can see
that we can see the impact of spinning
up groups basically that is captured by
this metric and we can reduce the
spinning the number of spinning ups by
increasing the budget but overall
Pelican performance extremely well for
that for that metric now for time to
first byte you need to remember that in
Pelican we inherently by design need to
spin up groups to process requests and
we can see that even for the very
lightweight workload the minimum latency
we see time to first byte is basically
the time it takes to spin up a group but
this is okay because we are targeting
all data workloads that don't have such
a low latency requirement and therefore
this is a reasonable time to wait and
also it's significantly lower than they
of course if P doesn't have this
constraint of spinning up disks and so
it has significantly lower time to first
byte but this is coming at the expense
of having all the disks active in the
system and so if we look at the power
consumption this is the aggregate power
grow in kilowatts for all the disks in
Iraq and to have a baseline this is what
all disc spun down would consume we
disks that are spun down still consume
some some power because their
electronics are up and so this is 1.8
kilowatts now if we look at all disks
active we can see that it's
significantly higher it's around 11
kilowatts just for a rack this is very
high for a single rack and this is by
the way what FP would consume now if you
look at Pelican we can see that it is
significantly closer to 0 disk spun down
than all disks active and we can see
that the power draw actually varies with
the workload rate because when the
workload rate is low most of the groups
are basically spun down but then when
the workload is high all of them are
active because we're doing extensive
batching and in the middle here we have
a lot of disk spinning spinning up and
spinning down and spinning up a disk
takes more power that's why we see this
one but overall even for peak
performance where nicely capped at 3.7
kilowatts which represents around 3x
lower than FP so to summarize in front
in this Pelican project we isolated some
principles on how resources should be
provisioned for the work load
requirements and managed dynamically at
runtime we have a pelican prototype
deployed in a data center unfortunately
it's a public lecture so it can't really
talk about how this is going by it's
going well and and and then finally we
ended up having a very generic simulator
for Iraq scale systems and this
simulator not only simulates performance
but also resource consumption so you can
specify an input Iraq description
describing all the resource constraints
in Iraq and it will be able to
instantiate that in the simulator and
compare it against several different
workloads and it's fine it's fine grain
it's cross-validated against pelican and
because it's so flexible it is now used
across several projects in the systems
and networking group and these two bits
are my contribution to the Pelican
project so in a second part we're going
to talk
networking so this is slightly different
context than pelican because we're not
looking at storage racks but at high
density compute racks so we're talking
about hundreds of servers within a
single rack now in a traditional rack
today in compute track you have
something like 40 servers so we're
talking about a significant increase in
density but this is not an unreasonable
assumption because over the past few
years several vendors have released
products and actually managed to achieve
that kind of density with existing
technology so we end up with hundreds of
servers per rack and the reason the
commonality between these designs is
that they aggressively leverage hardware
integration to reduce costs so they for
example remove all the cabling from the
enclosure and then the the most of the
interconnect is done on printed circuit
board they use integration on the
silicon and this all aggressively
reduces costs and in particular one
technology that allows to reduce costs
is what is called the system on a chip
the system on a chip is basically a
piece of silicon with a CPU and a lot of
additional features like IO and memory
controllers and networking so for
example the Boston varieties uses a chip
form from a company called canxida that
is an arm CPU collocated with a 10
gigabit packet switch with eight ports
all within the same silicon so we assume
that we have a packet switch a low very
low cost packet switch within each
server and now the way the networking is
scaled is that because we have hundreds
of servers it's really hard to have a
single top-of-rack switch with so many
ports at high bandwidth so what these
systems do is that they typically remove
the top of rack switch and they directly
connect the small packet switches
together forming what is called a
distributed fabric so we move from the
traditional top of rocks which based
design in the rack to something which
looks like this so here is for example a
3d tourist apology each vertex is a sock
and is connected to a set of
socks six in that case with in Iraq and
and if we if it wants to talk to other
socks in Iraq it needs to do multi-hop
routing within this topology and these
3d tourists apologies for example what
the indc micro uses so this is all great
for a cost because we can now scale the
network to hundreds of service in Iraq
but it has a challenge and the challenge
is that this design inherently lacks a
flexibility because if you think about
it the old top of rocks which design is
very flexible because the top of rocks
which is fully provisioned meaning that
all the servers can send and receive
data the full line rate within saturate
without saturating the switch now this
is no longer the case with these direct
connect apologies meaning that a server
cannot send and receive data on all of
its ports at full line rate anymore and
this happens because the topology
defines fundamental properties like
average path lengths and bisection
bandwidth and so then this network is
limited by these properties in the
topology and because the topology is
static we can't really change this this
property's once we build the system and
we know that the best apology is work
load dependent so when workloads are
different the topology behaves better or
worse and this is a big challenge and
for example HP in this in its its system
has four different separated fabrics
with four different topologies for four
different workloads and this is
obviously not not optimal so to address
that challenge we designed X fabric in
which the topology can be dynamically
changed at runtime at the physical level
and for that we use we design X fabric
as a packet-switched Network running on
top of a physical circuit-switched
apology so let me describe it a bit
better so in Pelican Pelican in the X
fabric we need to building blocks first
we need the socks with the packet
as in a traditional distributed fabric
but then we'll also need a set of cross
point switches and a cross points which
is a very simple component that has n
ports and can set arbitrary circuits
among pairs of ports and when traffic
comes in on one port it is it is
forwarded through the circuit directly
to the outgoing part so for example here
is a schematic representation of a cross
points which with n ports connected to a
set of socks and because there are no
circuits established basically there is
no connectivity on the socks because the
signal is not forwarded but now we can
establish circuits and now the
corresponding socks gets connected at
the physical level because the
crosspoint the switch starts to forward
the signal and it's really as if the
cable was plugged in between the socks
because the only thing that the cross
points which does is forward the signal
it does no queuing no packet inspection
it doesn't know what a packet is and so
it's very similar to a cable and the
latency is very low and bounded so it's
for the for the components we use it
around to nano seconds for the
forwarding and now of course we can
change the circuit configuration and the
physical connectivity between the socks
is changing so we can build topologies
using that technique so if here's a
simple example of a topology that is
instantiated by circuit configuration on
the switch and now we can change the
circuit configuration and change the
topology at runtime and the good news is
that this cross points which is our
commodity parts that are available as
single silicon chips so we can buy today
160 port cross points which at 12 gig
12.5 gigabits per second per port and
that's all within 45 millimeter 45
square millimeter form factor and the
cost for that
is extremely low because this component
is very simple so it's around three
dollars per port for a 12.5 gig which is
significantly lower with the battle then
what the packets which are the same rate
would cost and vendors ensure that they
can build up to around 300 ports at 16
gigabits per second without changing
their manufacturing process
significantly so that's that's all great
we can build this reconfigurable
topologies but how much does it cost how
many chips do we actually need well
let's take a simple example say we have
a rack with 300 socks and six ports per
sock how many crosspoint switches do we
need well let's first think that we
could have full reconfigure ability
meaning that all the ports of all the
socks can be directly connected by a
circuit now how can we do that the
simplest thing to do would be to have a
single crosspoint switch and connect all
the ports to it and then you could
establish any circuits within that
single cross points which but can we do
that actually a tracks can we can't
really do it because if we look at the
scale that requires around 1,800 ports
on a single cross point and this is way
too high of what a single crosspoint can
provide so can we enable full
reconfigure ability attract scale well
we can we can build it using multiple
cross point switches and compose them in
what is called the folded clothes
topology but the problem is that while
this is offering the same abstraction as
a single chip if you want a fully
non-blocking folded bus topology we will
require a significant number of ports
because there are additional ports for
internal connectivity this bit here and
so we would need something like 3300
port chips so this is a significant
overhead in power and cost for the rack
so we assume that the full reconfigure
ability is actually way too expensive so
in fact in X fabric what we do is we
trade off a bit of frequency
our ability to significantly reduce
costs so instead of having any two ports
that can be connected by a circuit we
just ensure that each port can be
connected to any other sock in the in
Iraq but not necessarily on the port on
any port so it's a slightly less
reconfigure ability but it buys us that
we can now reduce the number of chips so
if we connect each sock to a different
cross point on each of its ports we end
up with having in this example only six
crosspoint switches and the only ports
on the same cross points which can be
connected so all the socks can still be
yes also on the dimension trap to delete
on the diagram on the left there see
what's the effect on having notable it's
go through multiple chips is that
significant well so the cost of the
latency is very low in propagation so
it's not too much it so as I said it's
it like to nano seconds plus the time to
actually reach the chip it's it's a
handful of nanoseconds it's not
significantly because the chips or
powers oh yeah and then I think probably
the power because you have an enclosure
yeah yeah so in X fabric we with this
design we only need it for this example
6 crosspoint chips with 300 ports for
the entire rack and this represents a 5
X reduction in both cost and power while
trading off just a bit of flexibility
and so injured in the general case for n
socks and deport Spurs up we would need
d crosspoint chips with n port this is
the architecture is actually quite
simple so we have our the required
number of crosspoint chips we have the
socks and then each sock is connected to
every cross point so this is the example
for sock 1 and this is for sukkah too
but you need to imagine that all of them
are connected to the cross points and as
I said we would need d cross points and
imports / cross point and
socks with the with deportes but now we
need to actually manage this dynamically
we need to establish circuits to
instantiate topologies when this is done
by a controller the controller operates
a track scale it runs it's a process
that runs on one of the socks it's a it
receives periodic updates about traffic
statistics from all the servers within
the rack and it builds a traffic matrix
within the rack using these statistics
and then periodically it generates a
topology that is optimized for that
traffic matrix and for that it uses a
fast apology generation algorithm that
is able to handle partial reconfigure
ability so we manage the cost of the
complexity of having partial reconfigure
ability in software so the generation
algorithm produces only topologies that
can be instantiated with the given
hardware and it's a fast greedy
algorithm because we need to reconfigure
on rail relatively frequently in the
order of seconds or something like that
and once the topology has been computed
the controller actually updates the
circuit configuration on cross points
using a control plane and it also needs
to update the forwarding tables on the
socks because we need to forward packets
over the new topology so this is done on
a periodic basis so to evaluate this
design we use a simulator and a
prototype but due to time constraints
I'm going only going to show results for
the prototype so the prototype is
composed of 27 servers and 6th circuit
switches to which every server is
connected so each server has multiple
network interfaces and each of them is
connected to a different cross points
which and we need servers because we
don't have socks yet so each server
emulates the functionality of the sock
and in particular it acts as a software
packet switch so it receives packets on
one interface
and according to the destination send
them on a different interface for
forwarding the server support on
modified applications they support
tcp/ip that is forwarded from multiple
hops and for the circuits which is we
have a nice design by Daniel clatter ho
that uses a non-blocking crosspoint
commodity ASIC at one gigabits per port
and so I am responsible for most of the
software design in the system and Soto
for the evaluation we have a flow level
trace of a production workload and we
use that to generate a synthetic trace
that is then replayed on the servers so
when the server replace the trace it
creates TCP flows to different
destinations according to the trace and
this DCP flows yeah with the service use
TCP IP and we compare the other we
measure the average path lengths in the
completion time and so one of the
interesting one of the things we wanted
to know is how fast shall we reconfigure
to get benefit so if we measure the past
links in function of the reconfiguration
period on the controller and we compare
it to the 3d Taurus we can see that as
we get more frequent reconfigurations
the path length actually drops and this
is because the topology is more
aggressively targeting the workload and
matches the requirements so flows become
shorter yes from how long is the cross
point down while you reconfiguration
it's so one benefit of the electrical by
closed circuit switching this is very
fast so it's like 70 90 seconds or
something 70 nanoseconds yeah it's it's
it's all done within the chippies yeah
yeah and we can see that well obviously
the 3d torus is not changing because
it's a static topology but interestingly
that even at this small scale we have
benefit having this reconfiguration and
now let's have a look at the completion
time
we normalize it to the 3d Taurus and we
for the same experiment and you can see
that as we reconfigure more and more
often the completion time actually drops
and this is because each TCP flow now is
an average shorter it uses less network
resources and therefore the good boot of
the network is increasing and this
happens up to a certain point if we
reconfigure like once per second we have
something like twenty three percent
improvement but then interestingly it
increases again if we reconfigure every
100 milliseconds and this is happening
because every time we reconfigure we can
lose packets and TCP is very bad at
handling lost packets because it assumes
it's it's a congestion so it drops the
throughput and we can see that yeah the
best we achieved for this experiment is
a second per second and configure a tree
configuration where we have to
twenty-three percent improvement
compared to the three terms now this
might seem not very big but actually
it's it's it's a positive result because
the scale is relatively small and then
therefore the torus behaves relatively
well but as we increase the scale the
torus increasingly requires more hops so
a track scale we actually see in the
simulation about 5x improvement compared
to the X fabric for the same workload so
yes so I presented these two projects
that in which basically I try to
illustrate the benefits and the
challenges of highly of designing things
attract scale this rack scale computer
approach so I did that first for storage
with Pelican and then for networking
with X fabric and for bus project we see
that designing systems attract scale has
significant benefits but it also has
additional challenges and this
additional complexity
is can be very well addressed in
software so the lessons learned I can
mention that basically right
provisioning the resources for the
workload is offers very good performance
per dollar for certain workloads and but
in some cases you also need to
dynamically adapt at runtime and this is
better done with as less as as less
complexity as possible in software in
Hardware sorry and as much as possible
pushing it to the software and so we are
currently doing a another project which
is called Falcon it's about storage but
it's a lot cold storage like Pelican
it's a more generic storage system and
here we try to merge the lessons learned
for the most projects so we inject some
at very low cost ins with simple
hardware will inject some reconfigure
ability within the system and this is a
prototype that it sits on the third
floor so if you're interested I can show
you today tomorrow and with Falcon we
circuits which disk well circuits which
storage we circuits which SSD is
currently so this is a 120 SSDs this is
one circuit switch and this is designed
by Hugh Williams and and now instead you
have some servers you don't see and now
instead of having direct attached
storage on each of the servers you can
attach each SSD to an arbitrary server
within Iraq and then flexibly adapting
to the demand both in load balancing and
scaling the compute to storage ratio
thank
so in a couple of points you had to you
know you chose maybe a greedy algorithm
because you needed to be fast or a
static layout of classes and so on and
there are obviously some kind of big
optimization problems that you could
imagine applying at runtime to further
optimize these things and I think we
discussed up until we've discussed it in
terms of two racks for pelicans so if
you're going to double vision then so
but it but my question is more in
general there are these optimization
problems um so yeah you do you do you
want to look at those do you know where
to go in the lab to help for help with
those subjects so we actually have a
follow-up project or Pelican the disc on
flamingo in which we try to well in
which we did automate the design of this
racks using a solver so we tried with
dead 3 and it was so we would tried for
a long time and we didn't quite succeed
with that three so we wrote a custom
solver but it's now automated yes and
then who was our an improvement um over
some you know the like static baseline
or whatever so if we look at the
provisioning that Pelican Pelican has
the layout that is automatically
generated performs the same way right so
we are the same for Pelican but the
flexibility now is that we can vary the
provisioning very the design and
generate a new storage stuck within a
few hours instead of like a few months
like we did manual in Pelican so huge
improvement yes but not for medical so I
think with the pelicans suck you're
making the assumption that all of the
discs for the erasure code to read back
from the erasure coding all of the discs
or a particular blog have to get active
at the same yarn
if you added some buffering and
potentially took a hit on the latency
and had a more complicated scheduler is
there potential to improve the thing is
that you need to be careful with
buffering because we store potentially
very very large blobs it could be
hundreds of gigabytes so if you start
buffering things you need to be very
careful in bounding the buffering and
it's unclear that you'll be able to do
it in the general case for a arbitrary
science clubs so obviously buffering
would help but we really need to be
careful about that and that's why we did
we didn't okay any more business so I
just one more 14 seconds sounds like a
long time how much of that is the
episode and can use as a profile system
or so I think Richard looked deeply into
that and the numbers we have now are
significantly lower than this maybe
something like 2x better ok well we
actually thought it looked very
carefully at the software costs and we
made some changes to Windows to
eliminate oh I'm getting away those or
no it's dominated by the pitches one at
different answer depending ok more
questions if not let's thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>