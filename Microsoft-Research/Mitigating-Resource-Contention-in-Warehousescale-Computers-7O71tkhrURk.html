<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mitigating Resource Contention in Warehouse-scale Computers | Coder Coacher - Coaching Coders</title><meta content="Mitigating Resource Contention in Warehouse-scale Computers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mitigating Resource Contention in Warehouse-scale Computers</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7O71tkhrURk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
a great pleasure today to introduce link
Tang from the University of Virginia
Linga is getting her PhD finishing her
PhD and she's been working in the area
of improving the performance of data
center or warehouse scale computers she
recently won an award the CTO best paper
award and her work has been recognized
by Google oh she's worked closely with
Google in a number of internship so it's
a great pleasure thank you alright
thanks this is working thanks so my name
is Linda thanks for the introduction to
make it easier for you guys to pronounce
it it's pronounced as ninja but with the
link with the L feel so today I'll be
talking about mitigating memory cells
contention on the commodity multi-core
machines to improve efficiency in the
modern data has a warehouse skill
computers so I also want to share
something sighs and some the discussion
and discuss with you guys about when you
rethink the row of compiler and runtime
system in this kind of emerging
computing domain so let me just first
gave a little bit background I'm sure
you guys a lot of you guys are really
familiar with this warehouse scale data
center the start of the computing domain
that my work is focused on so in the
last couple years especially in the last
couple of years the macro scale data
centers have gained tremendous momentum
as emerged as a very important computing
domain companies such as Microsoft
including Microsoft and Google Facebook
are constructing those mega scale data
centers their hosts large-scale Internet
services such as web search and Mail as
I've shown here and a lot of people
really a very familiar with this kind of
applications and some people are really
addicted with it so as The Economist
magazine points out the data centers
have become very vital for the
functioning of the society nowadays
those kind of data centers very
expensive to build construct and to
operate so typically each of
data center may cost you hundreds of
millions of dollars and the cost and the
size of those data centers have gained I
have grown rapidly in the last few years
so back to a few years ago we were
talking about this kind of data center
we would say oh it's a
football-field-sized building housing
tens of thousands of machines and feel
back a few years later now we're talking
about those buildings as tens of
football field-sized and we're talking
about hundreds of millions of dollars in
the recent later center this one show I
found this picture on the Internet it's
a post new data center that's actually
costing a billion dollars right so those
things are grown really really expensive
nowadays and of course when people spend
a billion dollars building data centers
and they want it to be very efficient so
efficiencies really critical for the
cost so my work mostly focuses on the
memory sauce contention on the commodity
multicolor architecture and which is
very significant limiting factor for the
efficiency in the warehouse skill
computers and I'm going to explain why
so those are the server racks that
populate the data centers and if you
look at the machines we use in those
kind of data centers which typically is
commodity multi-core machines so on
those kinds of machines we have multiple
processing costs share part of the
memory subsystem right so each cow here
has its private l1 cache but they shared
l2 cache memory controller and bus
penguins to the memory so if you have
multiple applications running kind of
running on those kind of architectures
on multiple applications threats they
may content for some of the memory
resource for example when applications
that could evict the data of another
application threat out of the ship cash
right causing significant performance
degradation so in addition to
significant performance degradation this
memory resource contention on this kind
of multi-core machines also limits
server utilization and I'm going to
explain why in the just next couple of
slides but I want to point out here that
one percent improvement in either in
terms of application performance or
utilization in a massive scale of data
center translates to millions of dollars
potentially saved
it's really important all right so let
me first let me a further illustrate the
impact of memory source contention on
both performance and utilization so
here's a really you know simplified
model we have two applications and we
have two options of how we run those
applications right first week right you
say each application gets its own
dedicated server as shown here so we
have two machines running two
applications each application may or may
not occupied all the processing costs of
this machine so you have low utilization
of those machines but you get you know
peak performance they are the solutions
say that's allowed colocation so we
reduce we improve the utilization of the
server but we reduce the number from
machine we need but we actually
potentially have significant performance
degradation because of the memory
resource contention right so things gets
a little trickier when you have latency
sensitive applications such as web
search or mail those are application
that a user facing you care about so
latency you care about users keeps
experience at all so you don't want to
delete you don't want to actually
sacrifice a lot of the quality of
service performance of those
applications right so you care about
those and their potential hyper a low
priority application in this data
centers such as batch applications that
you don't care that much about but would
you really care about latency sensitive
application so those if those
application cannot really deliver the
except of Kos when we actually co-locate
those applications or other applications
then we don't want to do this kind of
colocation right so collocations often
disallowed for latency sensitive
application so typical approaches we
choose to actually disallow colocation
and have sacrificed utilization for
performance so just to show you the low
utilization is pretty common and it's
actually a one of the big challenge for
data centers here's the graph from HP
data center production data center so
one of the key feature of this graph
that you observe ation you can see from
this graph is we're spending sixty
percent of the time or even more than
sixty percent of time running at ten
percent of server utilization so this is
quite low data has google has a select
any better data so the official data is
there
thirty percent of utilization I was
talking to the IBM fellow and they were
saying the IBM data center production
data than the x86 around eight to twelve
percent average utilization so those are
really low number so why is low
utilization expensive if you look at the
total cost of ownership of a datacenter
server purchase new server is a huge
chunk of this cost and then on top of
that constructing data centers is a big
chunk 2 and then the power right so if
you actually have low utilization low
utilization meaning given same amount of
work you want more machines to around
those work you want a bigger data center
so those are very costly so you want to
improve utilization in the data center
so therefore mitigating contention is
really critical for improving both
performance go ahead understand it a
little better ok is it wouldn't be fair
to say that the server costs must be
curry costs mm-hmm because you know
servers keep failing and then you keep
buying more and more servers whereas the
other stuff hmm not excluding the power
link that data center oh that's
basically a fixed cost oh this is
actually total cost of ownership and
you'll break down so assuming you have
the server for four or five years and
then you amortize ticket costs all right
so the big picture of the work that I'm
going to present here in my dissertation
work first we want to understand the
interaction between the data center
applications and underlying memory
subsystem and on top of that based on
understanding and insights we want to
actually manage result sharing between
application threats so it can improve
the application performance also we want
to manipulate the application
characteristics so a crow actually
facilitate more colocation and work
consolidation on those kind of servers
to improve utilization so some of the
results I have the publication in those
domains so I'm going to briefly talk
about those two and then focus most of
work on this this part of the the work i
also have other work related to the data
center how we improve data center
efficiency in data center tomorrow
Jason's going to talk a lot about those
very interesting work you're going to
hear a lot of them um so the outline of
the top I've discussed the motivation
and
go of the work so I'm going to talk a
little bit of the characterization of
those applications and how they interact
with memory subsystem first so let's
take a dip a deeper look at the memory
sauce sharing for those kinds of
architectures so this is a intel xeon
clover town this is actually widely
deployed in google production data
center so on this kind of machine we
have two sockets and where each stuff
you would have for processing costs and
two calls are sharing a last level cache
for cross on their sake are sharing a
bus and then to sell kids are sharing a
memory controller so you have three
major components of sharing on this kind
of architecture it's one thing to inter
when interesting to notice this the
sharing would potentially have a
constructive or destructive performance
impact if application if threats
actually belong to the same application
and share data and it fits nicely to the
 cash then you basically prefetch
for each other and you're reducing the
coherence traffic and a barrage traffic
so it's a good thing but potentially
they will also content for those
resource or when that happens is a
destructive kind of a performance impact
for those applications another
interesting thing is the threat to calm
mapping determines which resources
shared among multiple threats in this
simple example those two threads are
sharing those three component resources
so if i move the threat to another call
I'm only sharing memory but bus and
memory controller if you move the threat
with another socket we're only share
memory controller here so it's a really
interesting feature of those kind of
architecture so the research question of
the first this part of the talk is
what's the performance impact of those
threat to call mapping and the memory
sauce sharing for the application
performance for emerging large-scale
performance that we care about data
center performance so some of the prior
work people are being using spec and
they conclude their significant
performance degradation due to memory
resource contention there's a lot of
work in the hardware community based on
but there's a recent work in p pop 2010
that concludes the captioning does not
matter for the contemporary
multi-threaded applications so our
question is you know what what about the
large is going emerging data center that
we care about what about the
missing pack for those applications so
let me show you some of the key results
that we found on there are more results
and details of those applications in my
publication so you could we could talk
offline so this is a trap location
sharing meaning which we're interesting
finding out they are sharing among
threads that belong to the same
application so here are the three
large-scale application we use including
web search in BigTable content analyzer
since it's a semantic analyzer so in
each case each application we run it on
the preview or show the clover town
machines in three different
configuration sharing configuration so
from the left to right you can actually
think as more and more sharing among
threat to the bunch of the same
application so the y-axis is the
normalized to performance so one thing
to notice gears there's a big
performance swing up to twenty two
percent for those applications for
example semantic analyzer degree for
more sharing indicating there's
contention amount threats on the
performance suffocates as observers from
the contention and web search as well so
but there's also an interesting thing to
to see from this graph that is a post
constructive and destructive impact
right because if you look at content
analyze and web search the degrading
from a cache contention byways
contention but big table actually
benefits a lot from the car sharing go
ahead make sure Anderson so you care
about single machine performance
ignoring potential contention on the
network yeah this is really just single
machine microarchitecture resource
contention so we're not looking at menlo
contention and for example the web
search we're looking like leaf node
performance so this is a request comes
in and it's completely executed on that
machine yeah doesn't make any remote
calls no angels yeah so basically we use
our queries of a real what queries a
trace of World War queries than we
actually fed this queries to this
machine so then you've no sense of how
much this impact actually translate to
the end
latency which is what the user sees um I
don't know I don't have the exact number
of how it translates to reel in the
complete latency I think for web search
latency on the real machine on the
backend machine does not it is it's not
going to be the major dominant part for
the latency for for web search because
the network latency is so big that your
Yorker latency inning a single server
might not really matter that much but
the the key for the back end is really
really want to improve performance to
actually reduce costs right so it's not
so much about performance it's a lot but
cost but we actually we've done that or
would you care about the latency of
those queries morelos machines it
doesn't want the best machines for those
curries a best performance for those
queries on those machines oh all right
so so there's a constructive and
destructive impact for both application
for three applications and then we look
at inter application sharing so
basically what's the impact of hearing
among application so if you in this case
where we actually co-locate another
application with multiple thoughts on to
the same architecture what's the
performance impact so here the results
of each graph is is a single application
countin analyze the three different kind
of running scenarios running along of
thinner already running with another
stitcher is image processing for Street
View and another benchmark so you can
see that performance there's performance
degradation from contention because the
performance and basically dropped from
running alone to running with other
applications another really interesting
about this graph that I want to point
out is the optimal surgical mapping
actually changes when they're coronet
changes right so if you think about
content analyzer one is running alone
its best performance is running actually
spreads threats across their two sockets
but runaway stretcher makes it actually
benefit from cluster or its source
threats together and similar for web
search it changes in different settings
and in this case it's preferred to
actually share cash with own threads but
not share penguins on threats
bigtable always benefit from the sharing
and it's also interesting to notice that
this difference between three-car
mapping performance could be pretty
significant it's up to forty percent for
big table in this case right so a quick
summary I have a lot of state more data
about this kind of study in the
publication that I don't have time to
actually show here but I want to
highlight some of the things we found so
a contrary to the prior work we found
that the memory sauce sharing actually
really matters a lot for a large-scale
data center applications and the
performance wings up to forty percent
simply based on different threat to call
mapping and the interesting thing is
optimal mapping changes when application
changes when a Carano changes when
architecture changes um and I highlight
those two is because basically what
they're saying is there is a up to forty
percent performance opportunity that we
left on the table without thinking about
electric our mapping right without
thinking about this interaction between
the application and the
microarchitecture and so it really the
next question will ask is how do we
achieve the optimum a threat to combat
be all this application that you have
mentioned do they involve any
synchronization
uh yes but not so much it's of course
Inca like a synchronization in those
kind of applications but it's not a huge
amount of synchronization bounded
applications
um and that would be really interesting
to see you know lock contention kind of
that kind of performance impact on those
application but we're only focusing our
memories as contention on the
supplication so you know you say that
these are sort of large-scale that's not
applications but then you look at the
single box right performance right so
what is really different about these
applications than some other
applications that would not run a large
scale in a data center is it is it the
workloads are different the matter the
memory access patterns are different
that really exists now interesting
compared to log data center application
there's actually a publication that
coming out of Microsoft actually really
are those multiple application comparing
those workloads with spec in terms of
you know different how they access a
different kind of a functional unit or
the eyepiece a difference in memory
access different patterns difference and
I think you will find there's a quite a
quite a difference between those
applications between spec and there's
new work coming out thinking about how
to use this different characteristics to
actually design new servers
architectures that's not really you know
for for example instruction cache is
really important for those kind of
application which is kind of different
for spec so there's a lot of difference
but we could talk offline about it so
how do we achieve this optimal threat to
comment mapping for better performance
so I'm going to just talk really briefly
about this section so we basically
manage the resource sharing by in doing
intelligence illogical mapping of those
application architecture to improve
performance so there are two approaches
that would present in a paper on when is
a heuristic basic poach and the other is
adaptive poach and so the heuristic
based approach the key idea is that's
actually figure out the applications
characteristics and map the threads
according to the applications
characteristics and the Machine memory
topology so what kind of cash sharing
this machine actually has for example
and we identify three important
application characteristics including
how the amount of data sharing among
threats in this application how much
cash usage and bandwidth usage yeah
at this thing depends on the coroner so
how can you make this decision in
isolation yeah that's a good question
I'm going to get to it in there so um
the Harris based approach is basically
saying let's say you have a application
and you have a co runner the key idea is
to say compare the potential contention
within the application versus the
contention between the application and
then you choose the best third to come
back pain and minimize this contention
right so because if you actually have a
lot of data sharing that you potentially
prefer just share data with our own
threads instead of sharing cash with
other applications right if you have a
lot of contention among it so that lets
you have another application coming with
less contention then you potentially
want to share cash with another
application so this is how we actually
decide in mapping so you need to know
the application characteristic of all
corners and you need to know the memory
topology so again the ok for coronary so
you can go around already so this is
fixed for architecture but depends on
the coroner you may go different paths
of deciding how to map so you know the
correct all right you are not choosing
oh yeah yeah that's yeah that's a good
question yeah we're not choosing karana
that's a different research question
we're basically note that we need to
actually manage those to a multiple
applications then we decide how do we
actually threat Oh God so in these these
words warehouse scale computing
scenarios is the stead of corners
borderless static and remains unchanged
over a long period or it changes very
fast they're really long running
applications they so when you're
actually once you decide the quranic
situations they potentially won't change
for days but but they will actually
change right so if you if your program
crashed for example you should do it
either so the job mapper with new tests
coming will decide where the tasks is so
they use basically of being papping at
being packing algorithm and they decided
we were to actually met those
application to the machine in that case
you actually have Carano changes for
once it's running it's going to run
around for a while so and this is to
address your question its architecture
specific and potentially change them
with the corner and we require profiling
so this is not this is doable but it's
not necessarily optimal so we propose
the optimum of the adaptive ttc mapping
and the inside is because of the optimal
mapping changes based on a lot of things
changes Carano changes architecture
changes so why do we just do it online
so the key idea is we have the
competition heuristic we have the
learning phase that way actually
basically goes through the search space
in a smart way and then we pick the best
to threat your comment mapping and then
we execute it for a while so we
basically loop this and then to address
your the question about data center
application their long-running and they
have very steady faces in general but
they they may change coroner's and
different tasks the same binary could be
potentially mapped to different
architectures right so this kind of
approach is more dynamic addressing
those kind of changes in environment so
some of the evaluation for the adaptive
mapping the supposed on real application
on real hardware
those are hot we're seeing production
hardware and the performance is on the
y-axis xss it's a workload the blue bar
is the average rent of mapping so the
green is adaptive and the yellow is the
optimal mapping you could do for those
kind of workloads and the baseline is
the worst-case mapping so basically
where this is definitely better second
even better than worst case you worst
case is not know you don't want to do
worst case basically and we're really
close to the optimum and are beating
rent and napping so on both
architectures I mean are those bar is
significantly do although Realtors
probably variability right between oh
yes so the benchmark year was the way
benchmark rised our application so i
basically make sure it's always
repetitively run the same kind of a the
trace that were using and then the
benchmark around between rounds is one
percent difference so is in the work
load soros AD&amp;amp;D definer is the same but
they're depending on what the requests
are coming in yes you're gonna change
right so to address that we actually so
it basically use really long traces to
queries so it's basically it's kind of
average dial in so we're looking at the
average performance of the screen the
section summary arm we present the
adaptive mapping service to actually
improves the performance for those
applications and I want to highlight the
importance of taking advantage of the
interaction between applications among
applications and between applications in
the microarchitecture that I think this
is where a lot of runtime system will
come in to actually improve the
performance for those kind of emerging
dooming um so I talked a little bit
upper fault about improving performance
I want to switch gear a little bit of
talk about a server utilization so how
do we actually improve server
utilization um so when you think about
improving performance with we manage
application level so application level
manage how they actually share resources
share the common memory resource on
multi-core architectures for improving
server utilization ago is too can we
actually manipulate
applications characteristics using
compliation technique or other
techniques to improve the they are to
reduce the memory contention and improve
the server utilization so here we
propose we present the compilation
technique to a manipulator plication
characters so we can facilitate more
local consolidation on those kind of
missions so here's um I've showed this
graph before but basically I want to
highlight the goal of this work so again
the option one is to actually say this
disallow collocation so each application
is running on a single dedicated server
so you have idle cost for each server
you has really good performance but low
utilization they are the option is
saying we co locate those applications
but we potentially have really
significant performance degradation
because of the memory source contention
and for latency sensitive application we
don't do this because we want to
actually deliver the acceptable quality
of service so the goal of this is to say
that's mitigated contention the low
priority application generates to the
high priority application so the high
priority application the latency
sensitive application couldn't can
deliver the acceptable quality of
service so when we do this we can
facilitate more Safeco location meaning
the colocation when we do this the
adapting the high priority application
delivers satisfactory qos a performance
and this we could actually improve
utilization even if we actually
sacrifice a little bit of the low oops
sorry even if we sacrifice a little bit
low priority applications performance
we're still getting this kind of
utilization right because if you compare
this graph to this graph those part of
the idol cause the previously idle and
we actually basically extract the
utilization from that part of the
architecture so that's the goal of this
part of a work for improving server
utilization so a lot of research
questions that come with how do we do
this how can we actually minimize
mitigate the contention by changing the
applications characteristics so we want
to make the low priority application
less contentious so how do we do this
how do we actually identify can we
identify the coat regions of the low
party application that are really
contentious and how do we actually
change
the the characteristics so compelling
for niceness it's a uh it's our approach
it's a first a compliation approach to
address this kind of challenges for
multiple rounding applications in this
kind of domain so the highlight of this
technique we will pinpoint the
contentious kill regions then we
actually apply novel compliation
techniques to check to manipulate it's
contentious characteristics so reduce
the contention this low priority
application may generate when school
running with other applications so it
actually allow it to really cool run
with high priority applications so some
of the king size before I go into the
details of this work traditionally when
people think about compiler to think
about we're compiling an application is
going to be the single application or
running on this architecture and so
we're focusing on performance a lot but
in this kind of new domain when you have
a lot of workloads you want to
consolidate those workloads on the
server and the number of cost on those
servers is growing with every generation
so you have a big pressure and we can't
really over look this trend of we want
to actually consolidate we're closing
this domain in this kind of jamming
there are more objectives you may need
to think about when you think about
compiler right how aggressive you can
app lication who really aggressively use
memory source and actually been and get
the performance but is it really the
best way to do it what about the
influence this application has to the
running applications for them
traditionally you could imagine having
OS parties yeah just scheduling things
scheduling the one that you know
should contended a little priority why
is that not a good solution in this myth
well so OS has its well i would say
mostly because you want to change the
applications characteristic of the low
priority application and an OS it's not
really tasked with doing that and here
is not really a time-sharing kind of a
situation where traditional when you
think about where you will think about
nice being application time sharing a
resource so you gradually to tone down
the priority of one application
potentially and actually do the time
sharing I don't but here are two
applications are cool running at the
same time they always sent running
simultaneously right arm and also a lot
of challenges comes with how to do this
for OS but I'm going to talk about it a
little bit more when I don't but is it
true that they don't do time sharing at
all click on an individual core and this
where's big risk a warehouse skin
computing scenarios there's definitely
some kind of amount of time sharing but
not so much because if you actually have
your threats or your application less
than the actually available cause then
you don't have to do time sharing for a
lot of situations so nicest is a matter
of really building for the true costs of
and it's an important for my application
if you if you did data center building
how would you bill due to get niceness
into them to the mix don't you built for
mega mega bytes of memory and 40 seconds
of computer time right I'm this work
mostly looking at on the data center
provides internet services right so you
basically have control of those
applications so now yeah you feel
yourself so it's easy to negotiate and
say which application are which
application has higher priority right
and we often do this thing in the data
center but I mean I could imagine if you
do this this may be a wild claim that I
haven't really look into it but if you
if you do this you could actually say
select my priority with this and billing
price right I could I have batch
application you could track me really
cheaply just get it done whatever so
those kind of options one more question
pressure basic assumptions so a lot of
the data centers seem to assume that you
only one job per CPU hey so what are
they doing when they're overloaded when
there's more than one job for CPU that
needs to be done yeah it not due time
sharing it had two good up at the queue
until somebody's finished Mike stairs
that they often to time sharing at this
point and then your performance is going
to degrade a lot and then the operator
is going to have to come in and say we
can't do this yeah so then shouldn't
look cute that sending them to the
yeah they do pink packing algorithm so
they basically application will come
with the subtle resource requirement
saying how many mega by Anwar how many
Scipio time and one and then the map
were the job mapper the classroom
scheduler basically decides how to
actually map those applications if the
if the machine still have available
still has available resources and I'm
going to map this job then if ya so it
seems to me that you know a lot of that
decade you know decade-long research and
is all the topic that they used to make
such a fuss about they are just alone in
warehouse kill computing would I be is
that correct easy I I mean I think OS
people really need to rethink about uh
in this kind of dumbing what kind of
priorities and the optimization criteria
was not quality of service on a single
job its fairness your met and so the
operating systems community optimized
for an important part of the specs and
that is a different now now it's how
fast you get the answer back or
someone's not going to click on your
website
Thanks so another insight is in this
kind of Jemaine when you sacrifice a
little bit for performance of low
priority applications you can facilitate
a safe collocation and wing a lot of
utilization in the end so as in other
terms when you sacrifice a little bit of
your self-interest and performance you
whip you win big in a big picture and as
a life set a lesson it'd be nice is
sometimes good for the big picture and
also in this kind of data centers we
know the application that we're running
their long-running and then we have the
source code available so we call our
system killers compile and here's a
review of this system that we have a
profiler that basically identify the
contentious region of those low price
applications and we also have a compiler
that target those application of those
contentious region and when we compile
those regions to mitigate the contention
nature so I'm going to take a little bit
of the compiler oh the profiler first so
this profiler is based on hardware
performance counters because they are
fairly low overhead that's the idea is
based on how aggressive those
applications using certain memory source
we actually predict the how contentious
II maybe when it's running with other
applications so the applications running
in the profiler based on the the
performance counters actually predict a
contention skull of the execution phase
so we identify the execution phase that
ways hi contention skull and then
associate find the associate coat
regions they have the high conscientious
scum and there's a lot of thinking and a
work that goes into how to actually
build this kind of prediction model and
we use a regression and here's that one
model and like to talk more offline if
you're interested but let's take a look
at the performance of this model here
are the two prior work those are the
work that publish s plus that using last
level cache miss rate and reference rate
actually predict how contentious the
application yes and this is our model so
as you can see we have a much better
linear regression on linear coefficient
correlation um and jobs or what what are
the inputs to your mom right so they put
the input to the model is the
performance counters for example last
level cache licensing of the application
ultralight slicing there's a lot of
reasoning of why we actually select
certain performance counters the output
is contingent skull ray so basically
imagine you have certain lining on
application runs that we actually
calculate skull then we compare this
girl with the average degradation in
this application when running costs
costs to the co runners as they said 15
something different as well as using
multiple inputs to help you predict yeah
right yeah okay okay so is this on a per
region basis I'm not sure I understand
well what the timeline is oh sorry this
is actually application level the
average average application average
cache miss rate for example for the
entire how do you do it by region how do
you define contentious
um I have a graph that I'm not sure if I
have this in the backup slides but um so
basically one is running where every
millisecond simple the performance
counters so you basically get every
minute second scared of contention skull
right and then by region wait we did an
instrumentation to actually find
identifing I so we're basically just
looking for the execution phase that
corresponds to that pod the co-regent
that corresponds to that part of
execution phase which is not do you do a
phase analysis as far as um what we or
did pain will use pain to actually so we
the way we did it is basically so I do
to pass kind of analysis one pass we
basically get a contentious Gulf the
execution phase we're also recording
instruction executed during that phase
and then the next phase in the next pass
we say basically a guy the competition's
got everywhere minute second and the
instructions executed every one min a
second the second pass would be paying
instrumentation so basically counting
and instructions executed and then
identify the coat regions that are
executing the mapping the instructions
of the first to face and identify the
coat riches so I think we have more
details in the paper input but we could
talk more I'll flying if you're right so
there's also a face graph that will show
the face we actually do a pretty good
job identify the faces so that's the
compiler oh that's the profiler let's
take a look at the compiler so the
intuition behind the compiler see we use
rate reduction to actually reduce the
contentious nature of epic of
contentious fake region so we basically
reduce the memory request rate of those
region and here's some illustrative
example that to show the intuition
behind when you reduce the memory
request rate you reduce the contention
nature of application so if you have to
application running application a and B
and let's assume some certain initial
last level cache countin and access
order you have students you will have
execution times would be for example so
when you actually slow down ace
play KL access basically what you change
is the access order here that you
prioritize peace execution in the memory
request right because ace actually
slowed down so be memory requests get
prioritized and that will actually have
a big impact on the performance of B so
in this illustrative example this
execution is greatly reduced so based on
this intuition we have two techniques to
actually reduce the memory quest rate
one is padding basically we insert no
apps into those coal regions so the
ideas that the application execute few
instructions and then instead of
executing no ops so during this phase no
memory request issued another one is
snaps insertion so basically in
searching nap so you basically can
control let the application around
certain milliseconds for example and
then let asleep for a few milliseconds
to use those to those techniques to
control the rate reduction and you may
wonder that hope those two techniques
are really similar right because they're
just basically application to execute
for a while and let it sleep for a while
inserting no apps for a while that they
should have the same kind of performance
but in experiments we found out that
might not really necessary always be the
case so here is some performance showing
padding and leptin searching action y
axis here is the quality of service a
high priority application the I x-axis
is execution rate after low priority
application so from one x 2.4 x we slow
down the X this application by inserting
no ops or padding are napping as we slow
down this application you can see the
performance that the qos of the high
priority education improves right in
these three lines are napping with women
is second every nap or a 10 min a second
and padding so in this case we actually
have really similar performance for
three techniques and it's in this case
however we can see that nap every 10
milliseconds actually outperforms all
the other the other two greatly so even
you slow down the application one
application to the same execution rate
but different techniques you may have a
different
effect in terms of improving the qos
Sault Ste Marie reduction may not really
have the same effect and so it turns out
this this performance difference is
because of the granularity when you do
the rate reduction so numb I illustrate
this one so if you think about a
performance of a high priority
application let's say it's actually
starting in the performance really low
because of the other applications in
generating interference and the nap and
padding starts here we start to actually
slow down a higher low priority
application in this case high priority
applications performance only picks up
instead of jumping up to the really
optimal performance only picks up that's
because when you just start napping and
padding on there still memory requests
that coming from the low priority
applications you're getting served in
the system so it takes a while to
actually cool down those memories
request it also takes a while to warm up
the cash for the high priority
application for it to pick up the
performance right so what this really
says it's actually when you slow down
the application the granularity your pic
to do this actually matters so smaller
fine granularity for padding which is in
terms of psychos may not really give you
the best wing for the slow down you
actually sacrifice also comparing
padding and napping on napping is easier
to do a more accurate timing control
because you crack potentially have a
timer and it's more power efficient
because we're now really executing no
ops in this case alright so some of the
evaluation for QX compile and this is
combining both identified co regions and
apply the code transformation to those
applications here on the x axis we have
the high priority application y is y
axis is the quality of service of the
hypolitan applications each bar the
first part is windows application
running with original the original IBM
was no padding on the original and
basically no POS compile and the yellow
bar is a one-way actually identify the
co-regent sending searching naps into
those coal regions and this is every 10
milliseconds napping for 10 minutes
seconds the the purple is every 10
minutes seconds 20 minutes seconds as
you can see here we actually richly in
the performance of those high priority
applications up to eighty-five percent
and ninety percent of its original peak
performance similar results and they're
more results in the in the paper and
let's take a look at the utilization
when gang so basically those are the
application the low party application
when it's running without those high
poetry application with because we slow
down so we're not really running at 11
hundred percent of the execution ray boy
still gained significant around forty
percent in this case fifty percent on
this case of the utilization of the
costs that prove that previously will be
idled if we don't really do this kind of
colocation so we also have a really
similar results for the Google
applications where we actually improve
via the web search for up to close to
ninety percent in this case so the
summary is we pinpoint this contentious
regions and we apply a coat
transformation to those regions to
mitigate the contention the low party
application generates to the high
priority application so we could improve
silver utilization because we will
facilitate this kind of workloads
consolidation and I want to actually a
stress the multi-objective or compilers
in this kind of error that you're
actually looking at a lot of trade-offs
are more than we actually used to very
used to think about before alright but
so you're you just did set inserting is
no officers you know stretching out the
execution oh have you thought at all
about sort of more sophisticated
techniques or other things that can be
maybe before like you know do you think
what I compiler compiler can reason
about the code home you know and figure
out do different instruction scheduling
things like that are there other things
it could be done there were more
sophisticated I I think it's potentially
there's things that could be done I was
looking at you thinking that some of the
optimization you could think about you
know memory hierarchy optimization the
compiler optimization we do to actually
higher up device for the cash structure
another or loop transformation that
potentially we change the memory
characteristics but when we look at
those kind of optimizations the
performance impact or the the impact in
terms of mitigating contention is quite
small comparing to you really slow down
the applications if you think about loop
transformation potentially you're
looking at maybe six percent eight
percent of performance well for in force
six percent of a person performance in
terms of changing the high priority
applications if you change the memory
characteristic access pattern of your
low priority application by looping by
loop transformations the impact it has
on the high priority applications really
small compared to just really slow down
a lot so I think potentially there are
things to be done but I choose to
actually go this route because they are
basically the benefit you're getting
from those is not as big all right so it
would be nice if not necessary so the
last part of the work that I want to
talk about is a fairly new work that
would dynamically regulate the pressure
that we have on a memory subsystem so
the key idea is compounding for niceness
is a static technique right so you
basically have to be really conservative
you have to start on application without
knowing how much the high protein may
suffer from the performance degradation
or contention so why do we do this
reactively online we could dynamically
manipulate the application
characteristic based on the performance
degradation we actually observe and
based on the contention that we actually
detect right so we can avoid and
necessarily slow down so if the high
priority application for example it's
not really suffering from contention we
don't have to slow down
low party application and we could
actually achieve more accurate us
control because you are actually
monitoring the qsr online alright so the
overview of this technique we have the
compiler that basically is a profiling
based and identify the contentious
region and then strip instrument
mock-ups to actually invoke the runtime
system we also have the runtime system
that basically tasked with monitoring
the QSR detecting contention and
feedback control to actually control
they are then how much thought to link
down and how much nice you want this low
part your application to be so the
compiling compiler here is really
similar to the compound for niceness we
use the same profiler to actually
identify the contentious kill regions
but instead of actually apply inserting
know up subheadings we insert a mock-up
to actually invoke the runtime here and
the runtime system is where a lot of the
interesting things happen so we have the
monitor that's attached to the high
priority application which can monitor
the performance counter information so
it's actually stalled in a shared memory
in our system in the circular buffer and
the the nap engine is the pot that's
actually attached low priority
application so the low part of the
application can evoke around time of the
snap engine basically this an app engine
will read the monitored information of
the high priority application and detect
the contention and actually respond
according to the contention detect
detected and the qsr of the
high-priority applications so so I'm
going to talk a little bit more of this
part because this is actually really
interesting so to detect the contention
and react to this contention we could
have potentially implemented different
kind of policies so in this work the
first policy we try it is that's do a
simple way that's just conservatively
throttle down whenever the high priority
application suffers Q&amp;amp;As degradation
right so it could be fast positive but
let's just do it really conservatively
because the qsr degradation may now
really just be simply due to contention
another way is the story feedback
control so
we we have different states we execute
for a while then we actually go to the
Czech state to actually see whether
napping or our technique will
potentially have impact on the qsr if it
does then we actually continue to nap if
it doesn't which means maybe there's qsj
gradation but that's not necessarily
caused by contention because our
technique is not going to be able to
address reduce this contention or reduce
this kill improve this chaos so it's not
necessarily caused by contention so
that's actually just let that low
priority application execute so let's
take a look at those two FL technics in
action so here this is a sphinx running
with which is suspect benchmark running
with a low priority application and the
red this is a the ref input and in the
excesses of the time and y-axis is the
normalized IPC so if it's one meaning
there is no contention it's running on
peak performance so here you say the qos
when its core ones fingers occur
co-located with the original or prior
application and the blue bar is when we
actually apply the reacting niceness
here using simple to the low priority
application so you can see the IPC
improvement here similarly here we have
the green and the red bar the red line
is a similar line which is the original
performance here is the green line is
when the the targeted performer reacted
nicely supply to the queue or to the low
priority so the big big takeaway point
from this graph is you can see if this
line is very stable it's around ninety
percent which is our target Kos and
complain especially comparing to the
simple they're both effective but the
targeted achieves much more accurate qos
control because it's actually monitoring
the feedback it's monitoring the
feedback an effect of soldering
throttling down on those application to
find on just the high priority
application or is it both the Highbury
because it's just the high priority I
could just say well I'm not going to
even run the little priority ones right
and I maximize your life or that you
actually sacrificed utilization sure but
but if qos is what you're trying to
optimize it but I guess the question is
what the objective function ok step into
consideration both utilization and call
you sir I would think the optimization
question if you want to really formally
define it is the constraint is the key
OS that you want you as to hit certain
constraint with that constrain you want
to maximize the utilization so that's
how I will actually think about this
optimization um so for example the
constraint years we really wanted to
actually guarantee ninety percent of the
u.s. and then we could say with that
gave me the maximum performance you can
actually gave me but um oh well I want
to say this is I think this is a really
nice way of thinking about it because
you are actually giving people a lot of
flexible spectrum of selecting the
trade-off between the qsr and
utilization right you could say I care
about eighty percent or gave me the
maximum utilization here so people
basically have you know an OP that they
could actually work with in a data
center all right so let's take a look
another graph showing comparing simple
and targeted so again the red line is Q
as when co-located with original the
blue line here is simple when we apply
it to simple so it's actually fairly
stable here on the green line is using
target so they have similar very similar
experiments however when we look at the
nap duration decided by the nap engine
of those two are heuristics the green
line here is targeted which is much
lower than a simple meaning we are
actually doing much less slowing down to
achieve the simple performance because
nap duration is basically how long you
actually slow down applic you let the
application to to sleep so and this is
because target is using a feedback
control that it basically detects how
much of the necessary throughout
throughout going down you need for the
certain performance you you want so
again they're both are very effectively
target is better for improving
utilization and simple
oh so don't care so the you tonight so
this is um I think this is every two
minutes seconds we now so this is every
two minutes seconds our napping duration
so it's a little over point five so it's
point 5/2 dating to the nap time yeah so
so let's say every two min a second two
naps every two milliseconds you run full
speed and then you do a nap for example
Oh point five milliseconds then your
utilization point five over 2.5 there's
twenty eighty percent alright so let's
create a look at the evaluation this is
comparing our reactive niceness to cure
let's compile again to high priority
applications with running with the
original low priority a low priority and
running with the reaction niceness and
this is running with the Kos and you can
see those two performance are really
similar however when you compare the
utilization which is much much higher
than the qos compile this is because we
are dynamically figuring out how much
utilization will could get for the
performance we desirable performance and
this also is static because you based
you basically so you only see two parts
because you basically decide how much
you want this application to start or
down before even knowing a it doesn't
really change when you actually change
coroner's necessarily unless you
actually have to specify but this one's
more dynamic that you could actually
based on what kind of application the
low party application is running with I
thought it down those applications are
calling me alright so more evaluation
and the nurses are going to go through
but here is to demonstrate that those
are the utilization number and those are
the keywords target that we hit we get
about eighty percent ninety percent and
we're doing pretty good job maintaining
ninety percent if you if that's our
target alright so some of the
performance efficiency number that the
red bar is where we actually run to
application on a separate machines as
disallowed colocation and the green bars
we actually run those application
together and using reacting nice needs
to control ninety percent of the QSR
sorry
chief ninety percent of us we have
utilization our much more power
efficient alright so the section summary
here the reacting nice list basically a
you take advantage of static compilation
technique but also dynamically regulates
the memory pressure on those kind of
machines to facilitate utilization a
collocation and improve utilization so
conclude um we filled the characterizing
impact of memory resource contention on
the large-scale internet service
applications in turns out they are
actually very very important and based
on the insights we showed the threat to
calm mapping technique that improved
performance and also the reactive the
compounding feel nice legs then reaction
niceness to manipulate application
characteristics to improve the
utilization in the data center some of
the future work on that I'm thinking um
so we're also using a lot of management
I'm in a data center that we we have
been really looking to how to actually
construct manage around time that that's
aware of this kind of trade-offs between
curious and utilization and potentially
there could be more flexible research
that could be done in the manager on
time space again runtime system
infrastructure in the wsa is still quite
new and emerging that there are a lot of
interesting things that we could
actually look at particularly take
advantage of interaction between
application among applications and
pitching application and the
microarchitecture also it's there are a
lot of mobile system that are emerging
the hot word that has been evolving with
a really fast turnaround generation time
for mobile services and there was a lot
of hydrogen energy going on in this kind
of system now we really need to think
about how do we build a software stack
and compile around ten system to really
take advantage of those system that I
think it would be really interesting to
look at all right so with that take
questions you love your last
graph and killer it scuttled our web to
the beat or two in any case number look
slow but I have another number I skill
instruct instructions to little jewel or
instruction per second per web
uh I I look into it I don't yeah maybe
maybe all right I didn't well basically
what we did is using the power meter
yeah power meter gives you energy on
listen I'm tight a bridge
doesn't give you power gives you energy
even though it's called a power meter it
did oh yeah using the Sandy Bridge 10 we
just basically use the one you could
actually clucking for measuring like
household appliance kind of thing so we
actually matter the system performance
because we really care about the whole
diocese also on the wall so you're just
doing draw out of the wall yet
performance counter yeah I've never
tried the performance Scott is that me
to be really interesting to China's
oldest methods yeah well priority
application doesn't have this is that
you'll do this if you tighten below mhm
yeah
Oh robbers and general are those results
are you confident that they would
basically the general insights that you
have drawn from this work by two data
centers where can you go into that
I think it's really general I think I've
shown some of the Google applications
and those that the data that we do with
school grappling on their production
machines so and I do know that this up
they're really working on this kind of
techniques right now so maybe to point
out a little bit more on that so you
opened up and stock and talk about low
utilization that was kind of the
motivation and then a lot of the
benchmarks though worst respect which
are just into your codes that don't do
any item hmm so I'm curious how much do
you have any ideas about how much I owe
is going to have an impact on some of
these things because the spec benchmarks
are not designed not to have i oh right
so the reactive niceness work we're not
use and in Google applications because I
was not really doing interesting for
them aside did this work but the qos
compile was using comparing spec and
Google application and we actually
achieve really similar results that's
why we're really confident that require
sure you expect for those kind of
applications for in terms of i/o where
the reactive nice mates will actually
have been in fact on Io I don't have
data so i could only tell you with that
so the reason why i didn't really
worried that much about using spec is
because a lot of web search of you those
are kind of applications their data
mostly resides in the main memory so
that you want to reduce as much as the
i/o operations for that particularly of
that kind of application just because
you want higher latency oh no how it is
you don't want high latency you are low
relative so you wanted everything to be
in the memory but there are certain
application that I hope we will be
interesting thing to look at the cupola
gmail actually has a lot of i/o that
I yeah I don't want to really say that's
the street I think the main idea maybe
potentially applicable in those are the
kind of applications but I don't even
have data for</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>