<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Parallel Inference and Learning with Deep Structured Distributions | Coder Coacher - Coaching Coders</title><meta content="Parallel Inference and Learning with Deep Structured Distributions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Parallel Inference and Learning with Deep Structured Distributions</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kUPxtUS06fY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
at four
Sebastian I guess I'm the first speaker
to start late here and so and machine
learning is transforming our society
well if and if you think about areas
like language processing or computer
vision or robotics they are all
permeating our daily life more and more
but in particular they do use and
machine learning techniques to produce
impressive free sites on tasks such as
speech recognition or object detection
or autonomous control to just name a few
and why do we get those impressive free
sites these days where we do get those
impressive results in part due to a
growth make improvements but also
because of the huge amount of data that
we have available these days and indeed
taking everyone's devices into account
we all together took more than one
trillion pictures in the first half of
2015 and this number is astonishing
because it means we took more pictures
in the first half of 2015 then we took
in the entire photographic history until
2050 and that underlines an incredible
growth rate but similar growth rates can
also be found for other data sets such
as YouTube videos Facebook content
shares here at Microsoft you probably
care more about the number of skype
conversations or in the scientific area
imagenet data set pascal you see data
set are just two of them so what do we
want to do with all this data well we
want to extract information from data
but importantly we want to do so as
efficiently and as effectively as
possible and but this process these days
is not at all efficient if you ask a
machine learning PhD PhD student about
the most tedious part of his or her job
she or he is going to answer quickly
without thinking much annotating data
after having annotated the data we
typically spend a lot of time thinking
about or formalizing permit rising the
problem and why doing so we better also
keep an eye on two important
optimization problems and machine
learning being the one being the
inference and the other being the
learning part so what do I mean by
annotation parameterization and
optimization when annotating data X
we're interested in assigning that
ground truth s from the set of all
possible output space objects
calligraphic s that best describes the
observed scene let me make this very
clear suppose we are given as our input
data X and image as illustrated on the
left hand side and we're interested in
finding the faces of the walls as well
as an object now a possible ground truth
for this task is shown on the right hand
side and I guess you can imagine that it
took quite a while to actually come up
with this picture and importantly we
spent all this time only to create this
data set d which contains pairs of input
data X and corresponding ground truth s
now clearly going forward we want to
spend less time on this particular
problem so one important challenge that
we need to solve in the future is how do
we better extract information from
unlabeled or weekly label data now in
the second part the parameters ation
part reach and we are typically
designing functions f functions F that
matter the compatibility the fitness
between a hypothesis s and a data point
X and those functions f depend on
parameters W importantly there is a huge
a wide space a very large space of
possible functions out there to give an
example the function f could be a linear
function we are taking the inner product
between the parameter vector W and some
handcrafted feature vector F theta
alternatively it could be a shallow
model where we are first transforming
the data via function f 2 before we
measuring the fitness and to the
hypothesis by another function f work
or it could be a deep model which is
very popular these days now one of the
challenges in that area is how do we
design those functions how do we do so
effectively without much human
interaction particularly if you have
data from multiple modalities from
different sensors now a third the third
part that I mentioned is the
optimization challenge and two important
problems are the inference problem in
the learning problem in inference we are
interested in finding that output space
object s star that maximizes over all
possible output space Corps
configurations calligraphic s this
fitness function this scoring function f
and the second problem and that is very
related is how do we find the parameters
W of this fitness function let's known
as the learning task and in this
particular problem we phrase it as an
optimization problem as well and the
challenge in this area is how do we deal
with the huge amount of data that we
have available these days as I mentioned
before and this is exactly what I want
to focus on in this particular talk I
want to first look at the inference
problem and i want to show you how to
solve inference and graphical models in
a distributed way before we're then
going to turn our attention to an the
learning problem where i'm going to show
you how to combine deep learning with
graphical models so why is this
inference problem finding s star the
output space object that maximizes over
the set of all possible output space
configurations calligraphic s our
scoring function f such a problematic
task well if you think about the
motivating example that I showed you the
image were interested in finding their
faces it turns out we need roughly 50 to
the power of 9 3d configurations in
order to discretize the 3d space in a
reasonably dense matter now clearly
searching over all those 50 to the power
of 9 discrete configurations one by one
meaning computing 50 to the power of 9
and scores f is not fees
which then means the question is how do
we perform this inference task this
optimization problem effectively in
large output spaces and how can we do so
in parallel and the key observation that
we are making is that oftentimes we are
not interested in predicting a single
variable s but rather are we interested
in predicting a set of verb is s sub 1
through S sub n now why is this such an
important observation well using this
observation we can assume and it turns
out to be true for many applications
that our and that our scoring function f
of s which is in fact a scoring function
that depends on S sub 1 through S sub n
decomposes additively into a sum of more
local scoring functions f are each
depending only on a subset on a
restriction or a region of variables s
are now and throughout this talk I'm
going to assume that the variable s are
discrete which then means that those
local scoring functions FR f 12 in the
example on the slide behind me are
nothing else but arrays look-up tables
with a bunch of numbers stored in them
in this example f12 that 11 and 12 and
so on and so forth for visualization
purposes throughout this talk I'm going
to draw ellipses for each and every
local scoring function and within the
ellipse i'm going to write the scope of
this particular function now let me make
this very clear and by looking at an
example and the example I chose consists
of three local scoring functions one
depending on two variables s1 and s2
jointly and to each depending on one
variable s1 and s2 for this example I'm
also going to assume that the variables
s1 and s2 both only have two states two
possible and choices now what I want to
show you is that the inference task
shown on top of this slide is equivalent
to an integer linear program and to
show you that let me introduce variables
be and in particular i'm going to
introduce one very will be for each and
every array entry so the total amount of
array entries is two values for the
function f 1 2 values for the function f
2 and 4 values for the joint function
and the corresponding introduced
variables be on the left hand side if we
now multiply the introduced various be
with the corresponding function entries
and maximize the inner product we don't
quite get what we'd like to have we need
additional constraints and what do we
want the variable speed to do well we
want the various be to select entries
from those arrays so we better require
them to be either 0 or 1 importantly we
want the very speed to only select a
single entry per array so we better make
sure that the various be are properly
normalized now there is one more
constraint that is missing suppose I
told you what entry you have to pick for
the more global scoring function from
this information you can deduce what
entry you have to pick for the more
local scoring functions s1 and s2 in
this example and this property is known
as the marginalization property don't
worry about the precise mathematical
form shown on the lower right hand side
of this slide what I want you to keep in
mind is that I'm drawing edges between
regions between which we want this
marginalization property to hope now
here's the program again it's a linear
cost function subject to linear in
integrality constraint hence it's an
integer linear program which is np-hard
to solve so how can we address this task
well let me rephrase this problem little
bit instead of using the inner product
notation I'm going to use the some
notation now instead of explicitly
writing down the marginalization
constraint in this talk I'm just going
to refer to it by its name I combine the
non-negativity constraint with the
normalization constraint by saying that
the world's be our local probabilities
and if we now throw away
the integrality constraint we end up
with what is known as an LP relaxation
which then means we can at least
approximately solve the inference
problem using a standard ap solver so
why am i standing up here making such a
big mess about an inference problem that
we can approximately solve by an episode
where it turns out if you try this for
reasonably sized inference problems he
will quickly realize that standard API
servers are slow which means we need
specifically tailored albums errands
that take advantage of some structure
within the problem and what is the
structure that we can take advantage of
in this problem well I think you're all
guessing it right it's the graph
structure defined via the
marginalization constraints and think
about a bigger graph with a few sports
edges here and there this is exactly the
structure that many many message passing
solvers are taking advantage of they are
very very effective because they look at
those edges they optimize with respect
to various define on those edges and
they do so by solving a small sub
problem to global optimality and then
they keep iterating over those edges now
there is a small issue here which is um
we need to take specific care to make
sure that we obtain the globally optimal
solution of the LP relaxation I don't
want to go into the details of this
particular issue i'm very happy to
discuss this offering though i want to
rather get back to the question that i
post initially now since we know how to
at least approximately solve the
inference task how can we do so in
parallel on multiple machines so let me
make this goal a little more precise
what do we want to do when we want to
optimize the LP relaxation object we
want to leverage the problem structure
defined via the marginalization
constraints we want to distribute memory
and computation requirements that's the
question that I posed initially but
importantly we want to main
10 convergence and optimality guarantees
of existing solvers meaning if i take my
problem my graphical model splitted
across multiple machines solving on my
TV machine separately i want to get the
same solution that i would get if i
would solve it on a single machine how
do we do that where we can do that by
making use of the dual decomposition
extension of message passing solvers let
me show you the intuition behind this
weird sounding sentence so suppose we
are given a problem like the one
illustrated on this slide where we are
having a eight scoring functions each
depending on one variable and then
locally connect it via pairwise
functions that depend on two variables
now instead of explicitly writing down
those pairwise functions let me just
denote them by edges so I did not change
the functional structure here the only
thing i did was i am not explicitly
characterizing the pairwise functions
now this looks very much like a markov
random field that probably many people
in here are familiar with what i want to
now do is I want to take this mark of
random field I want to solve it on two
machines cup of one and cup or two as
illustrated on this slide and so what do
we do how can we do this well earlier we
introduced those variables be we are
going to do the exact same thing here
but importantly we are only going to
introduce a very be on a machine if that
function say as a function f1 is also an
allocated or partition to that
particular machine which then means
there's already some distribution with
respect to memory going on here right a
very be one is never going to show up on
computer capital however there is some
regions and like the one connecting
various two and three whether when
connecting various six and seven which
appear on both machines so upon
convergence we need to make sure that
not only the local probability
constraint and the marginalization
constraint hood but we are
need to make sure that those regions
those very base B that are introduced on
two machines but are supposed to be
identical are consistent and that's the
additional consistency constraint that
we need to introduce now given this
intuition we are ready to state the
distributed LP relaxation objective it's
a linear cost function subject again to
local probability constraints on every
machine subject to marginalization
constraints that have to hold on every
machine and subject to those additional
consistency constraints that have to
hold on every machine now if you look at
this program you might be wondering
where that looks like it's entirely
decoupled but obviously that can't be
quite true the problem was originally
covered so it still has to be covered
and and indeed it is the couple it's
copied via the consistency constraints
but importantly it's the only coupling
that happens so the problem is almost
entirely covered except for the
consistency constraints and that helps
us to derive an eyelid and i really like
consists and an iron that iterates two
parts at first we r am kind of ignoring
the consistency constraint and we are
performing message passing independently
and separately on the individual
machines eventually we stopped and in
order to make sure that upon conversions
consistency constraints hold we have to
exchange information between machines
before we then go back perform some more
message passing iterations and then
again and exchange information now and I
wouldn't with that structure poses a few
questions first of all and how does it
compare to existing state-of-the-art
servers question number two how often do
we have to exchange messages between
computers and question number three to
what scale of problems and can we can we
go what scale of problems can be solved
now let's look at those three questions
one by one when comparing it to state of
are we compared it to a library for
discreet approximate inference as well
as and to GraphLab now a famous company
in Seattle and what we're seeing here is
a bunch of baseline methods as well as
our general implementation I'm showing
you here the run time in seconds and
this is more or less to just show that
our implementation does something decent
we didn't screw up terribly importantly
we are also able to get a good primary
value since we are maximizing higher is
better now if we distribute our problem
onto multiple machines we can improve
the performance even further why not
screwing up the primary value too much
so um that's a good research I guess and
going forward how often do we then have
to exchange messages between the
different machines and that's
illustrated on those two plots here so
what I'm showing you here is the on the
left hand side the dual energy over the
number of iterations we're maximizing
the primer so we are minimizing the
corresponding dual and i'm going to show
you the dual when exchanging information
every single iteration 5 10 20 50 or 100
iterations and what you can see here is
that if you exchange messages every
single iteration the dual converges
faster then if you'd exchange messages
every five to 10 20 50 100 serrations
that's to be expected but importantly
what this graph does not take into
account is the time it takes to actually
exchange information between different
machines and if we take the time into
account that's what is illustrated on
the right hand side we see the dual it
energy over time we see that now and
turns out that exchanging information
only every five to ten iterations is
more beneficial than exchanging
information every single iteration or
hired durations like 50 or 100 which is
to be expected now to answer the final
question how often and to what scale of
problems can we go and so we looked at a
disparity map estimation problem from
computer vision where we are given two
images and I quickly continue this and
then I get two questions
and we're given two images and we are
trying for every single pixel to
estimate the distance from the camera
center now what we we scale up this
problem and for every single pixel we
used 280 states and we had a 12
megapixel image which means the size of
the output space was 282 the power of 12
million I guess at least by now you can
imagine that using a standard at peace
or worse at peace over for a task like
this would not be suitable and but due
to the 280 discrete States where we were
able to get quite smooth disparity maps
and and can visualize nice pictures so
with that I want to conclude the first
part of the talk where I showed you how
to and solve this inference problem of
finding that the Maximizer s star over
the entire and the yet where we found
the Maximizer s star so there were a
couple of questions here you really
should be setting the number of the
number of how often you exchange
information adaptively over the course
that's yeah that's absolutely right and
that's a very good strategy that would
improve the performance even further we
did not look particularly into that but
that's that's a very good suggestion and
yeah I should have mentioned that also
the type of problem that we are looking
at was a standard mark of random fields
for connected grid graphs and just like
the ones that I showed you in in the
intuition and the connectivity between
the computers are also forgot to mention
that is a standard local area connection
and so obviously and the graphs that i
showed you depend on the connectivity
between the computers and the graph
structure and so you could always design
graphs adversarial graph structures
where this type of approach would not
work very well and you could always
design a graph structures favorable
graph structures where this type of
approach would work quite well
costs account for just passing enough
information along each edge rather than
passing the whole state to everybody and
so the so we are essentially the time as
I mentioned take into account the entire
inference time which accounts for
exchanging information between different
the information between different
computers that is required so that's
specific variables and that we need to
exchange so we are not taking the entire
state of one computer transferring it to
the other computer we're just
transferring specific specific and
specific problems as specific as
specific variables between computers yes
any more questions so if they're sure
the primary problem is not strongly
convex that's precisely one of the
issues and related to convergence and if
it were strongly convex then there
wouldn't be an issue otherwise since
it's block corner to send solvers and
you need to pay specific here and this
is exactly the point that I mentioned
and I'm very happy to discuss that
offline on on how to fix that issue but
yeah and strict convexity is is a good
point and so that brings me then to the
second part of the talk and where I try
to answer the question how to find the
parameters W of our scoring function f
now and this is known as the learning
problem where we're interested in
finding good parameters given a data set
D that contains pairs of data X and
corresponding ground truth annotation
ass if you are looking into literature
there are two types of approaches that
exist on the one hand there are
log-linear structured models like
conditional random fields or structured
support vector machines where the
scoring function f is typically assumed
to be this in a pro
between a parameter vector W and some
handcrafted feature vector s satira and
particularly recently nonlinear models
like conclusion area networks became
very important too and in these cases
the scoring function f is just the
general and arbitrary and scoring
function typically a composite function
now what I want to show you is that
those two seemingly different frameworks
are not so different from each other
after all in fact that there are two
specific cases two special cases of what
we refer to as nonlinear structure
prediction let me show you the intuition
of what I mean by that and to do that
assume a simple problem suppose we are
given an image X and we are interested
in finding the mode that the category of
the most prevalent object now one way to
solve this task is by taking the input
image eggs passing it through a set of
transformation and predicting a single
variable at the top now let's extend
this task let's go one step further and
let's assume we are not only interested
in finding the most the category of the
most prevalent object but that we are
also interested in finding the category
of the second most programming object
how can we do this where a simple
approach will be to design another
convolution in your network which tries
to solve this particular task so this
convolution unit your network or
whatever function it is and might share
some parameters between them but it
tries to predict another variable at the
output now clearly there is correlations
between the most prevalent object and
the second most parent object but this
correlation is at best taking into
account implicitly in this framework so
what we're trying to do is explicitly
modeling the correlations between and
those those two variables why yet
another function f that I refer to here
is another convolution in your network
so how can we do this wait to see that
let's quickly go back to the inference
task remember inference was this problem
we're interested in finding that output
space object as star that maximizes over
our entire output space calligraphic s
this scoring function f now instead of
maximizing and the scoring function f i
want to introduce the probability of
that configuration and then maximize the
probability so let's do that the
probability of a configuration s is
nothing else but taking the score F
exponentiating the score F and making
sure that it's properly normalized by
what is known as a normalization
constant of partition function C now
clearly with that definition of the
probability at hand we can rephrase
maximization of F to be equivalent to
maximizing p now that can be easily seen
because the normalization confident as
the name says is indeed a constant for
all possible output space configurations
s the normalization constant does not
depend on the output space configuration
is so we can just ignore it when
plugging p into this formula the
exponential function is a monotone
function which means we can also ignore
it and therefore maximizing p is the
same as maximizing f now with that
definition of the probability at hand
what do we want to do during learning
well during learning we are given a data
set d and we want to maximize the
likelihood of our training data set d
that's exactly that equation that i
showed you in and the outline slit now
what is the likelihood of a data set D
well we are typically assuming the
samples were not within our data set D
to be independent and identically
distributed which then means the
likelihood of the entire data set D is
the product of the individual
probabilities now plugging the
definition of the probability into this
cost function will quickly arrive
at the problem shown on the bottom of
that slide that we are interested in
solving no magic happened here the only
thing I did was I took an additional
logo now how can we solve a problem like
this where we can just compute suppose
we are starting at a point W we compute
the gradient at a particular point w we
walk into the gradient direction weary
compute the gradient we walk into
another gradient direction and we keep
them doing that so that's standard
gradient descent procedure so we need to
know what is the gradient of our problem
going to look like now taking the cost
function going through a bunch of
derivations again not very complicated
we'll quickly end up with the gradient
the gradient has an interesting form
it's the difference between two
distributions we're taking and then
scaled by a partial derivatives of our
scoring function with respect to W now
the two distributions are on the one
hand a probably distribution data which
is zero everywhere except for one entry
which corresponds to the ground truth
annotation where it's equal to one and
then the second distribution is our
current predicted distribution now I'm
clearly that makes perfect sense suppose
we would perfectly predict the
annotations which then means this
predicted distribution would be equal to
the ground truth distribution which
would then cancel out which would mean
that the gradient is 0 so what do we
need in order to design an algorithm
where we need to compute our predicted
distribution then we need to compute the
difference between the predicted
distribution and the ground truth
distribution and scale this by the
partial derivatives of our scoring
function so a standard deep learning
algorithm then performs four steps first
of all compute the functions f once you
have computed your functions f you
transform them into probabilities by
exponentiating and making sure that it's
properly normalized then you take the
difference between the ground truth
distribution of the predicted
distribution scale it by your partial
derivatives which is typically done by a
backward pass and then you update your
parameters W and then you keep iterating
them now in such an architecture what's
the challenge where is the problem well
how do we compute our scoring function f
if the size of the output space s is
large and even if we could somehow do
that how do we then compute the
probabilities P and to show you that
this and this is exactly what I want to
get into now kind of showing you a
solution to deal with large output
spaces and to show you that this is
indeed an challenge just think back
about the motivating example this
endorsing understanding example where we
have an output space of 52 the 9 and
elements clearly computing the scores
for all those elements is not going to
be efficient and but the observation was
that we are not interested in predicting
a single value but rather are we
interested in creating a set of various
S sub 1 through a sub N and we could use
this observation to assume that our
scoring function decomposes importantly
I'm not telling you what those local
scoring functions are those local
scoring functions can be anything they
can be convolution in your networks they
can be whatever function and you see or
you doing reasonable for your task but
this decomposition assumption is a
crucial assumption that is also going to
help us in order to design an a
efficient deep learning a framework for
large-scale its large structured output
spaces so how do we do that well one of
the issues was that we needed to compute
this gradient and the gradient was
computed as the difference of two
distributions importantly two
distributions define on large output
spaces we are summing over large output
spaces 50 to the power of 9 elements for
each of them
now if you plug in this take composition
X assumption and go through the
derivations we quickly see that the
gradient decomposes instead of having to
one difference over to large
distributions we're now having many
differences over smaller small marginal
distributions and that is obviously a
lot more effective to do now the
question is how do we then compute those
marginal distributions PR well that's
unfortunately a challenging problem and
we cannot compute these marginals
exactly but we can obtain approximations
approximations that are similar to the
various be that are introduced in the
first part of my talk we can obtain them
either wire exactly the same methods
that I showed you in the first part of
my talk and message passing methods we
can also obtain them by a meaningful
techniques branch and bound service also
sampling techniques and that would be
another choice now we went ahead and we
implemented this so we came up with a
sample parallel implementation where we
repeat five steps first of all we
compute forward in a forward pass all
those local scoring functions F are
after having done that we use those
local scoring functions to perform
inference in order to obtain
approximations for our marginals after
having obtained approximations for our
margins which can be done and under
certain assumptions very efficiently in
fact we use those to compute the
difference between the marginal
approximation and the ground truth
margin and then we scale those via back
propagating after having done that we
need to synchronize the gradients across
different machines before we then update
our parameters and we keep iterating now
how do we deal with a large number of
training samples where we do what other
people have done is where we use mini
batches and we paralyze across different
machines
and we deal with large output spaces and
that's different from what people have
done by using those structured
distributions this assumption that our
scoring function decomposes now how does
this perform in practice wait to show
you that I'm going to look at em three
tasks on the one hand prediction of
words from noisy images second task
tagging of Flickr photographs and the
third task is this M layout example that
I use to motivate this talk so for
predicting letters from noisy images and
we're given images like the ones
illustrated here and I'm showing you the
ground truth on the bottom the size of
the output space in this case is 26 to
the power of 5 26 and possible letters
for five-letter words and the graphical
models we're comparing here are on the
one hand linear chains first order
Markov models and then loopy models
which also take into account longer
range interactions the unary functions
are multi-layer perceptrons and my piece
and the pairwise functions are also
linear on my delay a linear or
non-linear multi-layer perceptrons now
I'm going to show you hear the word
accuracy for one layer at the word
accuracy for a one-layer configuration
with different parameters for the one
layer as well as the word accuracy for a
two layer configuration with different
parameters for the second layer and what
we are comparing in this chart is if we
are only using a CNN that's known as
unary only if you're using a chain graph
or if you're using the loopy graph so
first order or second order model what
we can see is across the board the
deeper and the more structured our model
the better that performs with more
structured I mean taking longer range
connections into account now in the next
slide here I'm showing you the word
accuracy again for a one-layer
configuration at two layer configuration
with different parameters but now I'm
comparing CNN only with a linear pair
wise and a nonlinear Paris and we see
again across a set of different
parameters
nonlinear pairwise functions improve our
linear pair wise functions which in turn
improves over not using am not using
correlations at all not taking
correlations into account at all and so
that's that's nice I'd say but that's
kind of a very simple example so let's
make things a little more complicated
and to do this I change to the second
example the Flickr data set where we
given an image and we're interested in
assigning tags so given an image we want
to we have a set of 38 tags and we want
to know which subset of tags describes
the observed content way and in this
case we have 2 to the power of 38
possible output space symbols attack can
or cannot be assigned and we have 38
over the graphical model is a fully
connected k38 graph and the unary
function in this case is Alex nut named
after Alex Khrushchev ski and the
pairwise function is a linear function
in this case and again we are comparing
here only using CNN which gives us a
prediction error of nine point three six
percent to Train using the graphic air
using the CNN to obtain unary features
which are then fed into a graphical
model that's piecewise training which
gives us a performance of 7.70 percent
and then going one step further and
training the whole system 20 which gave
us an error of seven point two five
percent just you have to get everything
right and that's kind of this correct
prediction and now that was say
decomposable sir one loss in this
example
what's up what is it and so we are given
images and attack could be people female
male and I'm going to show with you
actually on this slide here so we're
given images like those ones as input
and ground truth tags are shown in the
row below and then the predicted tags
are shown in the third row so the
implementation does fairly reasonably
I'd say there is a few obvious and
mistakes like instead of predicting see
here we are predicting sky which looking
at the image seems somewhat reasonable
and we're missing the indoor tag here
which looking at the image I wouldn't be
able to tell whether this was
indoor/outdoor and I mean I guess our
algorithm is not very romantic it
doesn't know that this is a rose but
what is more interesting than looking at
those pictures is probably looking at
the correlations that it learns so for a
subset of the tags a female people
endure and I'm showing you the learned
class correlations we were seeing is
that attack female is very likely an
image that is tagged female is very
likely to be tagged people or you're
looking at a tag in nor then it's very
unlikely to be tagged with sky or plant
life which seems reasonable now going
forward I want to also show you some
results on this 3d indoor seen
understanding task where we are given a
single input image as our data and we
are interested in finding the faces of
the room as well as an object we're
assuming that the Manhattan world
assumption holds and then if we use in
this case handcrafted features like
orientation maps geometric context or a
combination of both we are obtaining
pretty decent performance I'd say but
instead of looking at numbers let me
show you rather what we can use this for
so as I mentioned before we're given a
single image I should click
I we are given a single image like the
one illustrated here and what I'm
showing you now is on the right hand
side if you're only predicting the
layout and on the left hand side what do
you obtain if you jointly predict
layouts and end objects now clearly and
predicting layouts and objects reasons
more holistically about our environment
which that means we are obtaining a more
realistic scene interpretation so for
yet another example here given the
singer image if you only predict the
layout will observe clear artifacts
which we can avoid if we're more
holistically reasoning about em the
environment now with all that at hand
where should we go next what what is
according to my opinion important
problems that we haven't properly
addressed yet now what I'm very excited
about this and helping us to extract
information from data as efficiently and
as effectively as possible as I try to
outline in the very beginning there's am
clearly a few issues that we haven't
addressed properly yet in particular
we're getting very good at this task by
leveraging huge amounts of data but
these huge amounts of data also help us
to reason more holistically about the
environment about our environment but
two recent more holistically we need to
take better advantage of those
structured distributions now there's a
few issues that as I try to outline we
have we haven't addressed properly
according to my opinion on the one hand
we need to better take advantage of
distributed computing violence we need
to get better at addressing and
extracting information from data using
weekly or unlabeled data and we need to
get better at designing those functions
automatically particularly if there is
mighty modal data involved so let me
show you a few projects that could be
done to go forward in this direction so
for taking better into account
distributed computing violence
and let's look at the items that I
showed you both of them were synchronous
albums meaning we had this barrier
somewhere in our element where we had to
stop to exchange information between
machines now this works fine if our
compute environment is very homogeneous
however these days we're getting more
and more into areas where we have
heterogeneous computing violence so we
need to bet get take better advantage of
these architectures for example via a
synchronous optimization techniques and
also hierarchically distributed items
become important if you think about
applications like like autonomous
driving or we're having cars that have
very weak computation environments and
then large data centers data centers
that have a lot more computational
resources now when getting into a week
learning from weekly labor data there is
a few extensions that are possible they
are generally involve try to extend this
these intricate relationships and that
exists between items like Gaussian
mixture models and k-means which are
related to each other via small variants
asymptotics just the same as hidden
conditional random fields are related to
late instruction with support vector
machines and those two are in turn
related to the gaussian mixture model
and k-means by exchanging the sufficient
statistics now we can go further and
extend this and I call this the graph of
algorithmic relationships and there is a
few missing pieces and that I think are
very important particularly in the area
of investigating directly process
k-means for the structured setting which
hasn't been done yet now going forward
in the direction of learning functions
from learning functions automatically
there is also a variety of problems here
some more practical some more
theoretical on the more practical side I
showed you this am 3d endorsing
understanding task where we are given a
single image and we can reconstruct the
the scene we recently extended that to
incorporate data metadata like floor
plans to also extract in for reconstruct
entire apartment now we can go forward
and X in corporate additional sensors
for example like gyroscope data which is
readily available in many many and
cameras these days and that is useful
for better localization estimates or to
also go forward and use this for
prediction of habits in smart indoor
environments now with that being said I
want to thank my advisors mark ricotta
mirror and Ruslan who have joined me on
this journey as well as the the many
many co-authors that I was very happy
and lucky to be able to work with and so
let me briefly summarize and I showed
you how to design distributed inference
algorithms that make use of to
decomposition techniques to paralyse
graphical model inference across
multiple machines in the second part I
then showed you how to combine the
learning techniques with inference and
graphical models to train systems that
we refer to as nonlinear structure
prediction and in the third part i try
to hint at a few future directions that
are all general related to the steam
that i'm very interested in of
extracting information more effectively
from data thank you I was wondering how
this kind of structured model with
relatively small receptive field
compared to non structured model
basically the scenery potential with a
large receptive field so in the first
example you had I think each neuron CNN
had access to the part of the image
responding to one digit or one character
right and they weren't connected on top
that's right fit so what if each CNN had
access to old image then it could kind
of do this
to inference inside the neural network
right right in the second example I
think that Alex didn't had access to the
whole image is that correct that's right
yes so do you have any like idea how
these two extreme cases mixes and what
is the right balance right I mean you're
definitely right and what you're hinting
at is to what extent do those and
nonlinear models implicitly and capture
correlations with in data and that's a
very good very good question and there
is no obvious answer to that and the key
point that I want to make though is that
if you're training at your networks
you're not plugging in say a higher
order loss or a loss that captures
correlations between higher order
margins you're essentially just plugging
a loss that has how accurately am I
predicting where everyone how accurately
am I predicting very bit too but you're
not capturing how accurately are you
predicting both of them and jointly and
that's something which can be done with
those graphical models which cannot be
done if you're just using and your
neural networks with individual
predictions on top you're taking example
if i remember right yes then shouldn't
the base optimal decision rule be
conditionally independent and so the
benefit that you've shown the two
percent from your from your models where
does it come from them and yeah that's
absolutely right if the loss would be
decomposable then this then it should it
should alleviate it really give us the
same result however am due to us
optimizing non convex problems where we
actually have no guarantees anymore m at
all right so um what I what I want to
hint that is that if we put those
graphical models on top we're actually
simplifying our learning problem and
we're not learning we are able to take
information into account more
effectively and in a sense it performs
like an additional regularization
and this is where we believe these
improvements come from in your case you
had big great start across right that's
right you only have so many options to
cut through the partition that right but
if you give them a general Olaf answer
to grab how would you partition it
across multiple machines because I would
imagine that the grass circle itself is
not enough information to do so right
how to do so intelligent is you need
something more how would you do I mean
the graph structure in itself and
particularly the potentials on the graph
kind of tell you how how strongly
certain regions are connected so I
believe the graph structure in addition
to the potentials that you have on your
graph and are are quite a bit of
information that could be used in
particular this is all the information
that is used during optimization as well
so that should be all the information
that you need now we haven't precisely
looked at what are good strategies for
partitioning graphs we looked a little
bit that what could be good
possibilities but we haven't quite
followed up on this direction and you're
definitely right for the graphs that I
looked at the grid graphs there it's
it's it's somewhat obvious how to
partition then graph and it's going to
be more tricky if if you're looking at
problems in the area of say protein
folding for example where the graph
structure is bye-bye nowhere close to
any regular structure and in those
settings and finding good partitioning
is crucial we haven't looked at that we
assume this as input to our algorithm so
far and by we haven't looked at what
could be a good metal rhythm that could
be put up front to kind of find it with
partitioning but yeah it's a good
question would be an interesting problem
to look at operators building for
distributed stuff presumably use the
quadratic cost on the agreement between
the per machine variables
and no there is no quadratic cost on the
agreement between the per machine
various its and what we're doing is we
have this consistency constraint and
then we're we're forming a Lagrangian we
just use Lagrange multipliers we are not
using the augmented lagrangian technique
so the augmented lagrangian technique
would introduce squared losses on
however we are not using augmented
lagrangian techniques so we're just
forming the La Crosse in itself which is
a Lagrange multiplier x the constraint
itself and so there is no squared and
loss or anything of that form but there
if you go through the items through the
derivations the effect this and what's
going to happen is that essentially you
have you have those two inference steps
and then you're exchanging information
and the exchange of information is
similar to an averaging procedure so I
guess and this is what you were hinting
it and there when going through the
variations that research are very
similar the variational methods that you
were using in the first experiments but
some people report problems with the
interaction of approximate inference
with learning did you say something
about that I can can get that the second
part with me off some the interactions
of approximate inference and learning so
if you're not exactly solving this
inference problem in the inner loop when
you're computing your gradients then
this can cause havoc through there some
reports of that right yes the oven
that's that's a very good question and
typically so oftentimes were what i use
in my research on bass and convex belief
propagation type of procedures and i use
them both during training as well as
during learning and so we are using a
consistent i will them 4m during both
parts and so i will wear this issue
comes in and is more tricky is if you're
say using some approximations say
sampling techniques and during
during learning and then you would use
loopy belief propagation during
inference those will be clear mismatches
and we have seen similar issues that
using consistency of algorithms during
both learning and inference is important
which we try to do here as well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>