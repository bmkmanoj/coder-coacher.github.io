<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computational Video: Methods for Video Segmentation and Video Stabilization, and their Applications. | Coder Coacher - Coaching Coders</title><meta content="Computational Video: Methods for Video Segmentation and Video Stabilization, and their Applications. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Computational Video: Methods for Video Segmentation and Video Stabilization, and their Applications.</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-8rL4j4Ut7U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I guess I got started on Issa who has
been at Georgia Tech since 1996 the
profession professor there spanning
multiple departments or the job spanning
multiple departments and has been and
before that he was at the median up from
88 to 96 doing a pretty trick of getting
a job before you start your masters so
your dog a fund for work on many areas
of computer vision I particularly think
about him is one of the first people
sort of at that intersection between
vision and graphics but lots of other
work on textures faces and animations
and analysis of humans and journalism
unusual for many of us is what month's
best paper Awards but they beat his work
as a consultant with Disney and Google
which have led to some obvious super
high in fact outputs and I think the
thing I'm really excited about moment is
the YouTube video stabilizer where you
just upload a video and it does a nice
job of stabilizing it and about
extensions of that and other work thank
you Andrew and everybody can hear me and
feel free to interrupt
if you notice the title is long which
basically means that came up with it by
combining two different titles when you
start seeing a lot of ads you know that
should kind of give you an interface so
what I'm going to try to do is give you
a kind of a sense of two different
topics one is video segmentation the
other one is video stabilization one of
the core areas that my group works on is
really kind of trying to do stuff on
enhancement and analysis of video I mean
time is one of the basic characteristics
of a signal that most of our work is
about and you'll see examples of that
today so just to kind of situate it
before giving your core of your first
the to the research
questions that my group works on is one
basically is the first question is can
you actually analyze in synthesized
video so we can improve both the quality
of the video and also support its
understanding of the content in it the
direct second question that actually is
also is can we actually use again video
and other temporally varying signals to
analyze what's happening so one of the
areas if we work on is can we actually
understand and ask questions what is
happening in its video sequence can we
model it and actually also predict it so
there's some work we're doing in this
area that's a little bit out there and
trying to also book predicting what's
likely to happen and also assessing how
well something happens I won't be
talking much about that today neither
will I be talking about the third axis
which is the whole area of trying to
understand how and this could be about
images and videos which is again what my
interest is but I'm working with a lot
of other people in both text machine
learning and stuff like that to kind of
understand the whole question about how
do we generate content and how can we
actually validate its quality so today
though I'm gonna focus on two different
ideas one is this video stabilization
that Andrew just mentioned the goal here
is you know I was actually about five
years ago
trying to record my son giving his and
us of course we have commencement
speeches even at your 6th grade and if
you notice there was an earthquake an
action there was no earthquake
I was nervous he was nervous and the
camera really shook and looking at that
video are kind of said well accessed
that the next one is the solution we
actually now have this is something
which we've been working on for a while
well there are some artifacts there but
it's actually much more watchable so
that's one of the bigger questions of
video stabilization that I'll give you
me a little bit more understanding of
what we tried to do here the second
question was you know this is a question
that basically imagine that we want to
be able to find regions and pixels over
video that we want to understand more
about salience e is one of the biggest
issues in computer vision trying to find
the most important pixels or regions
that we want to do something with and of
course we were jealous of a whole are
other P
while trying to do this kind of analysis
but we wanted to be able to do it much
quickly in the kind of interface I just
defined or showed here one click we
won't actually in this instance throw
ground background segmentation but we
won't actually do more than just two
layers you want to be able to extract a
lot more layers so using this kind of
stuff can we do scene analysis can I
learn every actually aspect of
ice-skater
or in this case you've actually seen
results that I'm gonna talk a little bit
about this can we now figure out where
the roads are where objects are where
the buildings and trees and the skyline
is again using that kind of segmentation
but again remember not in images but in
video so that's the two things we will
focus on and gonna go through a little
bit of stuff very fast there's a lot of
technical details on the slide but we
won't get to all of it and and again as
I said if you want more technical detail
find me and we'll talk about it so these
are the people that have been working
with as Andrew said I've also been
spending a little bit of time at Google
these days as a consultant I spend
twenty percent of my time every week day
month to work with them on various types
of things again my goal is research I
don't speak for Google I speak still for
Georgia Tech and myself but I do consult
with a whole lot of stuff that they
what's going on here's what we're going
to try to do first we're going to try to
talk a little bit about super pixel
super walk Sylvester has video
segmentation I'm going to give you a
little bit of technical details but more
importantly what I'd like to do is kind
of lead you up to a system that we have
running so one of the basic charters and
if you notice in the slide is the
stabilizers are working on YouTube we've
also taken a video segmentation stuff
and build up API I've gotten this new
bug in me as I wanna actually do vision
research that actually gets used so
that's one of the kinds of stuff we've
been pushing on a lot so that's what
we're going to talk about so one of the
things we'll end up with is a system and
if you want video segmentation comm is a
system right now available and running
that you can upload your videos and
start playing around with and I'll talk
about some of the goals and we how we
got there this is all work based on
something we did a paper on awhile ago
but this is something we've been
actually continuing to work on and
Addington so let's get started that so
what are the basic questions in images
has been is how do you kind of find a
bunch of pixels next to each other
that have some sort of commonality and
basically built on connectedness and you
know four pixels or eight pixels the
neighborhood and find something in there
and one of the basic things we'll always
use is color distance or some sort of
weighted with gradients but also kind of
do you know some sort of classification
so we can actually combine these regions
to give us continuation or some sort of
region that it's not just pixels but a
group of pixels now of course you need
to take this to video domain and this is
one example that we just basically let's
assume do something like fell to drop
and Hardin Locker per frame this is what
we kind of get so I'm gonna play this
again now of course in this one we're
not trying to build any continuation
between frames we're just saying is okay
let's take one frame run something like
you know I was in Hutton Locker and use
that to define regions but in this when
you notice there is no temporal
coherence one of the biggest things that
I like about the image segmentation
approaches and this was something else
is that many years ago I would never
have thought of working on super pixels
but this one one of the interesting
questions was well as I think Victoria
Ferrari said it one of the good things
with super pixel methods is you actually
know very quickly where you're wrong
because there are usually much more
efficient in computation so that's one
of the goals we had was how do you take
this approach and start adding it the
temporal angle to it to make it much
better and also do understanding of how
to do correspondence or coherence
between them so now of course we look at
the same problem that I had before what
I want to do is I want to connect this
pixel forward and backwards now all of a
sudden this problem explodes so one
second of a 360p video has 90 million
edges it's a lot more than you want to
deal with because for a single image it
was just one image one one image at a
time oh sorry one a 1 million for each
image case so how do you actually do
this connections so that was one of the
problems we wanted to deal with and also
what we wanted to actually say is okay
let's start taking information that
allows us to do this in time so of
course basically said let's start using
some sort of optical flow information so
displace connections in time along a
dense optical flow allows us to start
looking for more information from one
image to the other and actually the next
slide I'll show you what actually
happens for the famous flower garden
sequence if we do super pixels but take
the connections that are optical flow
driven it allow us to connect these
types of regions so here of course you
notice in this one the tree looks a
little bit much more cogent and the
other one there is super pixels rotating
around floating around in that kind of
stuff now of course still you might be
saying is it doesn't tell me where the
tree is and will actually start looking
into more of how to actually get us
towards giving us a segmentation of
trees and stuff like that
this of course the flower garden
sequence many people have spent a lot of
time analyzing into detail I showed this
to somebody like Michael Black and he
actually can tell you which pixels are
wrong I don't know if you guys are very
guys good at that kind of stuff but
again this is one of those well test
data sets another set of examples again
where we can actually use predecessor
flows and stuff like that
again very hard to look at I'm not
showing you the original ones but now
you kind of see notice the difference
between each types of datasets or each
types of methods and different
parameters or different methods one of
the things we want to do is going to
expose all these types of methods and
settings to allow you to play around
with these types of things so why graph
based segmentation well one of the
biggest things that came out of this
work was we want to be able to compute
these types of things fast we want to be
able to also create streaming based
methods and we also don't want to do a
lot of initialization we want to
actually compute as much as possible if
you want to do initialization we want to
do it for the user interface types of
things like where is the skater where is
a coke can and stuff like that there's a
huge amount of literature in this field
some of you may know it all the way from
you know using watershed algorithms to
grab the types of craft based methods
that have been out there but what we
want to do is again leverage just like
people who have done this kind of
clustering methods is play around with
agglomerative clustering so which
basically says is I want to find two
pixels that are similar to each other I
want to basically combine them together
but what that requires us to do is come
up with various types of distance
metrics and also I creative processes to
combine them but also what we need is we
need some sort of cost measures so this
allows us to kind of start
saying is okay I have those two types of
things I can look for those and start
bringing bringing these types of graph
structures out of it of course I have to
also keep an eye on the cost and most of
the time I will put a threshold to cut
things off after certain cost to get at
least a solution now I'm gonna go
through some of this very fast because
the you know these details are again
something I can discuss if anybody wants
in person
today what I'm going to try to do is
basically what we can do is we can
actually look for three different types
of clustering methods single link which
basically looks for the minimum distance
between two different clusters or look
at maximum ones or sometimes also the
average link distances falchion show up
in huddle all could use a single link
I'm gonna skip some of this but this
allows you to kind of start saying is
now we can actually start looking for
these types of distances between
clusters and again this is something
which was in Feld's jatropha not
lockable we need it to be able to do
more because we actually had a lot much
larger graph network that we want to be
able to actually optimize over I'm gonna
skip this too the only thing to look for
is that cost function we need to be able
to model so basically the idea that we
want to do is falchions robin portulaca
is a single-leg our clustering approach
we want to be able to look for local
terminations in the dendogram spacing to
be able to create a cost function and of
course we want to look for monotonic
convergence for criteria a monotonic
criteria to be able to take those two
plus ters and merge them because one of
the things we want to do is merge
different types of things any type of a
cosmetic will work here so how do we
take this to the video based
segmentation well we want to be able to
create local criteria but in that kind
of a context of video local is what the
next frame how many frames that we want
to start playing around with also for
video the volume is bigger a lot more
data comes in and we don't really know
in this case of video what is the size
of the regions that we want to look at
so for practical considerations we will
actually make some assumptions and
you'll see some of those types of things
so this is a case where if you notice a
very simple example homogeneous reasons
not a lot of texture in the scene and we
want to be able to create a segmentation
from it so here you see this is the
segmentation from super woxall method so
we come out with not very clean but you
know you can see some regions are locked
and merged a little bit and again we can
play around with various types of merges
one we can actually force the merges
from one frame to the other or without
it and if you notice in this case the
segmentations are bit better even though
for well defined for Street are
homogeneous regions again remember what
I said super pixel super box's methods
will give you something really quick and
what I mean by quick you'll see examples
of that in a bit when I talk about the
system we did a lot of analysis on this
kind of stuff and the truck example
basically kind of said that we can
actually do understanding of look at
these types of things and figure out
where the small regions are and large
regions are for textured regions of
course we have more merging going on
than we need I'm going to skip this too
and basically this says we came up with
various types of merging criteria and
again I'm gonna skip this because I'm
not sure we're gonna get to the details
of this Wow this slide was supposed to
be a skipped already ok so now let's
talk about hierarchy so one of the
things we've done is we want to be able
to look at these things at different
spatial resolutions we want to look at
larger regions and smaller regions
separately so what we want to be able to
do is and as it's shown by here is we
want to find different regions and see
if they can be connected in different
ways so we can do merges at different
types of hierarchies and this allows us
to take both information from appearance
texture and motion to be able to collect
these things together and segment
regions and super regions which allows
us to basically now do things which so
far we've talked about doing in terms of
video we play around with various types
of features just color flow histograms
and all kinds of different types of
measures that allow us to do this and
actually now I'm not talking about it we
can actually add depth so for example
you can actually add an RGB D sensor and
as additional thing and actually works
with the same hierarchy
and you skip this to tears let's look at
basically the example now again our goal
is to work with complicated videos like
this so here you'll see a big explosion
boom and that's one of the over
segmented results you can actually even
see the explosion happen a lot of things
are changing in this one I do not know I
wouldn't claim this is a perfect answer
but this is one of the answers let's
look at each and every different types
of hierarchy that we can look at
hierarchy at 20% you actually now see
him as a complete one person now of
course as the explosion happens a lot of
texture appearance changes his shoulder
pixels change a little bit on when 20%
hierarchy but it's actually contiguous
here even though has some errors we can
go up the hierarchy more and now
remember this is actually giving you
super pixels not just foreground
background but a whole bunch of them the
same example that I've shown before
again we play around with these types of
hierarchies there many different
examples here this is a great
complicated example again just pick up
any camera play around with this and we
want to be able to create segments out
of this so in this one we kind of manly
to India to kind of have some sort of a
similarity but the answer is on this
side here on the left and one more thing
to come out of it is that we can
actually combine flow flow in
segmentation flow in time flow in space
all of that kind of stuff in hierarchies
across the board and stuff like that and
of course variety of different answers
come now one of the other things we did
is I did kind of touch on it we want
this to work on video and we wanted to
work on stream videos this is a long
video sequence you can actually put a
whole video of a movie to this it does
it on stream basis and I'll talk a
little bit about what we do with this
things too because one of the bigger
goals is we want to be able to find all
the possible layers
and examples this was one our favorite
examples I mean very complicated lots of
motion but if you pay an attention to it
of course thankfully the snowboarder or
sorry waterboarding he's actually
wearing the same black shirt even with
the turn you can actually still keep his
feminine track and you can see water
flowing kinds of all over the place and
we're idea of parameter settings can be
used to kind of look at different types
of things this is another example luck
call it I don't know but you can
actually keep track of him in the middle
again with many different layers coming
out of it so now we have build a system
of course one of our goals was to do
this and use this for scene analysis so
we basically kind of started making this
kind of stuff available and many people
have been starting using it this is a
work on trying to do scene recognition
from swetlana
group and UNC I'm just going to show you
the video which basically uses this as a
method to kind of get the segments and
uses this for doing scene analysis
so they use our system to get the
temporally coherent stuff and now but
just driving a car you can kind of see
you can actually do much better at quote
unquote scene analysis now they can
create labels and the same kinds of
labels that we will play around with
also in a bit
another method this is actually
something we did and there was an idea
presented for images a while back called
geometric context which basically was
attempted looking at an image saying or
where it can be found the ground image
or floor or cars moving objects trees
skyline and all that kind of stuff by
actually first going around taking a
bunch of images labeling where these
things were contextually with reference
to each other where the trees were where
the road was and all that kind of stuff
and once you do that you can actually
now take any image based on what you'd
learn from prior images you can actually
start doing labeling not just
segmentation based on the segmentation
well we want to take the same idea to
video and that's what basically this was
another cvpr paper that lets us do this
where we basically take images label
them or sorry videos label them using a
tool we came up with and after we label
it we can actually now start classifying
regions and aggregate the predictions
over different types of hierarchies
again remember we have hierarchical
representations available and now we can
actually get a system that allows us to
take scenes like this and classify where
the roads are where the possible moving
tracks of cars and such would be where
the trees are
and with the buildings in skyline are so
this is approximately the approach and
one of the things we have videos you
don't ever know how long it's going to
take to do this of course classification
stuff we also build this tool that lets
you do and this is the answers that come
out of it this is a tool that allows you
to do labeling so now you can go into
video take one image rub over certain
things remember when I showed you this
example of the ice skater this is
we're coming up with is now we can label
different things it's almost like a
painting brushing interface for those of
you working graphics this is the kind of
stuff has been done for using
rotoscoping and all that kind of stuff
for building really nice models of
different types of things here you're
not gonna be as accurate as rotoscoping
but we basically clicking on a super
pixel at different hierarchies and
propagating this both in a hierarchy and
in time and I'll actually show you this
is something which we've been extended
even further
another simple idea that actually we
took advantage of this is working with
Ringling hang at Microsoft Research
Cambrian sorry in Redmond we wanted to
be able to do the same thing but one of
the biggest problems is we want to do
this with handheld cameras but handheld
cameras if I take a picture video here
and move it there because of auto gain
controls and all kinds of stuff it gets
brighter and darker we want to be able
to do radiometric calibration from the
data itself to be able to then separate
out and do segmentation that actually is
consistent so we basically build this
whole system to actually support things
where we can now do segmentation so here
is basically if you notice the original
sequence is changing because of game
controls but we can calibrate it and
segmentation now works because remember
one of the metrics we're using is color
values and all you know the information
that's basically the histogram of colors
so in these things segmentation now
actually is also sensitive but at the
same time dealing with the fact that
Auto gain and such will change the image
another example of work we did on the
same thing but we took it to the YouTube
scale this was work with people at
Google is that if you now take an input
video of let's say a dog and somebody
goes in and paints from these videos
which pixels belong to a possible
location of a dog and you use that as a
trainer and now we can start classifying
new videos come in now of course when
that basically we will start kind of
looking for both texture and appearance
over time that might resemble a dog and
we can do this for variety of different
types of regions I mean basically again
in this instance we do also use a
stabilizer which I'll talk about next
which allows you to find the salience of
where the most interesting things are
then
activate the segment's regions and stuff
like that and allows you to basically
take all of these types of different
features remember now the regions is one
more feature we can play around with and
use it in there for the learning
framework we can now find regions so
this was the data set we had and now you
see stabilized input and our output find
course this is basically both stabilized
and segmented and then basically you can
start kind of looking at a video based
on the data
find the regions and it labels is as a
dog because these pixels have some
similarity to classifier we'd built for
that kind of stuff again sometimes you
know if you know it doesn't get the
whole dog not each and every pixel but
it gets a majority of the types of
pixels and we did some elaborate studies
on it this is something we're actually
now working on a longer version of the
paper that should come out soon so what
comes out all of this is now we've built
a complete online segmentation
annotation suit a suite of software for
this goal is we want to be able to make
it available people can actually do
their own segmentation play around with
this initially it was running on our
work coming on a couple of machines in
my lab right now it's entirely on the
cloud I got Google to support a complete
cloud-based architecture for it and
using this we can now build a system and
I'll show you examples of this I'm gonna
skip some of the details we came up with
the lots of approaches to make it faster
and all that kind of stuff
and one of the basic ideas was that we
basically could take a video volume like
this compute on 30 frames find an
overlap of a let's say 8 or 10 frames
solve for this but now use that as a
constraint from one to the other so this
would be for example a constraint system
that overlaps two clips and allowing us
to use this kind of computation to be
able to do this in much more of a stream
based clip basic things which basically
says we compute 30 frames take 10
compute 10 thirty frames and keep on
overlapping it we never look back
we always go forward I'm going to skip
this do
this was actually one of the ideas that
basically so this for example one of the
things that I meant the reason I
mentioned sweat Lana's work was they
actually wanted to do scenes like this
and one thing they found out was that
pixel grouping as you go forward and
backwards are problematic so but
basically they can't actually take those
regions separate them out what they
learned was and actually this is an
insight that we actually benefited from
that if they actually ran the video
backwards the segment's were more
coherent because we they were actually
kind of looking for things that kind of
evolved on their own so we actually
added that kind of stuff into our
framework so if you notice this is going
backwards the regions are a little bit
more coherent so we build in a little
bit of back and forth but not much
because we still wanted to do this over
time and I'm gonna actually this is all
stuff available for anybody who wants to
watch it or actually wants to use it I'm
gonna get to the system this is the
system that works when you have animated
slides that take a lot longer to go
through yeah okay so this is the
approach basically we compute flow we
over segments we have hierarchies again
this is done entirely for a video volume
we actually also account for
paralyzation a lot kind of stuff okay
this is skip all of this as I said I did
have two talks that I'm merging into one
and I forgot to control all of the
animations out of this
I'm gonna go the easy way
so this is the system video segmentation
calm that basically you can upload your
own videos I'm going to show you a live
use of this here hopefully yeah so you
can basically the only thing is you have
to log in you can upload your videos you
can create all of your own personal
settings that you want on some of them I
talked about and save those settings
it's running entirely on the cloud
I lost my mouse
where's my mouse there okay move next
one
so this is the video I'm going to fast
play through this remember this video
we've seen many times you can segment it
after you've segmented it you can
annotate it so now you notice that I can
go in and point at different things
label them on the web look at different
hierarchies when you label them it's
basically picking up the segments of
different hierarchies that you kind of
can collect
you can save the video for yourself
download things for yourself if you want
all of that's completely as I said then
basically also what we've done now is
made this completely available as a
source code anybody wants to go in
download it's available across the board
basic idea again is you can actually
take a bunch of videos run the code get
the segments one of the things are the
reason I'm actually talking about this
if you actually have your own video
segmentation code we'd like to add that
into this hierarchy so we would have
multiple types of solutions with this
alright this took a little longer than
I'd hoped but partly it was my fault we
can actually also give you access to the
code and you can actually basically
generate your own types of api's or use
the API to write C++ code on this kind
of stuff let's get back to this now
topic of segmentation these are again a
series of papers we've done on this
topic this is also work with Google in
fact this one is actually much more
Google work than Georgia Tech work it's
all protected and all that kind of stuff
but it's all publicly now known because
it's something that's been running for a
while in ER I'm gonna talk a little bit
about some of the ideas that we've been
playing around with the goal again is
you know this is a GoPro video again an
earthquake because the surgeon is
wearing this GoPro on their head and
running a marathon this is the Boston
Marathon
now of course Microsoft just showed us a
beautiful way of summarizing of course
we're not doing summarizing or trying to
take and reduce this we actually want to
show each and every frame this is the
output of our approach and the two side
by side this is a system that I kind of
also said is now available on youtube so
if you upload your video and I know
Andrew told me that he's used it if you
upload a video like this it'll pop up
and this is the whole suite now
available where you can do various types
of enhancements on this suite this is
the much more elaborate version of it
and of course you can do things like
filters and colors and all that kind of
stuff which is not built-in but there's
one button which is auto fix which
actually will try to enhance it and
stabilize and if you notice the 5
seconds and now you now have real-time
preview and you have an interface where
you can now see the original or the
stabilized one
by just moving that slider this is
real-time preview of course a preview is
about half the video NTSC video
resolution this actually does work on
full h HD or by the way the thing I've
skipped over a whole lot our video
segmentation now also works on Full HD
it doesn't just work on NTSC video
signals it works on i-ight resolution so
let's talk a little bit about how we can
do this kind of stuff and of course you
can run this on your own play around
with these types of things and you know
various things show up one thing I'll
talk about in a second also is when you
actually upload a video like this it'll
actually show up several of the time
saying hey we've detected that this
needs to be enhanced and we will
actually then so if this thing will
popped up we detected your video maybe
shaky
would you like us to stabilize it so
that one button is the only input the
system requires these were the
requirements from the YouTube
enhancement folks we actually initially
came up with that hack several Striders
and all that kind of stuff they said
nope our users don't like sliders has to
be automatic only yes and no and that
was the user interface and the design
they developed we actually build the
algorithm to support it so how does it
work well so one basic thing that comes
up in this is we've actually simplified
the whole problem a lot and way we
simplified it for course started looking
at the literature one of the best things
we saw is of course there is a whole lot
of work out there in stabilizers
sometimes it's the hardware solutions
which are built in camera types of
things or there is of course
post-process stabilizations out there
which removes low frequency information
or just does everything in the back end
we were much more on the post-process
side of the world we wanted to really I
mean our requirement was that this would
be something we wanted running on
YouTube we want to be able to estimate
the jerky motions we want to smooth the
camera path so basic idea was we wanted
to be able to find a camera path and try
to smooth that out to the best of our
ability so that's what L 1 and L 2 types
of optimizers were used and then we want
to be able to synthesize a new frame so
here's the idea this is the original
video now few assumptions that act
they are true these days on the world of
YouTube may not be true in the
professional video domain is most of the
time of these types of small videos you
will try to keep the subject in the
center - most of the videos would be
wide-angle nature of the beast again
true with GoPros true with your
smartphones and stuff like that so how
do we actually now leverage this well we
want to do is we want to find a viewport
in this larger video and we want to be
able to find and stabilize the best
region out of this and we want to be
able to synthesize those frames so this
is basically one of our outputs on the
earliest outputs so the red marker
basically now finds the best possible
viewport within an optimization
framework and displays just that so yes
there's a little bit of cropping now one
of the biggest questions we get asked is
oh why do you do cropping well and again
if you notice there are different types
of things will show up in here's one is
why do we cropping and by basically we
don't want to go outside the window
because if you took this video out and
kept the same size we would have to do
some sort of in painting there are many
solutions for doing in painting but most
of the time there is a little bit of an
artifact that shows up we decided that
was something not worth it for us so
that was an assumption and in kind of
the design decision we did if you notice
in this case the window is getting
smaller and bigger there was one more
thing we added how do you kind of keep
the size of the crop window now all of
you who know computer vision pretty much
easily kind of starts saying is OB
basically trying to look for
translations rotation scale and a little
bit of filmography and 2d to be able to
model all of this and you'd be right
because that's in essence what we do is
we basically kind of estimate the
motions by looking at again nothing else
but optical flow types of scenarios and
looking in information so here for
example in this video we're basically
computing motion from frame to frame and
using that to compute different types of
information that could be allowing us to
now take this frame or this viewport
and stabilize it within a sequence so
much simpler approach but there
few other snow things we have to do to
make this work so of course here a
couple of things to look at is if we
just completely looked for
foreground/background no not paid to
attention to foreground background and a
car drove by it might move the camera
with the car we don't want to do that so
we did put a little bit of a thresholds
to look for foreground verses background
motions so we didn't want to separate
that out now remember previous work I
talked about with doing video
segmentation part of the motivation was
to get towards that but that was still
too computationally costly for us to use
that here so we have much different
smaller metrics for doing this kind of
just foreground background segmentation
here so examples of this kind of stuff
is you know we want to be able to do
this with as few degrees of freedom as
possible so these are the kinds of
videos that you may upload to the system
and you know if you run things like you
know simple klt types of crackers on it
you would get motions like this or data
like this what we want to do with this
is extract out translation of course
which basically kind of says is all we
want to look for how this region moves
over and x and y if we just do this on a
video frame you are fine it might work
but it's still gonna be shaky as you
notice on the top frame after just
showing you the result so just simple XY
translation is good but not efficiently
the right answer the next part is okay
similarity translation in both x and y
we will also want to look for scale so
that's why our window needs to get
bigger and smaller or the viewport needs
to get it's more 4 degrees of freedom
and this one now actually you can start
seeing it's got a rotation it could get
bigger and smaller again using 2d
information as much as possible but now
we want to also kind of look at so here
the answer is yes it's there but if you
notice it's got a little bit of wobble
because the warping artifacts are coming
in third answer 30 states now we
actually look for homography
translation x and y axis rotation and
scale all of them included more degrees
of freedom 8 degrees of freedom we
actually now can capture skew and
perspective variations this is a little
more stable so this would be our
simplest way to get things done right
and actually one of the things we added
to its various ways of trying to
optimize over the path so here if we do
an example looking at this thing is
looking at this frame computing over it
the blue line is approximately the path
over the frame both in X then in Y now
we can add scale so this allows us to
now start kind of putting constraints on
how to smooth out these paths of course
I'm still keeping it a little bit simple
and how I'm explaining this but it'll
gray and grow in complexity so now we
want to take those paths and smooth them
so here for example would be our
different paths right approximate
original path the red line is where the
original path might be me one actually
now compute the blue path now what we
did was we added constraints to it and
we understood a lot more about cameras a
camera that's basically kind of on a
tripod will actually just do very simple
kinds of tripod fixed motions the other
one which might be a pan or one that
might pan and come back are different
orders of frequent hooks different
differentials on this information so we
can secretly we can look at a tripod
which would be fixed which would just be
a straight line we could look for is
some sort of a tripod with dolly or pan
which would allow us to look for these
types of additional constraints and then
we could actually figure out how to go
from one to the other and add various
types of motions through it and again
what we would do is actually add
parabolic segments so here would be an
example original video and now what
we've actually done is we found the
whole path the envelope are all possible
motions that are constrained by the
original viewport because that's the
maximum I could go and now we have
basically a variety of things so you can
actually imagine this also gives us a
sense of how big our crop would be so
another thing we added was automatically
figuring out how big a crop window we
can actually yeah so again one of the
things we added in these types of things
was various types of optimizers and we
could play around with
and we could play around with different
types of cost functions we could use to
figure out what would be the best
possible path now one advantage we had
was this is a purely data-driven system
we don't know what camera is coming from
and we can actually analyze a lot of
footage to figure out what kinds of
camera motions are good for different
types of things so we actually what we
did was initially we even sat down and
digitized a bunch of very famous movies
and looked at clip 'let's of that and
learned these kinds of what are the best
set of parameters we can use to model
this and basically our martian thing was
that within this envelope we want to
remain data-driven but at the same time
find the optimal path through it
of course then we want to be able to
synthesize that best frame now a couple
of things I'm going to skip over some of
them are dealing with things like motion
blur some of them are dealing with
things like some blurry artifacts that
come because you know optical flow kind
of warping messed up or different types
of things we are various types of ways
of finding filters to be able to adapt
to them but if you notice the top one
the window is getting bigger and smaller
and if you notice I'm showing a lot of
our own videos because you know this is
a presentation we have to actually have
a lot of our own videos because you know
otherwise we get in trouble or publicly
available videos I can show in talks
like this so here is in fact this was
one of our first few results but four
years ago publicly upload it original
with crop and that next one is the eight
result now of course you'll notice her
head might get cropped off I don't know
if this is the video you know this one
it doesn't but in several videos yeah it
might be a little bit more cropping than
you need
of course we've done a lot of user
studies and also say that people
actually do prefer some of the
stabilized versions and of course all of
a sudden we have now a huge amounts of
data that we can play around with so
this is for example one of the classic
examples we played with before very
shaky video now of course this actually
has other artifacts that are not just
because of the car I mean this has
rolling shutter and which will be
resolved in a bit this is somebody
wearing a camera on helmet biking around
a motorbiking and I think in
somewhere in Middle East so let's talk
about rolling shutter
so we basically started getting really
kind of getting into this word and is
that this was about the time and more
smartphone videos were coming up on
YouTube and smartphone videos have an
interesting problem this is of course us
using a camera and rotating it and you
notice a lot of non-rigid in the scene
right the beams look like they're not
religion at all of course in this camera
the camera is being rotated and that's
so one of the worst examples of rolling
shutter so next to each other
comparison this is the original video
I'll let this play by the way for those
of you curious the next video that looks
stable is actually still a video I keep
see the person is walking so that's one
of the and there's a little bit of shake
at the bottom that you can see so how do
we do this of course in this one we also
may want to be able to take the camera
motion and from that camera motion be
able to compute things that allow us to
Naru for example sorry I skipped through
this fast one of the bigger problem with
rolling shutter is that we need to
account for the fact that by the time
you come and you're scanning down your I
mean your image might be at a different
location because or your camera might be
at different locations how do we account
for those types of things we need to be
able to then kind of model an image into
various types of strips right so in
essence one of the reasons warp is
happening is by the time the camera is
registered different types of things the
camera might have moved a little bit so
global shutter would have kept that
image straight rolling shutter of course
if you notice the subject is a little
bit warped and if you imagine moving
those types of things will be worse so
this is basically when we need to
account for and you know this has been
of course work done before our Simon
Baker from Microsoft Research has done
work on this kind of stuff this was the
original video so when we actually
initially developed our YouTube system
this was the output from our system
didn't account for all Saturday
so we basically came up with through the
data kind of understanding or modeling
the readout variables because we don't
know exactly what the camera is doing we
want to only get it from the data itself
so again we model this kind of behavior
by looking at now strips and computing
the homography for each and every one of
those strips so same kinds of stuff that
we did for the whole window now we do it
for small strips and using that we come
up with measures of how do you actually
compute the homography of course now we
have to down leverage although some
biographies back into the system this is
a solution from Baker at all improved
over the original but now after rolling
shutter now yes there is a little bit
more blur because we are doing a little
bit of blending of different types of
things but in this bad quality video
it's a little bit more stable there
might be a little bit of floating of
different types of things going on so we
model the whole electric kind of the
shutter phenomenon from data and this
results are kind of going through much
more detail and I'm gonna again as usual
skip over a lot of stuff we compute
again optical flow different types of
things and here is basically we predict
based on knowing from the data where
each pixel is likely to be from one
frame to the other or where these
registration marks might be again so we
predict a lot of this and actually done
using again purely empirical information
from the data itself and we can figure
out from that where are the likely warps
in d warps that are based on this take
the different homography create a
Gaussian mixture of all homography and
then use that to bridge astir back onto
the original image again original with
homography mixtures little crisper now
if you notice there's a problem here we
are cropping so that's why you see the
label floating in and out on the top
that's one of the artifacts of cropping
this is one of the phenomenon of just
having text done before we also looked
at data just like we did for other types
of things and look at which homography
parameters are the one
that get varied most on online videos
just emphasize those which allows us to
do a lot less computation so we just
look at in and reduce the degrees of
freedom much significantly and of course
now the whole pipeline is allowing us to
go from input videos look for
similarities but also look for warping
and we can do it at different
frequencies and allow this to do warp
and crap open cropped crap and then
basically use this to remove wobble now
there is work on this kind of stuff
before many people have done this kind
of different types of approaches to do
rolling shutter and when ours is
basically ma graffia mixtures and more
importantly it acquires no calibrations
purely data-driven we also did a full
user study on this kind of stuff with
looking at variety of things we you know
showed people each and every example of
course the labels here are is Baker at
all and all that kind of stuff are not
existent in the study by the way notice
if it works for images and videos like
this very less features and stuff like
that it's dark compared it and of course
I wouldn't be showing this to you unless
and until our results were which are of
course unread and it works on different
types of data
iPhones and walking with are you know
diff GoPros and stuff all the way to
playing around with and these are the
kinds of results again very complicated
this was a explosion in Maryland
somewhere about few years ago somebody
upload a video if you notice not a lot
of good texture here to lock into this
is the result still not perfect a lot of
blurriness but if you just compare them
next to each other
oops played it fast
oh one thing which I'm not talking about
today is and if anybody's interested
there is a full API available if you do
any kind of coding on YouTube to use the
whole software system that I talked
about to really prove from Google if
anybody is interested just you know ask
me or just search for it it's available
there just to show you again this is the
interface on YouTube shakey pops this up
with a question saying would you like to
update it preview then you hit this yes
Save Changes this will save of course
the previewed version and then will
actually save in a few minutes the whole
HD version of your video and you can
actually download it one of the biggest
questions I get from people is oh that
means Google has my video well yes but
you can download it for your workstation
also I love the interface that comes out
of this is that we can actually
visualize this kind of stuff very
cleanly I'm not going to go into the
details of the pipeline but basically
this is running over a huge cloud many
many computers are used to support this
kind of stuff first of course the
estimation is done then stabilization is
done then back to a live stream to user
that's why usually when you upload a
video it takes like about four or five
you know milliseconds or something like
that or almost a second for it to pop up
and again you know this is the kinds of
results that we have gotten this is
again you know not our video
of course I November and you send me one
of his skiing videos also not a lot of
texture here but and the thing that
again the same idea that I showed you
with video segmentation it's actually
clip based and it can run on infinitely
long videos so there is no bound to it
and there is no back-and-forth and here
actually this is one of those examples
because if you notice the viewport is
getting smaller and bigger based on the
content again yeah we've actually run
this on a couple of hours of movies not
just segments like this
a couple of addition things that have
come up is if you do these days
stabilization and if you have an overlay
like this because people can add
overlays it's a disaster this is
actually the video of me hooding
Matthias grenman the person who did all
the work for his dissertation and it's
not so now what we can do is we've added
abilities to look for overlays detect
overlays and not run the algorithm or
change the coding a little bit to
actually keep the window much bigger but
also accounting for the stabilization
that comes in so here you notice it's
shaky but it knows that it's detected
automatically I did that there might be
something else much more fixed so it's
shaky a lot more but once the overlay
goes away starts doing the optimization
yes and if you actually here you notice
a little bit of the example of the blur
I was talking about there is a little
bit of blur and actually we there is one
enhancement that we have added since
that actually gets rid of the blurred so
if you run it now some of the blurred
stuff is better all right so this I'm
gonna skip but on actually this is one
of the basic ideas right now the numbers
are such and these are publicly
available numbers 100 hours of video is
uploaded to YouTube every minute but 10
hours of video every minute is using the
stabilizer after all of the hundred goes
through the check saying hey it's a
stabilized or not the good thing that we
like about it is that many of the people
who actually get the stabilized result
ninety-five percent are actually are
keeping it some of them are turning it
off five percent of them are turning it
off and you know people are accepting
stabilization a lot more out of this
some of them are actually must admit I'm
surprised they keep it because I look at
the results and I said I would not have
kept it but people like wow this stuff
is now also inkay put into other types
of things so if you upload images with
burst photography on Google+ these
things are also done so this is when you
do a burst photography like this using
the same approach we can create an auto
jiff which is basically aligned and
created an animation version of your
child we can also do exposure
HDR same approach has to be of course a
burst really frames close to each other
and also much more scary stuff like this
so we can actually make you know people
create auto gifs and kinds of stuff out
of it lots of other types of things are
also showing up using this kind of again
this is the final result I think this I
like a lot these two I like a lot
they're cute the face stuff really does
kind of creep me out but it's there for
those of you who actually play around
with it many videos like this now show
up where basically again this was a
professional car driver and you notice
the difference shaky video from a car
driver in the car versus the stabilized
version next to each other various types
of things come out of this we've seen
some amazing videos we have to always
hunt down and get permission from people
to share it but we've been doing a lot
so in summary basically again I've told
you two different systems one a video
stabilizer that you can play around with
or you may have actually already used
you might not know it which is actually
very good then the other thing I showed
you is this kind of a much more research
east of to actually do localization of a
group of pixels in a video and then use
that for doing scene analysis we played
around with those types of things I mean
to me this still remains an interesting
question can I actually now do scene
analysis or behavior analysis with
multiple layers of regions over time to
be able to then find all possible
activities of motions of ice skater or
scenes like this purely the kind of
stuff we've been doing in images and I'm
not saying that those problems are
solved in images they're still hard in
images but we can actually start doing
those types of things in video more
information look at my website all my
papers and more importantly the links to
my students who have done all of the
hard work aren't there and you can hunt
us down and more importantly just play
around with both of them video
segmentation calm is live it's on the
Google cloud initially you could hack
when you get there and many somebody
else was using it you would be put on a
queue now it just spawns off another
cloud workstation to do it in that I
finished almost in time thank you
we will use the Kinect camera and also
variety of other types of RPG rgvida
cameras including the new were Kinect
version also so basically we if we can
now I mean as long as the depth can be
aligned somehow the other reason these
cameras could be we can actually now do
both the scene segmentation but also go
towards classification what kinds of
examples I showed you basically that's
exactly what we do that's exactly what
we're doing I mean basically this is
another modality of doing things like
connections super fixes are a very
useful intermediate representation but
one of the problem with phobics is is
that it's hard to assess the quality
independent of the tasks that you will
use the super fixers force for example
in a semantic segmentation applications
you mentioned sinks like temporal
continuity or over segmentation may not
be as critical if you can still slope
for the semantic label give me some
acceleration can you say a little bit
how you want to connect the how you
could connect the things that you have
done to sort more the tasks so basically
for example adoptee super big sensation
to a task so I mean so one thing which
you're leading up to is you know simply
if I create this hierarchy across an a
video sequence and now I can for example
see a coke can or I can see the coke can
with the coca-cola written on it and the
swirl on it differently in different
hierarchies how do I connect them up
into one semantic object so we actually
are kind of more moving towards asking
that question and actually some other
people are also I mean I think Baron
Chile's group has done some nice work on
that kind of getting to the semantic
label what we are providing right now is
a tool to let you kind of create those
semantic labels so now actually if you
can go in and look at a coke
and not create a hierarchical
representation that combines the
coca-cola text and the red region over
time that'll allow you to create a
semantic label and then you can detect
it so to be very clear when I'm program
presenting this video segmentation I'm
presenting this is an additional cue
towards doing semantic analysis of
videos I don't think that's the final
answer you need to do a little bit
better representation of connectedness
of an object both in space and time I
mean super pixels as you notice if you
look at it as humans we are very good at
figuring out where the person was and
the outline was but most of the time
they're broken up so in that case of you
know no country for what was that moving
no country something something for all
men I mean you basically even though he
was bearing a different shirt and a pant
converted himself into one person some
extent if you really look at in super
pixel that was the wrong answer it
should have had the different shirt and
a different pants but the colors of
those things were similar that's why I
became one so those types of things
remain a problem we actually still are
we actually working on methods right now
and I'm happy to talk to anybody else
that lets you figure out better ways of
which hierarchical to choose so right
now we don't know which hierarchy to
choose right which way where do you pick
up from over segmentation to under
segmentation that whole space is
available so they basically kind of
point and again look at different
hierarchies in their annotation and
choose the one that best suits them so
the user is the one that's defining how
to connect it but there should be a
better way of doing that if you have
more data and actually that's the kind
of so we're working on I know so Atlanta
is working on that kind of stuff do now
so where lots of people are trying to
get to it I know some people here also
at Oxford are working on that red
Cambridge are working on that kind of
stuff too China have that answer is a
question but I'm basically looking for
solutions myself for that type yep just
a short question when you do this
rolling shutter business with the
mammography do you relate them like
because I would expect them to be
related like if you have one shift from
from like the highest block to the
second you would expect some
to the next strip so you do relate the
human face are they completely
independent so they are connected
because we know about the readout where
each and every marker fee is so we look
for the highest resolutions by doing the
Gaussian mixtures it actually
automatically selects the one that
actually has the most perturbation now
if for example there is a huge so for
example to be very honest when you do
rotations like this it can predict very
quickly what's going to happen because
now it's figured out from the rotation
of the camera the angular flow so the
mixtures work very well because it can
look for those types of things but if
you just kept on doing large motions
like this which are not actually what's
something they can deal with then you
know let's see we see a lot more
artifacts so what we do is arts windows
can get bigger and smaller
there is no sub regions but we for
practical purposes to kind of keep it
that you look for few neighborhoods next
to each other in our much more detailed
version which is not anywhere close to
the speed we can do a much better job of
looking at the mixtures but for
practical purposes we eventually kind of
emitted neighborhoods that allows us to
look for specific types of motions that
answer your question yeah more the
homography sphere do you cost like a
difference in the homography s-- that
you because you have different
homography for every step exactly so so
dude do you have like a cost do you use
the information like let's say the first
two strips like the motion that you have
in time between vision group for the
next ones yes we do
it's completely temporal so we basically
looked for a joint so it's a completely
temporal measure looking at it from one
frame to the other and across the thing
and that's how it keeps it smooth and
actually that's why some the blurring
comes in because it's doing some sort of
a filtering in time and therefore some
pixels get blurred and that's why we
have to then look at the warp that comes
in from the mixture and unwarped it to
get a sharp mask again
anything else sorry there was a lot of
stuff in there but Carla and I ready for
lunch now and if you'd like to get some
time enough let's call them to later
around this afternoon so if you want to
talk to him email Polly each year
Microsoft Research helps hundreds of
influential speakers from around the
world including leading scientists
renowned experts in technology book
authors and leading academics and makes
videos of these lectures freely
available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>