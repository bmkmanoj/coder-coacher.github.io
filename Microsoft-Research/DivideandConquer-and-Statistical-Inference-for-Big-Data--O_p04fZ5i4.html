<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Divide-and-Conquer and Statistical Inference for Big Data | Coder Coacher - Coaching Coders</title><meta content="Divide-and-Conquer and Statistical Inference for Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Divide-and-Conquer and Statistical Inference for Big Data</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-O_p04fZ5i4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you I'm delighted to be here it's
been a great day with great talks this
is my second time in china in two months
talk about big data it's a phenomenon
all over the world and there's lots to
do if you're a student looking for
things to work on this problem is just
getting started so one of the main
things i want to do today is introduce
some of the conceptual issues having
surrounded big data it's the kind of
field where one person sitting in a room
thinking about the problem can actually
have a really big impact which seems
kind of surprising because you think you
would need huge numbers of computers and
huge amounts of data to work on the
problem almost by definition and I hope
by the end of the talk you'll be
convinced that that's actually not the
case so there's a phenomenon which I
don't really have to spend much time on
I think everyone knows about it there's
huge amounts of being data being
collected and John hop crosstalk sort of
reminded you of some of this one of them
for factors is big science generates
huge data sets astronomy particle
particle physics or it's good examples
genomics the other is information
technology which is the one which is
perhaps more present all of our minds
activity on the internet generally
massive data for personalization and
then linking in these sensor networks
are becoming Perseids so that's the
phenomenon but what's the intellectual
problem really is it just big is it just
store more storage more capacity more of
everything that solves the problem I
argue known so computer scientists think
about the world in terms of resources
the classical resources have been time
and space to sort of underpinned the
field for all these years and energy to
some degree data has not been thought
about really as a resource it's a thing
as you apply resources to okay but
what's new is that data really now is
being viewed as a resource something you
give me that I make use of and get value
out of and I get knowledge out of it so
if it's a resource we have to combine it
with all of our other resources like
space and time and we're worried about
the trade-offs in designing a system
that takes into account that we have
some data resource we have some time
with some space and they trade off in
various ways
the funny thing about data though is
that it's an unusual resource it should
be the case with any resource the more
of you give me the happier I am all
right if I give you more time more space
you're happier and happier and with data
that's not the case and so if you go
into a company and ask them what's their
biggest problem the problems they have
too much data and so in our current
state of knowledge its really really the
case that more data causes more trouble
and we have to work around this issue so
there's two reasons for this one
statistical and the other is
computational the statistical one is it
may be a little more subtle so let's
spend a little time on that so let's
think of like a database person thinks
our data is is an array a table the rows
are say people and the columns are
descriptors of people so classical
databases maybe you had a thousand rows
thousand people in your database how
many descriptors would you need well
maybe not a few no more than a few say
where the person lived their age their
height their income and that's enough to
kind of make some of the distinctions
you might care to make in a database now
we're thinking about billions of rows
because we're really interested in all
the personalized details of all of you
so if you live in tianjin and you like
to ride bicycles and you love Michael
Jackson what's the probability you're
going to click on my ad or what's the
probability that you could have a
certain disease or etc etc where instead
every single one of you so we have data
about all of you okay so the number of
rows are growing somehow linearly maybe
the number of columns have got to grow
too because we're interested in all
these distinctions maybe some of the
columns are your genome their what food
you ate yesterday they're your
preference in music what books you like
to read and so on so forth will collect
all these things the problem is that
we're interested not just in the
individual columns we're interested in
combinations of the columns okay it's if
you live in tianjin and you like to ride
bicycles and your favorite food is kung
pao chicken then will you do something
that's a particular combination of all
the columns and the problem is this i
have an exponential number of
combinations of the columns so if i'm a
linear growth in rows linear growth and
columns i have an exponential growth in
the number of
I'm considering so think about a medical
example let's suppose one of the columns
is liver disease all right so I'm
interested and I look at all the ones
every time I see liver disease that's a
one if it's a zero you didn't have liver
disease so let's now look at all the
cases where their liver disease there's
going to be some combinations of the
columns which is going to perfectly
predict liver disease all right maybe
it's you live in two young Jin and you'd
like to ride bicycles and you eat
bananas all right every such person had
liver disease all right so now you
arrive with the doctor and they say
where do you live and you say to young
Jin and you say what do you like to do
on your on Saturday ride bicycles what's
your favorite food bananas and say we
have to take out your liver oh I'm sorry
all right and so the problem is that in
any data said if we have exponential
number of things we're looking at and
linear numbers of items in which to
verify those things we're going to find
meaningless patterns just by noise alone
all right and so it's Dana get bigger
and bigger and bigger this problem is
getting worse and worse and worse okay
so big date is not this wonderful thing
give me all the data i call the
knowledge in the world big data is a big
problem give me this data it's harder
and harder to turn in at the knowledge
real meaningful things that are real
then we can act on and believe ok so now
statisticians worry about these issues
this isn't new how do I get rid of the
noise and give her the bias and get the
knowledge out but statistical procedures
are themselves algorithms and they take
time and we have to run them on the
computer and maybe in big data sets they
take too long all right I can't make
quick decisions anymore so we really
have a really big problem on our hands
we don't know how to run statistical
procedures and make good decisions
quickly at scale all right so in fact
the second reason this is a long slide
but it's an important set of points the
first reason is statistical the second
really is computational algorithms take
a certain amount of time to run say in
log in and cubed and squared all right
suppose I need to make it as a decision
in a second like in an online auction or
less than a second all right you start
giving me a certain amount of data and
my in log n algorithm it's perfectly
good it'll run in the one second but now
you give me more and more data and
suddenly in log in its not going to
finish in that allotted amount of time
what do i do I
maybe throwing away the data but what
rate do I throw away data it could be my
statistical error will grow if I start
throwing away data too fast so I have to
throw it away more slowly but now it'll
take me too long all right so we really
have a big problem when we bring
together time with statistical error
with growth and data set sizes we don't
have algorithms that will gracefully
scale to larger and larger data alright
so this these are real problems if you
go with the industry and look at groups
that have really large data sets these
are the problems that are facing and I
view these as fundamental and difficult
here's one way to say a theoretical goal
which tries to get at this give an
inferential goal that's the
statisticians kind of goal usually a
risk function and a fixed computational
budget a minute of second an hour
provide a guarantee that the quality of
inference will increase monotonically
has data crew without balance so without
balance we like a computer science I
don't have to face this problem every
generation I want to face it once and
for all what are the principles that
scale statistical risk against time I
think we're very far away from being led
to solve this problem I think this is
going to take several decades to solve
so I'm gonna talk about little bit of
progress we've been making women working
on this one of them is kind of a
bottom-up problem we're going to try to
bring algorithm principles more fully
into contact with statistical inference
and I'm going to go to my favorite
algorithm principle which is divide and
conquer and tell you about how it
relates to statistical inference and
it's it's a interesting almost paradox
statistical inferences about aggregating
things bringing things together the more
things you brings together the small the
error bars are the better things are
divine conquerors about somehow
separating things out so they somehow
fight each other the basic ideas and
statistics and computer science or
sometime a little bit in conflict and
that intrigues me so I'm going to talk
about that and then we're going to go
back to this more theoretical problem
and we'll talk about how to trade or was
it me the trade-off statistical
efficiency and computational efficiency
all right first problem something we
call the Big Data bootstrap so
acknowledge some of my colleagues at
Berkeley I've been working with this on
this for a couple of years now so the
bootstrap is a solution to a really
important problem which is to assess the
quality of inference
most machine learning research doesn't
worry about the quality of inference you
have an input it goes into some box of
some kind and outcomes an answer say 9.5
all right but most statisticians aren't
satisfied with that they want to know
what the Arab RA is on that 9.5 so for
example if the number was bigger than 10
you're going to take out your liver okay
and the number came out 9.5 all right
well is that really 9.5 is there a big
error bar is it possible that it's
actually bigger than 10 or is it really
clearly not bigger than 10 all right to
make real decisions in the real world
you have to have error bars all
engineers know this and we computer
scientists should know this as well so
I'm in fact worked with people in the
boot in the in the database community
trying to build databases that query
comes in outcomes an answer with an
error bar router on every single query
that's what we should be doing as
real-world engineering oriented people
error bars everywhere okay so let's turn
to a different field statistics and ask
how do you get error bars on things well
this is kind of interesting on very
simple things like the sample mean
there's a formula you can take an
elementary stat class and you'll learn
that if you calculate the sample mean
and it's 9.5 there's something called
the standard error of the mean which is
a formula it's something like this
sample standard deviation divided by the
square root of the number of data points
and so you plug that in you get an error
bar alright but what if you didn't
calculate the sample mean what if you
calculate the median you sort the data
you find that thing in the middle that's
called the median and that came out to
be 10.2 what's the error bar on that
well there is no formula alright so how
do you get an error bar on something
like the median alright well there is an
answer that's something called the
bootstrap which will allow you to do
that it's a generic procedure for
calculating error bar so the problem is
this me the bootstrap doesn't scale to
large data so we will talk about that so
what's the quality of inference issue so
we have data x1 through xn a machine
learning or stat person might calculate
some kind of a prediction or a parameter
estimate based on the data so I'm
calling it a parameter but it could be
the output of a classifier it could be
any kind of prediction let's call it
theta n I'm not interested in this talk
about theta in and procedures for
scaling that black box
I'm interested in the quality of theta n
what's the error around it ok and there
are procedures for calculating that not
just the theta n but the quality of
theta Anna ok so here's what you would
do if your ideal frequent to
statistician this is almost the
definition of being a frequentist which
is that you wouldn't just have one data
set of size n you'd have multiple data
sets of size in and on each data set you
calculate your median say or your other
estimate ok and now because they're
different data sets they're going to
fluctuate and you look at the spread of
those things and that's your error bar
ok that's almost by definition what you
mean by an error bar it's the error you
get on multiple realizations of the data
alright well you don't have multiple
data sets if you were the supreme being
in heaven you could do this again and
again and n generate data and look at
the fluctuations we only get one data
set all right so let's think inception
about how you might be able to
nonetheless get around this issue and
get error bars even though you have only
one data set well the data came from
somewhere there's an underlying
population up there in heaven that
generates data and so let's let's make a
little histogram like object here a
distribution reflecting the underlying
population all right so if you were the
supreme bean if such a thing exists you
up in heaven where you'll be able to do
what we have on this slide here you
would take the population you would
generate one data set two data sets em
data sets and each one of your data sets
you'd calculate the thing you care about
the prediction theta 1 theta 2 theta M
and then they go into some formula for
the spread and that's your error bar all
right now one nice thing about this is
that if the supreme bean has a parallel
computer he or she could do this in
parallel because you can generate these
data sets in parallel each computer goes
off computes the estimator and then gets
it all brought back together to
calculate the error in the estimates ok
now we don't have multiple data sets we
can't do that slide but what we can do
is the following we observe that if
there's an underlying population the
data came from that population and we
can think of the data not as a list of
numbers now but as a histogram you
probably all know how to make histograms
right
data the Instagram itself is a
distribution it's called the empirical
distribution but it's a distribution it
can be used to generate more data
because it's a distribution even though
it's not as curvy is the night one up
there it's itself a distribution you can
sample from it all right so this is
beautiful idea which is called
approximation which is that that
empirical distribution approximates the
truth because it came from the truth all
right and so you can pretend now that
you don't have that inner line truth but
you have this approximation of the truth
and now you can live in a world where
you're the Supreme Being and that's the
truth and you can generate data from
that truth and you can do it not just
once you can do it multiple times so
it's a very subtle but very deep idea
which you could take one data set and
from that generate multiple data sets
that idea is called the bootstrap and
it's due to Brad Efron in 1979 he got a
National Medal of Science in the United
States for that so talk about awards
that's kind of the biggest one you can
get for this very simple idea one person
sitting in a room thinking a little bit
about the problem and and realized this
can be done so here's the picture you
take in the data and from the data you
form a histogram and you can sample from
that histogram multiple times M times
say you can do this in parallel this is
that even though this alga was developed
in 79 there was no cloud no distributed
anything this was a perfectly paralyzed
abbasi jure okay and they bring that all
together on your central server and you
got an error bar now and so you could do
this for any query you could do I you
know why can't we have all of our
databases doing resampling on the cloud
sounds like a wonderful idea and that's
sort of the thing that I thought about a
couple of years ago why don't we put the
bootstrap into computer scientists more
fully but there's a god show so let's
think about a terabyte of data sitting
maybe on a server somewhere or maybe
it's already distributed if you re
sample if you take that data set and you
do what I said you take it and you
sample from it well I should go back
here I mean I can I go back with yes I
can do this right what does it mean to
sample from that histogram right there I
should make this clear all right well it
was based on n data points but forget
that that's gone it's not
just a histogram it's a distribution I
can sample from at any number of times I
want but let's sample from an end times
because that's the scale or data
occurred on and so I need a sample from
the end times get the same size data
sets all right now I can do that once
and do it again and again and again what
does it mean to sample from a histogram
it means to take one of the points it's
a discrete distribution so I'm going to
get one of those points it's it's
discreet and let me get it with
proportion to the probability proportion
of the height so I get one of those
points and then I do it again I could
get the same point over again or I could
get a new point and I do this again and
I do this n times some of the points
that form that histogram I'll get
several times and some of them I won't
get at all that's exactly the same as
sampling with replacement I take out a
Sam thing I put it back in i could get
it again i could get it again and maybe
I can get a different one alright so the
bootstrap is often described as take the
original data resample it with
replacement in x and do that many times
that's the bootstrap okay now if I
sample a terabyte if I sample n data
points with replacement you can do a
little mathematical calculation and see
you get point 632 of the data points
occur in your resampled data if I had a
terabyte that means I get six hundred
and thirty two gigabytes all right now
I'm not going to send 600 32 gigabytes
off to my my 200 servers so I can do the
bootstrap that's good hopefully let's go
topless Lee blow away my network all
right so you can't do that with a
terabyte of data this sounds like a bad
probably there was our main procedure
for getting error bars on arbitrary
estimators we can't do it at scale and
it's going to get you know terabytes you
know small data these days all right so
this is a serious problem all right so
there is another statisticians worked on
this for a little while they weren't
thinking about computation very much
they're thinking about Theory the
bootstrap fails in some cases actually
and another procedure came out that
fixes some of those theoretical cases
it's called subsampling but it seems
like it's the answer and it's not going
to be but let's think about this for a
minute so subsampling is the same
pictures before there's an underlying
truth you get a data set and form the
histogram based on that and now what you
do is you
say well NN is too big let's now take a
subsample of size be maybe size square
root of n to really bring it down a lot
now I can commute my estimator on the B
points and I'll get a certain number no
9.5 but I could do that again because
there's many ways of taking be points
out of in so I could do this many times
let me get some multiple values of the
estimator I'll get some fluctuations all
right so it sounds pretty good out of
one data set I'm getting multiple values
of the estimator the problem is is that
if you just do that naive that's wrong
because b points the size of the error
bars depends on the number of points
right if I have lots of points and we
get small error bars small numbers of
points big error bars you know the
sample the status you divided by square
root of n for the classical sample later
the standard error of the mean alright
so I bear to get fluctuations but
they're going to be on the wrong scale
and for the statisticians everything if
you get the wrong size the error bars
you're just wrong okay all right so you
can't subsample and just naively compute
estimators and return that as the answer
that just gives you the wrong answer
this seems problematic because you have
a really big data set the only thing I
can think of doing is taking smaller sub
samples of it you know it's the same
task you have to do that but you get the
wrong answer so these guys who proposes
realize this of course it's this key
issue arises that you're on the wrong
scale so they said well let's take that
estimate that that's that error bar
which is now too big because I had
smaller data sets and rescale it so it's
now on the right scale right but how do
you rescale you need to analytically
correct the error bars and you don't
know in general how to do that so for
some black box at somebody and a
database system put in a so-called
user-defined function you don't know how
the scaling is we're classic less than
me like the mean it's the square root of
the neighbor data points but it's not
true for other estimators ok Sony but
that's a problem you'd have to do theory
for every single black box that someone
put into your database that seems a
little bit problematic but</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>