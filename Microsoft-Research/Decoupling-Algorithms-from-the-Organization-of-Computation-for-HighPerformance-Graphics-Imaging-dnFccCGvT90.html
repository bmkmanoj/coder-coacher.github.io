<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Decoupling Algorithms from the Organization of Computation for High-Performance Graphics &amp; Imaging | Coder Coacher - Coaching Coders</title><meta content="Decoupling Algorithms from the Organization of Computation for High-Performance Graphics &amp; Imaging - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Decoupling Algorithms from the Organization of Computation for High-Performance Graphics &amp; Imaging</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dnFccCGvT90" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everybody
so I'm modern Maserati from the research
in South engineering group and I'm it's
my pleasure to introduce Jonathan Ragan
Kelly today to Microsoft to give a talk
here his expertise is in
high-performance graphics but his
interests span multiple disciplines
systems architecture or graphics and
compilers it's actually very nice to see
graduate students working in multiple
areas and excelling in all of them so
he's he did it he's he's almost
graduated from PhD from MIT - he did his
PhD with freedo Duran and some and I'm
gonna sing gay and he is headed to a
postdoc and will very soon know where
that is
so Jonathan thank you dear I won't touch
the cable alright anyway thanks for
coming and thanks for having me so this
is gonna be a kind of overview of a lot
of different research I've done but most
focused on on halide which is a
programming language for for high
performance imaging which some of you
may be familiar with which is the focus
of my thesis so graphics and imaging are
everywhere today
and not just in games movies and cameras
where we traditionally focus in a
cigarette community but also in things
like 3d printing medical imaging mapping
and satellite based science and even
high-throughput gene sequencing in cell
biology in my research I built compilers
systems and algorithms for these sorts
of data intensive applications and
graphics and imaging and specifically I
worked on rendering high-performance
imaging and 3d printing and I'll touch
on all of these but I'm mostly going to
discuss halide again which is a language
and compiler for image processing in
this talk I'll show that the key
challenge is to reorganize or
restructure computations and data to
balance competing demands for
parallelism locality and avoiding
redundant or wasted work and I'll show
that by changing how we program and
build systems to folk
this sort of reorganization we can
enable simpler program code to run an
order of magnitude faster on existing
machines and to scale on the highly
parallel and bandwidth constrained
architectures Moore's laws giving us
I'll show an example of how we can
deliver 20 times the performance of
everyday CEO or double the performance
of a hand-tuned parallel vectorized
imaging pipeline optimized over three
months by an expert working on Photoshop
all while using 25 times less code that
we wrote in a single day and I'll show
how we can generate a completely
different organization on a GPU
delivering several times more
performance without rewriting a single
line but first why do we care about
performance in this domain in the first
place I'd argue that graphic graphics
and imaging applications today are still
orders of magnitude from good enough so
take light-filled cameras by capturing
and computationally reintegrating a 40
array of light you can do things like
refocus photographs after they're taken
correct aberrations in the lens or
extract high quality depth from a single
shot but if we wanted to produce 8
megapixel 4k light-filled video at 60
frames a second current software would
take about an hour for each second of
light-filled video in rendering even
using the same GPUs used to play PC
games and movie frame is still six
orders of magnitude from real-time
multi-material 3d printing has huge
potential to transform manufacturing but
to directly print production quality
objects we need about a trillion voxels
and something the size of a tennis shoe
and the printing hardware is almost
ready to do this but it's just a ton of
computation and data so in this case if
we wanted to finish 10 shoes an hour
we need to synthesize about 4 billion
voxels a second just to keep the printer
busy at the same time always-on image
sensors are going to be all around us
but we need to figure out what to do
with all the data they produce so just
sending it to the cloud isn't gonna be
enough since the best-selling cellular
radio we could build would need at least
a thousand times more energy to send
each frame than it took to capture it
these days most complex sensing uses
imaging somewhere under the hood
from high-throughput gene sequencing to
automated cell biology experiments to
neural scanning and connectomics for
example today it takes a large
supercomputer just to process the scan
of a single cubic millimeter of mouse
brain neurons and early lots of what I'm
going to say in the rest of this talk
applies to almost any data intensive
application not just those involving
pixels so the good news is Moore's law
is continuing to give us exponentially
more computational resources now as you
as we've been told for the past decade
one of the big challenges we face will
be exposing more and more parallelism
for this hardware to exploit now
superficially this seems easy since all
these applications I showed her
enormously data-parallel but the real
challenge here is that all these
millions of data parallel computations
are independent they need to be able to
communicate with each other and the same
hardware trends that have pushed us from
big uniprocessors to lots of parallel
cores have made communication and data
movement dominate the cost of
computation as a whole and my
communication clearly I don't mean over
networks
I even mean inside a computer inside a
single chip and as a result of that
locality or moving data around as little
as possible is at least as big a problem
for future programs as parallelism so to
put this in perspective today relative
to the energy cost of doing some
arithmetic operation on a piece of data
loading or storing that data in a small
local SRAM like a cache can be several
times more expensive moving the result
10 millimeters across a chip is an order
of magnitude more expensive and moving
it to her from off chip Ram is three to
four more orders of magnitude more
expensive than computing the value in
the first place in this disparity
between local communication computation
and global communication is only getting
bigger over time so because of this it
can often be most efficient to make
surprising trade-offs doing things like
redundantly recomputing values that are
used in multiple different places
instead of storing and reloading them
from memory and this is the first key
message I want you to take away from
this talk that parallelism locality and
we're done in the total amount of work
we do interact and complicate
ways they often need to be traded off
against each other to maximize
performance and efficiency of a
particular algorithm on a particular
machine now the architecture community
is well aware of this and they're trying
to exploit locality and parallels and to
improve energy efficiency in our
Hardware designs on the software side
I think we've long recognized the need
to expose increasing parallelism in
applications but as a whole these
challenges are still acute in how we
design software again especially
locality at least as much as parallelism
so given these pressures where does
performance actually come from we
usually think of it in terms of two
factors the hardware and the program
running on it but the competing
pressures on locality parallelism in
redundant work depend heavily on how the
program is mapped to the hardware so the
second key message I want you to take
away from this talk is that because of
this I think it's useful to actually
think of the program is two separate
concerns and this is motherhood and
apple pie for for compiler and
programming language people I think but
still I'm gonna try to emphasize it
anyway it's useful to think of the
program somewhat separately as the
algorithm or the fundamental operations
which need to be performed and the
organization of computation including
where and when those operations are
performed and where their results get
stored and loaded my work focuses
specifically on enabling and exploiting
the reorganization of computation and
secondary to that it's interaction with
algorithms and hardware and in this talk
I'll show that the organization of
computation is key to the things we need
to optimize performance and efficiency
but it's challenging because of the
fundamental tensions in between them but
ultimately if we want to keep scaling
with Moore's law I think we need to
treat the organization of computation as
a first class issue now these same
themes have come up throughout my
research and I'm going to touch on them
in the context of six different projects
I've done in grad school but I'm mostly
going to focus on halide so hey lights a
programming language and compiler for
modeling and exploiting this exact set
of trade-offs and image processing
pipelines this is joint work with a
number of other people at MIT and
elsewhere but most significantly is
Andrew
Adams who was my day-to-day collaborator
on this project at MIT since finishing
his PhD at Stanford and there are papers
covering different parts of this both at
SIGGRAPH last year and then one I just
presented a couple days ago at POV I
here so to see what I mean by
organization of computation I think it's
useful to look at an example if you want
to do a simple 3x3 box filter as two
three by one passes we can write simple
C++ code like this so it's just a series
of two sets of loops over the image
where the first computes a horizontal
three by one blur and stores in a
temporary buffer and the second computes
a vertical one by three blur of that to
produce the output so here I'm using
some slightly assuming operator
overloading to hide some of the
multiplication the indexing but these
are basically just raw arrays now what
if we exchange the loops in each loop
nest most compilers would say these are
the same algorithm since this loop
reordering is is easy to understand
since the operations within the loops
are independent of each other
and this is something most compilers can
easily do today it turns out it's also
important as trivial as it is because of
poor locality the column major order is
actually 15 times slower than the row
major version on a modern machine this
is just a trivial reorganization against
something that most compilers can easily
do already but our focus with halide is
to take this to the next level starting
from what expert programmers currently
have to do by hand and the key thing to
do that is going to be understanding
that the more complex dependence across
the two sets of loops not just within
each one so this is a hand optimized
version of that same function it's
parallel vectorize tiled and fused we
had to restructure all the loops and
introduced redundant computation on the
tile boundaries to decouple them and we
had to change the data layout to make
this most efficient it's a complete mess
given that all we're doing is averaging
together 3x3 pixels but it's another
order of magnitude faster near the peak
throughput for this machine so in this
talk I'm gonna deconstruct what this
optimized version actually does and to
do that first I'm gonna sketch a theory
of this problem in its challenges and
then I'll show how we apply this to
build a model from which we can actually
generate code
now this kind of optimizations hard both
for people and for compilers because
getting parallelism and locality
requires transforming program and data
structure in very deep ways but to begin
with it's not always obvious what
transformations are legal and even when
we know something's legal it's hard to
know whether it's a good idea so again
the things we want to optimize are often
at odds with each other and the optimal
balance is usually very subtle and
complex so just making a single
heuristic guess like most compiler
optimizers do we might actually wind up
making our program ten times slower
rather than faster and critically
libraries of optimized code don't solve
this either taking Intel performance
primitives for example it's extremely
fast at individual operations but no
matter how much they tune the assembly
inner loops in the math kernal library
the individually optimized kernels still
compose an inefficient pipeline since
they can't interleave stages for
locality across function batteries I
believe the right answer is to
explicitly decouple the definition of
the algorithm from the organization of
computation which we're going to call
this schedule so in halide - the
algorithm defines what Diaries are
computed for each pipeline stage while
the schedule defines where and when they
get computed this makes it easier for
the programmer to write algorithms since
it strips out lots of unnecessary
details it makes it easier to compose
small fragments of algorithm into larger
pipelines it makes it easy for either a
compiler or a programmer to specify and
explore the space of possible
optimizations and then it leaves the
back-end compiler free to do the complex
but deterministic work of generating
fast code that implements a pipeline
given a defined schedule so ultimately
that'll let us do the same thing as a
hand optimized version with just these
four lines of code or just to the top -
if we use auto tuning to infer a good
schedule so this example and the work
I'm gonna focus on is at the
intersection of stream programs and
stencil computation but it also draws
heavily on ideas from loop optimization
parallel work scheduling region based
languages and even what I have here is
just a tiny sample I think relative to
most prior language
and compilers the two biggest
differences are that we focus on
treating treating the organization of
computation is a first-class part of the
front-end programming model and we don't
just optimize parallelism and locality
but we also focus on the potential
potential of introducing redundant
computation to optimize the overall
organization so for this type of data
parallel application that I'm focused on
I think it's often easier to understand
the organization if we look at it as a
pipeline and for the simple blur example
if the pipeline looks like this and I've
reduced it to one D but all the problems
are still the same the input images at
the top flowing down through the blur X
and blur Y stages below he's just
corresponding to the two loops we had
I'm gonna use this view to introduce a
theory of the types of reorganization
that we do in halide first so again
what's most significant here is not what
happens within each stage but what
happens across the two stages and the
first versions we looked at execute C
executed each stage breadth-first which
means they compute every pixel in the
first stage before computing anything in
the second that means the first stage
produces lots of intermediate data
before it ever gets used in the second
and as a result it has to be sent far
away and written to memory before
computing the next stage that has to
slowly read it back in and this is
fundamentally because the way we've
organized the computation here has poor
locality so once we compute a given
value in the first stage we have to
compute lots of other unrelated values
before we ever need to actually use it
in the second and in this view once we
compute a value in the first stage again
we have to compete lots of other
unrelated stuff before you ever need to
use it and in this view the locality is
a function of the reuse distance along
this red path we take through the
computation so we can improve locality
by interleaving these two stages instead
I'm going to start with a cartoon
version ignoring the dependencies
between the stages will take care of the
final details later so in an interleaved
organization we first compute a small
piece of blur X then immediately send
the results down and compute the
corresponding piece of blur Y after
we're done we can throw away the
intermediate result and move on to the
next piece
and there we repeat the same process to
compute the two stages on the next part
of the image repeating this interleaving
over the whole image improves
producer/consumer locality and it does
this by reducing the reuse distance
between where values get produced in the
first stage to where they get consumed
in the second and in practice this means
that can be kept nearby like in
registers or the cache instead of having
to move long distances to and from main
memory
this reorganisation of computations
often called fusion because we've merged
the two the computation of the two
stages together in a practice this makes
optimization a global problem of
carefully interleaving the computation
of the whole pipeline so again I want to
emphasize that you cannot address
locality just buy locally tweaking
operations in your in your inner loops
now Fusion is something that the
compiler community has done for decades
but this type of pipeline is actually
more complex than most of what people
traditionally do with what they call
fusion so to understand how we can
actually fuse a given computation we
have to look at the dependencies and
these are the details I was ignoring in
the cartoon version a second ago these
dependencies are a fundamental
characteristic of the algorithm we've
defined and in this case looking at one
pixel and blur why it depends on three
neighboring pixels in blur X but its
neighbor depends on an overlapping set
of pixels this means that the pieces or
tiles that we wanted to interleave
during fusion actually depend on
overlapping tiles further up the
pipeline and we usually wanted to couple
the execution of tiles both to minimize
reuse distance across them and to
execute them in parallel but to do this
we have to break the dependencies
between them and splitting them up means
we need to duplicate the shared work the
shared computations along the boundary
which introduces redundant work and this
is again a key trade-off that experts
make all the time when hand optimizing
this sort of pipeline but which most
existing compilers can't consider so by
removing that dependency we can execute
in parallel both within each tile of
each stage and across the tiles so now
we can independently compute a tile to
first send it down and compute parallel
tiles of the second from that then each
of these independent computations can
throw out their intermediate data moving
on to the next
that they need to process so we not only
have parallelism here we also get good
locality from the short fixed reuse
distance with it within each of these
tile computations but to get there we
had to make a trade-off we had to
introduce redundant work to break the
dependencies in the algorithm and enable
this overall reorganization and in this
case that's the right choice but it's
not always going to be so this blur
we're looking at it's just a trivial two
stage pipeline and something more
realistic looks like local applause and
filters on the right and even actually
that as a heavily simplified cartoon of
local of flash and filters graph local
apply and filter says about a hundred
stages connected in a complex graph and
locally most of the dependencies are
simple stencils like in our blower but
globally they're extremely far-reaching
and complex and overall this gives us
lots of choices about the organization
and degree of fusion at each of dozens
of stages throughout the whole graph
each strategy can mean a totally
different set of dozens of kernels or
loop nests completely restructuring all
the code and the difference between two
plausible strategies can easily be an
order of magnitude so put this in
concrete perspective local applying
filters is used heavily in in Adobe's
Camera Raw pipeline which is a major
feature of both Photoshop and Lightroom
Adobe's original version was written by
one of their best developers and about
1,500 lines of C++ it's manually
multi-threaded and hand coded for SSC
and it took him about three months to
implement and optimize just like our
blur example the optimized version is
about ten times faster than a clean
version ran in about three hundred lines
of C but then last summer I took a
single day to rewrite their version of
local apply and filters in halide and
integrated into lightroom it took sixty
lines of code to express the same
algorithm and ran twice as fast as their
version in the same eight core machine
at the same time Adobe was starting to
investigate GPU acceleration but they
were constrained by the cost of
potentially rewriting tens of thousands
of lines of optimized code across this
and related code bases but on that same
day and actually this number is now 9x
with a slightly different schedule on
that same day just by changing the
halide schedule we also generated mixed
CPU GPU code that ran
seven - now now nine times faster than
the original eight core CPU version and
the best scheduled schedules on
different architectures here optimize
parallelism and locality in different
ways so this wasn't just a translation
this was a totally different
organization of the computation it was a
few hours of each and a few hours of
integration and I was actually partly
drawing on some existing related stuff
we've done so it might have taken a day
and a half if I'd done it entirely from
scratch so so the model we have for
expressing the space of choices is
exactly the same and there's a
one-to-one mapping between the way those
choices work out and one back-end and
the way they work what kind of what they
mean and another but the best choices
are gonna be different and I'll get to
this later but the the we don't have any
heuristic optimizers going on here so
there's so so far all of these
everything I'm gonna show is either
handwritten schedules or stochastic
search over the space so there's no
awareness of whether locality might
matter more or less in different
contexts it's just purely here uh
empirical it's based on what runs fast
yeah I'll come back to them in the end a
little bit more at least and something
this complicated it's the answer is a
lot of tiny stuff and it's actually hard
to even fully tease it apart it's a
basically a different giant set of
several dozen loops or kernels but yeah
I will come back to that in the end so
this brings us back to the first message
I gave you which was that optimizing
performance requires fundamental
trade-offs so we saw there's a tension
between locality or reuse distance and
introducing redundant work to break long
dependency chains and an algorithm we
can trade-off along this axis by varying
the granularity at which we enter leave
these stages so we can use smaller tiles
for better locality or bigger tiles to
minimize the fraction of verdun a
computation along the tile boundaries it
turns out there's also a tension between
these and parallelism which requires
independent work both within and across
stages so my second message is that
these trade-offs are determined by the
organization of computation so we've
seen that changing the order and
granularity of computation can have a
huge impact on performance again in this
visualization we've been using these
aspects of organization or given by the
red path we take through the computation
and the algorithm defines the
dependencies between the individual
tasks this leads to my last major
message which is that the ways we can
reorganize this graph and the effect
each choice has on parallelize and
locality and total and a total amount of
work we do is a function of the
fundamental dependencies in the
algorithm and this is also one of the
places where domain-specific knowledge
can make the biggest difference so
putting it all together I think really
what I've been showing you is actually a
very general problem formulation we
basically just have a task graph we have
pixels or you know per pixel
computations as the tasks themselves
they're linked by their dependencies
which are encoded in the algorithm and
then we can organize the computation by
choosing the order in which we traverse
the graph or the task schedule we can
change the schedule either to minimize
reuse distance for locality or to
maximize the number of independent tasks
for parallelism and then finally we can
also break dependencies and split the
graph but this introduces we're done the
computation on the boundaries
so we have all these choices but most
traditional languages deeply conflate
the underlying algorithm with these
sorts of choices about how to order and
schedule its computation so back to the
optimized blur again it's completely
different than the original the code
uses different instructions different
data layout and deeper loop nests each
airs almost no code with the original
version but all that's happened here is
this is the sort of reorganization I
just showed to optimize for parallelism
in locality and specifically it's mostly
doing that sort of tile level fusion
that was the main thing that I just used
so the parallelism here comes from
distributing the work across threads and
from computing 1/8 wide Sindhi chunks
within those threads just like you might
imagine but again exposing that
parallelism in a problem like this is
actually far and away the easy part just
as important and often a lot harder to
think about our Express is a locality
through the pipeline without this kind
of locality optimization even the most
well parallelized pipelines still
severely bottlenecked on memory
bandwidth and here the optimized code
improves the locality by computing each
stage in tiles interleaving the
computation of tiles across stages and
keeping the intermediate data and small
local buffers that never leave the cache
it has to redundantly recompute values
on the boundary between intermediate
tiles to decouple them now as
programmers we often think of this as a
different algorithm from the original
but just like the simple loop
interchange I showed at the beginning
I'd argue it's more useful to think of
them as the same algorithm but where the
computations just been reorganized and I
don't expect everyone to agree exactly
with my choice of the word algorithm
here but I do hope that you can see that
the separation of concerns is useful so
then for a given algorithm we want to
find the organization that optimizes
performance and efficiency by making the
best overall use of parallelism and
locality while minimizing the total
amount of work did we do and the
compiler communities done heroic work to
automatically understand and reorganize
computations like this coming out of you
know an existing language like C but
it's fundamentally a challenging problem
again halides answer here is to make the
problem
easier by explicitly decoupling the
definition of the algorithm from the
organization of computation which we
called the schedule so the algorithm
defines what values get computed for
each pipeline stage and the dependencies
in between them
well the schedule defines aware and when
they get computed so once we strip out
the concerns of scheduling the algorithm
is defined as a series of functions from
pixel coordinates two expressions giving
the values of those coordinates the
functions don't have side effects so
they can be evaluated anywhere in an
infinite domain and the required region
of each stage gets inferred by the
compiler the execution order and storage
are all unspecified so the points can be
evaluated or re-evaluated in any order
that can be cached duplicated thrown
away or recomputed without changing
their meaning so the resulting code
looks like this for the simple 3x3 blur
the first stage blur X is just defined
at any point X Y is the average of three
points in the input and then blur Y at
any point X Y is the average of three
points and blur X and notice that we
don't have any balance over which these
are being computed these these are
functions over an infinite range of free
variables this is basically a field over
a 2d grid the specific scope of this
programming model is what lets us have
lots of flexibility in scheduling so
first we only only model functions over
regular grids up to four dimensions in
the current version second we focus on
feed-forward pipelines which is a good
fit for this domain but not for
everything we can't express recursive
functions and reductions but these have
to have bounded depth at the time
they're invoked these last two points
actually mean that our programming
models not technically turn complete
because we need potentially infinite
size pipelines to express arbitrary
complexity computations also because of
the fact that as you saw with a little
snippet of code for the blur algorithm
there aren't any bounds or sizes of
regions that are computing here to work
well we need to be able to accurately
infer the dependence patterns between
different stages and for most patterns
or analyses are both general and fairly
precise but in some cases the programmer
might need to explicitly clamp an index
expression to a reasonable range to
ensure we don't allocate too much
intermediate buffering and finally our
problem domain is different than most
traditional work on stencil computations
in scientific computing because we're
those usually deal with thousands of
repeated APRI applications of the same
stencil to a single grid our
applications are often are usually large
graphs of heterogeneous computations
often over 100 different stages or more
with complex and varying dependence in
between them and we potentially need to
schedule all those stages differently
yeah so the only context we can infer
good bounds through everything except
for stuff that's minoo like just
fetching an arbitrary memory address and
in that case the the bounds are just the
bounds of the type we can throw a
warning there we don't currently um
usually you will run into the fact that
that will allocate too much memory
instantly the first time you try to run
it that's actually an easy thing to
check and then we have a built-in
clamping expressions in the language
that you'll frequently want to use
anyway for at least at the very
beginning generally you wind up writing
you know clamp to edge on your inputs or
a few other things like that and then
through the whole rest of the pipeline
you don't bother with thinking about
boundaries and little you know fringes
of necessary excess are added as needed
but but everything kind of carries
through fine yeah so um so if you look
at how the algorithms were written
everything's written from the
perspective of the output point so one
stage basically says might you know
you're not you're not eating over some
domain and reading from somewhere in
writing to somewhere you're saying at
any logical point in my domain which in
that case would be the output matrix the
value at that point is given as some
expression of you know the coordinates
and other previous functions so in the
case of transpose that would be easy in
the case of situations where you end up
with kind of all all communication like
trying to write an FFT in this that is
not the focus of most of the scheduling
transformations that I'll show you we
can do that but it's but it's not the
Conte
where we can have the most power so you
mean like a data dependent kind of work
or yeah so you you I mean what we would
usually wind up doing in that context is
right basically right like a
reconstruction the second stage would be
you know a nine tap filter over the
previous stage with the actual indices
being computed values based on the scale
factor and the in the input indices does
that make sense
are you wondering about how we can
actually comprehend what the bounds of
that are so that's I mean that would
just be an expression that I think that
would just wind up being an expression
where the weights are a function of some
other expression if that makes sense but
you you would usually still be writing
it from the perspective of every every
output location so I mean it's it's the
same way you'd write it if you wrote a
GPU shader that wanted to do that if
that makes sense if you were manually
filtering your textures from on some
some non integer sized your sampling
operations because it just as soon as
the data is there
so let's say you're doing some imagery
scaling yep you want to do it by
user-selectable now and we're gonna even
ignore the filtering part you know just
nearest neighbor right but if I say in
one iteration of my rescaled by three
point seven
yup exploration by seven - yeah if that
is a bounce analysis has to change
correct well mmm the bounds analysis
just has to be conservative with respect
to that so one thing that we would
commonly do in that context is just say
that that the schedule which I'm gonna
explain now should have computed to that
entire prior stage if if you're
potentially going to be using large
rescaling factors so that you so that
you you have you know you may be
accessing very you know differing ranges
of the image that so I guess I take two
things first our bounds analysis doesn't
infer static constant bounds it
statically infers expressions that
compute the bounds which means that if
those bounds are some kind of expression
in terms of like every scaling parameter
and an input image size parameter or
something like that then the bounds at a
given point in the computation will be
computed immediately before doing it so
it's it can adapt in that kind of way
but if you're likely to be you know
touching an entire image for even very
small regions of an output then it
usually is the best choice to make sure
that entire image has been computed into
a buffer that you can access just
because that's the most efficient
strategy yes but but at any given if you
think of splitting it up into tiles and
you schedule which I'm about to explain
be the input to that to be computed as
needed for tiles of the output then the
bounds on what what size tile of the
input you need for each of those tiles
is going to be computed dynamically
every iteration so we're really
injecting dynamic expressions that
compute those bounds as it goes around
like a as needed so given that model of
the algorithm the schedule defines the
organization of computation by
specifying two things first
for each stage in what order sure to
compute its values
so remember computing whole grids of
pixels here
so in what order should we compute the
points in this grid for each function
second when should each stage compute
its inputs or at what granularity and
this defines the interleaving of
computation between the producers and
consumers in the pipeline which
determines the amount of fusion between
stages and this is the model we used to
describe this the space of computation
organizations I showed earlier it can be
specified explicitly by a programmer or
inferred and optimized by the compiler
but different choices about order within
and across stages can describe all the
types of reorganization I showed earlier
so first the schedule defines the order
we compute values within each stage and
this is just simple stuff you might
imagine if we have a 2d function we can
traverse it sequentially across Y and
then X giving a simple row-major loop
nest we can feat the X dimension and
four-wide vectors we can transpose these
dimensions if we want we can distribute
the scan lines across parallel threads
and we can split the x and y dimensions
to separately control the outer and
inner components of each so in this case
by reordering the split dimensions we
get a simple tiled reversal now what all
these choices actually do as you might
imagine is specify a loop nest to
traverse the required region of each
function so for example the serial Y
serial x order gives a basic row-major
loop nest over the required region of
the function and it's to the point about
dynamic bounds I'm using y-min and y-max
kind of expressions there and in
practice there would be some computation
immediately before this that might be
hoisted if it's independent of
everything in between but might not if
it's not that actually computes what
y-min and y-max are in terms of its
whole environment so putting everything
together
we can split into two by two tiles
spread strips out over parallel threads
and compute the scan lines and vectors
this synthesizes a for deep loop nest
including a multi-threaded parallel loop
and an interval up and the specific
complexities here don't really matter I
just want to give a sense that the
choices here are both rich and
composable and to be clear this stuff on
the right is not code any programmer
writes it's just meant as a
representation of what these choices on
the left
correspond to so this is basically what
the compiler is synthesizing under the
covers yes
I know you specified this for all the
stages you can specify this for all the
stages there are same defaults but but
you but you specify this for every stage
so you're actually making both these
order choices and the interleaving
choices that I'll show in a second for
basically every edge in the whole graph
it's a lot more less verbose than that
sounds and it can be inferred
automatically so then that's once we
have the loop nests for evaluating each
functions own domain we need to decide
how to interleave them with each other
to compute the whole pipeline so the
other thing the schedule defines is the
order of evaluation between producers
and consumers in the pipeline which
determines the degree of fusion between
stages so for simple point wise
operations it makes sense to fuse the
pipeline in line in the computation of
each point in the first stage into the
corresponding point in the second then
as soon as we apply the tone adjustment
to one pixel we can immediately color
correct it throw it away and do this
across the image producing and consuming
individual pixels through the pipeline
but the simple strategy only works
because the one-to-one dependence
between pixels here and that's normally
what we would mean by loop fusion so if
instead we had a large blur as the
second stage then each pixel in the blur
depends on a lot of pixels in the in the
first stage so the dependence between
even far away pixels has lots of overlap
and if we fused we'd have to recompute
or reload lots of the same values over
and over again for each point in the
blur so in this case it makes more sense
to pre-compute the entire producer stage
before evaluating the entire consumer
stage so we can choose to sacrifice
locality to avoid wasting too much work
and finally for stencil operations like
a three by one blur each point in the
consumer stage depends on a bounded
window of the previous so in this case
full fusion would inline lots of
redundant computation where the stencils
overlapped while computing breadth-first
would completely sacrifice
producer/consumer locality so not
surprisingly the right answer is
somewhere in between so we can schedule
the stages to be computed in smaller
tiles size to fit into nearby caches and
if we then interleave the computation or
for producer tile with the correspondent
consumer tile and do the same thing over
the image this is the same choice we saw
with the author
- 3x3 blur example earlier we may have
two redundant leary compute values on
the shared boundary between tiles but
again by changing the size of the tiles
we can trade off between locality and
redundant computation which is spanning
the space of choices between full fusion
and breadth-first execution so our model
of scheduling is specifically designed
to span the space of locality trade-offs
and to describe them separately for
every pair of stages in the pipeline and
the way we actually trade this off is
the most fundamental idea that
highlights design we do it by
controlling the granularity at which the
storage and computation of each stage
get interleaved so the granularity with
which we interleave the computation at
terminus house when you use values after
computing them so here fine interleaving
provides high temporal locality quickly
switching back and forth between
computing and consuming values while
coarse interleaving provides poor
temporal locality computing lots of
values before using any of them then the
storage granularity determines how long
you hold onto values for potential reuse
so fine grain storage only keeps values
around for a short time before throwing
them away which potentially introduces
redundant work if we need them again
later
well coarse grain storage keeps values
around longer to capture more potential
reuse so using the examples from the
previous slide full fusion interleaves
the storage and computation of two
stages at very fine granularity this
maximizes locality but potentially
introduces lots of redundant work
breadth first execution avoids any
redundant work but to get that at
sacrifices locality entire level fusion
trades off between these extremes
depending on the size of the tiles that
are interleaved but now what about the
whole other axis of this space it turns
out it's also possible to maximize
locality while doing no redundant work
but doing so requires constraining the
order of execution which limits
parallelism so by setting the storage
granularity to the coarsest level while
interleaving the computation per pixel
which was the far bottom-right corner of
that triangle we get a sliding window
pattern like this
so now after computing the first two
values in the first stage for every new
value computed we can immediately
consume it to compute the corresponding
value in this
second this says excellent locality and
it computes every value only once
wasting no work but to do this notice
that we have to walk across the two
images together in a fixed order so
we've sacrificed potential data
parallelism to optimize locality and
redundant work now I want to emphasize
that this these examples I've been
showing are not just a laundry list of
scheduling options fundamentally it's a
parameterization of the space of
trade-offs for pipelines of stencil
computations like this and our schedules
directly model and span this whole space
yeah yeah absolutely so so one of the
reasons why the domain order stuff
matters is because that it's the
dimensions of those loop nests at which
we can determine how to interleave
storage and computation so you can split
dimensions a whole bunch of times create
a recursive tiling and and actually I'll
in just a second I'll show a couple
things like that so yes the whole point
is that we can combine all of this stuff
in different ways for all the different
stages
it's not gonna be a flame grading org so
the way we actually think about it is
one of the big realizations here is that
you actually can think about it that way
I'm sorry so if you think about it in
terms of compute and storage granularity
you can express the sliding window
ignoring the fact that we're reusing
storage like if you imagine I left all
the previous pixels I computed sitting
around it corresponds to the coarsest
granularity meaning I can I store
everything for all time a coarse
granularity of storage but the finest
grained interleaving of computation so
for every new pixel I compute I compute
one of the consumer and then the actual
transformation that that reduces the
amount of storage to use a small
circular buffer and recognizes exactly
what previously computed values can be
excluded is dependent on the fact that
we're walking along it in a fixed order
so it's basically by picking some point
down here in the space where we have a
large window over which we could reuse
old values but we're quickly
interleaving computing one new value and
consuming a new value combined with the
fact that the intervening loops are all
in a known order you know they have a
known dependence vector that gives that
pattern so that kind of makes sense it's
a little easier to see on the whiteboard
but so yeah one of the big realizations
here is that that actually does fall
into this space which was a bit
surprising
so even for a pipeline as simple as the
two-stage blur we can express an
infinite range of choices and you don't
have to follow what's going on in all of
these and I'm actually just gonna jump
through quickly but the top three sitted
extremes of the trade-off space and the
bottom three balance them all in
different ways and here's what they
actually look like running so some of
these your Western Matt actually are
doing combinations of you know basically
sliding window on chunks or or line
buffering or other things like that and
the bottom three in particular are
making fairly complicated combinations
of tiling and sliding windows and so on
and different strategies here are best
depending on the exact algorithms use
the structure into which of a pipeline
at which they're composed and how they
interact with the underlying hardware
and the difference between these can be
more than an order of magnitude and
performance and efficiency on the same
machine even between two plausibly
high-quality ones so putting this all
together to describe the optimized blur
looks like this it's the same blur
algorithm from before and a figure at
the scheduled equivalent to the
optimized C++ so first it schedules blur
Y to be evaluated in 256 by 32 pixels
tiles each of these tiles is computed in
eight wide vectors across the inner x
dimension and and the scanline strips
are spread across parallel threads then
it schedules the intermediate stage blur
X to be computed and stored as needed
for each tile of blur Y and it also
vector eise's it across X so these
compute and store at correspond to the
positions on the two axes of that
triangle and they're given relative to
effectively loop levels of the the loop
structure that we've defined for the
previous function that makes a bit of
sense it's a little easier to see on a
whiteboard this generates nearly
identical machine code with the
optimized C++ but unlike to see it's not
only simpler the halide version makes it
easy to add new stages change the
algorithm or change the organization it
can also compile not just to x86 but
also to fast arm neon code with a few
tweaks to the schedule we get a GPU
implementation actually a whole range of
GPU implementations we have a bunch of
other language features which I'm
actually going to skip our current
implementation is an embedded DSL in C++
and it uses LLVM for code
generation so to compile a pipeline we
take the halide functions in the
schedule and combine those into to
synthesize a single loop nest and a set
of allocations describing the entire
fuse pipeline for a given architecture
then after our own vectorization off the
pass and other optimizations we pass it
LLVM to emit vectorized x86 or arm code
or CUDA kernels and graphs of CUDA
kernels in the x86 code including the
logic to launch and manage them and in
that last case on the GPU we can
actually generate heterogeneous mixtures
of both vectorized multi-threaded CPU
code intertwined with many different GPU
kernels depending on the structure
implied by the schedule so to pop back
up the last question is how can we
determine two good schedules and so far
we've worked on two ways and I've
alluded to this a couple of times this
it's come up but first you can write
them partly or completely by hand
we weren't even when we did built this
model we weren't ever expecting real
people to write this by hand but why not
being surprised how far you can get with
a teeny bit of syntactic sugar on top of
it
just how terse the schedules wind up
being even for fairly complicated
programs so it's a lot easier than
traditional hand optimization both
because they're they're compact but also
because you can't accidentally break the
algorithm and the compiler synthesizes
all the complex control flow and data
management for you that's implied by
what your schedule says we found expert
developers usually using this at places
like Adobe and Google actually
appreciate the control and
predictability this gets them second we
can automatically search the space of
schedules for a given pipeline on a
given machine using Auto tuning and the
dimensionality of this problem is large
so we focused on stochastic search but
it works surprisingly well in practice
now when present results from a few real
computational photography and imaging
applications we've built in halide which
I chose because they represent a wide
range of different program structures
and levels of optimization in this
domain and for all of these I'm going to
show the complexity of the best hand
written schedule but in every case the
auto tuner was able to match or beat
that in a you know some number of hours
without any human intervention so first
we looked at the bilateral grid which is
a fast algorithm for the bilateral
filter we implemented it in halide and
compared it to a pretty efficient but
clean sea
plus plus version by the original
authors our version uses 1/3 of the code
and runs six times faster on the same
machine and then with a different
schedule we can run on the GPU instead
and there we looked at the author's
handwritten cuda version and started
with the schedule that matched their
strategy and that gave almost the same
performance they had but then we tried a
few alternative schedules and found a
slightly non-intuitive one that was
twice as fast as their hand tuna crudo
so to the point earlier about why we're
beating hand-tuned code in this case in
every case we're not doing anything you
couldn't do by hand it's just a question
of making it much easier to search the
space quickly so in this case the ideal
trade-off even on it on a very parallel
GPU chose to sacrifice a little
parallelism to improve locality during
the reduction stuff that computes the
grid and that made an enormous
difference in the performance
yeah um so the that so all of the
allocations in one of these pipelines
are implicit in the schedule they're
they're basically implied by the store
app granularity and the semantics are
when you're running on a conventional
you know CPU ish architecture what we're
basically doing is building a giant loop
nest or set of loop nests that all runs
in one place and so allocations wherever
all kind of mean the same thing we have
some optimizations for keeping stuff on
the stack or doing small pool
allocations if we know that something is
constant sized or whatever but generally
it memory is memory in the case of
running on the GPU we have two separate
memory spaces so everything that's
allocated externally to stuff that
corresponds to a kernel launch is is
mirrored in both spaces and lazily
copied it's I mean it's actually not
it's not eagerly narrowed it's it's
lazily neared which you basically have
to do if you're doing computations and
both spaces but we have semantics where
allocations that correspond to
granularity within the the thread
dimensions of your kernel launch
correspond to to local memory and
allocations at the block level
correspond to shared memory so you can
effectively span the space of things you
reasonably do that are very important to
organization like registered pressure
and memory bank conflicts are very
important anymore actually you would you
be surprised yet basically that they
didn't used to be doing cache reads and
writes in the general memory system and
now that the cache is generally yeah so
actually even even even for me
that got better so it's basically now
instead of throwing reads and writes
directly at the memory system and hoping
that they'll cover all the banks you
read and write to cache lines and the
cache lines are size such that those
transactions fill the memory system it's
the same way as CPU works
yeah so that's actually defined by the
there's basically a mapping between you
know what would have been like parallel
loops when I showed you the loop
synthesis and you carving off some set
of those and saying these correspond to
the block and thread dimensions of a GPU
launch and so the granularity which you
block that corresponds exactly to the
block size
to get launched on the device yep so
suppose I work for a big company and I
have a problem that I've run over and
over and over again thousands and
thousands machines and maybe suppose
that program is the one that you just
showed me yep how do I know or do you
have any ideas as to how like how I'm
willing to wait you know have this thing
run for weeks on item to find the best
schedule do you have an idea as to how
blue the schedule you can get you're
searching for needle in haystack so all
these problems do you know like what's
the throughput of the machine and how
close so we haven't for simple examples
it's relatively easy to build kind of a
roof language model so for the blur
that's relatively easy to figure out
actually being able to reason about
bounds based on the fundamental
structure of the of the program you know
what the intrinsic arithmetic intensity
of different choices is stuff like that
is something that we're interested in
doing but it's not something we've done
at all I think this type of
representation gives you a lot more view
into that than just writing C that reads
and writes memory willy-nilly and that
kind of view that I showed earlier where
you think about reuse distance and and
you know like crossing dependencies and
stuff like that as a natural view from
which to begin to get a more concrete
sense of that kind of thing of what the
fundamental limits on an algorithm are
but in practice we haven't done anything
yet so I'm gonna skip this example it's
a hell of a lot faster than even very
good MATLAB not surprisingly but not
because MATLAB slow but because of
locality and finally in the local apply
and filters example I showed earlier the
win here wasn't again because we emitted
better code it actually came from fusing
different stages to different degrees
and accepting some redundant computation
in exchange for better locality so our
implementation didn't just make a few
extra optimizations beyond what the
original developer found it had the fuse
and schedule each of the hundred stages
differently to globally balanced
parallelize and locality in the total
network and our strategy is actually
substantially different through it
basically it's like we can talk about it
offline with some of the some of the
interesting things we noticed but and
then the strategy we wound up using on
the GPU is completely different from
that yet again so it's if you were
rewriting this to
what are these different choices you
would be tearing up and rewriting
something on the order of 80 different
imperfectly nested loops and all the
allocations and boundary handling and
everything along the way so just sorry
anecdotally kind of going back to the
fireplace yeah did you see that that on
average most most slates so you get an
order magnitude range in this way the
kind of the claims you're making right
yeah you see those numbers bear out but
our most schedules around average and
some a handful or so so it's you can
write the whole space of things that we
model includes a great deal of stuff
that makes absolutely no sense
introduces huge amounts of redundant
computation has you know crazily large
intermediate allocations other things
like that so it's I wouldn't say that
most in any statistical sense are around
average the space is pretty spiky we
generally found without putting any
heuristics into the auto tuner it
converged you know a long kind of a
smooth exponential decay kind of curve
you can make of that what you will there
may be sort of a normal distribution of
performance in that space I'm not sure
when we did this stuff by hand we
usually kind of sampled a few we had a
few hypotheses about points in the space
or strategies that might work kind of
tried them twiddle some stuff around
them and usually tried about ten total
things in the variation among the best
few was usually not that big the
variation overall was often order of
magnitude but which we're gonna be best
we didn't know ahead of time and we were
often totally wrong so that's our
experience so far with halide recap
again the big idea is to explicitly
decouple the definition of the algorithm
from the organization of computation and
I think the biggest thing here is that
we designed this model of scheduling
specifically to span the space of
trade-offs fundamental to organizing
computation and data parallel image
processing pipelines and more than
anything else I think it's that model
that's actually what let's this be
possible
it's the unification of and
composability of that model that lets us
promote it all the way to be visible to
the user unlike most compiler
optimizations it's you know the the kind
of orthogonality of that model that let
span the whole space and walk through it
automatically and that's really the key
to all this and also in this domain and
I think probably in others you want to
be really important to put put emphasis
on the potential of redundant
computation to break actual dependencies
in an algorithm and minimize computation
because on modern machines that actually
pays for itself an impressive amount of
the time so here are the end results
were simpler programs that run faster
than hand-tuned code will give in
composable and portable performance are
composable in portable components that
can scale all the way from our mobile
phones which I didn't show we have a
whole suite of all these results on arm
all the way to massively parallel GPUs
our implementations open source under a
I think permissive enough for you guys
license and MIT license and is being
actively developed currently
collaboration of people at Google and
Adobe people are using it at both
companies and I can actually now tell
you that every picture taken on Google
glass is processed on board by pipelines
written and compiled with a line so I
have other projects to discuss but we've
spent a lot of time talking about other
things so I think I'm gonna leave it
there thank you yes oh I think this not
as a domain-specific language in the
sense of being having much to do with
image processing itself so much as the
schedules in particular or a print are I
think a fairly complete parameterization
of the ways you can interleave
computation and storage on computations
over graphs of computations over
rectangular regular grids which is
basically what images are and so we have
I've done some you know fluid simulation
a few other things in this context like
I said the things that do all - all
communication don't benefit from a lot
of these types of reorganization they
benefit from things that from this
perspective would be actually
algorithmic reorganization if you wanted
to do an FFT or something like that that
doing algorithmic combined with
scheduled reorganization for
approximate convolutions is actually
something we're beginning to look at the
extreme example being the GPU efficient
recursive filtering kind of idea I'd see
that in this view that's not just a
reorganisation it's not just a change of
order it also takes advantage of you
know a substitute E in linearity and so
on to rewrite the algorithm or what we
would mean by the algorithm here and
then also actually among the other
things I was going to touch on we worked
on applying some similar ideas outside
of graphics in a different language so
this is not in halide but we were using
the pedo bricks language sorry this is
actually not a great slide to talk over
is meant to be a quick summary but we're
using the pedo bricks language which is
built around auto tuning to determine
lots of choice similar sorts of choices
and in this context we were working on
mostly traditional HPC kinds of code
this is also mostly over regular grids
that's their program models primarily
focused on stuff this data parallel in a
relatively similar fashion but the
trade-offs explored here included not
just the types of organization I
discussed but also algorithmic choice
so in particular things that that
naturally lend themselves to divide and
conquer algorithms there's often
different different types of algorithms
you can use at different levels of
version and and in this case there was
we were basically inferring a decision
tree to decide what strategies to use
including different algorithmic choices
your razor generate a shopping
so that the scheduling model doesn't
express anything like that but we have
when I say we have our own vectorization
we're not just relying on an automatic
loop vectorizer or something like that
we're directly constructing kind of SOA
sim decode and then our back-end
actually does you know people
optimization kind of low-level
instruction selection for yes so we
generate whatever has been necessary so
far we basically have a people you know
a pattern matching phase at the end that
looks for common substructure in the in
our vector ir and substitutes common
intrinsics for those so we're emitting
machine code directly but we're going
through we omit LLVM intrinsic search
basically just say I want this exact
instruction to be selected here right so
we have a variety of optimizations in
the different backends
this type of code is actually a large
subset of exactly what both SSE and neon
were designed for so there's a lot of
esoteric instructions that actually want
it being useful in different contexts
and dealing with strided loads and
things like that or packing and
unpacking stuff is one of the common
contexts where we try to make sure that
the front end and main lowering process
will always generate relatively common
patterns that we can pick up in the
backend and replace with specialized ops
and I can think of a variety reasons
for example if I write up an app I want
to know that it works well and
generational forever and any generation
yes other example is like the Photoshop
layers listening where the user is in
charge of composition and just don't
know a runtime yeah so I think of those
being different actually in the first
case I don't think you need runtime
compilation you just need what you think
of a sort of load time compilation so
performance portability probably
requires you know well for one thing one
of the big reasons why we have this
whole idea of split scheduling for them
from algorithm definition is that our
assumption is that the choices we're
making in the in the organization are
not going to be performance portable
that you're gonna want different
organization on different machines so
you need to have some way of deciding
what the best organization is on some
future machine but that compilation
doesn't need to be happening with
amazingly low latency because it's not
in like the UI loop it's happening once
when the user salutes the program on the
new machine or something
the latter context is a bit harder where
you're actually re specializing programs
depending on but and so you you're
potentially rescheduling or the the more
extreme case which I worked on in the
context of visual effects is that you're
actually specialized in the computation
based on knowledge that certain
parameters are static or other things
like that and trying to fold it in
different ways and so in in this context
we were basically trying to accelerate
lighting preview people are only
changing a few parameters of a 3d
rendering and so we automatically
analyze the program figure out what was
changing what wasn't the vast majority
wasn't we could auto paralyze the stuff
that was we had all kinds of aspirations
about adapting to to individual
parameters being edited one at a time
and it turned out that dealing with that
was monstrously more complicated and
much higher overhead then then just
doing a fairly good job specializing for
the common case I think if you look at
something like Lightroom it's kind of
halfway in between the general Photoshop
layers or you know compositing graph
kind of case and a totally static case
they have a bunch of fast paths that
they hand code right now where when
you're you know they're from say a
half-dozen fast paths for preview
or final res with or without a few major
features turned on and off like whether
you do or don't have to do the scale
decomposition to do all the local tone
adjustments or like whether or not
you've cashed the results of D mosaicing
and I think picking a handful of of fast
path optimizations compiled ahead of
time can be extremely fruitful because
then you're just quickly choosing among
you know a few places to jump to but
injecting the compiler into the middle
of the overall process is at least a
hassle and often hard it's hard to even
if 80% of the time it makes things go
better having nasty hiccups 20% of the
time is often not worth it to say it's
for how I'd summarize my experience is
that kind of thing so one thing you can
do with with Auto tuning and people have
done is try to auto tune over a large
suite of tests producing a more than one
answer for the right for the for the
tuning so you can say I want the five
schedules that best span this space you
know best optimized overall best for
this large set of benchmarks and if into
that set of benchmarks you include
different types of patterns which might
be specialized against you know just
running quick previews with certain
things off versus having everything
turned on that's a much easier context
in which to automate that kind of thing
I think
is precision versus yep and there's a
whole bunch of bite specific business
vector instructions yeah have you
thought about early on was so our native
types I didn't show you any types
anywhere because everything is all
inferred but it's all also it's all just
inferred using basically C style
promotion rules from and for you can use
explicit casts from whatever your input
types are so we do a lot of computation
on like 8 and 16-bit fix point numbers
in many of the pipelines we use we did
work hard on the sort of vector op
pattern matching for those types because
that's a context in which there's a lot
of specialized operations that can be
actually really useful
you know saturating ads and other weird
things like that we did early on we were
thinking that that we might want to have
these programs be able to be
parameterised over some space of choices
on precision that you might use and even
be able to build in potentially higher
level knowledge of like fixed point
representations to handle that kind of
thing automatically we wound up deciding
to keep it simple and keep it to the
semantics of basically the kind of stuff
you'd write by hand in in something like
C today but in the long run I think that
would be a useful higher-level front-end
feature that would get both more
flexibility of the compiler and make it
easier to explore that space by hand
so the the vectorization way we express
it you remember that we were
synthesizing loops over the domain of a
given function so we have some rectangle
that we have to compute we're
synthesizing loops that cover that our
model of vectorization is some one of
those loops or a split subset of one of
those loops gets computed in vector
stride of some constant width so I say I
want the X dimension to be vectorized in
strides of eight if that one function
includes a bunch of different types
we're not necessarily going to be the
back end might be cleverly selecting
different different width ops so doing
you know two sequential ops on on 32-bit
numbers while doing only one off on
16-bit numbers if we have a hundred
twenty-eight bit vector unit usually
what we'd wind up doing is the vector
isin to the kind of the maximum width
that any of the types flowing through
that function would have if that makes
sense
I can show you on a whiteboard but we
the all our operations all our general
operations are agnostic of the
underlying vector types and so if you
express and you know a 256 bit vector op
like adding to 8 by floats together if
you're on SSE or neon that will get Co
generated into a sequence of 2 ops so
you can think about it as just like
unrolling the regular vector loop and
that those kinds of details lined up
being taken care of in in the backend
instruction selection but you're not
specifying like per op how wide you want
to do it it's just perfection so how
much of the performance numbers you're
getting our function of your people
optimizations
just relying on LLVM to be the right
thing um I think that the big win so if
we if we throw in schedules that are
equivalent to simple see the big winds
are from restructuring the computation
so it's not a we're mostly relying on
LLVM for for register allocation really
low-level instruction selection and and
and and low-level instruction scheduling
no not until now yeah three point three
which came out like two days ago is the
first time they have any any significant
vectorization how can you say well
there's multiple answers to that
depending on what you mean in terms of
loop Auto vectorization or super word
level parallels and stuff that's only
out literally right now and we've never
built on any of it depending on the
backend they especially in the case of
neon they actually do a pretty good job
of the same kind of people optimization
that we're doing or they're where
they'll recognize a sequence of you know
of simple you know AB Mull
something-something and recognize that
that can be fused in a certain way into
a magic neon op so generally the way you
target weird ops in neon is to figure
out what their magic pattern is and and
use it but that's really only true for
our min our experience on x86 they don't
make any effort to do clever instruction
selection for you they generate like if
you're multiplying two single precision
float for vectors they'll give you a Mel
PS and other than that they will just do
the stupidest simple thing you could
possibly imagine and so we have to be
much more aggressive with the people
optimization on x86 but I think that's
that's mostly just like the last factor
of two of that kind of stuff it's just
making sure it doesn't do something
stupid and then it really is vectorizing
by the width you told it to all the time
and not spilling everything to the stack
and you know packing and unpacking the
vectors between every op and that kind
of thing
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>