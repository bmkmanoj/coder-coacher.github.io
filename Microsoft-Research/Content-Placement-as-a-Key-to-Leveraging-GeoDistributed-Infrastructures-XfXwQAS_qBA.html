<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Content Placement as a Key to Leveraging Geo-Distributed Infrastructures | Coder Coacher - Coaching Coders</title><meta content="Content Placement as a Key to Leveraging Geo-Distributed Infrastructures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Content Placement as a Key to Leveraging Geo-Distributed Infrastructures</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XfXwQAS_qBA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thanks for coming today we have each
other pleasures here I began sorry from
Yuma's I began finishing his PhD with
that alone and he has done some very
interesting work on infrastructures for
content distribution networks and also
some work on mobility and stuff so I
began hi thanks for the introduction I'm
I began from UMass and i'll be talking
about ygo distributor internet services
must get their content placement right
before anything else so let's start by
looking at how our computing
infrastructure is going to look like in
the future today cloud computing
platforms with tens of data centers are
pretty common now these geo distributed
platforms are important because
propagation delays on the Internet are
fundamentally limited by the speed of
light now having a data center close to
the user reduces propagation delay LED
leading to and improve performance a
given the importance of geo distribution
it is entirely feasible that in future
we may see computing platforms with
hundreds or even thousands of locations
besides data centers other forms of
computing infrastructures can emerge for
example set-top boxes cellular base
stations and routers are all being
discussed as potential avenues for
running general-purpose computation and
in future we may see these devices being
used for them now so it's possible that
in future our infrastructure is going to
be more geo distributed and possibly
heterogeneous as well another trend
that's already common and is going to
become probably more common in future is
the network mobility of endpoint
principles such as interfaces devices
internet-connected thinks content and
services now network mobility presents a
challenge to establishing end-to-end
communication because it changes the
network location of an endpoint now
there are several examples of network
mobility a user Alice who's initially at
work and has a particular interface on
Wi-Fi and cellular when she reaches home
she may have a different interface on
these different IP address on these
interfaces
similarly a file stored in dropbox may
be on server in one data center but over
time it could get moved to a server in
another data center there by changing
the network location at which the
particular content is available so given
these projections about how a computing
infrastructure is going to look like in
future my research vision for future
internet services comprises of three
main elements first we'll need geo
distributed services that leverage that
the geo distributed resources that are
potentially heterogeneous as well second
we'll need services that enhance our
ability to handle the mobility of
endpoint principles and third we have
need net up metta services that automate
the deployment and reconfiguration of
these geo distributed services now in my
word towards realizing this vision in
the start I am going to talk about two
things first is a geo distributed
service which is a network CDN and this
is a content delivery network that's
deployed by an Internet service provider
from the core of the network will move
on to the endpoints and we'll see how
auspice global name service enhances our
ability to handle the mobility of these
endpoints now keet focus of this talk is
geo distribution and so let's start by
looking at what are the advantages geo
distribution bring in terms of the
degrees of freedom so in the simplest
case when resources are at one location
only the degree of freedom we have is
that of choosing the underlying network
routing from the end user to the data
center where we are located on the other
hand when you resources that geo
distributed to additional degrees of
freedom come into play first this
content placement that is selecting the
locations where content is placed and
second is request redirection that is
among the multiple locations that have a
content which copy of content should a
particular request be sent to so three
degrees of freedom placement redirection
and routing so let's start with network
routing first now network routing are
traffic engineering is
problem that's commonly solved by isp
internet service providers today and a
simple view of this problem is as
follows the input is a traffic matrix
and the IJ entry in this matrix
represents the traffic demand from a
node I to another node J in the network
now based on this traffic matrix and the
link capacities of the links in the
network traffic engineering outputs
network routing or the set of physical
paths between all nodes in the network
now these traffic engineering functions
typically optimize network cost
functions and these cost functions are
defined in terms of are typically convex
functions of link utilization for
example a common cost function is the
maximum link utilization so why is the
utilization of links important that's
because a low utilization of all links
implies that that work is free from
congestion hotspots and has more spare
capacity now a simple example can show
how traffic engineering reduces this
link utilization so on the Left we have
a shortest path routing and we send two
mbps of traffic from node a to note B
you send this via the bottom link
because it has more capacity and this
gives an ml u of 2 over 3 or point 6 7
on the other hand if we are a bit more
smart about the routing we can split
this traffic between the top and the
bottom link which gives a smaller ml U
of O point 5 so a smarter routing can
reduce link utilization and network
costs now among the three decisions
placement redirection and routing it is
content placement that's perhaps the
most important to leverage the power of
geo distribution so let's start with
redirection first now the objective of
redirection is to select a location to
which to send request among the ones
that have the content but if your
content is placed only at a far away
location then irrespective of how you do
a direction and end user is going to
observe high latency on the other hand
content placement can create more
options for the redirection scheme to
choose a location that is nearby so
let's
consider placement versus routing
suppose we need to compute routing for
this traffic matrix the flexibility of
placing content is so powerful that it
can change this traffic matrix itself
now if we if all the content that a node
in the network wants is placed at the
same node now in that case there's no
need to send traffic to any other node
and all the entries in this traffic
matrix turn 20 so there's no doubting
problem to be solved now this is clearly
an extreme example and you may have
resource constraint on what you can
place at a location but it demonstrates
the power of placement / routing so with
this introduction to placement
redirection and routing let's see how
these play out in the case of a network
CDN now this is a new and relatively new
and a potentially transformative change
in CDN landscape the traditional
tripartite p of content delivery has
three sets of entities now you have gone
in providers who produced content
internet service providers to provide
network connectivity and a CDN which
delivers content via an overlay network
of servers now in more recent times ISPs
have started to deploy Syrian
functionality as part of their
infrastructure giving rise to network
CDN soar NCD ins so why are these ISPs
interested in and Syrians so one reason
for this is the rise in Internet traffic
especially video traffic a nice PC NCD
ins as a way to leverage content caching
to reduce traffic on their backbone and
reduce the cost of provisioning network
capacity another major motivation for
isps is to monetize this NCD and service
for example by selling an on-demand
video service to the end users today 30
plus n syrians are there including some
of the biggest telecom providers in the
US including AT&amp;amp;T and verizon and this
number is expected to grow because
syrians have started offering the
technology to isps to deploy these NCD
ins so NCDs are potentially a
game-changer for the syrian industry and
in this stock will cyn seed managing a
10 Syrian is a different problem for
managing a traditional eyes p or
traditional CDN
so before we go there let me quickly go
over the the abstract model of an NCD
and that we work with so we have points
of presence or pops of a network CDN
which consists of backbone routers but
as well as content servers that deliver
content to end users additionally we
model origin servers that are maintained
external to the N CDN and they can be
reached via a few exit nodes that
connect to the origin the origin acts as
the source of all content when an end
user makes it request it first goes to
the nearest pop if the content is
available there send to the user
otherwise the content is fetched from
any other pop inside an N CDN and send
to the user if it happens that the
content is unavailable inside the end
CDN it is of course fetched from the
origin and returned to the user so based
on this network CDN model we posed the
following NCD and management problem now
to our knowledge we are the first to
pose this question where a single entity
handles all three tasks placement
redirection and routing and the main
difference from the traffic engine
problem is that the input here is a
Content matrix and the IJ entry is this
in this matrix is the demand for a
Content I at a node J so based on the
another two inputs are the resource
constraints that we have which are the
link capacity and the storage capacity
that is available at each point of
presence we also have an input on the
content size vector which represents the
size of each content in the content
matrix and the output is a set of
variables that define placement
redirection and routing now a natural
question is to ask okay why are we
trying to make these three decisions
together why not we solve these problems
independently and the reason is that
these decisions interact in case of a
network CDN and this interaction between
placement and loud routing illustrates
this so suppose that's our network and
we have a Content that's placed at say
one of the nodes in the network see with
a demand of one at node a and point five
at node D
now the question is how do we serve this
demand to minimize the maximum link
utilization the optimal solution turns
out to be follows which gives an ml u of
0 point five now in order to achieve
this ml you you need to use a flow split
routing which splits traffic from C to a
among the two paths CBA and CDA now
unlike the traditional isp model where
the ISP has no control over content
placement and in Syrian has the power of
placing content so let's say NN CDN uses
this power to also place the content at
node B now in this case that it's easy
to see that the mle reduces to a much
smaller value and moreover we can use a
simpler shortest path routing to achieve
this minimum ml you so this example
illustrates the sophisticated
interaction that happens between
placement and routing in a 10 CDN and it
is to leverage these types of
interactions that we define the joint
optimization for NCD ins and joint
optimization works as follows first we
measure content matrix all over then CDN
joint optimization runs as a centralized
computation its output placement and
redirection are fed back to the content
servers and the network routing is
configured at the network routers now we
have shown that this joint optimization
problem is np-complete and is in a
proximal to within a constant factor
hence how do we solve this so for that
we defined a mixed-integer program a
formulated and mixed into the program
and solve it using a standard
mathematical programming solver a
dismissing to the program optimizes the
cost function with subject to
constraints on link and storage
capacities and the mixed integer program
cost function could be a traffic
engineering related cost function such
as ml you our cost function dependent on
user perceived latency now we use this
joint optimization to define a planned
approach for managing an NC DN to
understand this approach let's consider
let's divide time into equal length
intervals and
a particular time interval T now we use
CMT to denote the content matrix in that
time interval and we use prrt to denote
the placement redirection routing in
that time interval so the planned
approach uses CM t minus 1 to compute PR
PR RT that is it uses the content matrix
in the immediately preceding interval to
compute the decisions for the next
interval now this is a realistic way to
do joint optimization because it uses
only previously observed content Amanda
if we have also defined a scheme Oracle
which has future knowledge of content
demand and that scheme uses CMT to
compute Bihar RT now Oracle is clearly
unrealistic to implement because you may
not have perfect knowledge of future
content demand for all content but still
Oracle is useful to benchmark the
performance of the other schemes now
this planned approach is not that simple
to implement first it requires fine gain
measurement of content matrix in each
interval and moreover it requires
solving a potentially expensive mixed
integer program that becomes intractable
for large problem sizes therefore we
also considered some simple unplanned
asst approaches and these reproaches
require no measurement of the content
matrix and use simple heuristics for
placement redirection and routing so for
placement it uses simple LRU caching at
each node it redirects each request to a
location that is closest in terms of hop
count distance and has the available
content and for network routing it uses
a static shortest path routing in which
link weights are configured to be
inverse of link capacities and this link
weight settings that's default
configuration for shortest path routing
protocols and commercial routers and
next I'm going to present some results
which compared this unplanned approach
with the plan and the Oracle approaches
for real network topologies and real
content access traces so we collected
content access traces from Akamai the
world's largest CDN the data contained
data from 7 million users who made more
than downloaded for than 1,400 terabytes
of data and the results shown here is
for the network topology of a Tier one
isp in the US the y-axis in this curve
is the network cost function value an
x-axis is storage ratio so we are
operating in a constrained storage
regime because if we have infinite
storage then we can place probably all
content at all locations so not much
problem to be solved there and now as
what we see that as storage ratio
increases content can be placed at more
and more locations in the network so
network so traffic on the network link
reduce which reduces the maximum link
utilization now what's surprising is
that the plan joint optimization
actually has a 3x higher value than the
simple and planned approach so why is
this so the reason is that there's a lot
of change in content popularity even on
a daily basis in the real content
workloads and a lot of requests are for
new content that's published that day
the planned approach makes its decision
based on content matrix for the previous
day and so it often does not have the
content placed inside the network it
needs to fetch this content from the
origin which increases the utilization
of the network links and increases the
ml you now we have also considered
several other variants of this plant
approach which vary the duration of the
content matrix the frequency at which
the planned approach is implemented and
combined plan with unplanned approaches
but still the planned approach performs
worse than the unplanned approach in
those experiments also so that's one the
next thing is that the simple and
planned approach actually performs
pretty well and has a network cost the
difference is actually eighteen percent
at the storage ratio of four so why does
the unplanned approach perform well the
reason is that its placement strategy
LRU caching is actually pretty good now
due to locality in real world workloads
what we see is that its cache hit rates
exceed ninety percent in some of the
cases so it's effective in placing
content inside the network which reduces
the network costs and leading to a good
performance so the high-level take away
from this graph is that
content placement matters tremendously
network syrians the planned joint
optimization performs poorly because
this placement strategy is ineffective
in placing content in the network on the
other hand the unplanned approach works
well because it's placement LRU caching
achieves good cache hit rates so a
natural next question is to ask how much
does network routing matter in network
CDN yeah sure cause fully unplanned case
also include the fixation of the coming
from the origin servers yes we do
account for that cost for all schemes so
even when planned approach goes from one
configuration to the other there is a
cost of content movement and that is
also accounted for so how much does
routing matter and to answer this
question we considered our basic
unplanned approach with static shortest
path routing which is scheme that does
traffic engineering that is it optimizes
routing based on previously measured
traffic matrices and the y-axis in this
curve shows the maximum reduction in ml
you that traffic engineering achieves
over the static shortest path routing
across all storage ratios and x axis
shows the results for the various
stresses now we find that the maximum
reduction in m lu is ten percent or less
which suggests that optimizing routing
gives only a small cost reduction in
network syrians so based on these two
findings are overall takeaway message to
network syrians are as follows first
network CDN should keep it simple
because a realistic joint optimization
actually performs worse than even simple
and planned schemes such as LRU caching
and stylish hardest part outing and
moreover the unplanned approach performs
close to the ideal at higher storage
ratio suggesting that there is only a
small room for improvement over the
simple and plant approaches the second
message is that content placement
matters more than routing a network
Syrians it matters more where you place
the content in the network and matters
less how you route to that
there is no not really point for doinker
and networks idiots this is that
effective means their findings suggested
optimized you know work of operating the
networking operating the city and see
the baby please more or less just talk
tonight yeah that is true but it doesn't
operators still have motivation for
network syrians because if they have the
syrian service a they can have a wider
deployment in their network and be if
they have it they can make money by
selling the seed yeah you can operate
yes so you do not need joint
optimization yeah the simpler approach
is work well okay so from the core of
the network will move now to the
endpoints and we'll see how hospice
global name service enhances our ability
to handle these mobility of these
endpoints now let's start by looking at
the state of affairs for mobility in
today's internet today tis difficult to
initiate a general purpose communication
with a mobile device because there is no
global infrastructure to locate it now
as a result mobile communication
initiation is mostly unidirectional in
the internet which is from the Mobile's
to the fixed hosts now in the name of
mobility support what we have are
redundant app specific solutions
developed by several applications such
as wipe and messaging applications
notification services cloud cloud
storage applications and so on now how
many times have you faced this problem
you're downloading a large file from a
web server and you need to leave for
somewhere anything okay should I let
this download complete and then leave or
should I leave now and restart to
download all over again later similarly
popular wipe applications sometimes drop
calls instead of seamlessly switching
from Wi-Fi to cellular now the root
cause of these problems is that
communication on the internet happens
based on IP addresses which keep
changing due to mobility now what does
not change is the name or the identity
of who we are communicating with thus
if we can enable communication our names
we can address mobility now a global
name service enables name based
communication by keeping an up-to-date
mapping from name to network addresses
for all names now given the usefulness
of gns or global name service and
handling mobility we stake the goal for
designing it as follows so we seek to
design a massively scalable geo
distributed global name service to
enable secure name based communication
it should allow us to communicate with a
flexible set of endpoint principles such
as interfaces devices services content
and so on and it should not restrict how
these endpoints will choose names it
could be a hierarchical name like
today's DNS are a flat cells certifying
identifiers or other application desired
formats now the challenge here because
industrial and other proposals to solve
this problem like list which Cisco's big
on are you going describe why you're
sort of revisiting this problem ah duh
there's so this broadly it has been
three different ways to handle this
problem and one of the ways is to is the
name service approach the in fact this
particular approach has also been
proposed what we are going to see why we
need a new design for a naming system
and that I will discuss in the later
part of the talk ok ah so besides the
naming approach there's other approaches
that have been proposed but we believe
that a naming service approach is
preferable to these other browsers and i
will refer you to the paper if you are
interested in that question i will
discuss why we need a new naming system
design naming service design in the talk
and the challenge is to handle the
update load which mobility creates and
imagine the scale of 10 billion devices
that are changing their addresses at the
rate of 100 times per day and that's an
average load of 1 million updates per
second so now in achieving this goal we
make two main contributions first
is a clean slate naming system design
that has support for arbitrary names and
has a decentralized root of trust now
neither of these are satisfied by
today's naming system dns and next is a
scalable service for mapping names to
network addresses that provides small
latency for a name to address mappings
under high mobility and the service is
deployed well in DNS today so let's
start with the naming system design now
in DNS so this goal of security in this
naming system design is to verify the
identity of who we are communicating
with and in DNS we have domain names
that got me up to IP addresses and a set
of DNS SEC extensions have been proposed
to enhance the NSS security but the
problem there is that due to DNS is
hierarchical design security in DNS six
depends on a chain of trust with a
single root of trust so how do we avoid
the single root of trust problem in fact
our problem is more general in that we
are seeking to math arbitrary names to
IP addresses and not just hierarchical
names like DNS our solution is to this
problem is to decouple the process of
name certification from name resolution
by introducing an additional layer of
naming and this is a self set this
additional layer is a self certifying
globally unique identifier or a GU ID
and by that I mean it's the public key
of the name owner we propose the use of
name certification services to map an
arbitrary name to that bind an arbitrary
name to itself certifying GU ID and the
identity of the gns provider now this
gns provider will then map the GUI d to
current set of IP addresses so this this
split allows multiple name certification
services to exist there by resolving the
single root of trust problem but there's
two additional issues issues first is
how does it how do you know which
arbitrary name maps to a particular name
certification service and to that end we
propose the use of certificate sells
services that index
that index certificates from various
ncess and next the next question is how
do you verify the identity of this cell
certifying GUI d now this isn't really a
problem because an endpoint principal
whose name is a self certifying GUI d
can authenticate its identity to anyone
without requiring any NCS unknown
certification service now the design is
also more flexible in the dandy honest
because it allows for arbitrary names
and moreover the gns provided design can
itself map names to network addresses
that are represented in any format and
of course it can map domain names to IP
addresses so this can be deployed in DNS
today as a scalable authoritative name
service provider and in the next part of
the talk then the next part with rock
I'm going to talk about the design of
the system auspice and the chair key
challenge that this system addresses is
to provide small name lookup latency
under high mobility so why is this
challenge and this challenge is there
because there is a cost of replicating
these name records so let's say we have
this rectangle which represents the
total capacity we have a false service
now a part of this capacity would be
used in handling name to address lookups
from the clients so that's the look up
cost now another part of this cost is
the update cost for maintaining the
consistency of the name records that are
stored in the system because these are
getting updated by the clients now in
fact the more the number of replicas of
the name record that we maintain this
update cost increases now in general
when you consider a name in the system
its update cost depends on the number of
active replicas as well as the rate at
which the name is updated and this is
what creates a trade-off between the
update cost and the latency of name
lookups and simple schemes replication
schemes do not achieve a good trade-off
for example a scheme such as replicate
at all locations that scheme can
potentially reduce
toluca platon see if there is enough
system capacity but it's still a poor
choice because of the high update cost
on the other hand a consistent hashing
waste scheme which is another well-known
scheme with a few statically chosen
replica locations that this scheme can
potentially reduce would reduce the
update cost but it's bad in terms of
look up Layton sees because these
replicas are placed at randomly selected
locations and not close to regions of
demand now we want this particular
system to achieve a much better
trade-off so when resources are small so
that you can only create few replicas we
still want the system to give a lower
latency lookup latency but when more
resources are available we want the
system to use these resources to further
reduce lookup latency so our system does
achieve this favorable trade off using a
demand of a placement approach and the
number of replicas in this approach is
chosen proportional to the lookup rate
that is which means that the most
popular name the greater the number of
replicas and inversely proportional to
the update rate which reduces the update
cost for names that have a high update
rate and the proportionality constant
there is chosen based on the system load
now what about the locations of these
replicas so let's say these ten are our
geo distributed servers and we have a
name I which is popular in the regions
shown by the blue circle and another
named J which is popular in the region's
shown by the yellow so we place the
replicas of a name in a geo locality
aware manner close to regions of demand
but this approach alone is not enough
because it can sometimes create load
hotspots for example if we repeat the
same process with the name Jay it
creates a load hotspot on were in the
service therefore auspice places a
fraction of these replicas randomly to
trade off locality and load balance now
there's one more important component in
the system which is the component that
actually implements this demand of a
placement approach and that is the
placement engine so to understand this
let's assume s 1 to s 10 are are the
different servers that we have so the
role of the placement engine for a name
is given to
a fixed subset of these servers so we
use consistent hashing based on the name
to select this fixed subset of service
and that those servers act has the
placement engine for this name and the
the job of these placement engine
service is to select then the active
replicas which actually store the name
record so let's say these four servers
are chosen as the active replicas and
these active replicas in order to help
the placement engine make these
decisions they have they report the Geo
distribution of demand back to the
placement engine and if again the do on
geo distribution changes the placement
can get a plate now this is a
distributed system operating in a
synchronous environment so consistency
guarantees are important and the
placement engine input provides a strong
consistency guarantee by providing a
total ordering of rights to name to a
total ordering of Rights and the active
replicas they provide they have an
option of in this design they had they
can provide flexible consistency
choosing between a less-expensive
eventual consistency and more expensive
total load right of rights for a
particular name need I guess doesn't the
lesson from the first part of the talk
then why doesn't something simpler work
here in that you know if you think about
names most likely they'll have a zippy
and distribution in terms of popularity
in which case if it's a 95 5 you just
click the gate the five percent active
everywhere and the ninety-five percent
nowhere has at one location with that
suffice so what we're talking about here
are you know first of all the different
distribution itself we are talking about
names of individual mobile entities and
so a particular name may not be that
popular it's possible that most of them
are very unpopular okay so distribution
be almost a flat line yeah probably is
why demondo where doesn't you know so
that is accurate the aggregate request
rate may be similar but the geo
distribution of demand for a particular
name may still be different and can
still be exploited
so the total demand by that I mean the
number of requests that comfort that may
be the same but the geo distribution can
still be exploited so this service does
not exist today so it's hard to get
evidence but we can we can still surmise
that you know if you a large fraction of
people that you communicate with maybe
from one country or two countries so
there is evidence that you can have to
your distribution so that was that is
actually pretty a difficult evaluation
challenge that we have in our system and
and this experiment demonstrates that to
some of the simple schemes that we
considered before as well as DHT based
scheme or do not achieve the same trade
offs as auspice so the first scheme is
replicate at all locations and that
scheme actually can sustain a very small
request load because it quickly uses all
its capacity in processing rights at all
locations so the moment the load
increases our little the it exhaust is
capacity and the lookup latency just
shoots up so x axis is the request load
y-axis is the lookup latency now the
next scheme is random k which makes
three replicas of a name at randomly
chosen locations and the third scheme is
a DHT based design that is proposed for
DNS so somewhat surprisingly we see that
the random k scheme actually performs
better than the DHT based design and
this difference occurs because the
difference in redirection policies so
DHT sends a request to a replica
selected using DHT based routing which
is often not the closest one but the
randoms the case sends a request to the
closest of the three replicas which is
why the random k does better but the
real gains that we see are from auspice
which gives which places these replicas
more intelligently close to regions of
demand and as the load increases what we
see that and the other difference is
that auspice can also sustain a high
request load so unlike replicate or
twill which quickly fizzles out and
whose latency initially is actually
pretty good but then can't sustain a
high request load auspice reduces the
number of replicas as load increases
which also enables it to sustain a
higher request load as well so the
takeaway here is that
static placement like replicate at all a
random k as well as DST based design do
not achieve the same trade-offs s
hospice and next we compared auspice
against the state of the art
authoritative name services that are
there in DNS today and these are called
managed DNS providers one such provider
is ultra DNS and which has 16 known data
center locations yes what is the mom
coming from should you generally kind of
random demand sorry I should talk about
that question yes so we didn't have a
real-world workload so this is a
synthetic workload and the demand for a
name so we define a Geo locality metric
which says which allowed us to play with
it so the gol a calorimetric essentially
is that seventy-five percent of the
requests are coming from ten percent of
the location and the rest 25 persons are
distributed randomly so that's the Geo
distribution question there are other
aspects of the workload read to write
ratios yeah so we have a sensitivity
analysis of those in the paper of course
the system does better when there's more
geo locality well when there is no geo
locality even then it does better than
some of the something like random k
because it can choose the number of
replicas more intelligently in that case
most of this does the other endpoint
send each know where my mobile phone is
are going to be services that run in the
cloud somewhere right why don't you just
let each service manage its own naming
of the end points that need to talk to
it is it can even be the same data
center do you really need a global
naming think rehearsal yeah that is the
central argument we do need a naming
service because each service today is
solving this problem redundantly for
themselves in that it needs to maintain
infrastructure which maintains
persistent connections to all these
phones to know where they are what we
are saying is that we can provide a
different service I mean a common
infrastructure service which all
services can then use to know where the
mobile endpoint is and that can allow
them to communicate with mobile
endpoints so we are trying to abstract
out a common functionality with
different services are implementing
today
hadi running as a single internet white
service or could it just could you just
replicate the code at each of these web
services there would be benefits for
statistical multiplexing which you would
get by running it as a centralized
service of know the weather you expect
you not to be like in a realistic load
scenario could assume that you know the
more you go to us the right the less is
the benefit of your solution from got to
random so the the load really is here is
relative to the overall system capacity
actually I the last point after that
both this this random k and the auspice
both of them showed up so that really is
the load limit when you don't have any
more system capacity after that last
point okay so this is the comparison
with a managed DNS provider so first we
measured ultra ID genesis performance
for its customer domain names from many
plant lab locations and then we deployed
auspice on planet lab but restricted it
to create only 5 10 or 15 replicas now
the y-axis here shows the distribution
of look up late and sees four different
names and x axis shows the results for
various experiments so what we see is
that with one-third the number of
replicas are already in is that is one
thread the update cost auspice gives a
similar median of lookup latency and
when the number of replicas is similar
to ultra DNS it gives a sixty percent
lower median Layton sees and these gains
come from being able to choose the
locations of these replicas in a more
intelligent manner so the takeaway from
this is that auspice reduces costs or
latency or today's managed DNS providers
which use a static placement scheme such
as replicate at all locations what
performance is it sunrise ok can you
play the future
yes it does seem to have it I am sorry I
don't have a good explanation for that I
wouldn't explode in detail why the tail
performance ahead but yeah that is there
yes it could be I do not have a good
explanation for that game right one
difference is that auspice is running on
a shared infrastructure and planetlab
pitches already shared by the other
services that could play out we have to
be a factor so the summary of this start
acepta so auspice enable secure name
based communication allows both names
and the network locations to be
represented arbitrarily it has support
for flexible endpoint principles and the
main differences for dns are these first
it decouples certification in named
resolution second it allows for active
it makes aggressive use a factor of
replication and demand of a placement to
reduce the latency of name lives okay
yeah you kind of wish you and promised
capability of drink Curie's around
target i mean a challenge in your case
lacked long and stuff but in that
context whatever that you do their
mapping from the crystal mapping doesn't
seem such beauties so something like a
lat long is is a much more sophisticated
query to make to a name service so that
it so the system that we have it does
provide support for storing that
information and that you can store more
complex attributes lat latitude
longitude but the query is not the query
pattern is not optimized it and the
system that's actually a future topic of
virtue which some of the other people in
the group are exploring on how to how to
enable the name service to support
something like a lat long query yes yes
yes yeah yes a a two-dimensional query
so that is the more sophisticated
you extend it to supporters career it
something very different and you need
you need a different service you need a
different service but you need your
global name service of an a sibling of
the global in service service okay so
the overall takeaway is that in a
Content placement decisions make a lot
of difference when your resources are
geo distributed I have two pieces of
news first the bad news the bad news is
that if you start off with a poor
placement then you can't use redirection
and routing to redress the damage fully
so you have to do your placement right
then good news is that simple schemes
for placement such as LRU caching and
the simple place materialistic in
hospice give good results so any
questions from so with that I'm not
interested I will talk a bit about some
of the future work that I'm interested
in doing so until lobby I'd assume that
the infrastructure is dedicated to
running a particular service but in
general infrastructure is shared among
multiple services so in that case
natural problem is to ask okay which are
the locations that I am going to run
this particular service on and making
these decisions automatically based on
say the the pricing of computational
resources at eat infrastructure the
demand geo distribution of at several
locations moreover the whatever we have
we have a different cost budget for
running a particular service which may
determine how many locations we can
choose and I use particular service may
have a particular performance
requirement which may also determine
which locations to run a particular
service so automating the deployment of
a Geo distributed services based on
these constraint I think it is an
important problem and solving it would
simplify our management of deploying
these geo distributed services and I
believe that we can design a meta
services that automates the deployment
and reconfiguration of these services
that can solve this problem is a common
task for all these services however the
challenge it's not
simple problem because there may be a
cough be a cost to creating a new
deployment locations for a service
including the cost of data movement the
cost of moving VM images and so on and
wherever the service today is very
complex it may have several
interconnected components so the
placement for a particular component may
restrict where other components are
placed but I think doing this in an
automated fashion would simplify how we
deploy to your distributed services so
that's one question I'm interested in so
people are talking about running
computational resource info general
purpose application and so on some of
these platforms and a risk security risk
associated with doing so is that if
you're in network devices untrusted
applications and if these applications
misbehave then they could degrade
performance for a wide range of futures
for example by flooding the network with
packets so how do we design a software
platform that enables these applications
to be useful but restricts the
misbehavior that they can cause a third
copic which I am currently working on I
am interested in future is the energy
efficiency of our computing
infrastructure currently I am working on
a system for a particular class of data
centers which is a content data center
and we are looking at ways to reduce the
energy use of servers and switches by
turning a fraction of them off and the
key novelty here in the system is that
it allows an operator to say okay this
is the level of performance that I want
now you can save the maximum energy that
you can do but under this constraint so
determining the right point of operating
in this energy performance trade-off is
the problem that this particular work is
solving it's a natural next step would
be to look at the problem of global
energy management of data centers now
you can you can play around with more
variables here you can have exploit the
end prices of electricity between
different locations or even turn off
some of the data centers to further
reduce like operational costs like
schooling but doing so good in too great
performance for users because it could
increase network latency increase
network loss by sending over a different
path so what we need is a system that
that you
available information from a network
information plane to make these
decisions now so these ideas have been
explored by a theoretical analysis and
simulations but designing distributed
systems that do this for real is an
interesting research challenge okay with
that i'm including and i'm happy to take
more questions a few questions okay if I
got a right you said that if I use Liu
by cashing forum for placement then
that's a strategy double so yeah so I'm
kind of missing inside there because I
would expect the kind of content you're
talking about to be a sort of follow-up
power distribution in which case Liu
doesn't perform well at all I because i
want a distribution power law all of you
of that says right so you have some
company success a lot and allowing the
day of that israeli access so if you
have that can you sell like you my
intuitions that that's not going to will
work well at all site that an RPG
elaborate a little um i have an opposite
conclusion which is that what it is for
the a large the fee and popularity
distribution we're a small most of the
recursive for a small fraction of
content that you can have still have a
small resources and still get most of
the cache hit rates that you can achieve
it you do exploit the so one thing is
the isn't if if you have a few content
which are very popular then you're then
most of your cache hit rates will be
derived from that particular the most
popular content the tale of the content
doesn't contribute to cash it if you
have a lot of if you have a lot of a
request for things that are already
accessed they're going to kick out the
stuff you said think I'm fast enough
they can end up leaving no room for for
for the content they can get more heat
so um we can take this offline notice
yeah it's done I would say that the
amount of first of all the results are
for real traces and
we haven't significantly over
provisioned allow you at all so the
total amount of storage that we
provision is essentially just twice the
amount of twice the size of content that
accessed on a typical day I mean twice
the size of unique content that access
so your replication is essentially two
in the system so we aren't
over-provisioning LRU and these results
are for the actual trace although I
would agree with one question mean if as
the workload becomes less and less cute
the more flat it becomes the verse is
the performance of hell are you yeah
cheated stricted is the hospice
deployment on planet lamb and as it does
it include all the mechanisms like serfs
are defying certificates and all the
mechanisms that you mention are
implemented and the problem I have and
can we use it there is a there is a
portal which you can create a GUI d and
make it simple operations that
deployment is not on plant lab that is
on ec2 we did deployment on planet
laughs for 100 locations and it did use
the it did it did perform all the
operations for the name certification we
have a simple name email based
verification service where you can
create a GOI d by which Maps the your
email address to a particular gyd and
the name service does do the crypto
operations and all that new because the
SMS trick I was also used in like the
donor work by walfish right so what's in
the security trick is there actually
knew the security trick there is so the
use of Justice L certifying GOI DS is
not new but to I don't remember seeing a
paper where it has been said that doing
so decouples name certification and
resolution and solves the single root of
trust problem Indianness this
observation i think is new we I haven't
seen that in any prior work
any more questions for Abigail if not
let's track team again thank you all for
coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>